LEARNING DISJUNCTIONS OF CONJUNCTIONS

L.G. Valiant
Aiken Computation Laboratory Harvard University Cambridge, 11A 02138

ABSTRACT
The question of whether concepts expressible as d i s j u n c t i o n s of conjunctions can be learned from examples in polynomial time is i n v e s t i g a t e d . P o s i t i v e r e s u l t s are shown f o r s i g n i f i c a n t sub classes that allow not only propositional pre d i c a t e s but also some r e l a t i o n s . The algorithms are extended so as to be provably tolerant to a c e r t a i n q u a n t i f i a b l e e r r o r r a t e in the examples d a t a . I t i s f u r t h e r shown t h a t under c e r t a i n r e  s t r i c t i o n s on these subclasses the learning algo rithms are well suited to implementation on neural networks of t h r e s h o l d elements. The p o s s i b l e im portance of d i s j u n c t i o n s of conjunctions as a know ledge representation stems from the observations t h a t on the one hand humans appear to l i k e using i t , and, on the other, that there is circumstantial evidence t h a t s i g n i f i c a n t l y l a r g e r classes may not be learnable in polynomial time. An NP-completeness r e s u l t corroborating the l a t t e r is also p r e  sented.
1. Introduction
The a b i l i t y of humans to l e a r n new concepts is remarkable and mysterious. L i t t l e progress has been made in the problem of s i m u l a t i n g t h i s process b y computer. Worse s t i l l there i s l i t t l e agreement on what r e a l i s t i c specifications of the desired behavior of such a s i m u l a t i o n might look l i k e . The approach taken to t h i s problem in a previous paper [13] was to appeal to computational complexity theory to provide some g u i d e l i n e s on t h i s l a s t question. In particular, various classes of prop o s i t i o n a l concepts were explored that could be learned from examples, or sometimes o r a c l e s , in a polynomial number of computational s t e p s . Pre sumably b i o l o g i c a l systems cannot learn classes that are computationally intractable.
The o v e r a l l model was h i e r a r c h i c a l . A concept to be learned was expressed as a p r o p o s i t i o n a l expression in terms of concepts already known. If t h i s expression was too complex it was computation a l l y i n f e a s i b l e t o l e a r n i t . The maximal complex i t y of such expressions that could be learned f e a s i b l y was s t u d i e d .
That i n v e s t i g a t i o n p o i n t e d to one p a r t i c u l a r concept class that especially warranted further investigation--those that could be expressed as a
Supported in part by National Science Foundation grant MCS-83-02385.

disjunction of conjunctions, called disjunctive normal form (DNF). The a t t r a c t i o n of t h i s class is t h a t humans appear t o l i k e i t f o r representing knowledge as is evidenced, for example, by the success of the production system paradigm [6] and of Horn clause l o g i c s [ 8 ] . Our i n v e s t i g a t i o n s sug gest t h a t s i g n i f i c a n t l y l a r g e r classes may not be learnable in polynomial t i m e . Hence t h i s class may t u r n out to have a c e n t r a l r o l e .
This paper is organized as f o l l o w s . In Sec t i o n 2 the formal framework w i l l be described f i r s t . I t w i l l then b e shown t h a t s i g n i f i c a n t sub classes of p r o p o s i t i o n a l DNF expressions can be learned in polynomial time just from examples. Whether the whole class can be so learned remains an open problem.
The s e c t i o n f o l l o w i n g t h a t w i l l discuss how the above r e s u l t s can be extended to allow v a r i  ables and r e l a t i o n s in the manner of p r e d i c a t e c a l c u l u s . The extension is to a very r e s t r i c t e d part of the predicate calculus where there is only e x i s t e n t i a l q u a n t i f i c a t i o n and a l i m i t e d number of variables.
In Section 4 the question as to whether the above a l g o r i t h m can be made robust to e r r o r s in the data i s discussed. I t i s shown t h a t the l e a r n i n g algorithms can indeed be made robust to a small rate of even maliciously constructed e r r o r s .
The s u i t a b i l i t y of our l e a r n i n g algorithms to implementations on d i s t r i b u t e d " b r a i n - l i k e " models of computation is the subject of Section 5. In the propositional case, under certain natural r e s t r i c  t i o n s , such implementations are immediate, but t h i s does not appear to extend to the case a l l o w i n g r e  lations.
Finally in Section 6 we observe that the l i m i t s o f l e a r n a b i l i t y are not f a r o f f . Even i f i t i s known t h a t a s i n g l e p u r e l y c o n j u n c t i v e expression explains f i f t y percent of the occurrences of a con cept, discovering or approximating this conjunction may be NP-hard.
The word l e a r n i n g has been used in numerous senses in philosophy, psychology and A I . A useful d i s t i n c t i o n to make in t h i s context is whether the available data from which the learning or induction is to be made is c o n s i s t e n t w i t h many v a r i e d hypoth esis, or essentially with just one. In the former s i t u a t i o n , which is often t a c i t l y accepted in the philosophy of science, and also in machine learning

(e.g. [ 5 ] ) , the learning problem is i l l - d e f i n e d because of lack of i n f o r m a t i o n , and the best one can hope f o r is to s e l e c t , w i t h good t a s t e , from among the several p l a u s i b l e hypotheses0 In con t r a s t , in the latter situation, sufficient informat i o n is a v a i l a b l e in the data and the only problem is to deduce the hypothesis from it computation a l l y . Our c u r r e n t work is directed at the l a t t e r s i t u a t i o n and is motivated by the observation that d i s t i n c t human i n d i v i d u a l s appear to converge remarkably to e s s e n t i a l l y the same notions about familiar concepts.
The t e c h n i c a l idea t h a t makes our p o s i t i v e r e s u l t s possible is the p r o b a b i l i s t i c method of s p e c i f i c a t i o n . We assume t h a t p o s i t i v e and negative examples of concepts are drawn from c e r t a i n prob a b i l i t y distributions that are determined by the r e a l w o r l d , but arc otherwise a r b i t r a r y and possibly unknown. The task of the learning procedure is to discover a recognition algorithm for the concept t h a t gives the c o r r e c t answer w i t h high p r o b a b i l i t y on f u r t h e r examples drawn from the same d i s t r i b u  tion.
The main assumption made by our methodology as outlined in [13] is that useful classes of learnable concepts can have simple mathematical c h a r a c t e r i z a  t i o n s . Although we believe t h a t DNF expressions c o n s t i t u t e a good and relevant knowledge representa t i o n , we don't wish to make this an equally central assumption. A l t e r n a t i v e classes may be also worth exploring, such as those considered in [13],
As a philosophical note we observe that our p r o b a b i l i s t i c s p e c i f i c a t i o n of learning has p a r t i c  u l a r effectiveness in certain extreme e m p i r i c i s t or p r o c e d u r i s t worlds. Suppose t h a t the world is too complex to model but i n t e l l i g e n t systems manage to cope using simple s t r a t e g i e s ( c . f . [ 1 1 ] ) . Then s p e c i f y i n g how these s t r a t e g i e s achieve t h e i r goals may be too complex to describe and consequently programming or discovering these s t r a t e g i e s may be very d i f f i c u l t . In our model the simple s t r a t e g i e s correspond to the r e c o g n i t i o n a l g o r i t h m s . The com plex world is captured by the d i s t r i b u t i o n , which need never be described. The important p o i n t is that the efficiency of our learning procedures is limited only by the complexity of the algorithm to be learned and not by the complexity of the d i s t r i  b u t i o n , which may indeed by a r b i t r a r i l y complex w i t h no i l l e f f e c t . We conclude that in such worlds l e a r n i n g may have an unexpectedly c e n t r a l r o l e t h a t cannot be replaced by programming (c.fo Simon [ 1 2 ] ) .

L. Valiant 561

562 L. Valiant

from c a l l s of EXAMPLES+ or EXAMPLES" f o r the given concept F. This is feasible if appropriate conditions hold for the d i s t r i b u t i o n s , such as the f o l lowing :
D e f i n i t i o n . D i s t r i b u t i o n s {D+,D-} have polynomially generable monomial sets (p.g.m.s.) if there is an a l g o r i t h m t h a t can c a l l EXAMPLES+ a n d EXAMPLES-, r u n s in time polynomial in t and h, and generates a set
of monomials t h a t w i t h p r o b a b i l i t y at least
has the f o l l o w i n g p r o p e r t y :

where summation is over every v such t h a t

f o r any

[N.B.

I n a s elsewhere,

v and m are i d e n t i f i e d w i t h Boolean functions

in t h e o b v i o u s way. Thus v ->m means t h a t t h e

t r u t h v a l u e s d e f i n e d in v make m t r u e , ]

The m o t i v a t i o n f o r t h i s l a s t d e f i n i t i o n i s t o justify certain strategies for finding an M1 that would otherwise appear to be u n j u s t i f i e d h e u r i s t i c s .

Example 2 . 2 . Suppose t h a t f o r some s m a l l p a r a m e t e r

we c a l l e d EXAMPLES+ a c e r t a i n number of t i m e s

and put i n t o M1 every monomial t h a t appears as a

submonomial of the p o s i t i v e examples w i t h frequency

at least

F o r some d i s t r i b u t i o n s t h i s s e t w o u l d

be exponentially large. For certain others, which

one c o u l d d e f i n e , t h e set would be o n l y p o l y n o m i a l l y

large.

Example 2 . 3 . A good p r a c t i c a l p o s s i b i l i t y might be

t o l e t M 1 b e t h e i n t e r s e c t i o n o f some M ( k ) o f

Example 2.1 w i t h the set generated in Example 2 . 2 .

For example if we choose k =3 and

then the

monomial set is simply a l l conjuncts of sets of up to

three predicates that are ever s a t i s f i e d simulta-

n e o u s l y i n some p o s i t i v e e x a m p l e .

The q u a n t i t a t i v e aspects o f our l e a r n i n g s t r a t egies are captured by the f u n c t i o n L ( h , S ) : Suppose we have an u r n c o n t a i n i n g a l a r g e number ( p o s s i b l y i n f i n i t e ) of marbles of at most S kinds. We want to sample a number x of marbles ( w i t h replacement) so t h a t w i t h p r o b a b i l i t y at l e a s t 1 - h - 1 the sample is such that if a further marble is picked at random t h e n t h e p r o b a b i l i t y t h a t t h i s new marble i s not already represented in the previous sample is l e s s than h - 1 . I n [13] i t i s shown t h a t t h e r e i s a function L(h,S) <2h(S + loge h) that is an upper bound on t h i s x f o r a l l possible r e l a t i v e f r e quencies of the S kinds of marbles.

To v e r i f y (a) we note t h a t in case ( i ) the

monomials of F c o n s i s t o n l y of members of M1 and

w i l l always be contained in the monomial set of g.

Hence

In case ( i i )

some monomials of F w i l l be m i s s i n g f r o m g b e -

cause they were already missing from M1. But by

the definition of p.g.m.s. the probability

summed o v e r a l l v s u c h t h a t

f o r any

is bounded by h-1 w i t h p r o b a b i l i t y at least

Hence

summed o v e r v such t h a t

g ( v ) = 0 i s bounded b y t h e same q u a n t i t y .

To show (b) suppose to t h e c o n t r a r y t h a t t h e

sum

is at least

Suppose we consider

each

that is not a monomial of F to be a

marble and t h a t a c a l l o f v<-EXAMPLES" i s i n t e r -

p r e t e d as t a k i n g a sample of at l e a s t one marble,

( i . e . the submonomials of v cannot be monomials

o f F ) . Then a f t e r s u c h c a l l s w i t h

probability at least

the set of monomials

in M1 t h a t are not in F and have not been chosen

have t o t a l p r o b a b i l i t y of occurrence

Our b a s i c r e s u l t s a b o u t l e a r n i n g DNF e x p r e s -

sions from examples alone w i l l be expressed w i t h

respect to our previous definitions of a concept

having d i s t r i b u t i o n s D+, D- where

w i t h summation o v e r some

It is

e s s e n t i a l l y the dual of Theorem A in ] 1 3 ] .

Algorithms in the style of algorithm E are natural in several contexts in inductive inference [ 2 ] . I n t h e c u r r e n t c o n t e x t i t i s made n o t e w o r t h y b y i t s provably good behavior f o r a l l d i s t r i b u t i o n s .

Theorem 2 . 1 . There is an a l g o r i t h m E c a l l i n g

EXAMPLES+ and EXAMPLES" s u c h t h a t ( i ) w h e t h e r

f o r some M 1 t h a t i s p . g . d . o r ( i i ) w h e t h e r

{ D + , D ~ } has a p . g . m . s . , E w i l l g e n e r a t e a DNF

formula g in time polynomial in h and t t h a t

w i l l have p r o b a b i l i t y a t l e a s t

of having

3. DNF Expressions w i t h Relations
In AI applications there is often need f o r r e f e r r i n g t o several d i s t i n c t objects and t o expressing r e l a t i o n s h i p s among them. For sheer

L Valiant 563

expressive power the f u l l predicate calculus is very useful, of course, but it clearly includes much power, such as arbitrary alternations of quantifiers, that are l i t t l e used. It is an interesting question to delineate the minimal part of the predicate c a l culus that is s t i l l expressive enough for convent i o n a l AI applications. Only a few attempts have been made in this direction [ 4 ] .
Here we are concerned with the question of how much the class of expressions that is learnable can be extended to allow relations without s a c r i f i c i n g polynomial time l e a r n a b i l i t y . We shall now allow object variables x 1 , . , . , x r . Instead of propositional predicates we w i l l allow relation predicates
P1,...,Pt each of which has a number, i t s a r i t y , that defines the number of arguments it has. A monomial is now an existentially quantified conjunction of relational predicates, such as:

For notational brevity the existential quantificat i o n w i l l be suppressed.

This notation is sufficient to express a predicate on a scene that depends on the existence of certain subobjects in the scene that obey certain relations among themselves.

For a fixed r and a fixed set of predicates p 1 , . . . , p t we can define the class MQ of a l l monomials that can be formed from then in the obvious way. We regard two monomials are i d e n t i c a l i f f they can be obtained from each other by permuting the variable names.

Definition. A set

is polynomial generable

deterministically (p.g.d.) i f f there is a determin-

i s t i c algorithm that in time polynomial in t and

r terminates and generates a l l the members of M1.

Example 3 . 1 . For any fixed numbers r and k the set of monomials consisting at most k predicates, negated or not, is p.g.d. In fact there are no more than (2r+1t)k of them0

In treating the p r o b a b i l i s t i c generation of monomials from examples a problem would arise if we had to relate the variables {xi} to the primitive input vectors { v } . In the case under consideration, however, t h i s whole problem can be completely finessed. Let D+, D- be probability distributions on the positive and negative examples of the p r i m i t i v e input vectors { v } . We extend D+, D- to the domain of monomials Mo as follows: D+(m) is the probability that the conjunctive r e l a t i o n m holds for a random output v of EXAMPLES+. Similarly for D-.

D e f i n i t i o n . Distributions {D+,D-} have polynomially

generably monomial sets (p.g.m.s.) if there is an

algorithm calling EXAMPLES+ and EXAMPLES" that runs

in time polynomial in h, t and r and generates

a set

of monomials that with probability

at least- "

has the following property:

where summation is over every v such that

for any mM-^ [ i . e . , as in the propositional case

the monomials of F missing from M1 account for

a small fraction of the probability space { v } .

Now, however,

has the complex meaning that

the monomial expression m is true of primitive

input v . ] .

Examples 3.2 and 3.3. The analogues of Examples 2.2 and 2.3 can be defined in the obvious way.

Theorem 3 . 1 . The statement and proof are i d e n t i c a l

to those of Theorem 2.1 except that the monomial

set M1. has the more general interpretation allow-

ing relations.

o

In summary, the learning algorithm we are

suggesting, if we use Example 3.3 with

as

the monomial set, is the following: List a l l mono-

mials with up to r object variables and k predi-

cates, that hold in at least one observed positive

example. Eliminate from t h i s l i s t a l l monomials

that occur in some observed negative example. Take

the disjunction of the remaining monomials as the

desired expression.

4. Robustness

How much tolerance to erroneous data should we require of a learning system? On the one hand experience suggests that learning is d i f f i c u l t enough from error-free data and becomes well nigh impossible if the data is seriously flawed. This suggests the p o s s i b i l i t y that the learning phenomenon is only feasible with very low error rates. On the other hand some robustness is clearly essent i a l and it is reasonable to investigate the maximum error rate that can be compensated f o r .

In t h i s section we w i l l modify Algorithm E and show that the modified form Algorithm E*, is r e s i s tant to a quantifiable, if low, error rate, whether in the propositional or extended relational case of Section 3.

D e f i n i t i o n . If A is an oracle then we say that

oracle B is "oracle A with error rate

if at

each c a l l of B (i) with probability

it

calls oracle A correctly, and ( i i ) with proba-

bility

it produces an arbitrary answer possibly

chosen maliciously by an adversary.

The modified algorithm that has access only to

an oracle "EXAMPLES" with error rate

will

now perform

cycles and w i l l depend on a

parameter

, where

and T are related

below. 2 is an integer array indexed by the mem-

bers of M1 and i n i t i a l i z e d to zero. Algorithm E*

is the following:

564 L. Valiant

end

The f o l l o w i n g analogue to Theorems 2 . 1 and 3.1 can be proved. We shall r e s t r i c t ourselves to the d e t e r m i n i s t i c case. The other case, when M1 has p . g . m . s . , can be t r e a t e d s i m i l a r l y by d e t e c t i n g any m a l i c i o u s l y generated monomials having too many submonomials by t h e i r low frequency of occurrence. Thus the r e s u l t r e f e r s to an a r b i t r a r y f u n c t i o n F t h a t can be expressed as the sum of monomials from M1.

Theorem 4 . 1 . There is an a l g o r i t h m E* c a l l i n g

EXAMPLES" w i t h e r r o r r a t e

that will

generate a DNF formula g in time polynomial in h

and t such t h a t w i t h p r o b a b i l i t y at least

g has the f o l l o w i n g p r o p e r t i e s

Proof. We w i l l show t h a t the described a l g o r i t h m E*

suffices if

and

(a) A monomial m of F can be missing from

g only because it has been removed from M1 by

having occurred with frequency greater than E in

the negative examples. This can only happen if the

oracle behaved erroneously at least ET times in

T t r i a l s where the p r o b a b i l i t y of each such occur-

rence is at most

By the binomial distribu-

t i o n (e.g. [3]) the probability of t h i s happening

is less than

under the assumed bounds on T and

Hence the p r o b a b i l i t y of t h i s happening to any m

in F is at most h-1.

(b) A monomial m such t h a t

for

some v such t h a t m =>v can be included in g if

there have been fewer than £T negative examples

c o n t a i n i n g m. Now if some m M1 is i m p l i e d w i t h

frequency at least

by D- then the probability

of having found at most

negative examples in T

t r i a l s each w i t h p r o b a b i l i t y of success at least

[ i . e . , allowing f o r the 6 e r r o r r a t e ] can

be bounded above by

Hence the p r o b a b i l -

i t y t h a t at l e a s t one such monomial w i t h frequency

at l e a s t 2e has been wrongly included in g is at

most

under the assumed bounds.

Hence if only those monomials remain t h a t have f r e -

quency less than

then their t o t a l contribution

to is at most

D

5. Naive Neural Modelling
The methodology behind t h i s study is t h a t of determining the largest classes of "concepts", that can be learned in a f e a s i b l e t o t a l number of comput a t i o n steps. Since learning is exhibited by biol o g i c a l systems it is an i n t e r e s t i n g afterthought

to determine whether the algorithms discovered happen to be w e l l suited to implementation on a " b r a i n l i k e " model of computation, such as considered by Feldman [6] or Ackley et al. [1], f o r example.

In t h i s section we observe that a p a r t i a l implementation of our algorithm on networks of linear threshold devices can be found that is noteworthy f o r i t s s i m p l i c i t y . I n the p r o p o s i t i o n a l case i t is able to l e a r n DNF expressions if the d i s t r i b u t i o n s are such that the monomial sets are generable in a c e r t a i n sense on these networks. While t h i s class is s t i l l p l a u s i b l e and accommodates robustness exactly as in Section 4, it does r e q u i r e more r e s t r i c t e d d i s t r i b u t i o n s than before and does not allow for the relations of Section 3.

The e s s e n t i a l idea is t h a t if the i n p u t nodes of the network are exposed to the vectors c o n s t i t u t i n g the examples, then the monomials in the monomial set M1 w i l l be learned in unsupervised fashion at various d i s t i n c t nodes in the network. The f i n a l d i s j u n c t i o n over these is learned in supervised mode at a s p e c i f i e d node. To accomplish the unsupervised stage symmetry in the network is broken by assuming randomness in the network ( e i t h e r of the connections or of the weights).

We consider the following d i s t r i b u t e d model of

computation. An instance of a net is described by

a triple

where V is a set of n nodes

{1,2,...,n),

is a set of d i r e c t e d edges and

A:E --> [0,1] l a b e l s each edge ( i , j ) w i t h a numeri-

c a l weight a i j . Each node i has a s t a t e s.

t h a t at any time has value 0 or 1.

A set of nodes, say { l , . . . , t } is regarded as c o n s t i t u t i n g the i n p u t . When an example is presented to the net, for learning or recognition, the states s 1 , . . . , s t are set to the Boolean values corresponding to the truth values of the propositional predicates p , . . . , p ,

A node j " f i r e s " ( i . e . , sj =1) in a time interval if in the previous interval

A node continues to f i r e as long as the above condition holds.
The l e a r n i n g mechanism allowed is the f o l l o w i n g : When a new example is presented we w a i t f o r the s t a t e s to s t a b i l i z e and assume t h a t they do. A weight aij can be changed only if node i or node j is f i r i n g . I t s new value depends only on the current values of si Sj and on the values of aij and aji before the presentation of the current example. The new value of a i j becomes o p e r a t i v e only when a f u r t h e r example has been presented.
We consider two senses in which a net learns a f u n c t i o n F ( p 1 , . . . , p t ) as a consequence of a t r a i n ing session of examples. F has been learned in unsupervised mode if the weights have been so modif i e d during the t r a i n i n g session t h a t some node j now f i r e s whenever ( i . e . almost whenever) F ( s 1 . . . , s t ) - 1 . F has been learned i n supervised mode if the same has been achieved but in the course

L. Valiant 565

of the t r a i n i n g session a d i s t i n g u i s h e d node, say j was c o n t r o l l e d by an external omnipotent agent and made to f i r e if and only if F ( S 1 , . . . , s t ) = 1 was t r u e of the t r a i n i n g example (thus o v e r r i d i n g p o s s i bly the threshold condition above).
For implementing counters on the weights we use an a r b i t r a r y continuous s t r i c t l y monotone f u n c t i o n

F i r s t , we n o t i c e t h a t if F is a simple mono-

tone disjunction (e.g. , p1 vp3 vp7) or a simple

monotone conjunction then it can be learned in super-

vised mode at any node j to which a l l the r e l e v a n t

i n p u t s are connected. The learning r u l e f o r d i s -

j u n c t i o n s involves changing those a . . where

(i,j)

provided F=0 as follows:ij

In both cases

is the inverse of the

o f Theorem 4 . 1 . I f each a i j i s i n i t i a l i z e d t o

1/2, say, then each

w i l l tend to 0 or 1 as

desired according to wherner i t s frequency in the

corresponding examples is less than or greater than

To recognize disjunctions of conjunctions we w i l l show how c e r t a i n monomial sets M1 can be generated in unsupervised mode from examples. A d i s j u n c t i o n over any subset of M1 can then be learned by the above method at any node t h a t is connected to a l l the nodes computing M1 .

The basic idea is i l l u s t r a t e d by the simple

case of wanting to learn the monomial P1P2 . . . Pk

from a d i s t r i b u t i o n D* of examples when the nodes

l , 2 , . . . , k , . . . , d , are a l l connected to some node

j >d,, [Note that here D* w i l l be a d i s t r i b u t i o n

t h a t t y p i c a l l y includes both p o s i t i v e and negative

examples of F and no-one informs node j as to

which is the case.] To accomplish this we define

a as above but scaled to the range [ 0 , ( k - 1 / 2 ) 1 ] ,

and choose it so that

is always suitably

small compared w i t h k - 1 . I f each a i k i s i n i t i a l -

ized to

then the following learning

algorithm, for

w i l l achieve the task

provided the d i s t r i b u t i o n D* has favorable propert i e s such as the following:

The idea is t h a t f o r the m a j o r i t y of i n p u t s

f o r These w i l l cause s- to f i r e and hence

increase the values of

to tend towards

For the remaining values of

f o r the m a j o r i t y of inputs and hence alj w i l l

tend to zero as desired.

To make a r b i t r a r y monomials of length k

( i . e . , M(K) ) learnable in unsupervised mode, one

approach is to make the network random so t h a t f o r

each set of k out of the n nodes, there w i l l be

at least one, and at most a few, nodes to which a l l

k are connected. Connecting each node to a random

set of about

others achieves t h i s . Then

any monomial

can be l e a r n t at some node,

and, equally importantly, w i l l be learnt at only a

few, thus allowing the other nodes to l e a r n d i f f e r -

ent monomials. In t h i s way about n d i s t i n c t mono-

mials from M(K) can be l e a r n t . To make a l l t h i s

possible D* has to be so defined t h a t at each node

at most one monomial from M(K) has o v e r r i d i n g

influence in the sense of the previous paragraph.

To make the above concrete consider the case

k = 2 and suppose t h a t we want P1P2 to be l e a r n -

a b l e . Then D* has to be such t h a t if we p i c k a

set of

P i ' s to include P1/P2 but to be o t h e r -

wise random, then with high l i k e l i h o o d the following

holds: If v D* is chosen randomly to have at

least two of the chosen pi t r u e , then w i t h prob-

a b i l i t y greater than a half these w i l l include p1

and P2*

To summarize we have shown t h a t a r e s t r i c t e d version of Algorithm E of Section 2 can be implemented on a network of threshold elements. It w i l l b u i l d up recognizers for a set M* of monomials t h a t occur in examples provided the members of M* are s u i t a b l y separated by D* in the manner i n d i cated above. The network can then l e a r n a r b i t r a r y d i s j u n c t i o n s over M* in supervised mode.

6 . S i m p l e R u l e s o f Thumb May b e H a r d t o L e a r n
There is evidence that c e r t a i n s i g n i f i c a n t extensions to the concept classes here or in [13] r e n d e r them c o m p u t a t i o n a l l y i n t r a c t a b l e . One s o u r c e of evidence is cryptography as described in the l a t t e r paper. Here we s h a l l d e s c r i b e an NP-completeness r e s u l t t h a t shows t h a t e x t e n s i o n s in a c e r t a i n other direction are probably computationally intractable also.
In the learnable classes considered a simple f o r m u l a was assumed to a c c o u n t f o r a hundred p e r c e n t of a l l p o s i t i v e examples of a concept. Suppose t h a t the s i m p l e f o r m u l a i s now a s i n g l e monomial t h a t accounts f o r j u s t f i f t y percent, and the r e s t i s accounted f o r by a f u n c t i o n whose complexity is u n s p e c i f i e d . W e s h a l l show t h a t i n t h i s s i t u a t i o n it is NP-hard to discover either t h i s monomial, or

566 L. Valiant

one t h a t approximates it in the sense of accounting for of the distribution for arbitrarily large h.
Theorem 6 . 1 , Given a f u n c t i o n F expressed as a sum of monomials and d i s t r i b u t i o n s D+ and D- of p o s i t i v e and negative examples, it is NP-hard to determine whether a monomial m of the following k i n d e x i s t s , o r t o construct one i f i t does:
[The d e s c r i p t i o n s of D and D do not need to be counted in the input s i z e . ]
Proof, We reduce the BALANCED COMPLETE BIPARTITE SUBGRAPH problem proved NP-complete by Garey and Johnson ( [ 7 ] , p. 196) to t h i s problem. In the former problem we are given a b i p a r t i t e graph
and we have to determine whether there are subsets

Then the d i s t r i b u t i o n ensures t h a t any m s a t i s f y i n g the theorem must have at l e a s t t / 2 predicates defined as t r u e , and, a l s o , must imply at least t / 2 of the m i ' s . But t h i s is exactly the condition that the graph G has a (t/2) x (t/2) complete b i p a r t i t e subgraph.

The above shows t h a t determining whether the

desired m exists or not is NP-hard. Constructing

it on the assumption t h a t t h i s good r u l e of thumb

exists is therefore also NP-hard since the decision

procedure reduces to t h i s .

o

Also if no such m exists then the value of

summed over a l l

is at most

and the constant 1/2 cannot be

approached to w i t h i n h-1 uniformly in a number of

steps polynomial in h and t (unless NP-complete-

ness does not imply i n t r a c t a b i l i t y ) .

F i n a l l y note t h a t i t i s not known whether the

size of the largest balanced complete b i p a r t i t e

subgraph can be approximated to a r b i t r a r i l y good

multiplicative constant factors in polynomial time.

Hence i t i s p o s s i b l e t h a t the above r e d u c t i o n estab-

lishes a stronger r e s u l t than claimed, namely t h a t

f i n d i n g monomials accounting f o r

of the

distribution for sufficiently small constant e is

also d i f f i c u l t .

REFERENCES

[1] D.H. Ackley, G.E. Hinton, and T . J . Sejnowski. "A Learning Algorithm f o r Bolzmann Machines'.', Cognitive Science 9 ; 1 (1985) 147-169.

[2] D. Angluin and C.H. Smith. "Inductive Inference: Theory and Methods", Computing Surveys 15:3 (1983) 237-269.

[3] D. Angluin and L.G. V a l i a n t . " P r o b a b i l i s t i c Algorithms f o r Hamiltonian C i r c u i t s and M a t c h i n g s " , J . Comp. S y s t . S c i . 18:2 (1979) 155-193.

[4] R.J. Brachman and H.J. Levesque. "The T r a c t a b i l i t y of Subsumption in Frame-based Description Languages", Proc. AAAI-84, A u s t i n , Texas (1984) 34-37.

[5] T.G. D i e t t e r i c h and R.S. M i c h a l s k i . "A Comp a r a t i v e Review of Selected Methods f o r Learning from Examples'.', in [ 9 ] , pp. 4 1 - 8 1 .

[6] J.A. Feldman and D.H. B a l l a r d . "Connectionist Models and Their P r o p e r t i e s " , Cognitive Science 6 (1982) 205-254.

[7] M.R. Garey and D.S. Johnson. Computers and I n t r a c t a b i l i t y , Freeman P r e s s , San F r a n c i s c o (1979).

[8] R. Kowalski. Logic for Problem Solving. N o r t h H o l l a n d , New Y o r k ( 1 9 7 9 ) .

[9] R.S. M i c h a l s k i , J.G. Carbonell, and T.M. M i t c h e l l . Machine Learning. Tioga Press, Palo A l t o , CA (1983).

[10]

A. N e w e l l and H.A. S i m o n . Human P r o b l e m Solving. Prentice H a l l , Englewood C l i f f s , NJ (1972).

[11]

W.E. Reichardt and T. Poggio. "Visual Control of F l i g h t in F l i e s " , In Recent Theoretical Developments in Neurobiology. W.E. Reichardt, V.B. Mountcastle and T. Poggio (eds.) (1979).

[12] H.A. Simon. "Why S h o u l d Machines L e a r n " , In 19], pp. 25-37.

[13] L.G. V a l i a n t . "A Theory of the Learnable", CACM 2 7 : 1 1 (1984) 1 1 3 4 - 1 1 4 2 .

