EUROGRAPHICS 2009/ M. Pauly and G. Greiner

STAR – State of The Art Report

Time-of-Flight Sensors in Computer Graphics

Andreas Kolb1, Erhardt Barth2, Reinhard Koch3, Rasmus Larsen4

1 Computer Graphics Group, Center for Sensor Systems (ZESS), University of Siegen, Germany

2 Institute for Neuro- and Bioinformatics, University of Luebeck, Germany

3 Institute of Computer Science, Christian-Albrechts-University Kiel, Germany

4 Department of Informatics and Mathematical Models, Techn. University of Denmark, Copenhagen, Denmark

Abstract
A growing number of applications depend on accurate and fast 3D scene analysis. Examples are model and
lightﬁeld acquisition, collision prevention, mixed reality, and gesture recognition. The estimation of a range map
by image analysis or laser scan techniques is still a time-consuming and expensive part of such systems.
A lower-priced, fast and robust alternative for distance measurements are Time-of-Flight (ToF) cameras. Re-
cently, signiﬁcant improvements have been made in order to achieve low-cost and compact ToF-devices, that have
the potential to revolutionize many ﬁelds of research, including Computer Graphics, Computer Vision and Man
Machine Interaction (MMI).
These technologies are starting to have an impact on research and commercial applications. The upcoming gen-
eration of ToF sensors, however, will be even more powerful and will have the potential to become “ubiquitous
real-time geometry devices” for gaming, web-conferencing, and numerous other applications. This STAR gives
an account of recent developments in ToF-technology and discusses the current state of the integration of this
technology into various graphics-related applications.

Categories and Subject Descriptors (according to ACM CCS):
Graphics and Realism I.3.8 [Computer Graphics]: Applications

I.3.7 [Computer Graphics]: Three-Dimensional

1. Introduction
Acquiring 3D geometric information from real environments
is an essential task for many applications in computer graph-
ics. Prominent examples such as cultural heritage, virtual
and augmented environments and man machine interaction,
e.g. for gaming, clearly beneﬁt from simple and accurate de-
vices for real-time range image acquisition. However, even
for static scenes there is no low-priced off-the-shelf sys-
tem available, which provides full-range, high resolution dis-
tance information in real-time. Laser scanning techniques,
which merely sample a scene row by row with a single laser
device, are rather time-consuming and therefore impractica-
ble for dynamic scenes. Stereo vision camera systems suf-
fer from the inability to match correspondences in homoge-
neous object regions.

Being a recent development in imaging hardware, the
Time-of-Flight (ToF) technology opens new possibilities.
Unlike other 3D systems, the ToF-sensor is a very compact
device which already fulﬁlls most of the above stated fea-

c(cid:13) The Eurographics Association 2009.

tures desired for real-time distance acquisition. There are
two main approaches currently employed in ToF technol-
ogy. The ﬁrst one utilizes modulated, incoherent light, and
is based on a phase measurement that can be implemented
in standard CMOS or CCD technology [XSH∗98,OBL∗05].
The second approach is based on an optical shutter technol-
ogy, which has been ﬁrst used for studio cameras [IY01],
and has later been developed into miniaturized cameras such
as the new Zcam [YIM07].

Within the last three years, the number of research activ-
ities in the context of ToF-sensors increased dramatically.
Taking a look at the publications cited in this STAR we ﬁnd
an increase from the year 2006 to 2008 by factor of 8 to 9.
While the initial research focused on more basic questions
like sensor characteristics and the application of ToF-sensors
for the acquisition of static scenes, recently other applica-
tion areas came into focus, e.g. man-machine interaction and
surveillance.

This STAR aims at making the computer graphics com-

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

Figure 1: Left: PMDTec/ifm electronics CamCube sensor; Middle: MESA SR4000 sensor; Right: The ToF phase-measurement
principle.

munity aware of a rapidly developing and promising sensor
technology and it gives an overview of its ﬁrst applications
to scene reconstruction, mixed reality/3D TV, user interac-
tion and light-ﬁelds. Moreover, recent results and ongoing
research activities are presented to illustrate this dynamically
growing ﬁeld of research.

The STAR ﬁrst gives an overview on basic technolog-
ical foundations of the ToF measurement principles (Sec-
tion 2) and presents current research activities (Section 3).
Sections 4 and 5 discuss sensor calibration issues and ba-
sic concepts in terms of image processing and sensor fusion.
Section 6 focuses on applications for geometric reconstruc-
tion, Section 7 on dynamic 3D-keying, Section 8 on interac-
tion based on ToF-cameras, and Section 9 on interactive light
ﬁeld acquisition. Finally, we draw a conclusion and give a
perspective on future work in the ﬁeld of ToF-sensor based
research and applications.

2. Technological Foundations
2.1. Intensity Modulation Approach

This ToF-principle is used by various manufacturers, e.g.
PMDTec/ifm electronics (www.pmdtec.com; Fig. 1, left),
MESA Imaging (www.mesa-imaging.ch; Fig. 1, middle)
and Canesta (www.canesta.com).

The the intensity modulation principle (see Fig. 1, right,
and [Lan00]) is based on the on-chip correlation (or mixing)
of the incident optical signal s, coming from a modulated
NIR illumination and reﬂected by the scene, with its refer-
ence signal g, possibly with an internal phase offset τ:
s(t)· g(t + τ) dt.

Z T /2

c(τ) = s⊗ g = lim
T→∞

−T /2

For a sinusoidal signal, e.g.

g(t) = cos(ωt),

s(t) = b + acos(ωt + φ)

where ω is the modulation frequency, a is the amplitude of
the incident optical signal, b is the correlation bias and φ is

the phase offset relating to the object distance, some trigono-
metric calculus yields c(τ) = a

2 cos(ωτ + φ) + b.

The demodulation of the correlation function c is done
using samples of the correlation function c obtained by four
sequential phase images with different phase offset τ: Ai =
c(i· π
φ = arctan2(A3 − A1,A0 − A2) ,

I = A0 + A1 + A2 + A3

2 ), i = 0, . . . ,3:

,

(cid:113)
(A3 − A1)2 + (A0 − A2)2

4

,

a =

2

where I is the intensity of the incident NIR light. Now, from
φ one can easily compute the object distance d = c
4πω φ,
where c ≈ 3· 108 m
s is the speed of light. Current devices ac-
quire range maps at 20 FPS, some of them support Suppres-
sion of Background Intensity (SBI), which facilitates out-
door applications. Having a sensor with SBI, the intensity
I mainly reﬂects the incident active light.

ToF-sensors use standard optics to focus the reﬂected ac-
tive light onto the chip. Thus, classical intrinsic calibration
is required to compensate effects like shifted optical centers
and lateral distortion. Furthermore, using ToF-sensors based
on the intensity modulation approach involves major sensor-
speciﬁc challenges:

Low Resolution: Current sensors have a resolution be-
tween 64 × 48 and 2042 (PMDTec’s “CamCube”, see
Fig. 1, left). This resolution is rather small in comparison
to standard RGB- or grayscale-sensors.

Systematic Distance Error: Since the theoretically re-
quired sinusoidal signal is practically not achievable, the
recalculated depth does not reﬂect the true distance but
comprises a systematic error, also called “wiggling” (see
Fig. 3, left).

Intensity-related Distance Error: Additionally, the mea-
sured distance (in the raw data) is inﬂuenced by the total
amount of incident light. This fact results from different
physical effects in the ToF-camera, both the semiconduc-
tor detector and the camera electronics. However, this is

c(cid:13) The Eurographics Association 2009.

light sourceIncoherent IRModulatorDataCCD chip withcorrelation3D scenes(t)Phase shiftg(t+τ)τA. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

not a generic ToF problem and some manufacturers seem
to have found solutions to this problem.

Depth Inhomogeneity: The mixing process in a pixel that
observes a region with inhomogeneous depth results in
superimposed signals and leads to wrong distance val-
ues (“ﬂying pixels”), e.g. at object boundaries (see Fig. 3,
right).

Motion Artifacts: The four phase images Ai are acquired
subsequently, thus sensor or object motion leads to er-
roneous distance values at object boundaries (see Fig. 3,
right).

Multiple Reﬂections: Especially when used in enclosures
and cluttered environments, the distance measurement can
be falsiﬁed due to superposition of direct reﬂected active
light with active light that travels one or more indirect
paths.

General Aspects of Active Systems: The active illumina-
tion ToF-sensors comprise error sources, that are common
to many other active sensing systems. Firstly, using sev-
eral sensors in parallel leads to interference problems, i.e.
the active illumination of one camera inﬂuences the re-
sult of all other cameras. Secondly, object areas with ex-
tremely low reﬂectivity lead to a low signal. Highly spec-
ular objects or reﬂections in the surroundings lead to er-
roneous distance measurement due to indirect ray paths.
And ﬁnally, the reﬂected-light energy decreases with the
square of the distance.

From a theoretical perspective, the systematic distance er-
ror can be removed if the correlation function c(τ) is repre-
sented including higher Fourier modes [Lan00, Rap07], i.e.

c(τ) =

ck cos(k(ωτ + φ) + θk).

l

∑
k=0

A least square optimization over N ≥ 2l + 1 samples of the
correlation function, i.e. phase images Ai = c(i· 2π
N ), leads to
following phase demodulation scheme:
Aie−2πik i

(cid:32)N−1

φ = arg

(cid:33)

.

N

∑
i=0

In practice, extending the demodulation scheme for higher
frequencies is impracticable as the number of required phase
images as well as the calculation effort for the demodulation
increases dramatically. Furthermore, the higher number of
samples would result in an increase of motion artifacts.

2.2. Optical Shutter Approach

This alternative ToF-principle is based on the indirect mea-
surement of the time of ﬂight using a fast shutter tech-
nique [IY01, YIM07]. The basic concept uses a short NIR
light pulse, which represents a depth range (“light wall”, see
Fig. 2, top-left). The optical signal is reﬂected by the scene
objects leading to a “distorted” light wall, resembling the
objects’ shapes. A shutter in front of a standard CCD sensor

c(cid:13) The Eurographics Association 2009.

Figure 2: The Shutter Principle: A “light wall” is emitted
from the sensor (top-left) and reﬂected by the object (top-
right). Gating the reﬂected optical signal yields distance-
related portions of the “light wall”, which are measured in
the CCD-pixels [IY01].

opens a time gate T = [tmin,tmax], which represents the min-
imal and maximal distance of interest [dmin,dmax]. The pixel
intensity Igate is the portion of the reﬂected light arriving in
the time gate T which linearly relates to the distance of the
respective object

d = (1− α)dmin + αdmax, α = Igate/Itotal,

where Itotal is the total amount of reﬂected light wall. The
computation of α normalizes the object’s reﬂectivity as well
as the attenuation of the active light due to the object dis-
tance.

Distances below dmin and above dmax can not be measured
in a single exposure. Thus, if larger depth ranges need to
be observed, several exposures are used with varying gating
parameters [GKOY03a].

Regarding the error sources for the sensor type, almost
all challenges stated in Sec. 2.1 should be present as well.
However, currently there are very few research results pub-
lic available, that have been conducted using this sensor,
thus speciﬁc sensor characteristics may not be completely
revealed.

2.3. ToF-Sensor Simulation

In the context of the development of ToF-sensors and
their application, ToF-simulators play an important role.
Very ﬂexible, but rather inefﬁcient simulation approaches
are based on general purpose simulation tools like MAT-
LAB [PLHK07]. A real-time simulator incorporating major
sensor errors, i.e. the systematic distance error, ﬂying pixels
and motion artifacts has been implemented using the parallel
GPU programming paradigm [KKP07, KK09]. This allows
for the direct replacement of sensors in real-time processing
pipelines in order to evaluate new sensor parameters.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

3. Current Research Projects and Workshops

4. Calibration

The ﬁeld of real-time ToF-sensor based techniques is very
active and covers further areas not discussed here. Its vivid-
ness is proven by a signiﬁcant number of currently ongoing
medium and large-scale research projects, e.g.

Dynamic 3D Vision (2006-2010): A bundle of 6 projects
funded by the German Research Association (DFG).
Research foci are multi-chip 2D/3D-sensors, dynamic
scene reconstruction, object localization and recognition,
and light-ﬁeld computation (www.zess.uni-siegen.
de/pmd-home/dyn3d).

ARTTS (2007-2010): “Action Recognition and Tracking
based on Time-of-Flight Sensors” is EU-funded (www.
artts.eu). The project aims at developing (i) a new
ToF-camera that is smaller and cheaper, (ii) a combined
HDTV/ToF camera, and (iii) algorithms for tracking and
action recognition with a focus on multi-modal interfaces
and interactive systems.

Lynkeus (2006-2009): Funded by the German Ministry of
Education and Research, BMBF, (www.lynkeus-3d.de),
this project strives for higher resolution and robust ToF-
sensors for industry applications, e.g. in automation and
robot navigation. Lynkeus involves 20 industry and uni-
versity partners.

3D4YOU (2008-2010): An EU-funded project for estab-
lishing the 3D-TV production pipeline, from real-time
3D ﬁlm acquisition over data coding and transmission,
to novel 3D displays at the homes of the TV audience
(www.3d4you.eu). 3D4YOU utilizes ToF range cameras
to initialize the depth estimation from multiple high-
deﬁnition cameras to compute a 3D scene representation.
“Multi-
for Environmental Ex-
of
ToF-based
fusion

Modal
ploration
sensor-based
man-machine
sensor
(www.zess.uni-siegen.de/ipp_home/moses).

applications
interaction and multi

MOSES (2008-2012): The

various
including

(MOSES)”

covers

Sensor

Systems

research

school

aspects

Furthermore, a series of workshops have been held in the
last years and will be held in the near future, documenting
the worldwide productivity of researchers in this ﬁeld.

IEEE Int. Symp. on Signals, Circuits & Systems, 2007,

half-day session on “Algorithms for 3D time-of-ﬂight
cameras”, Iasi, Romania.

Symp. German Ass. for Patt. Recogn. (DAGM), 2007,

full-day workshop on “Dynamic 3D Imaging”, Heidel-
berg, Germany.

IEEE Comp. Vision & Patt. Recongn. (CVPR), 2008,

full-day workshop on “Time of Flight Camera based
Computer Vision (TOF-CV)”, Anchorage, USA.

Symp. German Ass. for Patt. Recogn. (DAGM), 2009,

full-day workshop on “Dynamic 3D Imaging”, Jena,
Germany.
See call-for-paper: www.multi-sensorics.de/dyn3d.

As mentioned in Sec. 2, ToF-sensors require lateral cal-
ibration. For ToF-sensors with relatively high resolution,
i.e. 160× 120 or above, standard calibration techniques can
be used [LK06]. For low-resolution sensors, an optimiza-
tion approach based on analysis-by-synthesis has been pro-
posed [BK08]. However, this technique requires a sensor
model incorporating the systematic distance error for accu-
rate image reconstruction (see below).

Considering the systematic error of ToF-sensors, the ques-
tion of the acquisition of reference data (“ground truth”)
arises. Early approaches used track lines [LK06, KRG07],
which requires rather cost
intensive equipment. In the
robotic context, the known position of the robot’s tool cen-
ter point can be used to locate the sensor in a global ref-
erence frame [FM08, FH08]. Alternative techniques use vi-
sion based approach to estimate the extrinsic parameters of
the sensor with respect to a reference plane, e.g. a checker-
board [LK07]. However, as for the lateral calibration, more
complex approaches are required in case of low resolution
ToF sensors.

Regarding the systematic distance error, ﬁrst approaches
assumed a linear deviation with respect to the objects dis-
tance [KS06]. A closer look at the error reveals a nearly pe-
riodic, sine-like function (see Fig. 3, left). This systematic
depth error can be corrected using look-up-tables [KRG07]
or correction functions like b-splines [LK06]. In [LK06] an
additional per-pixel adjustment is used to cope with individ-
ual pixel errors. A very comprehensive study of various ToF-
sensors has been carried out in [Rap07]. One major result of
this study is that the systematic error behaves quite simi-
lar for different sensor types. Differences appear in the near
range (over-saturation).

The noise level of the distance measurement depends on
the amount of incident active light. Also, an additional depth
error related to the intensity I is observed, i.e., object regions
with low NIR reﬂectivity have a non-zero mean offset com-
pared to regions with high reﬂectivity. One approach is to
model the noise in the phase images Ai under the assumption
of a linear, but varying gain for the phase images Ai [FB07].
In [LK07] the systematic and the intensity-related errors are
compensated using a bivariate correction function based on
b-splines directly on the distance values, assuming both ef-
fects to be coupled. Alternatively, instead of dealing with
the intensity value, one can also consult the sensors ampli-
tude values a [RFK08]. Assuming constant environmental
effects, homogeneous depth information per pixel and ideal
sensors, the amplitude a and the intensity I strongly corre-
late.

Regarding the intensity images delivered by ToF-sensors,
[SPH08c] presents an approach to normalize the intensity
variation related to the attenuation caused by the active de-
vice illumination.

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

Figure 3: Error sources of PMD-based ToF-sensors. Left: Systematic distance error for all pixels (grey) and ﬁtted mean devi-
ation (black). Middle: Intensity-related distance error sensing a planar object with varying reﬂectivity. Right: Motion artifacts
(red) and ﬂying pixels (green) for a horizontally moving planar object in front of a wall.

From a practical point of view, a major challenge is the
large number of reference data required. Usually, some 15-
20 distance measurements are used as ground truth for the
systematic error and some 5-10 measurements are used for
different intensities. This results in approximately 60-200
reference data sets that need to be acquired. Current research
aims at reducing this heavy burden. To relieve the user from
manually collecting this large amount of data, an automatic
multi-camera calibration scheme was devised that combines
optical camera calibration based on a planar checker-board
calibration object with automatic depth adjustment of the
ToF camera in one step [SBK08b]. Starting with checker-
board corner ﬁtting, an iterative intensity and depth ﬁtting of
all data minimizes the overall re-projection intensity error,
taking into account all internal and external camera param-
eters, including polynomial ﬁtting for radial and depth dis-
tortions. Resulting residual errors are typically well below
one pixel. A calibration toolbox can be downloaded from
www.mip.informatik.uni-kiel.de.

Regarding the systematic depth error, another approach
incorporating an alternative demodulation scheme based on
the assumption of a box signal has been introduced [LK08].
Even though the demodulation scheme produces similar er-
rors as the one based on the assumption of a sinusoidal
signal, combining both demodulation schemes reduces the
overall error when using only as few as four reference mea-
surements.

Whereas the systematic and the intensity-related error
are highly non-linear regarding to the distance and inci-
dent active light, respectively, their dependency of the expo-
sure time can be modeled as constant offset [KRI06, LK07,
Rap07].

Multiple reﬂections are a principal problem in ToF mea-
surements [GAL07]. In [FB08, Fal08] the authors describe
a model for multiple reﬂections as well as a technique for
correcting the related measurements. More speciﬁcally, the
perturbation component due to multiple reﬂections outside
and inside the camera depends on the scene and on the cam-
era construction, respectively. The spatial spectral compo-
nents consist mainly of low spatial frequencies and can be

c(cid:13) The Eurographics Association 2009.

compensated using a genuine model of the signal as being
complex with the amplitude and the distance as modulus and
argument. The model is particularly useful if an additional
light pattern can be projected on the object.

Some work has been conducted in the area of sensors in-
ternal scattering effects. First results in determining the point
spread function of ToF-sensors are at hand, based on the su-
perposition of Gaussian functions [MDH07] and on empir-
ical scatter models based on reﬂectance measurements for
point reﬂectors [KKTJ08]. Both works show, that the inten-
sity pattern caused by scattering strongly varies across the
image plane.

Regarding motion artifacts, the device manufacturers at-
tempt to reduce the latency between the individual exposures
for the four phase images, which is mainly caused by the
data readout from the chip. However, the problem remains
and might be solved by motion-compensated integration of
the individual measurements.

5. Range Image Processing and Sensor Fusion

Before using the range data from a ToF-sensor, usually some
pre-processing of the input data in required. To remove out-
liers, the ToF’s amplitude value a can be used as conﬁ-
dence measure since it represents the accuracy of the on-
pixel correlation process. Using a constant amplitude range,
e.g. [20%,80%], one can remove pixels with low accuracy or
saturated pixels. However, the amplitude value relates to the
amount of incident active light and thus decreases for distant
objects and objects at the image boundary, since the active
illumination units normally have a radial fall-off in their in-
tensity proﬁle. Thus, different methods like local distance
distributions may be used. Furthermore, most applications
need to deal with ﬂying pixels, since they represent false ge-
ometric information. This can be achieved by applying a bi-
lateral ﬁlter to the range data [HSJS08]. Alternatively, edge-
directed resampling technique can be used, combined with
an upscaling technique applied to the range image [LLK08].

ToF-sensors deliver both distance and intensity values for

-10-5 0 5 10 15 20 25 1 1.5 2 2.5 3 3.5 4 4.5error [cm]distance [m]bsplineA. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

Figure 4: Two acquired ofﬁce scenes using a 2D/3D camera
combination (seen from a third person view) [HJS08].

every pixel. Therefore the distance signal can be used to im-
prove the intensity signal and the intensity signal can be used
to correct the distance measurement [OFCB07]. In [LLK08],
depth image reﬁnement techniques are discussed to over-
come the low resolution of a PMD sensor in combina-
tion with an enhancement of object boundaries, which fol-
low approaches from boundary preservation of subdivision
surfaces. A bimodal approach using intra-patch similarity
and optional color information is presented in [HSJS08].
In [STDT08] the authors introduce a super-resolution ap-
proach to handle the low device resolution using depth maps
acquired from slightly shifted points of view. Here, the low
input depth images are obtained by unknown sensor rota-
tion and the high resolution range image is formulated as the
result of an optimization techniques. This approach is not
real-time capable.

The statistics of the natural environment are such that a
higher resolution is required for color than for depth infor-
mation. Therefore different combinations of high-resolution
video cameras and lower-resolution ToF-sensors have been
studied.

Many researchers use a binocular combination of a ToF-
sensor with one [LPL∗07,HJS08,JHS07,LKH07,SBKK07]
or with several RGB-cameras [GLA∗08b], enhancing the
low resolution ToF data with high resolution color informa-
tion. This ﬁxated sensor combinations enable the computa-
tion of the rigid 3D transformation between the optical cen-
ters of both sensors (external calibration) and intrinsic cam-
era parameters of each sensor. Utilizing this transformations
the 3D points provided by the ToF camera are co-registered
with the 2D image, thus color information can be assigned to
each 3D point. A commercial and compact binocular 2D/3D-
camera based on the optical shutter approach has been re-
leased by 3DV Systems [YIM07].

In many approaches a rather simple data fusion scheme
is implemented by mapping the ToF pixel as 3D point
onto the 2D image plane resulting in a single color value
per ToF pixel [LPL∗07, HJS08, JHS07]. A more sophis-
ticated approach presented in [LKH07] projects the por-
tion of the RGB image corresponding to a representative
3D ToF pixel geometry, e.g. a quad, using texture map-
ping techniques. Furthermore, occlusion artifacts in the near

Figure 5: Improvement of range map quality using the shad-
ing constraint. From left: Intensity image; lateral view of
raw measured surface; and surface reconstructed using the
shading constraint in lateral and frontal views.

range of the binocular camera rig are detected. [DT06] im-
prove the resolution of a low-resolution range map by fus-
ing it with a high-resolution color image. Their method ex-
ploits the fact that depth discontinuities often co-occur with
color or brightness discontinuities; a Markov Random Field
(MRF) is used to estimate the high-resolution range map.
Yang e.a. [YYDN07] combine a high-resolution color im-
age with a depth image by upscaling depth to color resolu-
tion. They apply bilateral ﬁltering and sub-pixel smoothing
on the depth data with good results.

There are also a number of monocular systems, which
combine a ToF sensor with a conventional image sensor
behind a single lens. They have the advantage of making
data fusion easier but require more sophisticated optics and
hardware. The 3DV VisZcam [IY01] is an early example
of a monocular 2D/3D-camera aimed at TV production. A
monocular 2D/3D-camera based on the PMD-sensor has
been introduced in [LHLW07]. This 2-chip sensor uses a
beam-splitter for synchronous and auto-registered acquisi-
tion of 2D- and 3D data.

Another

research direction aims at combining ToF-
cameras with classical stereo techniques. In [KS06], a PMD-
stereo combination has been introduced to exploit the com-
plementarity of both sensors. In [GAL08], it has been shown
that a ToF-stereo combination can signiﬁcantly speed up the
stereo algorithm and can help to manage texture-less re-
gions. The approach in [BBK07] fuses stereo and ToF es-
timates of very different resolutions to estimate local sur-
face patches including surface normals. A global data fu-
sion algorithm that incorporates belief propagation for depth
from stereo images and the ToF depth data is presented by
[ZWYD08]. They combine both estimates with a MRF to
obtain a fused superior depth estimate.

A recent technique [BHMB08b] for improving the accu-
racy of range maps measured by ToF-cameras is based on
the observation that the range map and intensity image are
not independent but are linked by the shading constraint: If
the reﬂectance properties of the surface are known, a cer-

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

tain range map implies a corresponding intensity image. In
practice, a general reﬂectance model (such as Lambertian
reﬂectance) provides a sufﬁcient approximation for a wide
range of surfaces. The shading constraint can be imposed
by using a probabilistic model of image formation to ﬁnd a
maximum a posteriori estimate for the true range map. The
method also allows the reﬂectivity (or albedo) of the surface
to be estimated, both globally for an entire object and locally
for objects where albedo varies across the surface. The algo-
rithm substantially improves the quality of the range maps,
both in terms of objective measures such as RMS error and
in subjectively perceived quality (see Fig. 5).

First works investigate multi-view setups. A major chal-
lenge is to realize a system that hinders the interfer-
ence of the active ToF-sensors. The approaches presented
in [KCTT08, GFP08] use different modulation frequencies,
however the authors to not discuss the constraints that the
different modulation frequencies need to fulﬁll in order to
guarantee noninterference.

Meanwhile, however, some manufacturers have already
implemented more sophisticated active illuminations that
make use of binary codes by which different sources can be
separated [MB07].

6. Geometry Extraction and Dynamic Scene Analysis
ToF-cameras are especially well suited to directly capture
3D scene geometry in static and even dynamic environ-
ments. A 3D map of the environment can be captured by
sweeping the ToF-camera and registering all scene geome-
try into a consistent reference coordinate system [HJS08].
Fig. 4 shows two sample scenes acquired with this kind of
approach. For high quality reconstruction, the low resolution
and small ﬁeld of view of a ToF-camera can be compensated
for by combining it with high-resolution image-based 3D
scene reconstruction, for example by utilizing a structure-
from-motion (SFM) approach [BKWK07, KBK07]. The in-
herent problem of SFM, that no metric scale can be ob-
tained,
is solved by the metric properties of the ToF-
measurements [SBKK07]. This allows to reconstruct met-
ric scenes with high resolution at
interactive rates, for
example for 3D map building and navigation [PMS∗08].
Since color and depth can be obtained simultaneously, free
viewpoint rendering is easily incorporated using depth-
compensated warping [KES05]. The real-time nature of the
ToF-measurements enables 3D object recognition and the re-
construction of dynamic 3D scenes for novel applications
such as free viewpoint TV and 3D-TV. A high-deﬁnition TV
camera or multi-view rig is combined with a ToF camera to
give a depth estimate of the scene. The depth is upscaled and
fused like in [YYDN07,ZWYD08], and a layered depth and
color map is constructed for each image frame. This layered
depth video is than coded and stored for playback on a 3D
auto-stereoscopic display to give glass-less 3D impression to
the viewer.

c(cid:13) The Eurographics Association 2009.

Simultaneous reconstruction of a scene with wide ﬁeld of
view and dynamic scene analysis can be achieved by com-
bining a ToF/color camera pair on a computer-driven pan-tilt
unit and by scanning the environment in a controlled man-
ner. While scanning the scene, a 3D panorama can be com-
puted by stitching both depth and the color images into a
common cylindrical or spherical panorama. From the center
point given by the position of the pan-tilt unit, a 3D envi-
ronment model can be reconstructed in a preparation phase.
Dynamic 3D scene content like a moving person can than be
captured online by adaptive object tracking with the camera
head [BSBK08]. Fig. 6 shows the technical setup of such a
system. Examples can be found in Fig. 8.

Figure 6: Setup comprising of a ToF-camera (SwissRanger
3000) mounted together with a CCD ﬁrewire camera on a
pan-tilt unit and a ﬁsheye camera mounted at the bottom
right.

In addition to the color camera, a second camera with ﬁsh-
eye optics is added to allow easy tracking of the complete
system, thus allowing to move freely inside of the scene.
The hemispherical view of the ﬁsheye camera is used to lo-
cate the current position of the camera rig within the envi-
ronment even for very long time sequences without position
drift [KBK07]. Fig. 7 describes the layout of a complete sys-
tem that is used to model the scene and to track the dynamic
object. The information obtained here can be utilized for ex-
ample in depth-based keying, object segmentation, shadow
computation and general Mixed Reality tasks, as described
in Sec. 7.

A number of application oriented contributions based on
ToF cameras have been made. In [TBB08], for example, a
method for using a ToF camera for detection and tracking of
pipeline features such as junctions, bends and obstacles has
been presented. Feature extraction is done by ﬁtting cylin-
ders and cones to range-images from inside the pipeline.

ToF cameras have an obvious potential for external sens-
ing in automotive applications. In [AR08] a system design
for parking assist and backup has been presented. A further

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

paper [GMR08] uses a RANSAC algorithm for ﬁtting planes
to 3D data such as to enable the recognition of curbs and
ramps.

Regarding dynamic scene analysis, one of the ﬁrst ToF-
based applications was the so called out-of-position system
where the airbag in the car is deployed as a function of head
position [FOS∗01]. The application requires the recognition
of different seat-occupancy classes like for example adult,
child, rear-facing child seat, cargo etc. In addition, the head
must be tracked such as to avoid deployment in cases where
the head is close to the airbag. In this context, a human body
tracker based on Reeb graphs extracted from ToF data has
been developed [DCG∗07].

ToF camera systems can be successfully utilized to detect
respiratory motion of human persons. Possible applications
are emission tomography where respiratory motion may
be the main reason for image quality degradation. Three-
dimensional, marker-less, real-time respiratory motion de-
tection can be accomplished with available ToF camera sys-
tems with an accuracy of 0.1 mm and is clearly competi-
tive with other image based approaches [PSHK08,SPH08a].
Moreover, one can use ToF cameras to monitor respiration
during sleep and detect sleep apnea [FID08].

Some medical applications like cancer treatment require
a re-positioning of the patient to a previously deﬁned posi-
tion. ToF cameras have been used to solve the problem by
segmentation of the patient body and a rigid 3D-3D surface
registration. Tests with human persons yielded registration
errors in the millimeter range [ASPH08].

7. Dynamic 3D Depth Keying and Shadow Interaction
One application particularly well suited for ToF-cameras is
real-time depth keying in dynamic 3D scenes. A feature
commonly used in TV studio production today is the 2D
chroma keying, where a speciﬁc background color serves
as key for 2D segmentation of a foreground object, usu-
ally a person, which can then be inserted in computer gen-
erated 2D background. The 2D-approach is limited, how-
ever, since the foreground object can never be occluded by
virtual objects. An alternative approach for high-resolution
foreground-background segmentation incorporating a bilat-
eral ﬁltering of the object boundary based on 2D/3D-images
is presented in [CTPD08]. ToF-cameras can allow for true
3D segmentation, possibly combined with 3D object inser-
tion for live online Augmented Reality [Tho06, IY01] or
Shape-from-Silhouette reconstruction [GLA∗08b].

Guan et.al. [GFP08] present a system that combines mul-
tiple ToF cameras with a set of video cameras to simulta-
neously reconstruct dynamic 3D objects with Shape-from-
Silhouettes and range data. Up to four ToF cameras are illu-
minating the scene from wide baseline views at slightly dif-
ferent modulation frequencies, interleaved with color cam-
eras for silhouette extraction. They extract dynamic 3D ob-

c(cid:13) The Eurographics Association 2009.

Figure 7: System for 3D environment modeling, 3D object
tracking and segmentation for Mixed Reality applications
(from [BSBK08]).

(1)

(2)

(3)

Figure 8: Column (1): The texture and the depth image (dark
= near, light = far) as panorama after scanning of the envi-
ronment. For visualization, the panorama is mapped onto a
cylindric image. Column (2): Two images out of a sequence
of a person walking through the real room with a virtual oc-
cluder object. (2) Original image; (3) ToF depth image for
depth keying.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

ject volumes from the probability distribution of the object
occupancy grid over time.

In [BSBK08], a Mixed Reality system using a combined
color and ToF-Camera rig is discussed. The overview of the
system is given in Fig. 7. The key features in this system are
the dynamic 3D depth keying and the mixing of real and
virtual content. A ToF-camera mounted on a pan-tilt unit
(Fig. 6) allows to rapidly scan the 3D studio background in
advance, generating a panoramic 3D environment of the 3D
studio background. Fig. 8, column (1), shows the texture and
depth of a sample background scene. The scan was gener-
ated with a SR3000, automatically scanning a 180◦ × 120◦
(horizontal × vertical) hemisphere, the corresponding color
was captured using a ﬁsh-eye camera with the same ﬁeld-
of-view. The depth of foreground objects can be captured
dynamically with the ToF-camera and allows a depth seg-
mentation between the generated background model and the
foreground object, providing the possibility of full visual in-
teraction of the person with 3D virtual objects in the room.
Fig. 8, columns (2) and (3) show the online-phase, where a
moving person is captured both in color (2) and depth (3).
Thus a full 3D representation of both environment and per-
son is available in the online phase.

Depth keying and seamless mixing of real and virtual con-
tent is now possible. Fig. 9 shows the different steps of the
process. The real object can be extracted from the real scene
by depth background subtraction using warped ToF depth
and background depth. A result is shown in Fig. 9 in the
center. Virtual objects will then be inserted at the correct
position and with correct occlusion automatically, since a
depth map of real and virtual content is available. Finally,
even correct light, shading and shadows can be computed
if the position and characteristics of the light sources are
known [SBK∗08a]. A shadow-map [Wil78] is computed by
projecting shadows of the virtual objects onto the recon-
structed real environment, and a simple color attenuation is
used to render shadows from multiple real light source po-
sitions in real-time directly on the GPU. See Fig. 9 bottom
for results. Since the ToF camera captures the dynamic ob-
ject as depth map on the ﬂy, a uniﬁed mesh representation of
all scene parts – environment, real person, virtual objects–
can be constructed, allowing even to cast shadows from the
virtual objects onto the real person and vice versa.

8. User Interaction and User Tracking

An important application area for ToF-sensors is that
of interactive systems such as alternative input devices,
games, animated avatars etc. An early demonstrator real-
ized a large virtual interactive screen where a ToF-camera
tracks the hand and thereby allows for touch-free interac-
tion [OBL∗05]. [SPH08b] present a similar application for a
touch-free navigation in a 3D medical visualization.

The “nose mouse” [HBMB07] tracks the position of

c(cid:13) The Eurographics Association 2009.

Figure 9: Overview of the color mixing and shadowing cast-
ing of real and virtual objects on the GPU. On the left hand
side all the input images are displayed. Based on the differ-
ent depth images mutual occlusions can be handled in the
augmentation. Moreover foreground segmented depth im-
ages and mixed depth images are delivered. The scaling of
the augmented image via the light map yields the ﬁnal color
image output (from [SBK∗08a]).

the nose in the camera image and uses this to control
Dasher [WM02], an alternative text-input tool, allowing
hands-free text input with a speed of 12 words per minute.
The tracker is based on geometric features that are related
to the intrinsic dimensionality of multidimensional signals.
These features can be used to determine the position of the
nose in the image robustly using a very simple bounding-box
classiﬁer, trained on a set of labelled sample images. Despite
its simplicity, the classiﬁer generalizes well to subjects it was
not trained on. An important result is that the robustness of
the nose tracker could be drastically increased by using both
the intensity and the depth signals of the ToF-camera, com-
pared to using either of the signals alone (see Fig. 10). A
similar approach has been used in [WLL07] to detect faces
based on a combination of gray-scale and depth information

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

from a ToF-camera. Additionally, active contours are used
for head segmentation.

Figure 10: Left: Example nose-detection results shown on
ToF range image; detection error rate is 0,03 [BHMB08a].
Right: The direction the user is pointing in can be computed
from the difference vector between the positions of the head
and hand [HBMB09].

Figure 11: Top: An overlay of two ToF images in a gait se-
quence. The two image correspond to one stride. Bottom: the
articulated human pose model ﬁtted to the ToF gait sequence
using a pose-cut algorithm for ToF data [RPL09].

Human-Machine-Interaction during an intervention in the
sterile environment of an operation room is becoming an
important application due to the increasing incorporation of
medical imaging. Available ToF cameras have been success-
fully used to provide a robust, marker-less, real-time, three-
dimensional interaction interface by detecting hand gestures
and movements [PSFH08, SPH08b].

Gesture recognition is another important user interaction
area, which can clearly beneﬁt from current ToF-sensors.
First results are presented in [HM08], where only range data
are used. Here, motion is detected using band-pass ﬁltered
difference range images. [HMF08] extends this to full body
gesture recognition using spherical harmonics.

Deictic (pointing) gestures are an important class of ges-
tures. ToF-cameras make it possible to measure directly
where a user is pointing in space. The pointing direction
can be used to determine whether a gesture is made to-
wards the system or other people, and to assign different
meanings to the same gesture depending on pointing direc-
tion [HBMB09]. In this work, deictic gestures are used to
control a slide-show presentation: Pointing to the left or right
of the screen and making a “hand ﬂick” gesture switches to
the previous or next slide. A “virtual laser pointer” is dis-
played when the user points at the screen. Fig. 10 shows an
example detection of a deictic gesture.

Furthermore, the ﬁtting of articulated human models have
also been reported. In [ZDF08] a set of upper body feature
points are tracked over time in a ToF image data stream, and
an articulated human model is subsequently used for esti-
mating the pose of the body parts in a closed loop tracking
control algorithm. Moreover, the model provides a feedback
to the feature detection in order to resolve ambiguities or to
provide estimates of undetected features. Based on a simple
kinematic model, constraints such as joint limit avoidance,
and self penetration avoidance are implemented.

In [RPL09] a foot-leg-torso articulated model is ﬁtted to a
ToF image stream for gait analysis using the so-called pose-
cut algorithm [KRBT08]. Here the segmentation and pose
problem is formulated as the minimization of cost function
based on a Conditional Random Field (CRF). This has the
advantage that all information in the image (edges, back-
ground and foreground appearances), as well as the prior
information on the shape and pose of the subject can be com-
bined and used in a Bayesian framework. In Fig. 11 an exam-
ple of the ﬁt of a human articulated model to a gait sequence
of ToF data is shown (from [RPL09]).

Recent work considers the application of ToF-sensors for
user tracking and man-machine-interaction. Tracking peo-
ple in a smart room, i.e. multi-modal environments where
the audible and visible actions of people inside the rooms
are recorded and analyzed automatically, can beneﬁt from
the usage of ToF-sensors [GLA∗08b]. The described ap-
proach comprises one ToF- and six RGB-sensors. A re-
ﬁned shape-from-silhouette technique, based on an initial

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

binary foreground segmentation for RGB- and range data,
is used to construct the visual hulls for the people to be
tracked. Another different tracking approach has been dis-
cussed in [HHK∗08]. Here, only one ToF-sensor is used ob-
serving a scene at an oblique angle. Segmented 3D data of
non-background clusters are projected onto a plane, i.e. the
ﬂoor, and are approximated by ellipses. Due to occlusion,
the tracking involves merging and elimination of individual
clusters.

The estimation of range ﬂow can facilitate the robust in-
terpretation of complex gestures. In [SJB02] the authors pro-
pose methods that estimate range ﬂow from both range and
intensity data. These methods are of particular value for
ToF camera applications because ToF cameras provide both
types of data simultaneously and in a perfectly registered
fashion.

The incorporation of ToF-sensors in a mobile robot sys-
tem has been studied in [SBSS08]. The goal was to setup
an environment model and to localize human interaction
partners in this environment. This is achieved by tracking
3D points using an optical ﬂow approach and a weak ob-
ject model with a cylindrical shape. In [GLA∗08a] a system
to control an industry robot by gestures is described. The
system incorporates a monocular 2D/3D sensor [LHLW07].
The range image is used for a ﬁrst hand segmentation, fol-
lowed by a fusion with the 2D color information. The pos-
ture classiﬁcation uses a learning based techniques.

9. Light-Fields
Light-ﬁeld techniques focus on the representation and recon-
struction of the so-called plenoptic function, which describes
the intensity of all light-rays at any point in 3D. Thus, light-
ﬁelds are capable of describing complex lighting and mate-
rial attributes from a set of input images without a tedious
reconstruction of geometry and material properties [ZC03].
Image synthesis based on light-ﬁelds incorporates interpola-
tion techniques applied to spatially neighboring rays. If these
neighboring rays do not correspond to neighboring object
points, ghosting artifacts arise, which can only be resolved
by using a dense sampling of the plenoptic function, thus
requiring a large number of input images [CTCS00].

represent

Other approaches

light-ﬁelds with addi-
tional geometric information, e.g. a coarse polygonal
model [GGSC96]. In general, this requires pre-knowledge
or exhaustive geometry extraction in a pre-processing step.
Alternative techniques have been introduced based on range
maps, yielding an interleaved RGBz light-ﬁeld representa-
tion [TRSKK08]. The light-ﬁeld samples are arranged in a
spherical manner, thereby guaranteeing a uniform light-ﬁeld
representation. This approach provides a more efﬁcient and
accurate means for image synthesis, since the correspon-
dence problem can be solved directly using a ray-casting
technique. Thus, ghosting artifacts are minimized. Addition-
ally, this light-ﬁeld representation and rendering technique

c(cid:13) The Eurographics Association 2009.

has been extended with progressive data transfer and level
of detail techniques [TRSBK08] and it has been applied to
interactive high-quality rendering in various application ar-
eas [RSTK08].

ToF-cameras can be used to acquire RGBz light-ﬁeld
samples of real objects in a natural way. An additional ben-
eﬁt results from the immediate visual feedback due to the
direct incorporation of new data into the light-ﬁeld repre-
sentation without any pre-calculation of depth information.
However, the stated challenges of the ToF-cameras, espe-
cially the problems at object silhouettes, severely interfere
with the required high-quality object representation and im-
age synthesis for synthetic views.

In [TLRS∗08] a system has been proposed that uses
RGBz light-ﬁelds for object recognition based on a analysis-
by-synthesis approach. A current research setup described
in [TRSKK08] includes a binocular acquisition system using
a ToF-camera in combination with adequate data process-
ing in order to suppress artifacts at object silhouettes (see
Fig. 12). Furthermore, this approach includes the re-binning
of light-ﬁeld samples into the regular spherical light-ﬁeld
representation, eliminating the requirement to locate the sen-
sor at a pre-deﬁned camera positions on the sphere.

10. Conclusion and Future Development
In this report we presented a review of the ongoing research
on novel real-time range-sensing devices based on the Time-
of-Flight (ToF) principle. These sensors are currently un-
der development and ﬁrst commercial cameras are available.
ToF cameras based on intensity modulation deliver informa-
tion about range, amplitude and intensity. Range is derive
from the phase shift between the emitted and the reﬂected
light, the amplitude values describe the amount of correla-
tion between the two, and the intensity relates to the amount
of incident active light, which is itself determined by the
object’s distance and reﬂectivity. An alternative approach is
based on optical shutter techniques.

From the application perspective, these devices exhibit
a large number of speciﬁc effects that must be considered.
Therefore, some space has been devoted to approaches that
deal with effects like the systematic and the intensity-based
distance errors, with ﬂying pixels caused by depth inhomo-
geneity and sensor noise. However, not all of these effects
can be compensated for. Problems like motion artifacts re-
main and model based approaches aimed at reducing the ef-
fects of scattering and multiple reﬂections are still open for
further improvements.

A frequently used approach is the combination of ToF-
sensors with high resolution grayscale- or RGB-sensors,
most often in a binocular setup. This leads to a simple yet ef-
ﬁcient multi-modal sensor system that delivers high resolu-
tion intensity and low resolution range data in real-time. The
proposed sensor fusion approaches are already quite mature.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

Figure 12: RGBz light-ﬁeld example from [TRSKK08] using a ToF-sensor. A PMD Vision 19k cameras has been used. The
artifacts result from the inaccurate depth information at the object boundaries.

A very natural application of ToF-sensors is the recon-
struction of object geometry, but here ToF-sensors deliver
rather inaccurate distance measurements compared to, for
example, laser range scanners. However, quite a few ap-
plications have been realized based on ToF-sensors. Espe-
cially depth segmentation of dynamic scenes with respect
to a static background have been successfully implemented,
enabling mixed reality applications like proper integration of
real and virtual objects including shadows. There are quite a
few open problems in this area of application. An example is
free-viewpoint synthesis, which requires a proper integration
of several ToF-sensors in real-time.

The ﬁeld of user-interaction and user tracking has been
widely studied in the last two years, resulting in a number
of signiﬁcant improvements in robustness and functional-
ity based on the incorporation of ToF-sensors. This research
ﬁeld has numerous applications in areas, where touch-free
interaction is required, such as in medical and industrial ap-
plications. Depending on the application, the current restric-
tions of ToF-sensors in the range distance can be a limiting
factor. However, the proposed multi-modal systems already
beneﬁt from the usage of these new range sensors and in
particular the fusion of range and intensity data has been
shown to considerably increase the robustness of tracking
algorithms.

First results in ToF-based acquisition of light ﬁelds are at
hand. Here, the limited accuracy of ToF sensors still causes
severe problems with image synthesis based on light ﬁelds
acquired from real scenes.

Overall we are conﬁdent that the growing interest in ToF
technology, the ongoing development of sensor hardware,
and the increasing amount of related research on the algo-
rithmic foundations of real-time range data processing will
soon lead to further solutions of the discussed problems, as
well as of further problem domains and new applications.

Short Bibliographies
Andreas Kolb is head of the Computer Graphics and Mul-
timedia Systems Group at the University of Siegen since
2003. His academy background is computer graphics and

visualization. He received his PhD at the University of Er-
langen in 1995 under supervision of Prof. H.-P. Seidel.
Andreas Kolb is member of the Center for Sensor Sys-
tems (ZESS) at the University of Siegen. His research in-
terests are computer graphics and computer vision, includ-
ing light-ﬁelds [TLRS∗08], real-time processing of sensor
data [LK06, LLK08, LK08], and real-time sensor simulation
using graphics hardware [KKP07]. Andreas Kolb is speaker
of the DFG research package “Dynamic 3D Vision”, takes
part in the BMBF-funded “Lynkeus” project and is principle
investigator in the Research School MOSES.

Erhardt Barth is currently the head of the vision group
the Institute for Neuro- and Bioinformatics, University of
Luebeck. His main research interests are in the areas of hu-
man and computer vision [AMS∗06] and interactive sys-
tems [HBMB07, BDB∗06]. He received his Ph.D. in electri-
cal and communications engineering at the Technical Uni-
versity of Munich and has conducted research at the Univer-
sities of Melbourne and Munich, the Institute for Advanced
Study in Berlin, the NASA Vision Science and Technol-
ogy Group in California, and the Institute for Signal Pro-
cessing in Luebeck. In May 2000 he received a Schloess-
mann Award. Dr. Barth is currently coordinating the Eu-
ropean projects “Action recognition and tracking based on
time-of-ﬂight sensors” (ARTTS, see www.artts.eu and
[HBMB07]), and “Gaze-based communication” (GazeCom,
see www.gazecom.eu and [BDB∗06]), which both involve
interactive real-time graphics.

Reinhard Koch is head of the Multimedia Information
Processing group at the Christian-Albrechts-University Kiel
since 1999. He received his PhD (Dr.-Ing.) in Electrical En-
gineering in the ﬁeld of 3D scene modeling from the Uni-
versity of Hannover in 1996, and from 1996–99 he lead
the 3D modeling team at the KU Leuven, Belgium, in the
vision group of Prof. Luc van Gool. His research inter-
ests are 3D modeling from video and images [KES05],
fast camera tracking and calibration algorithms [BK08], the
conﬂuence of Computer Graphics and Computer Vision in
Mixed and Augmented Reality, and real-time image pro-
cessing [SBKK07, KBK07]. He was involved in several Eu-
ropean projects for image-based reconstruction and is cur-

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

rently working in the EU project 3D4YOU concerned with
real-time depth estimation and view generation for 3D-TV.

ToF-Camera based Computer Vision (2008).
10.1109/CVPRW.2008.4563157.

DOI

Rasmus Larsen Rasmus Larsen is a full professor and head
of the section for image analysis and computer graphics at
the Department for Informatics and Mathematical Modelling
at the Technical University of Denmark. He received his
Ph.D. degree in statistical image analysis from The Techni-
cal University of Denmark in 1994 under supervision of pro-
fessor Knut Conradsen. He has had sabbaticals at the Uni-
versity of California San Diego in 1995 and Stanford Uni-
versity in 2003–2004. Rasmus Larsens research interest are
centered on image analysis and statistical methods for hyper-
dimensional data. In particular he has focused on modelling
and analysis of shape and deformation with application in
medical and biological and food image analysis and object
recognition. In March 2008 he received the Statoil prize for
internationally recognized research in medical image analy-
sis.

References
[AMS∗06] AACH T., MOTA C., STUKE I., MÜHLICH
M., BARTH E.: Analysis of superimposed oriented pat-
IEEE Transactions on Image Processing 15, 12
terns.
(2006), 3690–3700.

[AR08] ACHARYA S., RAFII A.: System design of time-
of-ﬂight range camera for car park assist and backup ap-
plications. In IEEE Conf. on Computer Vision & Pattern
Recogn.; Workshop on ToF-Camera based Computer Vi-
sion (2008). DOI 10.1109/CVPRW.2008.4563164.

[ASPH08] ADELT A., SCHALLER C., PENNE J.,
Patient positioning using 3-d sur-
In Proc. Russian-Bavarian Conf. on

HORNEGGER J.:
face registration.
Biomedical Engineering (2008), pp. 202–207.

[BBK07] BEDER C., BARTCZAK B., KOCH R.: A com-
bined approach for estimating patchlets from PMD depth
images and stereo intensity images. In Proc. of the DAGM
(2007), LNCS, Springer, pp. 11–20.

[BDB∗06] BARTH E., DORR M., BÖHME M., GEGEN-
FURTNER K., MARTINETZ T.: Guiding eye movements
for better communication and augmented vision. In Per-
ception and Interactive Technologies (2006), vol. 4021 of
LNCS, Springer, pp. 1–8.

[BHMB08a] BOEHME M., HAKER M., MARTINETZ T.,
BARTH E.: A facial feature tracker for human-computer
Int. J. on Intell.
interaction based on 3D ToF cameras.
Systems Techn. and App., Issue on Dynamic 3D Imaging
5, 3/4 (2008), 264–273.

[BHMB08b] BÖHME M., HAKER M., MARTINETZ T.,
improves accuracy
BARTH E.:
In IEEE Conf. on
of time-of-ﬂight measurements.
Computer Vision & Pattern Recogn.; Workshop on

Shading constraint

c(cid:13) The Eurographics Association 2009.

[BK08] BEDER C., KOCH R.: Calibration of focal length
and 3D pose based on the reﬂectance and depth image of
a planar object. Int. J. on Intell. Systems Techn. and App.,
Issue on Dynamic 3D Imaging 5, 3/4 (2008), 285–294.

[BKWK07] BARTCZAK B., KOESER K., WOELK F.,
KOCH R.: Extraction of 3D freeform surfaces as visual
landmarks for real-time tracking. J. Real-time Image Pro-
cessing 2, 2–3 (2007), 81–101.

[BSBK08] BARTCZAK B., SCHILLER I., BEDER C.,
KOCH R.:
Integration of a time-of-ﬂight camera into a
mixed reality system for handling dynamic scenes, mov-
ing viewpoints and occlusions in realtime. In Int. Symp.
on 3D Data Processing, Visualization and Transmission
(3DPVT) (2008).

[CTCS00] CHAI J.-X., TONG X., CHAN S.-C., SHUM
H.-Y.: Plenoptic sampling. In ACM Trans. Graph. (Proc.
SIGGRAPH) (2000), pp. 307–318.

[CTPD08] CRABB R., TRACEY C., PURANIK A., DAVIS
J.: Real-time foreground segmentation via range and
color imaging. In IEEE Conf. on Computer Vision & Pat-
tern Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563170.

[DCG∗07] DEVARAKOTA P. R., CASTILLO M., GIN-
HOUX R., MIRBACH B., OTTERSTEN B.: Application
of the reeb graph techniques to vehicle occupant. In Proc.
of IEEE Computer Vision and Pattern Recognition Work-
shop (2007).

[DT06] DIEBEL J. R., THRUN S.: An application of
Markov random ﬁelds to range sensing. In Advances in
Neural Information Processing Systems (2006), pp. 291–
298.

[Fal08] FALIE D.: 3D image correction for time of ﬂight
In Int. Conf. of Optical Instrument and

(ToF) cameras.
Technology (2008), pp. 7156–133.

[FB07] FALIE D., BUZULOIU V.: Noise characteristics
In IEEE Sym. on Signals
of 3D time-of-ﬂight cameras.
Circuits & Systems (ISSCS), session on Alg. for 3D ToF-
cameras (2007), pp. 229–232.

[FB08] FALIE D., BUZULOIU V.: Distance errors cor-
In Euro-
rection for the time of ﬂight (ToF) cameras.
pean Conf. on Circuits and Systems for Communications
(2008), pp. 193–196.

[FH08] FUCHS S., HIRZINGER G.: Extrinsic and depth
calibration of ToF-cameras. Proc. IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR) (2008).
DOI: 10.1109/CVPR.2008.4587828.

[FID08] FALIE D., ICHIM M., DAVID L.: Respiratory
motion visualization and the sleep apnea diagnosis with
the time of ﬂight (ToF) camera. In Visualisation, Imaging
and Simulation (2008).

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

[FM08] FUCHS S., MAY S.: Calibration and registration
for precise surface reconstruction with time-of-ﬂight cam-
eras. Int. J. on Intell. Systems Techn. and App., Issue on
Dynamic 3D Imaging 5, 3/4 (2008), 278–284.

[FOS∗01] FRITZSCHE M., OBERLÄNDER M., SCHWARZ
T., WOLTERMANN B., MIRBACH B., RIEDEL H.: Ve-
hicle occupancy monitoring with optical 3D-sensors. In
Proc. IEEE Intelligent Vehicles Symp. (2001).

[GAL07] GUDMUNDSSON S., AANÆS H., LARSEN R.:
Environmental effects on measurement uncertainties of
time-of-ﬂight cameras. In IEEE Sym. on Signals Circuits
& Systems (ISSCS), session on Alg. for 3D ToF-cameras
(2007), pp. 113–116.

[GAL08] GUDMUNDSSON S., AANÆS H., LARSEN R.:
Fusion of stereo vision and time-of-ﬂight imaging for im-
Int. J. on Intell. Systems Techn.
proved 3D estimation.
and App., Issue on Dynamic 3D Imaging 5, 3/4 (2008),
425–433.

[Gar08] GARCIA F.: External-Self-Calibration of a 3D
time-of-ﬂight camera in real environments. Master’s
thesis, Université de Bourgogne, Heriot-Watt University,
Universitat de Girona (VIBOT - Erasmus Mundus Mas-
ters in VIsion & roBOTics), Le Creusot - France, Edin-
burgh - Scotland, Girona - Spain, 2008.

[GFP08] GUAN L., FRANCO J.-S., POLLEFEYS M.: 3D
object reconstruction with heterogeneous sensor data. In
Int. Symp. on 3D Data Processing, Visualization and
Transmission (3DPVT) (2008).

[GGSC96] GORTLER S., GRZESZCZUK R., SZELISKI
R., COHEN M.: The lumigraph. In ACM Trans. Graph.
(Proc. SIGGRAPH) (1996), pp. 43–54.

[GKOY03a] GVILI R., KAPLAN A., OFEK E., YAHAV
In Proc SPIE, Video-based Image
G.: Depth keying.
Techniques and Emerging Work (2003), vol. 5006. DOI:
10.1117/12.474052.

[GKOY03b] GVILI R., KAPLAN A., OFEK E., YAHAV
G.: Depth keying. In Proc. SPIE, Stereoscopic Displays
and Virtual Reality Systems X (2003), vol. 5006, pp. 564–
574.

[GLA∗08a] GHOBADI S. E., LOEPPRICH O. E., AH-
MADOV F., BERNSHAUSEN J., HARTMANN K., LOF-
FELD O.: Real time hand based robot control using 2D/3D
images. In Int. Symp. Visual Computing (ISVC) (2008),
vol. 5359 of LNCS, Springer, pp. 307–316.

[GLA∗08b] GUÐMUNDSSON S. Á., LARSEN R.,
AANÆS H., PARDÁS M., CASAS J. R.: TOF imaging
in smart room environments towards improved people
tracking. In IEEE Conf. on Computer Vision & Pattern
Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563154.

[GLHL07] GHOBADI S., LOEPPRICH O., HARTMANN

K., LOFFELD O.: Fast and robust 2D/3D gesture seg-
mentation. In IEEE Int. Conf. on Tools with Artiﬁcial In-
telligence (2007), pp. 29–31.

[GMR08] GALLO O., MANDUCHI R., RAFII A.: Robust
curb and ramp detection for safe parking using the canesta
ToF camera. In IEEE Conf. on Computer Vision & Pat-
tern Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563165.

[HBMB07] HAKER M., BÖHME M., MARTINETZ T.,
BARTH E.: Geometric invariants for facial feature track-
In IEEE Sym. on Signals
ing with 3D TOF cameras.
Circuits & Systems (ISSCS), session on Alg. for 3D ToF-
cameras (2007), pp. 109–112.

[HBMB08] HAKER M., BÖHME M., MARTINETZ T.,
Scale-invariant range features for time-
BARTH E.:
In IEEE Conf. on
of-ﬂight camera applications.
Computer Vision & Pattern Recogn.; Workshop on
ToF-Camera based Computer Vision (2008).
DOI
10.1109/CVPRW.2008.4563169.

[HBMB09] HAKER M., BÖHME M., MARTINETZ T.,
BARTH E.: Deictic gestures with a time-of-ﬂight camera.
In Int. Gesture Workshop (2009). (submitted).

[HHK∗08] HANSEN D., HANSEN M., KIRSCHMEYER
track-
M., LARSEN R., SILVESTRE D.:
In IEEE Conf. on
ing with time-of-ﬂight cameras.
Computer Vision & Pattern Recogn.; Workshop on
ToF-Camera based Computer Vision (2008).
DOI
10.1109/CVPRW.2008.4563156.

Cluster

[HJS08] HUHLE B., JENKE P., STRASSER W.: On-the-ﬂy
scene acquisition with a handy multisensor-system. Int. J.
on Intell. Systems Techn. and App., Issue on Dynamic 3D
Imaging 5, 3/4 (2008), 255–263.

[HM08] HOLTE M., MOESLUND T.: View invariant ges-
ture recognition using the CSEM SwissRanger SR-2 cam-
era. Int. J. on Intell. Systems Techn. and App., Issue on
Dynamic 3D Imaging 5, 3/4 (2008), 295–303.

[HMF08] HOLTE M., MOESLUND T., FIHL P.: Fusion of
range and intensity information for view invariant gesture
In IEEE Conf. on Computer Vision & Pat-
recognition.
tern Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563161.

[HSJS08] HUHLE B., SCHAIRER T.,

JENKE P.,
STRASSER W.: Robust non-local denoising of colored
depth data. In IEEE Conf. on Computer Vision & Pattern
Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563158.

[IY01]

IDDAN G. J., YAHAV G.: 3D imaging in the stu-

dio. In Proc. of SPIE (2001), vol. 4298, pp. 48–56.

[JHS07]

JENKE P., HUHLE B., STRASSER W.: Self-
localization in scanned 3DTV sets. In 3DTV CON - The
True Vision (2007). DOI: 10.1109/3DTV.2007.4379421.

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

[KBK07] KOESER K., BARTCZAK B., KOCH R.: Ro-
bust GPU-assisted camera tracking using free-form sur-
face models. J. Real-time Image Processing 2, 2–3 (2007),
133–147.

[KBK08] KOLB A., BARTH E., KOCH R.: ToF-sensors:
In IEEE
New dimensions for realism and interactivity.
Conf. on Computer Vision & Pattern Recogn.; Workshop
on ToF-Camera based Computer Vision (2008). DOI
10.1109/CVPRW.2008.4563159.

[KCTT08] KIM Y. M., CHAN D., THEOBALT C., THRUN
S.: Design and calibration of a multi-view ToF sensor fu-
sion system. In IEEE Conf. on Computer Vision & Pattern
Recogn.; Workshop on ToF-Camera based Computer Vi-
sion (2008). DOI 10.1109/CVPRW.2008.4563160.

[KES05] KOCH R., EVERS-SENNE J.: 3D Video Commu-
nication - Algorithms, concepts and real-time systems in
human-centered communication. Wiley, 2005, ch. View
Synthesis and Rendering Methods, pp. 151–174.

[KFM∗04] KRAFT H., FREY J., MOELLER T., AL-
BRECHT M., GROTHOF M., SCHINK B., HESS H.,
BUXBAUM B.: 3D-camera of high 3D-frame rate, depth-
resolution and background light elimination based on im-
proved PMD (photonic mixer device)-technologies.
In
OPTO (2004).

[KK09] KELLER M., KOLB A.: Real-time simulation of
time-of-ﬂight sensors. J. Simulation Practice and Theory
(2009). submitted (2nd review cycle).

[KKP07] KELLER M., KOLB A., PETERS V.:

A
simulation-framework for time-of-ﬂight sensors. In IEEE
Sym. on Signals Circuits & Systems (ISSCS), session on
Alg. for 3D ToF-cameras (2007), pp. 125–128.

[KKTJ08] KAVLI T., KIRKHUS T., THIELEMANN J. T.,
JAGIELSKI B.: Modelling and compensating mea-
surement errors caused by scattering in time-of-ﬂight
cameras.
DOI
10.1117/12.791019.

In Proc. SPIE (2008), vol. 7066.

[KLSK07] KUHNERT K., LANGER M., STOMMEL M.,
KOLB A.: Vision Systems. Advanced Robotic Systems,
Vienna, 2007, ch. Dynamic 3D Vision, pp. 311–334.

[KRBT08] KOHLI P., RIHAN J., BRAY M., TORR P.
H. S.:. Int. Journal of Computer Vision 79, 3 (2008), 285–
298.

[KRG07] KAHLMANN T., REMONDINO F., GUILLAUME
S.: Range imaging technology: new developments and
applications for people identiﬁcation and tracking.
In
Proc. of Videometrics IX - SPIE-IS&T Electronic Imag-
ing (2007), vol. 6491.

[KRI06] KAHLMANN T., REMONDINO F., INGENSAND
H.: Calibration for increased accuracy of the range imag-
ing camera SwissRangerTM. Image Engineering and Vi-
sion Metrology (IEVM) 36, 3 (2006), 136–141.

c(cid:13) The Eurographics Association 2009.

[KS06] KUHNERT K., STOMMEL M.: Fusion of stereo-
camera and PMD-camera data for real-time suited precise
3D environment reconstruction. In Intelligent Robots and
Systems (IROS) (2006), pp. 4780–4785.

[Lan00] LANGE R.:

3D time-of-ﬂight distance mea-
surement with custom solid-state image sensors in
CMOS/CCD-technology.
PhD thesis, University of
Siegen, 2000.

[LHLW07] LOTTNER O., HARTMANN K., LOFFELD O.,
WEIHS W.: Image registration and calibration aspects for
a new 2D/3D camera. In EOS Conf. on Frontiers in Elec-
tronic Imaging (2007), pp. 80–81.

[LK06] LINDNER M., KOLB A.: Lateral and depth cali-
bration of PMD-distance sensors. In Proc. Int. Symp. on
Visual Computing (2006), LNCS, Springer, pp. 524–533.
[LK07] LINDNER M., KOLB A.: Calibration of the
intensity-related distance error of the PMD ToF-camera.
In Proc. SPIE, Intelligent Robots and Computer Vision
(2007), vol. 6764, pp. 6764–35.

[LK08] LINDNER M., KOLB A.: New insights into
In IEEE Conf. on
the calibration of ToF-sensors.
Computer Vision & Pattern Recogn.; Workshop on
ToF-Camera based Computer Vision (2008).
DOI
10.1109/CVPRW.2008.4563172.

[LKH07] LINDNER M., KOLB A., HARTMANN K.: Data-
fusion of PMD-based distance-information and high-
resolution RGB-images. In IEEE Sym. on Signals Circuits
& Systems (ISSCS), session on Alg. for 3D ToF-cameras
(2007), pp. 121–124.

[LLK08] LINDNER M., LAMBERS M., KOLB A.: Sub-
pixel data fusion and edge-enhanced distance reﬁnement
for 2D/3D images. Int. J. on Intell. Systems Techn. and
App., Issue on Dynamic 3D Imaging 5, 3/4 (2008), 344–
354.

[LPL∗07] LINARTH A. G., PENNE J., LIU B., JESORSKY
O., KOMPE R.: Fast fusion of range and video sensor
data. In Advanced Microsystems for Automotive Applica-
tions (2007), pp. 119–134.

[MB07] MECHAT M.-A. E., BÜTTGEN B.: Realization
of multi-3D-ToF camera environments based on coded-
In Proc. of the Conf. on
binary sequences modulation.
Optical 3-D Measurement Techniques (2007).

[MDH07] MURE-DUBOIS J., HÜGLI H.: Optimized
scattering compensation for time of ﬂight camera.
In
Proc. SPIE, Two- and Three- Dimensional Methods for
Inspection and Metrology V (2007), vol. 6762. DOI
10.1117/12.733961.

[OBL∗05] OGGIER T., BÜTTGEN B., LUSTENBERGER
F., BECKER G., RÜEGG B., HODAC A.: Swissranger
sr3000 and ﬁrst experiences based on miniaturized 3D-
In Proc. of the First Range Imaging Re-
ToF cameras.
search Day at ETH Zurich (2005).

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

[OFCB07] OPRISESCU S., FALIE D., CIUC M., BUZU-
LOIU V.: Measurements with ToF cameras and their nec-
essary corrections. In IEEE Sym. on Signals Circuits &
Systems (ISSCS), session on Alg. for 3D ToF-cameras
(2007), pp. 221–224.

[PLHK07] PETERS V., LOFFELD O., HARTMANN K.,
KNEDLIK S.: Modeling and bistatic simulation of a high
resolution 3D PMD-camera. In Proc. Congress on Mod-
elling and Simulation (EUROSIM) (2007).

[PMS∗08] PRUSAK A., MELNYCHUK O., SCHILLER I.,
ROTH H., KOCH R.: Pose estimation and map building
with a PMD-camera for robot navigation. Int. J. on Intell.
Systems Techn. and App., Issue on Dynamic 3D Imaging
5, 3/4 (2008), 355–364.

[PSFH08] PENNE J., SOUTSCHEK S., FEDOROWICZ L.,
HORNEGGER J.: Robust real-time 3D time-of-ﬂight
based gesture navigation. In Proc. IEEE Int. Conf. on Au-
tomatic Face and Gesture Recognition (2008).

[PSHK08] PENNE J., SCHALLER C., HORNEGGER J.,
KUWERT T.: Robust real-time 3D respiratory motion de-
tection using time-of-ﬂight cameras. Computer Assisted
Radiology and Surgery 3, 5 (2008), 427–431.

[Rap07] RAPP H.: Experimental and Theoretical Inves-
tigation of Correlating ToF-Camera Systems. Master’s
thesis, University of Heidelberg, Germany, 2007.

[RFK08] RADMER J., FUSTÉ P. M., KRÜGER J.:

In-
light related distance error study and calibra-
cident
In IEEE
tion of the PMD-range imaging camera.
Conf. on Computer Vision & Pattern Recogn.; Workshop
on ToF-Camera based Computer Vision (2008). DOI
10.1109/CVPRW.2008.4563168.

[RPL09] RAMSBØL R., PAULSEN R. R., LARSEN R.:. In
Proceedings of the Scandinavian Conference on Image
Analysis (2009), LNCS, Springer. accepted for publica-
tion.

[RSTK08] REZK-SALAMA C., TODT S., KOLB A.: Light
ﬁeld galleries from volumetric data. Computer Graphics
Forum (Proc. EuroVis) 27, 3 (2008), 839–846.

[SBK∗08a] SCHILLER I., BARTCZAK B., KELLNER F.,
KOLLMANN J., KOCH R.: Increasing realism and sup-
porting content planning for dynamic scenes in a mixed
reality system incorporating a time-of-ﬂight camera.
In
IEE-IET Conf. on Visual Media Production (2008).

[SBK08b] SCHILLER I., BEDER C., KOCH R.: Calibra-
tion of a PMD camera using a planar calibration object
together with a multi-camera setup. In The Int. Archives
of the Photogrammetry, Remote Sensing and Spatial In-
formation Sciences (2008), vol. Vol. XXXVII. Part B3a,
pp. 297–302. XXI. ISPRS Congress.

[SBKK07] STRECKEL B., BARTCZAK B., KOCH R.,
KOLB A.: Supporting structure from motion with a 3D-
In Scandinavian Conf. Image Analysis
range-camera.
(SCIA) (2007), LNCS, Springer, pp. 233–242.

[SBSS08] SWADZBA A., BEUTER N., SCHMIDT J.,
SAGERER G.: Tracking objects in 6d for reconstructing
static scene. In IEEE Conf. on Computer Vision & Pat-
tern Recogn.; Workshop on ToF-Camera based Computer
Vision (2008). DOI 10.1109/CVPRW.2008.4563155.

[SJB02] SPIES H., JÄHNE B., BARRON J. L.: Range ﬂow
estimation. Computer Vision and Image Understanding
85, 3 (2002), 209–231.

[SPH08a] SCHALLER C., PENNE J., HORNEGGER J.:
Time-of-ﬂight sensor for respiratory motion gating. Med-
ical Physics 35, 7 (2008), 3090–3093.

[SPH08b] SOUTSCHEK S., PENNE J., HORNEGGER J.:
imag-
3D gesture-based scene navigation in medical
In IEEE
ing applications using time-of-ﬂight cameras.
Conf. on Computer Vision & Pattern Recogn.; Workshop
on ToF-Camera based Computer Vision (2008). DOI
10.1109/CVPRW.2008.4563162.

[SPH08c] STÜRMER M., PENNE J., HORNEGGER J.:
Standardization of amplitude-values acquired by ToF-
In IEEE Conf. on Computer Vision & Pattern
cameras.
Recogn.; Workshop on ToF-Camera based Computer Vi-
sion (2008). DOI 10.1109/CVPRW.2008.4563166.

[STDT08] SCHUON S., THEOBALT C., DAVIS

J.,
High-quality scanning using time-of-
THRUN S.:
In IEEE Conf. on
ﬂight depth superresolution.
Computer Vision & Pattern Recogn.; Workshop on
ToF-Camera based Computer Vision (2008).
DOI
10.1109/CVPRW.2008.4563171.

[TBB08] THIELEMANN J., BREIVIK G., BERGE A.:
Pipeline landmark detection for autonomous robot nav-
In IEEE Conf.
igation using time-of-ﬂight imagery.
on Computer Vision & Pattern Recogn.; Workshop on
ToF-Camera based Computer Vision (2008).
DOI
10.1109/CVPRW.2008.4563167.

[Tho06] THOMAS G.: Mixed reality techniques for TV
and their application for on-set and pre-visualization in
ﬁlm production. In Int. Workshop on Mixed Reality Tech-
nology for Filmmaking (2006).
[TJNU97] THOMAS G. A.,

JIN J., NIBLETT T.,
URQUHART C.: A versatile camera position measurement
In Proc. of Int. Broadcasting
system for virtual reality.
Convention (1997), pp. 284–289.

[TLRS∗08] TODT S., LANGER M., REZK-SALAMA C.,
KOLB A., KUHNERT K.: Spherical light ﬁeld rendering
in application for analysis by synthesis. Int. J. on Intell.
Systems Techn. and App., Issue on Dynamic 3D Imaging
5, 3/4 (2008), 304–314.

[TRSBK08] TODT S., REZK-SALAMA C., BRÜCK-
BAUER L., KOLB A.: Progressive light ﬁeld rendering
In Proc. Workshop on
for web based data presentation.
Hyper-media 3D Internet (2008), pp. 23–32.

[TRSKK08] TODT S., REZK-SALAMA C., KOLB A.,

c(cid:13) The Eurographics Association 2009.

A. Kolb, E. Barth & R. Koch & R. Larsen / Time-of-Flight Sensors in Computer Graphics

KUHNERT K.: GPU-based spherical light ﬁeld rendering
with per-fragment depth correction. Computer Graphics
Forum27, 8 (2008), 2081–2095. accepted for publication.
Casting curved shadows on
In ACM Trans. Graph. (Proc. SIG-

curved surfaces.
GRAPH) (1978), ACM, pp. 270–274.

[Wil78] WILLIAMS L.:

[WLL07] WITZNER D., LARSEN R., LAUZE F.: Improv-
In IEEE Sym. on
ing face detection with ToF cameras.
Signals Circuits & Systems (ISSCS), session on Alg. for
3D ToF-cameras (2007), pp. 225–228.

[WM02] WARD D. J., MACKAY D. J.: Fast hands-free
writing by gaze direction. Nature 418, 6900 (2002), 838.
[XSH∗98] XU Z., SCHWARTE R., HEINOL H.,
Smart pixel – pho-
In Proc. Int. Conf. on

BUXBAUM B., RINGBECK T.:
tonic mixer device (PMD).
Mechatron. & Machine Vision (1998), pp. 259–264.

[YIM07] YAHAV G., IDDAN G. J., MANDELBAUM D.:
3D imaging camera for gaming application. In Digest of
Technical Papers of Int. Conf. on Consumer Electronics
(2007). DOI: 10.1109/ICCE.2007.341537.

[YYDN07] YANG Q., YANG R., DAVIS J., NISTER D.:
Spatial-depth super resolution for range images. Proc.
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) (2007). DOI: 10.1109/CVPR.2007.383211.

[ZC03] ZHANG C., CHEN T.: A Survey on Image-Based
Rendering. Tech. rep., Electrical and Computer Engineer-
ing Carnegie Mellon University, 2003.

[ZDF08] ZHU Y., DARIUSH B., FUJIMURA K.: Con-
trolled human pose estimation from depth image streams.
In IEEE Conf. on Computer Vision & Pattern Recogn.;
Workshop on ToF-Camera based Computer Vision (2008).
DOI 10.1109/CVPRW.2008.4563163.

[ZWYD08] ZHU J., WANG L., YANG R., DAVIS J.: Fu-
sion of time-of-ﬂight depth and stereo for high accu-
racy depth maps. Proc. IEEE Conf. on Computer Vi-
sion and Pattern Recognition (CVPR) (2008). DOI:
10.1109/CVPR.2008.4587761.

c(cid:13) The Eurographics Association 2009.

