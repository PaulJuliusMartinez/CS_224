A sharp concentration inequality with applications

Stephane Boucheron Laboratoire de Recherche en Informatique
B^atiment 490 CNRS-Universite Paris-Sud
91405 Orsay-Cedex

Gabor Lugosi  Department of Economics, Pompeu Fabra University Ramon Trias Fargas 25-27, 08005 Barcelona, Spain,

Pascal Massart Mathematiques Ba^timent 425 Universite Paris-Sud 91405 Orsay-Cedex

April 15, 1999

Abstract
We present a new general concentration-of-measure inequality and illustrate its power by applications in random combinatorics. The results nd direct applications in some problems of learning theory.

The work of the second author was supported by DGES grant PB96-0300
0

1 Introduction
The phenomenon of measure concentration has recently received distinguished attention due to its much better understanding and its spectacular power and simplicity in applications. The basic methods for proving concentration inequalities have been
1 martingale methods|see McDiarmid 22 , 23 for excellent surveys; 2 information-theoretic methods, see Alhswede, Gacs, and Korner 1 , Marton
17 , 18 , 19 , Dembo 5 and Massart 21 ; 3 Talagrand's induction method 27 , 25 , 26 , which led to a large variety of pow-
erful new inequalities. Recently, a new proof technique emerged based on logarithmic Sobolev inequalities, see Ledoux 14 , 13 . The method has been shown to provide the sharpest inequalities for empirical processes Massart 20 . The purpose of this paper is to show that the concentration inequalities obtained by this method have wide applications outside of empirical process theory. In Section 2 we present a general concentration inequality, which is derived from results in Massart 20 . In Section 3 this inequality is applied to prove sharp concentration of certain random combinatorial objects such as the empirical vc dimension of a family of sets as well as to sharpen earlier concentration inequalities for the length of the longest increasing subsequence and for other con guration functions. In Section 4 we show that the new inequality may be used to prove new concentration inequalities for quantities like the number of increasing subsequences or the empirical Vapnik-Chervonenkis entropy vc-entropy of a class of sets. These concentration results have direct applications in learning theory, which are illustrated in Section 5.
1

2 A concentration inequality for nonnegative functionals

The main result of the paper is the following concentration inequality for functionals of independent not necessarily identically distributed random variables:
Theorem 1 Let X1; :::; Xn be independent random variables taking values in some measurable set X , and let f : X n ! 0; 1 be a function. Assume that there exists another function g : X n,1 ! R such that for any x1; : : : ; xn 2 X , the following
properties hold:

0  fx1; : : : ; xn , gx1; : : : ; xi,1; xi+1; : : : ; xn  1; for every 1  i  n 1

and
Xn fx1; : : : ; xn , gx1; : : : ; xi,1; xi+1; : : : ; xn  fx1; : : : ; xn:
i=1

2

Denote Z = fX1; : : : ; Xn, and de ne h u = 1 + u log 1 + u , u, for u  ,1.

Then for every positive number t

P

Z

E

Z

+t


 exp ,E

Z


h
E

t Z


:

3

Moreover for every positive number t  E Z

P

ZE

Z

,t


 exp ,E

Z


h, E

t Z


:

4

The proof of inequality 3 may be obtained by a modi cation of the proof of an inequality in 20 for the right tail of the supremum of a nonnegative empirical process. The left-tail inequality 4 is new. The proof of a more general version of the theorem is given in the Appendix.
Remarks. 1. A typical application of the theorem is to the supremum of sums of
abDnnoeedninnndediengenagpotetifnvinedxgeb1nb;oty:u:n0:d;;e1xdnNnr-sav=onamldusoeeumdppotrvsaaNintridiPvaoebmnil=ien1stvxeagier;teimarabpsnliuedrcsihcgaatnlxhdp1ar;tco:ofc:ne:ssx;is1xde;esn:r,:.1:ZI;nx==dnesseuu=dpp, tPtleNNtin=PXP1 x1inni;=i=,;:11:1,:X;xoXiin;;ttne,.
obviously has

0  fx1; : : : ; xn , gx1; : : : ; xi,1; xi+1; : : : ; xn  xi;  1

2

and therefore
Xn fx1; : : : ; xn , gx1; : : : ; xi,1; xi+1; : : : ; xn  Xn xi; = fx1; : : : ; xn:
i=1 i=1

2. Using the same argument as in Massart 20 , we see that inequality 4 and

similarly 3 is in some sense unimprovable. Indeed, consider N = 1 and suppose

that X1; :::; Xn are independent Bernoulli trials with probability of success p = 1 , q.

In

this

case

4

states

that for
P Z

every 0
np , t



t
exp

np,
,np

h



,t
np



:

Given  0, taking p = =n and setting t = ", this inequality may be written as

P Z   , "  exp ,h ," ; for every " 2 0; 1 :

5

But Z follows the binomial distribution B n; =n and therefore follows asymptoti-
cally the Poisson distribution with parameter  as n goes to in nity. Moreover, the right-hand side of 5 is known to be the Cramer-Cherno deviation upper bound for a Poisson random variable with parameter  . This implies that the exponent in this upper bound cannot be improved since Cramer's large deviation asymptotic ensures
that for for every " 2 0; 1

lim!1inf

nl!im1

1 

log

P

Z

  , "

 ,h ," :

3.

It

is

worth

noting

that PZ

3 and 4
E Z +t

respexepctiv,el2yEimZptl2+y

the simpler
2t=3

inequalities

6

and

P

Z

E

Z

,t



exp


,

t2 2E Z



7

which hold for any t 0. 6 follows immediately from the inequality

and 7 is trivial when t
t 2 0; 1 ,

ht



2

t2 +

2t

;

t

0;

3

E Z and follows from 4 otherwise since, for every

h,t



t2 2

:

3

3 Con guration functions

In this and the next section, we show that Theorem 1 has many natural applications outside empirical process theory. More precisely, we apply Theorem 1 to random combinatorial quantities that were called con guration functions in 25, section 7 .

De nition 1 Assume that we have a sequence of spaces 1; 2; : : : and that we have a property P de ned over the union of nite products of spaces:  i1 i2    in with ij ij+1, that is, for any element xi1; : : : xin 2 i1 i2      in, we may
decide whether xi1; : : : xin satis es property P . Moreover assume that P is hereditary in the following sense: if xi1; : : : xin satis es P then so does any subsequence
xj1; : : : xjm of xi1; : : : xin where fj1 : : : jmg  fi1 : : : ing and jk is increasing. The
function fn that maps any tuple xi1; : : : xin to the size of the largest subsequence satisfying P is the con guration function associated with property P . Any subsequence of maximal length satisfying property P is called a witness.

When 1; : : : n is provided with a product probability measure, results about concentration around the median for con gurations functions were proved by Talagrand using the convex distance approach 25, 27 . Here we provide concentration
around the mean and slightly better constants. Moreover the proof is completely straightforward from Theorem 1.

Theorem 2 Let fn be a con guration function, and let Z = fnX1; : : : ; Xn, where

X1;

:

:

:

;

Xn

are

independent random
P ZE Z +t

variables. Then for



exp


, 2E

t2 Z+

an t 

t=3 :

0,

and

P

Z

E

Z

,t



exp


,

t2 2E Z


:

Proof. Let fx1; : : : ; xn = fnx1; : : : ; xn and gx1; : : : ; xn,1 = fn,1x1; : : : ; xn,1.

It su ces to show that f and g satisfy the conditions of Theorem 1. Condition 1

is trivially satis ed. On the other hand, let fxi1; : : : ; xikg fx1; : : : ; xng be a sub-

sequence of cardinality k witnessing the fact that fxi1; : : : ; xik = `. Note that
such a set exists. Observing that for any i  n such that xi 2= fxi1; : : : ; xikg,

fx1; : : : ; xn = gx1; : : : ; xi,1; xi+1; : : : ; xn, we see that 2 is also satis ed, which

concludes the proof.

2

4

To illustrate the fact that con guration functions are rather natural objects, let us describe three of them originating from di erent elds.

1. Increasing subsequences. Consider a vector x = x1; : : : ; xn of n di erent
numbers in 0; 1 . The positive integers i1 i2    im form an increasing subsequence if xi1 xi2    xim where i1  1 and im  n. Let Lx denote the
length of the longest increasing subsequence. fnx = Lx is a clearly a con guration function taking i = 0; 1 for all i, and therefore Z = LX1; : : : ; Xn satis es the inequalities of Theorem 2, where the Xi's are independent uniform random variables on 0; 1 . This improves the constants of the inequalities obtained and Talagrand 25
for the same random variable. See also Frieze 8 for early work on the concentration on LX.

2. Independent sets in random graphs. In the Gn; p model for random

graphs, the random graph G = V; E with vertex set V jV j = n and edge set E

is generated by starting from the complete graph with n vertices and deleting each

edge independently from the others with probability 1 , p. A subset of vertices A is

independent in G if and only if no two vertices from A are adjacent in G. Independence

is an hereditary property. The size of the largest independent set is the independence

number of the graph and it is denoted by G.

To show that the independence number can be regarded as a con guration func-

tion, we merely have to show that the Gn; p model may be regarded as a product

probability space. This is well known see, e.g., 22 : the ith component of the prob-

ability space just de nes the set of edges between vertex i and vertices with index

j i. Thus, Z = G satis es the inequalities of Theorem 2, regardless of the values

of n and p.

Results concerning the average value of G and concentration of G around

it have been known for a while for both sparse p = d=n with d constant and dense

p constant random graphs. It is well-known that the independence number of dense

random graphs is nearly deterministic see Bollobas 3 . In this case Theorem 2 does

not provide anything new. On the other hand, in the sparse case Frieze 7 proved,

that for any 0, for su ciently large d and n:

j

G

,

2n d

,

log

d

,

log

log

d

,

log

2

+

1j



n d

8

with probability going to 1 as n ! 1. Frieze uses the method of bounded-di erences,

but gives no explicit concentration inequality for the independence number. It does

not seem obvious to get sharp concentration inequalities independent of d, from the

construction presented in 7 . Such issues are handled in an e ortless way by viewing

the independence number as a con guration function.

5

3. vc dimension. Let A be an arbitrary collection of subsets of X , and let x = x1; : : : ; xn be a vector of n points of X . De ne the trace of A on x by
trx = fA fx1; : : : ; xng : A 2 Ag :
The shatter coe cient, or Vapnik-Chervonenkis growth function of A in x is T x = jtrxj, the size of the trace. T x is the number of di erent subsets of the n-point set fx1; : : : ; xng generated by intersecting it with elements of A. A subset fxi1; : : : ; xikg of fx1; : : : ; xng is said to be shattered if 2k = T xi1; : : : ; xik. The vc dimension Dx of A with respect to x is the cardinality k of the largest shattered subset of
x. From the de nition, it is obvious that fnx = Dx is a con guration function, and therefore satis es the conditions of Theorem 2.
Remarks. 1. To illustrate that the constants of Theorem 2 are optimal, consider the
vc-dimension and the following example: let X = X1; : : : ; Xn be a vector of i.i.d. random variables taking values in the set of nonnegative integers. Let the common
distribution of the Xi's be such that PfX1 = 0g = 1 , c=n for some positive constant c, and PfX1 = ig = c=n3 for i = 1; : : : ; n2. Then it is easy to see that, for large n,
DX is approximately distributed as a Poissonc random variable, so the optimality of the bounds are seen as in Remark 2 following Theorem 1 above.
2. It is likely that Theorem 2 does not provide the sharpest possible answer for the longest increasing subsequence problem, and it does not provide the right answer for the independence number in dense random graphs 2, chapter XI . The longest increasing subsequence has already been commented by Talagrand in 27 : empirical evidence suggests that the longest increasing subsequence is more concentrated than suggested by Theorem 2.
4 Combinatorial entropies
The analysis of combinatorial optipization problems has been often complemented by the analysis of counting versions see, e.g., 12 : rather than determining the largest independent set or increasing subsequence one may be interested in estimating the number Ix of independent sets or the number Nx of increasing subsequences. In statistical pattern recognition, the shatter coe cient T x is of primary interest.
The next result shows sharp concentration of combinatorial entropies and particularly of the vc entropy log-shattering coe cient. These bounds are completely new, we do not know whether any of the previously known concentration inequalities may be used to derive similar bounds. The constants are again optimal by the same example as above.
6

A combinatorial entropy is de ned as follows:

De nition 2 let x = x1; : : : ; xn be an n-vector of elements of X to which we associate a set Trx Yn of n-vectors whose components are elements of a possibly

di erent set Y. We assume that for each x 2 X n and i  n, the set Trxi =

Trx1; : : : ; xi,1; xi+1; : : : ; xn is the the projection of Trx along the ith coordinate,

that is,

Trxi

=

n
yi

=

y1;

:

:

:

;

yi,1;

yi+1;

:

:

:

;

yn

2

Y on,1 :

9yi 2 Y such that y1; : : : ; yn 2 Trx :

9 10

The associated combinatorial entropy is Hx = logb j Trxj where b is an arbitrary
positive number.

Remark. The logarithm of the number of subsequences that satisfy an hereditary
property is obviously a combinatorial entropy. The key property of combinatorial entropies is that they all satisfy condition 2
of Theorem 1:

Lemma 1 Let Hx be a combinatorial entropy. Then for any x 2 X n,
Xn ,Hx , H xi  Hx:
i=1

Proof.
Y is

Recall that the Shannon entropy of base b of a discrete random variable
hbY  = , X PfY = yg logb PfY = yg; y

where the sum is taken over all possible values of Y . The key of our proof is the

following inequality of Han 9 see also Cover and Thomas 4, page 491 : for any n

discrete random variables Y1; : : : ; Yn,

hbY1;

:

:

:

;

Yn



n

1
,

1

Xn
i=1

hbY1;

:

:

:

;

Yi,1;

Yi+1;

:

:

:

;

Yn:

Now we are prepared to prove the lemma. Consider the uniform distribution over the
set Trx. This de nes a random vector vector Y = Y1; : : : ; Yn 2 Yn. Then clearly,

Hx = logb j Trxj = hbY1; : : : ; Yn:

7

Since the uniform distribution maximizes the Shannon entropy, we also have, for all
i  n, that

Hxi  hbY1; : : : ; Yi,1; Yi+1; : : : ; Yn:

The statement now follows from Han's inequality.

2

Remark. The relationship between isoperimetrical issues and concentration of
measure has been underlined many times: concentration-of-measure results may be
used to prove or replace isoperimetric inequalities see for example Ledoux 13 . Here, isoperimetry comes at the rescue of concentration: Han's inequality may be viewed as a weak but general isoperimetric inequality. A geometric version of it had been
known for decades before they were formulated in the language of information theory, see Loomis and Whitney 15 .

Remark. Let us notice that Han's inequality and the tensorization inequality
for entropies inequality 15 in the Appendix that play a key role in the proof of
theorem 1 are both consequences of the subadditivity of the Shannon entropy and the fact that the Shannon entropy decreases with conditioning. Moreover it is easy to check that in the discrete case, Han's inequality and the tensorization inequality
15 can be derived from each other.

Theorem 3 Assume that Hx = logb j Trxj is a combinatorial entropy such that for all x 2 X n and i  n,

Hx , Hxi  1:

If X = X1; : : : ; Xn is a vector of n independent random variables taking values in

X , then the random combinatorial entropy H = HX satis es

P

HE

H

+t


 exp ,2E

H

t2 +

2t=3



;

and

P

H

E

H

,t



exp


,

t2 2E H


:

Moreover,

E

logb j TrXj

 logb E

j TrXj



1 ln 2

E

logb j TrXj

:

11

8

Remark. Borrowing the terminology of statistical physics, we may call logb E j TrXj
the annealed combinatorial entropy. This quantity is often much easier to handle than the expected combinatorial entropy. 11 shows that the two quantities are always closely linked together.

Proof. The rst two inequalities follow from a straighforward combination of

Lemma 1 with Theorem 1. The rst inequality of 11 is an obvious consequence of

Jensen's inequality. As logb j TrXj satis es the conditions of Theorem 1, we may
use 18 in the Appendix and nd that, for all  0,

e   e :j j, j j X XE b blog Tr  E log Tr  

e,,1 E logb j jXTr 

The choice  = log b yields the desired result.

2

Next we discuss some of the applications of Theorem 3.
1. vc entropies. The vc entropy is de ned as Hx = log2 T x, where T x is the shatter coe cient de ned in the previous section. The vc entropy is a simple example of a combinatorial entropy. It may be generalized to a class of functions with a nite
range. More precisely, let k 1 be a positive integer, and let F be a class of functions X ! f1; : : : ; kg. Given a vector x = x1; : : : ; xn 2 X n, de ne Trx f1; : : : ; kgn as the set of all di erent n-vectors fx1; : : : ; fxn with f 2 F. Then it is immediate to see that Hkx = logk j Trxj is a combinatorial entropy satisfying the condition
of Theorem 3. The case k = 2 i.e., the case of the vc entropy Hx is of particular interest, as
it plays a key role in some applications in pattern recognition and machine learning see, e.g., 6 , 28 . In this case we obtain the following:

Corollary 1 For any class of sets A and for all t 0, the random vc entropy

satis es

P

HE

H

+t


 exp ,2E

H

t2 +

2t=3



:

and

P

H

E

H

,t



exp


,

t2 2E H


:

Also,

E

log2 T X

 log2 E

T X



1 ln 2E

log2 T X

:

9

In 28 , Vapnik considers the limit of the average vc-entropy rate E log2 T X =n and the limit of the annealed vc-entropy rate log2 E T X =n as criteria for consistency and fast convergence of an inference rule called Empirical Risk Minimization. The last statement of the corollary shows that either these two quantities converge to zero or none of them. This answers, in a positive way, an open question raised by Vapnik 28, pages 53 54 : the empirical risk minimization procedure is non-trivially consistent and rapidly convergent if and only if the annealed entropy rate 1=n log E T X converges to zero. For the de nitions and discussion we refer to 28 . 2. Increasing subsequences. Recall the setup of the rst example of Section 3, and let Nx denote the number of di erent increasing subsequences of x. Observe
that log2 Nx is a combinatorial entropy. This is easy to see by considering Y = f0; 1g, and by assigning, to each increasing subsequence i1 i2    im of x,
a binary n-vector y = y1; : : : ; yn such that yj = 1 if and only if j = ik for some k = 1; : : : ; m i.e., the indices appearing in the increasing sequence are marked by 1. Now condition 9 as well as the condition of Theorem 3 are obviously met, and therefore HX = log2 NX satis es all three inequalities of Theorem 3. This result signi cantly improves a concentration inequality obtained by Frieze 8 for log2 NX. 3. Independent sets in random graphs. The logarithm of the number of independent sets was considered by Zuckerman 29 . This logarithm can also be regarded as a combinatorial entropy: Tr is the set of bitvectors of length n, such that the vertices corresponding to coordinates equal to 1 form an independent set.
10

5 Learning and model selection

In this section we point out some immediate applications of the results of Section 3 to some problems emerging in learning theory and pattern recognition. The results presented here are not necessarily optimal, they merely intend to illustrate how some of the new concentration inequalities may be applied in a virtually e ortless manner to re-prove and improve some previously obtained results in statistical learning theory.
Let the data Dn = X1; Y1; : : : ; Xn; Yn consist of independent, identically
distributed copies of the random variable pair X; Y  taking values in Rd f0; 1g. The goal of pattern classi cation see 6 , 28  is to construct a function fn : Rd ! f0; 1g that mimimizes Lfn where the loss functional L is de ned by

Lf = P fX =6 Y

for all f : Rd ! f0; 1g. The empirical loss of such a function is de ned simply by

b XLfn

=

1 n

n i=1

IffnXi6=Yig;

where IA denotes the indicator function of an event A. Often the data are used to
select a set An Rd from a given class A of subsets of Rd, and the classi er is de ned
as

fnx = Ifx2Ang:

The following theorem shows how the loss of such a classi er may be bounded by some purely empirical quantities. It involves the random shatter coe cient of the
class A de ned in Section 3. Introduce the notation X1n = X1; : : : ; Xn.

Theorem 4 Let A be an arbitrary class of subsets of Rd, and let fn be de ned as

above. Then for any 0, the probability that

Lfn



Lbfn

+

r

6

log

T X1n n

+

s

5

log n

2

is greater than 1 , .

11

Proof.

For

any t" 0, and u 0, r P Lfn , Lbfn 6 "  P Lfn , Lbfn "  P Lfn , Lbfn

lorrg Tn36ElXolg1ongTnT+XnrX1n1n6n+t ++6tuu+


u

+P E log T X1n 2 log T X1n + 2t :

The rst term on the right-hand side may be bounded by recalling the Vapnik-

Chervonenkis

inequality see
P sup

28, Theorem
Lf , Lbf

f =IA :A2A

3.1

:


E

T

X12n e,n

2;

which is true for any

0.

Now

it

is

easy

to

see

that

for

any

x2n 1

=

x1; : : :

; x2n,

T x21n  T x1nT x2nn+1, so by independence we have log E T X12n  2 log E T X1n.

Therefore,

P

"r Lfn , Lbfn 3E
"  P Lfn , Lbfn

log T X1n
rn
2 log E T n


+u
X1n 

+


u



b"y inequality 11 P Lfn , Lbfn

frorm

Theorem 3 log E T X12n
n

+


u

by the argument above

 e,nu2;

where at the last step we used the Vapnik-Chervonenkis inequality.

To bound the second term, we use the lower-tail inequality of the concentration

result

Theorem 3 for the vc entropy P E log T X1n 2 log T X1n

log T + 2t

X1n:
 exp


,

,

1 2

E log T X1n + 2E log T X1n

t2

!

 e,t=2:

Summarizin"g, we have P Lfn , Lbfn

r

6

log

T X1n n

+

r

6t n

+


u



e,nu2

+

e,t=2:

12

q
Choosing u = n1 log 2 and t = 2 log 2 concludes the proof.

2

The above result is important because the unknown loss may be controlled by

a purely empirical quantity. Such a result is useful in automatic model selection.

Assume now class there is

that a a classi

seerqufbkenwcehicohf

classes chooses

of sets A1; A2; : : : is given, its hypothesis from class Ak

and i.e.,

ffobkrxeac=h

Ifx2Akg for some Ak 2 Ak. Then it is immediate from the above Theorem that with

probability greater than 1 , , simultaneously for all k  1,

Lfbk

Lbfbk

+

r

6

log

TkX1n n

+

s
6 log

2

+2 n

log

k

;

where TkX1n is the random shatter coe cient of class Ak. This suggests a model
selection rule based on minimizing the empirical quantity on the right-hand side. Note
that the right-hand side is the empirical loss penalized by a data-dependent penalty, involving the random vc entropy of the k-th class. In particular, the following result
is an immediate consequence.

Theorem 5 Let fb1; fb2; : : : be a sequence of classi ers de ned as above. Assume that

the

classi

er

fn

is

selected among these by minimizing the quantity

Lbfbk

+

r

6

log

TkX1n n

+

s
6

log

2

+2 n

log

k

over all k = 1; 2; : : : . Then with probability greater than 1 , ,

Lfn



inkf

0 @Lbfbk



+

r

6

log

TkX1n n

+

s
6

log

2

+2 n

log

k

1 A

:

Similar results, but with signi cantly more involved proofs were shown by ShaweTaylor, Bartlett, Willamson, and Anthony 24 and Lugosi and Nobel 16 . For discussion on the signi cance of these results and related work we refer to these papers.

13

Appendix: Proof of Theorem 1

Here we prove the following, stronger, version of Theorem 1:

Theorem 6 . Let fXi; i 2 Ig be some nite family of independent random variables. De ne X = Xjj2I and for every i 2 I, let Xi = Xjj2Infig. Let Z = 
 X, be
some nonnegative and bounded measurable function of X. Assume that for every
i 2 I there exists some measurable function Zi of Xi such that

0  Z , Zi  1:

12

Assume furthermore that

X ,Z , Zi  Z:
i2I

13

De ning h as h u = 1 + u log 1 + u , u, for u  ,1, the following inequalities

hold. For every positive number t

P

ZE

Z

+t


 exp ,E

Z


h
E

t Z



and for every positive number t  E Z

P

ZE

Z

,t


 exp ,E

Z


h, E

t Z


:

To prove Theorem 6, we need a modi cation of an information inequality for functionals of independent variables presented in Massart 20 see Lemma 2.3 therein.

Lemma 2
with values

Let I be
in some

some nite measurable

set. Let, space i

for every i and de ne

2

I,
=

QXi be j2I

sjo,meira=ndQomj2Ivnafriigablje.

Let 
 be some real valued measurable function on and for every i 2 I, 
i be

some real for every
fXi; i 2 I

valued
i2I g to be

measurable function on .i Xi = Xjj2Infig, Zi = 
 independent and the Laplace

, De ne X
X .i i transform

=
If 

Xjj2I , Z = 

!weEasesuZmetothbee

X and variables nite on

some non empty open interval I then, for any  2 I

E ZeZ , E eZ log E eZ  X E eZ ,,Z , Zi ;

14

i2I

where denotes the function z ! exp z , z , 1.

14

Proof. As the proof is exactly the same as that of Lemma 2.3 in Massart 20 , we just give a sketch. For every i 2 I, we denote by Ei the expectation operator
conditionally on X .i Then, introducing t = t log t, the tensorization inequality

for entropy see Ledoux 14 , yields, for any nonnegative function g on

G = gX

satis
E

es E G jlog Gj 1,  G ,  E G   E

"X E i

 G

,,  E i

G  :

i2I

such that 15

Then, for every every i 2 I, the variational de nition of entropy 11 asserts that for
every positive measurable function Gi of Xi, one has
Ei  G ,,  Ei G   E i G ,log G , log Gi , ,G , Gi :

Applying the above inequality to the variables G = eZ and Gi = eZi, one gets
E i  G ,,  E i G   E i eZ ,Z , Zi

which, via 15, leads to 14.

2

Proof of Theorem 6. We apply Lemma 2 so that inequality 14 holds for any

. Since the function is convex with

,Z,,uZi

u ,. Hence it follows from , and therefore we derive

0 12 from

= 0, that 14

for any for every and 13

,and,an,yZu,2Z
that

i0; 1

,


E ZeZ , E eZ log E eZ



"
E

,eZ X ,Z , Zi

 ,E ZeZi2I:

Introduce Ze = Z , E

Z

and de

hi
ne, for any , F  = E eZe .

Setting v = E

Z,

the above inequality becomes

,

,

F 0  F 

,

log F





v

, ;

16

which in turn implies
,1 , e, 0  ,    v , with   = log F  :

eNqouwatoiobnse,r1ve,

teh,at

the function 0
0  ,   =

= v

v is
,.

a solution We want to

of the ordinary
show that  

di 0.

erential In fact,

if 1 =  , 0, then

,1 , e, 01  , 1   0:

17

15

Hence, de ning f = log ,e , 1 and g = e,f1 , we have ,1 , e, f0g + g0 , g  0;

which yields since f01 , e, = 1
,1 , e, g0  0:

Hence g0 is nonnegative on ,1; 0 and nonpositive on nondecreasing on ,1; 0 and nonincreasing on 0; 1.

0; 1 and
Now, since

tZeheirsefcoernetegreids

01 0 = 0. Using the fact that e,f tends to 1 as  goes to 0, we conclude that
g tends to 0 as  goes to 0. This shows that g is nonpositive, therefore   0

and we have proved that

log

E

eZ,E

Z






v

 for every  2 R:

18

Then by Markov's inequality

P Z , E Z  t  exp , sup t , v  0

and  
P Z , E Z  ,t  exp , sup ,t , v  : 0

The proof can be completed by using the easy-to-check and well-known relations:

sup 0 t , v  = vh t=v for every t 0 and sup 0 ,t , v  = vh ,t=v

for every 0 t  v.

2

16

References
1 R. Ahlswede, P. Gacs, and J. Korner. Bounds on conditional probabilities with applications in multi-user communication. Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte gebiete, 34:157 177, 1976. correction in 39:353 354,1977.
2 B. Bollobas. Random Graphs. Academic Press, Orlando, 1985.
3 B. Bollobas. Combinatorics. Cambridge University Press, Cambridge, 1986.
4 T.M. Cover and J.A. Thomas. Elements of Information Theory. John Wiley, New York, 1991.
5 A. Dembo. Information inequalities and concentration of measure. Annals of Probability, 24, 1996.
6 L. Devroye, L. Gyor , and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer-Verlag, New York, 1996.
7 A.M. Frieze. On the independence number of random graphs. Discrete Mathematics, 81:171 175, 1990.
8 A.M. Frieze. On the length of the longest monotone subsequence in a random permutation. Annals of Applied Probability, 1:301 305, 1991.
9 T.S. Han. Non negative entropy measures of multivariate symmetric correlations. Information and Control, 36:133 156, 1978.
10 D. Haussler, N. Littlestone, and M. Warmuth. Predicting f0; 1g-functions on randomly drawn points. Information and Computation, 115:248 292, 1994.
11 R. Holley and D. Stroock. Logarithmic Sobolev Inequalities and stochastic Ising models. J. Statist. Phys.,46 1987, 1159 1194.
12 D.S. Johnson. A catalog of complexity classes. in Handbook of Theoretical Computer Science, A:67 162, MIT Press, Boston, 1990.
13 M. Ledoux. Isoperimetry and Gaussian analysis. In P. Bernard, editor, Lectures on Probability Theory and Statistics, pages 165 294. Ecole d'Ete de Probabilites de StFlour XXIV-1994, 1996.
14 M. Ledoux. On Talagrand's deviation inequalities for product measures. ESAIM: Probability and Statistics, 1:63 87, 1996. http: .www.emath.fr ps
15 L.H. Loomis and H. Whitney. An inequality related to the isoperimetric inequality. Bull. Amer. Math. Soc. 55:961-962, 1949.
17

16 G. Lugosi and A. Nobel. Adaptive model selection using empirical complexities. under revision, 1996.
17 K. Marton. A simple proof of the blowing-up lemma. IEEE Transactions on Information Theory, 32:445 446, 1986.
18 K. Marton. Bounding d-distance by informational divergence: a way to prove measure concentration. Annals of Probability, 24:857 866, 1996.
19 K. Marton. A measure concentration inequality for contracting Markov chains. Geometric and Functional Analysis, 6:556 571, 1996. Erratum: 7:609 613, 1997.
20 P. Massart. About the constants in Talagrand's concentration inequalities for empirical processes. Annals of Probability, page to appear, 1998.
21 P. Massart. Optimal constants for Hoe ding type inequalities. Technical report, Mathematiques, Universite de Paris-Sud, Report 98.86, 1998.
22 C. McDiarmid. On the method of bounded di erences. In Surveys in Combinatorics 1989, pages 148 188. Cambridge University Press, Cambridge, 1989.
23 C. McDiarmid. Concentration. In M. Habib, C. McDiarmid, J. Ramirez-Alfonsin, and B. Reed, editors, Probabilistic Methods for Algorithmic Discrete Mathematics, pages 195 248. Springer, New York, 1997.
24 J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and M. Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44:1926 1940, 1998.
25 M. Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications Mathematiques de l'I.H.E.S., 81:73 205, 1995.
26 M. Talagrand. New concentration inequalities in product spaces. Invent. Math., 126:505 563, 1996.
27 M. Talagrand. A new look at independence. Annals of Probability, 24:1 34, 1996. Special Invited Paper.
28 V.N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995.
29 D. Zuckerman. Every np-complete problem has a hard version. Proceedings of eighth IEEE Structure in Complexity Theory Conference, 1993, 305 312.
18

