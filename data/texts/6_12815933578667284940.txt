Support Vector Machine Classi cation of Microarray Gene Expression Data
William Noble Grundy Department of Computer Science University of California, Santa Cruz
http: www.cse.ucsc.edu bgrundy

Outline
Introduction DNA microarray expression data Support vector machines Results Future work

Genomics
Genomics is the convergence of two great innovations of the latter half of the twentieth century: computers and biotechnology. The post-genomic era will give us our rst comprehensive view of the cell, of the tree of life, and of human disease at the molecular level.

Genome-wide data sets
A variety of genome-wide data sets are available, including
Expressed Sequence Tag EST libraries; full genomic sequences, including all genes that are 1. paralogs within the same organism, 2. orthologs from closely related organisms, and 3. remote functional and structural homologs from dis-
tantly related organisms; Single Nucleotide Polymorphism SNP databases, representing the natural variation within a population; mRNA expression levels obtained from microarray technology; protein expression levels from mass spectrometry technology; measurements of protein-protein and protein-DNA interactions, and a host of new methods to alter the expression of speci c genes.

Two tasks
Given megabases of newly sequenced DNA per day, along with EST hits", protein database hits", and di erential mRNA expression data,
nd the protein coding genes in this DNA, identifying exons, promoter, regulatory binding sites, and other key features, and make an initial classi cation and analysis of the protein sequences derived from this DNA.

DNA microarray hybridization experiments
Each location in the array contains a DNA sample xed to the glass slide. A single experiment consists of hybridizing the microarray with two di erently dyed cDNA samples from di erent experimental conditions.

Gene expression matrix
Experiments 1 2 ... j ... 79 1 2 Genes ... i ... 2467
The matrix entry at row i, column j is the logarithm of the ratio of gene i's expression level in experiment j to gene i's expression level in the reference state. We use data from 2467 S. cerevisiae genes in 79 experimental conditions, including the cell division cycle, sporulation, heat shock, reducing shock, cold shock, and diauxic shift http: rana.stanford.edu clustering.

Functional similarities

Log expression ratio

3

2

1

0

-1

-2

-3

-4

-5 alpha

elu cdc

spo he re co di

Microarray experiment

Cytoplasmic ribosomal proteins

Expression pro les of functionally related genes are similar to one another.

Unsupervised learning
Previous work has employed clustering techniques to group functionally related genes.
Each technique begins with a de nition of similarity. Eisen et al. use hierarchical clustering. Tamayo et al. use self-organizing maps.

Supervised learning
Support vector machines SVMs are a supervised learning technique developed by Vapnik and others. Training an SVM requires labeled training data. A trained SVM can answer two questions:
What other genes are related to my set? Does my set contain genes that do not belong?

Functional classes
In this work, each gene is classi ed according to ve different MIPS Yeast Genome Database functional classes:
tricarboxylic-acid pathway respiration chain complexes cytoplasmic ribosomes proteasome histones These classes have been pointed out by Eisen et al. in their discussion.

Separating hyperplane

--
--
-
-
-
--

-

-
-
-

-

-

-

-
+

-

-

-
+ + +

-

----
++ + +
++

- --

++

- ++

+ +

+

+ +

+

++

++

+ +

+ +

+

+ +++

+ + +

++ +

+ ++

+

++ ++

+ +

+

+

++

+ +

++ + +

++

+ + + ++

+ +

+ +
++++

+ + +

++ +

Each vector in the gene expression matrix may be

thought of as a point in a 79-dimensional space.

A simple way to build a binary classi er is to construct a hyperplane separating class members from non-members in this space.

This is the approach taken by perceptrons, also known as single-layer neural networks.

Support vector machines
Support vectors machines SVMs rst map the data from the input space to a higher-dimensional feature space. A separating hyperplane is found in the feature space. The feature space is not represented explicitly, in order to avoid computational overhead.

Maximum margin hyperplane

Positives

+
+ + _

Possible Hyperplanes

_

+ Maximum Margin Hyperplane _
_ Negatives

When multiple separating hyperplanes exist, the SVM uses the hyperplane that maximally separates positive from negative examples. The use of the maximum margin hyperplane helps the SVM avoid over tting.

Soft margin

011010000111101110111101100110101100010101010101100110101100010110100110011101000011110101110101101110001101000000000001001000101111001001100001010101111010100001011011001100100011111001011010000111001110011000111001101001001011010010111100001111000000101001010001001100110001111101111100011100001011110000001011000110110101101110111011001011000111010100011001100110110010100111111111001010110110101001010100101010011010101010110100101001010010100110100101001101001010010100101001101001011101001111001100101101011010001011100110110100100111000110001100111001010101010110100011001000111100110010010001110001100111100111100001001111000110001100110111000110010010111101011000100110101110010010011000010111001001101001010110010111110000101111111111111111111111111111111111111111111111111100000000000000000000000000000000000000100000000000000100000000111111111001000111001111100110101111010010010010100011001011100100100011001001111001100110001110000111110001110000111100000100111111111110101001011000011010010001100010001101011110011100110001001100110100000010110110001011101001110000000110111010111101110010110010010011100101011110110100110100101101011011000110101100001011001011001010011001001011000110001010101110100110011100110101111010000100011101001101101010101010011110110000010001110101001010110011100011010101110010101010011000110110000100011111001111100100001010101010101000110010101011111110100100011001001001111000011110111100011000010101101011000010010101001001010101010010100101001101001010010101010100101100101100101001010101001101010101010101101101010010101011111111010101011000100011110110011010011100001011000111010100100011010100101010011001110111010110110110101001011000110110000111010001110010110011110100111000110101001101100010110000001011110001100111101100011110110000011011110000001011010000100110111110010010111011100001010100101010000101111000000000000000000000000000000000000000000000000000000000000011111111111111111111111111111111111111111111111111111111111111110010010001011100100110010011101000111001010101100110100110010111001010011111011100100000+0011000110101111111111010110101101100000000000+0111001100001110011110101000100011010010110011+++1011000011100101110110010101110101001100000011+0110001100110101000000111101111100111000001101+ 1110000000111111001000011101110100000001111101++1001000111001111000100011100011100011001111100+1110000001110101010011111110000011100110110000+111100010010010101011100111000101110010101010001010110001011011111111111110000000000000001111010010001011100011111111111110010100000000100001000111110010110001101100000101110011101011001001100010111100000000000000001111111111111111000100111001111001111101000001010100000011111- -10001011110001--00101111010001---

-

-
-
-

-
-

When no separating hyperplane exists, the SVM uses a soft margin hyperplane with minimal cost.

The kernel function
Training an SVM requires nding the separating hyperplane for a given labeled training set. Classifying an unlabeled test example requires locating the given data point in the feature space. Both training and classi cation can be stated entirely in terms of vectors in the input space and dot products in the feature space. The kernel function plays the role of the dot product in the feature space. Any continuous, positive semi-de nite function can act as a kernel function. The use of a kernel function allows us to avoid representing the feature space explicitly.

Linear decision boundary
K X; Y  = X  Y
Using a simple dot product as the kernel creates a linear decision boundary in the input space. This kernel, with normalization, corresponds to the distance metric used by Eisen et al. to perform hierarchical clustering.

Polynomial decision boundary
K X; Y  = X  Y  + 13
Raising the dot product to a power yields a polynomial decision boundary in the input space.

Gaussian decision boundary

K X;

Y



=

exp

0 @

,jjX

,Y

jj2

1 A

22

A radial basis kernel function yields a Gaussian decision boundary in the input space.

SVM summary
A support vector machine nds a nonlinear decision function in the input space. This boundary corresponds to a hyperplane in a higherdimensional feature space. The computational complexity of the classi cation operation does not depend on the dimensionality of the feature space. Over tting is avoided by controlling the margin. The hyperplane is represented sparsely as a linear combination of points. The SVM automatically identi es a subset of informative points and uses them to represent the solution. The training algorithm solves a simple convex optimization problem.

Other supervised learning techniques
Decision trees Each tree is binary, with simple classi ers at internal nodes and a classi cation at each leaf. The standard algorithm of this kind is C4.5 Quinlan 1997. MOC1 Wu et al. 1999 is a variant motivated by VC learning theory.
Parzen windows is a generalization of k-nearest neighbor techniques that employs a radial basis function as the distance metric.
Fisher's linear discriminant projects high-dimensional data onto a line and performs classi cation in this one-dimensional space.

Consistently misclassi ed genes
Family Gene Locus Error Description TCA YPR001W CIT3 FN mitochondrial citrate synthase
YOR142W LSC1 FN subunit of succinyl-CoA ligase YNR001C CIT1 FN mitochondrial citrate synthase YLR174W IDP2 FN isocitrate dehydrogenase YIL125W KGD1 FN -ketoglutarate dehydrogenase YDR148C KGD2 FN component of -ketoglutarate dehydro-
genase complex in mitochondria YDL066W IDP1 FN mitochondrial form of isocitrate
dehydrogenase YBL015W ACH1 FP acetyl CoA hydrolase Resp YPR191W QCR2 FN ubiquinol cytochrome-c reductase
core protein 2 YPL271W ATP15 FN ATP synthase epsilon subunit YPL262W FUM1 FP fumarase YML120C NDI1 FP mitochondrial NADH ubiquinone 6
oxidoreductase YKL085W MDH1 FP mitochondrial malate dehydrogenase YDL067C COX9 FN subunit VIIa of cytochrome c oxidase Ribo YPL037C EGD1 FP subunit of the nascent-polypeptide-
associated complex YLR406C RPL31B FN ribosomal protein L31B YLR075W RPL10 FP ribosomal protein L10 YAL003W EFB1 FP translation elongation factor EF-1 Prot YHR027C RPN1 FN subunit of 26S proteasome YGR270W YTA7 FN member of CDC48 PAS1 SEC18 family
of ATPases YGR048W UFD1 FP ubiquitin fusion degradation protein YDR069C DOA4 FN ubiquitin isopeptidase YDL020C RPN4 FN involved in ubiquitin degradation pthwy Hist YOL012C HTA3 FN histone-related protein YKL049C CSE4 FN required for proper kinetochore function

An error in the training labels
3 YLR075W
2 1 0 -1 -2 -3 -4 -5
YLR075W is associated with the ribosome both by homology and puri cation.

alpha elux cdc spo hessatme dtt cold diau

Fuzzy boundaries between classes

2
Tricarboxylic-acid pathway Respiration chain complexes 1.5

1

Log expression ratio

0.5

0

-0.5

-1

-1.5 alpha

elu cdc

spo he re co di

Microarray experiment

The tricarboxylic-acid pathway and the respiration chain complexes are tightly coupled in the production of ATP.

Variation in regulation

Log expression ratio

2.5 2
1.5 1
0.5 0
-0.5 -1
-1.5 -2 alpha

elu cdc

spo he re co di

Microarray experiment

YDL020C

YDL020C and YDR069C are auxiliary factors of the proteasome that may be regulated speci cally during sporulation.

Training set outliers
Gene Weight Errors YLR075W 2.093 5 YOR276W 1.016 4 YNL209W 0.977 4 YAL003W 0.930 5 YPL037C 0.833 5 YKR059W 0.815 2 YML106W 0.791 1 YDR385W 0.771 2 YPR187W 0.767 1 YJL138C 0.757 3
The magnitude of the training set weights predicts outliers.
The Weight" column lists the average learned weight of the gene over ve di erent three-fold splits of the data. The Errors" column lists the total number of times the gene was incorrectly classi ed. The table lists the negative genes with the largest weights.

Conclusions
SVMs provide an accurate means of functionally classifying genes using only expression data. The radial basis SVM appears to provide better performance than SVMs using the dot product kernel. If the complete data set were available, our trained SVMs could be used to predict functional classes of unannotated genes. The method is scalable, which is essential, given the amount of data becoming available.

Future work
Incorporate data from other sources, such as protein features and promoter region features.
Design better kernel functions. The kernel function incorporates prior knowledge of the learning domain. The kernel function could model dependencies among experiments.
Employ SVMs in an iterative framework. SVMs are members of the larger class of kernel methods. Use an unsupervised method as the rst step. Use the SVM to clean a given classi cation.

Collaborators
Michael Brown David Lin Nello Cristianini Charles Sugnet Manuel Ares, Jr. David Haussler

