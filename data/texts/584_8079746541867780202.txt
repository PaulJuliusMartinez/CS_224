From: AAAI Technical Report SS-96-05. Compilation copyright © 1996, AAAI (www.aaai.org). All rights reserved.

Learning Rules that Classify E-Mail
William W. Cohen
AT&TLaboratories 600 Mountain Avenue Murray Hill, NJ 07974
(908)-582-2092 wcohen~research.att.com

Abstract
Twomethodsfor learning text classifiers are compared on classification problemsthat might arise in filtering andfiling personMe-mail messages:a "traxiitionM IR" method based on TF-IDFweighting, and a newmethodfor learning sets of "keyword-spotting rules" based on the RIPPERrule learning algorithm. It is demonstratedthat both methodsobtain significant generalizations froma small numberof examples; that both methodsare comparablein generalization performanceon problemsof this type; and that both methodasxe reasonablyefficient, evenwith fairly large training sets. Howevert,he greater comprehensibility of the rules maybe advantageous in a system that allows users to extendor otherwise modifya learned classifier.
Introduction
Perhaps the most-discussed technical phenomenon of recent years has been the rapid growth of the Internet---or more generally, the rapid growth in the numberof on-line documents. This has led to increased interest in intelligent methodsfor filtering and categorizing documents. For example, there is nowa great deal of interest in systemsthat allow a technically naive user to easily construct a personalized systemfor filtering and classifying documents such as e-mail, netnews articles, or Webpages.
This paper will compare methods for learning text classifiers, focusing on the kinds of classification problems that might arise in filtering and filing personal e-mail messages. A particular focus will be on learning plausible message categories from relatively small sets of labeled messages.
The goal in performing this study is to determine how "traditional" IR learning methods compare with systems that learn concise and easily-interpreted classifters; specifically, to comparetraditional IR methods with systems that learn sets of keyword-spotting rules.
In a "keyword-spotting" rule the primitive conditions test to see if a wordappears (or does not appear) in a certain field of a mail message. Anexample of a set of keyword-spotting rules is below.

cfp ~ "cfp" E subject, "95" E subject. cfp *--- "cfp" E subject, "1995" E subject. cfp *--- "call" E body, "papers" E body.
This rule set states that an e-mail messagewill be classifted as an instance of the class "cfp" (call for papers) if it contains the tokens "cfp" and "95" in the subject
field, or the tokens "cfp" and "1995" in the subject field, or if it contains the tokens "call" and "papers" tin the body of the message,
The motivation for learning keyword-spotting rules is based on first, a belief these rules will relatively easy for end users to understand and modify, and second, a suspicion that learning methods alone are not an adequate solution for categorization problemsof this type. It seems likely that instead somemix of automatically and manually constructed classifiers will be necessary to account for the fact that both the user's interests and the distribution of messages change (sometimes quite rapidly) over time. For instance, at the time of this writing, the ruleset above maybe accurate for messages I have received over the last few months; however, at some point it will certainly becomeappropriate to
modify it by replacing "95" with "96" and "1995" with "1996".
Although keyword-spotting rulesets have the advantage of comprehensibility, to my knowledge they have not been extensively evaluated on text categorization problems. It should be noted that these rulesets are quite different from the classifiers constructed by more commontext categorization learning methods, such as naive Bayes or term frequency/inverse document frequency weighting (TF-IDF) (Salton 1991). Rather than making decisions based on a weighted combination of all the words in a document, rules make decisions based on a small number of keywords. Also, keyword-spotting rules do not base classification decisions on word frequency, only on the presence or ab-
sence of a word. Onegoal of our evaluation is to determine howmuch
1The learning algorithm considered here doesnot assumethat keywordslike "cfp" or "1995" are drawn from somesmall set. Instead, any token appearing in any training exampleis a possible keyword.

18

accuracy (if any) is lost by using keyword-spotting rules, relative to other classifiers. A second goal is to determine how muchCPUtime is needed to learn accurate rulesets on moderate-sized sets of examples--in particular, whether it is reasonable to use rule learning as a componentof an interactive message-filtering system. A final goal is to gain some understanding of the numberof examples necessary to learn accurate classifiers.
Learning algorithms
Twotext categorization algorithms will be compared. The first uses TF-IDF weighting (Salton 1991). The implementation used here follows Ittner et al. (1995), who adapt Rocchio's relevance feedback algorithm (Rocchio 1971) to classification. Briefly, each document is represented as a vector, the components of which correspond to the words that appear in the training corpus. For a documentd, the value of the component for the word wi depends on the frequency of wi in d, the inverse frequency of wi in the corpus, and the length of d. Learning is done by adding up the vectors corresponding to the positive examples of a class C and subtracting the vectors corresponding to the negative examples of C, yielding a "prototypical vector" for class C. Documentvectors can then be ranked according to their distance to the prototype. A novel documentwill be classified as positive if this distance
is less than somethreshold tc, which can be chosen so as to balance recall and precision in someset manner. In our experiments, tc was chosen to minimize error on the training set.
The second algorithm is an extension of the rule learning algorithm RIPPER,which is described in detail elsewhere (Cohen 1995a). Briefly, RIPPERbuilds a ruleset by repeatedly adding rules to an emptyruleset until all positive examples are covered. Rules are formed by greedily adding conditions to the antecedent of a rule (starting with an empty antecedent) until no negative examples are covered. After a ruleset is constructed, a optimization postpass massages the ruleset so as to reduce its size and improveits fit to the training data. A combination of cross-validation and minimum-description length techniques are used to prevent overfitting. In previous experiments, RIPPERwas shown to be comparable to C4.5rules (Quinlan 1994) in terms of generalization accuracy, but much faster for large noisy datasets.
Before running these experiments, RIPPER was modified so as to be moreefficient for text categorization problems. In the initial implementation of RIPPER, examples were represented as feature vectors. This implementation could be used to learn keywordspotting rules; however, it wouldbe necessary to construct a boolean feature for each possible condition of the form "wi E field", and then to represent each document as a vector of these boolean features. This is rather inefficient since even a moderately large corpus

will contain hundreds or thousand of words.
One commonway of avoiding this problem is to restrict the vocabulary, for example by only considering frequent words or highly informative words. I chose instead to extend RIPPERto allow the value of an attribute to be a set of symbols(as well as single sym-
bolic value or a number). This meansthat a structured documentcan be easily and naturally represented. For example, an e-mail message is represented with four attributes, from, 1;o, subject, and body. The value of each attribute is the set of all wordsthat appear in the
corresponding section of the mail message.
The primitive tests on a set-valued attribute a (i. e., the tests which are allowed in rules) are of the form "wi E a" or "wi ~ field". Whenconstructing a rule, RIPPER finds the test that maximizes information gain for a set of examples S efficiently, makingonly one pass over S for each attribute. All words wl that appear as elements of attribute a for sometraining example are considered by RIPPER.
Using set-valued attributes allows one to represent a set of documentseasily and naturally. It is arguably more elegant and potentially more robust than using, say, entropy-based feature selection (Lewis and Ringuette 1994; Apt~ et ai. 1994) to derive a small of features. This representation also simplifies the preprocessing of examples--a worthwhile aim if one's eventual goal is integration of the learner into an interactive system. Set-valued attributes are discussed
in more detail elsewhere (Cohen 1996a). A second extension to RIPPER, also motivated by
text categorization problems, allows the user to specify a loss ratio (Lewis and Catlett 1994). A loss ratio indicates the ratio of the cost of a false negative to the cost of a false positive; the goal of learning is to minimize misclassification cost on unseen data. Loss
ratios in RIPPERare implemented by changing the weights given to false positive errors and false negative errors in the pruning and optimization stages of the learning algorithm.
Recall that the TF-IDFlearning algorithm can trade off recall for precision by makingan appropriate choice of its similarity threshold tv. By using an appropriate loss ratio RIPPERcan also make this trade-off. The experimental comparisons discussed below, however, focus on classifier error; unless otherwise stated a loss ratio of 1 wasused.
In all of the experiments described below, messages were parsed into a header and body. All words from the from, to, and subject fields were extracted from the header, and the first 100 wordswere extracted from the body. (A word is any maximal-length sequence of alphanumeric characters, normalized by converting uppercase to lower-case characters.) For RIPPER,examples were represented by four set-valued attributes. For TF-IDF, examples are represented by a set of tokens of the form f_w, wherew is a word and f is the field w appeared in; for instance the word call appearing in

19

1

0.9 ...............~. .

FOIL(re1) -*--FOIL (prop) ....

0.8 ~, .'".......

RIPPE-R-~....

g 0.7
.,4 0.6

C4.5 ¯ ..........I......P.rob-..*-.-

..4

0.5

0.4

0.3

0.2

0.1 i i , , i 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 reoa].l

Figure 1: AP problems with uncertainty samples

the subject field would be encoded as subject_call. Header fields other than "to", "from", and "subject" were discarded. Any random sample that contained no positive examples of the concept to be learned was discarded¯
The decision to limit the size of an example by using only the first 100 words in a message body was driven by efficiency considerations; absent this restriction, the occasional very long message (e.g., Postscript source for a journal paper) would make learning much more expensive¯ Using the first 100 words in the body of a documentis also a sort of feature selection since usually the first few sentences in an e-mail messageindicate the message's general content¯
Experiments
Preliminary experiments on text
The text version of RIPPERhas been tested on a number of categorization problems. Figure 1 gives a flavor for one of these comparisons. This graph summarizes RIPPER'sperformance, as measured by recall and precision, on a corpus of AP newswire headlines (Lewis and Gale 1994). All statistics are averaged across 10 categories (equally weighted)¯ The various points the precision-recall curve were generated by varying RIPPER's loss ratio. All training is done on 999example samples selected by a procedure called "uncertainty sampling("LewisandGale1994).RIPPER's averageperformancise comparabloersuperiotro the systemspreviouslayppliedto thisproblem,namely C4.5(Quinla1n994),a probabilistBiacyesiacnlassif2ier(LewisandGale1994)andFOIL(Quinla1n990). I havealsoobtainesdimilarleyncouragirngesultosn theReuters-221c7o3rpus(Lewis1992),anotherset newsstoryclassificatipornoblem(sCohen1996b).
2Twoencodings were used with FOIL,a "propositional" encoding which supports rules similar to the rules constructed by RIPPER,and a "relational" encoding which also supports rules containing sometypes of phrases. For details see Cohen(1995b).

None of these problems, however, are particularly
representative of the categorization problems likely to arise in processing e-mail. One obvious difference is the amount of training data; the examples for the AP titles, for instance, were selected from a total set of over 300,000, far more than could be expected to be available for construction of a personalized e-mail filter.
However,there are subtler issues aa well. For the AP titles, for instance, the documentsare generally much shorter than a typical e-mail message(a little over nine
wordsin length, on average). Also, for both the APtitles and the Reuters-22173 documents, the documents being classified have been written by professionals with the goal of making their subject immediately apparent to a newspaper reader. Finally, the topics, which are generally based on relatively broad semantic categories like "nielsens ratings" or "wheat", are perhaps atypical of the categories of interest in processing e-mail. Elsewhere it has been noted that in the Reuters-22173 data there is considerable variation amongtopics in the relative performanceof different classifiers (Wiener et al. 1995); hence it is important to evaluate learning methods on problems that will be representative of the problems actually encountered in practise.
Other researchers (Lung 1995; Armstrong el al. 1995) have noted that learning methods based on TFIDF often perform quite well, even relative to more complex learning learning methods, and my experiments with RIPPERon the AP titlesdatasetand Reuters-221t7e3ndedtoconfirtmhisobservatioFnu.r-
thercomparisonisn thispaperwillfocuson TF-IDF
and RIPPER.
Recognizing talk announcements
The category of "talk announcements" was chosen as a more representative test case. A corpus was assembled of all 818 messages posted to a center-wide generalinterest electronic bulletin over a 4-year period¯ All messages were manually labeled as to whether they were talk announcements.I also partitioned the corpus chronologicallya into a training set of 491 messagesand a test set of 325 messages.
TF-IDF and RIPPER were compared on different
sized subsets of the training data. The results, averaged over 25 trials, are summarizedin the upper lefthand graph of Figure 2. Generally speaking, RIPPER does better than TF-IDF, particularly for small numbers of examples.
Thedifferenicsefrequentsltyatisticasliglnyificant. Aftertraininogn eachsubsamplea,pairedtestwas
performedcomparingRIPPER and TF-IDF'shy-
potheseosnthetestdata.In70 of175pairedtests,
RIPPER'hsypotheswiassstatisticalsligynificanstlu-y
periora,ndinonlythreetrialswasTF-IDF'hsypothe-
sisstatisticaslilgynificanstulpyerioIrn.theremaining
trialtshedifferenwcaesnotstatisticaslilgynificant.
ale.e,verymessagienthetestsetwasposteadftetrhe
latest messagein the training set.

20

talk announcements
24

22 4..%

20

~ \,

\ 18

16 '\

RIPPER ¯ TF- IDF -~--

14
12 ",,,,,

10

8
6 ...............1.....
4 // /
50 100 150 200 250 300 350 400
#training examples

5 4.5
4 3.5
3 2.5
2 1.5
1 20

Sydneyunl folders
......
TF-IDF -~--
"\\\
""~,,,~.

i iii

iii

i

40 60 80 100 120 140 160 180 200 ltraining Iws-T_ les

\ .... JAIR folders
10 9.5

9

8.5

8

7.5 ",%
7

6.5

6

5.5

5 II II

50 100

150 200

|training

examples

=
I 250 300

5.5

ML94 folders

RIPPER-I--TF-IDF -~--

2.5 i i i i i i i i
200 400 600 800 100012001400160018002000 #training ew-~D_lee

Figure2: Errorrates for variousproblems

E-mail folders
I also lookedat a numbeorf categories whichreflect howI file myownmail. Myfiling systemfor e-mail messagesis currently completely manualand somewhathaphazarda,ndI amreluctant to use mostof the categoriesof the form"messagetso be filed in folder
foo" as exemplary of useful and/or natural classes.
However, some filing subtasks seemed to be reason-
able categories to experimenwt ith. I decidedto use the followingcorporaandcategories.
SydneyUniversity folders This is a corpus of 269 messages,savedin 38 folders, that werereceivedin SeptembearndOctoberof 1995,duringa visit to the
Basser Department of Computer Science at Sydney
University.Manoyf the folders explicitly correspond to messagesfroma single sender,or else are highly
correlated with the sender. One class was generated
for eachfolder--themessagefsiled in that folderbeing positive examplesa,ndthe messagefsiled in other folders beingnegativeexamples.

Theresults, averagedover 25 trials per class and 38 classes, are summarizeidn the upperright-hand graphof Figure 2. RIPPEpRerformsslightly better than TF-IDFon average, but the comparisonis not nearlyso one-sidedas on the talk announcement problem;it is probablymostaccurateto say that one these problemsthe performanceof RIPPERand TF-IDFis aboutthe same. In the pairedtests, for instance,RIPPEwRasstatistically significantly better thanTF-IDF68 times, statistically significantly worse52 times, andis statistically indistinguishable onthe remaindeorf the 3013trials.
JAIRfolders This is a corpusof 397 messages,saved in 11folders, pertainingto mydutiesas editorof the
Journal of AI Research (JAIR). One class was generated for each folder. In contrast to the Sydney University folders, all of these folders are semanti-
cally defined. Mostof themcontain correspondence connectedwith a single JAIRsubmission.

21

The results, averaged over 25 trials per class and 11 classes, are summarizedin the lower left-hand graph
of Figure 2. Again, the two algorithms are roughly comparable. On average, RIPPER gets a slightly
lower error rate with manytraining examples, and TF-IDFgets a slightly lower error rate with fewer training examples. In the paired tests RIPPERis statistically significantly superior 161 times, statis-
tically significantly worse 184 times, and indistinguishable the remaining 547 times.
ML94folders This is a corpus of 2162 messages,
saved in 71 folders, pertaining to myduties as cochair of the 1994 Machine Learning Conference. One class was generated for each folder that contained at least 22 messages (i.e., more than 1%of the total), for a total of 21classes.
These folders are a mixedlot. Mostare semantically
defined (e.g. "budget") but a few are associated with a particular person. Manyof the classes are not completely disjoint, and hence a numberof messages were filed in multiple folders. The dataset should thus appear noisy to the learning algorithms, which assumethe classes to be disjoint.
The results, averaged over 10 trials per class and 21 classes, are summarizedin the lower right-hand graph of Figure 2. Onthese problems the two systems are again comparable in performance, although TF-IDFhas a slight edge for most sample sizes. In the paired tests RIPPERis statistically significantly superior 146 times, statistically significantly worse 355 times, and indistinguishable the remaining 993 times. Regardless of statistical significance, however the absolute differences between the two algorithms is small--on average their hypotheses agree on nearly 98%of the test cases.
E-mail filters
In this section I will consider another type of message category--categories useful for filtering or prioritizing unread mail.
The categories I considered were obtained as follows. Recently some colleagues implemented a customizable e-mail reader called Ishmail (Helfmanand Isbell 1995). This mail reader allows a user to define an ordered list of mailboxes, each of which has an associated classification rule. By convention the last mailbox in the list
is called mist, and is associated with a classification rule that always succeeds. Before it is read, any incominge-mail messages is placed in the first mailbox with a rule that "accepts" the message (i.e. classifies the messageas positive). The classification rules used by Ishmail are boolean combinations of substring and regular expression matches--a language that includes the keyword-spotting rulesets considered above---and, in the current implementation, must be explicitly programmedby the user.
Ishmail allows a numberof other properties to be associated with a mailbox. For instance, user can request

User Problem
Name userl software
conferences talks user2 association1 taskl localtalks subject1
user3 subject1 personal
associationl todo

#positive #negative examples examples
II 256 15 256 87 256 344 3782 54 3782 81 3782 62 3782 206 1763 575 1763 lO7 1763 12 1763

Table 1: Summaryof the e-mail filtering problems

that the messages placed in a mailbox be automatically archived at set intervals (e.g. weekly). Auser can also ask to be alerted only whena mailbox has at least k un-
read messages, or whenthe oldest unread message is at least d days old; thus Ishmail can be used to prioritize unread mail. Onetypical use of Ishmail is to put commontypes of electronic "junk" mail into special mailboxes, which are then given appropriate alerting and archiving policies. Manyof these messages--notably messages from specific mailing lists--can be detected reliably by looking only at the sender.
I interviewed a number of Ishmail users and found that several users were employingthe system in the following intriguing way. These users defined mailboxes corresponding to particular messagecategories of interest, but gave these mailboxeseither vacuousclassification rules, or else highly approximate rules. Messages
were then manually movedfrom the misc mailbox (or wherever else they woundup) into the semantically appropriate mailbox. The reason for doing this was usually to take advantage of Ishmail'sautomaticarch i v i n g features on messagecategories for whichit wasdifficult to write accurate classification rules.
I realized that, in manually correcting missing or erroneous classification rules, and also automatically archiving the results of these actions, these users had been unknowinglyproviding training data for a learning system. In each case, the messages manually placed into a mailbox are positive examples of the associated classification rule, and messages placed in later mailboxes are negative examples.
It should be noted that all of these users were computer scientists--one a researcher in the area of com-
putational linguistics--and all had written someclassification rules by hand. It seemslikely, therefore, that these categories are relatively difficult to explicitly pro-
gram; this makes these categories are particularly interesting as targets for learning.
I assembled eleven datasets of this sort from three different users. The characteristics of these problems are summarized in Table 1. (In the interests of privacy I have replaced the actual user names and mail-

22

box names with rather generic terms.) These datasets represent several months worth of mail, and two of
the users had used very complete archiving strategies; hencethe datasets are rather large. In each case, I simplified the problem slightly by using only the default "m£sc" mailbox as a source of negative examples. Two of the eleven categories were sets of talk announce-
ments, providing a nice confirmation of the typicality of the first problemI selected for study.
Figure 3 shows average error rates on these problems. I followed a methodology similar to that used above, training the two learning algorithms on different-sized subsets of the total data, and then measuring the error rates on the remaining data; however,
since the sizes of these datasets are different, I used a range of percentages of the whole dataset, rather than a range of absolute sizes for the training set.
Overall RIPPERperforms better on these problems.
In paired tests, RIPPERis statistically significantly superior 132 times, statistically significantly worse 21 times, and indistinguishable the remaining 331 times.
As an additional (and perhaps more easily interpreted) point of comparison, I also measuredthe error rates of the two systems on these problemswith 10-fold
cross validation. The results are summarized in Table 2, with statistically significant differences marked
with an asterisk (*). RIPPER'serror rate is slightly higher on three of the eleven problems, and lower on the remaining eight. On six of these problems RIPPER'serror rate is dramatically lower--less than half
of TF-IDF's error ratc and RIPPERis never statistically significantly worse.
I will conclude with a few general comments.First, on almost all of problems, RIPPER does somewhat
better than TF-IDF,if given a sufficiently large number of training examples. Compared to RIPPER, TFIDFseems to perform best when there is little training data, and particularly whenthere are few positive examples. Second, while the performance of the learners with a very small number of training examples is roughly the performanceof the default classifier, this performance seems to improve quite rapidly. In the talk announcement problem and the folder problems, for example, there is a noticeable and significant reduction in error with even 100 training examples.
Run-time performance
TF-IDF's run-time performance is relatively wellunderstood; an efficient implementation requires only a
small numberof passes over the corpus, leading to linear run-time with a low constant. RIPPER'srun-time performance, on the other hand, has been evaluated mostly on very large noisy datasets, and the efficiency
of the extension to set-valued attributes has not been previously investigated.
In brief, RIPPERseems to be reasonably efficient for problems of this sort, although perhaps not quite fast enoughto be used in an interactive system on cur-

rent hardware. In learning from the complete set of 491 talk announcement examples, a corpus containing
9760 distinct words and a total of almost 80,000 words, RIPPER requires 43 seconds on a Sun 20/60. About 14 seconds of this time is spent reading in the dataset. (For comparison, TF-IDF takes about 17 seconds on the talk announcementproblem, and most of this time is reading in the data.) RIPPER'sperformance on the 2000-example samples of the ML94folder concepts is comparable: RIPPERtakes an average of 42 seconds to construct a ruleset.
Related work
In a previous related study, the rule learning system SWAP1was compared to other learned classifiers on the Reuters-22173 dataset (Lewis 1992), a corpus containing 22,173 documentsthat have been classified into 135 different categories (Apt~ et ai. 1994). The documents in the Reuters-22173 collection are all news stories, averaging around 80 words in length, and the
training sets used were largc on the order of 10,000 labeled examples.
The focus of this comparison is on text categorization problems that are representative of those that arise in handling e-mail correspondence. As noted above, there is substantial variation even amongthe different Reuters categories, and little reason to suppose that they wouldbe typical of e-mail categorization problems.
Another technical difference in these two studies is that the text categorization rules learned by Apte et al. contained primitive tests that compareword frequency with a fixed threshold, rather than simply checking for the presence or absence of a word. This representation
is presumably more accurate but less comprehensible than keyword-spotting rules.
Weare currently conducting further studies with the Reuters-22173 collection. One goal of these studies is to compare the performance of RIPPER and SWAP1.
Conclusions
Motivatedby an interest in learning classifiers for text that are easy for end users to understand and modify, this paper has compared an extension of the RIPPER rule learning method and TF-IDF on a number of text categorization problems. The benchmark problems used are problems that might plausibly arise in filing and filtering personal e-mail.
One encouraging result of the study is that both methods have fairly steep learning curves; i.e., significant amounts of generalization are obtained with a relatively small number of examples. In each of the graphs in Figure 2, for instance, a significant reduction in the error rate occurs with only 100 examples, and muchof the learning takes place with around 200 examples--for many users, only a few days worth of mail. I found this rapid learning rate somewhat surprising--since manyof the categories are relatively

23

0.16 0.15 0.14 0.13 0.12 0.11
0.1 0.09 0.00 0.07 0.06 0.05
0

Averse of filtering ~, ,,
,\

problenm
RIPPER -~-TF-ZDF ....

mmi i t i m
10 20 30 40 50 60 70 t of data umed for tra4ning

80

Figure 3: Error rate averaged over the filtering problems

User Problem

%Error)

RIPPER (se) TF-IDF

(se)

userl software

4.53 (1.02) 4.15 (1.28)

conferences talks

7.02 (1.48) ¯ 8.16 (1.37)

5.53 (1.67)
20.08 (2.02)

user2association1 ¯ 2.84 (0.24)

6.79 (0.38)

taskl

¯ 0.65 (0.11)

1.38 (0.22)

localtalks ¯ 0.93

subject1

1.48

user3subject1

¯ 3.60

personal

¯ 21.00

associationl ¯ 0.91

todo

0.73

(0.10)
(0.31) (0.31) (0.79) (0.27)
(0.15)

11..8897

(0.14)
(0.26)

9.40 (0.78)

23.86 (0.69)

4.44 (o.45)

0.67 (0.15)

Table 2: Performance on e-mail filtering problems (10-CV)

infrequent, one would expect to see only a handful of
positive examplesin a training set of this size.
Although more work clearly needs to be done, the experiments also shed somelight on the relative performance of rule learning methods and traditional IR methods. A priori, one might expect rule induction methods to work well whenthere is a concise keywordbased description of a category, and work poorly in
other cases. In filtering e-mail, of course, a number of arguably natural types of categories do have simple rulesets. One example is categories that are associated with a single unique sender; for instance, messages from a particular mailing list. Another example is categories distinguished by salient, attentiongrabbing keywords; the "talk announcements" category is an instance of this sort of category. (Most talk announcements include a number of salient keywords, such as "talk", "abstract", and "speaker".) However, the experiments of this paper suggest that induction of keyword-spotting rulesets is competitive with traditional IR learning methods on a relatively broad class of text categorization problems. In particular, the rule methodsare competitive even in situations in which all or most of the categories are semantically defined.

This suggests that a system which combines userconstructed and learned keyword-spotting rules may indeed be a viable architecture for a personalized email filtering system.
References
Chidanand Aptd, Fred Damerau, and Sholom M. Weiss. Automatedlearning of decision rules for text categorization. ACMTransactions on Information Systems, 12(3):233-251, 1994.
R. Armstrong, D. Frietag, T. Joachims, and T. M. Mitchell. WebWatcher:a learning apprentice for the world wide web. In Proceedings o/the 1995 AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, Stanford, CA, 1995. AAAIPress.
William W. Cohen. Fast effective rule induction. In Machine Learning: Proceedings of the Twelfth International Conference, Lake Taho, California, 1995. Morgan Kaufmann.
William W. Cohen. Text categorization and relational learning. In Machine Learning: Proceedings of the

Twelfth International Conference, Lake Taho, California, 1995. Morgan Kaufmann. William W. Cohen. Learning with set-valued features. Submitted to AAAI-96, 1996. William W. Cohen. Some experiments in text categorization using rules. In preparation, 1996. Jonathan Isaac Helfman and Charles Lee Isbell. Ishmail: Immediate identification of important information. Submitted to CHI-96, 1995. David J. Ittner, David D. Lewis, and David D. Ahn. Text categorization of low quality images. In Symposium on Document Analysis and Information Retrieval, pages 301-315, Las Vegas, NV, 1995. ISRI; Univ. of Nevada, Las Vegas. Ken Lang. NewsWeeder:Learning to filter netnews. In Machine Learning: Proceedings of the Twelfth International Conference, Lake Taho, California, 1995. Morgan Kaufmann. David Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine Learning: Proceedings of the Eleventh Annual Conference, NewBrunswick, NewJersey, 1994. Morgan Kaufmann. David Lewis and William Gale. Training text classitiers by uncertainty sampling. In Seventeenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, 1994. David Lewis and Mark Ringuette. A comparison of two learning algorithms for text categorization. In Symposium on Document Analysis and Information Retrieval, Las Vegas, Nevada, 1994. David Lewis. Representation and learning in information retrieval. Technical Report 91-93, ComputerScience Dept., University of Massachusetts at Amherst, 1992. PhDThesis. J. Ross Quinlan. Learning logical definitions from relations. MachineLearning, 5(3), 1990. J. Ross Quinlan. C4.5: programs for machine learning. Morgan Kaufmann, 1994. J. Rocchio. Relevance feedback information retrieval. In Gerard Salton, editor, The Smart retrieval system--experiments in automatic document processing, pages 313-323. Prentice-Hall, EnglewoodCliffs, NJ, 1971. Gerard Salton. Developments in automatic text retrieval. Science, 253:974-980, 1991. E. Wiener, J. O. Pederson, and A. S. Wiegend. A neural network approach to topic spotting. In Symposium on Document Analysis and Information Retrieval, pages 317-332, Las Vegas, Nevada, 1995.
25

