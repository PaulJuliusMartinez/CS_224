From: Proceedings of the Eleventh International FLAIRS Conference. Copyright © 1998, AAAI (www.aaai.org). All rights reserved.

Facial Expression Recognition
Christine L. Lisetti
Department of Computer Science Stanford University
Stanford, CA 94305 USA lisetti@psych.stanford.edu

Using a Neural Network
David E. Rumelhart
Department of Psychology Stanford University
Stanford, CA 94305 USA der@psych.stanford.edu

Abstract
Wediscuss the developmentof a neural networkfor facial expression recognition. It aims at recognizingand interpreting facial expressionsin terms of signaled emotions and level of expressiveness. Weuse the backpropagation algorithm to train the systemto differentiate betweenfacial expressions. Weshowhowthe network generalizes to newfaces and weanalyze the results. In our approach, weacknowledgethat facial expressions can be very subtle, and proposestrategies to deal with the complexityof various levels of expressiveness. Our databaseincludes a variety of different faces, including individuals of different gender, race, andincluding different features such as glasses, mustache, and beard. Even given the variety of the database, the network learns fairly succesfuily to distinguish variouslevels of expressiveness, and generalizes on newfaces as ~ell.
Introduction
Within the field of computer vision, there has recently been an increasing interest in the field of computer vision to recognize facial expressions. Psychologists have established correlations between various affective states and facial expressions, which humanscan recognize with somelevel of accuracy. As it is characterized as a problem of pattern recognition in humancognition, performance by computers with pattern recognition abilities could potentially perform as well than some humansfor this task. Futhermore, it is expected that three to ten years from now, the price of cameras will have dropped considerably. This will makevisual awareness research for artificially intelligent systems a very interesting alley for developing computer environ-
nlents.
Indeed, there exists a number of applications which can benefit from automatic facial expression recognition. Face recognition in real-life environments, for example, such as airports, and banks, often involves various different viewpoint and expressions of an individual. Being able to recognize facial expressions can assist facial recognition algorithms (Yacoob, Lanl, arid
Copyright1998, AmericanAssociation for Artificial Intelligence (www.aaal.org).All rights reserved.

Davis 1995). Furthermore, body language is an important part of human-humancommunication (Birdwhistle 1970). Developing methods for a computer to automatically recognize humanexpressions, could enhance the quality of human-computerinteraction and enable the construction of more natural adaptive interfaces mid environments (Hayes-Roth et al. 1998), (Lisetti 1998). Facial expression recognition is useful for 'adapting interactive feedback in a tutoring system based on the student's level of interest, or for monitoring pilots and drivers alertness state. Automatic recognition could also be used on video recording of group interactions, to trace and documentchanges in the expressions of the participants, or to retreive pieces of a video based upon a particular facial expression of a subject (Picard 1997). Yet another application is found in the development of psychological emotiontheories by facilitating the experimental data collection of facial expressions associated with particular emotions.
A number of systems have already dealt with facial
expressions using different technical approaches such as the memory-based rule system, JANUS(Kearney and McKenzie1993), spatio-temporal templates (Essa, Darrell and Pentland 1994), image motion (Black and Yacoob 1995), among others. One of our motivations is to explore the potential that neural networks offer for this type of pattern recognition problem. Anearly neural network which dealt with facial expressions was the single perceptron which could classiC" snfiles from frowns, and which was tested on one person only (Stunhain 1986). Since then, other connectionist systems have been implemented to classify mid recognize facial expressions with good results (Cottrell and Met-
calfe 1991), (Roscnblum, Yacoob, and Davis 1994}.
Our researc~ project continues to explore the potential of neural networks to recognize facial expressions. In the present paper, we work with two expressions, nmnely neutral and smiling. In order to addrcss some questions from psychology about the discreteness of facial expressions, we study howvariations in expressiw:ness can affect the performance of the system. Wediscuss what approaches are most promising given that some expressions may be ambiguous, even for human recognition. Wealso mention the future direction of

ourongoingproject.
Facial Expressions as Emotional and Communicative Signals
Facial expressions can be viewed as communicativesignals (Chovil 1991), associated with syntactic displays, speaker displays, or with listener commentdisplays in a conversation. This approach has been used to improve human-computerinteraction with speech dialog (Nagao and Takeuchi 1994).
Facial expressions can also be considered as expressions of emotion (Ekman and Friesen 1975), raising ongoing debates about their discretness and universality. One of the most documentedresearch effort led by Ekmanhas permitted to identified six basic universal emotions: fear, anger, surprise, disgust, happiness, and
sadness(EkmanandFriesen1975).OtherslikeRussel preferto thinkthatfacialexpressioannsdlabelsare probablayssociatebdu,tthattheassociatimonayvary
withcultur(eRusse1l994). Whetherornotthereexistuniversaflacialexpres-
sionsW,ierzbicpkoaintastthedifficultyotalkabout
emotions,andwarnsthatwhatwe referto as basic emotions with labels such as "anger", may have concepts which may very well be culturally determined (Wierzbicka 1992). Studying these concerns is beyond the scope of this paper, and need to be addressed in further details whendealing with particular applications. Relevant expressions and their interpretations mayindeed vary depending upon the chosen type of application (Lisetti, 1998).
In this present paper, however, we address some of the issues above by focusing on getting a neural network to be able to recognize differences in levels of expressiveness from two emotions: happy and neutral.
Facial Expression Interpretation Using a Backpropagation
By contrast with non-connectionist approaches which usually use geometrical face codings, connectionist approaches have typically used image-based representation of faces in the form of 2D pixel intensity array. While this model has the advantage of preserving the relationship between features and texture, it has a very high sensitivity to variations in lighting conditions, head orientation, and size of the picture (Valentin et al. 1994). These problems typically justify a large amount of work in preprocessing the images. Normalization for size and position is necessary and can bc performed automatically with algorithms for locating the face in the image and rescaling it (Turk and Pentland 1991). our particular data set, there wasno need for rescaling, as all the images were consistent with each other along that dimension.
The Data Base of Images
Weused images from a variety of sources. The results described below- have been derived mostly from using

the FERETdata base of face images which inchlded smiling faces and neutral faces. 2 The FERETdatabase
includes pictures of faces with various poses (such as full face, profile, and half profiles) for each person. These pictures are useful to build face recognition algorithms in terms of person identification from different angles.
Since weare presently exclusively interested in facial expressions, however, we built a sub-set of the FERET data base to include only two different poses per person: namely one full face with a neutral expression, and the other full face with a smile. Not every one of the pictures had the same degree of neutrality, or the
same degree of "smilingness". Wehave designed various approaches to test this scalability amongimages. As stated above, one of the advantages of the FERET images was that all the images were consistent enough in terms of size and position.
Interpreting facial expressions of an individual in terms of signaled emotions requires us to work with minute changes of some features with highly expressional value. Someexamples of those are found in the mouthsuch as the orientation of the lips (up or down),
in the eyes such as the openess of the eyes, etc. There are three areas of the face capable of inde-
pendent muscular movement: the brow/forehead; the eyes/lids and root of the nose; and the lower face including the cheeks, mouth, most of the nose and the chin (Ekman1975). Furthermore, not every expression is shown in the same area of the face. For example surprise is often shownin the upper part of the face with wrinkles in the forehead and raised eyebrow's, while smile is mostly shownin the lower face.
The Network Architecture
Our network is designed to deal separately with the three areas of the face capable of independent muscle movementmentioned above. It, is illustrated in figure 1. Each portions of the face is pre-processed by cropping the initial full-face/background images (manually at this stage) to smaller sizes, and normalized in terms of intensity by histogram.
The network includes one layer of input units, one layer of hidden units and one layer of output units. The input units are 2Dpixel intensity arrays of the cropped
images. The output units express the value of expressiveness of a particular expression ranging from 1 to 8, or from 1 to 6, depending upon the experiment. The hidden layer is connected to each portion of the face and to each output unit in order to simulate "holistic" face perception. Alternatively, the connections can bc loosen, to model more local pattern recognition algorithm.
Finally, because, in the future, we also want to bc able to refer to the recognized expression with a label such as "happy", or "angry", we provide an ordered
2Portions of this research in this paper use the FERET database of facial images collected under the ARPA/ARL FERETprogram.

Neural Networks329

input maskof binary values for each of tile emotions to lie recognized. These can be the 6 basic emotions, l)ius neutral: bit 1: fear, bit 2: angry, bit 3: surprise, bit 4: disgnst, bit 5: happiness, bit 6: sadness, bit 7 neutral. These binary output values are to be turned on to "1' if the expression is identified, while the others remain se, t to '0'. An example of output will be in addition to the level of expressiveness from the regular output units, the left set of output units will point to the label "3" by turning on the third bit of the maskas
in [0.0,1,0,0,0,0].

Figure 2: Full Face Neutral

~..,,,.,:.,.~,,> ¯- [

'v~'/EneO"v~aIre~VD~.l~l.~Ot'Tltm(~V.~l~-I

' IliddelL~.

Figure 3: Full Face Smiling

,,
E%PRFS$iD,'~Fa'('E

,,
BRDWI

lI

EYES.'

MOL'11]

Ime

Figure 1: Network Design
In order to test each portion of the network, however, we built subnetworks each dealing with different portions of the face. Wediscuss here the results for the full face subnetwork, and the mouth/part-of-nose/partof-chin subnetwork. Wedesigned different strategies to test whichapproach wouldbe better fit to recognize fa(:i'M expressions in terms of degree of expressiveness: in this case, the degree of smilingness.
Wereduced our images to 68X68pixel images, for our fltll-face experiments, and we reduced them to 74X73 for our lower fa(:e experiments.
Full Face Network
Test 1: Graded Answers In our initial stage, we l)reprocessed the images marmally, cropped the outer edges of the face leaving the flfll face as shownin figures 2 and 3. Weperformed histogram equalization for normalizing intensity across the different images.
Data: The network had 40 hidden units, one input unit per pixel, and one single output unit. Wetrained tim network on 40 input images. We included images of 20 different persons, two images per person. Weselected randomly 30 images in our training set arid 10

imagesin our testing set for generalization. Target valties ranged from 1 to 8 (similarly to psychological tests used in emotion research), to indicate the level of expressiveness of the image.
Results: The results of the training shownin figure 4 indicate that the network learned accurately. The plots compare: (1) the given target expressed in terms the degree of expressiveness of a particular image('x') with (2) the actual output of the network generalization and training ('o'). Generalization was then tested on new faces of persons that the network had ne~r "seen" before . The generalization results are shown in figure 5, indicating that the network did not generalize completly, as can be observed by the differences between targets and outputs.
Lower Face Network
Happiness and satisfaction are typically shownin the third area of the face capable of independent movement. It includes the mouth, most of the nose and the chin (Ekman1975). Wetherefore isolated this area of the face and generated cropped images including the lower face only. The input images were 74x73 pixel images.
Wedesigned two procedures to test howwell the network could generalize on new images that it haxt not been trained with: (1) testing whether the network could generalize if it had been trained on the person with one expression but not on the other, (2) testing whether the network could generalize on people which it had never been exposed to (i.e. with neither expression).
Test 2: With prior exposure to the person Data: In this case, we selected intermittently neutrM faces, arid smiling faces. That is, instead of training the network on both expressions for each individual, we trained the network on each individual, soinetimes including both expressions but sometimes witholding either one of the two expressions. In that manner, we

330 Lisetti

Figure 4:I~i11FaceProcessing

.,., ....... , ..T..I.M...1. ".G...m...l.lT, m...z. Wlon ¯ Lqrget o oumm

o

o

Figure 6: Full Face Processing

i
o IO
Figure5:FullFaceProcessing
couldtesthowwellitgeneralizewdi,thouhtavingbeen exposetdo bothexpressionofs thesamepersonduring the training stage.
ResultsT:heresultasreshownin figure7s and8. Onceagainthetraininwgasverysuccessfuals the networkapproximateidtsoutputto be verycloseto thevalueswe hadgivenit as targets.Thenetwork generalizveedryaccuratefloyreachofthetestcasesas canbeobservefdromfigure8. We alsowantedto know ifthenetworckouldgeneralioznesmileosfpeoplethat ithadneverbeenexposetdo.
Test 3: Withoutprior exposureto the person Data:Thistime,we trainedthenetworkon116input imagesofsize(74X73).
ResultsW:eincludeidmagesof58differenptersons, twoimagesperperson.We selecte9d4 imagesforour traininsgetand22imagesforourtestinsgetforgeneralizatioPnl.otsinfigure9s and10compare(:1)the giventargetexpresseidntermsofthedegreeofexpressivenesosfa particuliamrage('x')with(2)theactual outputofthenetworgkeneralizatmizodntrainin(g'o'). As canbe observefdromthegraphs,theoutputsmatch closeltyhegiventargetsA.summaroyftheerroraveragesforeachtestisgivenbelowinabsolutvealues(see table1).Itliststhedifferenbceetweetnargevtalueasnd output values, averaged over the number of images.

Figure 7: Lower Face Processing
Conclusion
Our results indicate that zooming in particular areas of the face for expression detection offers better results than processing the full face as a whole. Our next approach will be to isolate each area of the face, and combine their inputs into a single network to increase precision of our recognition algorithm as described earlier. Oneextension of the facial expression system will be the integration of the recognition scheme with a real-time tracker. This coupling is planned to enable the system to perform real-time recognition of facial expressions.
Acknowledgement
Wewouldlike to acknowledgeIntel Corporation for partial funding for this research.
References Birdwhistle. 1970. Kinesics and Context: Essays
on Body Motion and Communication. University of Pennsylvania Press. Black, & Yacoob, Y. 1995. Recognizing Faces Showing Expressions. Proceedings of the International Workshop on Automatic Face and Gesture Recognition, IEEE Press. Black, M., and Yacoob, Y. 1995. Tracking and Recognizing Rigid and Non-Rigid Facial Motions using Local Parametric Models of Image Motion. Proceedings Int'l Conference Computer Vision ICCV'95, 374-381. Chovil, N. 1991. Discourse-Oriented Facial Displays in Conversation. Research on Language and Social Interaction 25:163-194. Cottrell, G. and Metcalfe 1991. EMPATHF:ace, Emotion, and Gender Recognition using Holons. Ad-

NeuralNetworks331

Figure 8: Lowcr Face Processing

s: m ss. ¯ i.s,* 4s,

--P

Figure 10: LowerFace Without Prior Expt)sm'e

Table 1: Error Averages (absolute values)

Test 1 Test. 2 Test 3

TRAINING 0.0496 0.0315 0.0428

GENERALIZATION
0.1072 0.0722 0.0756

'! & ~, = ....... ~ -" .o ~ ., .o
Figure 9: LowerFace without Prior Exposure
vances in Neural Information Processing, Morgan KaufmannPublishers. Ekman, P., and Friesen, W. V. 1975 Unmasking the Face: A Guide to Recognizing Emotions from Facial Expressions, EnglewoodCliffs, NewJersey: Prentice Hall, Inc. Essa, I.; Darrell, T.; and Pentland, A. 1994 Tracking Facial Motion, Proceedings of IEEE Workshopon Nonrigid and Articulate Motion. IEEE Computer Society Press. Hayes-Rx)th,B.; Ball, G.; Lisetti, C.; Picard, R.; and Stern, A. 1998. Panel on Affect and Emotionin the User Interface. In Proceedings of the 1998 International Conferenceon Intelligent User Interfaces, 9194. New York, NY: ACMPress. Kem'ney,G. D. 1993. MachineInterpretation of Emotion: Design of a Memory-BasedExpert System for hlterpreting Facial Expressions in Termsof Signaled Emotions. Cognitive Science 17, 589--622. Lisetti, C. L. 1998. An Environment to Acknowledge the Interface between Affect and Cognition. In Working Notes of the AAAISpring Symposiumon Intelligent Environment, AAAIPress. Nagao, K., and Takeuchi, A. 1994. Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics.
332 Use~

Picard, R. 1997. A~ective Computing. Cambridge, MA:MIT Press book.
rtosenblum, M.; Yacoob, Y.; and Davis, L. 1994¯ Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture. IEEE Workshop on Motion of Non-Rigid azld Articulated Objects.
Rowley, H. A.; Baluja, S.; and Kanade, T. 1996. Neural Network-Based Face Detection. Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, 6:285-319.
Russel, J. 1994. Is There Universal Recognition of Emotion From Facial Expression? A Review of CrossCultural Studies Psychological Bulletin 115(11102141.
Stonhanl, T. J. 1986. Practical face recognition and verification with Wisard. In H. Ellis and M.A.Jeeves (Eds.), Aspects of face processing. Lancaster, England: Martinus Nijhoff.
Turk, M., asld Pentland, A. 1991. Eigenfaces for Rerognition. Jom'nal of Cognitive Neuroscience. 3, 71-86.
Valentin, T.; Abdi, H.; O'Toole, A.; and Cottrell, G. 1994. Connectionist Models of Face Processing: A Survey. PatteTm Recognition 27: 1290-1230.
Valentin, T. (Ed.) 1995 Cognitive and Computational Aspects of Face Recognition: Explorations in Face Space. NewYork, NY: Routledge.
Wierzbicka, A. 1992. Defining Emotion Concepts. Cognitive Science 16:539-581.
Yacoob, Y.; Lain, H.; and Davis, L. 1995. Recogniz-
ing Faces Showing Expressions. In Proceedings of the International Workshopon Automatic Face and Gesture Recognition, IEEE Press.

