BfCoaryaelLcseuOalarpnttiioinmngaoalfCPthlearescsLeipeatcrarontniionnWg ACitluhgroNvreiotiohsfme

Manfred Opper
Institut fuer Theoretische Physik Justus-Liebig-Universitaet Giessen
Giessen, Germany maopper@dgihrz01.bitnet

David Haussler
Computer and Information Sciences U.C. Santa Cruz
Santa Cruz, CA 95064 haussler@cis.ucsc.edu

Abstract

The learning curve of Bayes optimal classi -

cation algorithm when learning a perceptron

from noisy random training examples is cal-

culated exactly in the limit of large training

sample size and large instance space dimen-

sion using methods of statistical mechanics.

It is shown that under certain assumptions,

in this \thermodynamic" limit, the probabil-

ity of misclassi cation of Bayes optimal algo-

rithm is less than that of a canonical stochas-

itnicglepa2rniansg

algorithm, by a factor approachthe ratio of number of training

examples to instance space dimension grows.

Exact asymptotic learning curves for both al-

gorithms are derived for particular distribu-

tions. In addition, it is shown that the learn-

ing performance of Bayes optimal algorithm

can be approximated by certain learning al-

gorithms that use a neural net with a layer

of hidden units to learn a perceptron.

1 Introduction

Extending a line of research initiated by Elizabeth

Gardner Gar88, GD88], exceptional progress has been

made in recent years in applying the methods of statis-

tical mechanics to the analysis of the process of learn-

ing from random examples, as exempli ed in the learn-

iwnogrkalgoDrSitWhm+s87u]sedHLtoWt8r8a]in

neural BH89]

networks. VJP89]

Recent LTS89]

GT90] HS90] STS90] OKKN90] has focused on

quantifying what is known in the neural net litera-

ture as the generalization performance of learning al-

gorithms. This is the probability that the learning

algorithm will correctly predict the classi cation of a

new random instance, after it has seen a certain num-

ber of random classi ed instances, called training ex-

amples. In other literature, this is referred to as the

probability of a mistake or the expected 0-1 loss.

Most neural net learning algorithms make predictions on novel instances by selecting a hypothesis, represented by couplings or "synaptic" weights of a neural

network, that performs well on the training examples.

AGibcabsnoanlgicoarlitahlmgo1r,itwhams

of this type, which we call studied from a statistical

the me-

chanics perspective in GT90, HS90, STS90], and in

a more abstract setting in LW89] (as the random-

ized weighted majority algorithm) and HKS91]. For

noise-free training examples, the extreme "zero tem-

perature" version of this algorithm simply chooses a

hypothesis at random from among those that are con-

sistent with all the training examples, as in Maa91].

Here we apply similar methods from statistical physics

to study Bayes optimal classi cation algorithm, a spe-

cial case of the weighted majority algorithm Lit89,

LW89, Vov90] (see also DMW88]). Further inves-

tigation of the Bayes and Gibbs algorithms appears

in HKS91], from both an information theory and a

Vapnik-Chervonenkis theory perspective.

The performance of any learning algorithm will depend on the target function, i.e. the input/output mapping to be learned. In the Bayesian approach, variability in the selection of target function is modeled by assuming an a priori probability distribution over possible target functions. When there is noise in the examples, a priori information about the nature of this noise is also incorporated. One then seeks a learning algorithm that will give the best average generalization performance (i.e. minimum average loss) on target functions and noise processes selected according to this a priori distribution. This is what Bayes optimal classi cation algorithm does. The performance of Bayes algorithm provides the natural standard against which other algorithms may be compared.

In this paper we derive expressions for the average generalization performance for both Bayes algorithm and the Gibbs algorithm for the simplest neural network: the single layer perceptron. We assume that a target perceptron is selected at random according to a prior distribution, and that noisy training examples are generated from this target, where classi cation label of each example is ipped independently with some probability 0 1=2. The noise-free case ( = 0) was investigated previously in OH91].

1This algorithm was called the Boltzmann algorithm in OH91].

In the noisy case, one can measure the probability of

misclassi cation as either the probability that a mis-

take is made in predicting the noisy label, or the prob-

ability that a mistake is made predicting the underly-

ing classi cation label, before the noise is added. Both

of these are with respect to the random choice of the

target concept, the random choice of the training in-

stances, the random noise added to the training in-

stances, the random choice of the test instance, and

any internal randomization in the algorithm. Let us

adopt the latter probability as our notion of the prob-

ability of misclassi cation here and in the following

paragraph. (We look at both notions in this paper.)

Let N denote the dimension of the instance space, m

the number of training examples and = m=N. We

show that as m; N ! 1 such that remains con-

stant, the probability of misclassi cation after m ran-

dfaocmtortrpai2nifnrgomextahme polpetsimfoarl

the Gibbs algorithm is a performance of Bayes al-

gorithm, asymptotically as becomes large. Here we

make minimal assumptions on the the a priori density

on the weight space used to select the target function,

and on the density used to select the training exam-

ples.

When the latter densities are chosen to be uniform on

the surface of the sphere, then we can give explicit

formulae for the probability of misclassi cation as a

function of . For large and noise-free training ex-

amples, this probability is approximately 0:44= for

Bayes algorithm and 0:62= for the Gibbs algorithm.

These results show that the general upper bounds de-

rived in HKS91] for the performance of the Bayes and

Gibbs algorithms on hypothesis spaces of nite VC

dimension are tight to within a relatively small con-

stant in this case. When the training examples are

corrupted by random classi cation noise with rate ,

the asymptotic probability of misclassi cation for the

Gibbs algorithm is given tion C( ) is (actually C(

by C( )(1 ?

2)=)2,)

where the funcis as plotted in

Figure 4. As mentioned above, we show that the prob-

abbyilaitfyacotformoisfcpla2ss.i cation for Bayes algorithm is less

It turns out that Bayes algorithm is di cult to imple-

ment on a neural network, so we also look at a series of

learning algorithms that approximate the performance

of Bayes algorithm. These algorithms use a neural net-

work with a layer of n hidden units between the input

and output, for n = 1; 3; 5; : : :, where the output node

just n=

takes 1 we

a majority vote of the hidden get the Gibbs algorithm, and

ausnnits!.

F1or,

the performance of these algorithms approaches Bayes

optimal.

2 Results

2.1 Basic De nitions

In the simple binary classi cation learning problem we

consider, one tries maps from a set X

(ttoheleianrsntaanctearsgpeatcef)unincttoiofn?f1;

that +1g.

Here we take X to be vectors and consider

etahcehNc-odmimpeonnseinotnaxl(si)paocfe~xof2reXal

to be the state of an input node for a neural network

on instance ~x. Each possible setting of the vector of

weights w~ in the neural network de nes a classi cation

fsuinngclteiolnayfew~r

from X into f?1; +1g. For the

perceptron fw~ (~x) = sign (w~

x~ =pN );

case

of

the

where sign (x) = +1 x 0. The division

ibfyxp>N

0 and sign (x) = ?1 if is not really necessary

here, but it will be convenient later, since we restrict

the weight vector to the sphere w~ w~ = N, and this

e ectively normalizes w~ to unit length.

Learning a perceptron was recently investigated by

several authors from a statistical mechanics perspec-

tive VJP89, Gyo90b, GT90, OKKN90]. We look at

a simple model of learning where we assume that the

function f can be learned perfectly by the neural net-

work, call w~

i.e. f the

ta=rgeftw~vefcotrorso. mIne

weight vector the process of

w~ . We will learning we

ag0ianpaasnesnboemsnndidiu+ssledieema1rtleya(evmnsewtc~eate+tqtrde11hiu;ida,a=daenb=2wetnmd.nlache+att(eTsei1.crtkoh)1reaafua;Wlr=lins=gyn2edkets;+tohtad=:ame1sv:isnms:est.uw;cc+irkmegtiif1osbtnmwe~hrNux+o=ttowm(~1pfeh~ix)dr+seakoy(a1it)ibsfcs,=ha1caks;bkl1eca(is=llg~lx=s2eeais-t1;cqsvi1y:;t?sua;~e:xicel121d:2uan;c;;?tecwa:a:ideot:t:mii:nt:oo+rr;,h;naafm~1xwlnnp)aimldndhar+b+oodoabeebr1mmr1eelees)--,l

is ipped independently with probability . We call

the noise rate.

Given only training examples (~x1; 1); (~x2; 2); : : :; (~xm; m)
adonficdtthitenhlseetaalrannbcieenlg~xamml+g+o11,r.itthhTemheleisagtrehnneienrpgarloiazblagatobiroiilntihteymrtrhomrautsi(ttmpp;rree)-dicts wrong, as a function of the training sample size m and the noise rate . Note that in this formulation the object is to predict the noisy classi cation label. Clearly no algorithm can achieve a success rate better than at this task.

An alternate formulation is to consider the generaliza-

tion error to be the probability that the prediction of

dtthheeenoulteneadrtenhrilinysgipnargolgnbooaribistielh-imftryeebonyclain(smssita;cnac)t.eioI~xntmifs+w~1cl(ed~xaimr e+trh1sa).ftrWome

(m; ) = (1 ? ) (m; ) + (1 ? (m; ))

= + (1 ? 2 ) (m; );

(1)

so either one of these quantities is easily obtained from the other.

2.2 Posterior Density

xFomr+1noiws ,

let xed

us assume that the instance and the only randomization is

sequence over the

choice of the target vector w~ , which is chosen accord-

ninogisteoeavnenatsprmio+ri1,dwenhsiicthy

d is

(w~ ), chosen

and the sequence of as described above.

For each m 1 let

(w~; m) = (w~; m(w~ ; m))

be the number thime=h=ypi(fow~t1h;(e:~xs:ii:s);

r=6 oemfpf)rtwe~imts(he~xenaistt)e,idanir.ebet.hypetwr~he.edrisnctutemmdbeienxrcaoomfrrplealcebtsleyltshbainyt

Fix a constant > 0. The constant plays the role

of an inverse temperature from a statistical mechan-

ics viewpoint; large represents low temperature and

small represents high temperature. From a learning

point of view, will govern the tradeo between good-

ness of t to the sample data and a priori plausibility

of the hypothesis. Large will force the algorithm,

when predicting rst m examples

wme+ll1(,i.teo.

use hypotheses those with low

that ),

t the even if

they are a priori unlikely to be the target. Small will

allow the combined e ects of the a priori most likely

hypotheses to carry more weight, so long as they don't

have too large a .

Speci cally, for each m de ne the posterior density

d m(w~) = d (w~)e? (w~; m(w~ ; m))=Zm; where

Zm = Z( m)

= =

Z(xm; m(w~ ; m))

Z
e?

(w~; m(w~ ; m))d (w~)

is the normalizing constant for this density. In sta-

tistical mechanics, tion. Note that in

tZhme

is called posterior

the partition funcdensity, the (unnor-

malized) weight of a hypothesis is reduced exponen-

tially in proportion to the number of times it is in-

correct on the training sequence, as in the weighted

majority algorithm LW89]. For noise rate , we

can easily show that when = ln((1 ? )= ), the

dsietnysiotvyerdalml

is the correct possible target

Bayesian posterior denvectors w~, assuming the

prior density is d (w~) and we are given the ex-

sasthuhmoreapwblcnelosen)in(d~xsite1iLt;ointo1a8f)l9;]w()p~x.er2iog;bThat2hb)i;visl:ei:tcmy:t;oe(rat~xshnmasW;tth,mtahR)te.w~f2otW(arTrgdahenitsymi(vs(w~emca)telosaiors-

was chosen from W, given the observed examples

l(a~xt1e; th1e);u(n~xc2o;nd2)it;i:o:n:a;l(~xpmro;bamb)il.itTyoofsetehethlaisb,elrssetqcuaelnccue-

m by conditioning over possible targets w~, weighted

by the prior. This gives

Z
Pr( m) =

(w~; m)(1 ? )m? (w~; m)d (w~)

=

Z
(1 ? )m e?

(w~; m)d (w~)

= (1 ? )mZm:

(2)

Therefore

Pr(w~ 2 W j m)

=

Pr( mjw~ 2 W) Pr(w~ 2 W) Pr( m)

= =

R
w~2W

(w~; m)(1 ? )m? (w~; m)d (w~)

R
w~2W

e?

(1 ? )mZm (w~; m)d (w~)

Z Zm

= w~2W d m(w~):

In the limit when = 1 (the zero temperature limit),

toohnfeatlphl eowsseteeigtrhioftrw~vde:ceftnows~ri(ts~xytkhd)a=tmafriw~es cz(o~xenrkos)i;setfveoenrrtykww=hitehr1e;t:he:ex:;cemrpsgtt

m training examples. This is called the version space

in the AI literature Mit82]. Thus in the zero temper-

ature limit, all hypotheses that contradict even one

training example are eliminated from consideration.

The volume of the remaining version space, as a frac-

tion of the volume of the original hypothesis space, is

igsivjeunstbythtehempeaarstuirteioonffuthnectvioenrsiZomn ,swpahcicehuinndtehristhceasae

priori density d . (For the zero temperature case this

volume is the nite

denoted case, it

bisyuVsmefuinl toOtHh9in1k,

a posteriori volume measure on the

HKS91].) Even in ohfyZpmothaessaisksipnadcoe,f

which decreases as the number m of training examples

increases.

2.3 Gibbs and Bayes Algorithms

ItnheoGrdiebrbstoalmgoarkitehimtscphroeodsiecstiaonhyopnotthheesiins sw~taantcerax~nmd+om1,

according esis space

taontdheppreodsitcetrsioarcdceonrdsiitnygdtomthoins

the hypothhypothesis.

This is the stochastic learning algorithm discussed in

GT90, STS90, LW89].

Let

Z

Zmright =

e?
fw~:fw~ (~xm+1)= gm+1

(w~; m(w~ ; m))d (w~);

i.e. the a posteriori volume (after the rst m examples)

of those hypotheses that predict correctly on the m +

1st example. Let

Z

Zmwrong =

e?
fw~:fw~ (x~m+1)6= gm+1

(w~; m(w~ ; m))d (w~);

i.e. the a posteriori volume of those that predict wrong. Clearly

Zm = Zmright + Zmwrong:

Note also that

Zm+1 = Zmright + e? Zmwrong:

Hence Zm ? Zm+1 = (1 ? e? )Zmwrong: Thus, since the Gibbs algorithm chooses its hypothesis matarkaensdaommiastcackoerdiinngprteoditchteinpgosmte+ri1orwditehnspirtoybdabimli,tyit

Zmwrong Zm

=

1 1 ? e?

1

?

Zm+1 Zm

:

(3)

A similar formulation has been obtained in LTS89] and LW89].

The average generalization error of the Gibbs algo-

rithm, when the target vector w~ is by d (w~ ) and the noise sequence

cmh+os1enisagternaenradtoemd

srtaanndcoems lxymw+1ith=
by

noise (~x1; :

rate , but the rst : :; ~xm+1) are xed, is

m+ thus

1 ingiven

Gibbs(xm+1; )

=

1 1 ? e?

1

?

Z(xm+1; Z(xm;

m+1) m)

w~ ; m+1

=

1 1 ? e?

1

?

Zm+1 Zm

;
w~ ; m+1

(4)

wavheerraegehiow~ve;rmt+h1e

denotes integration noise values 1; : : :

over d m+1.

(w~

)

and

aTvheerapgreesseonvterfow~rmaunladtiomn+tr1eiantsainnotengsryamtimonestroivcewraw~y.aTndo

facilitate the subsequent calculations we can remove

this asymmetry by replacing the average over targets

and noise labelings by xing

bm=y =alnn(((e11q;u?iv2a;):l=e:n:t);

aamsve)arb2agovefeo.1vgNemro.taellWthpeaotdssotibhslioes

implies that

1?

=

1 1 + e?

:

(5)

Then since Zm = Z( m), from Equation (2) we have

Pr(

m) =

Z( m (1 + e?

) )m

:

(6)

Thus from Equation (4) we get

Gibbs(xm+1; )

=

1 1 ? e?

X Pr( m+1)
m+12f 1gm+1

1

?

Zm+1 Zm

=

1 (1 + e? )m+1(1 ? e? )

X Z( m+1)
m+12f 1gm+1

1

?

Z( Z(

m+1) m)

:

(7)

If the goal is to maximize the probability of a correct

prediction, it turns out that the Gibbs algorithm is

not the best algorithm to use. The best possible pre-

tdoictatilopnofsotrerimor+1prwoobualbdilbitey,ogbitvaeinnetdhbey

calculating the rst m training

eapxnroadbmcapoblmielsipt,yaorffionargl?lthw1~.i'ssOwtnhietahtthtwehnoeucclhdoorrpoesrseepsdoitcnhtdei+nog1utpopnoustt~xewmrii+toh1r

largest posterior probability. Since the noise rate

is at most one half, this clearly maximizes the proba-

bility of a correct prediction. This strategy is known

as Bayes optimal classi cation algorithm or Bayes al-

gorithm for short. In its general form, where is

not necessarily set according to the noise rate , the

algorithm is called the weighted majority algorithm

Lit89, LW89].

sIpthroeidws isctiltoehanarottnhtlhayitswBihsaeeynqeusZimavwalrgloeonnrgitthtomZmrmigahkt.esAalnittmleisatlagkeebrina

1 + e? 2

?

Zm+1 Zm

0:

rTithhums tihsethaeveerxapgeectgaetnieornaloizf2ation error for Bayes algo-

1 + e? 2

?

Zm+1 Zm

when the target w~ and the noise sequence m+1 are

randomly chosen, where (x) is the unit step function,

i.e. (x) = 1 if x 0, (x) = 0 if x < 0.

As above, we can write this in a more symmetric form:

Bayes(xm+1; )

=

1 + e? 2

?

Zm+1 Zm

=

1 (1 + e? )m+1

X Z( m+1)

m+1 2f 1gm+1

w~ ; m+1

(8)

1 + e? 2

?

Z( Z(

m+1
m)

)

When learning a perceptron, the hypothesis used by

Bayes algorithm cannot itself be represented by a per-

ceptron. However, we can construct a network with

one hidden layer for which the average generalization

ethrreorhicdodnevnerlgaeysert,ogoBeasyteos

as in

n, the number of units in nity. To do this we train

an odd number n of perceptrons independently us-

ing the Gibbs algorithm and make them hidden units.

Each hidden unit output (i), i = 1; : : :; n, is passed to

2Here

fZoZmrm+1

is (0).

we assume that the probability that zero, so it doesn't matter what value A more general treatment is given in

1+e2? = we choose HKS91].

faUusnixcntegidoEnoquutmaptuaijotnp=e(r3sci)ge,pnfto(rrPoanni=t1hxaedt(ict)oa)mragpsetutthceoesntchneepatlmow~aujto,prtuihttye. probability that k perceptrons make the right predictbiyonthoenb~xinmo+m1 iaanl ddisntr?ibkuttihoenwrong prediction is given

the expectation expectation can

obfeacpalocluylnaotmediaflrofumnctthioenpoofsiYtimve.

This inte-

ger moments of the this calculation, we

arabnbdreovmiavtaertihabeltehYemr.thTmo foamcielnittaotef

Ym by

ak =

n k

1 ? 1 ?1Z?me+?1=Zm

k

1 ?1Z?me+?1=Zm

n?k y(m; r) = hY r(xm+1; m+1(w~ ; m+1))iw~ ; m+1 xm+1
Averaging directly over possible label sequences m+1

=

1 (1 ? e? )n

n k

Zm+1 Zm

?

e?

k

1

?

Zm+1 Zm

n?k

TamvhiesurtsaagkteeheginepnpreorrebadaliibzciatlitiotionyntisheraPrtobkrtn=h=o0e2fcmtahkae.jolIertaitrfyonlivlnoogwteas lmtghoaarktitethshmae with n hidden units is given by

n(xm+1;

)

=

*bXn=2c

+
ak

k=0 w~ ; m+1

(9)

we obtain

y(m; r)

= =

*

X Pr(
m+12f 1gm+1
1*

m+1)

Z r (xm+1 ; Zr(xm;

m+1) +
xm) m+1

X Zr+1(xm+1; m+1)

(1 + e? )m+1 m+12f 1gm+1 Zr(xm; m)

for r = 0; 1; 2 : : :, using Equation (6).

+
(10)
xm+1

As n ! 1, Pbkn==02c ak converges to

The speci c calculation of y(m; r) for the perceptron alenndgtshuyb,sesqouwenetornelcyonskstertuchctiiotn. Fofirsnt(mon;e ) nisdssomtheawt hfoart

1 2

?

1

?1Z?me+?1=Zm

=

1 + e? 2

?

Zm+1 Zm

the perceptron,
Zr+1(xm+1; m+1)

wwhitehnerevseprec1t?Z1t?ome+w~?1=Zamnd6=

1=2. m+1,

Taking the expectation as in (9), we recover in

this limit the average generalization error of Bayes al-

gorithm given in (8). Thus by varying the number n

nori1ft=hh(mxi1dm;sd3+ew;n15i;t;uh:n):ai=:tvswe,riwBathgaeeydeg1se(e(xnxnmemer++aa1l1i;;szeaq)t)iu.=oennceGerirblobesras(rxnmnin(+xg1m;o+f)1a;lagno)d-,

=

Z

Yr+1
d

(w~a) mY+1 rY+1(e?

+ (1 ? e? )

((w~a

x~k)

p
k= N

))

a=1 k=1 a=1

(11)

tThehepreoxbparbesisliitoynth(Z1ar+t+e1r?(x+)m(1m+i1+n;1d)(mer++p11e))ndcaenntbreanindtoemrprreetpeldicaass
of the network weights and noise values give the same output 1; : : :; m+1 on inputs ~x1; : : :; ~xm+1.

2.4 Calculation of the Learning Curves

In the following we now assume that the instances

fx~r1o;mx~2;a:n: :N; ~x-mdi+m1enarseiondaralwpnroibnadbeiplietnyddenisttlryibauttiroann.doImn

particular, we assume that for each i and j, where

1xmabke(laejsn)siwo;tfjihtx~hakt,ztekNhro=e, cm1toh;eoe2ar;dnc:ia:na:ran;ttdemesis+aynns1itt,ceeaomcrmeoivpsia.oicr.nhidaeo.nnsrtceasennxCdsukoij(cmi.h)

and variThis that

the mean of the distribution on the instance space is at

the origin. For each n, the average generalization error

tfohnr(emtahv;eern)a=gheidodvneen(rxtumhn+eit1r;alena)dronxmimn+gs1ea,lelwgcothiroeitrnheomhfi~xxis1m;d+:e1:n:do;te~xenmdo+tbe1ys.

For each m let

Ym = Y (xm+1;

m+1(w~ ;

m+1))

=

Z(xm+1; Z(xm;

m+1) m)

From Equation (9) it is clear that for each n, the average generalization error n(m; ) can be written as

We use Equations (10) and (11) to calculate y(m; r).

However, we can calculate y(m; r) exactly only in the

limit of large instance space dimension N, and then

only by making some further assumptions on the dis-

tribution over the instance space and the prior den-

sity d on the hypothesis space. In particular, we

assume a = 1; :

that ::;r

for + 1,

almost drawn

all sets of randomly

r+ and

i1ndveepcetnodrsenw~tlay,

tuaicavc(ao~xrr)ida=itnegw~cetaontdx~ra=l,pltiNhme,itraatn=hdeo1or;me:m:v:,e;crit.e+o.r

with components 1, obeys the mulfor large N, the

mNjoiun!lttivp1aroribaitatbeisiGlietaxyuasdcsetilanynsiGtdyiasuotrfsisbtihauneti.ounaS,iinascneadpwipnerotaxhrieemlaiamstseiultymaas-

ing also that the mean of the distribution over ~x is

the origin, for all w~ the means of the corresponding

random satis ed

vfoarriaabwleidseucalaasrseozfedrois.trTibhuetsieonass.suFmorpetxioanms palree,

they hold asymptotically for large N when the distri-

obvuetriotnheovBeorotlheeanincsutabnecfe

s1pgaNce,

is discrete and uniform and the density d is a

symmetric function of the cartesian components w(j),

wtioitnhonnittheevsaprhiaenreceo.f TrahdisiuisncpluNde, sasa

uniform well as a

distribudiscrete,

uniform distribution on the Boolean cube. The same

irnessutaltnscehoslpdacfoeristhaelscoasceonwthineunotuhseadnidstruinbiufotiromn oonn tthhee

sphere.

bCbFdyyiosr=trQrw~i+abf1bu;C1:t=i:ijmo:gn;Naw~itso?rrni1+tx(h1-twev~hdaaerNClauiwwne~bdsnbyt)arr,aanNfnncoddercooos1mmpvaalvyrciaefaa.rrn;ioacbUmbe;nldemdeQra,rt+lrbeoitexu1tr,dhfoGweerhrant+euhersd1ee-

Wsviaaerniacaabsnlseusnmouwap(tc~xiao)rnr=,yQw~oauabti~xtsh=tephaeNvceoraavnagdreiuaobnv(ec~xre)to=hfetw~hmbe+r~xa1=nspdt oNinm-.

stance in (10) and its two possible outputs explicitly. The resulting expression can be

remex+p1r=essed1

as an average over targets and noise. We nd

y(m; r) = 2(2

)?

r+1 2

(1

+

e?

)?1

*

Z1 ?1

rY+1 a=1

dua(e?

+ (1 ? e?

)

(ua))
+

(det

Q)?

1 2

exp(?

1 2

X
a;b

ua(Q?1)abub)

;
Q

(12)

where we de a function f

ne as

the

auxiliary

average

hf(fQabg)iQ

for

Z Y dQabf(fQabg) (fQabg)

ab

and the density3

(12) reduces to the value at Q0( ) respectively.

Qab = Q1(

) and Qaa =

Let q( ) = Q1( )=Q0( )

and ( ) = pq( )=(1 ? q( )):

Inserting the replica symmetric ansatz into (12), one nally obtains

y( ;r)

=

2 1 + e?

Z 1 Dt e?
?1

+ (1 ? e? )H(t (

))]r+1

(13)

with

Dt = (2

)?1=2exp(?t2=2)dt and H(z) =

R1 z

Dt.

Note that the distribution of instances and targets en-

ter the result only through q( ). Before calculating

q( ) for a speci c distribution on the instance space

and prior measure d (w~), we derive a general relation

between Gibbs and Bayes.

tFehvieresrtyp,olleytn>oFm0(Yialmelt)PFbbken==d0a2encnyaoktpefortloyhmneolmSimeicaitltiooonff

Ym (such as 2.3) and for the expecta-

tion of F constant.

(TYhmis)

as m; N ! 1 with = expectation is with respect

m=N to the

held ran-

dom choice of the instances, target concept and noise

values. Equation (13) enables us to calculate F . In

particular, if

= F (Ym) = c0 + c1Ym + : : : + cnYmn

DR Qra+=11 d

m

(w~a)

Q
a

b

(Qab

?

N

?1(w~a

E
Cw~b)) w~

;

m

is the joint probability density, averaged over all target

then xm F

= c0y(

; 0) + c1y(

; 1) + : : : + cny( ; n):

concepts, noise and training instances, for the r(r + It then follows from (13) that

t1a)k=e2nwreaignhdtoemd liynnfreormprtohdeucptosstQeraiborofdris+tri1buvteicotnoras fwt~ear the rst m training examples.

Let y(m;

= m=N. r) as m; N

W!e

1willwciathlculahteeldthecolnimstaitnyt.(

; r) of To do

this, we make the crucial assumption of replica sym-

metry MPV87]. This means that for any pair a; b of

daNtisi?nt1ign(,wc~atanCrdewp~clboi)cnavbseercagonemds etfsoorsaealf-txayevpdeircsaaiglnignslegeq,uoier.edn.ecrepnoaofrnalmaubceettluesr-,

bQu1t(io)n,s

which only of ~x and w~

depends .

on

the

probability

distri-

F

=

1

2 + e?

Z 1 Dt H^ (t;
?1

)F

H^ (t;

)

;

(14)

where H^ (t; ) = e? + (1 ? e? )H(t ( )):

This result is easily extended to any function F that can be approximated by polynomials, including the function.

Replacing also Q0( ), under

the this

daiassguomnaplteiolenmtehnetswQhaoalebyavtehreagvealuine

3Here we use Dirac's function, which is the general-

ized function de ned as functions f) yields f(x)

=thRe?1k1erdntefl

that (for well
(t) (x ? t).

behaved

T(fao9nnor)(dmaaGpn;ipdb=bl)ys((1a1+tes4h?;e)mi?,s);ur,Naseinasnfdu!gtletrtB,1hsaleoyeetmwfsa(iectnthl(;etnh;)ga.=tth)TymHhdaee=(nln?Ngoefx,trbe)oarman=thdwEe1seqi?lumiomabHitltiiatao(rixnolny)sf

n( ; ) =

+ (2 ? 4 ) bXn=2c
k=0

n k

Z 1 H~ (t; k;
?1

)Dt;

(15)

where H~ (t; k; ) = Hk+1(t ( ))(1 ? H(t ( )))n?k:

For n = 1 and n = 1 this yields

Gibbs( ; )

Z1

= + (2 ? 4 ) Dt H(t ( )) 1 ? H(t ( ))]

?1

= + (1 ? 2 ) ?1 arccos(q( ))

(16)

and

Bayes( ; )

Z1

= + (2 ? 4 ) Dt H(t ( )) 1 ? 2H(t ( ))]

= + (1 ? 2 ) ??11arccos(pq( ))

(17)

From Equation (1), we can now also calculate the probability that these algorithms incorrectly predict the underlying noise-free classi cation fw~ (~xm+1). We get

Gibbs(

;

)

=

(

Gibbs( ; 1?2

)

?

)=

?1 arccos(q( )) (18)

and

the same verges to

lpim2.itOthne

ratio Gibbs( ; the other hand,

)= for

Bayes( = 0,

; ) conboth al-

gorithms have generalization error 0:5, which is equiv-

alent to random guessing. However, as grows positive, the generalization error of Bayes algorithm drops

much faster from its random guessing value than the

generalization error of the Gibbs algorithm. For small

we have

0:5 ? Bayes( ; )

(

(0:5 ?

Gibbs(

;

)))

1 2

:

Finally, we give results for a uniform spherical dis-

tribution on the instance space, corresponding uniform spherical

i.e. prior

Cdiisjtr=ibutijio,nanond

the target vector w~ (see Figure 2). In this case the or-

der parameter q( ) becomes identical to the Edwards-

Anderson parameter KS78]. It naturally appears in

the calculation of the averaged free energy F(m). This

is de ned by

F (m)

= =

? ?

Dhln (1 +

Zmiw~ ; m 1* e? )m

E
xm
X
m2f 1gm

Z(

m) ln Z(

+
m)
xm

(21)

Using the replica trick, this is given by

F (m)

=

?

nli!m1

@ @n

ln

*

X Zn(
m2f 1gm

+
m)
xm

By a calculation similar to that in GT90], one nds q( ) by extremizing the expression

Bayes(

;

)

=

(

Bayes( ; 1?2

)

?

)=

?1 arccos(pq( )) (19)

It should be noted that q( ) implicitly depends on the

noise rate . However, by eliminating q( ) one arrives

at the result

Bayes( ; ) = ?1 arccos cos1=2( Gibbs( ; )) :

(20)

Remarkably, this functional relation does not depend

on the noise rate . that the same is true functions of Gibbs( ;

From for all ).

Enq(ua;tio)n=(1(5n)(1?o;n2)e?

)ndass

IenGrailbiFbzsia(gtui;orne)ef1rororwrne=n(s1h;;o3w;)7c;o2nn1(v;e1r;g.e)Fs otaorslzaearrgoeffuonr,catthliloenng.eInon-f

f( ) = m;N!1lim;m=N= ? F (m)=N

=

1 2 +

ln(1 ? 2
1 + e?

q(
Z

)1) +H^12(tq;(

?1

) ) ln H^(t;

)Dt

(22)

where H^ (t; ) = e? + (1 ? e? )H(t ( )):

Plugging this value of q( ) into (18) and (19), and

solving for large and Bayes( ; )

,

wp1e2

nd that C( )= as

Gibgbso(es;to)

1,Cw(he)=re

C(

)

=

?

R1 ?1

z

exp(?

p2(1 ? 12z2) ln

2 )?1 + (1 ?

2

)H(z)]dz : (23)

Note that for all learning curve is

th?e1.asyAmspltoowtiecr

behavior of convergence

the (

)? 1 2

of

the

generalization

error

has

been

predicted

in GT90] for a deterministic learning algorithm that

tries to learn a perceptron with noise by minimizing

the number of errors on the rst m examples. Though

the type of noise discussed in GT90] is not the same as

ours, we expect that the use of a nonzero temperature

in the algorithm is essential in obtaining faster conver-

gence. A similar result has been given in Gyo90b].

The function C( )(1?2 )2 is shown in Figure 4. Since

C(0) 0:62, as a special case we obtain the generaliza-

tion error 0:62= =

without classi cation noise: 0:62N=m, as in OH91]. As

alGreiabbdsy(

; 0) noted,

tph2e.corresponding Bayes value is smaller by a factor of

In Figure 2 the generalization errors for the noise-free

case = 0 are depicted as functions of . As ap-

perraolaizcahteison0,etrhreordeovfiaBtaioynes0a:5lg?oriBthamyesf(ro;m0)poufrtehlye

genran-

dom guessing is

(2

=

3)

1 2

,

which

has

in

nite slope

at tity

is=li0n,ewarhienreaswfoitrhtshleopGeibb2s=alg2.orithm

this

quan-

The tain

positive moments of the more information than

rjaunstdotmhevdariiaebrelentYmvaclounes-

of generalization errors of the learning algorithms we

have examined. For example, consider the random

variable

1 1 ? e?

(1 ? Ym) ;

which is the probability that the Gibbs algorithm

makes the wrong prediction on the m + 1st classi ca-

tion label for xed training instances, target function

and noise values. uctuations of this

rTahnedodmistvraibriuatbiloenwohfenYmthegitvreasintinhge

instances, target function and noise values are drawn

at random. Using (14), from q( ) we can obtain the

complete probability density of xed m=N = . We denote this

Ydmenasistymb;yNf!(Y

1 ).

for

We nd for the = 0 case that

f(Y

)=

2Y (

)

exp

?

1 2

t2(1

?

(

)2)]

(24)

where t is the solution of Y = H(t ( )).

In Figure 5 we show f(Y ) for 3 values of q = q( ), namely q = 0:1; 0:3; 0:7. Here we have dropped the dependence of q on for convenience.

For small around Y

q, =

f12

is :

approximately

a

Gaussian

centered

f(Y )

q?

1 2

exp

"
?

1 2

Y

?

1 2

2#
2 =q

Thus for small q, which means small when = 0,

tphreoxviomluamteelyZemquisalpsairztei.tioTnheids

in two means

subvolumes of apthat the learning

algorithm (Bayes or Gibbs) does not do signi cantly

better than random guessing.

FafthsoecrYhadalenln!gvseaistlyu1d.erfsa(AomYftaq)qt<icg=ao12lel,ys21,t,thaotenhdidenefnbonseriihttyaayvllfaio(svYraYlou)fegst!ohoeefs1dq.toe>nTzsehirt12iyos,

means that it is more likely that there is only a small

rdtehidcetuilcnetagironcnoinrirgneacttlhglyeo.rviItonhlumfmacheta,fsrfooamrhaiZgnhymertpo>rZo0bm,aa+bs1il,qitay!nodf1tphtrhuees-

ppreorfbeacbtiglietnyetrhaalitzaYtmion

1 in

? converges this limit.

to

zero,

giving

The largest this case (

)u=ctu1aatinodnsfroofmYEmquaaptpieoanr

for (24)

qw=e si21m. pIlny

get

f(Y ) = 2Y

for 0 Y 1.

We have also estimated the density f for nite m; N

by performing numerical simulations on perceptrons.

The can

bcealceuaslailtyiopnerofforZmmedatwhen=

0 for a the prior

xed sample distribution

counbtehef

w1egiNgh.t

vectors w~ Using this

is uniform on the Boolean distribution on the weight

vectors, when the instances are distributed uniformly

on the sphere, replica symmetry is known to be valid at

= 0 for values of up to 1:245. Above this value a

discontinuous transition to perfect generalization takes

place Gyo90a]. Figures 6 and 7 display results of our

simulations for m = 4; N = 14 and m = 16; N = 20 re-

spectively, averaged over many samples. The smooth

curves are the theoretical predictions from Equation

(24). The histograms represent the experimental re-

sults. The range of Y was partitioned into equal width

bins. The height plotted for each bin the fraction of

times the value of Y lands in the bin divided by the

width of the bin.

csoItcapasipselionasglGistioebbilsni(mteir;tes)ti!anngd0tooBrsatuyed!sy( t21h; ei)nlehmaarvonerinetghdeceutaarsivyle.msIpninttoththiiecs Gibbs( ; ) gGibbs( 2 )

and Bayes( ; ) gBayes( 2 );

bwyheerxepathnedifnugn(c2ti2o)nfsorgGsmibbasllan.d gBayes are calculated This high temperature and high noise limit results in a much simpler equation for q( ) when the distribution on instances and prior on weights are both uniform on the sphere:

s

q( )

1 + q( ) 1 ? q( )

2 2:

SFoolrvin2g n!um1eriwcaellynwde obtain the results of Figure 3.

Gibbs( ; )

4
2

1 (1 ? 2

)2

;

which is the dashed line in Figure 3. It follows from this and Equation (1) that

Gibbs( ; ) + 2

+

(1

1 ?2

)

:

In STS90] related results are given for the Gibbs algorithm in a general but somewhat di erent setting.

3 Conclusion

Our results show that in the limit the relationship be-

tween the generalization error of the Bayes and the

Gibbs algorithms is independent of the particular as-

sumptions made about the densities used to choose the

target perceptron and to generate the ples, as long as the latter distribution

thraasin~0inmg eexanam, a-

central limit theorem holds and the replica symmetry

assumptions made above are valid. This also holds for

the approximations to Bayes algorithm we have de-

ned using neural nets with a layer of hidden units

and a majority gate output unit as well. Our experi-

mental results show the formulas derived in the limit

are already fairly accurate for relatively small sample

size and instance space dimension, at least for some

simple cases. We expect that the quantitative relation-

ships shown in Figure 1 are quite robust, and that in

practice, this use of 2-layer nets with majority output

may give better generalization performance in many

circumstances.

Acknowledgements
Helpful discussions with Haim Sompolinsky, Naftali Tishby and Michael Kearns are greatfully acknowledged. D. Haussler's research was supported by ONR grant N00014-91-J-1162. Part of this work was done while M. Opper was visiting the Physics department of the University of California at Santa Cruz. He would like to thank them for their hospitality.

References
BH89] E. Baum and D. Haussler. What size net gives valid generalization? Neural Computation, 1(1):151{160, 1989.
DMW88] Alfredo DeSantis, George Markowski, and Mark N. Wegman. Learning probabilistic prediction functions. In Proceedings

of the 1988 Workshop on Computational Learning Theory, pages 312{328, San Mateo, CA, 1988. Published by Morgan Kaufmann. DSW+87] J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. Hopeld. Automatic learning, rule extraction and generalization. Complex Syst., 1:877{ 922, 1987.

Gar88] E. Gardner. The space of interactions in neural networks. J. Physics A, 21:257{270, 1988.

GD88] E. Gardner and B. Derrida. Optimal storage properties of neural network models. J. Physics A, 21:271{284, 1988.

GT90] G. Gyorgyi and N. Tishby. Statistical theory of learning a rule. In K. Thuemann and R. Koeberle, editors, Neural Networks and Spin Glasses. World Scienti c, 1990.

Gyo90a] G. Gyorgyi. First order transition to perfect generalization in a neural network with binary synapses. Phys. Rev. A, 41:7097, 1990.

Gyo90b] G. Gyorgyi. Inference of a rule by a neural network with thermal noise. Phys. Rev. Lett., 64:2957, 1990.

HKS91]

D. Haussler, M. Kearns, and R. Schapire. Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. In Proceedings of the Fourth Workshop on Computational Learning Theory, 1991.

HLW88] D. Haussler, N. Littlestone, and M. Warmuth. Predicting 0,1-functions on randomly drawn points. In Proceedings of the 29th Annual Symposium on the Foundations of Computer Science, pages 100{109. IEEE, 1988.

HS90] D. Hansel and H. Sompolinsky. Learning from examples in a single-layer neural network. Europhys. Lett., 11(7):687{692, 1990.

KS78] S. Kirkpatrick and D. Sherrington. In nite ranged models of spin glasses. Phys. Rev. B, 17:4384, 1978.

Lit89] N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. PhD thesis, University of Calif., Santa Cruz, 1989.

LTS89]

E. Levin, N. Tishby, and S. Solla. A statistical approach to learning and generalization in neural networks. In R. Rivest, editor, Proc. 2nd Workshop on Computational Learning Theory. Morgan Kaufmann, 1989.

LW89] N. Littlestone and M. Warmuth. The weighted majority algorithm. In 30th Annual IEEE Symposium on Foundations of Computer Science, pages 256{261, 1989.

Maa91]

Wolfgang Maass. On-line learning with an oblivious environment and the power of randomization. In Proceedings of the Fourth Workshop on Computational Learning Theory, 1991.

Mit82] T. M. Mitchell. Generalization as search. Art. Intell., 18:203{226, 1982.

MPV87] M. Mezard, G. Parisi, and M.A. Virasoro. Spin Glass Theory and Beyond, volume 9 of Lecture Notes in Physics. World Scienti c, 1987.

OH91] M. Opper and D. Haussler. Generalization performance of Bayes optimal classi cation algorithm for learning a perceptron. Physical Review Letters, May 1991. to appear.

OKKN90] M. Opper, W. Kinzel, J. Kleinz, and R. Nehl. On the ability of the optimal percepton to generalize. J.Phys.A.: Math.Gen., 23:L581{L586, 1990.

STS90] H. Sompolinsky, N. Tishby, and H.S. Seung. Learning from examples in large neural networks. Phys.Rev.Lett., 65:1683{ 1686, 1990.

VJP89] F. Vallet, J.Cailton, and P.Refregier. Linear and nonlinear extensions of the pseudo inverse for learning Boolean functions. Europhys.Lett., 9:315{320, 1989.

Vov90]

Volodimir Vovk. Aggregating strategies. In Proceedings of the 3nd Workshop on Computational Learning Theory, pages 371{383. published by Morgan Kaufmann, 1990.

Figure captions

Figure 1: Generalization errors of the n-hidden utnhGnaeilibtlbcisnan(seee)t)wcnfooorr=rrkens1wp=oitn(1hld;os3mw;tea7os;jtto2hrc1eiu;tyG1rvieo.b)ubTtcsphoauerltrgceaosasrpseiothannmd=fsuint1tscoe(tldiBfo,iaanaygneoods-f
optimal algorithm. The results are independent of , so this parameter is omitted.

Figure 2: Generalization errors of the n-hidden unit

neftowronrk=w1it;h3;m7a; 1jor,iatyssouumtpinugt

at = 0 as a a spherically

function of symmetric

distribution on target vectors and instances. The case

n = 1 (highest solid curve) was recently calculated in

cGasTe9n0]=an1d

corresponds to the Gibbs algorithm. The (lowest solid curve) corresponds to Bayes

optimal algorithm.

Figure 3: Learning curves for Gibbs (upper solid

line) and Bayes (lower solid line) algorithm in the

high temperature limit ! 0, assuming a spheri-

cally symmetric distribution on target vectors and in-

smtaanticoens.g(Th2 e

)dashe24d

line is for the

the asymptotic Gibbs learning

approxicurve.

Figure 4: The function C( )(1 ? 2 )2 de ned by
Equation (23).

Figure 5: Probability density f(Y ) for q( ) = 0:1
(bell-shaped curve) q( ) = 0:3 ( atter curve) and q( ) = 0:7 (curve peaked at 1).

Figure 6: Probability density from numerical simu-
lations of perceptrons with weight vectors on Boolean cube, for m = 4 and N = 14. The smooth curve is the theoretical prediction. See further explaination in text.

Figure 7: Same as Figure 6, but with m = 16 and
N = 20.

