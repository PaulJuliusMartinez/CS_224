UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

Video Motion Capture

Christoph Bregler

Jitendra Malik

Computer Science Division University of California, Berkeley
Berkeley, CA 94720-1776 bregler@cs.berkeley.edu, malik@cs.berkeley.edu

Abstract
This paper demonstrates a new vision based motion capture technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. It does not require any markers, body suits, or other devices attached to the subject. The only input needed is a video recording of the person whose motion is to be captured. For visual tracking we introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degreesof-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and reanimate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.
CR Categories:
Keywords: Computer Vision, Animation, Motion Capture, Visual Tracking, Twist Kinematics, Exponential Maps, Muybridge
1 Introduction
In this paper, we offer a new approach to motion capture based just on ordinary video recording of the actor performing naturally. The approach does not require any markers, body suits or any other devices attached to the body of the actor. The actor can move about wearing his or her regular clothes. This implies that one can use historical footage­motion capture Charlie Chaplin's inimitable walk, for instance. Indeed in this paper we shall go even further back historically and show motion capture results from Muybridge sequences­the first examples of photographically recorded motion [15].
Motion capture occupies an important role in the creation of special effects. Its application to CG character animation has been much more controversial; SIGGRAPH 97 featured a lively panel debate[4] between its proponents and opponents. Our goal in this paper is not to address that debate. Rather we take it as a given that motion capture, like any other technology, can be correctly or incorrectly applied and we are merely extending its possibilities.
Our approach, from a user's point of view, is rather straightforward. The user marks limb segments in an initial frame; if multiple video streams are available from synchronized cameras, then the limb segments are marked in the corresponding initial frames in all of them. The computer program does the rest­tracking the multiple degrees of freedom of the human body configuration from frame to frame.

Attempts to track the human body without special markers go back quite a few years ­ we review past work in Sec. 2. However in spite of many years of work in computer vision on this problem, it is fair to describe it as not yet solved. There are many reasons why human body tracking is very challenging, compared to tracking other objects such as footballs, robots or cars. These include
1. High Accuracy Requirements. Especially in the context of motion capture applications, one desires to record all the degrees of freedom of the configuration of arms, legs, torso, head etc accurately from frame to frame. At playback time, any error will be instantly noticed by a human observer.
2. Frequent inter-part occlusion During normal motion, from any camera angle some parts of the body are occluded by other parts of the body
3. Lack of contrast Distinguishing the edge of a limb from, say the torso underneath, is made difficult by the fact that typically the texture or color of the shirt is usually the same in both regions.
Our contribution to this problem is the introduction of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation scheme. This formalism will be explained fully in Section 3. The advantage of this particular formulation is that it results in the equations that need to be solved to update the kinematic chain parameters from frame to frame being linear. Also the only parameters that need to be solved for are the true degrees of freedom and pose parameters­there are no intermediate stages which may be unnecessarily hard. For instance recovering the local affine motion parameters of each and every limb segment separately is harder than the final goal of knowing the configuration of all the joints from frame to frame­the fact that the joints are constrained to move together reduces considerably the number of degrees of freedom. This in turn provides robustness to self-occlusions, loss of contrast, large motions etc.
We applied this technique to several video recordings of walking people and to the famous photo plates of Edweard Muybridge. We achieved accurate tracking results with high degree-of-freedom full body models and could successfully re-animate the data. The accompanying video shows the tracking results and the naturalness of the animated motion capture data.
Section 2 reviews previous video tracking techniques, section 3 introduces the new motion tracking framework and its mathematical formulation, section 4 details our experiments, and we discuss the results and future directions in section 5.
2 Review
The earliest computer vision attempt to recognize human movements was reported by O'Rouke and Badler [16] working on syn-

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

thetic images using a 3D structure of rigid segments, joints, and constraints between them.
Marker-free visual tracking on video recordings of human bodies goes back to work by Hogg and by Rohr [8, 18]. Both systems are specialized to one degree-of-freedom walking models. Edge and line features are extracted from images and matched to a cylindrical 3D human body model. Higher degree-of-freedom articulated hand configurations are tracked by Regh and Kanade [17], full body configurations by Gravrila and Davis [7], and arm configurations by Kakadiaris and Metaxas [11] and by Goncalves and Perona [5]. All these approaches are demonstrated in constrained environments with high contrast edge boundaries. In most cases this is achieved by uniform backgrounds, and skintight clothing of uniform color. Also, in order to estimate 3D configurations, a camera calibration is needed. Alternatively, Weng et. al demonstrated how to track full bodies with color features [20], and Ju et. al showed motion based tracking of leg configurations [10]. No 3D kinematic chain models were used in the last two cases.
To the best of our knowledge, there is no system reported so far, which would be able to successfully track accurate high-degree-of freedom human body configurations in the challenging footage that we will demonstrate here.

3 Articulated Tracking
There exist a wide range of visual tracking techniques in the literature ranging from edge feature based to region based tracking, and brute-force search methods to differential approaches.
Edge feature based tracking techniques usually require clean data with high contrast object boundaries. Unfortunately on human bodies such features are very noisy. Clothes have many folds. Also if the left and right leg have the same color and they overlap, they are separated only by low contrast boundaries.
Region based techniques can track objects with arbitrary texture. Such techniques attempt to match areas between consecutive frames. For example if the area describes a rigid planar object, a 2D affine deformation of this area has to be found. This requires the estimation of 6 free parameters that describe this deformation (x/y translation, x/y scaling, rotation, and shear). Instead of exhaustively searching over these parameters, differential methods link local intensity changes to parameter changes, and allow for Newton-step like optimizations.
In the following we will introduce a new region based differential technique that is tailored to articulated objects modeled by kinematic chains. We will first review a commonly used motion estimation framework [2, 19], and then show how this can be extended for our task, using the twist and product of exponential formulation [14].

3.1 Preliminaries
Assuming that changes in image intensity are only due to translactioonnseocfultoivcealtiimmeagfreaminetesn  saitnyd, a ¢¡¤pa£ racmanetbriecdiemscargiebemd obtyiotnhebfeotwlloewening equation:

§¥ ©¦ ¨ ¡  ©¦ ¨§   ! "  ¡#% $ ©¦ ¨ & §  !    '¡ £ )(0§¥ ©¦ 1¨  §    (1)

2

% 

¥§©¦ ¨1 §    ©¦ ¨  3 "1 ! 

is%t$ h¦©e1¨ im§a g e©¦5 ¨147i6n"t8ednessictyr.ibTehsetmheotpioixneml oddisepllac©¦ e1¨ m"§ent

) ( de-

pendent on location

and model parameters . 9Fo( r

e2 @§xAam @8pB le@D, C

a@DE 2"DF 

aFffi$ n74 6 e

motion model is defined as

with

parameters



¦©¨ § G I( H

@§PA 8@ B @ C @ RE TQ S

H

¨ 

Q¡

H

F F$

Q

(2)

The first-order Taylor series expansion of (1) leads to the commonly used gradient formulation [12]:

V¥ U ¦©¨ " 8 ¡ 2 ¥  ¦©¨  8 ! "¥ $ ¦©¨ & 8 54 S  ¦©¨ "§  ¥Y`U ©¦ ¨ a  is the temporal image gradie¦©n¨t aa n d 2 ¥ 

GX( W ¦©¨1 8 ! "¥

$

(3) ¦©¨ 85 4

is the spatial image gradient at location model of b degrees of freedom (in case

of

the

. Assuming a affine model b

mo(0tioc n )

and a region of dfegb pixels, we can write an over-constrained

set of d equations. For the case that the motion model is linear (as

in the affine case), we can write the set of equations in matrix form (see [2] for details): h

sh ruwt yv x8

S rup¡ twvq i ( Wi

(4)

where

, and q i

. The least squares solution to

(3) is:

h h  A h

( 0 6 S

S 6 qi

(5)

Because (4) is the first-order Taylor series linearization of (1),

we linearize w arping the

iamroaugned¥§th¦  e ¡new£ 

solution and iterate. This using the motion model

is done by parameters

found by (5). Based on the re-warped image we compute the

new image gradients (3). Repeating this process is equivalent to a

Newton-Raphson style minr imization.

p¦©¨ro&Aba acboinlivtyenmieanstkre p¦©r¨es& e8n tatio2 DWn

o£ f4

the .

¦©s¨h"a8pe) (of

a£ ndiemclaagreesrethgaiot npiixseal

is part of the region. Equation (5) can ¦©b¨1e 8m odified, such

t ha¦©¨t i t8 w :eights the (

contribution h
 ©¦  S  6

of S

pixel h  

location A
S ¦© S

h

according to  6 q i (6)



is an d Rd

s ¦©"5  ( diagonal matrix, with



©¦ §¨ " d .

We assume for now that we know the exact shape of the region.

For example, if we want to estimate the motio n parameters for a human body part, we supply a weight matrix that defines the

image support map of that specific body part, and run this estima-

tion technique for several iterations. Section 3.4 describes how we

can estimate the shape of the support maps as well.

Tracking over multiple frames can be achieved by applying this

optimization technique successively over the complete image se-

quence.

3.2 Twists and the Product of Exponential For-
mula In the following we develop a motion model  ©¦ ¨"§1  for a 3D kinematic chain under scaled orthographic projection and show how these domain con straints can be incorporated into one linear system similar to (6). will represent the 3D pose and angle configuration of such a kinematic chain and can be tracked in the same fashion as already outlined for simpler motion models.

3.2.1 3D pose

t

The pose of an object relative to the cameraC frame can be repre-

sented as a rigid body transformation in using homogeneous

coordinates (we will use the notation from [14]):

A"q A A"q B A"q C F  sut

ge f (ih

S eYj

with

hkm( noul p p
p

B!q A Cgq A
W

p p
p

!B q B gC q B
W

p p
p

B!q C Cgq C
W

F$ F¢r v £

(7)

2

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

ic 2 n¨ agSmf e 2"¨sje crf f aa( l eVqffdrf a42 ¨6 mo£ j r.74et"6 h gojiegs trqtsahj ppe hr£coi2 4coF j6 er prceritFsosepjd$ aeo" cinF¢pntdirYotooi4inn6ngttwhpieinothiimtnhstaeciganoelebthpj eeo,cictnthatfmer2a¨pemro¢ a¡ ienf tarane ¢mdf¡ ei74e .n6 f Uth((se-

The 3D translation

can be arbitrary, but the rotation

matrix:

¤ "A q A A"q B A"q C r¨§©

£(

p p

B!q A Cgq A

p p

B!q B Cgq B

p p

!B q C Cgq ¦C ¥

¦ 

(8)

r§ p p p

has onlyh 3 degree¦ s  of freedom. Therefore the rigid body transfor-

mation

has a total of 6 degrees of freedom.

Our goal is to find a model of the image motion that is param-

eterized by 6 degrees of freedom for the 3D rigid§m© otion and the

scale factor   for scaled orthographic projection. Eule¦ r a ngles are

commonly used to constrain the rotation matrix to

, but they

suffer from singularities and don't lead to a simple formulation in

the optimization procedure (for example [1] propose a 3D ellip-

soidal tracker based on Euler angles). In contrast, the twist repre-

sentation provides a more elegant solution [14] and leads to a very

simple linear representation of the motion model. It is based on the

observation that every rigid motion can be represented as a rotation

around a 3D axis and a translation along this axis. A twist  has

two representations: (a) a 6D vector, or (b) a    matrix with the


upper





component as a skew-symmetric matrix:

(


nlonnn

A

B

C

! ! !

 $ r

us tttt v

(fonl or  "

W !r !$ W

!r W ! W

!$ !
W W

A us t B Cv W



(9)



! is a 3D unit vector that points in the direction of the rotation

axis. The amount of rotation is specified with a scalar angle # that

is multiplied by the twist: $# . The component determines the lo-

cation of the rotation axis and the amount of translation along this

axis. See [14] for ar d&tet%ailed geometric interpretation. For simplicity, we drop the constraint that ! is unit, and discar'rd §th e # coeffi-

cient. Therr eft or% e 

.

h

It can be shown [14] that for any arbitrary

¦   there

exists a 

twist representation. h

A twist can be converted into the representation with follow-

ing exponential map:

A"q A "A q B A"q C F 

kh (

nRlo p p p

B!q gC q
W

A A

( (0( 1 ) 2 ¡

¡

p
p p

!B q B gC q B W
¦ "



B

p p p
¡

!B q gC q
W

C C
¦

"



F$ F¢r £ C
¡

 " 53 4

4

st v
6776 6

(10)

3.2.2 Twist motion model

At this point we would like to track the 3D pose of a rigid object un-

der scaled orthographic projection. We will extend this formulation

oAi¦©n§¨f ¢pta¡honei"no ntb8 e¡jYexej tctsineicstthidoeenfiontboejdeacatksfinr2 ae  mm  ea6 tii4 sc6

ch( ain2  

r epA r esBen taC tio! n .

! T$ he!

pYr o4 s6 e .

projected to the image location

with:



H

¨ 8¡  ¢ ¡

Q

(IH

£ W

W £

W W

W W

QTS   S (01 ) S eYj

(11)

©¦ ¨3¢ ¡  8¡y

   3¡ £

The image motion of point

from time to time

is:

H

%  %$

Q

(IH

¨ 8¡ ¦   0¡ £ G   8¡ ¦   ¡0£ G

¨ ¢¡ ¦     ¢ ¡ ¦   

Q

(IH

£ W

W £

W W

W W

TQ S@9   ¦   '¡ £  S U(01B) A DC AFE S e j 

  ¦    S U(0G1 ) A E S e jIH

(IH

£ W

W £

W W

W W

TQ SP9 ¦ £ ¡

 S G  Q (0S1 ) R ¦2 H

S   ¦    ge f

(12)

¦ 1  ¡'£ P(   ¦   '¡ £ ) w( it  h¦    S  ¦" £ ¡   Q 

¦  ¡ Q
" "

Using the first order Taylor expansion from (10) we can approximate:

¦£ ¡

 SG  Q T( V1 ) U

¦£ ¡

 GQ S 2 ¡

¦£ ¡

 S GQ "

(13)

and can rewrite (12) as:

H

% $

Q

(IH

 Q
! rQ

 ! rQ
 Q

! $Q  ! Q

QA BQ

TQ S e f

(14)

with

! ¦   ¡'£ )(

! ¦  ¡

£

£¡

S !0Q 
 Q

¦ 1  ¡ £ )(

¦  ¡

£

£¡

S
 Q

Q

 ( 2  Q  motion from

QVA  BQ  ! tim e  

Q  to

! $Q  !  )¡  £

rQ .

74 6 codes the relative
Note that (14 ) does

nscoatliencalnuddetwiCsQ t.

Translation in the W direction of the camera frame is not measurable

under scaled orthographic projection.

¦©¨  "  

Equation (14) describes the im age motion of a point

 in

tYe ef r©¦m5 s

of the motion in the camera

parameters frame. The

3DanpdoitnhteYe cf o¦©r5 r©¦ e¨ issp" oc on md ipnugte3dDbpyoiinn-t

tersecting the camera ray of the image point

with the 3D

model. In this proximated by

paper we assume that ellipsoidal 3D blobs.

TthheerbeofdoyreseVefgmisetnhtes

can be apsolution of

a quadratic equation. This computation has to be done only once

for each new image. It is outside the Newton-Raphson iterations.

It could be replaced by more complex models and rendering algo-

rithms.

Inserting (14) into (3) leads to:

¥VU ¡ ¥  S 2  GQ V rT! Q  $0! Q  Q"A 4 S Ye f ¡ ¥ $ S 2 r0! Q   GQ Y  !0Q  BQ 4 S eYf X( W

X

¥V`U ©¦ 5 `¡ Y  S 2    QA  BQ  ! Q  ! $Q  ! rQ 4 6 (0W

(15)

¥ Uba (0¥ U ©¦ ¨ "  d! " ¥  a (0¥  ¦©¨§"  d !" ¥ $ a 0( ¥ $  ©¦ ¨§"  d  with

For d This can

bpeiwxerlittpeonsiitniomnsathrwixe


have form:

d



equations of the form (15).

S  ¡q i ( c

(16)

with h

Y A st

(

nol

YB v6¢76 6

v

Y

¥YU!¦©¨ A  A  s t

and

qi (

nlo

¥ U ¦©¨ B  B  v v76 76 6

v

¥ U ¦©¨  

 Finding the least-squares solution (3D twist motion ) for this equation is done using (6).

3

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

Body Frame

'% @& 9 Ais the Given a

padojionitne t

f

transformation associated with .1 on the 'th segment of a kinematic

chain,

its

motion vector in the image is related to the angular velocities by:

§B # #  # D CH

%  $

Q

I( H

£ W

W £

W W

W W

Q%S

QA
"

S#

A

¡

BQ
"

S#B

¡

¡76 ¢6 6

SQ
" #

S Ye f

Pose G0

 11

22

Camera Frame Figure 1: Kinematic chain defined by twists

FERech all (15) relates the image motion of a point e

f

(23) to changes in

# # # Gpose
t2 o  Q t heQA



. We combine coBQ m !biQ n e! d$Q v e! crQ to r

oA (1f5p)B o as¢6 en76 6dch(2a n347g)6 et:oanredlaatnegtuhlearimchaagnegemotio( n

IH # # #Y¥ U ¡ 

Y   S 2  

QYA 

BQ  0! Q  h $!0Q  r0! Q 4 6

¡





S

2
#

A

 #

B



76 676 #

4 6 (0W (24)

P G2  34 S ¡pq i ( c

(25)



with

H A us t

P HH(monl

B v v76 676

h and  q i as before

3.2.3 Kinematic chain as a Product of Exponentials

So far we have pc arameterized the 3D pose and motion of a body

segment by the parameters of a twist  . Points on this body

  ¢¡segment in a
era frame by

canonical objh ect the mapping

f( ram1e) .

are transformed into a Assume that a second

cambody

segment is attached to the first segment with a joint. The joint can

tbthheiesdarexofiitsna,teidaonnbdyaaxaipsnoiiannxttihse eAoofobnrjoettchatteiforaanxmiisne(btfihygeuaroe3bDj1e)cu.tnTfirthavimseceist.oarW!seoA

define along called

revolute joint, and can be modeled by a twist ([14]):

A(


H



!

A
!

A

eA

Q

(17)

A
¥£ ¤ ¡ §©¦ ¨  ¦A rotation of angle # around this axis can be written as: ( 1)

(18) (19)

The global mapping from object frame points on the first body seg-

ment into the camera frame is described by the following product:

£   ¡ ¦ ¨  ¦¦ A  (

#
£e f

(

h S V1 )

¦ #

A



S

e

j

(20)

If we have a chain of b ¡£ segments linked with b joints (kine-
matic chain) and describe each joint by a twist  , a point on segment is mhapped from the oA bjeBct frame into the camera frame de  pendent on and angles # , # , ..., # :

£     ¡ ¦ ¨  ¦ ¡  ¨   ¡  ¨ ¦ A  B  6776 6 G ( h

##

#

S V1 )

S 1)

S S67¢6 6 1 )

(21)

This is called the product of exponential maps for kinematic

chains.

# # #    "! tvhealTot chisietiaevseliln# oeAcaitr# yBcoo67mf76 6 ab#isneag(tsimoeneen[ot1f4t]wcfoiasrntsthb eeQA

d eBQ scr76 76i6b e dQ

with and

derivations):

a twist the angular

# # #( $!  '% &  Q (  0( 1 )3¦ 24¦ ¨ 5 5 5 ¨( 1 7)6 ¦ 2 86 ¦ 

 QA S # A ¡

BQ


S#B

¡

S76 76 6 Q 

#



(22)

H H H H
( 2  A   B  76 676  4

H  S RTQR  (

2 ¥  "¥ $ 4 S H

£ W

W £

W W

W W Q S " S e f

W

 if pixel is on a segment that

is not affected by joint 

G P P PThe

least

squares soh lution (  ¦ 2  34 6

to h(25) is: S 2  34 

A

S

h 2

 34 6

S qi

(26)

G is the new estimate of the pose and angular change between

two consecutive images. As outlined earlier, this solution is based

on the assumption that the local image intensity variations can be

approximated by the first-order Taylor expansion (3). We linearize

Garound¥§t¦h  i¡s n£ e w solution and iterate. This is done in warping the

image

using the solution . Based on the re-warped image

we compute the new image gradients. Repeating this process of

warping and solving (26) is equivalent to a Newton-Raphson style

minimization.

3.3 Multiple Camera Views

In cases where we have access to multiple synchronized cameras,

we can couple the different views in one equation system. Let's

Uassume we have different camera views at the same time. View

V corresponds to following equation system (from (25)):

XW W Y #h P #2



4 S lnonn

f A B

s ttt ¡

qif (

c

 v6¢676

#

(27)

Y f ( 2   fQ  QA"q f  !BQ q f  ! Q q f  ! $Q q f  ! rQ q f 476 describes the pose seen Vfrom view . All views share the same angular parameters, because
`ba 9d¢c e'fq hpg fi f r s ¢c et7u f7u uwhv r1   , and

4

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

the cameras are triggered at the same time. We can simply combine

Uall equation systems into one large equation system:

YYh

h
¤ Pc 76 6¢6 c

P ¤ Y #nlo c 67¢6 6

  h67¢6 6 c
¢6 676 67¢6 6 6¢676

P # #¡c c 67¢6 6

¡  £¢676¢6

sut v

S

nlnnonnnnn

A B
76 76 6
A B 6776 6

s tttttttt ¡ v



q i A sut

¢nlo

q iB
6776 6

v

(

c

qi

(28) Operating with multiple views has three main advantages. The estimation of the angular parameters is more robust: (1) the number of measurements and therefore the number of equations increases with the number of views, (2) some angular configurations might be close to a singular pose in one view, whereas they can be estimated in a orthogonal view much better. (3) With more camera views, the chance decreases that one body part is occluded in all views.

3.4 Adaptive Support Maps using EM

As in (3), the update can be constrained to estimate the motion only
 in a weighted support map for each segment using:

G  P P  P(   ¦©

h S2

 §4  6

h S2

  A  34 S ©¦ 

h S 2  34  6 q i (29)

We approximate the shape of the body segments as ellipsoids,

and can compute the support map as the projection of the ellipsoids

into the image. Such a support map usually covers a larger region,

including pixels from the environment. That distracts the exact mo-

tion measurement. Robust statistics would be one solution to this

problem [3]. Another solution is an EM-based layered represen-

tation [6, 9]. It is beyond the scope of this paper to describe this

method in detail, but we would like to outline the method briefly:

G G We start with an initial guess of the s upport map (ellipsoidal pro-
jection in this case). Given the initial , we compute the motion estimate (M-step). Given such a we can compute for each pixel

Glocation the probability that it complies with the motion model de-
fined by . We do this for each blob and the background (dominant

tmioontioton)£

and normalize the sum . This results in new

of

all probabilities per maps that are better

pixel loca"tuned" to

the real shape of the body segment. In this paper we repeat the EM

iteration only once.

3.5 Tracking Recipe

We summarize the algorithm for tracking the pose and angles of a

 kinematic chain in an image sequence:

¤

Input:

¥§¦    ¥§¦  3¡0£  h ,,

¦¦¥V!

#

A

¦

 

! 

#

B

¦

 

!

76 76 6

#



¦  

(Two images and the pose and angles for

 ¤

the first h
Output:

image).
¦¦¥ ¨¡ § ! A #

¦  1'¡ £

! 

#

B

¦  3¡0£

!

676¢6

#



¦ 1  ¡'£  .

(Pose and angles for second image).

©¦ ¨    

1.

C§¥ o¦   m pute the

for each 3D point

Veif m¦©a&  ge(ulsoicnagtieolnlipsoidsin

or more complex models and rendering

algorithm).

2. Compute for each body segment the
support map © .

E Eh $  3. Set

¦ 1  ¡'£  a ( h

¦ 

 ,



a ¦  1'¡ £  a ( ¦    # #.

4. Iterate:

(a) Compute spati¥ U`ot¥ em¥ p$ oral image

gradients:

.

G(b) Estimate using (29)

E E ¡ ¦ 1)(c) Update  ¦  1¡'£  a (  ¦  1¡ £  S ¦ £ ¡   Q  S  R R

"   # (d)  " E A  A  (e) £ 

¦ 1  ¡ £  a ( ¦ 1  '¡ £  ¡

Update #

#

Warh p by

t¦ 1  h¡ e £ r eS gi¦o 1n¡

i£ n sS i5¦ h deA    © S

#.

¦

 o  "f

¥§A ¦

R  ¡

.

3.6 Initialization
The visual tracking is based on an initialized first frame. We have to know the initial pose and the initial angular configuration. If more than one view is available, all views for the first time step have to be known. A user clicks on the 2D joint locations in all views at the first time step. Given that, the 3D pose and the image projection of the matching angular configuration is found in minimizing the sum of squared differences between the projected model joint locations and the user supplied model joint locations. The optimization is done over the poses, angles, and body dimensions. Example body dimensions are "upper-leg-length", "lowerleg-length", or "shoulder-width". The dimensions and angles have to be the same in all views, but the pose can be different. Symmetry constraints, that the left and right body lengths are the same, are enforced as well. Minimizing only over angles, or only over model dimensions results in linear equations similar to what we have shown so far. Unfortunately the global minimization criteria over all parameters is a tri-linear equation system, that cannot be easily solved by simple matrix inversions. There are several possible techniques for minimizing such functions. We achieved good results with a Quasi-Newton method and a mixed quadratic and cubic line search procedure.
4 Results
We applied this technique to video recordings in our lab and to photo-plate sequence of Eadweard Muybdrige's motion studies.
4.1 Single camera recordings
Our lab video recordings were done with a single camera. Therefore the 3D pose and some parts of the body can not be estimated completely. Figure 2 shows one example sequec nces of a person walking in a frontoparallel plane. We defined a DOF kinematic structure: One blob for the body trunk, three blobs for the frontal leg and foot, connected with a hip joint, knee joint, and ankle joint, and two blobs for the arm connected with a shoulder and elbow joint. All joints have an axis orientation parallel to the W -axis in the camera frame. The head blob was connected with one joint to the body trunk. The first image in figure 2 shows the initial blob support maps.
After the hand-initialization we applied the motion tracker to a
sequence of   image frames. We could successfully track all body
parts in this video sequence (see video). The video shows that the appearance of the upper leg changes significantly due to moving folds on the subject's jeans. The lower leg appearance does not change to the same extent. The constraints were able to enforce compatible motion vectors for the upper leg, based on more reliable measurements on the lower leg.
We can compare the estimated angular configurations with motion capture data reported in the literature. Murray, Brought, and

5

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

Figure 2: Example configurations of the estimated kinematic structure. First image shows the support maps of the initial configuration. In subsequent images the white lines show blob axes. The joint is the position on the intersection of two axes.

90 80 70 60 50 40 30 20 10
0 0

Knee Angles from Kinematic Tracker 5 10 15 20 25 30 35

Figure 3: Comparison of a) data from [Murray et al] (left) and b) our motion tracker (right).

Kory published [13] such measurements for the hip, knee, and angle joints. We compared our motion tracker measurements with the published curves and found good agreement. Figure 4.1a shows the curves for the knee and ankle reported in [13], and figure 4.1b shows our measurements.
We also experimented with a walking sequence of a subject seen from an oblique view with a similar kinematic model. As seen in figure 4, we tracked the angular configurations and the pose suc-
cessfully over the complete sequence of   image frames. Because
we use a scaled orthographic projection model, the perspective effects of the person walking closer to the camera had to be compensated by different scales. The tracking algorithm could successfully estimate the scale changes.
4.2 Digital Muybridge
The final set of experiments was done on historic footage recorded by Eadweard Muybridge in 1884. His methods are of independent interest, as they predate motion pictures. Muybridge had his models walk in an open shed. Parallel to the shed was a fixed battery of 24 cameras. Two portable batteries of 12 cameras each were positioned at both ends of the shed, either at an angle of 90 deg relative to the shed or an angle of 60 deg. Three photographs were take

Figure 5: Eadweard Muybridge, The Human Figure in Motion, Plate 97: Woman Walking. The first 5 frames show part of a walk cycle from one example view, and the second 5 frames show the same time steps from a different view
simultaneously, one from each battery. The effective `framerate' of his technique is about two times lower then current video frame rates; a fact which makes tracking a harder problem.. It is to our advantage that he took for each time step three pictures from different viewpoints.
Figure 4.2 and figure 4.2 shows example photo plates. We could initialize the 3D pose by labeling all three views of the first frame and running the minimization procedure over the body dimensions and poses. Figure 4.2 shows one example initialization. Every body segment was visible in at least one of the three camera views, therefore we could track the left and the right side of the person. We applied this technique to a walking woman and a walking man. For the walking woman we had 10 time steps available that contained 60 % of a full walk cycle (figure 4.£2). For this set of experiments we extended our kinematic model to   DOFs. The two hip joints, the

two shoulder joints, and the neck joint, were modeled by DOFs. The two knee joints and two elbow joints were modeled just by one rotation axis. Figure 4.2 shows the tracking results with the model overlayed. As you see, we could successfully track the complete sequence. To animate the tracking results we mirrored the left and right side angles to produce the remaining frames of a complete walk cycle. We animated the 3D motion capture data with a stick figure model and a volumetric model (figure 10), and it looks very natural. The video shows some of the tracking and animation sequences from several novel camera views, replicating the walk cycle performed over a century ago on the grounds of University of Pennsylvania.
For the visualization of the walking man sequence, we did not apply the mirroring, because he was carrying a boulder on his shoulder. This made the walk asymmetric. We re-animated the original tracked motion (figure 4.2) capture data for the man, and it also looked very natural.
Given the successful application of our tracking technique to multi-view data, we are planning to record with higher frame-rates our own multi-view video footage. We also plan to record a wider range of gestures.

6

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html Figure 4: Example configurations of the estimated kinematic structure of a person seen from an oblique view.

Figure 6: Eadweard Muybridge, The Human Figure in Motion, Plate 7: Man walking and carrying 75-LB boulder on shoulder. The first 5 frames show part of a walk cycle from one example view, and the second 5 frames show the same time steps from a different view

Figure 8: Muybridge's Woman Walking: Motion Capture results. This shows the tracked angular configurations and its volumetric model projected to 2 example views.
5 Conclusion

Figure 7: Initialization of Muybridge's Woman Walking: This visualizes the initial angular configuration projected to 3 example views.

In this paper, we have developed and demonstrated a new technique for video motion capture. The approach does not require any markers, body suits or any other devices attached to the body of the actor. The actor can move about wearing his or her regular clothes. We demonstrated results on video recordings of people walking both in frontoparallel and oblique views, as well as on the classic Muybridge photographic sequences recorded more than a century ago.
Visually tracking human motion at the level of individual joints is a very challenging problem. Our results are due, in large measure, to the introduction of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation scheme. The advantage of this particular formulation is that it results in the equations that need to be solved to update the kinematic chain parameters from frame to frame being linear, and that it is not necessary to solve for any redundant or unnecessary variables.
Future work will concentrate on dealing with very large motions, as may happen, for instance, in videotapes of high speed running. The approach developed in this paper is a differential method, and therefore may be expected to fail when the motion from frame-toframe is very large. We propose to augment the technique by the use of an initial coarse search stage. Given a close enough starting value, the differential method will converge correctly.

7

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html

Acknowledgements
We would like to thank Charles Ying for creating the Open-GL animations and video editing, Shankar Sastry, Lara Crawford, Jerry Feldman, John Canny, and Jianbo Shi for fruitful discussions, Chad Carson for helping to write this document, and Interval Research Corp, and the California State MICRO program for supporting this research.

Figure 9: Muybridge's Man Walking: Motion Capture results. This shows the tracked angular configurations and its volumetric model projected to 2 example views.
Figure 10: Computer models used for the animation of the Muybridge motion capture. Please check out the video to see the quality of the animation.

References
[1] S. Basu, I.A. Essa, and A.P. Pentland. Motion regularization for model-based head tracking. In International Conference on Pattern Recognition, 1996.
[2] J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. Hierarchical model-based motion estimation. In ECCV, pages 237­252, 1992.
[3] M.J. Black and P. Anandan. The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer Vision and Image Understanding, 63(1):75­104, Jan 1996.
[4] G. Cameron, A. Bustanoby, K. Cope, S. Greenberg, C. Hayes, and O. Ozoux. Panel on motion capture and cg character animation. SIGGRAPH 97, pages 442­445, 1997.
[5] L. Concalves, E.D. Bernardo, E. Ursella, and P. Perona. Monocular tracking of the human arm in 3d. In Proc. Int. Conf. Computer Vision, 1995.
[6] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39, 1977.
[7] D.M. Gavrila and L.S. Davis. Towards 3-d model-based tracking and recognition of human movement: a multi-view approach. In Proc. of the Int. Workshop on Automatic Face- and Gesture-Recognition, Zurich, 1995, 1995.
[8] D. Hogg. A program to see a walking person. Image Vision Computing, 5(20), 1983.
[9] A. Jepson and M.J. Black. Mixture models for optical flow computation. In Proc. IEEE Conf. Computer VIsion Pattern Recognition, pages 760­761, New York, 1993.
[10] S.X. Ju, M.J.Black, and Y.Yacoob. Cardboard people: A parameterized model of articulated motion. In 2nd Int. Conf. on Automatic Face- and Gesture-Recognition, Killington, Vermont, pages 38­44, 1996.
[11] I.A. Kakadiaris and D. Metaxas. Model-based estimation of 3d human motion with occlusion based on active multiviewpoint selection. In CVPR, 1996.
[12] B.D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. Proc. 7th Int. Joinnt Conf. on Art. Intell., 1981.
[13] M.P. Murray, A.B. Drought, and R.C. Kory. Walking patterns of normal men. Journal of Bone and Joint Surgery, 46A(2):335­360, March 1964.
[14] R.M. Murray, Z. Li, and S.S. Sastry. A Mathematical Introduction to Robotic Manipulation. CRC Press, 1994.

8

UCB//CSD-97-973, http://www.cs.berkeley.edu/~bregler/digmuy.html [15] Eadweard Muybridge. The Human Figure In Motion. Various
Publishers, latest edition by Dover Publications, 1901. [16] J. O'Rourke and N. I. Badler. Model-based image analysis
of human motion using constraint propagation. IEEE Trans. Pattern Anal. Mach. Intell., 2(6):522­536, November 1980. [17] J.M. Regh and T. Kanade. Model-based tracking of selfoccluding articulated objects. In Proc. Int. Conf. Computer Vision, 1995. [18] K. Rohr. Incremental recognition of pedestrians from image sequences. In Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recogn., pages 8­13, New York City, June, 1993. [19] J. Shi and C. Tomasi. Good features to track. In CVPR, 1994. [20] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: Real-time tracking of the human body. In SPIE Conference on Integration Issues in Large Commercial Media Delivery Systems, volume 2615, 1995.
9

