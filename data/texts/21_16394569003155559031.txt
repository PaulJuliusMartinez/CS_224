HARDWARE SOFTWARE CO-DESIGN OF RUN-TIME SYSTEMS
a dissertation submitted to the department of electrical engineering
and the committee on graduate studies of stanford university
in partial fulfillment of the requirements for the degree of
doctor of philosophy
Vincent John Mooney III June, 1998

c Copyright 1998 by
Vincent John Mooney III
ii

I certify that I have read this thesis and that in my opinion it is fully adequate, in scope and in quality, as a dissertation for the degree of Doctor of Philosophy.
Giovanni De MicheliPrincipal Adviser
I certify that I have read this thesis and that in my opinion it is fully adequate, in scope and in quality, as a dissertation for the degree of Doctor of Philosophy.
Oyekunle A. OlukotunAssociate Adviser
I certify that I have read this thesis and that in my opinion it is fully adequate, in scope and in quality, as a dissertation for the degree of Doctor of Philosophy.
Robert W. Dutton
Approved for the University Committee on Graduate Studies:
iii

Abstract
Trends in system-level design show a clear move towards core-based design, where processors, controllers and other proprietary cores are reused and constitute essential building blocks. Thus, areas such as embedded system design and system-on-a-chip design are changing dramatically, requiring new design methodologies and ComputerAided Design CAD tools.
This thesis presents a novel system-level scheduling methodology and CAD environment, the Serra Run-Time Scheduler Synthesis and Analysis Tool. Unlike previous approaches to run-time scheduling, we split our run-time scheduler between hardware and software, as opposed to placing the scheduler all in one or the other. Thus, given an already partitioned input system speci cation in an HDL and a software language, Serra automatically generates a run-time scheduler partly in hardware and partly in software, for a target architecture of a microprocessor core together with multiple hardware cores or modules.
A heuristic scheduling algorithm solves for priorities of software tasks executing on a single microprocessor with a custom priority scheduler, interrupt service routine, and context switch code. Real-time analysis takes into account the split hardware software implementation both of the scheduler and of the tasks. The scheduler supports standard requirements of both domains, such as relative timing constraints in hardware and semaphores in software.
iv

A designer who uses the Serra CAD tool gains the advantage of e cient satisfaction of timing constraints for hardware software systems within a framework that enables di erent hardware software partitions to be quickly evaluated. Thus, a hardware software partitioning tool could easily sit on top of Serra, which would generate run-time systems for di erent hardware software partitions chosen for evaluation. In addition, Serra's more e cient design space exploration can improve time-to-market for a product.
Finally, we present two case studies. First, we show a full analysis, synthesis, and simulation of a hardware software implementation of a robotics control system for a PUMA arm AKB86, Uni84 . Second, we describe a sample prototype of the split runtime scheduler in an actual design, a force-feedback real-time Haptic robot. For this application, the hardware part of the scheduler was implemented on programmable logic communicating with software using a standard communication protocol.
v

Dedication
To my parents, Vincent John Mooney Jr. and Eulalia Maria Mooney, without whose love and encouragement throughout the years this thesis would not have been possible.
vi

Acknowledgments
I have many people to thank for this dissertation. First and foremost, I would like to thank my advisor, Professor Giovanni De Micheli, for his keen insight in helping me choose an important Ph.D. topic and for his guidance throughout the Ph.D. There was more than one occasion where I arrived at a technical result, only to look back and marvel at his guidance in setting me upon the path that led to the solution, while avoiding many pitfalls which were crystal clear to me only in hindsight.
I would also like to thank Professor Oyekunle Olukotun for serving as my associate advisor and as a reader of this thesis. The interaction with Professor Olukotun and his research group including Rachid Helaihel, Jeremy Levitt, Basem Nayfeh and Mike Chen provided excellent opportunities for enriching and challenging the research ideas I followed. Similarly, Professor David Dill and his students including Han Yang, Je rey X. Su and Clark Barrett provided superb interaction without which my research would have been signi cantly compromised. Additional thanks go to Professors Olukotun and Dill for serving on my Ph.D. Orals Committee.
Special thanks go to Professor Robert Dutton for serving both as the Chair of my Ph.D. Orals Committee and as a reader of this thesis. I am very grateful to have such careful input from someone outside of my circle of immediate research colleagues.
As for the development of the Serra Synthesis System, I would like to acknowledge the contributions of Toshiyuki Sakamoto, who wrote the hardware-tasks in Verilog HDL and implemented interrupts in the MIPS R4000 model, Sera Linardi, who
vii

ported cinderella to MIPS, Firdaus Abdullah, who implemented the full Verilog simulations of the hardware-software run-time scheduler for the Robot Arm Controller, and Yau-Tsun Steven Li, who provided guidance and support for cinderellaM and associated analysis. I also would like to thank Meredith J. Goldsmith and Giuseppe A. Paleologo for extended discussions about the relation of the scheduling problem considered here to the work in operations research, as well as the help provided in formulating the problem in AMPL.
I also would like to thank Thoi Nguyen, Charles Orgish and Babak Moghadam for their network help, without which none of this would have been possible.
Many industry contacts have enriched this thesis tremendously: James Rowson, Shields Neely, Bill Mark, Mark Shand and Jim Ready, to name a few.
Finally, I would like to thank the members of the CAD group, the Robotics group, and my many friends I made while at Stanford. Claudionor Coelho, Luca Benini, Rajesh Gupta, Polly Siegel, Jerry Yang, David Filo, David Ku, James Smith, Luc Semeria, Tajana Simunic, Matija Siljak, Valeria Bertacco, Alessandro Bogliolo, Marco Platzner and Aneesh Koorapaty are current or former members of the CAD group who all helped me in some way, thanks. Diego Ruspini, Kyong-Sok Chang, and Oscar Madrigal of Professor Oussama Khatib's robotics group helped tremendously, as did Professor Khatib himself. Also, Bruce Romney of Jean-Claude Latombe's group helped in many ways, but I will only mention two: the PUMA robotics control software code and the TAship for EE271 Intro to VLSI. In addition, Scott Devine, Ben Werther, and Robert Bosch of Professor Mendel Rosenblum's Operating Systems group provided generous help at key moments. Finally, Oskar Mencer was an important ally in the Pamette Synopsys Xilinx struggles. As for my close friends, many of the names have already mentioned; I will say a few, but not all, of the names not mentioned: Derek Gerlach, Ajay Kapur, Rafael Betancourt, Sonja Schuemann, Lakita Garth, Florencia Cortina, Ekua Blankson, Afua Ntiwa, Manolo Clavel,
viii

Dr. Jose Meseguer, Peter Olveczky, Noel Vitug, Mike Vroman, Anthony McCarthy, Mike Pinto and Pedro Gutierrez. Dr. Rick Reis provided mentorship for which I am extremely grateful. Last but not least, my family is to be thanked: Martha Bowers Mooney, Vincent John Mooney Jr., Eulalia Maria Mooney, Alexander Xavier Mooney, Patrick Joseph Mooney, Laurie Jean Mooney, Emily Marie Mooney, Justin Patrick Mooney, and Margaret Ann Mooney.
This research was sponsored by ARPA, under grant No. DABT 63-95-C-0049, by a fellowship from National Semiconductor, and by a software donation from Synopsys.
ix

Contents

Dedication

iii

Acknowledgments

iv

1 Introduction
1.1 Hardware Software Co-Design : : : : : : : : : : : : : : : : : : : : : : 1.2 Requirements for Designing Hardware-Software Systems : : : : : : :
1.2.1 Scheduling at Di erent Levels : : : : : : : : : : : : : : : : : : 1.3 Objectives and Contributions : : : : : : : : : : : : : : : : : : : : : : 1.4 Thesis Outline : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :

1
2 5 6 7 8

2 Background
2.1 Previous Hardware Software Co-Design Systems : : : : : : : : : : : : 2.1.1 COSYMA : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.1.2 VULCAN : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.1.3 POLIS : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.1.4 COWARE : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.1.5 CHINOOK : : : : : : : : : : : : : : : : : : : : : : : : : : : :
2.2 Hardware Scheduling : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.2.1 Integer Linear Programming : : : : : : : : : : : : : : : : : : :

9
9 10 11 13 14 18 19 19

x

2.2.2 List Scheduling : : : : : : : : : : : : : : : : : : : : : : : : : : 2.2.3 Relative Scheduling : : : : : : : : : : : : : : : : : : : : : : : : 2.2.4 Conditional Process Graphs : : : : : : : : : : : : : : : : : : : 2.3 Software Scheduling : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.3.1 Round-Robin Scheduling : : : : : : : : : : : : : : : : : : : : : 2.3.2 Shortest Job First : : : : : : : : : : : : : : : : : : : : : : : : : 2.3.3 Rate-Monotonic Analysis : : : : : : : : : : : : : : : : : : : : : 2.3.4 Least Common Multiple : : : : : : : : : : : : : : : : : : : : : 2.3.5 Real-Time Kernel in Hardware : : : : : : : : : : : : : : : : : 2.4 Control-Flow Expressions : : : : : : : : : : : : : : : : : : : : : : : : 2.4.1 Formalism : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.4.2 Synthesis with CFEs : : : : : : : : : : : : : : : : : : : : : : : 2.4.3 Thalia : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.5 Bounds on Execution Time : : : : : : : : : : : : : : : : : : : : : : : 2.6 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :

20 20 21 21 22 22 22 24 25 25 26 28 29 29 30

3 Target Architecture, Kernel, and System Model
3.1 CAD Requirements : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3.2 Target Architecture and Kernel : : : : : : : : : : : : : : : : : : : : :
3.2.1 Task Execution : : : : : : : : : : : : : : : : : : : : : : : : : : 3.2.2 Run-Time Scheduler Implementation : : : : : : : : : : : : : : 3.2.3 Control of Software : : : : : : : : : : : : : : : : : : : : : : : : 3.2.4 Software Generation : : : : : : : : : : : : : : : : : : : : : : : 3.2.5 Priority Scheduler Template for Software : : : : : : : : : : : : 3.3 System Modeling : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 3.4 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :

32
33 37 37 38 39 40 41 42 45

xi

4 Real Time Analysis

46

4.1 Assumptions and Complexity : : : : : : : : : : : : : : : : : : : : : : 47

4.1.1 The Complexity of NSDS : : : : : : : : : : : : : : : : : : : : 50

4.2 Constructive Heuristic Scheduling : : : : : : : : : : : : : : : : : : : : 53

4.2.1 Constructive Heuristic Scheduling Formulation : : : : : : : : : 53

4.2.2 Constructive Heuristic Scheduling Solution : : : : : : : : : : : 58

4.2.3 Multiple NEV ER Sets of Hardware-Tasks : : : : : : : : : : : 65

4.2.4 Complexity Analysis : : : : : : : : : : : : : : : : : : : : : : : 69

4.2.5 Practical Considerations for the Calculation of WCET : : : : 70

4.3 Context Switch Cost and Out-of-order Execution : : : : : : : : : : : 72

4.3.1 Upper bound on extra calls to the Priority Scheduler and Con-

text Switch : : : : : : : : : : : : : : : : : : : : : : : : : : : : 73

4.3.2 Instruction Cache Analysis : : : : : : : : : : : : : : : : : : : : 83

4.3.3 Total Upper Bound on WCET : : : : : : : : : : : : : : : : : : 86

4.3.4 Constructive Heuristic Scheduling with Out-of-order Execution 87

4.4 Task Splitting : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 93

4.5 Critical Regions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 96

4.6 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 98

5 Implementation and Experimental Results

99

5.1 Design System Implementation : : : : : : : : : : : : : : : : : : : : : 99

5.1.1 Serra Run-Time Scheduler Analysis and Synthesis : : : : : : 101

5.2 Design Case Study: PUMA Robot Arm : : : : : : : : : : : : : : : : : 103

5.2.1 Two PUMA Arms : : : : : : : : : : : : : : : : : : : : : : : : 104

5.2.2 Verilog Simulation : : : : : : : : : : : : : : : : : : : : : : : : 109

5.2.3 Run-Time Scheduler Software : : : : : : : : : : : : : : : : : : 111

5.2.4 Run-Time Scheduler Hardware : : : : : : : : : : : : : : : : : 113

xii

5.2.5 Running the Simulation : : : : : : : : : : : : : : : : : : : : : 117 5.2.6 Design Gains : : : : : : : : : : : : : : : : : : : : : : : : : : : 121 5.3 Design Case Study: Haptic Robot : : : : : : : : : : : : : : : : : : : : 122 5.3.1 Original Design : : : : : : : : : : : : : : : : : : : : : : : : : : 123 5.3.2 Haptic Control Implememted with Split Run-Time System : : 126 5.3.3 System Architecture : : : : : : : : : : : : : : : : : : : : : : : 129 5.3.4 Software Generation : : : : : : : : : : : : : : : : : : : : : : : 131 5.3.5 Future Directions : : : : : : : : : : : : : : : : : : : : : : : : : 133 5.4 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 135

6 Conclusions and Future Work

137

6.1 Summary : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 137

6.2 Future Work : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 140

Abbreviations and Symbols

142

Bibliography

145

A A Mathematical Program Formulation

155

xiii

List of Tables
1 Link between Verilog HDL Constructs and Control-Flow Expressions 26 2 Entry Table for Software-Tasks : : : : : : : : : : : : : : : : : : : : : 40
3 Constructive Heuristic Scheduling Example Stage n , 1 = 4 : : : : : 63
4 Constructive Heuristic Scheduling Example Stage 3 : : : : : : : : : : 63 5 Constructive Heuristic Scheduling Example Stage 2 : : : : : : : : : : 64 6 Constructive Heuristic Scheduling Example Stage 1 : : : : : : : : : : 65 7 WCET Calculation Example : : : : : : : : : : : : : : : : : : : : : : : 72 8 WCET Calculation Example : : : : : : : : : : : : : : : : : : : : : : : 92 9 Code space, BCET and WCET for sw-tasks. : : : : : : : : : : : : : : 107 10 Results for the synthesis of hw-tasks. : : : : : : : : : : : : : : : : : : 108 11 Code space for software tasks. : : : : : : : : : : : : : : : : : : : : : : 132 12 Code space for hardware tasks. : : : : : : : : : : : : : : : : : : : : : 133 13 Statistics for Xilinx 4020E Mapping : : : : : : : : : : : : : : : : : : : 134 14 WCET found and run times for Constructive Heuristic Scheduling ver-
sus AMPL. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 157
xiv

List of Figures
1 Vulcan Synthesis Tool in context : : : : : : : : : : : : : : : : : : : : 12 2 CoWare simulation paradigm and sample implementation : : : : : : 17 3 Thalia2 Synthesis from CFEs : : : : : : : : : : : : : : : : : : : : : 28
4 PUMA Arms Courtesy of the Computer Science Robotics Lab at Stanford : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 33
5 Robotics Example: Concurrent Control Algorithms : : : : : : : : : : 34 6 Tool Flow and Target Architecture : : : : : : : : : : : : : : : : : : : 35 7 Target Architecture : : : : : : : : : : : : : : : : : : : : : : : : : : : : 38 8 Robotics Example: Main Task : : : : : : : : : : : : : : : : : : : : : : 42 9 Flattened CDFG of Robot Arm Controller : : : : : : : : : : : : : : : 43 10 DAG, BCET and WCET: The leftmost column shows the task names,
the middle column shows the Best-Case Execution Time, and the rightmost column shows the Worst-Case Execution Time. : : : : : : : : : 49 11 Example transformation of an SRTD problem to an NSDS problem. : 52 12 GraphWCET Example : : : : : : : : : : : : : : : : : : : : : : : : : 55 13 Constructive Heuristic Scheduling Example Stage 3 : : : : : : : : : : 57 14 Calculate WCET Algorithm : : : : : : : : : : : : : : : : : : : : : : : 60 15 Constructive Heuristic Scheduling Algorithm : : : : : : : : : : : : : : 61
xv

16 Sample DAG With Optimal Schedule Not Found By Heuristic: The constructive heuristic scheduling algorithm nds order d,b,c which yields a WCET of 43,000; however, the optimal order is b,d,c, which yields a WCET of 40,000. : : : : : : : : : : : : : : : : : : : : : : : : 66
17 Multiple NEV ER Set Example : : : : : : : : : : : : : : : : : : : : : 67 18 Constructive Heuristic Scheduling Algorithm with Multiple NEV ER
Sets : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 68 19 DAG, WCET and  Example : : : : : : : : : : : : : : : : : : : : : : 75 20 DAG With Out-of-order Execution Example : : : : : : : : : : : : : : 76 21 Extra Priority Scheduler and Context Switch Time Examples : : : : : 77 22 Execute Out-of-order Algorithm : : : : : : : : : : : : : : : : : : : : : 88 23 Example With WCET Calculation of Instruction Cache Re ll Time : 90 24 Constructive Heuristic Scheduling Example of Suboptimal Result : : 94 25 Example of Scheduling with Task Splitting : : : : : : : : : : : : : : : 95 26 Example Speci cation of Noninterruptible Task : : : : : : : : : : : : 96 27 Tool Flow and Target Architecture : : : : : : : : : : : : : : : : : : : 100 28 Block diagram of Serra: the boxes indicate tools and the ovals indi-
cate data. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 101 29 Directed Acyclic Graphs of Ohold1 Law, Set Torque, and Jhold Law
with Relative Timing Constraints : : : : : : : : : : : : : : : : : : : : 104 30 DAG of Robot Arm Controller with Relative Timing Constraints : : 105 31 Final Hardware Portion of Run-Time Scheduler : : : : : : : : : : : : 106 32 Simpli ed Block Diagram of the Simulation : : : : : : : : : : : : : : 110 33 Interrupt Asserted : : : : : : : : : : : : : : : : : : : : : : : : : : : : 117 34 PC Jumps to Start Address for Interrupt Service Routine : : : : : : : 118 35 Software task cjd completes : : : : : : : : : : : : : : : : : : : : : : : 119 36 Control restarts itself after task xb1 : : : : : : : : : : : : : : : : : : : 120
xvi

37 Haptic Robot With Graphics : : : : : : : : : : : : : : : : : : : : : : 122 38 System Architecture : : : : : : : : : : : : : : : : : : : : : : : : : : : 123 39 Sphere Characterization : : : : : : : : : : : : : : : : : : : : : : : : : 124 40 Synopsys-Xilinx Tool Flow : : : : : : : : : : : : : : : : : : : : : : : : 127 41 Run-Time Scheduler Control Communication : : : : : : : : : : : : : 128 42 PCI Pamette Version 1 Architecture : : : : : : : : : : : : : : : : : 130 43 Teapot Graphical Object With Proxy : : : : : : : : : : : : : : : : : : 131 44 AMPL data for dagopt problem. : : : : : : : : : : : : : : : : : : : : : 155 45 AMPL model for dagopt problem. : : : : : : : : : : : : : : : : : : : 156 46 The dagopt2 problem, generated from the dagopt problem Figure 16
by doubling the number of tasks. : : : : : : : : : : : : : : : : : : : : 158
xvii

Chapter 1
Introduction
The use of Computer-Aided Design CAD and synthesis tools in system-level design of digital systems has gained large acceptance in industry and academia. Synthesis CAD tools automate portions of the design process, allowing designers to spend more time at higher levels of abstraction. Thus, synthesis tools support i more e cient exploration of the available design space, ii the production of correct and optimal in some sense circuits, and iii a reduction in time-to-market. These three characteristics drive the increasing use of synthesis tools in design today.
Additionally, several important trends in system-level design a ect the use of synthesis. First of all, there is a signi cant movement towards core-based design, where pre-designed cores such as processor and microcontroller cores are used in system designs. Such core usage adds value through design reuse and the selling of Intellectual Property IP. Why reinvent a component which another design team has already spent six months or more optimizing? Instead, both time and e ort is saved by purchasing the component as IP.
A second trend in system-level design is the move to System-on-a-Chip SoC designs. For example, instead of making a board with eight separate chips, why not place all eight designs on the same chip? With ever decreasing transistor sizes, this is
1

CHAPTER 1. INTRODUCTION

2

a real possibility for more and more systems. However, placing a system on a single Integrated Circuit IC requires the integration of many heterogeneous components, such as digital, analog and memory.
A third trend to help deal with the enormous complexity is the move towards standards. This is evident in several initiatives, including the Virtual Sockets Interface Alliance VSIA DF98 , the European CAD Standards Initiative ECSI AM98 , and Reusable Application-Speci c Intellectual Property Developers RAPID RK98 . All three seek the establishment of open standards for the easy and reliable interfacing of cores designed by separate design teams.
The direction indicated by these trends is clear. New system-level design methodologies and CAD tools are needed.
1.1 Hardware Software Co-Design
We consider the design of mixed hardware software systems, such as embedded systems and robots. Most of today's hardware software systems are designed by deciding up front what functionality will be implemented in hardware and what functionality will be implemented in software, with relatively few changes as the system design progresses. The research of hardware software co-design targets altering this design strategy.
We aim at providing CAD tools that help bring hardware and software design ows closer together in order to allow designers to make tradeo s between software and hardware and thus more quickly evaluate design alternatives.
Approaches to hardware software co-design of embedded systems MS96, Mic97 can be di erentiated in several ways. One way is to consider the system-level speci cation, which is either homogeneous i.e. in a single speci cation language or heterogeneous i.e. involving multiple modeling paradigms. Another way to di erentiate

CHAPTER 1. INTRODUCTION

3

approaches is to distinguish how the CAD tool partitions the system speci cation: approaches consider either ne-grained partitions, i.e. at the operation or basic block level, or coarse-grained partitions, i.e. at the process or task level  HE96 de nes granularity in a slightly di erent way. For example, the Cosyma system Section 2.1.1 OBE+97 , the VULCAN system Section 2.1.2 Gup95  and the POLIS system Section 2.1.3 BCG+97  can be classi ed as homogeneous and ne-grained approaches, while the CoWare system Section 2.1.4 MBL+96  and the approach of Adams and Thomas AT95 are heterogeneous and coarse-grained. The method using the SpecCharts language NVG92 supports homogeneous speci cation in VHDL with both ne- and coarse-grained partitioning. We take the heterogeneous and coarse-grained approach in this thesis.
There has been much previous work in hardware-software partitioning Gup95, MBL+96, MS96 . However, system designs modeled by heterogeneous speci cations are often already partitioned by designers into modules or tasks. Whereas some optimality is lost in using a coarse granularity in partitioning, the resulting implementation is often closer to what designers expect, and interfacing hardware to software blocks is easier. We assume the availability of automated interface generation similar to COB95, MBL+96 .
Designers of real-time embedded systems often have timing constraints that they must meet for the design to be successful. To support soft and hard real-time constraints, system designers need tight bounds on execution delays. In hardware software co-design, scheduling resources to meet these tight bounds is a critical problem because there may be parallel threads of execution in the application with the same resource required by di erent threads.
In hardware software co-design an important problem is the management of software routines and their coordination with hardware. An indispensable component

CHAPTER 1. INTRODUCTION

4

to a system of cooperating hardware and software is a run-time scheduler. One approach to scheduling is to come up with a static schedule for hardware and software operations. However, the sequence of hardware and software tasks can change dynamically in complex real-time systems, since such systems often have to operate under many di erent conditions. For example, a robotics system which comes into contact with a hard surface may have to change its force control algorithm, along with its attendant sensor set, estimators, and trajectory control routines. Furthermore, there may be data-dependent and memory-dependent delays in execution, especially if the software runs on a processor core with caches. Thus, in many hardware software systems, dynamic scheduling is a necessity.
One clear and easy solution is to put the run-time system in software and suitably design the hardware such that it can be controlled from the software. Unfortunately, software schedulers may not be predictable as far as being able to satisfy real-time constraints. Therefore, this thesis proposes implementing the time-constrained portion of the scheduler in hardware, where delays are accurately known. Thus, we present a strategy for a mixed implementation of a dynamic real-time scheduler in hardware and software, and a CAD tool, called Serra, to synthesize the necessary hardware and software for the run-time scheduler as well as analyze the performance of the system.
This dissertation focuses on analysis and synthesis of a custom dynamic runtime scheduler in hardware and software for embedded applications such as robotics control. In particular:
We present a design approach for scheduling hardware software tasks de ned at a coarse level of granularity.
We present analysis and synthesis techniques for scheduling mutually exclusive tasks to minimize Worst-Case Execution Time WCET.

CHAPTER 1. INTRODUCTION

5

We present a Verilog simulation of a robotics system using our scheduling approach as well as a small prototype of the split run-time scheduler working in an actual robotics prototype.
1.2 Requirements for Designing Hardware-Software Systems
One common requirement for system-level design targeted for mixed hardware-software implementation is the ability to carry out complex calculations. For example, in robotics control design, state space representation of the kinematics and dynamics of the arm can involve large matrices and require signi cant computational power Lat91 . This complex functionality is often coupled with real-time constraints, such as the requirement to update robot arm torque inputs to its motors one thousand times a second. If missing the deadline may result in catastrophic results, such as damage to the robot or to the user, then we have a hard real-time constraint; if the deadline can be missed occasionally without signi cant negative e ects, then we have a soft real-time constraint.
Timing constraints can be classi ed into two types: rate constraints, specifying the rate of execution of a particular set of operations or tasks, and relative timing constraints, specifying the minimum and maximum time separation between two operations or tasks. Both types of constraints are typical requirements in embedded systems.
In order to design a hardware-software system at a coarse level of granularity, several steps must be completed, not necessarily in this order:
The system must be partitioned into tasks.
Each task needs to be allocated to hardware or to software.

CHAPTER 1. INTRODUCTION

6

The interface between tasks needs to be synthesized.
The tasks need to be scheduled and synchronized.
The rst three steps, while extremely important, have been addressed in other research and are not dealt with in this thesis. Instead, we focus on the last step, scheduling and synchronization.
Yet another requirement is the satisfaction of resource constraints. For example, there may be limited hardware, or all software-tasks might execute on the same CPU. Thus, tasks executed on the same hardware or on the same CPU cannot be scheduled at the same time, but must be mutually exclusive: one task must stop executing before the other begins to execute on the same resource. Notice that satisfying this scheduling requirement can pose a di cult optimization problem since the same task may be needed in concurrent control ows.
1.2.1 Scheduling at Di erent Levels
We emphasize here the di erences between scheduling in high-level synthesis and scheduling in system-level synthesis. In high-level synthesis, the main emphasis is the scheduling of operations within a basic block. Optimality of a design in high-level synthesis is usually given in terms of the optimality of the execution time in basic blocks or the cost of resources in basic blocks, such as the number of multipliers, adders or multiplexors. In system-level synthesis, on the other hand, we have to consider the interactions that cross basic block boundaries as well. When the system is partitioned in basic blocks some of the interactions of the system are converted into environmental constraints, such as relative timing constraints and precedence constraints, which should guide the scheduling tool in nding a feasible and optimal implementation. Whenever these environmental constraints cross implementation paradigms namely, hardware and software appropriate synchronization must be

CHAPTER 1. INTRODUCTION

7

added as well.
1.3 Objectives and Contributions
In this thesis, we present a system-level scheduling methodology and CAD tool. We look at a system as a collection of tasks, where a task is a hardware module or software thread. We automate the synchronization and scheduling of tasks in hardware and in software. In order to achieve this automation, we need both analysis of the system and synthesis for the run-time scheduler implemented in hardware and in software.
Speci cally, we present the following contributions to the eld of scheduling for hardware-software systems:
Design Style for Scheduling. We will present a simple design style for representing tasks that is independent of the tasks' implementation in hardware or in software. The representation will allow for dynamic scheduling of the tasks, where by dynamic we mean that the exact time when each task starts and nishes is not statically determined but instead is decided at run-time.
Co-Synthesis of a Hardware-Software Run-Time Scheduler. We will show how, given the control- ow of the tasks in the system, we can synthesize a run-time scheduler partly implemented in hardware and partly implemented in software. Such a mixed implementation can leverage the advantages of both domains.
Rate-Constraint Satisfaction Analysis. We will present techniques for analysis of the satisfaction of a single hard real-time rate constraint on the system, as is typical in robotics applications. The analysis will go hand-in-hand with the scheduling of the tasks and the generation of the run-time scheduler.
Resource Constraints. We will show how our scheduler synthesis procedure

CHAPTER 1. INTRODUCTION

8

satis es resource constraints, in hardware and software, while predictably satisfying timing constraints.
Application to Robotics. We will present a full analysis, synthesis, and simulation of a hardware software implementation of a robotics control system. We will also describe a small prototype of a split hardware software scheduler to control a force-feedback Haptic robot using a PentiumTM for the software and Xilinx FPGAs for the hardware.
1.4 Thesis Outline
This chapter gives an introduction and motivation for the thesis. Chapter 2 describes some of the previous work in hardware software co-design as well as some related work from other areas. We next give an overview of our target architecture and system-level scheduling design style in Chapter 3. The chapter also brie y describes the very small kernel running on the microprocessor core.
Chapter 4 presents the real-time analysis used to analyze whether the nal system will meet its timing and resource constraints. A heuristic scheduling algorithm is described in detail, together with extensions to provide support for preemptible tasks and semaphores in software.
Chapter 5 presents two design examples and how they were solved using the scheduling approach presented in this thesis. Finally, in Chapter 6, we will present some concluding remarks and some ideas for future research.

Chapter 2
Background
From the wide array of previous research in hardware software co-design, hardware scheduling, and scheduling algorithms for real-time systems, we examine a few representative samples which most directly impact the research of this dissertation.
2.1 Previous Hardware Software Co-Design Systems
We will rst examine two systems which focus on hardware software partitioning, after which we will examine two other systems which provide particular environments for hardware software co-design. The next two sections exemplify two opposite approaches to hardware software partitioning: 1 start with a software solution and migrate parts of the speci cation to hardware, and 2 start with a hardware solution and migrate parts of the speci cation to software.
9

CHAPTER 2. BACKGROUND

10

2.1.1 COSYMA
The Cosyma COSYnthesis for eMbedded micro Architectures system aims at
speeding up software execution to meet timing constraints EHB+96, OBE+97 . The speedup is achieved by using dedicated hardware to implement some of the functionality originally calculated by software. The original speci cation is in Cx, a minimum extension of the C programming language to allow parallel processes. Rate constraints are speci ed at the process level, while input output timing constraints can only be handled in a few specialized cases.
The original Cx speci cation is compiled into an Extended Syntax Graph of the
code, annotated with local and global data ow information. Timing information is calculated using several approaches, including pro ling and symbolic analysis EY97 . With this timing analysis, Cosyma can identify which constraints are met and which are not met with the all-software solution. Next comes partitioning.
Hardware software partitioning occurs at the basic block level, which is seen as a manageable compromise between ne-grained at the level of individual instructions and coarse-grained at the level of processes or threads partitioning. For basic blocks implemented with software, a mixed pro ling static analysis technique is used to estimate the worst case execution time WCET of the software code EY97 .
Basic blocks that are implemented in hardware are assumed to not have any
pointers. High-level synthesis is performed by the Braunschweig Synthesis System which produces Register-Transfer-Level RTL output for the Synopsys Design
CompilerTMDCTM. DCTM then produces a nal netlist. Several techniques are
used to estimate execution time of hardware, including list scheduling Mic94 and path-based scheduling HE95 .
Communication time is estimated based on the number of variables that need to be passed between hardware and software for a given partition. Burst-mode communication is not supported.

CHAPTER 2. BACKGROUND

11

Cosyma uses simulated annealing in the partitioning process. Tens of thou-
sands of possible hardware software partitions are considered very quickly less than a minute in a typical run.
Cosyma was originally targeted to single processor with one coprocessor with shared memory communication but has recently been expanded to target multiple heterogeneous processors and coprocessors running in parallel communicating over shared memory or point to point communication. Software processes mapped to the same processor are statically scheduled.
The nal output of Cosyma are the hardware blocks, statically scheduled software processes, and appropriate communication primitives in hardware and in software. If a solution is generated, it is guaranteed to meet the speci ed rate constraints while choosing the smallest hardware cost from among the partitions considered.
2.1.2 VULCAN
The VULCAN tool aims at reducing ASIC hardware cost Gup95 . The reduction in hardware cost is achieved by partitioning part of the design to software. The original speci cation is in Hardware-C KD90 , a Hardware Description Language HDL which can be synthesized down to netlists with the Olympus Synthesis System DKMT90 .
The Hardware-C description, with rate and relative timing constraints, is mapped to a ne-grained Control-Data Flow Graph CDFG intermediate representation. By ne-grained we mean that nodes in the CDFG correspond to individual computations such as arithmetic operations. This is the level at which VULCAN carries out hardware software partitioning. At locations in the CDFG where a split between hardware and software occurs, appropriate Inter-Block Communication IBC vertices are added. IBC vertices for communication can be blocking, nonblocking, or bu ered. In addition, software has to be generated for portions of the CDFG mapped

CHAPTER 2. BACKGROUND

12

to software.

Input description and compilation
HERCULES HDL
Specification

VULCAN

compilation
Graph Model

Co-synthesis tasks

Program Graph

code synthesis

C Program

DLXCC

Software compilation

compilation

constraint analysis partitioning
Interface

ASIC Graph Model

interface gen
HEBE CERES

strctural synthesis

Hardware synthesis

Assembly Program

ASIC Netlist

Figure 1: Vulcan Synthesis Tool in context

In order to map to software, all operations in the speci ed computation have to be serialized. Since the partial order of the CDFG speci cation is naturally parallel, this serialization problem is quite signi cant. A heuristic algorithm iterates over possible serial orders which also implement the partial order in the original specication without violating any rate constraints. The end result is a set of software program threads that can run with a custom software run-time scheduler. The runtime scheduling of software routines in VULCAN uses a non-preemptive scheme, for

CHAPTER 2. BACKGROUND

13

example as provided by a prioritized FIFO scheduler Gup95 . Hardware software partitioning is carried out by means of a heuristic graph parti-
tioning algorithm which runs in polynomial time Gup95 . The partitioning algorithm considers di erent partitions of the CDFG speci cation between hardware and software, with the goal of minimizing hardware cost while still meeting timing constraints. A graphical representation of VULCAN is shown in Figure 1.
So far we have considered, from the large amount of research, two representative systems for hardware software partitioning. Next we will review a system for control dominated hardware software co-design and then a system for signal processing hardware software co-design.
2.1.3 POLIS
The POLIS system aims at providing a synthesis system targeted to design and analysis of embedded controller applications with a mixed implementation split between software and Application Speci c Integrated Circuits ASICs BCG+97 . The design is originally speci ed in a high level language such as Esterel BG, BS91, Ber96 , graphical FSMs, or Verilog VHDL subsets.
The fundamental model of computation in POLIS is the Co-design Finite State Machine CFSM, which supports a globally asynchronous, locally synchronous formal model of the design. Each transition of a CFSM takes non-zero time, is atomic, and can take on any value from a set of nite values. The assumption of non-zero transition time is made to avoid the composition problem of Mealy machines, due to undelayed feedback loops. Communication between CFSMs is by means of events which may be dropped response to individual events is not guaranteed by POLIS.
In POLIS, the original design speci cation is compiled to a network of CFSMs. Sub-networks of CFSMs are targeted to hardware or to software; automatic synthesis supports either choice. Hardware synthesis is achieved by generating a synthesizable

CHAPTER 2. BACKGROUND

14

HDL description and passing it on to a logic synthesis tool. Interfaces between hardware, software, and the external world are automatically synthesized in the form of cooperating circuits and software I O drivers. For software executing on the same processor, a custom scheduler round-robin, static cyclic, or static priority can be compiled together with the CFSM-generated C-code, or a commercial Real-Time Operating System RTOS can be added by hand. Co-simulation is provided using the Ptolemy environment BHLM94 .
Thus, POLIS provides an environment where a designer can quickly evaluate choices of hardware software partitioning, architecture selection, and scheduler selection. The output of POLIS is the C-code for the selected processor and the optimized hardware. This can be used, for example, in a board level prototype where the hardware is implemented with Field-Programmable Gate Arrays FPGAs. The POLIS system is publicly available and has been used on several sample designs, such as a dashboard controller.
2.1.4 COWARE
The goal of the CoWare system is to provide a design environment for heterogeneous hardware software Digital Signal Processing DSP systems MBL+96, VRBM96 . CoWare was developed at IMEC Belgium, and is the basis for a commercial product CoWare N2C Cow98 . We describe here the original CoWare, based on published research papers and presentations at international workshops and conferences MBL+96, VRBM96, VLM96a, VLM96b, RVBM96, MBL+97 . The CoWare hardware software co-design environment allows the cospeci cation of hardware and software components using existing languages such as VHDL, Data Flow Language DFL WDC+94 , Silage and C. CoWare provides unambiguous speci cation of interfaces between hardware and software, and correct synthesis of these interfaces in hardware and software by generating both hardware interfaces and device drivers.

CHAPTER 2. BACKGROUND

15

CoWare is based on a data model of communicating processes and supports the gradual re nement of a high level description into an interconnection of programmable processors and dedicated, synthesizable hardware. The model supports the re-use and encapsulation of hardware and software by a clear separation between the functional behavior and the communication behavior of a system component.
The current version of CoWare supports the use of the ARM processor and various software tools such as a simulator and compiler for ARM and commercial VHDL simulators, logic synthesis and DSP synthesis tools.
The basis for this speci cation method is a data model for communicating processes. The model supports a strict separation between functional and communication behavior. Designs are made reusable by describing their functional behavior while maintaining an abstract model of their communication behavior. When a design is actually re-used in a system, the speci cation method allows one to re ne the abstract communication model into a detailed behavior that is more appropriate in the system context. The same speci cation method is used to model o -the-shelf programmable processors, and these models are used in a processor independent hardware software co-design methodology.
Synthesis tools and compilers are able to implement all processor, accelerator, and memory components once the global system architecture has been de ned. The CoWare design environment provides for integration of existing design technology by automatically generating the interfaces that link these design environments and by interfacing the generated and o -the-shelf processors in a way that is consistent with the system speci cation.
Designing a system with the CoWare environment involves four steps: functional speci cation, architecture de nition, communication selection, and component implementation VRBM96 .
Functional speci cation. A system is speci ed by means of communicating

CHAPTER 2. BACKGROUND

16

processes that exchange data via channels. The behavior of a process can be entered using a host language such as C, DFL or VHDL.
Architecture de nition. Optimally allocate processors, accelerators and mem-
ories, binding them to the functional speci cation. This interactive allocation and binding step includes the hardware software partitioning.
Communication selection. Automatically generate the necessary software and
hardware to make processors, accelerators and the di erent environments communicate. This step is performed via the SYMPHONY interface synthesis toolbox. Communication Blocks CBs provide pipelining and synchronization between accelerators. The communication between the hardware and the software for the ARM processor is more complex. The ARM interface includes address decoders, DMA channels, interrupts and I O ports. Within the ARM, software drivers must be synthesized and linked to the processes running on the ARM.
Component implementation. All components in the system such as acceler-
ator processors, interface hardware and software, memories, software running on a processor core, and debugging blocks are implemented using existing design environments. CoWare embeds di erent component compilers into the design environment, such as the ARM C-compiler and commercial VHDL DSP synthesis environments.
The basic model of communication in CoWare is the Remote Procedure Call RPC. An example can be seen in Figure 2. The RPC connections can be seen between blocks. The cascaded blocks show di erent abstraction levels of the same functionality. The abstract CoWare C" is C code written for CoWare and not targeted to any particular processor. C for CPU" is C code targeted to a particular processor, e.g. an ARM. Finally, RTL is a Register-Transfer Level description in some HDL, typically Verilog HDL or VHDL. Notice that any RPC connection can communicate with an RPC connection at any other level of abstraction abstract CoWare C, C for CPU, or RTL.

CHAPTER 2. BACKGROUND

17

Block A abstract CoWare C
C for CPU
RTL

RPC

RTL C for CPU Block B abstract CoWare C
RTL C

in, out, in/out

CPU

sw driver

hw I/F

I/F bus

HW Block
Figure 2: CoWare simulation paradigm and sample implementation
The bottom half of Figure 2 shows a hardware implementation of the RPC communication paradigm. CoWare synthesizes the software device driver as well as the logic in hardware to read data from the interface I F bus.
Thus, a mixed system level speci cation in which part of the system is already implemented while another part is still speci ed at the behavioral level can be cosimulated. For this purpose, existing simulators can be integrated into the environment. Currently, Synopsys' VSS simulator for VHDL and the ARM instruction set simulators both instruction accurate and cycle accurate have been linked MBL+97 .
CoWare operates very much like a linker, providing an executable that can be linked to instruction set simulators as well as other modules. The CoWare methodology imposes increased demands on the generation of library elements. Often, abstract and detailed models of IP blocks do not exist. Existing IP blocks, for which

CHAPTER 2. BACKGROUND

18

Verilog HDL code currently exists, require additional work to generate validated abstract CoWare C models.
2.1.5 CHINOOK
The Chinook system aims at providing automated interface synthesis within a hardware software co-design framework for embedded systems BCO96, COB95, CB94, CWB94, COB92 . A single speci cation language, e.g. Verilog HDL, contains both behavioral and structural descriptions of the application, including information about the processors, peripheral devices, and communication interfaces that will be used. Parts of the behavioral speci cation are tagged for preferred implementation in a particular processor or dedicated hardware, with any untagged speci cation assumed to be implemented in software. All interactions with the devices and interfaces are speci ed using a procedural abstraction layer.
Process scheduling in Chinook is achieved by assigning di erent modes of operation to the overall system. A di erent schedule is associated with each mode. Timing watchdogs can disable modes and cause mode transitions. Upon changing to a new mode, the system starts running the corresponding schedule. Timing constraints may be intermodal or intramodal. Each mode has a periodic set of tasks, which is unrolled and scheduled under timing constraints, using an extension of the relative scheduling formulation KM92 . With this scheduling technique, Chinook supports the mapping of an embedded system model to one or more processor and peripherals while ensuring the satisfaction of timing constraints.
Chinook synthesizes device drivers, interface logic, and bus logic necessary for communication among hardware and software. For processors with general purpose I O ports, a heuristic allocates the ports to minimize interface logic; otherwise, memory-mapped I O is used, which includes allocating address spaces. Knowledge about the interfaces of processors and devices, which Chinook needs to carry out

CHAPTER 2. BACKGROUND

19

the synthesis, is captured in libraries. New e orts in the Chinook system emphasize distributed architectures HB97,
OB97 .
2.2 Hardware Scheduling
In this section we will brie y discuss some of the scheduling approaches used in highlevel synthesis of hardware. In this case we have a model containing a set of operations and dependencies. The hardware implementation is assumed to be synchronous, with a given cycle-time. Operations are assumed to take a known integer number of cycles to execute. We will later consider removing this assumption. The result of scheduling, i.e., the set of start times of the operations, is just a set of integers. The usual goal is to minimize the overall execution latency, i.e. the time required to execute all operations.
2.2.1 Integer Linear Programming
The scheduling problem can be cast as an integer linear program ILP Mic94 , where binary-valued variables determine the assignment of a start time to each operation. Linear constraints require each operation to start once and to satisfy the precedence and resource constraints. Latency can also be expressed as a linear combination of the decision variables. The scheduling problem has a dual formulation, where latency is bounded from above and the objective function relates to minimizing the resource usage, which can also be expressed as a linear function. Timing and other constraints can be easily incorporated in the ILP model.
The appeal of using the ILP model is due both to the uniform formulation, even in presence of di erent constraints, and to the possibility of using standard solution packages. Its limitation is due to the prohibitive computational cost for medium-large

CHAPTER 2. BACKGROUND

20

cases. This relegates the ILP formulation to speci c cases, where an exact solution is required and where the problem size makes the ILP solution viable.
2.2.2 List Scheduling
Most practical implementations of hardware schedulers rely on list scheduling, which is a heuristic approach that yields good but not necessarily optimal schedules in linear or overlinear time. A list scheduler considers the timeslots one at a time, and schedules to each slot those operations whose predecessors have been scheduled, if enough resources are available. Otherwise the operation execution is deferred. Ties are broken using a priority list, hence the name.
2.2.3 Relative Scheduling
The synchronization of two or more operations or processes, often with exact cycle minimum and maximum separation timing constraints, is an important issue in hardware scheduling. Synchronization is needed when some delay is unknown in the model the assumption that all operations take a known integer number of cycles to execute is removed. Relative scheduling is an extended scheduling method to cope with operations with unbounded delays KM92 called anchors. The presence of an anchor means that a static schedule cannot be determined. Nevertheless, in relative scheduling the operations are scheduled with respect to their anchor ancestors. A FSM can be derived that executes operations in an appropriate sequence, on the basis of the relative schedules and the anchor completion signals. Relative scheduling support the analysis of timing constraints; when these constraints are consistent with the model, any resulting schedule generated is guaranteed to satisfy the constraints for any anchor delay.

CHAPTER 2. BACKGROUND

21

2.2.4 Conditional Process Graphs
A recently published paper considers the case where a Directed Acyclic Graph DAG speci es a set of processes with precedence constraints EKP+98 . Each edge in the DAG may have a conditional associated with it.
The goal is to generate a static schedule which will minimize the execution time of the DAG for any allowable value of the conditionals. Since this may require activations of di erent tasks in di erent orders, they keep track of the possible paths using a schedule table. Alternative paths through the DAG are captured with BDDs. There may be a con ict, where, for example, the optimal schedule of one path requires that process P3 be scheduled at time tk, while the optimal schedule of another path
requires that P3 be scheduled at time tl, tk 6= tl. Con icts are handled by adjusting
one of the path schedules. This technique is applicable to hardware software systems. The end result is a
distributed run-time scheduler composed of non-preemptive schedulers. Conditionals are broadcast so that individual schedulers can dynamically choose the appropriate schedule for the processes under their control. For the case where each process can be allocated either to hardware or to a programmable processor, then this scheduling technique applies to hardware software co-design.

2.3 Software Scheduling
We will next examine some representative examples of previous approaches to scheduling for real-time software systems. The goal is software scheduling to meet real-time constraints. In the following, the assumption is that a large scale software system, with hundreds or thousands of individual tasks with many di erent periods and deadlines, is being designed.

CHAPTER 2. BACKGROUND

22

2.3.1 Round-Robin Scheduling
The round-robin scheduling algorithm takes a small slice of time and allocates each process on a circular queue the time slice. If the process takes less than the time slice to execute, then the scheduler immediately goes to the next process in the circular queue. Otherwise it preempts the currently executing process at the end of the time slice and runs the next process in the queue. As the size of the time slice approaches in nity, the round-robin policy becomes the same as the First-Come-FirstServed FCFS policy. While this algorithm is very predictable and by design avoids starvation and deadlock, unfortunately it can result in large average waiting time and many extra context switches.
2.3.2 Shortest Job First
The shortest-job- rst scheduling algorithm requires that each process have associated with it the length of uninterrupted CPU execution it needs next. This length can either be the entire length of the process or the length of the next CPU burst where it will heavily use the CPU as opposed to waiting on I O or for synchronization with other processes. Then, shortest-job- rst assigns the CPU to whichever available process has the smallest length of uninterrupted CPU execution associated with it. When that process nishes, the CPU is assigned again to the process with the shortest length. While the shortest-job- rst algorithm is optimal in terms of minimizing the average waiting time, it may result in missing timing constraints where another schedule would have met the timing constraints.
2.3.3 Rate-Monotonic Analysis
Rate Monotonic Analysis RMA LL73 and Generalized Rate Monotonic Analysis GRMA SRS94 both assume that tasks are independent and that each task has its

CHAPTER 2. BACKGROUND

23

own period and deadline which are the same and never change. Furthermore, each task is assumed to have a constant run-time which does not change over time. In RMA, the rate-monotonic priority assignment assigns higher priorities to tasks with higher priorities. Such a priority assignment has been proven optimal in the sense that no other xed priority assignment can schedule a set of tasks which cannot be scheduled without missing deadlines by the rate-monotonic priority assignment LL73 . Liu and Layland were able to prove the following theorem:

Theorem 2.1 A set of n independent periodic tasks scheduled by the rate-monotonic
algorithm will always meet their deadlines for all task start times, if

C1 T1

+

C2 T2

+

:

:

:

+

Cn Tn



n2

1
n

,

1

where Ci is the execution time and Ti is the period of task i.

Theorem 2.1 ignores all overheads assumed to be zero. The bound on the uti-

lization

n2

1
n

,

1

rapidly

converges

to

ln

2

=

0.69

as

n

becomes

large.

GRMA adapts the RMA framework to deal with problems typically faced in real-

time software systems. For example, a typical problem is priority inversion. This

occurs when a lower priority process holds a critical resource, thereby preventing

a higher priority process from executing when it interrupts and tries to access the

critical resource: the priorities of the two process have been inverted because the

lower priority process has, in e ect, made itself higher in priority. Thus, GRMA

supports the priority ceiling protocol, which avoids mutual deadlock arising from the

priority inversion problem SRL90, Raj91 . The deadlock is avoided by having any

lower priority process holding a critical resource inherit the priority of any higher

priority processes which try to access the resource, until the lower priority process

CHAPTER 2. BACKGROUND

24

releases the resource at which point the lower priority process resumes its original priority.
RMA has also been extended to account for release jitter and some cases of resource contention ABD+95, ABR+93 .
However RMA and GRMA both fail when the tasks have precedence constraints. We assume the presence of precedence constraints in the system in this dissertation.
2.3.4 Least Common Multiple
RMA has also been extended to allow precedence among tasks by formulating the problem as a big task with the length of the Least Common Multiple LCM of all the periods Ram95, Ram90, PS89 . Unfortunately, this approach is usually impractical for hardware software co-design for several reasons:
First of all, it is di cult to handle a situation where the period and computation times are nondeterministic but bounded, since a period of a LCM does not represent all possible situations YW96, YW95 .
Secondly, the task periods can be large and co-prime, resulting in a LCM too large to be practical.
Thirdly, it discourages static allocation and scheduling because it treats di erent instances of the same task as di erent nodes in the LCM.
One approach to deal with the third problem mentioned above is to use the concept of an association array which keeps track of the priority level, allocation to hardware or CPU, deadline, and best worst-case execution time for each copy of each task DLJ97, DJ98 .

CHAPTER 2. BACKGROUND

25

2.3.5 Real-Time Kernel in Hardware
A common approach is to encapsulate software scheduling algorithms into a fast but general purpose operating system, called a Real-Time Operating System RTOS. The basic idea is to provide the functionality needed by real-time software systems without the large overhead associated with traditional operating systems. A good overview of RTOS research in scheduling algorithms is contained in SSNB95 .
One interesting RTOS research direction implements a small real-time kernel in hardware to run in parallel with multiple processors AFLS96, LSF95, Lin92, LS91 . The real-time kernel contains a scheduler with a priority scheduling algorithm, a dispatcher which controls the task switch mechanism, a wait queue for inactive tasks, a wait queue for tasks waiting for a time event, and a ready queue. A prototype of the system contains a VME bus connecting the real-time kernel in hardware, a bus arbiter in hardware, a large RAM, and three processors. To schedule a software task on a particular processor, the kernel triggers an interrupt on the processor, which results in placing the task id of the new task in a register on the processor. The new task id is read from that register, performing a task switch. The real-time kernel can handle a maximum of 64 tasks at 8 priority levels AFLS96 .

2.4 Control-Flow Expressions
Control- ow expressions CFEs CM96, Coe96, CM97 support system-level specications in an algebraic formalism that considers most of the language constructs used to model systems reacting to their environment, i.e. sequential, alternative, concurrent, iterative, and exception handling behaviors. Such constructs are found in languages such as C, Verilog HDL, VHDL, Esterel and StateCharts. CFEs can specify control ow that satis es relative timing constraints minimum and maximum

CHAPTER 2. BACKGROUND

26

Composition HL Representation CF Expression

Sequential begin P; Q end p  q

Parallel fork P; Q join pkq

if C

Alternative

P;
else

c:p+c:q

Q;

while C

Loop P ;

c : p

wait !C
P;

c : 0  p

In nite

always
P;

p!

Table 1: Link between Verilog HDL Constructs and Control-Flow Expressions

separation KM92 in hardware while also controlling dynamically the ow of execution.
2.4.1 Formalism
Input events of a control- ow are speci ed by conditionals, which enable di erent blocks of the speci cation to execute based on the input. Output control signals are speci ed by actions which control execution according to the control- ow; for example, an action becoming asserted may indicate that a multiplier should begin execution. Table 1 shows the correspondence between CFEs and standard Verilog HDL control- ow constructs.
Example 1 Suppose we have an alternative choice where based on conditional c, we execute
either an adder, represented by CFE action a, or a multiplier, represented by CFE action m.
The CFE description of this conditional choice, assuming that the adder and the multiplier are
single-cycle actions, is as follows: c : a + c : m 2

CHAPTER 2. BACKGROUND

27

Any CFE expression can be compounded sequentially or in parallel with any other CFE expression. Furthermore, with CFEs one can represent the control ow of most digital systems.
Synchronization constraints are speci ed through the use of NEV ER and ALWAY S
sets. NEV ER sets model mutual exclusion; for example, NEV ER = fa; b; cg indi-
cates that actions a, b, and c can never be active at that same time. In a similar vein, tasks that must begin execution concurrently are speci ed through the use of
ALWAY S sets; e.g. ALWAY S = fa; b; cg indicates that tasks a, b, and c must
each begin execution at the same time. Thus, with conditionals, NEV ER sets, and ALWAY S sets, CFEs can specify and consider constraints crossing concurrent blocks of the design, which are generally ignored in other synthesis tools.
Using conditional execution within a loop models an action with unbounded delay, e.g. as shown by the CFE construct c : p. Using a composition of these unbounded conditional executions can model a sequence of hardware and software tasks with unknown delay. Thus, with CFEs one can dynamically control the execution task, beginning a subsequent task after, and exactly after, all of its preceding tasks have nished execution.
Example 2 Suppose we want to execute the following tasks in nitely often: a followed by b,
in parallel with d followed by e. Tasks a and d have nondeterministic delay. Tasks b and e take one cycle each. There are no constraints, i.e. no relative timing constraints, NEV ER sets, or ALWAY S sets. Associating a CFE control signal and a CFE action with each task, we end up
with the following expression recall that in CFE semantics,  indicates zero or more cycles, jj indicates parallel execution,  indicates serial execution, and ! indicates an in nite loop: c1 : a  bjjc2 : d  e! 2

CHAPTER 2. BACKGROUND

28

2.4.2 Synthesis with CFEs
The synthesis procedure begins by converting the control- ow expressions into an automaton where design constraints such as timing, resource and synchronization are incorporated. The synthesis is conservative in the sense that a solution is produced only if the speci ed constraints, such as relative timing constraints, are satis ed. In order to generate the control-units for the design, two scheduling procedures are used. The rst procedure, called static scheduling, attempts to nd xed schedules for operations satisfying system-level constraints. The second procedure, called dynamic scheduling, attempts to synchronize concurrent parts of a circuit description by dynamically selecting schedules according to conditionals from the rest of the system. The solution of both scheduling problems are cast as Integer Linear Programming instances and solved using Binary Decision Diagrams.
CFEs

Control Flow Automaton
Thalia2
BDD Solver

Verilog

Control-units in synthesizable Verilog

Figure 3: Thalia2 Synthesis from CFEs

CHAPTER 2. BACKGROUND

29

2.4.3 Thalia
The algorithms to synthesize a controller from a CFE speci cation have been implemented in a tool called Thalia CM96, Coe96 which outputs a logic description. This logic description can be synthesized by commercial e.g. Synopsys or research e.g. Olympus DKMT90 or SIS SSM+92  synthesis systems. We have modi ed Thalia slightly in order to output synthesizable Verilog HDL; we call the new tool Thalia2, as shown in Figure 3.

2.5 Bounds on Execution Time
Real-time systems design requires bounds on execution time of the various components. The lower bound is often referred to as the Best-Case Execution Time BCET, while the upper bound is referred to as the Worst-Case Execution Time WCET. Recent previous approaches to such real time analysis have focused on software, since the performance analysis of ASICs is considered a well studied problem already. One such approach is that of the cinderella tool MWWL96, LM95 , which this section discusses since we will use cinderella in the timing analysis presented in this thesis.
cinderella addresses the problem of determining BCET and WCET bounds for a given program executed on a given processor, assuming uninterrupted execution. Two important issues in solving this problem are i program path analysis, which determines which sequence of instructions will be executed in the worst case, and ii microarchitecture analysis, which requires modelling the hardware system to determines the BCET and WCET execution bounds for a given sequence of assembly instructions. cinderella divides the assembly code into its basic blocks and extracts the control- ow among the blocks. Thus, explicit path enumeration is not required by cinderella. Software is assumed to not have dynamic memory allocation nor recursive function calls. However, loops are allowed as long as the user

CHAPTER 2. BACKGROUND

30

can provide upper bounds for each loop speci ed. Then, using an ILP formulation, cinderella nds a BCET and a WCET for the program in clock cycles of the microprocessor clock. Target microarchitectures include the Intel i960KB processor and the Motorola 68000 processor MWWL96, LM95 . We have ported cinderella to the MIPS R4000 processor.
2.6 Summary
We have shown some representative samples of previous work in hardware software co-design and algorithms for real-time scheduling of software. In hardware software co-design, we have seen several systems for hardware software partitioning and evaluation of di erent hardware software tradeo s. Only a few of the systems considered have as a primary goal the support of heterogeneous input in both a software language and an HDL, which will be the approach we take in this thesis. Finally, some of the previous work in scheduling for hardware and for nding bounds on the execution time of software was discussed.
None of the previous hardware software co-design systems focus on run-time scheduling, and all place the run-time scheduler on the CPU hardware is assumed to have a static schedule implied by the input description of the application. This thesis extends previous work to support dynamic interleaving of hardware-software execution, where by dynamic we mean that exact start times are not statically determined but instead are decided at run-time based on actual execution times. To achieve this, we present the rst implementation of a run-time scheduler split partly in hardware and partly in software. Furthermore, real-time analysis will be provided to determine a WCET for the system.
In the next chapter, we will explain the target architecture for our system, the small kernel to control software running on a microprocessor, run-time scheduler

CHAPTER 2. BACKGROUND
implementation details, and the approach to modelling of the system.

31

Chapter 3
Target Architecture, Kernel, and System Model
We aim at supporting system-level design with hardware software tasks custom designed for a target architecture. We refer to the tasks in hardware as hardware-tasks and to the tasks in software as software-tasks. We assume the existence of mature high-level synthesis tools and software compilers, as well as intellectual property in the form of processor and controller cores. We assume that the system requires both static scheduling, especially in the coordination of hardware-tasks, and dynamic scheduling, given the inexact delay of software and the randomness of the stimuli coming from the environment. A run-time scheduler must meet both of these scheduling requirements. We will present the Serra tool, which automates the generation of the run-time scheduler, thus providing for the synchronization and scheduling of system-level components in hardware and software.
Our approach assumes a coarse-grained partition of the system into tasks. We assume tasks model system components of signi cant sizes, and that the system consists of around ten to a hundred tasks. The tasks are assumed to model either hardware or software and to be written either in Verilog HDL or in C. This approach matches
32

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL33
design practice, where designers often describe their systems in a heterogeneous way, using description languages appropriate to the subsystem being implemented.
3.1 CAD Requirements
Figure 4: PUMA Arms Courtesy of the Computer Science Robotics Lab at Stanford
Example 3 As a motivational example, consider the set of control algorithms of Figure 5.
These algorithms calculate torques for the PUMA AKB86, Uni84 robot arms shown in Figure 4. We assume that the controller manages two arms at the same time, and thus any two of
the algorithms may be selected in each execution. An execution of the arm controller must complete calculation of new torques for the arms once every millisecond. Since each arm has six degrees of freedom, only six new torque values need to be communicated for each update; thus, the amount of data ow in the system is small. However, the algorithms  laws" in robotics terminology need to maintain oating point matrices representing the kinematics and dynamics of the arms, so that the computation would be di cult to represent concisely in, for example, nite-state machines. This control approach is also drastically di erent than the fuzzy logic adaptive control in ACJ96 .

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL34

Ohold2 Law:

Ohold Law:

Jhold Law:

singular
01
find Ohold jacobian Law

calc joint dynamics
calc lambda mu
sum vector

matrix vector multiply
oh2 1 0 saturate
velocity oh3
matrix vector multiply
sum vector

forward kinematics

find jacobian

oh2 sat

matrix vector multiply

oh3

calc gravity

find jacobian transpose
matrix vector multiply

oh4

calc gravity
calc joint dynamics
matrix vector multiply
matrix vector multiply
matrix vector multiply

convert force torque

Figure 5: Robotics Example: Concurrent Control Algorithms

Figure 5 shows three of the ten di erent algorithms laws used with a PUMA arm; Ohold2 Law, Ohold Law, and Jhold Law are top-level tasks which call subtasks in a particular sequence.
The coarse-grained partitions of Ohold2 Law, Ohold Law, and Jhold Law contains calls to many
common subtasks. Some of the subtasks involve hardware components with timing constraints
speci ed on a cycle basis. 2

The CAD requirements for co-design of a system such as Example 3 are as follows. First, we need to satisfy hard real-time constraints imposed by some of the hardware components in the system as well as by external hardware. Second, we need to optimize the run-time system over calls to multiple tasks in hardware and software. This involves allocation of tasks to hardware and software as well as interface generation for communication. Third, we need to guarantee a hard real-time rate constraint across tasks in hardware and in software. The handling of multiple-rate constraints is beyond the scope of this thesis.
We design our run-time scheduler CAD tool, which we call Serra, to work with

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL35

behavioral Verilog

System Specification C

constraints

User options (protocols, fifos, RAM model, core)
BC

Interface Generation
wcet

behavioral Verilog

Serra Run-Time Scheduler Synthesis
RTL Verilog

wcet cinderella-M
User options (microprocessor core, RAM size)

BC DC

V1 ... Vn
CPU 
Core1 L1

RTS.v
RTS.c
RAM *.c

Figure 6: Tool Flow and Target Architecture

existing hardware synthesis and software compilation tools. Figure 6 shows the tool ow in which Serra is embedded. Hardware-tasks are speci ed in Verilog HDL that can be synthesized by the Synopsys Behavioral CompilerTM Kna96 labeled BC in Figure 6; DC labels the Design CompilerTM. Software-tasks are written in C. Microprocessor cores, memories DRAM, SRAM, FIFO models, and other custom blocks are assumed as available inputs to the system.
The system-level tasks in Verilog HDL and C, as well as constraints, are input to a tool that generates the interface and to Serra. Constraints include relative timing constraints minimum and maximum separation, resource constraints, and a single rate constraint. The implementation of the synthesized system can vary from a system on a chip to a board or set of interconnected components. The overall control data ow of the run-time scheduler is synthesized into hardware, while the necessary code for calling tasks in software is generated as well. Further aspects of an RTOS can

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL36
be added in software by the user if desired, although Serra's WCET calculation assumes that only the software which Serra generates is run on the microprocessor.
We wrote a new backend for the tool discussed in Section 2.5, cinderella MWWL96, LM95 . The new backend is for MIPS assembly run on a MIPS R4000 processor; we call the new tool cinderella-M. Software-tasks are compiled and input to cinderella-M, which outputs a WCET for each task. Similarly, from the hardware synthesis of the Synopsys Behavioral CompilerTM BCTM Kna96 , we obtain an exact execution time for each hardware-task, which we take as a WCET for the hardware-task. The WCET value for each task is required in order to analyze whether or not we will always meet our rate constraint.
The run-time scheduler synthesis of Serra supports the execution of softwaretasks through an interrupt triggering mechanism where hardware communicates to a software scheduler which of the software-tasks are ready to execute. The Clara tool, which is embedded within the Serra system, takes as input the worst case execution time WCET for each task and then provides for the automated generation of priorities for the software-tasks to be run on a preemptive xed priority scheduler as well as the serial order for hardware-tasks executed using the same hardware resource. These software-task priorities and hardware-task serial orderings are chosen to minimize WCET for subsets of hardware- and software-tasks under a hard realtime rate constraint. Thus, Serra provides the user with the ability to evaluate the performance of di erent partitions with an automatically generated run-time scheduler system. For example, the user can migrate a task from C to Verilog HDL to speed up a critical path in the algorithm.
This thesis focuses on the synthesis and analysis of a custom run-time scheduler.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL37
3.2 Target Architecture and Kernel
Our target architecture consists of a CPU core with multiple hardware modules, each implementing a particular hardware-task. The CPU has a two-level memory hierarchy consisting of instruction and data caches with a large RAM. Since we target embedded systems, we assume that the RAM is large enough to hold all the program code needed.
3.2.1 Task Execution
We associate a start and a done event with each task in order to allow the scheduler to control the task. In hardware the two events are simply signals on an input port and an output port, respectively. For software, we have a start vector and a done vector which encapsulate the start and done events for each software-task.
Note that some tasks are called multiple times by other and di erent tasks, such as matrix vector multiply in our robot example, as can be seen in Figure 5. Some real-time constraints in hardware can be satis ed by high-level synthesis. However, constraints at the task level must be handled by the run-time system. How can the run-time system dynamically allocate tasks while at the same time predictably satisfying exact timing constraints between tasks?
The solution to predictability comes from a hardware solution with cycle based semantics. Thus, constraints between events in exact units of cycles can be predictably met. We solve this scheduling problem using a hardware cycle based FSM implementation of the part of the scheduler which chooses which tasks to execute next.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL38
3.2.2 Run-Time Scheduler Implementation
We split the run-time scheduler into hardware and software based on an analysis of the constraints. We hypothesize that exact relative timing constraints between tasks cannot be satis ed by software. Thus, we have the problem of choosing between the predictability of satisfying real-time constraints in hardware and the desirability of having some features of an RTOS. We try to accommodate both choices by putting in hardware a FSM corresponding to the task control ow of the system, while putting in software a reactive executive which calls the appropriate software-tasks when signaled by the hardware FSM.
Therefore we split the run-time scheduler into two parts:
An executive manager in hardware with cycle-based semantics that can satisfy hard real-time constraints.
A preemptive static priority scheduler that executes di erent threads based on eligible software-tasks as indicated by the start vector.

CPU core1

start done

L1

64

CPU Interface int

RTS.v
start done

memory controller

start done
V1

start
... done Vn

Figure 7: Target Architecture

RAM

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL39
Figure 7 shows the target architecture of our system. At the top we have a CPU core with a level 1 cache and copies of the start and done vectors in on-chip registers. The bottom shows n hardware tasks V 1 through V n. The executive manager hardware FSM is labeled RTS:v and generates all the start events as well as receives all the done events. This FSM is synthesized to implement the overall system control and can predictably meet the relative timing constraints, if satis able, speci ed in exact numbers of cycles between the start times of tasks.
3.2.3 Control of Software
The hardware run-time scheduler updates the start vector in software as follows. First, it updates its local register containing the start vector. Then it triggers an interrupt on the CPU. The CPU interrupt service routine ISR reads the register using memory-mapped I O and places it into the software copy of the start vector. Figure 7 shows both the start and done vectors in registers in RTS:v and their copies in on-chip registers in CPUcore1.
The start vector may specify that several software tasks are ready to be executed. Thus, we generate a preemptive static priority scheduler which executes the highest priority software-task among the tasks indicated by the hardware FSM as ready to execute. The priority-based scheduler is always called by the ISR after fetching the new start vector into memory, and whenever a software-task terminates.
When a software-task is nished executing, it updates the done vector by writing the new value of done out with memory mapped I O. Thus, the done vector in the run-time scheduler in hardware is updated. Notice that in the above two cases, a dedicated port could be used instead of memory-mapped I O, depending on the CPU.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL40
3.2.4 Software Generation
For the software that runs on the microprocessor core CPU, the individual softwaretasks are compiled and linked using standard C compilers and linkers. The software tasks are compiled and linked into assembly, with data and program memory statically allocated. Memory-mapped I O is called with C pointers set explicitly to the appropriate addresses. We thus have a table of software-tasks and their entry points as shown in Table 2.
Entry Value 0 Pointer to sw-task 0 1 Pointer to sw-task 1 . ... n Pointer to sw-task n Table 2: Entry Table for Software-Tasks
Therefore, given a particular value of the start vector, the appropriate softwaretasks can be executed. The typical sequence of events in software is as follows:
A hardware interrupt trigger the execution of the ISR.
The ISR updates the start vector and, if a higher priority task has become ready, calls save .context
A priority scheduler updates the task data structure and executes the highest priority task now ready. If needed, the priority scheduler calls restore .context
When a software-task is nished, it writes out the new value of the done vector.
An advantage of this approach is that it can support standard RTOS scheduling algorithms round-robin, rate-monotonic, etc., although we only consider a static

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL41
 xed priority scheme here. Multiprocessing is helpful when a low-priority, long duration software-task is ready to execute at the same time as a high priority, short duration software-task, but a price is paid when switching context. A disadvantage of multiprocessing is the slower response time due to added overhead for implementing the RTOS scheduling algorithm, polling executive, and associated context switches.
Another possible option which has lower overhead is to have the ISR directly invoke each software-task, executing each task in kernel mode, as discussed in MSM97 . Such a scheme, however, does not allow a lower priority task to execute while an unexecuted higher priority task is not yet ready. Thus, in this thesis we only consider a priority driven scheme.
3.2.5 Priority Scheduler Template for Software
A task can be in one of two states: running=suspended or ready=terminated. In our simpli ed real-time operating system, once a software-task has completed terminated, it is ready to run again, so we overlap the traditionally distinct ready and terminated states into one. The running=suspended state, combined with the information in the start and done vectors, tells us whether or not restore context needs to be called before invoking the highest priority task. In particular, if a higher priority task just nished execution and the next highest priority software task ready to execute is in the running=suspended state, then we know that it must have been executing earlier at some point. Thus, we execute a restore context for that process. Otherwise, we simply jump to the starting PC for the task.
Note that the interrupt service routine ISR is responsible for calling save context if needed. The register le that contains the process state information is saved only when the new start vector indicates that a higher priority task is now ready to execute i.e. we eliminate context switching when one task ends and a new task begins, in which case there is no need to save restore the register le.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL42

In operating systems terms, the run-time scheduler software portion implements priority-based job scheduling multiprogramming. Strictly speaking, this is not multitasking since there is no time-shared access to CPU compute cycles.
Clearly, for this implementation to work, we need a priority for each softwaretask. We obtain the priorities from the real time analysis, which will be explained in Chapter 4. We now turn to modeling issues.
3.3 System Modeling

Wait Next Tick (wnt)

Get Position1
(gp1)

Get Velocity1
(gv1)

Get Position2
(gp2)

Get Velocity2
(gv2)

Epsilon

Note: wnt, gp1, gv1, gp2, gv2, jh,oh,st1,st2, and hm are the start events associated with each task.

Ohold1 Law (oh)

Jhold Law (jh)

Which two laws to execute concurrently in the next iteration are selected by

Set Torque1
(st1)

Set Torque2
(st2)

Handle Message
(hm)
Figure 8: Robotics Example: Main Task

The input speci cation is a collection of tasks written in Verilog HDL or C, with one of the tasks designated as the main task. The main task begins execution and calls the other tasks. The main task speci es the overall sequence of tasks in the application an example of a main task can be seen in Figure 8. From each task we extract a Control Data-Flow Graph CDFG of the tasks it invokes, where each node in the CDFG corresponds to a call to another task. If a task does not call any other

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL43

task, then it has no such CDFG. We call this kind of task a leaf task. A task which is not the main task nor a leaf task is an intermediate task. An intermediate task must trace back its invocation to the main task, and the intermediate task must itself invoke at least one leaf task. We assume that an intermediate task has all computation speci ed in leaf tasks. If an intermediate task does contain some computations, a new leaf task can be generated containing these computations. This allows us to atten the hierarchical description and generate a CDFG of the system where all nodes are leaf tasks. We assume that we have a rate constraint speci ed for the CDFG of the system. In other words, we assume that the main task is invoked at a xed rate.

wnt

gp1
Ohold1 Law oh0

gv1

gp2

epsilon

fk oh1

mvm1

gv2
Jhold Law cg
cjd mvm2 mvm3 mvm4

2
xf1 -8 xb1

Set Torque

xf2

2
-8 xb2

hm
Figure 9: Flattened CDFG of Robot Arm Controller

Example 4 Figure 8 shows the overall ow of execution of the robot controller in the form
of a CDFG of the main task for the system. The original speci cation of the main task was in Verilog HDL. The other tasks are speci ed in C and Verilog HDL.
Note that the CDFG of Figure 8 must complete once every millisecond. Thus, we have a rate constraint on the graph.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL44
An example a attened CDFG where all the nodes are leaf tasks can be seen in Figure 9. The attened CDFG executes an appropriate subset of the control algorithms of Figure 5 to output torques for two PUMA robot arms. In this case, since there is no branching, the CDFG
is equivalent to a DAG with relative timing constraints. 2
Serra uses the Synopsys Behavioral CompilerTM for the synthesis of hardware tasks. Serra leverages previous research on system modeling using control- ow expressions CFEs CM96, Coe96 , as covered in Section 2.4. In Serra, CFEs represent an intermediate model of the run-time system that captures the global control- ow information in the system. This contrasts with earlier uses of CFEs to model systems at the operation level MCSM96 . Using CFEs to coordinate tasks hides the coordination of low-level operations from the CFE model and results in greatly reduced control logic. We assume that the total number of tasks in the system is around 50 to 100.
We support the speci cation of tasks that cannot execute concurrently through the use of CFE NEV ER sets. In general, NEV ER sets can model mutual exclusion; here, we use NEV ER sets to model resource constraints. We make use of this feature
to specify resource constraints such as i multiple calls to the same piece of physical hardware which implements a hardware-task, or ii software-tasks executed on the
same microprocessor. In this thesis, we consider any number of NEV ER sets. For a target architecture of one CPU core, it makes sense to have a single NEV ER set of software-tasks, which we use to serialize the software-tasks executed on the same CPU, and multiple NEV ER sets of hardware-tasks. This is the case we focus on in this thesis.
Thus, we do not consider ALWAY S sets explicitly in the formulation of our problem.

CHAPTER 3. TARGET ARCHITECTURE, KERNEL, AND SYSTEM MODEL45
3.4 Summary
In this section we outlined our approach and design style for hardware software codesign of a run-time scheduler split between hardware and software. We presented a target architecture and small software kernel to manage software-tasks. Finally, we reviewed our system-level modeling of task ow for any speci ed application.
In the next chapter, we will focus on the analysis and synthesis of a custom runtime scheduler, which requires the satisfaction of a single rate constraint and multiple resource constraints.

Chapter 4
Real Time Analysis
We aim at predictably satisfying real-time constraints in the form of control dataow precedence constraints, resource constraints, and a rate constraint. We assume that we have as input a CDFG representing the ow of tasks in the application, a rate constraint on the graph, and NEV ER sets specifying a resource constraint on software-tasks and resource constraints on hardware-tasks. In this chapter, we rst show a formulation which does not include NEV ER sets of hardware-tasks hardware resource constraints for the sake of simplicity of explanation. We expand the formulation to include multiple NEV ER sets of hardware-tasks in Section 4.2.3.
The outline of this chapter is as follows. In Section 4.1, we rst present the assumptions we make in de ning our problem and prove the resulting decision problem to be NP-complete. In Section 4.2 we present the Constructive Heuristic Scheduling approach to solving our problem, which is the ordering of tasks which use the same resource and thus must execute in a mutually exclusive fashion. In Section 4.2.3 we extend the heuristic of the previous section to deal with the case of multiple sets of mutually exclusive tasks. Section 4.3 presents analysis and a greedy heuristic to deal with the case of preemptible tasks which leads to increased context switches. Finally, Sections 4.4 and 4.5 improve the heuristic and extend it to support
46

CHAPTER 4. REAL TIME ANALYSIS

47

critical regions, thus providing the same functionality as semaphores.
4.1 Assumptions and Complexity
To predictably satisfy a rate constraint, we need a worst case execution time WCET for each task and a WCET for the control data- ow of the set of tasks under the rate constraint. We obtain the WCET times for the individual tasks from cinderellaM and BCTM Kna96 as mentioned in Section 3.1. We need some assumptions to compute the WCET for the set of tasks.
Assumption 4.1 We have a Directed Acyclic Graph DAG representing a set of
tasks,a WCET for each task, and a NEV ER set specifying tasks that must be executed
in a mutually exclusive manner. A rate constraint is speci ed for the execution of the whole graph.
Example 5 Figure 9 shows the DAG resulting from the parallel execution of Jhold Law and
Ohold1 Law. While the full CDFG can select more combinations, e.g. Ohold2 Law and Jhold Law, we consider here only the case where Jhold Law and Ohold1 Law are selected to execute in parallel. In other words, the CDFG has been e ectively reduced to a DAG. Note that the system is still dynamic since the start and done times of tasks in the DAG are not determined ahead of
time but are handled at run-time. Also, the DAG may contain relative timing constraints. 2
Note that reducing the CDFG to a DAG limits the amount of control- ow information in the graph to relative timing constraints among tasks. In particular, a control choice equivalent to branching statement is not modelled. Also note that for now we consider only a single NEV ER set of software-tasks executed on the same CPU. We assume that we have the resulting DAG in graph form GV; A, where V is the set of vertices and A is the set of directed edges.

CHAPTER 4. REAL TIME ANALYSIS

48

Assumption 4.2 Software is executed by a simple priority scheduler consisting of
four code segments: an interrupt service routineISR, a priority scheduler, a save context routine and a restore context routine.
Note that the priority scheduler is compiled for each embedded application; the other three routines are written in assembly and do not require any recompilation.
Assumption 4.3 Each task, once started, runs to completion.
Together with the previous assumption and the fact that the priority scheduler code only uses registers reserved for the operating system, we nd that the only overhead for software-tasks are the ISR and priority scheduler calls. We will relax the assumption of running to completion later when calculating WCET involving software-tasks which can be partially executed before being interrupted.
Assumption 4.4 Hardware-software communication time is included in the WCET
of each task and or is included as a distinct task.
We have several communication primitives, such as shared memory and FIFOs, with interface generation along the lines of COB95, MBL+96, VRBM96 .
Assumption 4.5 Interrupts that switch context come only from the hardware run-
time scheduler as described in Section 3.2.3.
Example 6 As an example, consider Figure 10. This represents a subset of the tasks from
the robot controller shown in Figure 9. The WCET times for the individual tasks have already
been calculated by cinderella-M and BCTM. Three tasks are speci ed in Verilog HDL: mvm, fk, and cg, corresponding to matrix vector multiply, forward kinematics, and calc gravity, respectively, in Figure 5. Task mvm has four instantiations in mvm1-4. Similarly, three tasks are speci ed in C: oh0, oh1, and cjd, where cjd corresponds to calc joint dynamics in Figure 5 and both oh0 and oh1 are coarser-grained groupings of tasks called

CHAPTER 4. REAL TIME ANALYSIS

49

src

Ohold1 Law oh0

Jhold Law cg
cjd

fk oh1

mvm2

mvm1 mvm3

NEVER = {oh0,oh1,cjd}

Task ---- cg oh0 oh1 fk cjd mvm1 mvm2 mvm3 mvm4 src sink

BCET (cycles) ----- 4,000 1,598 12,341 4,500 9,989 4,400 4,400 4,400 4,400 0 0

WCET (cycles) ----- 4,000 2,221 17,399 4,500 13,213 4,400 4,400 4,400 4,400 0 0

mvm4

sink
Figure 10: DAG, BCET and WCET: The leftmost column shows the task names, the middle column shows the Best-Case Execution Time, and the rightmost column shows the Worst-Case Execution Time.
by Ohold Law in Figure 5. Since our target architecture for this example contains only one
microprocessor, all three software-tasks are put into a single NEV ER set which states that
their execution times cannot overlap at all. Thus, the tasks must be serialized.
Consider the NEV ER set shaded in Figure 10. A rst-come- rst-serve scheduling algorithm
would schedule oh0 rst, then oh1 since mvm is still executing when oh0 nishes, and cjd
last. Without considering the small overhead of the priority scheduler, this results in a WCET
of 46,033 cycles for the graph. However, if oh1 were executed after cjd, the WCET would be
39,012 for the graph. 2
Example 6 shows a di cult problem in that a NEV ER set of software-tasks may cross parallel paths. This problem, which we refer to as the Never Set DAG Scheduling NSDS problem, cannot be solved with a single execution of a longest path algorithm because the execution start time of each task in a NEV ER set depends upon the scheduling of the other tasks in the NEV ER set. In fact, nding the serial order

CHAPTER 4. REAL TIME ANALYSIS

50

of tasks in the NEV ER set which minimizes WCET the NSDS problem will be shown to be NP-Hard in the next section.
4.1.1 The Complexity of NSDS
In this section we discuss the complexity of NSDS. As is customary, we cast NSDS as a decision problem. Note that in the following we distinguish between GraphWCET, the Worst-Case Execution Time for a set of tasks in a DAG, and WCET, the Worst-Case Execution Time for an individual task. We give a formal de nition of NSDS-decision as follows:
De nition 4.1 NSDS-decision INSTANCE: Directed Acyclic Graph GV; A, allowable GraphWCET of K for the graph, NEVER set N  V , set T of tasks with V = T and, for each task t 2 T , a length lt 2 Z+.
QUESTION: Is there a one-processor schedule for N that satis es the allowable
GraphW CET of K for the DAG, i.e. a one-to-one function : T ! Z0+, with, if ti 2 N; tj 2 N ,ftig, either ti tj implying ti  tj+ltj or tj ti implying tj  ti + lti, such that, for all t 2 T , t  tp + ltp, where tp
is a predecessor of t in GV; A, and sink + lsink K, where sink is the sink task in GV; A.
Notice that for the case of two tasks in the NEVER set, ti 2 N; tj 2 N , ftig, forcing either ti tj implying ti  tj + ltj to be true or tj ti implying tj  ti + lti to be true ensures a serial order for software-tasks executed on
the same processor. Note that records the start times for the tasks in GV; A. Also
note that for each task t 2 T, lt is equivalent to a WCET for t. For the actual
NSDS problem, of course, we are not given a maximum allowable GraphWCET of K, but instead we try to nd the minimum such K possible.

CHAPTER 4. REAL TIME ANALYSIS

51

Now, to analyze the complexity of NSDS-decision, we use the Sequencing with Release Times and Deadlines SRTD problem GJ79 , which is de ned as follows:

De nition 4.2 SRTD INSTANCE: Set T of tasks and, for each task t 2 T, a length lt 2 Z+, a release time rt 2 Z0+, and a deadline dt 2 Z+.
QUESTION: Is there a one-processor schedule for T that satis es the release time
constraints and meets all the deadlines, i.e. a one-to-one function : T ! Z0+, with, if t0 2 T , ftg, t t0 implying t  t0 + lt0, such that, for all t 2 T, t  rt and t + lt  dt?

The Sequencing with Release Times and Deadlines SRTD problem deals with the situation where the task t is executed from time t to time t + lt, cannot start executing until time rt, must complete by time dt, and cannot overlap the execution of any other task t0.

Theorem 4.1 NSDS-decision is NP-Complete.

Proof: First, given an order for the software-tasks which are all in the

NEVER set N, one can check in polynomial time if the GraphWCET

of the DAG is less than K.

Next, SRTD can be reduced to NSDS-decision as follows.

KLettoanbeinesqtaunacl etoISRPT Dti

of
2T

SRTD lti +

be given maxti2T

with rti.

T = ft1; t2; : :
Note there

:; is

tng. De
no need

ne to

add any dt to K, since dt is just a deadline. Now we de ne an instance

INSDS,decision of NSDS-decision by assigning nodes in NSDS-decision for
each task ti 2 T as follows:

let si1 be a hardware-task with WCET equal to rti, where the predecessor of si1 is the source and the successor of si1 is si2 let si2 be a software-task with WCET equal to lti, where the predecessor of si2 is si1 and the successor of si2 is si3

CHAPTER 4. REAL TIME ANALYSIS

52

let si3 be a hardware-task with WCET equal to K , dti, where
the predecessor of si3 is si2 and the successor of si3 is the sink note that if dti K, then we remove si3 from the DAG
Basically, si1 enforces the release time constraint and si3 enforces the deadline. Now, place all software-tasks in a single NEVER set. Clearly, ISRTD has a solution if and only if INSDS,decision has a solution. QED.

src

s_11 s_12

s_21 s_22

s_13

s_23

NEVER = {s_12,s_22}

task ---- s_11 s_12 s_13 s_21 s_22 s_23

wcet (cycles) -----
0 3 4 2 4 3

sink
Figure 11: Example transformation of an SRTD problem to an NSDS problem.

Example 7 Consider an instance of SRTD with two tasks: task t1 with length lt1 = 3,
release time rt1 = 0, and deadline dt1 = 5; and task t2 with length lt2 = 4, release time rt4 = 2, and deadline dt1 = 6. Then, we calculate K = 3+4+2 = 9. We add the following tasks to the NSDS instance to account for t1 in SRTD: s11 with a W CET of 0, s12 with a
WCET of 3, and s13 with a WCET of K , dt1 = 9 , 5 = 4. Next, we add the following
tasks to the NSDS instance to account for t2 in SRTD: s21 with a W CET of 2, s22 with a
W CET of 4, and s23 with a W CET of K , dt2 = 9 , 6 = 3. The two tasks s12 and s22 are placed in a single NEV ER set. Figure 11 shows the resulting NSDS instance. 2
Trivially, since NSDS-decision is NP-Complete, NSDS is NP-Hard at least as hard to solve as an NP-Complete problem GJ79 . In the context of our system

CHAPTER 4. REAL TIME ANALYSIS

53

design, solving the NSDS problem allows us to proceed with our real-time analysis. For example, once we have a WCET for the CDFG of Figure 8, then we can say if the robot controller nishes execution within one millisecond.
4.2 Constructive Heuristic Scheduling
We want to nd a schedule for the tasks, with a NEV ER set containing all the software-tasks, where the other tasks are all hardware-tasks. We nd an ordering of the software-tasks using a problem formulation which is reminiscent of dynamic programming HL95 . The formulation enables us to construct in polynomial time a schedule of the tasks which minimizes WCET the heuristic may nd a local minimum. Our constructive heuristic scheduling algorithm allows us to take into account precedence constraints, a rate constraint, and a resource constraint in the form of a NEV ER set of software-tasks. In Section 4.2.3, we will extend constructive heuristic scheduling to include multiple resource constraints in the form of NEV ER sets of hardware-tasks.
4.2.1 Constructive Heuristic Scheduling Formulation
We take as input the DAG GV; A annotated with WCETs one per task, a NEV ER set specifying the mutually exclusive software-tasks, WCETisr which is a WCET for the ISR, and W CET prsched which is a W CET for the priority scheduler code.
We divide the problem into stages according to the number of tasks in the NEV ER set. We rst nd a solution for the last stage, then the second-to-last stage, etc., up to the rst stage we proceed in reverse order from the stage number. We use the following de nitions:
De nition 4.3 Let there be n stages, where in each stage we decide which among n
tasks to schedule.

CHAPTER 4. REAL TIME ANALYSIS

54

The number of stages n is set equal to the number of tasks in the NEV ER set plus two for the source and the sink.
De nition 4.4 Let t denote a task, and let ti denote a task executed in stage i.
De nition 4.5 Let the multivalued decision variables xik, i 2 1; 2; : : : ; n,1 and k 2
Z+, denote the ordered set of tasks from the NEV ER set executed in the subsequent stages, i.e. after stage i.
Note that xik represents an ordered set of tasks.
De nition 4.6 Let Xi, i 2 1; 2; : : : ; n,1, denote the multiset of decision variables fxikg.
Example 8 Consider Figure 10. Since jNEV ERj = 3, there are 5 stages. In stage 3 we
could nd that X3 = fx31; x32; x33g = foh0,sink,oh1,sink,cjd,sinkg. Each x3k is an ordered set, and X3 is a multiset. 2
De nition 4.7 Let state si = ti; xik in stage i denote the current task ready to
start execution and the subsequent tasks from the NEV ER set executed in stages
i + 1; i + 2; : : : ; n , 1, where the sink is always executed in stage n.
Note that given an ordering of software-tasks, the rest of the graph is scheduled with an As Soon As Possible ASAP schedule that takes into account the dependencies induced by the ordering of the mutually exclusive tasks.
Example 9 In Figure 10 the tasks under consideration are src, oh0, oh1, cjd, and sink.
Since the sink is always executed last, Xn,1 = X4 = fsinkg. The possible tasks executed
before the sink, and thus in stage 4, are t4 = oh1 and t4 = cjd. Thus the possible states in stage 4 are s4 = oh1,sink and s4 = cjd,sink. 2
We denote the WCET for task t by WCETt.

CHAPTER 4. REAL TIME ANALYSIS

55

src

oh0 fk oh1

cg cjd
mvm2

mvm1 mvm3

mvm4

sink

NEVER = {oh0,oh1,cjd}

---s-4 -- oh1,sink cjd,sink

GraphWCET(s ) ----- 4 21,799 26,413

Figure 12: GraphWCET Example
De nition 4.8 Given a state si, let Gsi  G be the directed acyclic graph GsiVsi; Asi
de ned by the tasks in state si and their successors.
Example 10 Consider Figure 12. In this example we are in stage i = 4. The leftmost shaded
area covers Gs4 de ned by s4 = oh1,sink. In this case Vs4 = foh1,mvm1,sinkg. 2
De nition 4.9 Given a state si, let si be called valid if Gsi does not contain any
task which is in the NEV ER set but does not appear in si.
Example 11 Consider Figure 12 again. The two valid states in stage 4 are s4 = oh1,sink
and s4 = cjd,sink. State s4 = oh0,sink, however, is not a valid state because Gs4 contains oh1, which is in the NEV ER set but not in s4. 2
De nition 4.10 Given a valid state si, let GraphWCETsi be the worst case exe-
cution time WCET as determined by an As Soon As Possible ASAP schedule for
Gsi, where any tasks in Gsi which are in the NEV ER set are executed in the order in which they appear in si. If si is not valid, then GraphW CET si is unde ned.

CHAPTER 4. REAL TIME ANALYSIS

56

Example 12 Continuing with Figure 12, consider the leftmost shaded area again. For this
Gs4, we nd that GraphWCETs4 = WCEToh1 + WCETmvm1 = 21; 799. 2
In other words, GraphWCETsi is the overall WCET for stages i; i + 1; : : : ; n, given that the rst task ti in si is executed in stage i, and the rest of the tasks xik in si are executed in stages i + 1; i + 2; : : : ; n according to the order in which the tasks appear in xik.
De nition 4.11 Let fisi, i 2 1; 2; : : : ; n,1, denote a value equal to GraphWCETsi
if both si is valid and the order of tasks in si does not violate any precedence con-
straints; otherwise let fisi = 1. We de ne fnsn to be zero since there is no task
to execute after the last stage, and the last task executed is always the sink so that
it is always the case that sn = sink, whose execution takes zero cycles.
Example 13 A possible state for Figure 12 is s4 = t4; x41 = oh0,sink. However, this
state is not valid, and so for s4 = oh0,sink, f4s4 = 1. The other two possibilities for s4 are shown in Figure 12, and for those two we have f4s4 = GraphWCETs4. 2
Recall that tasks not in the NEV ER set are all hardware-tasks and are scheduled ASAP.
De nition 4.12 Let fisi, i 2 1; 2; : : : ; n , 1, be the minimum nite value of
fisi = fiti; xik over all possible xik for a given ti.
De nition 4.13 Given task ti the current task executing, let xik denote the value
of xik that yields fisi = fiti; xik.
Note that if there is no xik such that fisi = fiti; xik is nite, then we have no fisi nor xik de ned for task sequences beginning with task ti in this stage.
Thus, when computing fisi, we nd the following holds, if there exists at least one state xik for which fiti; xik is nite:

CHAPTER 4. REAL TIME ANALYSIS

57

fisi = mxiikn fiti; xik = fiti; xik; i 2 1; 2; : : : ; n , 1

src

oh0 fk oh1

cg cjd
mvm2

NEVER = {oh0,oh1,cjd}

----s-3----

GraphWCET(s -----

3

)

oh0,oh1,sink 24,020

cjd,oh1,sink

35,012

oh1,cjd,sink

43,812

mvm1 mvm3

mvm4

sink
Figure 13: Constructive Heuristic Scheduling Example Stage 3

De nition 4.14 Given a valid state si = ti; xik, let ts denote a successor of task
ti, where Gts G is the graph de ned by ts and the successors of ts. Then, we de ne GraphW CETsuccti; xik to be the largest GraphW CET ts; xik of any successor ts of task ti.
In calculating GraphWCETsuccti; xik, we schedule the subgraph induced by the successors of task ti using an ASAP schedule. If we nd a successor ts of ti that is in the NEV ER set, then we use GraphWCETts, which, since the state is valid, was already calculated in a previous stage that scheduled the tasks in xik.
Example 14 Consider Figure 13 where we are in stage 3; the leftmost shaded area shows Gs3
for s3 = oh0, oh1,sink. So we have t3 = oh0 and x32 = oh1,sink. One successor of task t3

CHAPTER 4. REAL TIME ANALYSIS

58

is oh1, which is a member of the NEV ER set. Thus we use GraphWCEToh1; x32 = 21; 799 as calculated in the previous stage. The other successor of task t3 is fk, for which we nd that
the subgraph consisting of tasks ffk; mvm1; gsink yields GraphW CET fk; sink = 8; 900
recall that a state si = t; xik consists of a task t and decision variable xik, where the tasks in xik must be in the NEV ER set, or be the src or the sink, but the task t need not be in the NEV ER set. The nal result is GraphWCETsucct3; x32 = 21; 799. 2
De nition 4.15 Giv8en a state si = ti; xik, we de ne the following: :GraphW CETextra s i = GraphW CETsuccti; xik , GraphW CET xik if GraGphrWapChWETCsuEcTctxii;kxik
0 otherwise
and GraphW CET si = W CET ti + GraphW CETextrasi + GraphW CET xik.
Example 15 Consider the leftmost shaded area in Figure 13 again; we have t3 = oh0, x32
= oh1,sink and s3 = t3; x32. We found previously that GraphW CET x32 = 21; 799 and GraphWCETsucct3; x32 = 21; 799. From these values we nd that GraphWCETextras3 = 0 and thus GraphWCETs3 = WCEToh0 + 0 + GraphWCETx32 = 24; 020. 2
This de nition allows us to take into account the case where GraphWCETsuccti; xik, the WCET of the subgraph covered by the successors of task ti, is not determined by GraphWCETxik i.e. the path through the software tasks in xik but instead is determined by a di erent path through the subgraph. At this point we have speci ed all the de nitions needed to calculate GraphWCETs for any state s.
4.2.2 Constructive Heuristic Scheduling Solution
The number of stages n we use is equal to the number of tasks in the NEV ER set which we call SWNEV ER since it is composed entirely of software-tasks plus two for the source and the sink. We use a bottom-up approach and set the last stage to be the sink and the rst stage to be the source we always have a source and a sink according to Assumption 4.1.

CHAPTER 4. REAL TIME ANALYSIS

59

In each stage, we compute the best sequence of tasks given that we start with a
particular task. That is, in stage i, for each possible rst task ti 2 SWNEV ER, we
nd the sequence of tasks starting with ti in stage i and xik in stages i; i + 1; : : : ; n which yields the smallest GraphWCET. Thus, since each distinct sequence of tasks
de nes a unique decision variable for the next stage, at most jSWNEV ERj decision variables are carried over from one stage to the next. Thus, for each ti 2 SWNEV ER, there are at most jSWNEV ERj candidates for xik. This limits the total number of task sequences considered in each stage to a maximum of jSWNEV ERj2, making
the algorithm polynomial instead of exponential. Unfortunately, it also makes the
algorithm a heuristic instead of an exact solution method.
Since the sink is always executed last and takes no time to complete, we assume
that this last stage has already been scheduled when we start. Note that in the following we number the stages 1; 2; : : : ; n and use index i to refer to current stage.
Thus, since the sink is always schedule in stage n, our approach starts with the
second to last stage, stage n , 1, and progressively works its way back to the rst
stage, stage 1.
The pseudo-code for the Constructive Heuristic Scheduling Algorithm is shown in
Figures 14 and 15. The algorithm of Figure 14 calculates the worst-case execution
time for a given stage, whereas the algorithm of Figure 15 actually implements the
constructive heuristic scheduling algorithm and selects the order for the tasks in SWNEV ER, which is a single NEV ER set of software tasks.
The algorithm of Figure 15 actually implements the core of the constructive heuristic scheduling algorithm. For stage n, no calculations are necessary since the sink
always takes zero time to execute.
Scheduling starts with stage n , 1, for which each task in SWNEV ER either
can be scheduled then or cannot be scheduled then. For example, if a software-task
ti has a precedence constraint where another software-task must execute after ti,

CHAPTER 4. REAL TIME ANALYSIS

60

C 1 2

alc

fiWnoriCtiEajlTi=ze1G;fj;iS,WfinN; ,EV1E; jRX+;i,i+;1n=;ff;i+;1;

Xi

f

3 4

ftjor=ejathchtatsakskinorSdWerNxEik V2EXRi; f

5 6

sifi

= tj, order

xik; not

possibleG;

tj

;

xik

f

* if order not possible due to constraints in G *

7 8

g elfsiesfi = 1;

9 calculate GraphW CETextrasi;

10 fisi = W CET tj + GraphW CETextrasi + fi+1xik;

11 g

* note that by de nition, fi+1xik = GraphW CET xik *

12 g

* fisi has now been calculated for all possible xik for this tj *

13 14 15 16 17

if g

fisi nite for

xfiiks=ix=ik

such fitj;

Xi,1 = Xi,1

some si = tj, xik f

* if we did not nd all fisi

that
xfiktj;;

fitj; xik
xikg;

is

minimized;

=

1

in

this

iteration

*

18 g

19
g

return fi; Xi,1;

Figure 14: Calculate WCET Algorithm

then clearly ti cannot be scheduled in stage n , 1 since no software-task can ever be scheduled after stage n , 1 recall that the sink is always scheduled last, i.e. in stage n. Each software-task which can be scheduled to execute in stage n , 1 without
violating any constraints is placed in a one element set and added to Xn,2 for the next stage.
Then, for stage n , 2, we calculate a jSWNEV ERj  jXn,2j table where we
place in each table entry the GraphWCET for each state determined by a software-
task eligible to execute in this stage n , 2 followed by a software-task that can

CHAPTER 4. REAL TIME ANALYSIS

61

Solve orderG; SWNEV ER; WCETisr; WCETprsched f

1 n = jSWNEV ERj + 2;

* number of stages *

2 increase WCET for each task in SWNEV ER by WCETisr + WCETprsched;

3 fnsn = fnsink = 0; 4 Xn,1 = fsinkg; 5 for i = n , 1; i 1; i , , f

* initial values for stage n-1 * * go through the stages in reverse order *

6 fi; Xi,1 = Calc W CET G; SW NEV ER; i;*n;rfeic+o1rd; XWiC; ET and state *

7g

* when this loop ends we have calculated f2 and X1 *

8 9

xf011;=Xt0h*e=X0CrshatalcsaWonndlCyoEnolTnye*Gssreee;ttcfoisnsrindrXccset0ga;t;ht1ee;xwn0;e1f2pwai;stXshe1dm;iinnimfusmrcWg

CET from src to CalcW CET

* *

10 11

GWCET = Gtask order

lfi1st=x0x10;1;

* annotate G with minimal overall WCET found * * record the task order found *

g

Figure 15: Constructive Heuristic Scheduling Algorithm

be executed in stage n , 1 if the two software-tasks selected cannot execute in the
chosen order due to precedence constraints, the table entry records a GraphWCET
of 1. For each task in SWNEV ER, we record a decision variable an ordered
set, see De nition 4.5 indicating the sequence starting with that task which has the minimal GraphWCET. The decision variables are accumulated in Xn,3 for the next
stage n , 3. Next, for stage n , 3, we again calculate a table of size jSWNEV ERj  jXn,3j
where we place in each entry the GraphWCET corresponding to an ordered set of three software-tasks. Each ordered set consists of a task from SWNEV ER followed by two software-tasks from an ordered set in Xn,3. Since Xn,3 can contain at most

CHAPTER 4. REAL TIME ANALYSIS

62

jSWNEV ERj sets, we calculate the GraphWCET for up to jSWNEV ERj2 combi-
nations of three sw-tasks. For each task tn,3 in SWNEV ER, we select the decision
variable xn,3k which minimizes GraphW CET tn,3; xn,3k  and add ordered set tn,3; xn,3k  to multiset Xn,4 for the next stage n , 4.
Continuing in this way for stages n , 4; n , 5; : : : ; 3; 2, we calculate the Graph,
WCET for each state composed of a task eligible to execute in that stage fol-
lowed by a particular order of software-tasks in the previous stage, selecting at most
jSWNEV ERj task orders to pass on to the next stage. Note that as we decrease the stage number by one, we increase the number of tasks in each ordered set xik 2 Xi
by one.
Thus, when we reach stage 1, we consider up to jSWNEV ERj task orderings
of all tasks in SWNEV ER, where the rst task executed is the src. From these
possibilities we choose the best and nd an order of execution for the tasks in the SWNEV ER set yielding the smallest GraphWCET among the orders considered.
Note that the nal list from which the solution is chosen consists of task orderings chosen based on the optimality of suborderings along the way, i.e. by selecting the xik that minimize the overall WCET for the graph the GraphWCET. Since choosing
local minima may accidentally kick out a subordering which later turns out to be nec-
essary for the global minimum, this formulation is a heuristic. However, it performs
in polynomial time.
We next show the application of the algorithm to our example. In order to begin with the last stage i.e. stage n = 5, we schedule the sink, yielding f5sink = 0.
For stage n , 1 = 4, the WCET is determined entirely by the current state
whichever task is chosen to execute. Therefore, our table of calculations need only include s4, f4s4 and X4.

CHAPTER 4. REAL TIME ANALYSIS

63

tX4 4 oh0

f4s4 sink
1

X3

oh1 21,799 oh1,sink

cjd 26,413 cjd,sink

Table 3: Constructive Heuristic Scheduling Example Stage n , 1 = 4

Example 16 Consider Figure 10. We have n = 5 stages. For stage 5 we found that
f5sink = 0. Table 3 shows the calculations for stage 4. From this we achieve one optimiza-
tion for the next stage already: oh0 cannot be scheduled in this stage due to control data- ow
precedence constraints. Thus, the multiset X3 calculated for the next iteration only has two
members.
Figure 12 showed the two sets of tasks scheduled and their WCET paths in this pass of the algorithm. 2
For stages n , 2 through 2, we use the fi+1 and Xi values calculated in the
previous iteration. Note that for each possible ordered set of tasks, in the worst case
njV j+jAj operations have to be performed in calculating GraphWCETsi, where
V denotes the vertices and A denotes the directed edges in the DAG of the task ow.

tX3 3 oh0

f3s3

oh1,sink cjd,sink

24,020

1

x3k
oh1,sink

f3t3; x3k
24,020

X2 oh0,oh1,sink

oh1 1 43,812 cjd,sink 43,812 oh1,cjd,sink

cjd 35,012

1 oh1,sink 35,012 cjd,oh1,sink

Table 4: Constructive Heuristic Scheduling Example Stage 3

Example 17 Continuing our attempt to schedule Figure 10, we pass now to stage 3. Table 4
shows the calculations for this stage. The rst nite-valued entry contains the GraphWCET if

CHAPTER 4. REAL TIME ANALYSIS

64

oh0 is scheduled in stage 3 and oh1 in stage 4 with the sink in stage 5. Note that it is not possible to schedule oh0 in stage 3 and cjd in stage 4 due to control data- ow constraints. Note also that there is no column for x3k = oh0; sink since it was not possible to schedule oh0 in stage 4.
To calculate the GraphWCET values for s3, given that we execute task t3 in this stage 3 and the rst task in x3k in the next stage 4, requires scheduling the subgraph covered by task t3, the tasks in x3k, and all of their successors. We use an ASAP schedule.
Figure 13 showed the states scheduled in this stage and and their WCET paths in this pass
of the algorithm. 2

tX2 2 1 f2 s2 x2  f2 t2; x2  X1oh0 1 1 1oh1 1 1cjd

oh0,oh1,sink 37,233

 oh1,cjd,sink
46,033

cjd,oh1,sink 37,233

k
cjd,oh1,sink
oh0,oh1,sink

 k 
37,233

oh0,cjd,oh1,sink

37,233

cjd,oh0,oh1,sink

Table 5: Constructive Heuristic Scheduling Example Stage 2

Example 18 Next consider stage 2 of the attempt to schedule Figure 10 using the construc-
tive heuristic scheduling algorithm. Table 5 shows the calculations for this stage. For the states
beginning with task oh0, the minimum value of f2 is selected by x2k yielding one value for f2. Note that x2k is a set that takes on two di erent values, namely cjd,oh1,sink and
oh0,oh1,sink, in the course of the calculation. On the other hand, X1 is a multiset that
contains all of the sets in its column, so X1 = foh0; cjd; oh1; sink; cjd; oh0; oh1; sinkg. 2
Note that the states eliminated in calculating fisi leave us carrying at most
jSWNEV ERj ordered sets of tasks to the next stage calculation. This means at most jSWNEV ERj2 di erent possible task orderings are considered in each stage,
just as we noted earlier. Unfortunately one of the states eliminated in calculating fisi, while suboptimal locally, may turn out to be the global optimum. The fact

CHAPTER 4. REAL TIME ANALYSIS

65

that this algorithm is a heuristic can be veri ed by applying it to the example of Figure 16.

X1 f1 s1  t1 oh0,cjd,oh1,sink cjd,oh0,oh1,sink

x11

f1t1; x11

X0

src

39,012

41,233 oh0,cjd,oh1,sink

39,012

src,oh0,cjd,oh1,sink

Table 6: Constructive Heuristic Scheduling Example Stage 1

Example 19 Now for the last set of computations, stage 1. There is only one starting state,

the source, so the table has only one row. Table 6 shows the calculations for this stage. The

minimum WCET for the graph is found in choosing x11. Note that the algorithm nally

takes into account the WCET for task cg, making the option of selecting cjd to execute

before oh1 less favorable.

We

end

up

with

X0

=

fx01g,

and

so

the

order

found

is

x 01

=

src,oh0,cjd,oh1,sink with a WCET of 39,012. Thus we give oh0 the highest priority, cjd

the

second-highest,

and oh1

the

lowest

priority.

Note

that

we

use

X0

and

x 01

only

to

record

the nal order found there is no stage 0. 2

Thus we have an order given our assumptions of execution of tasks in the NEV ER set which minimizes WCET from among the task orders considered. We use this order to statically set the priorities for the software-tasks.

4.2.3 Multiple NEV ER Sets of Hardware-Tasks
Up till now we have formulated our scheduling problem under the assumption that we have unlimited hardware and a single processor. Now suppose we do have limited hardware resulting in hardware-tasks implemented on the same hardware resource. We represent each such resource constraint with a NEV ER set of mutually exclusive hardware-tasks which cannot overlap execution.

CHAPTER 4. REAL TIME ANALYSIS

66

src

b e

a cd
f

NEVER = {b,c,d}

task ----
a b c d e f

wcet (cycles) ----- 5,000 3,000 20,000 15,000 5,000 11,000

sink
Figure 16: Sample DAG With Optimal Schedule Not Found By Heuristic: The constructive heuristic scheduling algorithm nds order d,b,c which yields a WCET of 43,000; however, the optimal order is b,d,c, which yields a WCET of 40,000.
We can include multiple NEV ER sets of hardware-tasks by extending the constructive heuristic scheduling algorithm in a straightforward fashion. We simply set the number of stages n equal to the total number of tasks in all NEV ER sets, plus two for the src and sink. Let the number of distinct NEV ER sets be d, where the rst NEV ER set contains all software-tasks in the application, while subsequent NEV ER sets contain hardware-tasks which utilize the same hardware resource to accomplish their computation.
Example 20 We consider a modi ed version of Figure 10 where the four tasks mvm1-4 are
all executed on the same hardware module mvm. Figure 17 shows the six of the seven tasks in
the two NEV ER sets as they are scheduled in stage 4 of the constructive heuristic scheduling algorithm. We have n = 9 stages, so f9s, f8s, f7s, f6s and f5s have already been calculated. The shading in Figure 17 identi es the tasks in the same NEV ER set scheduled
at this step of the algorithm; the thick arrows indicate the relative ordering among all of the tasks. The table for this stage is not shown here but would look similar to Table 5. except that
it would have seven by seven entries, one row column per task in a NEV ER set.

CHAPTER 4. REAL TIME ANALYSIS

67

src

oh0 fk oh1

cg cjd
mvm2

mvm1 mvm3

src

oh0 fk oh1

cg cjd
mvm2

mvm1 mvm3

src

oh0 fk oh1

cg cjd
mvm2

mvm1 mvm3

NEVER1 = {oh0,oh1,cjd} NEVER2 = {mvm1, mvm2,
mvm3, mvm4}

s ---

f-4*-(-s-) -

oh0 24,020

oh1 43,812

cjd 35,012

mvm1 -

mvm2 -

mvm3 -

mvm4 -

mvm4

mvm4

mvm4

sink sink sink
Figure 17: Multiple NEV ER Set Example

Note that at this stage we have already scheduled 5 tasks and are considering which task to schedule just before those 5. Due to precedence constraints in the DAG, none of mvm1-4 can be scheduled at this stage, and therefore the entries are empty. For example there is no way
to schedule mvm1 in this stage and thus have 5 tasks scheduled after mvm1 completes. 2

The constructive heuristic scheduling algorithm has already been shown in Fig-

ures 14 and 15. The only di erence in calling algorithm Solve order of Figure 15 is

that instead of passing in SWNEV ER, we call it with a multiset NEV ERSETS

which contains the rst NEV ER set equal to SWNEV ER, while the rest of the

NEV ER sets all contain only hardware-tasks.

Figure 18 shows the modi cations necessary to convert Figure 15 to handle mul-

tiple NEV ER sets of hardware-tasks. Note that the only changes are in lines 1,

2, and 7 of Figure 18. The

nal

task

order

found

in

x 01

contains

all

tasks

in

any

NEV ERi 2 NEV ERSETS. Thus, the task order for tasks in the same NEV ER

set

can

be

extracted

from

x 01

by

simply

removing

the

relevant

tasks

from

x 01

in

the

order they are found.

CHAPTER 4. REAL TIME ANALYSIS

68

Solve orderG; NEV ERSETS; WCETisr; WCETprsched f

1 2

SnW=NPENVEEV

R=
ERi2N

rst set in
EV ERSETS

NEV ERSE
jNEV ERij

TS; + 2;

* number of stages *

3 increase WCET for each task in SWNEV ER by WCETisr + WCETprsched;

4 fnsn = fnsink = 0; 5 Xn,1 = fsinkg; 6 for i = n , 1; i 1; i , ,f

* initial values for stage n-1 * * go through the stages in reverse order *

7

fi;

Xi,1

=

Calc

W

CET

G;

N

EV

ERSET

S;

i;

n;

f
i+1

;

Xi;

* record WCET and state *

8g

* when this loop ends we have calculated f2 and X1 *

9 f1; X0 = Calc W CET*Gre;cfosrdrcstga;t1e;xn0;1f2wi;tXh1m;inimum W CET from src *

10

x 01

=

the rst and only * X0 has only one

set set

in X0; since the

we

passed

in

fsrcg

to

CalcW CET

*

11 GWCET = f1x01; * annotate G with minimal overall W CET found *

12 Gtask order list = x01;

* record the task order found *

g

Figure 18: Constructive Heuristic Scheduling Algorithm with Multiple NEV ER Sets

Example

21

Consider a Figure 17 again.

Let

x21,

x 22

and

x 21

denote the decision variables

for the left, middle and right-hand graphs shown in Figure 17, respectively. For the leftmost

DAG,

we

have

x 21

=

fcjd,

mvm2,

mvm3,

oh1,

mvm4,

mvm1,

gsink

from

which

we

would

extract order f gcjd,oh1 for NEV ER1 and order fmvm2, mvm3, mvm4, gmvm1 for NEV ER2.

For

the

DAG

in

the

middle,

we

have

x 22

=

foh1,

cjd,

mvm1,

mvm2,

mvm3,

mvm4,

gsink

from which we would extract order f goh1,cjd for NEV ER1 and order fmvm1, mvm2, mvm3,

gmvm4 for NEV ER2.

Finally,

for

the

rightmost

DAG,

we

have

x 23

=

foh0,

oh1,

mvm1,

mvm2, mvm3, mvm4, gsink from which we would extract order f goh0,oh1 for NEV ER1 and

order fmvm1, mvm2, mvm3, gmvm4 for N EV ER2.

It turn out that the

nal

optimal

solution

is

found

from

leftmost

DAG

and

yields

x 01

=

CHAPTER 4. REAL TIME ANALYSIS

69

fsrc, oh0, cjd, mvm2, mvm3, oh1, mvm4, mvm1, gsink from which we would extract order foh0, gcjd,oh1 for NEV ER1 and order fmvm2, mvm3, mvm4, gmvm1 for NEV ER2. 2
The changes needed to alter Figure 14 to handle an input of multiset NEV ERSETS instead of the single set SWNEV ER are so few that we will simply describe them here in words. The rst change is to de ne the jth task of NEV ERSETS to be the
jth task encountered when processing each NEV ERi 2 NEV ERSETS one by one
in the same order each time i.e. in the order they are stored in NEV ERSETS. The second change from the single NEV ER set algorithm shown in Figure 14 occurs in scheduling the DAG at each step in the algorithm. Instead of a single ASAP schedule for the entire graph, we have to perform an ASAP scheduling of the graph
for each distinct never set. Thus, in the worst case, jV j + jAj  d operations have to
be performed in calculating GraphWCETextrasi of De nition 4.15, where d denotes the number of NEV ER sets contained in the multiset NEV ERSETS.

4.2.4 Complexity Analysis

First note that in order to calculate fisi = fiti; xik, we have to ASAP schedule

the DAG GsiVsi; Asi, where Vsi denotes the vertices and Asi denotes the directed

edges arrows in the DAG of the task ow of si. Note that GsiVsi; Asi has already

scheduled all resource-constrained tasks other than ti in the previous stage. For each

task in NEV ERi 2 NEV ERSETS, an upper bound on the number of constant

operations that have to be performed for the ASAP schedule is jVsij+jAsij. Since

in
P

each

stage

ti

ranges

NEV ERi2NEV ERSET S

over
jN E

the tasks in
V ERij, we

some NEV nd that ti

ER can

set, and take on

recalling that
any of n , 2

n,2 =
values.

Now, since for each possible value of ti we select at most one value of xik, Xi,1 has at

most n , 2 members in each iteration. Thus, since in each iteration we calculate fi

CHAPTER 4. REAL TIME ANALYSIS

70

for every possible state ti,xik, in the worst case n,22 calculations of fi are needed each iteration. Together with our earlier upper bound of jVsij + jAsij for calculating fi, we end up with an asymptotic upper bound of On , 22  jVsij + jAsij = On2  jV j + jAj calculations for one stage i.e. for Calc WCET of Figure 14.
For Solve order shown in Figure 15 or Figure 18, none of the lines take time
greater than On2  jV j + jAj. Thus, since we call Calc WCET at most n times,
our constructive heuristic scheduling algorithm with multiple NEV ER sets takes
time On3  jV j + jAj. Assuming we can bound V and A by constants, we have a
polynomial-time algorithm.
4.2.5 Practical Considerations for the Calculation of WCET
In order to make a correct calculation of the WCET, we have to consider the time spent executing the ISR and the priority .scheduler To be more speci c, we will consider the case where the processor is a MIPS R4000. To calculate the WCET of the entire graph, we use the following costs, obtained by analyzing our run-time scheduler software code executed on a MIPS R4000 model with no cache analysis, i.e. assuming we always miss in the instruction cache: interrupt overhead = 38 cycles and priority scheduler task selection = 98 cycles.
For the interrupt, we use pin Int0 on the MIPS R4000 model and do not save
the register set before passing control to the priority scheduler software. The priority scheduler template uses several registers reserved for the kernel; it also uses two general purpose registers, which it saves before using and restores just before exiting. Otherwise, with a general context switch, our interrupt overhead would be much larger. Also, since each task runs to completion Assumption 4.3, no context switches are needed between tasks in the following sections, we will show how to relax this assumption and still account for the worst case.
We use these costs to calculate the WCET of the entire graph. Note that in the

CHAPTER 4. REAL TIME ANALYSIS

71

actual implementation of the constructive heuristic scheduling algorithm, the WCET for the ISR and the WCET for the priority scheduler are added to the WCET for each software-task when calculating the task priorities.
We use the priority scheduler with the priorities found via constructive heuristic scheduling. Note that we assume that precedence constraints needed to implement the chosen task order is enforced by the run-time scheduler. In other words, no interrupts updating the start vector of start events for the software-tasks for a particular software-task until all higher-priority software-tasks are nished executing.
Example 22 Consider Figure 10. We use the priorities found in Example 19. We nd that
the run-time scheduler causes three interrupts. Since the hardware part of the run-time scheduler enforces the precedence constraint of cjd before oh1, 1,643 clock cycles go unused between the completion of oh0 and the start of cjd. After the third interrupt, oh1 executes concurrently with mvm2, mvm3 and mvm4. After oh1 nishes, then mvm1 executes.
A straightforward ASAP schedule is used. Several of the software- and hardware-tasks have loops, for each of which the user provided upper bounds the analysis of cinderella-M supports user speci cation of loop bounds MWWL96, LM95 . Notice how the critical path runs through both hardware and software in di erent execution paths. Table 7 shows the calculation.
The overall WCET is 39,284 cycles. 2
Recall that we assume that the hardware part of our run-time scheduler is the only source of interrupts for the CPU Assumption 4.5. Now we know that we can generate the FSM such that hardware part of the run-time scheduler only interrupts the software to indicate that the next highest priority task is ready to execute once the previous task in priority level has completed. Thus, we can guarantee that each software task runs to completion Assumption 4.3. With these two assumptions, we nd that no context switches ever occur in our software no calls to save context or restore context. Furthermore, only one call to interrupt service routineISR

CHAPTER 4. REAL TIME ANALYSIS

72

sw-task

 cycles hw-task  cycles

int-ser-routine

38 cg 4,000

priority-sch-sw

98

oh0 2,221

int-ser-routine

38 fk 4,500

priority-sch-sw

98

cjd 13,213

int-ser-routine

38 mvm2 4,400

priority-sch-sw

98

oh1 4,264

oh1 4,400 mvm3 4,400

oh1 4,400 mvm4 4,400

oh1 4,335 mvm1 4,400

Table 7: WCET Calculation Example

and one call to priority scheduler are needed per software task. Thus, we nd that the nal output of our WCET calculation is an upper bound on the WCET of the graph, given the priorities assigned to software-tasks in the same NEV ER set.
So we now can analyze satis ability of a rate constraint in a dynamically changing, concurrent execution of hardware-tasks and software-tasks, given our run-time scheduler implementation.
4.3 Context Switch Cost and Out-of-order Execution
In the previous section, we found a solution that minimizes WCET when softwaretasks are assigned priorities and not executed until all higher priority software-tasks have completed. However, in some cases there may be unused CPU cycles between two software-tasks with consecutive priorities, e.g. if a hardware-task needs to nish to satisfy precedence constraints captured in the DAG. Thus, we may want to relax

CHAPTER 4. REAL TIME ANALYSIS

73

Assumption 4.3 and allow lower priority software-tasks to execute during otherwise unused CPU cycles, even when some higher priority tasks have not yet executed. We call this situation out-of-order execution because we abandon the exact sequencing of software-tasks according to their priority as was done in the previous section.
However, now our WCET calculation must account for software-tasks which are partially executed and then interrupted. In our analysis the WCET of a context switch is for either saving the register set save context or for restoring a previous register set restore .context Since context switching is a major cost to consider when trying to optimize for real time1, we feel that the savings is worth the e ort spent separating the two kinds of contexts switches.
Note that when a particular software-task completes its execution, there are no registers to save when transferring the processor to another software-task. Similarly, when a particular invocation of a software-task rst begins execution, there is no register state to load. Eliminating context switches in these cases does not mean that there cannot be other processes switched out; it just means that saving or restoring the register set may not be necessary at that particular instant.
Interrupts are disabled during context switches. The priority scheduler is restarted if an interrupt is received during its execution. Note that due to the construction of the hardware part of the run-time scheduler, at most one interrupt will occur per software-task.
4.3.1 Upper bound on extra calls to the Priority Scheduler and Context Switch
Suppose we have m software-tasks whose order, assuming each runs to completion, has been found by the constructive heuristic scheduling algorithm described in Section 4.2.
1For example, the major result of HWS95 was a 66 reduction in context switch cost.

CHAPTER 4. REAL TIME ANALYSIS

74

Then, suppose we allow l of the software-tasks to execute out-of-order; that is, for any of the l software-tasks, if it is ready to start before software-tasks higher in priority are ready, we allow it to execute until one of the higher priority tasks is ready to execute. Clearly, l m since the highest priority task cannot execute out-oforder." Since at most one interrupt will occur per software-task for each execution of the application as captured in the DAG, the ISR overhead is xed based on the number of software-tasks. With interleaved execution of software-tasks, however, the number of calls to the scheduler is not xed. What is the overhead, in terms of extra executions of the priority scheduler and context switch code, incurred by allowing these l tasks to execute early out-of-order?
In order to begin our analysis, we de ne the following:
De nition 4.16 Let  assign a priority to each software-task that minimizes WCET
if each task runs to completion: if sa sb then the sa has a higher priority than sb.
Presumably we found  using the constructive heuristic scheduling algorithm of the previous section.
De nition 4.17 A software-task executes early when the run-time scheduler sets its
start event before all higher priority tasks have completed execution.
Clearly, a software-task that executes early can possibly execute out-of-order.
De nition 4.18 Let I = fi1; i2; : : : ; ilg = the set of l software-tasks allowed to exe-
cute early and possibly execute out-of-order.
Each task i 2 I can have part or all of it computation performed before the software-
task immediately preceding it in priority has even begun to execute at all.

CHAPTER 4. REAL TIME ANALYSIS

75

src

Ohold1 Law

Jhold Law cg

oh0

cjd

fk oh1 mvm2

mvm1 mvm3

mvm4

NEVER = {oh0,oh1,cjd}

Task ---- cg oh0 oh1 fk cjd mvm1 mvm2 mvm3 mvm4 src sink

BCET (cycles) ----- 4,000 1,598 12,341 4,500 9,989 4,400 4,400 4,400 4,400 0 0

WCET (cycles) ----- 4,000 2,221 17,399 4,500 13,213 4,400 4,400 4,400 4,400 0 0

TT(oh0) > TT(cjd) > TT(oh1)

sink
Figure 19: DAG, WCET and  Example
De nition 4.19 Suppose we have two tasks i and j with j i but under some
conditions it is possible that the run-time scheduler will assert the start event for i
before the start event for j. Then we say that software-task i can jump software-task
j.
Clearly, for it to be possible for i to jump j, then there cannot be any precedence constraint between i and j.
De nition 4.20 Given a set I of software-tasks that can execute early, let J = fj1; j2; : : : ; jqg = the set of q software-tasks that can be jumped by some i 2 I.
Example 23 Consider the DAG shown in Figure 19 where the NEV ER set speci es software-
tasks which must execute on the same CPU. The order of tasks in the NEV ER set which minimizes W CET for the graph is oh0,cjd,oh1 thus oh0 cjd oh1
and is shown by the two emboldened edges in Figure 19. Thus, the static priority scheduler in software has the highest priority assigned to oh0, the next highest priority to cjd and the lowest

CHAPTER 4. REAL TIME ANALYSIS

76

priority to oh1. Notice that after oh0 nishes, there are 8,779 cycles of delay before cjd can start, due to cg. If the run-time scheduler were to set the start event for oh1 right after oh0
nishes, then oh1 would execute early and cjd would be jumped. In this case we would have I = fi1g = foh1g and J = fj1g = fcjdg. 2
In general, a task can be in both I and J. Note that in Figure 20, Figure 21,

src

oh0 oh1

cg cjd

src i1 p j1 sc i1 rc snk

fk oh1 mvm2

mvm1 mvm3

mvm4

sink
Figure 20: DAG With Out-of-order Execution Example
Example 24 and the subsequent proofs, the abbreviation p stands for a call to the priority scheduler code, sc stands for a call to the save context code and rc
stands for a call to the restore context code.
Example 24 Figure 20 shows a graphical representation of the execution of the DAG of
Figure 19 where oh1 executes early i.e. out-of-order with respect to its assigned priority the thick arrows indicate the out-of-order execution ow. The two small columns show which extra
calls to the priority scheduler code p, save context code sc and restore context code rc occur. An extra call to p rst occurs to schedule i1 = oh1 right after oh0 nishes.

CHAPTER 4. REAL TIME ANALYSIS

77

There is no need to call any context switch code since one software-task is completely nished,
namely oh0, and the other software-task, oh1, starts up from the beginning of its code. Next,
j1 = cjd becomes ready, necessitating a call to sc to store the register state for oh1. Finally, cjd nishes and a call to rc is needed to continue execution of oh1 from its state when it was interrupted. Thus, after the source, i1 causes an extra call to p, j1 causes an extra call to sc, i1 causes an extra call to rc and nally the sink is reached. Thus, the columns show the extra overhead incurred in extra calls to p, sc and rc that would not have been incurred were the
tasks executed strictly in order of their assigned priorities. 2

jumped nodes (in set J )

src i1 p j1 sc i1 p,rc j2 sc i1 p,rc j3 sc i1 p,rc
j4 sc i1 p,rc j5 sc i1 p,rc j6 sc i1 p,rc j7 sc i1 p,rc j8 sc i1 p,rc j9 sc i1 p,rc j10 sc
i1 rc snk

src i3 p j1 sc i2 p j2 sc i1 p j3 sc i1 p,rc
j4 sc i1 p,rc j5 sc i1 p,rc j6 sc i1 p,rc j7 sc i1 p,rc j8 sc
i1 rc
i2 rc
i3 rc snk

src i3 p j1 sc i2 p j2 sc i1 p j3 sc i1 p,rc i2 p,rc j4 sc i2 p,rc j5 sc i2 p,rc j6 sc i2 p,rc j7 sc i2 p,rc j8 sc
i2 rc
i3 rc snk

src i1 p j1 sc i1 p,rc j2 sc i1 p,rc j3 sc
i1 rc i2 p j5 sc i2 p,rc j6 sc
i2 rc i3 p j8 sc i3 p,rc j9 sc i3 p,rc j10 sc
i3 rc snk

src i3 p i2 p,sc i1 p,sc j1 sc i1 p,rc j2 sc i1 p,rc j3 sc i1 p,rc
j4 sc i1 p,rc j5 sc i1 p,rc j6 sc i1 p,rc j7 sc i1 p,rc j8 sc
i1 rc
i2 rc
i3 rc snk

(A) (B) (C) (D) (E)
Figure 21: Extra Priority Scheduler and Context Switch Time Examples

Example 25 Let's consider the three examples of Figure 21. In A, I = fi1g and jIj = 1;
in B, C, D and E, I = fi1; i2; i3g and jIj = 3. Notice that in all ve examples the number of software-tasks that get jumped" is jJj = 10. Both B and C have some tasks in both I and J; for example, in B i1 and i2 can be jumped by i3, and so both i1 2 J and i2 2 J.

CHAPTER 4. REAL TIME ANALYSIS

78

In A, i1 is allowed to execute after the source. So, in every space between two softwaretasks, i1 tries to execute, causing an extra call to p and to rc before actually running any instructions of i1 itself. Then, when a task in J is ready to execute, a call to sc has to be made since i1 is not nished yet. Notice that no rc calls are needed for any of the tasks in J since each j 2 J runs to completion. 2

Next we propose two theorems about the number of additional calls to p, rc and sc if we allow software-tasks in a set I to execute while no higher priority tasks are
ready even though some higher priority task has yet to start execution. For the
sake of simplicity, note that in the following, given two sets A and B, we use A , B
to denote the elements of A not in B.

Theorem 4.2 Consider o hardware-tasks and m software-tasks fs1; s2; : : : ; smg with

priority  which execute on a single processor.

Let I = fi1g be a single software-task allowed to execute early. Furthermore, let the software-tasks that i1 can possibly jump be J = fj1; j2; : : : ; jqg, where j1

j2 : : : jq and q m.

Claim:

The number of additional calls to the priority pscheduler , save sccontext 

and restore rccontext  code due to allowing the software-task i1 to execute early

has an upper bound of

jJj  p + sc + rc:

4.1

Proof: In the worst case i1 executes before j1, causing an extra call to p,
but does not nish execution. Next j1 becomes ready to execute, causing
a call to p and sc. Since the call to p would have happened anyway, only the sc call is additional. After j1 nishes, in the worst case there is exactly enough time for only a single extra call to the p and to rc for i1 before
j2 is ready to execute. So, both of these calls occur. Next j2 is scheduled
to execute, but needs an extra call to sc to store i1's register set since i1

CHAPTER 4. REAL TIME ANALYSIS

79

did not nish. In the worst case, the calls continue in this way until jq, after which i1 immediately executes, since it is the next priority task. At
this step, only an extra rc call is needed for i1. The total number of extra calls is one p for i1 just before j1, one sc just before executing j1, then p + rc + sc for j2 through jq, and nally one rc for the nal execution of i1: p + sc + q , 1  p + rc + sc + rc This is exactly equal to q  p + rc + sc = jJj  p + rc + sc. QED.

Example 26 An example of the worst case scenario is shown in A of Figure 21, which
shows the case for q = 10. The total number of extra calls is 10  p + rc + sc. 2

Theorem 4.3 Consider o hardware-tasks and m software-tasks fs1; s2; : : : ; smg with
priority  which execute on a single processor.
Let I = fi1; i2; : : : ; ilg, where i1 i2 : : : il, be software-tasks, l n, such that all of them are allowed to execute early. Furthermore, let the di erent software-tasks that some i 2 I can possibly jump be J = fj1; j2; : : : ; jqg,
where j1 j2 : : : jq:
Claim:
The number of additional calls to the priority pscheduler , save contextsc and restore contextrc code due to allowing the software-tasks of I to execute
early has an upper bound of

jJ , J I Ij , 1  p + sc + rc:

4.2

Proof: We give a proof by induction. Base step: I1 = fi1g and J1 = fj1; j2; : : : ; jq1g, where J1 is the set of tasks
that i1 can possibly jump.

Clearly, I1  I and J1  J. By Theorem 4.2, an upper bound on the number of calls to p, sc and rc is jJ1j  p + sc + rc. By de nition of J1 and I1, J1 I1 = ;, and so the upper bound

CHAPTER 4. REAL TIME ANALYSIS

80

is
jJ1 , J1 I1j  p + sc + rc which, since jIj = 1, is equal to jJ1 , J1 I1 I1j , 1  p + sc + rc.
QED for base step.
Step k: Ik = fi1; i2; : : : ; ikg and Jk = fj1; j2; : : : ; jqkg, where Jk are the tasks that some i 2 Ik can possibly jump.
Assume true that the following upper bound holds:
jJk , Jk Ik Ikj , 1  p + sc + rc. Note that by de nition of J, Jk  J.
Step k + 1: Ik+1 = fi1; i2; : : : ; gik+1 and Jk+1 = fj1; j2; : : : ; jqk+1g, where Jk+1 are the tasks that some i 2 Ik+1 can possibly jump. From the given,
we know that ik ik+1. We have several cases.
Case i: i1; : : : ; ik ll all available spaces between tasks in J, so that ik+1 is unable to execute out-of-order. By hypothesis Step k, the upper bound on the number of additional calls to
p, rc and sc due to Jk and Ik = fi1; : : : ; ikg is jJk , Jk Ik Ikj , 1  p + rc + sc 1. If Jk+1 = Jk, i.e. there
are no additional jumpable tasks included in Jk+1 due to ik+1,
then jJk+1 , Jk+1 Ik+1 Ik+1j , 1 increases by 1 while no
additional calls are incurred since ik+1 just executes right away, after all the previous tasks in Jk+1 and Ik+1 have completed. So
the upper bound of jJk+1 , Jk+1 Ik+1 jIk+1 , 1  p + sc + rc holds.
So let's assume that there are additional jumpable tasks. Let these additional jumpable tasks included due to ik+1 and not
already in Jk Ik be fjr; jr+1; : : : ; jqk+1g we don't consider the
jumpable tasks that are also in I because for this case we assume ik has nished execution.
Just considering tasks fjr; jr+1; : : : ; jqk+1g and ik+1, we have an
instance of Theorem 4.2. So the upper bound is
jfjr; jr+1; : : : ; jqk+1gjp+sc+rc. If we add this to the previous

CHAPTER 4. REAL TIME ANALYSIS
upper bound 1 for fj1; j2; : : : ; jr,1g and Ik, we have jJk ,Jk Ik Ikj,1p+rc +sc+jfjr; jr+1; : : : ; jqk+1gj p + sc + rc 2. Now, since fjr; jr+1; : : : ; jqk+1g are not in Jk Ik, and since ik+1
cannot be in Jk+1, we nd that
Jk , Jk Ik Ik fjr; jr+1; : : : ; jqk+1g = Jk+1 , Jk+1 Ik+1 Ik Thus, from 1, we nd an upper bound of jJk+1 , Jk+1 Ik+1 Ikj , 1  p + sc + rc
which is clearly less than
jJk+1 , Jk+1 Ik+1 jIk+1 , 1  p + sc + rc
QED for case i. Case ii: ik+1 was able to execute early, e.g. right after the source but before j1, because none of i1; : : : ; ik were ready to execute or still had execution time left at that time see Fig-
ure 21, E, for an example. This causes an extra p. However,
in the worst case, just as ik+1 is about to be dispatched, an interrupt arrives saying that ik is ready to execute we do not
consider the time due to interrupts here. So, an extra p and sc are incurred. Similarly, ik,1 becomes ready, incurring yet another p and sc. This continues for all i 2 Ik+1 in increasing
level of priority until we reach i1. Thus, so far extra calls have
occurred in the amount of p + jIk+1j , 1  p + sc 3.
Note that we do not stipulate that all of these interrupts occur before j1, but only state that in the worst case they arrive in this reverse order and each have enough of a delay before the
next interrupt so that additional p + jIk+1j,1p+sc calls
are still made.
Consider each j 2 Jk+1 , Jk+1 Ik+1. From here on out, in the worst case each j 2 Jk+1 , Jk+1 Ik+1 will incur an extra call to sc because j interrupts an executing process. Thus, jJk+1 , Jk+1 Ik+1j  sc extra calls will occur 4.
Since all tasks in Ik+1 have become ready to execute, the only

81

CHAPTER 4. REAL TIME ANALYSIS
way for a task in Ik+1 to begin execution is if all higher priority
i 2 Ik+1 have already nished. In the worst case, i1 will not
nish executing until after the last jqk+1 for the situation where i1 nishes before jqk+1, see the next case, so that all of the
previous calls to p for i1 will have been extra, and only this call to p now that jqk+1 is done will not be an extra call but the call to rc will be additional see Figure 21, E, for an example with jIj = 3. Therefore, jJk+1,Jk+1 Ik+1j,1p+rc+rc extra calls will be made for i1 5. Similarly, for the rest of Ik+1, jIk+1j , 1 extra calls to rc will occur the remaining jIk+1j , 1 calls to p were necessary in the
normal course of events and so are not counted as extra. Thus,
a total of jIk+1j , 1  rc extra calls will be needed in the worst case 6. Combining 4, 5, 3, and 6 in this order, we nd that
in the worst case the number of extra calls needed is
jJk+1 , Jk+1 Ik+1j  sc + jJk+1 , Jk+1 Ik+1j , 1  p + rc + rc + p + jIk+1j , 1  p + sc + jIk+1j  rc = jJk+1 , Jk+1 Ik+1j , 1  p + sc + rc + sc + rc + p + jIk+1j , 1  p + sc + rc = jJk+1 , Jk+1 I+1k jIk+1 , 1  p + sc + rc.
QED for case ii. Case iii: Suppose in the previous case i1 does in fact nish
execution before jqk+1 e.g. consider the case in Figure 21, C,
where i2 executes after j3 and i1. In this case an additional call
to p and and to rc are needed for i2 in the worst case because
i2 had executed before and needs its context back. However, later on, i1 will not need to be executed, because it has already
nished. This saves calls to p and rc later: thus, the total amount of calls to p, rc and sc remain unchanged. This can be extended: if any ip; p  k + 1, nishes early, then ip will not
need to be executed later, leaving the total amount of calls to
p, rc and sc unchanged. This is true even if multiple ip's nish

82

CHAPTER 4. REAL TIME ANALYSIS

83

in the same space i.e. between the same two tasks of set J. Thus the upper bound found in the previous case still holds:
jJk+1 , Jk+1 Ik+1 jIk+1 , 1  p + sc + rc.
QED for case iii.
Now, since I was chosen arbitrarily, and since J is uniquely determined by I and , the above induction holds for any I,  and corresponding J. QED.
The main point of this section has been accomplished: to analyze the worst-case overhead incurred in allowing software-tasks to execute out-of-order. Our major result is Equation 4.2, which gives us a formula which quanti es the number of extra calls to the priority ,scheduler save context and restore context code, where we assume that each software-task necessitates one call to the interrupt service routine and priority .scheduler
4.3.2 Instruction Cache Analysis
We want to quantify all of the overhead associated with allowing software-tasks to execute out-of-order. The previous section dealt with the overhead in terms of extra calls to the priority scheduler and context switch code. What about the instruction cache?
To calculate the WCET of a software-task, we use cinderella-M. However, cinderella-M's instruction cache analysis assumes that no interrupts occur LM95 . In our case, the presence of interrupts means that a software-task's instructions can possibly be kicked out of the instruction cache if it is suspended to allow execution of a newly ready, higher priority software-task. Thus, we use the following heuristic to augment cinderella-M's analysis.
cinderella-M calculates the binary code size of each software-task. From this, we calculate the maximum number of instruction cache lines neeeded and the cost

CHAPTER 4. REAL TIME ANALYSIS

84

of reloading the entire intruction cache with the task's instructions. Note that cinderella-M's analysis MWWL96, LM95 already includes the worst-case e ects for the situation where the binary code size is greater than the instruction cache size, so the maximum number of instruction cache lines we have to consider is bounded by the size of the instruction cache.
Ideally cinderella-M would return a worst case instruction cache penalty due to the instruction cache being emptied of a task's instructions. However, we sim-
ply read the binary code size using the !View Function Statistics command of
cinderella-M. Then we use the following formula, where binarycodesize and icachelinesize are in bytes:
WCET reload icache = dbinicaarcyhceoldienseiszieze, 1e + 1  time to load a single icache line
4.3
Note the the binarycodesize , 1 and +1 in the formula are necessary to account for
the case where the rst instruction byte maps to the last byte of an instruction cache line. The only exception to this formula is when it gives a result greater than the time to load the entire instruction cache, in which case we take WCET reload icache to be the lower value, i.e. the time to load the entire instruction cache.
Thus, for each possible interruption by a higher priority task that a task can experience, we have to add the cost of reloading all the instruction cache lines for that task to the overall WCET for the entire graph. In the worst case, this additional
cost will be incurred for every possible call to rc. Thus, for each possible call to rc, we add the worst case instruction cache re ll time, as well as the WCET for the
restore context code.
Example 27 Software-task oh1, when compiled, has a binary code size of 3584 bytes. The
!View Function Statistics command of cinderella-M is one way to count the binary
code size, and this is the method we use. The MIPS R4000 we use has icache line size of 16,

CHAPTER 4. REAL TIME ANALYSIS

85

while the time to load a single instruction cache line is 18 cycles. Thus, for oh1, we nd the following using Equation 4.3:

WCET

reload

icache

=

d

3584 ,
16

1e + 1  18 =

4050

2

Practical Considerations in Instruction Cache Analysis
As in Section 4.2.5, we consider the case where the processor is a MIPS R4000. Note that the MIPS R4000 does not have a scratchpad section in its primary caches instruction and data are separate, nor is it con gurable to allow one. The cache controller is all in hardware. Thus, in order to calculate the WCET of the four operating systems routines we use ISR, priority ,scheduler save ,context and restore context, we always assume that they miss in the instruction cache; this assumption was implicit when we calculated these values in Section 4.2.5.
This estimate is obviously undesirable because the routines are called often and most likely will often be resident in the cache e.g. none of the three software-tasks considered in our robotics example take up the full instruction cache size of 8K. We could eliminate the instruction cache misses for these four routines in general by
either i nding with other analysis an upper bound on the number of times the routines can be kicked out of the instruction cache, or ii placing the four routines
into a scratchpad section of the instruction cache i.e. a scratchpad section is one that is never kicked out by the caching system in order to make room for new instructions
due to a cache miss. Unfortunately, ii is not available for the speci c CPU we
consider.

CHAPTER 4. REAL TIME ANALYSIS

86

4.3.3 Total Upper Bound on WCET
In this section we combine the results of the previous two sections in order to come up with a total upper bound formula for the case of tasks with priority  running on a CPU where the set I of tasks may execute early and the set J of tasks may be jumped.
Let W CET prsched be the WCET of the priority scheduler code, W CET savecntxt be the WCET of the save context code and W CET restorecntxt be the WCET of the restore context code. Furthermore, let W CET reload icachei be the maximum additional WCET due to extra instruction cache misses in task
i 2 I, and let WCET reload icache be the maximum additional WCET due to extra instruction cache misses for any i 2 I.
Now, for each possible interruption by a higher priority task that a task can experience, we have to add the cost of reloading all the instruction cache lines for that task to the overall WCET for the CPU. In the worst case, this additional cost
will be incurred for every possible call to rc. Thus, for each possible call to rc,
we add the worst case instruction cache re ll time, as well as the WCET for the restore context code.
Thus, by Theorem 4.2 and its corresponding Equation 4.1, for a given G and I
with one element fi1g and the associated J, an upper bound on the increase in overall
WCET for G is given by the following:
jJj  WCETprsched + WCETsavecntxt + WCETrestorecntxt + WCET reload icache
4.4 Similarly, by Theorem 4.3 and Equation 4.2, for a given G and I with associated J, an upper bound on the increase in WCET for G is given by the following:
jJ,J I Ij,1WCETprsched+WCETsavecntxt+WCETrestorecntxt+WCET reload icache
4.5
This completes our calculation of the total upper bound on WCET for the CPU with

CHAPTER 4. REAL TIME ANALYSIS

87

instruction cache analysis included.
4.3.4 Constructive Heuristic Scheduling with Out-of-order Execution
In this section we present a heuristic algorithm that can improve the solution of the constructive heuristic scheduling algorithm where we do not have Assumption 4.3 and thus software-tasks are not all necessarily atomic.
We rst compute the priorities by the algorithm of Section 4.2 for multiple NEV ER
sets. Thus, we have an order of software- and hardware-tasks contained in NEV ER,
SETS and the corresponding WCET for the DAG representing the application. Our goal is to increase CPU utilization by starting execution of a low priority softwaretask that is ready when no higher priority software-task is yet ready. However, if not done carefully, we could end up increasing overall WCET, although in general relaxing Assumption 4.3 will allow us to reduce WCET for the graph, thus improving our solution. We use the bounds proven in the previous section to guide our decision and guarantee that any out-of-order execution allowed will not worsen the WCET.
The basic insight that we gain from the previous section is the following. Suppose we consider a software-task pi lower in priority and thus later in execution if all software-tasks execute strictly in priority order than two consecutive priority software-tasks k1 and k2 which leave the CPU unused for a certain number of cycles between the completion of k1 and the beginning of k2. Let's de ne a function get spacek1; k2 that returns a number equal to the amount of unused CPU cycles. Should we allow pi to execute after k1 nishes assuming there are no control dataow constraints preventing pi from doing so? To answer this question, we use the bound found in Equation 4.5 of the previous section: if the amount of space unused cycles is greater than or equal to Equation 4.5, then yes, otherwise no. That is the

CHAPTER 4. REAL TIME ANALYSIS

88

Execute out of orderG; ; NEV ERSETS;

WCETprsched; WCET savecntxt; WCETrestorecntxt f

1 SWNEV ER = 1st set in NEV ERSETS; m = jSWNEV ERj;

* Get set and number of software-tasks *

2 = src; p1; p2; : : : ; pm where pi 2 SWEV ER; 1  i  m;

and p1 p2 : : : pm;

* stores the source followed by the software-tasks in priority order *

3 p0 = src;

* now we have = p0; p1; p2; : : : ; pm *

4 W = WCETprsched + WCETsavecntxt + WCETrestorecntxt;

5 I = ;; J = ;;  = ;;

*  keeps track of new precedence constraints *

6 i = 1; WCET reload icache = 0; * i counts the number of tasks in set I plus one *

* 7

the following for loop for l = m; l  2; l

considers
, , f

allowing

sw-tasks

pm;

pm,1;

:

:

:

;

p2

to

execute

early

*

8 9

kif2k=2p2l,J1; v = 0;

10 else v = 1; * v counts the number of tasks skipped by pl and not already 2 J *

11 new prec task = k2;

12 13 14

for k1 if 9

= a

pl,2 to k1 = p0 f
precedence constraint

fk2

!

plg

continue;

* exit inner fork1=. . .  loop *

15 if get spacek1; k2 

16 i + v + num tasks skipped  - 1*W + W CET reload icachei  f

17 new prec task = k1;

18 after prec task = k2;

19 if WCET reload icachei WCET reload icache

20 21

g WCET reload icache = WCET reload icachei;

22 k2 = k1;

23 if k2 : 2 J v + +;

24 g

25 if new prec task f

26

 =

fnew

prec task ! plg;
* add new precedence

constraint

fnew

prec

task

!

plg

to



*

27 update I, J;

28 reduce get spacenew prec task; after prec task

29 by i + num tasks skipped *W + W CET reload icachei;

30 i + +;

31 32

g else f  =  g

fpl,1 ! plg; g * add consecutive precedence constraint to  *

33 return, WCET reload icache;

g

Figure 22: Execute Out-of-order Algorithm

CHAPTER 4. REAL TIME ANALYSIS

89

insight behind the heuristic Execute Out-of-order procedure of Figure 22. We describe now the heuristic algorithm of Figure 22 that improves the exe-
cution time of a schedule by allowing out-of-order execution. From , which was computed by the algorithm described in Section 4.2.3, we obtain the software-task order p1; p2; : : : ; pm, where there are m software-tasks. Then we consider allowing a software-task pl to execute early one at a time in reverse order of the software-tasks from this set except for the rst software-task, for which it does not make sense to
execute early. Thus, given a software-task pl 2 p2; p3; : : : ; pm, and starting with the
software-task scheduled last i.e. pm, we consider allowing pl to execute early. For each such software-task pl we check if pl can execute in some unused space between two consecutive and higher priority software-tasks k1 and k2, assuming no precedence constraints are violated. If pl can execute in the space, then we check if the space is big enough to account for the worst-case extra execution time that will be incurred according to Equation 4.5. Note that we calculate Equation 4.5 in Figure 22 by using num tasks skipped , a function which returns the number of tasks currently in
J , J I i.e. not including the tasks currently under consideration, unless they
were already placed in I or J in a previous iteration. Now, if the space of unused CPU time is big enough, then we greedily schedule pl in that space and appropriately reduce the available space to re ect the new schedule; otherwise we add the prece-
dence constraint of strict in-order consecutive execution, namely fpl,1 ! plg. As we
go along, we keep track of I and J as we add tasks to each set. Continuing in this way, we consider all possible software-tasks one by one for early execution.
When this algorithm completes, we have a nal set of precedence constraints for the software-tasks that allows out-of-order execution without increasing the WCET of the application.
Example 28 Sample Application of Execute Out-of-order algorithm Consider Fig-
ure 23, which shows the BCET and WCET for each task, the icache re ll WCET for the

CHAPTER 4. REAL TIME ANALYSIS

90

src

Ohold1 Law

Jhold Law cg

oh0 cjd

fk oh1 mvm1

SWNEVER = {oh0,oh1,cjd}

Task ---- cg oh0 oh1 fk cjd mvm1 mvm2 src sink

Task BCET (cycles) ----- 11,000 1,598 12,341 4,500 9,989 5,000 5,000 0 0

Task WCET (cycles) ----- 11,000 2,221 17,399 4,500 13,213 5,000 5,000 0 0

icache refill WCET (cycles) -----
612 4050
3258

mvm2

TT(oh0) > TT(cjd) > TT(oh1)

sink
Figure 23: Example With WCET Calculation of Instruction Cache Re ll Time

software-tasks SWNEV ER, and the priorities found for the tasks: oh0 cjd oh1.
We begin by considering oh1 for out-of-order execution. We nd that the space between
the end of oh0 and the beginning of cjd is 11,000 - 2,221 + WCETisr + WCETprsched = 8,643 cycles using the costs of Section 4.2.5, from which we also nd that W = 422. At
this point in the algorithm of Figure 22, we nd that i + m + num tasks skipped,  , 1 = 1 + 1 + 0 , 1 = 1, and that WCET reload icachei = 4,050, giving us a move cost of 1  422 + 4; 050 = 4; 472. Since 8; 643  4; 472, we set new prec task of Figure 22 to
oh0. We next nd out that oh1 cannot execute before oh0 since oh1 requires data generated
by oh0. Thus, we add the precedence constraint foh0 ! oh1g which means that we do not add precedence constraint fcjd ! oh1g. Therefore, the run-time scheduler will set the start
event of oh1 as soon as oh0 nishes execution instead of waiting for cjd to nish.
We next consider cjd for out-of-order execution. We nd that it does not make sense to
try to have cjd execute before oh0 since oh0 starts right away. So we add the precedence
constraint foh0 ! cjdg which means that cjdg will run to completion since it cannot start
until the task immediately preceding it in priority executes. This completes the algorithm of
Figure 22 for the example of Figure 23.
Note that the precedence constraint foh0 ! oh1g in this case is redundant because the

CHAPTER 4. REAL TIME ANALYSIS

91

precedence constraint is already enforced by a control data- ow constraint in general, of course, such redundancy will not always be the case.
The result is that the lower priority task oh1 executes in the idle CPU time between the end
of oh0 and the beginning of cjd. 2
Calculation of WCET With Out-of-order Execution
In order to make a correct calculation of the WCET, we have to consider the time spent executing the ISR, the priority scheduler, and context switches. As in Section 4.2.5, we will consider the speci c case where the processor is a MIPS R4000. We use the following costs, obtained by analyzing our run-time scheduler software code executed on a MIPS R4000 model with no cache analysis, i.e. assuming we always miss in the instruction and data caches: save context = 162 cycles, restore context = 162 cycles, interrupt overhead = 38 cycles, and priority scheduler task selection = 98 cycles.
After execution of the Execute Out-of-order algorithm of Figure 22, we have a
maximum value for WCET reload icache which could be zero if jIj = 0.
With these costs, we calculate the WCET of the entire graph, scheduling everything ASAP where each software-task has WCETisr + WCETprsched = 136 cycles added to its WCET. At this point we have performed exactly the same calculations
as in Section 4.2.5. If jIj = 0, then this is our nal answer. If jIj =6 0, then both I and J are nonempty, and we have to account for extra
overhead. We use the bound found using Theorem 4.3 in Section 4.3.3, namely Equation 4.5, reprinted here for convenience:
jJ ,J I Ij,1WCETprsched+WCETsavecntxt+WCETrestorecntxt+
WCET reload icache. Adding this value to the WCET found from scheduling the graph gives us an upper bound on the WCET of the graph. This is the value we return to the user.

CHAPTER 4. REAL TIME ANALYSIS

92

sw-task

 cycles hw-task  cycles

int-ser-routine

38 cg 11,000

priority-sch-sw

98

oh0 2,221

int-ser-routine

38

priority-sch-sw

98

oh1 8,507

int-ser-routine

38 fk 4,500

save context

162

priority-sch-sw

98

cjd 13,213

priority-sch-sw

98 mvm2 4,400

restore context

162

WCET reload icache 4,050

oh1 90 mvm3 4,400

oh1 4,400 mvm4 4,400

oh1 4,402 mvm1 4,400

Table 8: WCET Calculation Example

Example 29 WCET calculation Consider Figure 23. If we make each software-task run
to completion, then with the optimal order of oh0, cjd, oh1 we calculate that the WCET
for the graph is 46,284 cycles. However, we found in Example 28 that we should allow oh1 to execute after oh0, even though oh1 has a lower priority than software-task cjd. This allows previously unused CPU cycles to be lled.
We have J = fcjdg, I = foh1g and J I = ;. The heuristic of Figure 22 gives
us WCET reload icache = 4050. Using our costs for WCETprsched; WCET savecntxt; WCETrestorecntxt and WCET reload icache, we nd that W = 422. From Equation 4.5,
we nd that
jJ , J I Ij , 1  WCETprsched + WCETsavecntxt + WCETrestorecntxt + WCET reload icache = 1  422 + 4050 = 4472.
Table 8 shows the ASAP graph schedule with the worst-case execution time added in. Notice
that the maximum context switch overhead and the maximum one additional call to the priority

CHAPTER 4. REAL TIME ANALYSIS

93

scheduler has been accounted for. The nal WCET is 42,113 cycles, which is less than our initial solution of 46,284 cycles. 2
This nal output is an upper bound on the WCET of the graph given the priorities assigned to software-tasks and the precedence constraints added to the graph and therefore implemented in the hardware portion of the run-time scheduler. In addition to helping to limit the increase in overall WCET due to software-tasks, the added precedence constraints also guarantee mutually exclusive invocation of hardware-tasks in the same NEV ER set.
Notice that with this result we do not know exactly when each software-task will begin and end. Software schedulers are by their very nature dynamic, especially with a system like ours that contains caches. Thus, a run-time system that statically schedules all software-tasks and their start nish times may require timers and other additional components, making such an approach infeasible or impractical. Also, the total WCET found for the system may be one that no single static schedule could achieve, because the possibilities for di erent interactions between tasks could not be so tightly arranged as with the dynamic approach here.
So we now can analyze satis ability of a rate constraint in a dynamically changing, concurrent execution of hardware-tasks and software-tasks with multiple resource constraints expressed with NEV ER sets, given our run-time scheduler implementation.
4.4 Task Splitting
One of the limitations of the Execute Out-of-order algorithm of the previous section is that the original priorities assigned to software-tasks is kept. However, having abandoned Assumption 4.3, one might be tempted to go back to the original formulation of the Constructive Heuristic Scheduling Algorithm of Section 4.2 used to assign

CHAPTER 4. REAL TIME ANALYSIS

94

priorities. Can we improve upon the algorithm when software-tasks are allowed to execute out-of-order? Are there optimal task priorities with out-of-order task execu-
tion which any algorithm will always miss because of Assumption 4.3? It turns out
that there are. Consider the following example:

src
a b
c
d

NEVER = {b,c}

task ---- a b c d src sink

WCET (cycles) ----- 3,000 6,000 4,000 2,000 0 0

TT(b) > TT(c)

sink
Figure 24: Constructive Heuristic Scheduling Example of Suboptimal Result

Example 30 Consider Figure 24. The constructive heuristic scheduling algorithm will com-
pare the two possible orderings, b; c and c; b, and will nd that the overall WCET is 12,000 cycles for the rst case and 13,000 for the second. Thus, software-task b will receive the highest
priority. Even an exhaustive algorithm which enumerates all possibilities will nd this result. Now we run the heuristic of Section 4.3.4 and nd that we cannot improve on the solution
since there is no space unused CPU cycles before b, which begins execution right away. Thus c must wait until b nishes to begin execution; overall WCET for the graph is still 12,000 cycles.
Suppose c had a higher priority than b and that out-of-order execution were allowed. Then, ignoring the software scheduling, interrupt and context switch overhead, b would execute for 3,000 cycles concurrently with a, then c would execute for 4,000 cycles, and nally b would nish in 3,000 cycles while d concurrently executes, resulting in an overall WCET of 10,000 cycles, which is signi cantly less than previously found. 2

CHAPTER 4. REAL TIME ANALYSIS

95

src

b1 b2

a c
d

sink

NEVER = {b (split = 2) ,c}

task ---- a bb12 c d src sink

WCET (cycles) ----- 3,000 3,000 3,000 4,000 2,000 0 0

TT(b1) > TT(c) > TT(b2) Result: TT(c) > TT(b)

Figure 25: Example of Scheduling with Task Splitting

To deal with this problem, we add the following heuristic: we allow the user to specify for a task s that it can be split into n equal chunks. We then split s into n sequential tasks s1; s2; : : : ; sn each with n1 of the WCET of s. Then we run the constructive heuristic scheduling algorithm as before, but from the nal order we set the priority of s to be the priority found for sn and discard the priorities found for s1; s2; : : : ; sn,1.
Example 31 Consider Figure 25. This time the user speci es that software-task b can be
split into n = 2 chunks. The modi ed speci cation of the NEV ER set, WCET for each task,
and resultant graph can be seen in Figure 25. The constructive heuristic scheduling algorithm
nds the ordering b1; c; b2 which is optimal, from which we extract the order only including bn, resulting in c; b2. Thus c receives a higher priority than b and we have c b.
Now we run the heuristic of Section 4.3.4 and nd that b should be allowed to begin execution right after the source, and then be suspended when c becomes ready. The hardware portion of
the run-time scheduler is synthesized to implement this, namely by interrupting the CPU right
away to communicate a start vector indicating that b is ready to execute. Ignoring the software scheduling, interrupt and context switch overhead, the overall WCET is now 10,000 cycles. 2

CHAPTER 4. REAL TIME ANALYSIS

96

4.5 Critical Regions

An important programming methodology to support is the use of critical regions. A critical region is a section of software code where critical resources are used or common variables are read written. In fact, software semaphores were originally created in order to allow the speci cation of critical regions in software. Thus, if our run-time scheduler can support the speci cation of critical regions, then we can accomplish the same goal without resorting to semaphores.

src
a b
c
d

NEVER = {b,c} NONINT = {b}

Task ---- a b c d src sink

WCET (cycles) ----- 1,000 6,000 4,000 2,000 0 0

sink
Figure 26: Example Speci cation of Noninterruptible Task
We support critical regions via noninterruptible software-tasks. The user can specify a set NONINT of noninterruptible software-tasks. If a task is in NONINT then the task will not be considered for membership in the set I of tasks allowed to execute
out-of-order. In other words, all higher priority software-tasks must nish before the
task is scheduled, so that any interrupts received during the task's execution cannot be from a higher priority task, thereby ensuring that the noninterruptible softwaretask is never kicked out. In this manner a set of critical regions, e.g. that access the same shared variables or other resource, can be de ned. The algorithms of Section 4.2 are modi ed to take into account that these processes are noninterruptible by simply

CHAPTER 4. REAL TIME ANALYSIS

97

retaining Assumption 4.3, namely that the task, once started, runs to completion. Note that one could implement a semaphore S by specifying each access to S as a noninterruptible software-tasks.
Since the entire critical region must run to completion, releasing the resource or no longer accessing the shared variable, the priority inversion problem does not arise in the nal implementation. The problem of priority inversion refers to a situation where a lower priority process holds a resource when a higher priority process interrupts which needs to use the held resource. In this case the higher priority process is prevented from executing and has to release control to the lower priority process; thus, the lower priority process has, in e ect, made itself higher in priority, i.e., the priorities of the two processes have been inverted. By design, a noninterruptible software-task cannot give rise to the priority inversion problem.
Note, however, that we assume that the critical region is located as a single task within a DAG. Thus, the only iteration on the critical region or semaphore allowed is that which occurs in each execution of the DAG. Loops de ned on a critical region are not allowed since a DAG cannot contain loops and remain acyclic. However, loops without critical regions are allowed in individual C and Verilog HDL tasks, as long as an upper bound can be given on the number of times a loop will repeat in a given execution of the task containing the loop.
Example 32 In Figure 26 task b is speci ed as noninterruptible. Clara nds that the order,
if each task runs to completion, is c; b. Since b is noninterruptible, we do not consider executing part of b during the unused CPU time available while a is executing. The precedence constraint
fc ! bg is generated. 2

CHAPTER 4. REAL TIME ANALYSIS

98

4.6 Summary
In this chapter we showed how to e ciently solve for the order of tasks in the same NEVER set using a heuristic that constructively builds a solution from the DAG representing the partial order needed for the hardware- and software-tasks to execute. Then we extended the heuristic to handle the case of multiple NEVER sets in hardware.
We next considered the case where software-tasks in the same NEVER set are allowed to interrupt and preempt each other. To handle this case, we statically calculate an upper bound on the number of extra calls that may be necessary to our kernel code namely, to the priority scheduler and context switch code. Using this number, we nd a total upper bound on the WCET with speci ed softwaretasks allowed to preempt other particular software-tasks. Based on this analysis, we propose a greedy heuristic for allowing some lower priority tasks to execute during idle time, thus possibly being preempted by higher priority tasks.
Finally, we presented a task-splitting method to improve our results in some cases where our assumptions change, as well as a feature to declare certain tasks to be noninterruptible, thus providing support for critical regions.

Chapter 5
Implementation and Experimental Results
This chapter consists of three sections. In the rst section, we present an overview of the tool ow implementing the design approach and algorithms of the previous chapters. In the second, we present an example of a PUMA robot control to show how a design can be successfully synthesized using the CAD system described. We verify the synthesis results of the PUMA controller via simulation. Finally, in the last section we present a sample prototype implementing the split run-time scheduler in a Haptic robot. The Haptic robot prototype was achieved by modifying existing Haptic robot in the Computer Science Robotics laboratory at Stanford.
5.1 Design System Implementation
Figure 27 repeated from Figure 6 for the reader's convenience shows our tool ow when applying our design tools to a system design. The hardware tasks are written in Verilog HDL and software tasks are written in C. Constraints include relative timing constraints, a single rate constraint, and resource constraints in the form of
99

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 100

behavioral Verilog

System Specification C

constraints

User options (protocols, fifos, RAM model, core)
BC

Interface Generation
wcet

behavioral Verilog

Serra Run-Time Scheduler Synthesis
RTL Verilog

wcet cinderella-M
User options (microprocessor core, RAM size)

BC DC

V1 ... Vn
CPU 
Core1 L1

RTS.v
RTS.c
RAM *.c

Figure 27: Tool Flow and Target Architecture

NEV ER sets. Precedence constraints are implicit in the task speci cation which takes the form of a Directed Acyclic Graph. Serra performs run-time scheduler synthesis and worst-case execution time WCET analysis. Satisfaction of relative timing constraints minimum and maximum separation in hardware blocks is dealt with in hardware control synthesized by the Thalia2 tool. Thalia2 generates a hardware FSM implementing a CFE speci cation of the system with relative timing constraints CFEs and Thalia2 were described brie y in Section 2.4 CM96, Coe96, CM97 .
The system-level tasks, written Verilog HDL and C, and the constraints are input to Serra and to a tool that generates the interface. One of the tasks is speci ed as the main task. Worst-case software execution time is found by the tool cinderellaM, which was described in Section 2.5 and which takes as input C programs for the

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 101

software tasks and outputs a WCET for each software task note that bounds on loops must be provided by the user MWWL96, LM95 . Similarly, from BCTM we obtain a WCET for each hardware-task loop bounds must be provided here in some cases as well. Since we compare BCTM-generated WCETs with software WCETs, we convert all delays to the number of microprocessor clock cycles since the hardware clock speed is typically slower.

behavioral Verilog

System Specification C

constraints

Priority Scheduler template

Diego
BC wcet

dataflow analysis
cdfg
cfe

wcet

cinderella-M

GCC

context switch, ISR templates

relocatable assembly code

priorities

Thalia2

Clara

linker

precedences

Run-Time Scheduler control FSM in RTL
Verilog

sw tasks assembly code

Run-Time Scheduler assembly code

Figure 28: Block diagram of Serra: the boxes indicate tools and the ovals indicate data.

5.1.1 Serra Run-Time Scheduler Analysis and Synthesis
The Serra design tool is shown in Figure 28, which expands the box labelled Serra in Figure 27. Serra rst extracts the task control- ow from the system speci cation. The user-speci ed main task contains the overall sequence of tasks in the application; from it we extract a CFE describing the task ow of the system. Diego can extract a

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 102
CFE description from a task written in Verilog HDL; for example, given the main task in Verilog HDL, Diego can generate in CFE format the sequence of task invocations calls from the main task. A WCET for each task is calculated using BCTM or cinderella-M; next, the CFEs are annotated with the WCET calculated for each hardware or software task. These WCET values are used to annotate the leaf tasks in the nal DAG of the system speci cation. Figure 10 showed a sample DAG and a corresponding table with the WCET annotations. Finally, a single rate constraint is speci ed in the form of invoking the main task at a xed rate.
Serra synthesizes the control-unit of the scheduler by means of tool Thalia2 which takes as input a CFE description and produces a logic-level description in synthesizable Verilog HDL CM96, MCSM96 . The timing, resource and precedence constraints speci ed in the CFEs input to Thalia2 are translated into a nite-state machine implementation if a solution is found which satis es the constraints.
The constructive heuristic scheduling algorithm is implemented by Clara, which generates the static priorities for the software and hardware tasks. Since we assume that all hardware-tasks are noninterruptible, in the case of hardware-tasks in the same NEV ER set, the static priorities found by Clara are converted into precedence constraints enforcing the order indicated. Serra synthesizes the control-unit of the scheduler into a hardware FSM which includes the additional precedence constraints found by Clara.
Clara can e ectively handle multiple NEV ER sets, split tasks Section 4.4, and noninterruptible software-tasks Section 4.5. Furthermore, Clara can generate precedence constraints among software-tasks in a single NEV ER set where lower priority software-tasks can execute during idle time when higher priority software tasks are not yet ready. The analysis for this case is also implemented by Clara, and thus it calculates WCET for the out-of-order execution using the results from Section 4.3.4. Clara has been implemented in 15,000 lines of C.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 103
To generate the run-time kernel's C code, Serra uses templates of the priority scheduler in C, the Interrupt Service Routine ISR in MIPS assembly and context switch code in MIPS assembly. For the software that runs on the microprocessor core CPU, the individual software-tasks are compiled together with the priority scheduler, ISR, and context switch code using standard C compilers and linkers. Data and program memory are statically allocated.
Serra also allows the user to override the priorities found by the heuristics of Clara. Even further, Serra allows the user to override precedences added to the hardware portion of the run-time scheduler, so that di erent software-tasks can be allowed to execute early in an order di erent from that found by the heuristic of Section 4.3.4. Thus, possible optimizations can be added by the user. Serra can then calculate the new WCET for the application with the new set of priorities and or new set of precedences. Serra thus provides for interactive performance evaluation and tuning of the run-time system, as well as synthesis for each particular implementation.
5.2 Design Case Study: PUMA Robot Arm
In this section, we use and show the Serra system to design a robot controller for manipulating two PUMA arms, which are a standard in the robotics industry AKB86, Uni84 Figure 4 showed two PUMA arms grasping an object. The controller implements concurrent models of two laws" that must calculate new torques every millisecond. We show how real-time constraints can be satis ed with a run-time system that also provides for dynamic allocation of resources, where by dynamic we mean that the exact times resources are allocated is not statically determined but instead is determined at run-time. We describe how we simulated the nal implementation.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 104

Ohold1 Law

oh0
fk oh1
mvm1
2
xf1 -8 xb1

Set Torque

Jhold Law
cg cjd mvm2 mvm3 mvm4
2
xf2 -8 xb2

Figure 29: Directed Acyclic Graphs of Ohold1 Law, Set Torque, and Jhold Law with Relative Timing Constraints
5.2.1 Two PUMA Arms
For our example, we consider the robot control algorithm of Figures 8 and 9. We implement the tasks required for executing Jhold Law and Set Torque in parallel with Ohold1 Law and Set Torque. The DAGs, including the leaf tasks that implement Set Torque, are shown in Figure 29; the full DAG is shown in Figure 30. Note that Xmit Frame1 xf1 and Xmit Bit1 xb1 of Set Torque1 have a strict relative timing constraint of xb1 starting no less than 2 cycles after xf1 and no more than 8 cycles after. The exact same constraint holds for Set .Torque2 This constraint could not always be satis ed with control signals generated by a run-time scheduler in software note our CPU in Figure 27 has an L1 cache. We assume that the full system drives Xmit Bit from hardware modules other than Xmit Frame and thus the two hardware tasks, although tightly coupled, must be kept separate.
We perform real-time analysis using the Clara tool. We rst use Constructive Heuristic Scheduling for multiple NEV ER sets and nd the order of oh0, cjd, oh1 for the software-tasks. Even with task splitting applied to oh1, the order does

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 105

wnt gp1 gv1 gp2 gv2

NEVER1 = {oh0,oh1,cjd} NEVER2 = {mvm1,mvm2,mvm3,mvm4} NEVER3 = {xf1,xb1,xf2,xb2}

Ohold1 Law oh0

epsilon

fk oh1

mvm1

2
xf1 -8 xb1

Set Torque

Jhold Law cg
cjd mvm2 mvm3
mvm4
2
xf2 -8 xb2

Task BCET Task hw/sw (cycles) ---- ---- ----- cg hw 11,000 oh0 sw 1,598 oh1 sw 12,341 fk hw 4,500 cjd sw 9,989 mvm1 hw 4,400 mvm2 hw 4,400 mvm3 hw 4,400 mvm4 hw 4,400 xf1 hw 1 xf2 hw 1 xb1 hw 322 xb2 hw 322 src - 0 sink - 0

Task WCET (cycles) ----- 11,000 2,221 17,399 4,500 13,213 4,400 4,400 4,400 4,400 1 1 322 322 0 0

icache refill WCET (cycles) -----
612 4050
3258

hm
Figure 30: DAG of Robot Arm Controller with Relative Timing Constraints
not change. Thus, we set the static priorities in the software scheduler such that oh0 cjd oh1. Then we run the Execute Out-of-order algorithm and nd, just as we did in Example 28, that we should allow task oh1 to execute on the CPU as soon as oh0 is nished. Therefore we nd the following precedence
constraints: foh0 ! gcjd and foh0 ! oh1g.
The Constructive Heuristic Scheduling algorithm found order mvm2, mvm3, mvm4, mvm1 for NEV ER2 and order xf2, xb2, xf1, xb1 for NEV ER3. Excluding redundant precedence constraints already present in the DAG, we nd the following
additional precedence constraints: fmvm4 ! gmvm1 and fxb2 ! xf1g.
As in Example 29, we calculate a WCET for of 42,113 for Figure 29 with out-oforder execution. This provides for the upper bound on execution speed for the tasks in Figure 29 under worst-case conditions.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 106

global_start hm_done wnt_done
gp1_done oh0_done cg_done
fk_done

s q' rq

s q' rq

s r

q' q

s r

q' q

s q'

oh1_done r q

cjd_done oh1_done & mvm4_done

s

q'

s r

q' q

mvm1_done mvm2_done mvm3_done

rq s q'
s q' r q rq

s q'

mvm4_done

rq

mvm1_done hm_done

s q' rq

c0
c1 c2 c3 c4 c5 c6 c7 c8 c9
c10

wnt gp1 gv1 gp2 gv2 oh0 cg fk oh1 cjd mvm1 mvm2 mvm3 mvm4 xf1 xb1 xf2 xb2 hm

Figure 31: Final Hardware Portion of Run-Time Scheduler

Figure 31 shows the hardware portion of the run-time scheduler. Signals wnt, gp1,gv1,. . . , hm in Figure 31 are the start events for the corresponding tasks in Figures 29 and 30. The signal global start kicks of execution for the very rst time; after that, the done signal of hm restarts the iteration. The right-hand box is the FSM generated from the CFE for the system CM96, MSM97 . Note that Figure 31 shows an optimization in the control logic for mvm1. Since the best case execution time, or BCET, of oh1 is greater than the WCET of fk, we can set the start signal of mvm1 based only on the done signals of oh1 and mvm4 rather than a conjunction of the done signals of fk, oh1 and mvm4. Similarly, due to the length of mvm1-4, we
nd that we do not need to add the fxb2 ! gxf1 precedence constraint. Finally,
note that the designer knows that hm does not need to wait for the transmission of the torque values to the robot arms; it can begin calculating right after mvm4 nishes. These optimizations were added in Serra manually by the user.
The software tasks are compiled and linked into assembly, with data and program memory statically allocated, as well as memory-mapped I O. Finally, the software

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 107

portion of the run-time scheduler is generated in the form of an Interrupt Service Routine that reads in a start vector which task needs to be executed in software, a priority scheduler which selects which software-task to execute, and routines for saving and restoring context.
The system begins each iteration once a millisecond. After obtaining the positions and velocities of the two robot arms, the run-time scheduler starts the execution of cg in hardware for Jhold Law and oh0 in software for Ohold1 Law. It continues with interleaved hardware-software execution as shown in Table 8 and pictured graphically in Figure 20. Finally, it tightly schedules accesses to Xmit Frame and Xmit Bit to set the torques for the robot.
Notice that from the point of view of the run-time scheduler, xf1 and xf2 are only one-cycle actions; we do not wait for any done signal, but assume that if xb1 completes then xf1 has completed, and similarly that if xb2 completes then xf2 has completed. This was a design decision made up front based on the Verilog HDL code for the tasks. On the other hand, notice that xf1, xb1, xf2, and xb2 are all in the same NEV ER set. This is because the same hardware-tasks, Xmit Frame and Xmit Bit, are used to transmit the torque data, and we do not want xf2 to begin while xb1 is still executing, nor xf1 to begin while xb2 is still executing. Thus we need to pay attention to the done events of xb1 and xb2.

Software-Task Lines Lines Task Task Icache re ll C Assem. BCET WCET WCET

oh0 oh1 cjd int-ser-routine context-switch priority-sch-sw

90 693 286 NA NA 107

237 1,598 3,263 12,341 1,177 9,989
26 11 42 34 141 26

2,221 17,399 13,213
38 162 98

612 4,050 3,258 NA NA NA

Table 9: Code space, BCET and WCET for sw-tasks.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 108

The complete, attened DAG with relative timing constraints is shown in Figure 30 reprinted from Figure 9 with additional information added. The epsilon task takes zero cycles and serves to synchronize the task executions by making sure every task before it has completed before continuing. The scheduling of tasks shown in Figure 30 but not in Figure 10 wnt, gp1, gv1, gp2, gv2, xf1, xb1, xf2, xb2, hm
together take 57,200 cycles in the worst case. Since our MIPS R4000 core runs at 100 MHz, the rate constraint allows us to use 100,000 cycles. Thus, we have 42,800 cycles left for the remaining tasks oh0, oh1, fk, cg, cjd and mvm1-4. The WCET of 42,113 we found ts our rate constraint note that without out-of-order execution, we would have had a WCET of 46,284, which would violate the constraint. Thus, our schedule guarantees that we meet our hard real-time rate constraint.

Hardware-Task Lines Area WCET Verilog

cg fk mvm xmit-frame xmit-bit run-time-sch-hw

2897 59,587 2362 42,168 629 33,645 108 987
66 199 484 413

11,000 4,500 4,400
322 322 99,701

Table 10: Results for the synthesis of hw-tasks.

Table 9 presents the results for the compilation of the software and best- and worstcase execution time estimation with cinderella-M. Unfortunately, cinderella-M does not perform any data-cache analysis, so all data references are assumed to miss, incurring the cost of loading in a data cache line.
In Table 10, we see the results for the synthesis of the hardware tasks of Figure 10 using the Behavioral CompilerTM, except for the run-time scheduler hardware part which was synthesized with the Design CompilerTM. The third column in Table 10

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 109
shows the number of gate equivalents of hardware required using the LSI 10K Logic library. The WCET values are initial estimates based on the number of control steps needed to execute the hardware as scheduled by the Behavioral CompilerTM. The estimates are rounded up and then scaled to allow direct comparison with the MIPS R4000 processor operating at 100MHz we clock the hardware at 10 MHz.
5.2.2 Verilog Simulation
Using a MIPS R4000 processor core model in Verilog HDL, we simulated the Robot Arm Controller, with its synthesized run-time scheduler, in Verilog HDL using Synopsys' Chronologic VCSTM. The simulation utilized memory-mapped I O as the medium of interface between the hardware and software tasks involved in the robotics control algorithm.
The run-time scheduler comprises, for hardware, the Run-Time Scheduler module and its associated set-reset latches as shown in Figure 31, and, for software, the Priority Scheduler and the Interrupt Service Routine.
Figure 32 shows a simpli ed block diagram of the Verilog simulation showing the memory mapping for the Priority Scheduler and Interrupt Service Routine, the three robot control software algorithms, and the memory-mapped and local start and done vectors. Note that the diagram neglects to depict the interrupt signal and the existence of the L1 cache. Our target architecture consists of a MIPS R4000 core with multiple hardware modules, each implementing a particular hardware-task. The CPU has a two-level memory hierarchy consisting of instruction and data caches with a large RAM.
The Priority Scheduler PRS, along with the robot control software algorithms are automatically placed in main memory starting at word address 0x64 by the linker. The Interrupt Service Routine ISR is placed in memory at location 0x2000 0060. This location was chosen because the Program Counter PC jumps to 0x2000 0060

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 110

Mvm mvm_done
mvm_start

fk_start

Fk

fk_done

Run-Time Scheduler

cg_start cg_done

Cg

Word Address

Memory-Mapped IO

2800_4000 Software Done Vector

2800_0000 Software Start Vector
Interrupt Memory 2000_0060 Interrupt Service Routine
Main Memory

Ohold1 Cjd Ohold0 0000_0064 Priority Scheduler

Local Start Vector (Register 26) Local Done Vector (Register 27)

Figure 32: Simpli ed Block Diagram of the Simulation

whenever an interrupt is asserted on the CPU. The memory-mapped start and done vectors were chosen to be at 0x2800 000 and
0x2800 4000, respectively. The lower 32 bit contents of 0x2800 0000 are loaded into register r26 by the ISR whenever an interrupt is asserted, whereas the contents of register r27 are written out to 0x2800 4000 by the PRS once a speci c software task is done. Registers r26 and r27 were chosen because they are kernel-reserved registers.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 111
5.2.3 Run-Time Scheduler Software
Compiling the Priority Scheduler
The Priority-Scheduler code is written in C in le prs:c. The task of this code is to:
Look at the on-chip 32-bit start register register 26
Determine which corresponding software task should be run
Write out the proper value to the on-chip done vector register 27 once a speci c software task is done. The contents of register 27 are later transferred to the done vector in memory-mapped I O.
However, reading from, and writing to, an on-chip register is somewhat di cult to do in C. To get around this problem, we can either inline assembly code in the C code, or change the compiled C. We have chosen the latter option since it was trivial to change the assembly code by hand.
Thus the procedure for compiling prs:c is:
Compile prs:c with the standard cc compiler on the SGI.
Edit the resultant le, prs:s, from the previous step to have all instances of start and done access registers 26 and 27 respectively. For instance, the code fragment start = start & ~CJD; in prs.c compiles to
.loc 2 178 start = start & ~CJD;
lw $12, $36$29 $29 is the stack pointer and $13, $12, -3

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 112
sw $13, $36$29 in prs:s. This needs to be be modi ed to access register 26, where the start
value really resides, instead of the stack, like so: .loc 2 178 start = start & ~CJD;
add $12, $0, $26 and $13, $12, -3 add $26, $0, $13
Also change done ioif = done;
lw $12, $32$29 sw $12, $40$29
to done ioif = done;
add $12, $0, $27 This transfers the done vector to register 27 sw $12, 0xA0010000 $2800 4000 in memory-mapped I O Note that just the instruction sw $27, 0xA001000 will not work as the compiler has allocated space for two instructions during the compilation of prs:c
although inserting a nop would work.
Assemble prs:s with the MIPS assembler available on the SGI using cc
Copy the objectcode, prs:h, to objectfile:h, and prs:start to objectfile:start. During initialization of the simulation, memory:v will read in objectfile:h and

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 113
place it in main memory at location 0x64 it actually varies depending on the start location speci ed by the linker in prs:h.
For purposes of veri cation, the contents of main memory just after initialization can be viewed in the le memstart:txt.
Interrupt Service Routine
The Interupt Service Routine code, isr:s, is written in MIPS assembly and transfers the start vector in memory-mapped I O to register 26 when an interrupt is asserted by the hardware Run-Time Scheduler rts:v; isr.s is assembled with the cc assembler on an SGI Indigo.Because the MIPS R4000 CPU transfers program execution to 0x2000 0060 whenever its interrupt pin is asserted, we have to forcibly place the ISR code there. This is simply done by changing the rst line of isr:h to read 2000 0060. However this means that we have to ensure isr:s does not contain any hard-coded" jumps branches to code within itself.
Initialization
Registers 26 and 27 have to be initialized to zero at the start of the simulation, otherwise a software task may be prematurely started. This is done in regfile:v.
At simulation initialization, memory:v will read in isr:h and place it at 0x2000 0060. This can be veri ed by looking at the contents of the le memstart int:txt.
5.2.4 Run-Time Scheduler Hardware
The Run-Time Scheduler hardware modules rts:v and srlatch:v are the heart of the run-time scheduler system. The le rts:v handles resetting the hardware modules involved in the control ow, passes the start and done vectors between each of the

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 114
hardware and software tasks, and restarts the control ow after each iteration. The le srlatch:v models a normal set-reset latch.
Initialization
The initial block present in rts.v asserts a RESET on the following hardware modules: MV M 3:v rst the hardware module for task mvm, CG 2:v task cg, F J 2:v task fj, and robotarmcheckzero:ver2, which was synthesized with Thalia2, at simulation start time. It also starts o the contol ow by setting c0 high for 10 processor cycles.
Control ow module
The synthesized Verilog HDL module of the control ow implemented in the simulation is stored in le robotarmcheckzero:ver2. It has inputs c1 to c6 with the following correspondences: c1-oh0, c2-mvm, c3-fk, c4-oh1, c5-cjd, c6-cg. It also has outputs wnt, gp1, gv1, gp2, gv2, mvm, cjd, cg, oh0, oh1, fk, xf1, xb1, xf2, xb2 which are actually the start vectors for each similarly-named task as shown in Figure 31.
Start done for hardware tasks
File rts:v instantiates a set-reset latch for each hardware task. Thus, the hardware start and done vectors are not in registers but are hooked up directly to each harware task. For example, the task cg has a set-reset latch called trigger cg. When task cjd completes, it sends a done signal to the set S port of trigger cg, and trigger cg then outputs a high. This output is hooked up to the input pin c6 of robotarmcheckzero:ver2. Hence a cycle later robotarmcheckzero:ver2 will assert a high on its output pin cg. Signal cg feeds directly into CG 2:v, so this starts o task cg. Once task cg is done, CG 2:v sends out a done signal to the reset

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 115
R port of trigger cg, causing trigger cg0s output to go low; this then prompts robotarmcheckzero:ver2 to start the next task, which in this case is xf2.
Start done for software tasks
File rts:v also instantiates a set-reset latch for each software task. For software tasks, however, a start value has to be sent to the start vector in memory-mapped I O. This is accomplished by feeding in the output start pins corresponding to each software task from robotarmcheckzero:ver2 into a register and concatenating them
thus HW data in in = f61'b0, oh1, cjd, oh0g . HW data in in was chosen as a 64-bit
register because data transfers to and from memory-mapped I O occur in doublewords. The initiation of a software task is detected whenever any of the pins oh1, cjd, or oh0 go from low to high. When this happens, a series of steps follow:
rts:v sends a write request to memory-mapped I O.
rts:v sends a number indicating the location where it wants to write to. This number is an index o set, with each index step being a double-word, from the start of memory-mapped I O word address 0x2800 0000. Normally we want to send a 0 to have the memory-mapped start vector be at 0x2800 0000.
rts:v then waits for an OK from memory-mapped I O it looks at the pin hw ok.
When the OK arrives, rts:v asserts an interrupt on the CPU IntB = 1'b0, writes out the contents of HW data in in to 0x2800 0000 and deasserts the write request.
A cycle later, the interrupt is deasserted to prevent the CPU from thinking there were two interrupts requested instead of just one.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 116
A few cycles after the completion of a particular software task, another series of steps, which also involve the Priority Scheduler, follow:
The on-chip done vector is written to its corresponding memory-mapped I O location by the Priority Scheduler. The on-chip done vector is actually written to the L1 cache rst and then, because the R4000 implements a write-through cache, to memory-mapped I O.
Whenever a cache line is written to memory, L1cache:v sets PadAddrValid high
memory mapped io:v notices that PadAddrValid is high and checks to see whether the cache line is being sent to memory-mapped I O.
If it is, memory mapped io:v checks whether the address written to is where the memory-mapped done vector resides 0x2800 4000.
If so, a cycle later it transfers 64-bits starting at 0x2800 4000 to a register called done in rts:v.
rts:v looks at the bits of done and asserts a done signal corresponding to which bit is on. For instance, if done 1 is on, rts:v asserts cjd done for two cycles and feeds it to the set port of trigger cg and trigger oh1, thereby starting those tasks. It also sends it to the reset port of trigger cjd, thus causing robotarmcheckzero:ver2 to deassert its ouput pin cjd, indicating that task cjd is nished.
Restarting the control ow
At the end of the last task in the control ow, we would like task execution to restart at wnt. This is achieved by asserting c0 for a cycle at the completion of the last task. We know what the last task is and the detection of the negative edge of a signal from the last task's output pin from robotarmcheckzero:ver2 is de ned as its completion.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 117
5.2.5 Running the Simulation
We start the simulation by executing Chronologic VCSTM with le F BLD:cmd, which contains a list of the Verilog HDL les that we want Chronologic VCSTM to compile. Some highlights from the simulation are outlined below. Note that trace commands in Chronologic VCSTM, such as vcdpluson and vcdplustraceon in system:v, should be turned o as they tend to signi cantly slow down the simulation by periodically writing to disk.
Assertion of an Interrupt
Figure 33: Interrupt Asserted In Figure 33 we see the process by which the Run-Time Scheduler hardware signals the start of a software task. We see c4 going high followed a cycle later by oh1's start vector being asserted. This prompts rts:v to request a write to memorymapped I O HW request, with the value that is going to be written out residing in HW data in in the value four . Memory-mapped I O replies with hw ok. Then rts:v asserts an interrupt on the CPU and writes the value four from HW data in in

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 118
to the start vector in memory-mapped I O SRAM start.
Figure 34: PC Jumps to Start Address for Interrupt Service Routine In about 30 cycles, as shown in Figure 34, the PC jumps to address 0x2000 0060 and start servicing the interrupt. This transfers the contents of SRAM start to the on-chip start vector in register 26.
Completion of a software task
Figure 35 illustrates the sequence of events at the completion of a software task. Here, the software task cjd has nished execution sometime prior to time 52,790 and the on-chip done vector in Reg27 contains the correct value. The done vector done in the Run-Time Scheduler hardware still has its old value, however. The PC has just nished executing instructions before 0x9C that write Reg27 to memory-mapped I O.
Thus PadAddrV alid goes high as Reg27 is rst written to the cache. At the same time, the address in memory-mapped I O where Reg27 is going to be written to is on PadAddress ie. 0x2800 4000. Because of the write-through nature of

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 119
Figure 35: Software task cjd completes the cache, the cache line gets sent to location 0x2800 4000 in memory-mapped I O; this occurs during the second pulse of PadAddrV alid. The memory-mapped I O controller detects the write to 0x2800 4000, recognizes that this is the software done vector and hence writes it out to RTS tmp. A cycle later done in rts:v gets updated with the correct value. Then rts:v asserts cjd done, feeding it to the set ports of the set-reset latches of the next tasks in line oh1 and cg, causing their start vectors to go high. Also, cjd done is routed to the reset port of task cjd's set-reset latch, thus deasserting cjd.
Note that the L1 cache and the signals associated with it PadAddrV alid, PadAddress and PadWriteMask are clocked on Padph2 not shown here.
Reiteration of the control ow
Looking at Figure 36, we see that at the completion of the last task, in this example xb1, the control ow loops back and restarts itself. This is predicated by c0 going high.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 120
Figure 36: Control restarts itself after task xb1 The Run-Time Scheduler smoothly switches between hardware and software tasks. The memory-mapped I O interface and the interrupt scheme employed to transfer control between hardware and software behave as we predicted. Hardware tasks start and nish at well-de ned edges, whereas software tasks are more variable for instance, there is a distinct lag between the time an interrupt is asserted and the time when program execution starts at the appropriate software task. The Verilog simulation veri ed the Serra-synthesized run-time scheduler, split between hardware and software, for a controlling two PUMA robot arms.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 121
5.2.6 Design Gains
The original PUMA arm controller was implemented all in C code. However, due to the millisecond timing constraint a hard real-time constraint, the amount of computation available limits the precision and scope of the control algorithms. For example, designers in the Computer Science Robotics Laboratory at Stanford indicated that while they would very much like to use larger matrices for the state space representation of the kinematics and dynamics of the PUMA arms, they are limited to three-by-three matrices due to the lack of computational power. When they increase the matrix size to ve-by- ve, the slow down is signi cant enough to miss the millisecond deadline occasionally. If they increase the matrix size to eleven-by-eleven, then it hardly ever meets the hard real-time constraint of a millisecond.
While the actual hardware-software implementation of the PUMA control system was not carried out, this case study nonetheless shows that such a design could be carried out e ectively and to the level of detail required for design space exploration and timing veri cation. Furthermore, the detailed simulation shows the practicality of the system designed, e.g. in the coordinated ow of start and done control signals between hardware, software, and the run-time scheduler. With the automation provided by the Serra system, hardware software co-design of the control system is greatly improved over manual speci cation and design of the run-time system. For example, handling multiple NEV ER sets of mutually exclusive tasks is much more e ciently dealt with automatically than by hand with the many possible orderings one would need to consider. This is important as the previous research reviewed in Chapter 2 did not handle the case of multiple NEV ER sets of tasks in hardware and in software. Overall, the Serra system can predictably satisfy relative timing constraints, resource constraints in the form of NEV ER sets, and a rate constraint, producing a synthesized run-time system and thus allowing for e cient design space exploration.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS
5.3 Design Case Study: Haptic Robot

122

Figure 37: Haptic Robot With Graphics
In this section we present a sample implementation of a split hardware software runtime scheduler controlling an actual real-time robotics application as opposed to simulation. We considered the design of the following real-time robotics application: a Haptic robot implementing force-feedback based on interaction through a graphics display RKK97 . The Haptic robotics device contains a thimble where the user places his or her nger. The thimble is connected to the end of a small robot arm which can exert force on the thimble in any direction. The object in the graphics display is represented by a collection of polygons, usually in the range of 10,000 to 20,000 polygons. Figure 37 shows a user interacting with a graphic display where the Haptic device gives feedback based on the position of a small point called a proxy on the screen. In particular, whenever the proxy collides with a graphical object, a force is generated and the user's nger in the Haptic device is stopped from continuing penetration in that direction. In fact, the feedback is quite complex: the tactile interaction

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 123
includes contact constraints, surface shading, friction, and texture RKK97 . Such a system has wide-ranging application possibilities, from helping surgeons operate on patients to training pilots with ight simulation. This application is a good case study because there are some tasks which are poorly implemented in software, e.g. collision detection, which could potentially run much faster in hardware. The rst step towards integrating a hardware implementation of such a task into the system is to have a scheduler for the application.

Graphic Display

Haptic Interface

User Application

low level control proxy update

HL Library

model construction

CLIENT

SERVER

Figure 38: System Architecture

5.3.1 Original Design
The original design consists of an all software solution running on a Silicon Graphics Indigo SGI workstation and an IBM compatible PC. The SGI client contains the graphics routines which update the display, and the PC server runs the low level routines for controlling the Haptic device. Our system architecture can be seen in Figure 38.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 124 Collision Detection

proxy

face covered by hierarchical spheres

Figure 39: Sphere Characterization
From measurement, we observed that approximately 50 of the CPU time is spent in detecting when the proxy collides with an object in the graphics display. Collision detection is achieved by an algorithmic approach rst described in Qui94 . The basic idea is to take a polygonal surface and cover each polygon with a small sphere. Then, from this initial set of spheres, they are hierarchically covered. Figure 39 shows the beginnings of covering a face using this method the actual algorithm was written for three dimensions. At the end, we have a root sphere with covers the entire graphical object and all subspheres. The resulting tree data structure of hierarchical spheres has height Olg n. Since the collision detection algorithm checks the sphere hierarchy to see if collision has occured, Olg n checks are needed.
Timing Constraints
Standard solutions are used for the low level hardware interactions that might otherwise involve strict timing constraints. For writing torque values to the Haptic device,

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 125
we use a device driver; for reading in the joint positions, we utilize the same device driver to read values from the port.
Model information about the graphics objects and the proxy are communicated between the SGI workstation and the PC by sending and receiving packets using the TCP IP protocol. In the actual code on the PC, we never perform a blocking wait: instead, we check to see if a packet has arrived, and if so we accept the packet and continue.
The overriding timing constraint we have is a rate constraint: the tasks of the following section must complete before a hard real-time deadline is reached. Any delay in updating the torques could damage the Haptic device or the user.
Haptic Library
The original code for controlling the Haptic device was written in C. Some of the most time-consuming tasks, such as that of communicating the polygons composing the graphics objects and then building a sphere hierarchy, are performed during the initialization and sphere building phases. Once a particular graphical display is up and running, the following tasks are executed in each iteration of a core loop called the servo loop:
wait for next millisecond clock tick
write torques to Haptic device
read joint angles of Haptic device
convert joint angles to x,y,z coordinates
collision detect
calculate new proxy position based on collision or not

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 126
compute new torques for Haptic device
if ready, send receive network packets new proxy position, etc.
For example, consider a user interacting with a graphical display of a teapot. When the proxy is in space not near the teapot, the user can move the proxy freely. However, as soon as the proxy comes close to the teapot, penetrating the sphere hierarchy an example penetration in two dimensions is shown in Figure 39, collision detection is used to check if the user's proxy on the screen has hit the teapot. The Haptic device provides force-feedback control to simulate the interaction of the proxy with the graphical object, e.g. when sliding along the curved surface of the teapot. Figure 37 shows a user utilizing the proxy to push around a spaceship merry-go-round. An execution of the servo loop for controlling the robot must complete once every millisecond.
5.3.2 Haptic Control Implememted with Split Run-Time System
The new design contains a slightly altered scheduler for the servo loop. We divide the loop into tasks in order to control their execution from a hardware FSM. Before entering the loop, we kick o execution of the FSM. Within the loop, we execute tasks as directed by the FSM.
Task Execution
We divided the tasks of Section 5.3.1 into three coarse grained groupings as follows:
Phantom" routines:
wait for next millisecond clock tick

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 127
write torques to Haptic device read joint angles of Haptic device convert joint angles to x,y,z coordinates Proxy" routines: collision detect calculate new proxy position based on collision or not compute new torques for Haptic device Network" routines, executed only if there are network packets ready to send receive:
send new proxy position to graphics over network receive new graphics info over network
We implemented an FSM in hardware to sequence the above three course granularity software threads. For the sake of experimentation, we use an FPGA-based board the PCI Pamette Sha98  for the hardware implementation. This hardware
Verilog
Synopsys BC, DC, FPGA
Synopsys-Xilinx Netlist Format
Xilinx Placement and Route
Xilinx FPGA bitstream
Figure 40: Synopsys-Xilinx Tool Flow

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 128
FSM portion of the run-time scheduler is speci ed in Verilog HDL and synthesized using the Synopsys-Xilinx interface; the tool ow is shown in Figure 40. The Synopsys tools used are the Behavioral CompilerTM BC Kna96 , Design CompilerTM DC and FGPA CompilerTM FPGA.
Task execution is described in Section 3.2.1. Brie y, we associate a start and a done event with each software task thread. In software, we have a start vector and a done vector which encapsulate the start and done events for each software-task. Since there are less than 32 distinct software-tasks, each vector is contained in a single word with a simple one-hot encoding.

CPU

FSMstart FSMdone
done

start

FSM

PCI start done

Figure 41: Run-Time Scheduler Control Communication
The run-time scheduler hardware FSM, synthesized to implement the control- ow of task invocations, updates the start vector in software as follows. First, it updates a local register containing the start vector. Then the CPU reads in the new value on a polling loop. When a software-task is nished executing, it updates the done vector by writing the value out with memory mapped I O. Thus, the the done vector in the run-time scheduler in hardware is updated.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 129
Note that we wanted to be able to turn the hardware FSM on and o from software, since the system initialization is directed by software. Thus, we added FSMstart and FSMdone signals to kick o and terminate, respectively, FSM execution. Figure 41 shows the communication of the FSMstart, FSMdone, start and done vectors.
Therefore we split the run-time scheduler into two parts:
An executive manager in hardware with cycle-based semantics that can satisfy hard real-time constraints.
A polling scheduler that executes di erent threads based on eligible softwaretasks as indicated by the start vector.
The Haptic library code was altered to accommodate this new split. In particular, a polling scheduler was written as the inner core loop implementing the three coursegrained tasks as described here.
The original system in the Computer Science Robotics Lab at Stanford was successfully ported to the NT environment all in software. Then we successfully implemented the split run-time scheduler in the actual design.
5.3.3 System Architecture
Our system architecture consists of an SGI workstation for the graphics, a PC with a PentiumTM processor, and a Haptic device connected to the PC.
The PC has a PCI Pamette Sha98 board connected to one of its slots. The PCI Pamette, shown in Figure 42, has one FPGA dedicated to talking to the PC using the 32 or 64 bit PCI protocol, with four more Xilinx 4020E FPGAs con gurable by the user. The two 128KB SRAMs are essentially scratchpad memories which the nearest FPGA can use. Sixteen bits of memory can be written to or read from each SRAM every cycle.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 130

SRAM

Download / Readback

FPGA

P C

FPGA (PCI

I protocol)

FPGA

PMC or
Daughter Board

clocks

FPGA SRAM

FPGA DRAM SIMM sockets

Secondary Daughter Board Connector

Figure 42: PCI Pamette Version 1 Architecture
For communication with the FPGAs, we use the PCI protocol as implemented by the PCI Pamette software library for Visual C++ and the FPGA on the PCI Pamette. From the point of view of the software code, this appears as a memorymapped read or write. However, there are timing constraints which must be observed by the two FPGAs that can read data from the 32-bit bus coming out from the FPGA implementing the PCI protocol: once an address appears on the bus, the data corresponding to that address must be read in the following cycle. Similarly, for writing to the bus in which case the software is executing a read from memorymapped I O, the data read must be driven to the bus on the following cycle and held there for six cycles. There are many more constraints explained in the PCI documentation Sha98 .
In order to meet these exact timing constraints, we latch values going on o chip using DCTM and then read the values using behavioral Verilog synthesized in cycleaccurate mode with BCTM.

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 131
5.3.4 Software Generation
The software for programming and controlling the PCI Pamette is available for Microsoft Visual C++ 4.0TM with Windows NT 4.0TM or for the DEC Alpha. Because we wanted to use a PC, we utilized the NT version.
The original code called the Haptic library" for controlling the Haptic device was written in 10,000 lines of C for Linux. In order to use the Pamette, we ported the Haptic library to Visual C++ 4.0TM with Windows NT 4.0TM. This porting e ort included writing a device driver in NT to control the Haptic device as well as rewriting the network code for communication with the SGI workstation using TCP IP.
Reading and writing to the SRAM on the Pamette is accomplished using memorymapped I O and hardware-tasks in the FPGA. The PCI interface takes an average of 5 to 9 CPU clock cycles to communicate a single 32-bit read or write.
Therefore, given a particular value of the start vector, the appropriate softwaretasks can be executed. The scheduler for the software is a simple polling loop. Note that for this to work we have to guarantee that after indicating that a particular
Figure 43: Teapot Graphical Object With Proxy

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 132

software-task has completed by writing to the done vector, the next start value must be updated and ready to be read before the software polling loop next reads in the start vector. Otherwise, the software scheduler could read in the exact same start vector again and thus fail to meet the rate constraint of updating the robot's torque values every millisecond. We veri ed that the FSM implemented in the FPGA was fast enough by extensive simulation.
Figure 43 shows a graphical teapot model which we used to test the design. The proxy is shown on the teapot near the base of the spout. The teapot is composed of 3,416 triangular surfaces. The client computer was an SGI Indigo2 High Impact running IRIX 6.2 and the Haptic server was a PC with a 266 Mhz Pentium Pro running Windows NT 4.0. The PC has 32 MB of main memory and a 512KB cache. Communication between the two computers was done through a standard ethernet TCP IP connection. The Haptic device used was a ground based PHANToM manipulator with 3 degrees of freedom in it force-feedback.

Task Lines

C

wait for next millisecond clock tick

65

write torques to Haptic device

50

read joint angles of Haptic device

48

convert joint angles to x,y,z coordinates 428

collision detect

2189

calculate new proxy position

664

compute new torques for Haptic device 10

send receive information over network 1328

device driver

899

Table 11: Code space for software tasks.

Table 11 shows the code space used for the various software tasks in the inner servo loop. The nal executable took up 485KB of memory; however, the code and

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 133

Task Lines Style of

Verilog Verilog

ebusread1.v

146 behavioral

ebuswrite1.v

114 behavioral

generatecontrol.v

48 behavioral

haptic.v

242 structural

hapticcontrol.v

178 behavioral

startcontrol.v

150 behavioral

transactionmodelib.v 154 structural

writestart.v

99 behavioral

Table 12: Code space for hardware tasks.

data used in the servo loop is much less and likely t entirely in the 512KB cache on the PC however, we did not verify this. Table 12 shows the code space used for reading and writing data from to the bus and the SRAM, starting terminating the hardware FSM, and the hardware FSM itself in .hapticontrol.v Notice that the FSM takes only 178 lines of Verilog HDL, while the supporting Verilog HDL code takes 1195 lines. Table 13 shows the various measures of utilization provided for the Xilinx 4020E which implements the Verilog HDL code. The 4020E can t at most around 20K logic gates. We are currently using about half of the available CLBs.
5.3.5 Future Directions
For future work, an ASIC implementation of the collision detection algorithm would drastically speed up the application, especially since the sphere checking is quite naturally parallelizable. The run-time scheduler described here could quite easily be augmented with such an ASIC. In fact, the inclusion of multiple components in hardware could be easily added to the system. The major practical design cost would be the speci cation and design of the collision detection ASIC.
The PC-Pamette architecture described in the previous sections provides the basis

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 134

Xilinx No. Max. Percent

Measure

Used Avail. Used

Occupied CLBs

401 784 51

Bonded I O Pins

72 160 45

F and G Function Generators 494 1568 31

H Function Generators

93 784 11

CLB Flip Flops

217 1568 13

IOB Input Flip Flops

33 224 14

IOB Output Flip Flops

18 224 8

3-State Bu ers

0 1680 0

3-State Half Longlines

0 112 0

Edge Decode Inputs

0 336 0

Edge Decode Half Longlines 0 32 0

CLB Fast Carry Logic

8 784 1

Table 13: Statistics for Xilinx 4020E Mapping

for a modular extensible hardware-software run-time system. Since the hardware part of the run-time system is in FPGAs, it can be recon gured quickly with the synthesis path of Figure 40. Currently we only use one of the four available FPGAs. Portions of the real-time Haptic control system can be migrated to hardware, either into FPGAs, ASICs or DSPs. For example, an ASIC implementing the collision detection algorithm which has a lot of parallelism could be integrated quickly into the run-time system.
For the nal embedded application, the hardware part of the run-time system is synthesized into hardware rapidly since it is described in behavioral Verilog and uses synthesis all the way down to the bitstream for programming the XILINX 4020E FPGAs. For example, given a working protype, one could design a single chip implementation of the control system using a Pentium core, dedicated logic for the logic implemented in FPGAs in the prototype, and a core for the ASIC implementing the collision detection algorithm. In other words, given the Intellectual Property IP for each component used in the prototype, it is possible that the entire design could be

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 135
placed on the same single chip and fabricated.
5.4 Summary
In conclusion, we have shown two sample applications of a run-time scheduler split between hardware and software. The rst veri ed via simulation the results of using the Serra system on a robot arm controller for two PUMA arms. The second successfully implemented the split run-time scheduler approach on a real-time Haptic robot prototype.
Note the CAD requirements for hardware software co-design of both systems. First of all, we needed to satisfy a hard real-time relative scheduling constraint imposed by two of the hardware components in the PUMA control system: tasks xf1 and xb1 had a relative timing constraint of activating the start event for xb1 no less than 2 and no greater than 8 cycles after activating the start event for xf1. This constraint was always satis ed in any of the run-time systems generated by Serra. Satisfaction of this constraint was a sine qua non for exploring di erent hardware software partitions of the PUMA system, which occurred as we moved some tasks from hardware to software and vice-versa. Second, we altered the NEV ER sets as we moved tasks around between hardware and software. Serra's run time system, when it found a solution, always satis ed the mutual exclusion required by the speci ed NEV ER sets. Notice that the WCET values for the di erent tasks can be quite large; thus, the e ect of high-level decisions as to the hardware software partition as well as the amount of hardware dedicated to each hardware task the more hardware area allowed would typically result in a faster hardware task could be evaluated quickly and in an interactive fashion. Thus, the designer was able to optimize that design by exploring the design space much more e ectively than if the run-time system had to be regenerated by hand for each partition. Furthermore, the automatic satisfaction

CHAPTER 5. IMPLEMENTATION AND EXPERIMENTAL RESULTS 136
of speci ed timing constraints and resource constraints freed the designer from time consuming calculations that can be automated using the approach in this thesis.
The real-time Haptic robot prototype showed the feasibility of the run-time scheduler approach of this thesis in an actual working design. Where as the PUMA controller example explored the range of options a orded by the Serra system, such as the support for timing and resource constraints, the Haptic robot prototype focused on the practicality of the design style. Thus, the split run-time scheduler functioned perfectly well in the nal prototype.
In short, both the PUMA controller simulation and the Haptic robot prototype show the feasibility and utility of the design style and CAD tool for extensible runtime systems in hardware and software proposed in this thesis.

Chapter 6
Conclusions and Future Work
6.1 Summary
We considered in this thesis analysis and synthesis techniques for hardware software run-time systems. We assume a system speci cation at the task level of hardware and software, e.g. hardware modules and software threads, together with a main task that speci es the control and data ow among all of the speci ed tasks. We further assume a target architecture of a CPU core to run the software-tasks together with custom hardware implementing the hardware-tasks. Unlike previous approaches to run-time scheduling, we split our run-time scheduler between hardware and software, as opposed to placing the scheduler all in one or the other. Our analysis also takes into account the split hardware software implementation both of the scheduler and of the tasks.
Thus, we have presented a new system-level scheduling methodology and CAD tool for hardware software co-design. In particular, we focused on the following aspects of hardware software run-time system analysis and synthesis:
137

CHAPTER 6. CONCLUSIONS AND FUTURE WORK

138

Design Style for Scheduling. We presented a design style for synchronization and scheduling in hardware software co-design where we represent each hardware- and software-task by an automaton that begins execution upon receiving a start event and indicates that it has completed execution by emitting a done event. These start and done events are appropriately implemented in hardware and software.
Co-Synthesis of a Hardware-Software Run-Time Scheduler. We showed how, given the control- ow of the tasks in the system, a run-time scheduler can be synthesized in hardware and software. The hardware part of the run-time scheduler consists of an FSM in hardware sequencing the start and done events such that all speci ed relative timing constraints are met; relative timing constraint satisfaction is assured by construction in that we only generate an FSM if we will meet the relative timing constraints. The software part of the run-time scheduler is a small kernel consisting of an interrupt service routine, context switch code, and a static preemptive priority scheduler. The priority scheduler is parameterized to t the exact number of software-tasks in the system. This mixed implementation leverages advantages of hardware, such as predictable and exact ne-grained timing separation of start signals, and advantages of software, such as preemptibility and exibility.
Rate-Constraint Satisfaction Analysis. We presented several scheduling algorithms for tasks in hardware and software under a hard real-time rate constraint where the precedence constraints among tasks are speci ed in a DAG. In e ect, these techniques extend Worst-Case Execution Time analysis from the pure software and pure hardware domains to a mixed hardware software implementation domain. Furthermore, with our small, custom kernel for the software

CHAPTER 6. CONCLUSIONS AND FUTURE WORK

139

operating system, we can analyze rate-constraint satisfaction in cases where task preemption may occur.
Resource Constraints. The scheduling style and algorithms support a NEV ER set of software-tasks implemented on the same CPU and multiple NEV ER sets of hardware-tasks. While the run-time scheduler can trivially maintain mutual exclusion among tasks in the same NEV ER set by the addition of precedence constraints, rate-constraint analysis gives a set of precedence constraints to add to the scheduler which yields the smallest WCET for the system among the task orders considered.
Application to Robotics. We presented a full analysis, synthesis, and simulation of a hardware software implementation of a system for controlling two PUMA robot arms. We also described a small prototype of a split hardware software run-time scheduler to control a force-feedback Haptic robot using a PentiumTM for the software and Xilinx FPGAs for the hardware.
The approach of this thesis allows the exploration of the design space of a runtime scheduler across the boundary of hardware and software. The designer gains the advantage of predictable satisfaction of timing constraints for hardware software systems within a framework that enables di erent hardware software partitions to be quickly evaluated. Thus, in relation to previous work in hardware software partitioning, a partitioning tool could easily sit on top of Serra which would generate run-time systems for di erent hardware software partitions chosen for evaluation. In addition, Serra's more e cient design space exploration can improve time-to-market for a product, enabling the product to enter the market sooner.

CHAPTER 6. CONCLUSIONS AND FUTURE WORK

140

6.2 Future Work
During the development of this work, we observed that several lines of research which promise signi cant impact in the coming years.
Hardware software co-design of a modular Real Time Operating System in hardware and software. With so much space available on the chip, future system-on-a-chip designs will provide an opportunity to rede ne the goals of RTOS research for embedded systems. In particular, with the high demands placed on tomorrow's dedicated applications, there is a role for an RTOS where hardware is used to both speed up critical bottlenecks and more e ectively manage concurrency among hardware and software. Such a modular RTOS would include providing functionality for updating and debugging the system, functionality which is modular" and thus can be thrown away when the real application is to be run at the highest possible performance.
Recon gurable architectures for application-speci c system-on-a-chip designs. For a set of applications, a system-on-a-chip design can be tuned to maximize performance for the speci c applications desired. Recon gurable interfaces among tasks as well as recon gurable scheduling can help obtain the best results. A design style with associated CAD tools for such application-speci c recon gurable architectures can key signi cant improvements in design space exploration, time-to-market, and even testability of the nal chip.
Hardware software co-design of recon gurable run-time systems with multiple processors. If the FSM part of the run-time scheduler presented in this thesis were implemented in recon gurable logic, for example with SRAM-based FPGA technology, then the hardware part of the scheduler would be recon gurable.

CHAPTER 6. CONCLUSIONS AND FUTURE WORK

141

The software part is already recon gurable by de nition. With multiple processors available, e.g. a DSP core and a microcontroller core, the design space to be explored is quite signi cant. Greatly needed are CAD tools to allow designers to e ciently explore this wide design space.

Abbreviations and Symbols
2 Symbol marking the end of an example. Symbol marking the end of a proof.
AMPL A Mathematical Programming Language ASIC Application Speci c Integrated Circuit BCTM Synopsys Behavioral CompilerTM BCET Best-Case Execution Time BDD Binary Design Diagram CAD Computer-Aided Design CB Communication Blocks CFSM Co-design Finite State Machine CDFG Control-Data Flow Graph CPU Central Processing Unit DAG Directed Acyclic Graph DFL Data Flow Language
142

CHAPTER 6. CONCLUSIONS AND FUTURE WORK
DCTM Synopsys Design CompilerTM DMA Direct Memory Access DSP Digital Signal Processing FCFS First-Come-First-Served FPGA Field-Programmable Gate Array FSM Finite-State Machine HDL Hardware Description Language IBC Inter-Block Communication IC Integrated Circuit ILP Integer Linear Program I O Input Output IP Intellectual Property LCM Least Common Multiple NSDS Never Set DAG Scheduling PCI Peripheral Component Interconnect bus for ASIC designers RAM Random Access Memory RMA Rate Monotonic Analysis RPC Remote Procedure Call RTL Register-Transfer-Level

143

CHAPTER 6. CONCLUSIONS AND FUTURE WORK
RTOS Real-Time Operating System SoC System-on-a-Chip SRAM Static Random Access Memory SRTD Sequencing with Release Times and Deadlines VHDL VHSIC Hardware Description Language VHSIC Very High Speed Integrated Circuit WCET Worst-Case Execution Time

144

Bibliography

ABD+95 N. Audsley, A. Burns, R. Davis, K. Tindell, and A. J. Wellings. Fixed Priority Pre-emptive scheduling: A Historical Perspective". Real-Time Systems, 8:173 198, 1995.

ABR+93 N. Audsley, A. Burns, M. Richardson, K. Tindell, and A. J. Wellings. Applying new scheduling theory to static priority pre-emptive scheduling". Software Engineering Journal, pages 284 292, 1993.

ACJ96

M. Abid, A. Changuel, and A. Jerraya. Exploration of Hardware Software Design Space through a Codesign of Robot Arm Controller". In Proceedings of the European Design Automation Conference, pages 42 47, September 1996.

AFLS96

J. Adomat, J. Furunas, L. Lindh, and J. Starner. Real-Time Kernel in Hardware RTU: A Step Towards Deterministic and High Performance Real-Time Systems". In Real-Time Workshop, June 1996.

AKB86

B. Armstrong, O. Khatib, and J. Burdick. The explicit model and inertial parameters of the PUMA 560 arm". Proceedings of IEEE International Conference on Robotics and Animation, 1:510 518, 1986.

AM98 A. Morawiec, President. European CAD Standardization Initiative. http: www.vsi.org , 1998.

145

BIBLIOGRAPHY

146

AT95 J. Adams and D. Thomas. Multiple-Process Behavioral Synthesis for Mixed Hardware-Software Systems". In Proceedings of the International Symposium on System Synthesis, pages 10 15, September 1995.

BCG+97

F. Balarin, M. Chiodo, P. Giusto, H. Hsieh, A. Jurecska, L. Lavagno, C. Passerone, A. Sangiovanni-Vincentelli, E. Sentovich, K. Suzuki, and B. Tabbara. Hardware-Software Co-Design of Embedded Systems: The POLIS Approach. Kluwer Academic Publishers, 1997.

BCO96

G. Borriello, P. Chou, and Ross B. Ortega. Embedded System CoDesign: Towards Portability and Rapid Integration". In G. De Micheli and M. Sami, editors, Hardware Software Co-Design, pages 243 264. Kluwer Academic Publishers, 1996.

Ber96 G. Berry. See http: zenon.inria.fr meije esterel , 1996.

BG G. Berry and G. Gonthier. The Esterel Synchronous Programming Language: Design, Semantics, Implementation". Ecole Nationale Superieure des Mines de Paris and Institut National de Recherche en Informatique et Automatique.

BHLM94

J. T. Buck, S. Ha, E. A. Lee, and D. G. Messerschmitt. Ptolemy: A Framework for Simulating and Prototyping Heterogeneous Systems". Int. Journal of Computer Simulation, special issue on Simulation Software Development", 4:155 182, April 1994. Also available from http: ptolemy.eecs.berkeley.edu.

BS91 F. Boussinot and R. De Simone. The ESTEREL Language". Proceedings of the IEEE, 799:1293 1303, September 1991.

BIBLIOGRAPHY

147

CB94 CM96 CM97
COB92
COB95 Coe96 Cow98 CWB94

P. Chou and G. Borriello. Software Scheduling in the Co-Synthesis of Reactive Real-Time Systems". In Proceedings of the 31stDesign Automation Conference, pages 1 4, June 1994.
C. N. Coelho Jr. and G. De Micheli. Analysis and Synthesis of Concurrent Digital Circuits Using Control-Flow Expressions". IEEE Transactions on CAD ICAS, 158:854 876, August 1996.
C. N. Coelho Jr. and G. De Micheli. Modeling and Synthesis of Synchronous System-Level Speci cations". In J. Berge, O. Levia, and J. Rouillard, editors, Models in System Design, pages 243 264. Kluwer Academic Publishers, 1997.
Pai Chou, Ross Ortega, and Gaetano Borriello. Synthesis of the Hardware Software Interface in Microcontroller-Based Systems". In Proceedings of the International Conference on Computer-Aided Design, pages 488 495, Santa Clara, November 1992.
P. Chou, Ross B. Ortega, and G. Borriello. The Chinook Hardware Software Co-Synthesis System". In Proceedings of the International Symposium on System Synthesis, pages 22 27, September 1995.
C. N. Coelho Jr. Analysis and Synthesis of Concurrent Digital Systems Using Control-Flow Expressions", March 1996. CSL-TR-96-690.
CoWare touts `interface synthesis' for codesign. EE Times, page 54, February 1998.
P. Chou, E. Walkup, and G. Borriello. Scheduling for Reactive RealTime Systems". IEEE Micro, August 1994.

BIBLIOGRAPHY

148

DF98 D. Fairbairn, President. Virtual Sockets Interface Alliance. http: www.vsi.org , 1998.

DJ98 B. Dave and N. Jha. CASPER: Concurrent Hardware-Software Cosynthesis of Hard Real-Time Aperiodic and Periodic Speci cations of Embedded System Architectures". In Proceedings of the Design, Automation and Test in Europe, pages 118 124, February 1998.

DKMT90 G. DeMicheli, D. C. Ku, F. Mailhot, and T. Truong. The Olympus Synthesis System for Digital Design". IEEE Design and Test, pages 37 53, October 1990.

DLJ97

B. Dave, G. Lakshminarayana, and N. Jha. COSYN: HardwareSoftware Co-synthesis of Embedded Systems". In Proceedings of the Design Automation Conference, pages 703 708, June 1997.

EHB+96 R. Ernst, J. Henkel, Th. Benner, W. Ye, U. Holtmann, D. Herrmann, and M. Trawny. The COSYMA environment for hardware software cosynthesis of small embedded systems". IEEE Micro, 20:159 166, 1996.

EKP+98

P. Eles, K. Kucheinski, Z. Peng, A. Doboli, and P. Pop. Scheduling of Conditional Process Graphs for the Synthesis of Embedded Systems". In Proceedings of the Design, Automation and Test in Europe, pages 132 138, February 1998.

EY97 R. Ernst and W. Ye. Embedded program timing analysis based on path clustering and architecture classi cation". In Proceedings of the International Conference on Computer-Aided Design, pages 598 604, Santa Clara, CA, November 1997.

BIBLIOGRAPHY

149

FGK93 GJ79 Gup95 HB97
HE95
HE96
HL95 HWS95
KD90

R. Fourer, D. Gay, and B. Kernighan. AMPL: A Modeling Language for Mathematical Programming. The Scienti c Press, 1993.
M. Garey and D. Johnson. Computers and Intractability. W. Freeman and Company, 1979.
R. Gupta. Co-Synthesis of Hardware and Software for Digital Embedded Systems. Kluwer Academic Publishers, 1995.
Ken Hines and Gaetano Borriello. Optimizing Communication in Embedded System Co-simulation". In International Workshop on Hardware Software Co-Design, pages 121 125, 1997.
J. Henkel and R. Ernst. A Path-Based Technique for Estimating Hardware Runtime in HW SW-Cosynthesis". In Proceedings of the International Symposium on System Synthesis, pages 116 121, September 1995.
J. Henkel and R. Ernst. The Interplay of Run-Time Estimation and Granularity in HW SW Partitioning". In International Workshop on Hardware Software Co-Design, 1996.
F. Hillier and G. Lieberman. Introduction to Operations Research. McGraw-Hill, 1995.
M. Humphrey, G. Wallace, and J. Stankovic. Kernel-Level Threads for Dynamic, Hard Real-Time Environment". In Proceedings of the Real Time Systems Symposium, pages 38 48, 1995.
D. C. Ku and G. DeMicheli. HardwareC - a language for hardware design version 2.0. CSL Technical Report CSL-TR-90-419, Stanford, April 1990.

BIBLIOGRAPHY

150

KM92

D. Ku and G. De Micheli. High-level Synthesis of ASICs under Timing and and Synchronization Constraints. Kluwer Academic Publishers, 1992.

Kna96 D. Knapp. Behavioral Synthesis: Digital System Design Using the Synopsys Behavioral Compiler. Prentice-Hall, 1996.

Lat91 J. C. Latombe. Robot Motion Planning. Kluwer Academic Publishers, 1991.

Lin92 L. Lindh. Idea of FASTHARD - A Fast Time Deterministic Hardware Based Real-Time Kernel". In Real-Time Workshop, June 1992.

LL73 C. Liu and J. Layland. Scheduling algorithms for multiprogramming in a hard-real time environment". Journal of the ACM, 201:46 61, 1973.

LM95 Y. Li and S. Malik. Performance Estimation of Embedded Software with Instruction Cache Modeling". In Proceedings of the International Conference on Computer-Aided Design, pages 380 387, Santa Clara, CA, November 1995.

LS91 L. Lindh and F. Stanischewski. FASTCHART Idea and Implementation". In Proceedings of the International Conference on Computer Design, pages 401 404, 1991.

LSF95

L. Lindh, J. Starner, and J. Furunas. From Single to Multiprocessor Real-Time Kernels in Hardware". In Real-Time Technology and Applications Symposium, May 1995.

MBL+96 H. De Man, I. Bolsens, B. Lin, K. Van Rompaey, S. Vercauteren, and D. Verkest. Co-design of DSP Systems". In G. De Micheli and M. Sami,

BIBLIOGRAPHY

151

editors, Hardware Software Co-Design, pages 75 104. Kluwer Academic Publishers, 1996.

MBL+97 H. De Man, I. Bolsens, B. Lin, K. Van Rompaey, S. Vercauteren, and D. Verkest. Hardware Software Co-Design of Digital Telecommunication Systems". Proceedings of the IEEE, 853:391 418, March 1997.

MCSM96 V. J. Mooney III, C. N. Coelho Jr., T. Sakamoto, and G. De Micheli. Synthesis from mixed speci cations. In Proceedings of the European Design Automation Conference, pages 114 119, September 1996.

Mic94

G. De Micheli. Synthesis and Optimization of Digital Circuits. McGrawHill, 1994.

Mic97 G. De Micheli. Special Issue on Hardware Software Co-Design. In Proceedings of the IEEE, March 1997.

MS96 G. De Micheli and M. Sami. Hardware Software Co-Design. Kluwer Academic Publishers, 1996.

MSM97

V. J. Mooney III, T. Sakamoto, and G. De Micheli. Run-Time Scheduler Synthesis For Hardware-Software Systems and Application to Robot Control Design". In International Workshop on Hardware Software CoDesign, pages 95 99, March 1997.

MWWL96 S. Malik, W. Wolf, A. Wolf, and Y. Li. Performance Analysis of Embedded Systems". In G. De Micheli and M. Sami, editors, Hardware Software Co-Design, pages 45 74. Kluwer Academic Publishers, 1996.

NVG92 S. Narayan, F. Vahid, and D. Gaski. System Speci cation with the SpecCharts Language". IEEE Design & Test of Computers, pages 6 13,

BIBLIOGRAPHY

152

December 1992.

OB97 Ross Ortega and Gaetano Borriello. Communication Synthesis for Embedded Systems with Global Considerations". In International Workshop on Hardware Software Co-Design, pages 69 73, 1997.

OBE+97 A. Osterling, T. Benner, R. Ernst, D. Herrmann, T. Scholz, and W. Ye. The Cosyma System". In Hardware Software Co-Design: Principles and Practice. Kluwer Academic Publishers, 1997.

PS89 D. Peng and K. Shin. Static Allocation of Periodic Tasks with Precedence Constraints in Distributed Real-Time Systems". In International Conference on Distributed Computing Systems, pages 190 198, 1989.

Qui94 S. Quinlan. E cient Distance Computation between Non-Convex Objects". International Conference on Robotics and Automation, pages 3324 3329, 1994.

Raj91 R. Rajkumar. Synchronization in Real-Time Systems: A Priority Inheritance Approach". Kluwer Academic Publishers, 1991.

Ram90

K. Ramamritham. Allocation and Scheduling of Complex Periodic Tasks". In International Conference on Distributed Computing Systems, pages 108 115, 1990.

Ram95

K. Ramamritham. Allocation and Scheduling of Precedence-Related Periodic Tasks". IEEE Proceedings on Parallel and Distributed Systems, 64:412 420, April 1995.

RK98 Ryo Koyama, Chairman, Board of Directors. Reuseable ApplicationSpeci c Intellectual Property Developers. http: www.rapid.org , 1998.

BIBLIOGRAPHY

153

RKK97

D. Ruspini, K. Kolarov, and O. Khatib. The Haptic Display of Complex Graphical Environments". Proceedings of SIGGRAPH, pages 345 352, August 1997.

RVBM96

Karl Van Rompaey, Diederik Verkest, Ivo Bolsens, and Hugo De Man. CoWare A design environment for heterogeneous hardware software systems". In Proceedings of the European Design Automation Conference, pages 252 257, September 1996.

Sha98 M. Shand. PCI Pamette V1. Digital Equipment Corporation, System Research Center, http: www.research.digital.com SRC pamette , 1998.

SRL90

L. Sha, R. Rajkumar, and J. P. Lehoczky. Priority Inheritance Protocols: An Approach to Real-Time Synchronizations". IEEE Transactions on Computers, pages 1175 1185, December 1990.

SRS94

L. Sha, R. Rajkumar, and S. Sathaye. Generalized rate monotonic scheduling theory: a framework for developing real-time systems". Proceedings of the IEEE, 821:68 82, January 1994.

SSM+92

E. Sentovich, K. Singh, C. Moon, H. Savoj, R. Brayton, and A. Sangiovanni-Vincentelli. Sequential circuits design using synthesis and optimization. In Proceedings of the International Conference on Computer Design, pages 328 333, Cambridge, MA, 1992.

SSNB95

J. Stankovic, M. Spuri, M. Di Natale, and G. Buttazzo. Implications of classical scheduling results for real-time systems. IEEE Computer, pages 16 47, June 1995.

Uni84 Unimation. Unimate PUMA Mark II Robot: 500 Series Equipment and Programming Manual 398P1". pages 1 36, April 1984.

BIBLIOGRAPHY

154

VLM96a

S. Vercauteren, B. Lin, and H. De Man. Constructing ApplicationSpeci c Heterogeneous Embedded Architectures from Custom HW SW Applications". In Proceedings of the Design Automation Conference, pages 521 526, June 1996.

VLM96b S. Vercauteren, B. Lin, and H. De Man. Embedded Architecture CoSynthesis and System Integration". In International Workshop on Hardware Software Co-Design, 1996.

VRBM96 D. Verkest, K. Van Rompaey, I. Bolsens, and H. De Man. CoWare A Design Environment for Heterogeneous Hardware Software Systems". Design Automation of Embedded Systems, 14:357 386, October 1996.

WDC+94 P. Willekens, D. Devisch, M. Van Canneyt, P. Con itti, and D. Genin. Algorithm Speci cation in DSP Station using Data Flow Language". DSP Applications, pages 8 16, January 1994.

YW95

T.-Y. Yen and W. Wolf. Performance Estimation for Real-Time Distributed Embedded Systems". In Proceedings of the International Conference on Computer Design, pages 64 69, 1995.

YW96 T.-Y. Yen and W. Wolf. Hardware-Software Co-Synthesis of Distributed Embedded Systems. Kluwer Academic Publishers, 1996.

Appendix A
A Mathematical Program Formulation

In this section we show a mathematical program formulation for optimal scheduling of hardware and software tasks in a DAG with a single NEV ER set. Speci cally, we show the formulation as implemented in the AMPL modeling language FGK93 . We

set DAG := src a b c d e f snk; set N := b c d;  set of Predecessors set P := src,a src,b src,c a,d b,e d,f c,snk e,snk f,snk;

param: src a b c d e f snk

WCET := 0 5000 3000 20000 15000 5000 11000 0;

Figure 44: AMPL data for dagopt problem.

155

APPENDIX A. A MATHEMATICAL PROGRAM FORMULATION

156

then compare the AMPL solution to our solution using the Constructive Heuristic Scheduling of Section 4.2.
For our example, we use the DAG of Figure 16 and refer to it as the dagopt problem. For our mathematical model in AMPL shown in Figure 45, we de ne the set DAG to contain the nodes shown in Figure 16, including the source and the sink. The set N contains the tasks in the same NEV ER set; in this example we have a
single NEV ER set. The set P contains the set of predecessors: x; y 2 P indicates

set DAG; set N; set P within DAG cross DAG;

 nodes in Directed Acyclic Graph  NEVER set  set of predecessors, starting with the src

param WCET DAG = 0;

 WCET for each node in DAG

param MAXTIME = 0 default 1000000;

var x i in N, j in N binary;

 = 1 if task j is processed before i, 0 o.w.

var starttime DAG = 0;

 time when each node starts

var sinkendtime = 0;

minimize max cost: sinkendtime;

subject to sinkendtime def i in DAG: sinkendtime = starttime i + WCET i ;

subject to Precedence s i,j in P: starttime j = starttime i + WCET i ;

subject to Mutual exclusion i in N, j in N: i != j: starttime j + WCET j = starttime i + 1 - x j,i *MAXTIME;

subject to Mutual exclusion2 i in N, j in N: i != j: starttime j + x j,i *MAXTIME = starttime i + WCET i ;

subject to same x2 i in N: x i,i = 1;

Figure 45: AMPL model for dagopt problem.

APPENDIX A. A MATHEMATICAL PROGRAM FORMULATION

157

that x is a predecessor of y in the DAG. With each element of the set DAG, we associate a WCET in the set WCET, which is de ned over the elements of DAG. The binary decision variable x decides the order of tasks in the NEV ER set, set N. Constraint Precedence s makes sure that no task starts until all of its predecessors have completed execution. Constraints Mutual exclusion and Mutual exclusion2
make sure that for two tasks i and j in set N, i 6= j, either task i nishes execution before task j, or task j nishes execution before task i. The MAXTIME constant
indicates that above MAXTIME cycles, we do not need to look anymore, because our real-time constraint is violated. The use of the MAXTIME constant enables the formulation of linear constraints in the AMPL model. The data corresponding to this model is shown in Figure 44.
We solve the problem in AMPL using the solver CPLEX FGK93 . CPLEX uses an exact solution method to nd the optimal schedule, which results in a WCET of 40,000 cycles. The solution found with Constructive Heuristic Scheduling, however, nds a suboptimal schedule resulting in a WCET of 43,000 cycles, exactly as described in Figure 16.

Example WCET w

ET w WCET w

ET w

heuristic heuristic AMPL

AMPL

dagopt

43,000 0.003 seconds 40,000 0.03 seconds

dagopt2 81,000 0.010 seconds 76,000 1.6 seconds

dagopt3 119,000 0.040 seconds 114,000 24 minutes

dagopt4 157,000 0.110 seconds

| one day

dagopt5 195,000 0.280 seconds

| out of memory

Table 14: WCET found and run times for Constructive Heuristic Scheduling versus AMPL.

This base example, which we label dagopt in Table 14, has been extended to four additional examples simply by doubling, tripling, quadrupling, and quintupling the

APPENDIX A. A MATHEMATICAL PROGRAM FORMULATION

158

number of tasks in dagopt the examples are named ,dagopt2 ,dagopt3 dagopt4 and ,dagopt5 respectively. For example, dagopt2 is shown in Figure 46. We nd, as expected, that the polynomial algorithm of the Constructive Heuristic Scheduling performs faster than the exact algorithm of CPLEX. Furthermore, in dagopt4 and ,dagopt5 which have 26 and 32 nodes, respectively, we nd that CPLEX does not even arrive at a solution after one day. The resulting WCET's, each with a corresponding ET Execution Time, are shown in Table 14. The Constructive Heuristic Scheduling algorithm was run on a Silicon Graphics INDY 4400 at 200 MHz with 64 MBytes of RAM. The AMPL program was executed on a Sun SPARCstation 20 at 150 MHz with 64 MBytes of RAM.

src NEVER = {b,c,d,b1,c1,d1}

b b1

a a1

c c1 d

d1

e e1

f f1

task ----
a b c d e f a1 b1 c1 d1 e1 f1

wcet (cycles) -----
5,000 3,000 20,000 15,000 5,000 11,000 5,000 3,000 20,000 15,000 5,000 11,000

sink

Figure 46: The dagopt2 problem, generated from the dagopt problem Figure 16 by doubling the number of tasks.

Of course, AMPL CPLEX can solve an extremely wide range of optimization problems exactly, whereas the Constructive Heuristic Scheduling algorithm is targeted to a speci c problem.

