Learning Sparse Topographic Representations with Products of Student-t Distributions

Max Welling and Geoffrey Hinton Department of Computer Science University of Toronto 10 King's College Road   Toronto, M5S 3G5 Canada welling,hinton¡ @cs.toronto.edu

Simon Osindero Gatsby Unit
University College London 17 Queen Square
London WC1N 3AR, UK simon@gatsby.ucl.ac.uk

Abstract
We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Studentt distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the "iterated Wiener filter" for the purpose of denoising images.
1 Introduction
Historically, two different classes of statistical model have been used for natural images. "Energy-based" models assign to each image a global energy, ¢ , that is the sum of a number of local contributions and they define the probability of an image to be proportional to ¥£ ¤§¦©¨ ¢ . This class of models includes Markov Random Fields where combinations of nearby pixel values contribute local energies, Boltzmann Machines in which binary pixels are augmented with binary hidden variables that learn to model higher-order statistical interactions and Maximum Entropy methods which learn the appropriate magnitudes for the energy contributions of heuristically derived features [5] [9]. It is difficult to perform maximum likelihood fitting on most energy-based models because of the normalization term (the partition function) that is required to convert £ ¤§¦©¨ ¢ to a probability. The normalization term is a sum over all possible images and its derivative w.r.t. the parameters is required for maximum likelihood fitting. The usual approach is to approximate this derivative by using Markov Chain Monte Carlo (MCMC) to sample from the model, but the large number of iterations required to reach equilibrium makes learning very slow.
The other class of model uses a "causal" directed acyclic graph in which the lowest level nodes correspond to pixels and the probability distribution at a node (in the absence of any observations) depends only on its parents. When the graph is singly or very sparsely

connected there are efficient algorithms for maximum likelihood fitting but if nodes have many parents, it is hard to perform maximum likelihood fitting because this requires the intractable posterior distribution over non-leaf nodes given the pixel values.
There is much debate about which class of model is the most appropriate for natural images. Is a particular image best characterized by the states of some hidden variables in a causal generative model? Or is it best characterized by its peculiarities i.e. by saying which of a very large set of normally satisfied constraints are violated? In this paper we treat violations of constraints as contributions to a global energy and we show how to learn a large set of constraints each of which is normally satisfied fairly accurately but occasionally violated by a lot. The ability to learn efficiently without ever having to generate equilibrium samples from the model and without having to confront the intractable partition function removes a major obstacle to the use of energy-based models.

2 The Product of Student-t Model

Products of Experts (PoE) are a restricted class of energy-based model [1]. The distribution

represented by a PoE is simply the normalized product of all the distributions represented

by the individual "experts":

  ¢¨ ¡ ¤£

¥

§ ¨

¦

© ¨ ¡   © 

©  

(1)

where

© ¨¡  

©

¦
are un-normalized experts and

denotes the overall normalization constant.

In the p roduct of Student-t (PoT) model, un-normalized experts have the following form,

¥

© ¨¢¡  £


¨ !¥

 "

¢¨ #%©$

¡

"
 ' )& (

©2143 0

(2)

where # © is called a filter and is the 5 -th column in the filter-matrix # . When properly normalized, this represents a Student-t distribution over the filtered random variable 6 © £ # ©$ ¡ . An important feature of the Student-t distribution is its heavy tails, which makes it a

suitable candidate for modelling constraints of the kind that are found in images.

 
Defining

¨¢7 £

£ §¤ ©¦ ¨ ¢

¨ 7

¦
 '8

, the energy of the PoT model becomes

¢ ¨ 7 9 £

§
@ )© ACEB D ¨ ¥!
©  0

¥ F

¨¢#

$©

¡

"


(3)

Viewed this way, the model takes the form of a maximum entropy distribution with weights
© on real-valued "features" of the image. Unlike previous maximum entropy models, h0 owever, we can fit both the weights and the features at the same time.

When the number of input dimensions is equal to the number of experts, the normally in-

tractable partition function becomes a determinant and the PoT model becomes equivalent

to a noiseless ICA model the inverse filters HG £ P# I



with will

Student-t represent

prior distributions [2]. independent directions

In in

that input

case the rows of space. So noise-

less ICA can be viewed as an energy-based model even though it is usually interpeted as

a causal generative model in which the posterior over the hidden variables collapses to a

point. However, when we consider more experts than input dimensions (i.e. an overcom-

plete representation), the energy-based view and the causal generative view lead to different

generalizations of ICA. The natural causal generalization retains the independence of the

hidden variables in the prior by assuming independent sources. In contrast, the PoT model

simply multiplies together more experts than input dimensions and re-normalizes to get the

total probability.

3 Training the PoT Model with Contrastive Divergence

When training energy-based models we need to shape the energy function so that observed

images have low energy and empty regions in the space of all possible images have high

energy.

The

maximum

l ik¢el¡ iho od¥£ l¤ ¤e¢a§rn©¦ in¨g  r ule

i£¥s ¤given by, ¤ ¢§ ©¦   "!""#%$ "'&

(4)

It is the second term which causes learning to be slow and noisy because it is usually

necessary to use MCMC to compute the average over the equilibrium distribution. A much

more efficient way to fit the model is to use the data distribution itself to initialize a Markov

Chain which then starts moving towards the model's equilibrium distribution. After just a

few steps, we observe how the chain is diverging from the data and adjust the parameters

to counteract this divergence. This is done by lowering the energy of the data and raising

the

energy

of

the

"con fa¥b¡ula tio£ ns¤ ¤ "¢ (pr¦©o¨d) uc ed

b£ y

¤

a
¤

¢

few
( ¦10

steps
I32 5 4

of MCMC.
2 6 7& 48!9 2

(5)

It can be shown that the above update rule approximately minimizes a new objective function called the contrastive divergence [1].

As it stands the learning rule will be inefficient if the Markov Chain mixes slowly because

the two terms in equation 5 will almost cancel each other out. To speed up learning we need

a Markov chain that mixes rapidly so that the confabulations will be some distance away

from the data. Rapid mixing can be achieved by alternately Gibbs sampling a set of hidden

variables given the random variables under consideration and vice versa. Fortunately, the

PoT model can be equipped with a number of hidden random variables equal to the number

of experts as follows,
 

¢¨ A¡ 5@ B



D¡ C 7I EG(IF H8PQ R ( S U T

VP XS W a(Y ` b V b T

S I

& ( bdc egf R (ih

(6)

BIntegrating over the variables results in the density of the PoT model, i.e. eqns. (1) and

(2). Moreover, the conditional distributions are easy to identify and sample from, namely

pB  ¨ 9 ¡  B  ¨ ¡ 

£ £

§
rq R ts vu¨ © ¥
©   ( 0
x `y 3¥u ¨ # # $  I

¥
¨#
F
 

©$

¡

"w

 £ p 

Q

©h

(7) (8)

xwhere denotes a Gamma distribution and a normal distribution. From (8) we see that q B the variables can be interpreted as precision variables in the transformed space £ # $ ¡ .

In this respect our model resembles a "Gaussian scale mixture" (GSM) [8] which also

multiplies a positive scaling variable with a normal variate. But GSM is a causal model

while PoT is energy-based.

The (in)dependency relations between the variables in a PoT model are depicted graphically in figure (1a,b). The hidden variables are independent given ¡ , which allows them to be Gibbs-sampled in parallel. This resembles the way in which brief Gibbs sampling is used to fit binary "Restricted Boltzmann Machines" [1].

To learn the parameters of the PoT model we thus propose to iterate the following steps:
B   1) Sample given the data for every data-vector according to the Gamma-
distribution (7).

u
x
(a)

u

(JTx)2  ¡¡  x

£¢£¢
J

(b)

u (JTx)2¥¤¤¥
x

W¦§§¦ J

(c)

1 2 3 4 5

(d)

Figure 1: (a)- Undirected graph for the PoT model. (b)-Expanded graph where the deterministic

relation (dashed lines) between the random variable ¨

©  
and the activities of the filters

¨ 

is made

explicit. (c)-Graph for the PoT model including weights  . (d)-Filters with large (decreasing from

left to right) weights into a particular top level unit ! . Top level units have learned to connect to

filters similar in frequency, location and orientation.

  B  2) Sample reconstructions of the data ¡ given the sampled values of for every
data-vector according to the Normal distribution (8).

3) Update the parameters according to (5) where the "k-step samples" are now given

 by A@by

the
£

re  c# o#n" st¡ r.uctions

¡

, the energy is given by (3), and the parameters are given

4 Overcomplete Representations

The above learning rules are still valid for overcomplete representations. However, step-2 of the learning algorithm is much more efficient when the inverse of the filter matrix #

exists. In that case we simply draw $ standard normal random numbers (with $ the num-

  VPber of data-vectors) and multiply each of them with ¤# I $

I
. This is efficient because

  VPI
the data dependent matrix

is diagonal while the costly inverse # I $ is data indepen-

dent. In contrast, for the overcomplete case we ought to perform a Cholesky factorization
on # # $ for each data-vector separately. We have, however, obtained good results by

proceeding as in the complete case and replacing the inverse of the filter matrix with its

pseudo-inverse.

From norm

experiments of the filters,

we
# ©$

have
#© £

also
'¥ &

5

found that in the overcomplete case we should fix , in order to prevent some of them from decaying

the % " to zero.

This operation is done after every step of learning. Since controlling the norm removes the

ability of the experts to adapt to scale it is necessary to whiten the data first.

4.1 Experiment: Overcomplete Representations for Natural Images

@We randomly generated ( 3 )3 3)3 patches of ¥ 03 ) ¥ 3 pixels from images of natural scenes1.

The patches were centered and sphered using PCA and the DC component (eigen-vector

@with largest variance) was removed. The algorithm for overcomplete representations using

the
( E3 3
a%

"pt-sinemouerdmsoo-oivnfevr¥ec.rosmAe wpslmaestaeul.lseWwdeetiogfihxtrteaddinetch( ae3 ywt2¥ ee14ri3mghetaxsnptdeoraths,mavi.oeem.0 ae© nrte£ upmr¥estaeenrnmdtattwhioeenrtehtheianfitcillstuedmrseodtroeinhthatahvnee

gradient updates of the filters. The learning rate was set so that initially the change in the filters was approximately 3!5 3)3 ¥ . In figure (2a) we show a small subset of the inverse-filters given by the pseudo-inverse of # 7$ 698A@7B , where C6 A8 7@ B is the ¥ )3 D3 F) GE E matrix used for

sphering the data.

1Collected from http://www.cis.hut.fi/projects/ica/data/images

5 Topographically Ordered Features

In [6] it was shown that linear filtering of natural images is not enough to remove all higher

order dependen" cies. the activities 6 © £

In
¢¨ # $©

p¡ ar"ticouflathr,eitfiwltaesreadrgiunepduttsh.atItthiesrtehaerreefroerseiddueaslidraebpleentdoenmcoiedseal mthoonsge

dependencies within the PoT model. By inspection of figure (1b) we note that these depen-

dencies can be modelled through a non-negative

the hidden variables

©

with the activities ¨¢#

¡$

¡

"


weight matrix . The resultant

  ©¢¤¡ £
model

3
is

, which depicted

connects in figure

(1c). Depending on how many nonzero weights   ©¥¡ emanate from a hidden unit © (say

¦ © ), each expert now occupies ¦ © input dimensions instead
these richer experts can be obtained from (2) by replacing,

of
¢¨ # ©$

ju¡ s t¨" o§ne.©

The
¡ 

ex¢© ¡ p¨¢r# e¡$ss¡ io n" .s

for We

©
have found that learning is assisted by fixing the %  -norm of the weights (

¡ 

©¥¡ £

¥&
5 ).

Moreover, we have found that the sparsity of the weights  can be controlled by the fol-

lowing generalization of the experts,

¥

© ¢¨ ¡ !£


¨ !¥

 ©  " ¡'(   

©¥¡  # ¡$ ¡    &E(

@ @©  143

  ¥© ¡ £ 3

0

(9)

The larger the value for  the sparser the distribution of  values.

Joint and conditional distributions over hidden variables are obtained through similar re-

placements in eqn. (6) and (7) respectively. Sampling the reconstructions given the states of

the hidden variables proceeds
Bdistributions  © "!! © ¥£ §¤ ¦©¨$#

by

©




©

first
 

sampling from with precision

 independent parameters %H£

ge" n era$ lizewd hLiacphlaacree

subsequently transformed into ¡ £ # I '$ & . Learning in this model therefore proceeds with

only minor modifications to the algorithm described in the previous section.

When we learn the weight matrix   ©¢¡ from image data we find that a particular hidden
 Bvariable © develops weights to the activities of filters similar in frequency, location and
orientation. The variables therefore integrate information from these filters and as a
result develop certain invariances that resemble the behavior of complex cells. A similar approach was studied in [4] using a related causal model2 in which a number of scale
variables generate correlated variances for conditionally Gaussian experts. This results in
topography when the scale-generating variables are non-adaptive and connect to a local
neighborhood of filters only.

We will now argue that fixed local weights  also give rise to topography in the PoT model. The reason is that averaging the squares of randomly chosen filter outputs (eqn.9) produces an approximately Gaussian distribution which is a poor fit to the heavy-tailed experts. However, this "smoothing effect" may be largely avoided by averaging squared filter outputs that are highly correlated (i.e. ones that are similar in location, frequency and orientation). Since the averaging is local, this results in a topographic layout of the filters.

5.1 Experiment: Topographic Representations for Natural Images

@For this experiment we collected ( 3

)3 3E3

¥3
image patches of size

)

¥3
pixels in the same

way as described in section (4.1). The image data were sphered and reduced to ¥ 3 E di-

3¥

omgvreiednr.scioEomancsphlbeyetxerpereemrptorvecsoinenngnte(actitoslonwwwitvihtahari) afi3)nx3ceedeaxwnpdeeirgtshhtwigohhfic vha¢©rw¡ iae£nrcee¥ o8(rDEgaCtno)izidteisrdeelcoftnaionand.saqWluleairtlesea(F r3Dnnee) digFahn3 -

bors, where periodic boundary conditions were imposed for the experts on the boundary.

2Interestingly, the update equations for the filters  presented in [4], which minimize a bound on the log-likelihood of a directed model, reduce to the same equations as our learning rules when the representation is complete and the filters orthogonal.

(a) (b)
Figure 2: (a)-Small subset of the  ¢¡¤¦£ ¨¥ §¢© learned filters from a  ¡¡ times overcomplete representation for natural image patches. (b)-Topographically ordered filters. The weights were fixed and connect to neighbors only, using periodic boundary conditions. Neighboring filters have learned to be similar in frequency, location and orientation. One can observe a pinwheel structure to the left of the low frequency cluster.

We adapted the filters # (% " -norm ¥ ) and used fixed values for " £ ¥ and  £ F . The re-

sulting inverse-filters are shown in figure (2b). We note that the weights  have enforced a topographic ordering on the experts, where location, scale and frequency of the Gabor-like

filters all change smoothly across the map.

In another experiment we used the same data to train a complete representation of ¥ 3 E

experts where we learned but with a fixed value of 

the
£

¥weights  (%  . The weights



-norm and

¥

"

), " and the filters # were kept positive

(unconstrained), by adapting their

logarithm. Since the weights  can now connect to any other expert we do not expect

topography.
the energies

To of

study whether the the filter outputs ¨ #

w©$ e¡ig h" tsw e

were modelling the dependencies between ordered the filters for each complex cell ©

Baccording to the strength of the weights connecting to it. For a representative subset of the
complex cells , we show the ¥ 3 filters with the strongest connections to that cell in figure

(1d). Since the cells connect to similar filters we may conclude that the weights  are

indeed learning the dependencies between the activities of the filter outputs.

6 Denoising Images: The Iterated Wiener Filter

If the PoT model provides an accurate description of the statistics of natural image data it

ought to be a good prior for cleaning up noisy images. In the following we will apply this

idea to denoise images contaminated with Gaussian pixel noise. We follow the standard

Bayesian approach which states that the optimal estimate of the original image is given by

 
the maximum a posteriori (MAP) estimate of

¨¡  &  , where &

denotes the noisy image.

For the PoT model this reduces to,

   `¡  B 8 £   

!

¥

¨&
F



¡  $#"

I  ¨&



¡

@

© A)B D%$& !¥

¥@ F

 

©¥¡

¨#

¡$

¡

" (0' 1) 

©0 ¡

(10)

(a) (b)

(c) (d)
Figure 3: (a)- Original "rock"-image. (b)-Rock-image with noise added. (c)-Denoised image using Wiener filtering. (d) Denoised image using IWF.

¡ £¢ ¢ ¢To minimize this we follow a variational procedure where we upper bound the logarithm

using ACEB D 7

7  ACEB D  ¥ . The bound is saturated when £ ¥ 8 7 . Applying this to

¤every logarithm in the summation in eqn. (10) and iteratively minimizing this bound over
¡ and we find the following update equations,

¢ ¦¥¥ 8

©

!¥ ¥ @ F

 

¥© ¡

¨#

¡$

¡

"


(11)

¥¡  B 8

¨" I 

¡

#

# $ I "

I


&

¤H £ p  Q  $ ¨ " )  h (12)

fiwlhteerrew) ithdennooisteesccoovmarpioannecnetw" isaenmd ualtGipaluicsasitaionnp. rSioinrcwe itthhe

second equation covariance ¨ # #

is $

j uI st

a Wiener we have

named the above denoising equations the iterated Wiener filter (IWF).

When the filters are orthonormal, the noise covariance isotropic and the weight matrix the

§identity, the minimization in (10) decouples into  minimizations over the transformed
variables 6 © £ # ©$ ¡ . Defining © £ # ©$ & we can easily derive that 6 © is the solution of the

©¨ §  §following cubic equation (for which analytic solutions exist),

6© 

" ©6 ©

F ¨ ¥

" ©6 ©  F © £ 3

(13)

0

We note however that constraining the filters to be orthogonal is a rather severe restriction if

the data are not pre-whitened. On the other hand, if we decide to work with whitened data,

the isotropic noise assumption seems unrealistic. Having said that, Hyvarinen's shrinkage

method for ICA models [3] is based on precisely these assumptions and seems to give good

results. The proposed method is also related to approaches based on the GSM [7].

6.1 Experiment: Denoising

To test the iterated Wiener filter, we trained a complete set of E E experts on the data de-

scribed in section (4.1). The norm of the filters was unconstrained, the " were free to adapt,

but we did not include any weights  .
Gaussian noise with standard deviation

The
£

image shown in figure F 3 , which resulted in

(3a) was a PSNR

corrupted

F)F
of

53

dB

with (fig-

ure (3b)). We applied the adaptive Wiener filter from matlab (Wiener2.m) with an optimal

W( i) en( ernfieilgtehrbinogrhhoaosdasPizSeNaRndofknF4o3 w5 E ndBnoainsed-ivsasrihaonwcen.

The denoised image using in figure (3c). IWF was run

adaptive on every

possible ¥ 3 ) ¥ 3 patch in the image, after which the results were averaged. Because the

filters # $ were trained on sphered data without a DC component, the same transformations

have to be applied to the test patches before IWF is applied. The denoised image using

IWF is shown in (3d) and has ¥ 5 F dB over Wiener filtering. It

a is

PSNR of our hope

F5
(
that

¥

dB, which is a significant improvement of the use of overcomplete representations and

weights  will further improve those results.

7 Discussion
It is well known that a wavelet transform de-correlates natural image data in good approximation. In [6] it was found that in the marginal distribution the wavelet coefficients are sparse" ly distributed but that there are significant residual dependencies among their energies 6 © . In this paper we have shown that the PoT model can learn highly overcomplete filters with sparsely distributed outputs. With a second hidden layer that is locally connected, it captures the dependencies between filter outputs by learning topographic representations.
Our approach improves upon earlier attempts (e.g. [4],[8]) in a number of ways. In the PoT model the hidden variables are conditionally independent so perceptual inference is very easy and does not require iterative settling even when the model is overcomplete. There is a fairly simple and efficient procedure for learning all the parameters, including the weights connecting top-level units to filter outputs. Finally, the model leads to an elegant denoising algorithm which involves iterating a Wiener-filter.

Acknowledgements
This research was funded by NSERC, the Gatsby Charitable Foundation, and the Wellcome Trust. We thank Yee-Whye Teh for first suggesting a related model and Peter Dayan for encouraging us to apply products of experts to topography.

References
[1] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771­1800, 2002.
[2] G.E. Hinton, M. Welling, Y.W. Teh, and K. Osindero. A new view of ICA. In Int. Conf. on Independent Component Analysis and Blind Source Separation, 2001.
[3] A. Hyvarinen. Sparse code shrinkage: Denoising of nongaussian data by maximum likelihood estimation. Neural Computation, 11(7):1739­1768, 1999.
[4] A. Hyvarinen, P.O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Computation, 13(7):1525­1558, 2001.
[5] S. Della Pietra, V.J. Della Pietra, and J.D. Lafferty. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380­393, 1997.
[6] E.P. Simoncelli. Modeling the joint statistics of images in the wavelet domain. In Proc SPIE, 44th Annual Meeting, volume 3813, pages 188­195, Denver, 1999.
[7] V. Strela, J. Portilla, and E. Simoncelli. Image denoising using a local Gaussian scale mixture model in the wavelet domain. In Proc. SPIE, 45th Annual Meeting, San Diego, 2000.
[8] M.J. Wainwright and E.P. Simoncelli. Scale mixtures of Gaussians and the statistics of natural images. In Advances Neural Information Processing Systems, volume 12, pages 855­861, 2000.
[9] S.C. Zhu, Z.N. Wu, and D. Mumford. Minimax entropy principle and its application to texture modeling. Neural Computation, 9(8):1627­1660, 1997.

