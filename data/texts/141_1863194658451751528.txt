Stick-breaking Construction for the Indian Buffet Process

Yee Whye Teh Gatsby Computational Neuroscience Unit
University College London 17 Queen Square
London WC1N 3AR, UK ywteh@gatsby.ucl.ac.uk

Dilan Go¨ru¨ r MPI for Biological Cybernetics
Dept. Scho¨lkopf Spemannstrasse 38 72076 Tu¨bingen, Germany dilan.gorur@tuebingen.mpg.de

Zoubin Ghahramani Department of Engineering University of Cambridge
Trumpington Street Cambridge CB2 1PZ, UK
zoubin@eng.cam.ac.uk

Abstract
The Indian buffet process (IBP) is a Bayesian nonparametric distribution whereby objects are modelled using an unbounded number of latent features. In this paper we derive a stick-breaking representation for the IBP. Based on this new representation, we develop slice samplers for the IBP that are efficient, easy to implement and are more generally applicable than the currently available Gibbs sampler. This representation, along with the work of Thibaux and Jordan [17], also illuminates interesting theoretical connections between the IBP, Chinese restaurant processes, Beta processes and Dirichlet processes.
1 INTRODUCTION
The Indian Buffet Process (IBP) is a distribution over binary matrices consisting of N > 0 rows and an unbounded number of columns [6]. These binary matrices can be interpreted as follows: each row corresponds to an object, each column to a feature, and a 1 in entry (i, k) indicates object i has feature k. For example, objects can be movies like "Terminator 2", "Shrek" and "Shanghai Knights", while features can be "action", "comedy", "stars Jackie Chan", and the matrix can be [101; 010; 110] in Matlab notation.
Like the Chinese Restaurant Process (CRP) [1], the IBP provides a tool for defining nonparametric Bayesian models with latent variables. However, unlike the CRP, in which each object belongs to one and only one of infinitely many latent classes, the IBP allows each object to possess potentially any combination of infinitely many latent features. This added flexibility has resulted in a great deal of interest in the IBP, and the development of a range of interesting applications. These applications include models for choice behaviour [5], protein-protein interactions [2], the structure of causal graphs [19], dyadic data for collaborative filtering applications [10], and human similarity judgments [11].

In this paper, we derive a new, stick-breaking representation for the IBP, a development which is analogous to Sethuraman's seminal stick-breaking representation for CRPs [15]. In this representation, as we will see in Section 3, the probability of each feature is represented explicitly by a stick of length between 0 and 1. Sethuraman's representation paved the way for both novel samplers for and generalizations of CRPs [7]. Similarly, we show how our novel stick-breaking representation of the IBP can be used to develop new slice samplers for IBPs that are efficient, easy to implement and have better applicability to non-conjugate models (Sections 4, 5.2, 6). This new representation also suggests generalizations of the IBP (such as a Pitman-Yor variant, in Section 3.2). Moreover, although our stick-breaking representation of the IBP was derived from a very different model than the CRP, we demonstrate a surprising duality between the sticks in these two representations which suggests deeper connections between the two models (Section 3.2). The theoretical developments we describe here, which show a stick-breaking representation which is to the IBP what Sethuraman's construction is to the CRP, along with the recent work of Thibaux and Jordan [17], showing that a particular subclass of Beta processes is to the IBP as the Dirichlet process is to the CRP, firmly establish the IBP in relation to the well-known classes of Bayesian nonparametric models.

2 INDIAN BUFFET PROCESSES

The IBP is defined as the limit of a corresponding distri-

bution over matrices with K columns, as the number of

columns K  . Let Z be a random binary N × K ma-

trix, and denote entry (i, k) in Z by zik. For each feature k

let µk be the prior probability that feature k is present in an

object.

We

place

a

Beta(

 K

,

1)

prior

on

µk ,

with



being

the strength parameter of the IBP. The full model is:

µk



Beta(

 K

,

1)

zik|µk  Bernoulli(µk)

indepedently k indepedently i, k

(1a) (1b)

Let us now consider integrating out the µk's and taking the

limit of K   to obtain the IBP. For the first object,

the chance of it having each particular feature k is inde-

pendently

 K

tion over the

once µk is integrated out, thus the distribu-

number

of

features

it

has

is

Binomial(

 K

,

K

).

As K  , this approaches Poisson(). For subse-

quent objects i = 2, . . . , N , the probability of it also

having a feature k already belonging to a previous object

is

 K

+m<i

k

 K

+1+i-1



m<i k i

where m<i k

=

j<i zjk > 0

is the number of objects prior to i with feature k. Re-

peating the argument for the first object, object i will

also

have

Poisson(

 i

)

new

features

not

belonging

to

pre-

vious objects. Note that even though the total number of

available features is unbounded, the actual number K+ of

used features is always finite (and in fact is distributed as

Poisson(

N i=1

1 i

)).

The above generative process can be understood using the metaphor of an Indian buffet restaurant. Customers (objects) come into the restaurant one at a time, and can sample an infinite number of dishes (features) at the buffet counter. Each customer will try each dish that previous customers have tried with probabilities proportional to how popular each dish is; in addition the customer will try a number of new dishes that others have not tried before.

To complete the model, let k be parameters associated with feature k and xi be an observation associated with object i. Let

k  H xi  F (zi,:, :)

independently k (2a) independently i (2b)

where H is the prior over parameters, F (zi,:, :) is the data distribution given the features zi,: = {zik}k=1 corresponding to object i and feature parameters : = {k}k=1. We assume that F (zi,:, :) depends only on the parameters of
the present features.

2.1 GIBBS SAMPLER

The above generative process for the IBP can be used directly in a Gibbs sampler for posterior inference of Z and  given data x = {xi} [6]. The representation consists of the number K+ of used (active) features, the matrix Z1:N,1:K+ of occurrences among the K+ active features, and their parameters 1:K+. The superscript + denotes active features. The sampler iterates through i = 1, . . . , N , for each object i it updates the feature occurrences for the currently used
features, then considers adding new features to model the data xi.
For the already used features k = 1, . . . , K+, the conditional probability of zik = 1 given other variables is just

p(zik

=

1|rest)



m¬i N

k

f

(xi

|zi,¬k

,

zik

=

1, 1:K+ )

(3)

where m¬i k = j=i zjk. The fraction is the conditional prior of zik = 1, obtained by using exchangeability among

the customers and taking customer i to be the last customer to enter the restaurant; the second term f (·|·) is the data likelihood of xi if zik = 1. It is possible to integrate 1:K+ out from the likelihood term if H is conjugate to F . In fact it is important for H to be conjugate to F when we consider
the probabilities of new features being introduced, because
all possible parameters for these new features have to be taken into account. If Li is the number of new features introduced, we have

p(Li|rest)



(

 N

)Li

e-

 N

Li!

×

f (xi|zi,1:K+ , zi,1:Li = 1, 1:K+ , 1:Li ) dh(1:Li ) (4)

w1h:Leirearzei,t1h:Leiir

are occurrences for the new parameters, the superscript 

features denoting

and cur-

rently unused (inactive) features. The fraction comes

from the probability of introducing Li new features under

Poisson(

 N

)

while

the

second

term

is

the

data

likelihood,

with the parameters 1:Li integrated out with respect to the

prior density h(·).

The need to integrate out the parameters for new features is similar to the need to integrate out parameters for new clusters in the Dirichlet process (DP) mixture model case (see [13]). To perform this integration efficiently, conjugacy is important, but the requirement for conjugacy limits the applicability of the IBP in more elaborate settings. It is possible to devise samplers in the non-conjugate case analogous to those developed for DP mixture models [10, 5]. In the next section we develop a different representation of the IBP in terms of a stick-breaking construction, which leads us to an easy to implement slice sampler for the nonconjugate case.

3 STICK BREAKING CONSTRUCTION
In this section, we describe an alternative representation of the IBP where the feature probabilities are not integrated out, and a specific ordering is imposed on the features. We call this the stick-breaking construction for the IBP. We will see that the new construction bears strong relationships with the standard stick-breaking construction for CRPs, paving the way to novel generalizations of and inference techniques for the IBP.

3.1 DERIVATION

Let µ(1) > µ(2) > . . . > µ(K) be a decreasing ordering

of µ1:K

=

{µ1, . . . , µK }, where each µl

is

Beta(

 K

,

1).

We will show that in the limit K   the µ(k)'s obey the

following law, which we shall refer to as the stick-breaking

construction for the IBP,

(k) i.i.d. Beta(, 1)

k
µ(k) = (k)µ(k-1) = (l)
l=1

(5)

We start by considering µ(1). For finite K it is

µ(1)

=

max
l=1,...,K

µl

(6)

where

each

µl

is

Beta(

 K

,

1)

and

has

density:

p(µl)

=

 K


µlK

-1

I(0



µl



1)

(7)

where I(A) is the indicator function for a condition (measurable set) A: I(A) = 1 if A is true, and 0 otherwise. The cumulative distribution function (cdf) for µl is then:

µl

F (µl) =

 K

t K

-1I(0



t



1)

dt

-


= µlK I(0  µl  1) + I(1 < µl)

(8)

Since the µl's are independent, the cdf of µ(1) is just the product of the cdfs of each µl, so

K
F (µ(1)) = µ(K1)I(0  µ(1)  1) + I(1 < µ(1) < )

= µ(1)I(0  µ(1)  1) + I(1 < µ(1))

(9)

Differentiating, we see that the density of µ(1) is

p(µ(1)) = µ(1-) 1I(0  µ(1)  1)

(10)

and therefore µ(1)  Beta(, 1).
We now derive the densities for subsequent µ(k)'s. For each k  1 let lk be such that µlk = µ(k) and let Lk = {1, . . . , K}\{l1, . . . , lk}. Since µ(1:k) = {µ(1), . . . , µ(k)} are the k largest values among µ1:K , we have

µl



min
k k

µ(k

)

=

µ(k)

(11)

for each l  Lk. Restricting the range of µl to [0, µ(k)], the cdf becomes

F (µl|µ(1:k)) =

µl 0

 K

t

 K

-1

dt

µ(k) 0

 K

t

 K

-1

dt

=µ-(k)K


µlK

I(0



µl



µ(k))

+

I(µ(k)

<

µl)

(12)

Now µ(k+1) = maxlLk µl with each µl independent given µ(1:k). The cdf of µ(k+1) is again the product of the cdfs of µl over l  Lk,

F (µ(k+1)|µ(1:k))

(13)

=µ-(kK) K-k



µ K-k K



(k+1)

I(0



µ(k+1)



µ(k) )

+

I(µ(k)

<

µ(k+1) )

µ(-k) µ(k+1)I(0  µ(k+1)  µ(k)) + I(µ(k) < µ(k+1))

as K  . Differentiating, the density of µ(k+1) is,

Notice that the µ(k)'s have a Markov structure, with µ(k+1) conditionally independent of µ(1:k-1) given µ(k).

Finally, instead of working with the variables µ(k) directly,

we introduce a new set of variables (k)

=

µ(k) µ(k-1)

with

range [0, 1]. Using a change of variables, the density of

(k) is derived to be,

p((k)|µ(1:k-1)) = (k-) 1I(0  (k)  1)

(15)

Thus (k) are independent from µ(1:k-1) and are simply

Beta(, 1) distributed. Expanding µ(k) = (k)µ(k-1) =

k l=1

(l),

we

obtain

the

stick-breaking

construction

(5).

The construction (5) can be understood metaphorically as
follows. We start with a stick of length 1. At iteration k = 1, 2, . . ., we break off a piece at a point (k) relative to the current length of the stick. We record the length µ(k) of the stick we just broke off, and recurse on this piece,
discarding the other piece of stick.

3.2 RELATION TO DP

In iteration k of the construction (5), after breaking the stick in two we always recurse on the stick whose length we denote by µ(k). Let (k) be the length of the other discarded stick. We have,

k-1
(k) = (1 - (k))µ(k-1) = (1 - (k)) (l)
l=1
Making a change of variables v(k) = 1 - (k),

(16)

k-1
v(k) i.i.d. Beta(1, ) (k) = v(k) (1 - v(l)) (17)
l=1
thus (1:) are the resulting stick lengths in a standard stick-breaking construction for DPs [15, 7].

In both constructions the final weights of interest are the lengths of the sticks. In DPs, the weights (k) are the lengths of sticks discarded, while in IBPs, the weights µ(k) are the lengths of sticks we have left. This difference leads to the different properties of the weights: for DPs, the stick
lengths sum to a length of 1 and are not decreasing, while in
IBPs the stick lengths need not sum to 1 but are decreasing. Both stick-breaking constructions are shown in Figure 1.
In both the weights decrease exponentially quickly in expectation.

The direct correspondence to stick-breaking in DPs implies that a range of techniques for and extensions to the DP can be adapted for the IBP. For example, we can generalize the IBP by replacing the Beta(, 1) distribution on (k)'s with other distributions. One possibility is a Pitman-Yor [14] extension of the IBP, defined as

p(µ(k+1) |µ(1:k) ) =µ(-k) µ(k-+11)I(0  µ(k+1)  µ(k))

(14)

(k)  Beta( + kd, 1 - d)

k
µ(k) = (l)
l=1

(18)

µ(1) µ(2) µ(3) µ(4) µ(5)
(5) µ(6)
(6)

(4)

(2) (3)

(1)

Figure 1: Stick-breaking construction for the DP and IBP. The black stick at top has length 1. At each iteration the vertical black line represents the break point. The brown dotted stick on the right is the weight obtained for the DP, while the blue stick on the left is the weight obtained for the IBP.

where d  [0, 1) and  > -d. The Pitman-Yor IBP

weights

decrease

in

expectation

as

a

O(k-

1 d

)

power-law,

and this may be a better fit for some naturally occurring

data which have a larger number of features with signifi-

cant but small weights [4].

An example technique for the DP which we could adapt to the IBP is to truncate the stick-breaking construction after a certain number of break points and to perform inference in the reduced space. [7] gave a bound for the error introduced by the truncation in the DP case which can be used here as well. Let K be the truncation level. We set µ(k) = 0 for each k > K, while the joint density of µ(1:K) is,

K

p(µ(1:K)) = p(µ(k)|µ(k-1))

(19)

k=1

K

=K µ(K)

µ(-k1)I(0  µ(K)  · · ·  µ(1)  1)

k=1

The conditional distribution of Z given µ(1:K) is simply1

N K

p(Z|µ(1:K)) =

µz(kik) (1 - µ(k))1-zik

i=1 k=1

(20)

with zik = 0 for k > K. Gibbs sampling in this representation is straightforward, the only point to note being that
adaptive rejection sampling (ARS) [3] should be used to sample each µ(k) given other variables (see next section).

4 SLICE SAMPLER

Gibbs sampling in the truncated stick-breaking construction is simple to implement, however the predetermined truncation level seems to be an arbitrary and unnecessary approximation. In this section, we propose a nonapproximate scheme based on slice sampling, which can be
1Note that we are making a slight abuse of notation by using Z both to denote the original IBP matrix with arbitrarily ordered columns, and the equivalent matrix with the columns reordered to decreasing µ's. Similarly for the feature parameters 's.

seen as adaptively choosing the truncation level at each iteration. Slice sampling is an auxiliary variable method that samples from a distribution by sampling uniformly from the region under its density function [12]. This turns the problem of sampling from an arbitrary distribution to sampling from uniform distributions. Slice sampling has been successfully applied to DP mixture models [8], and our application to the IBP follows a similar thread.
In detail, we introduce an auxiliary slice variable,

s|Z, µ(1:)  Uniform[0, µ]

(21)

where µ is a function of µ(1:) and Z, and is chosen to be the length of the stick for the last active feature,

µ = min

1,

k:

min
i,zik

=1

µ(k)

.

(22)

The joint distribution of Z and the auxiliary variable s is

p(s, µ(1:), Z) = p(Z, µ(1:)) p(s|Z, µ(1:)) (23)

where

p(s|Z, µ(1:))

=

1 µ

I(0



s



µ

).

Clearly,

integrat-

ing out s preserves the original distribution over µ(1:) and

Z, while conditioned on Z and µ(1:), s is simply drawn

from (21). Given s, the distribution of Z becomes:

p(Z |x,

s,

µ(1:))



p(Z |x,

µ(1:))

1 µ

I(0



s



µ)

(24)

which forces all columns k of Z for which µ(k) < s to be zero. Let K be the maximal feature index with µ(K) > s. Thus zik = 0 for all k > K, and we need only consider updating those features k  K. Notice that K serves as a truncation level insofar as it limits the computational
costs to a finite amount without approximation.
Let K be an index such that all active features have index k < K (note that K itself would be an inactive feature). The computational representation for the slice sampler consists of the slice variables and the first K features: s, K, K, Z1:N,1:K, µ(1:K), 1:K . The slice sampler proceeds by updating all variables in turn.
Update s. The slice variable is drawn from (21). If the new value of s makes K  K (equivalently, s < µ(K)), then we need to pad our representation with inactive features until K < K. In the appendix we show that the stick lengths µ(k) for new features k can be drawn iteratively from the following distribution:

p(µ(k)|µ(k-1), z:,>k = 0)  exp(

N i=1

1 i

(1

-

µ(k) )i )

µ(k-) 1(1 - µ(k))N I(0  µ(k)  µ(k-1))

(25)

We used ARS to draw samples from (25) since it is log-
concave in log µ(k). The columns for these new features are initialized to z:,k = 0 and their parameters drawn from their prior k  H.

Update Z. Given s, we only need to update zik for each i and k  K. The conditional probabilities are:

p(zik

=

1|rest)



µ(k) µ

f

(xi

|zi,¬k

,

zik

=

1, 1:K )

(26)

The µ denominator is needed when different values of zik induces different values of µ by changing the index of the last active feature.
Update k. For each k = 1, . . . , K, the conditional probability of k is:

N
p(k|rest)  h(k) f (xi|zi,1:K , ¬k, k)
i=1

(27)

Update µ(k). For k = 1, . . . , K - 1, combining (19) and (20), the conditional probability of µ(k) is

p(µ(k)|rest) µ(mk·)k-1(1 - µ(k))N-m·k I(µ(k+1)  µ(k)  µ(k-1))

(28)

where m·k =

N i=1

zik .

For

k

=

K,

in

addition to

tak-

ing into account the probability of features K is inactive,

we also have to take into account the probability that all columns of Z beyond K are inactive as well. The ap-

pendix shows that the resulting conditional probability of µ(K) is given by (25) with k = K. We draw from both (28) and (25) using ARS.

5 CHANGE OF REPRESENTATIONS
Both the stick-breaking construction and the standard IBP representation are different representations of the same nonparametric object. In this section we consider updates which change from one representation to the other. More precisely, given a posterior sample in the stick-breaking representation we wish to construct a posterior sample in the IBP representation and vice versa. Such changes of representation allow us to make use of efficient MCMC moves in both representations, e.g. interlacing split-merge moves in IBP representation [10] with the slice sampler in stick-breaking representation. Furthermore, since both stick lengths and the ordering of features are integrated out in the IBP representation, we can efficiently update both in the stick-breaking representation by changing to the IBP representation and back.
We appeal to the infinite limit formulation of both representations to derive the appropriate procedures. In particular, we note that the IBP is obtained by ignoring the ordering on features and integrating out the weights µ(1:K) in an arbitrarily large finite model, while the stick-breaking construction is obtained by enforcing an ordering with decreasing weights. Thus, given a sample in either representations, our approach is to construct a corresponding sample in an

arbitrarily large finite model, then to either ignore the ordering and weights (to get IBP) or to enforce the decreasing weight ordering (to get stick-breaking).
Changing from stick-breaking to the standard IBP representation is easy. We simply drop the stick lengths as well as the inactive features, leaving us with the K+ active feature columns along with the corresponding parameters. To change from IBP back to the stick-breaking representation, we have to draw both the stick lengths and order the features in decreasing stick lengths, introducing inactive features into the representation if required. We may index the K+ active features in the IBP representation as k = 1, . . . , K+ in the finite model. Let Z1:N,1:K+ be the feature occurrence matrix. Suppose that we have K K+ features in the finite model. For the active features, the posterior for the lengths are simply

µ+k |z:,k



Beta(

 K

+

m·k, 1 + N

-

m·k)

 Beta(m·k, 1 + N - m·k)

(29)

as K  . For the rest of the K - K+ inactive fea-
tures, it is sufficient to consider only those inactive features with stick lengths larger than mink µk+. Thus we consider a decreasing ordering µ(1) > µ(2) > · · · on these lengths. (25) gives their densities in the K   limit and ARS can be used to draw µ(1:K) until µ(K) < mink µk+. Finally, the stick-breaking representation is obtained by reordering µ1+:K+ , µ(1:K) in decreasing order, with the feature columns and parameters taking on the same ordering
(columns and parameters corresponding to inactive features
are set to 0 and drawn from their prior respectively), giving us K+ + K features in the stick-breaking representation.

5.1 SEMI-ORDERED STICK-BREAKING

In deriving the change of representations from the IBP to the stick-breaking representation, we made use of an intermediate representation whereby the active features are unordered, while the inactive ones have an ordering of decreasing stick lengths. It is in fact possible to directly work with this representation, which we shall call semi-ordered stick-breaking.
The representation consists of K+ active and unordered features, as well as an ordered sequence of inactive features. The stick lengths for the active features have conditional distributions:

µk+|z:,k  Beta(m·,k, 1 + N - m·,k)

(30)

while for the inactive features we have a Markov property:

p(µ(k)|µ(k-1), z:,>k = 0)  exp(

N i=1

1 i

(1

-

µ(k) )i ))

(µ(k))-1(1 - µ(k))N I(0  µ(k)  µ(k-1)) (31)

5.2 SLICE SAMPLER

To use the semi-ordered stick-breaking construction as a

representation for inference, we can again use the slice

sampler to adaptively truncate the representation for inac-

tive features. This gives an inference scheme which works

in the non-conjugate case, is not approximate, has an adap-

tive truncation level, but without the restrictive ordering

constraint of the stick-breaking construnction. The repre-

sentation of the K+

s, K+, Z1:N,1:K+, µ+1:K+ , 1:K+ consists active features and the slice variable s,

only

s  Uniform[0, µ]

µ = min

1,

min
1kK +

µ+k

(32)

Once a slice value is drawn, we generate K inactive features, with their stick lengths drawn from (31) until µ(K+1) < s. The associated feature columns Z1:N,1:K are initialized to 0 and the parameters 1:K drawn from their prior. Sampling for the feature entries and parameters
for both the active and just generated inactive features pro-
ceed as before. Afterwards, we drop from the list of active features any that became inactive, while we add to the list
any inactive feature that became active. Finally, the stick
lengths for the new list of active features are drawn from their conditionals (30).

6 EXPERIMENT
In this section we compare the mixing performance of the two proposed slice samplers against Gibbs sampling. We chose a simple synthetic dataset so that we can be assured of convergence to the true posterior and that mixing times can be estimated reliably in a reasonable amount of computation time. We also chose to apply the three samplers on a conjugate model since Gibbs sampling requires conjugacy, although our implementation of the two slice samplers did not make use of this. In the next section we demonstrate the modelling performance of a non-conjugate model using the semi-ordered slice sampler on a dataset of MNIST handwritten digits.
We used the conjugate linear-Gaussian binary latent feature model for comparing the performances of the different samplers [6]. Each data point xi is modelled using a spherical Gaussian with mean zi,:A and variance X2 , where zi,: is the row vector of feature occurrences corresponding to xi, and A is a matrix whose kth row forms the parameters for the kth feature. Entries of A are drawn i.i.d. from a zero mean Gaussian with variance A2 . We generated 1, 2 and 3 dimensional datasets from the model with data variance fixed at X2 = 1, varying values of the strength parameter  = 1, 2 and the latent feature variance A2 = 1, 2, 4, 8. For each combination of parameters we produced five datasets with 100 data points, giving a total of 120 datasets. For all datasets, we fixed X2 and A2 to the generating values and learned the feature matrix Z and .

mixing time

103
102
101
Stick-Breaking Semi-Ordered Gibbs Sampling
Figure 2: Autocorrelation times for K+ for the slice sampler in decreasing stick lengths ordering, in semi-ordered stick-breaking representation, and for the Gibbs sampler.
For each dataset and each sampler, we repeated 5 runs of 15, 000 iterations. We used the autocorrelation coefficients of the number of represented features K+ and  (with a maximum lag of 2500) as measures of mixing time. We found that mixing in K+ is slower than in  for all datasets and report results only for K+ here. We also found that in this regime the autocorrelation times do not vary with dimensionality or with A2 . In Figure 2 we report the autocorrelation times of K+ over all runs, all datasets, and all three samplers. As expected, the slice sampler using the decreasing stick lengths ordering was always slower than the semi-ordered one. Surprisingly, we found that the semiordered slice sampler was just as fast as the Gibbs sampler which fully exploits conjugacy. This is about as well as we would expect a more generally applicable non-conjugate sampler to perform.
7 DEMONSTRATION
In this section we apply the semi-ordered slice sampler to 1000 examples of handwritten images of 3's in the MNIST dataset. The model we worked with is a generalization of that in Section 6, where in addition to modelling feature occurrences, we also model per object features values [6]. In particular, let Y be a matrix of the same size as Z, with i.i.d. zero mean unit variance Gaussian entries. We model each xi as
xi|Z, Y, A, X2  N ((zi,: yi,:)A, X2 I), (33)
where is elementwise multiplication. Specification for the rest of the model is as in Section 6. We can integrate Y or A out while maintaining tractability, but not both.
The handwritten digit images are first preprocessed by projecting on to the first 64 PCA components, and the sampler ran for 10000 iterations. The trace plot of the log likelihood and the distribution of the number of active features are shown in Figure 3. The model succesfully finds latent features to reconstruct the images as shown in Figure 4. Some of the latent features found are shown in Figure 5. Most appear to model local shifts of small edge segments

log likelihood

×105 -4 -4.1 -4.2 -4.3
300 200 100
0

2500

5000 iterations

7500

20 40 60 80 100 k (feature label)

400

# iterations

300

200

100

10000

0 50

100 # active feats

150

# objects

150 100
50 0
120

5 10 15 # active feats

m (# objects with feature k) k

Figure 3: Top-left: the log likelihood trace plot. The sampler quickly finds a high likelihood region. Top-right: histogram of the number of active features over the 10000 iterations. Bottom-left: number of images sharing each feature during the last MCMC iteration. Bottom-right: histogram of the number of active features used by each input image. Note that about half of the features are used by only a few data points, and each data point is represented by a small subset of the active features.

Figure 5: Features that are shared between many digits.

ments. A direct consequence of our stick-breaking con-

struction is that a draw from such a Beta process has the

form A =

 k=1

µ(k) k

with

µ(k)

drawn

from

(5)

and

k drawn i.i.d. from the base measure H. This is a par-

ticularly simply case of a more general construction called

the inverse Le´vy measure [18, 9]. Generalizations to us-

ing other stick-breaking constructions automatically lead

to generalizations of the Beta process, and we are currently

exploring a number of possibilities, including the Pitman-

Yor extension. Finally, the duality observed in Section 3.2

seems to be a hitherto unknown connection between the

Beta process and the DP which we are currently trying to

understand.

As an aside, it is interesting to note the importance of feature ordering in the development of the IBP. To make the derivation rigorous, [6] had to carefully ignore the feature ordering by considering permutation-invariant equivalence classes before taking the infinite limit. In this paper, we derived the stick-breaking construction by imposing a feature ordering with decreasing feature weights.

To conclude, our development of a stick-breaking construction for the IBP has lead to interesting insights and connections, as well as practical algorithms such as the new slice samplers.

Figure 4: Last column: original digits. Second last column: reconstructed digits. Other columns: features used for reconstruction.
of the digits, and are reminiscent of the result of learning models with sparse priors (e.g. ICA) on such images [16].
8 DISCUSSION AND FUTURE WORK
We have derived novel stick-breaking representations of the Indian buffet process. Based on these representations new MCMC samplers are proposed that are easy to implement and work on more general models than Gibbs sampling. In experiments we showed that these samplers are just as efficient as Gibbs without using conjugacy.
[17] have recently showed that the IBP is a distribution on matrices induced by the Beta process with a constant strength parameter of 1. This relation to the Beta process is proving to be a fertile ground for interesting develop-

ACKNOWLEDGEMENTS
We thank the reviewers for insightful comments. YWT thanks the Lee Kuan Yew Endowment Fund for funding.
REFERENCES
[1] D. Aldous. Exchangeability and related topics. In E´cole d'E´ te´ de Probabilite´s de Saint-Flour XIII­ 1983, pages 1­198. Springer, Berlin, 1985.
[2] W. Chu, Z. Ghahramani, R. Krause, and D. L. Wild. Identifying protein complexes in high-throughput protein interaction screens using an infinite latent feature model. In BIOCOMPUTING: Proceedings of the Pacific Symposium, 2006.
[3] W.R. Gilks and P. Wild. Adaptive rejection sampling for Gibbs sampling. Applied Statistics, 41:337­348, 1992.
[4] S. Goldwater, T.L. Griffiths, and M. Johnson. Interpolating between types and tokens by estimating power-

law generators. In Advances in Neural Information Processing Systems, volume 18, 2006.
[5] D. Go¨ru¨r, F. Ja¨kel, and C. E. Rasmussen. A choice model with infinitely many latent features. In Proceedings of the International Conference on Machine Learning, volume 23, 2006.
[6] T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. In Advances in Neural Information Processing Systems, volume 18, 2006.
[7] H. Ishwaran and L.F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 96(453):161­173, 2001.
[8] M. Kalli and S. G. Walker. Slice sampling for the Dirichlet process mixture model. Poster presented at the Eighth Valencia International Meeting on Bayesian Statistics, 2006.
[9] P. Le´vy. The´orie de L'Addition des Variables Ale´atoires. Paris: Gauthier-Villars, 1937.
[10] E. Meeds, Z. Ghahramani, R. Neal, and S. T. Roweis. Modeling dyadic data with binary latent factors. In Advances in Neural Information Processing Systems, volume 19, to appear 2007.
[11] D. J. Navarro and T. L. Griffiths. A nonparametric Bayesian method for inferring features from similarity judgements. In Advances in Neural Information Processing Systems, volume 19, to appear 2007.
[12] R. M. Neal. Slice sampling. Annals of Statistics, 31:705­767, 2003.
[13] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249­265, 2000.
[14] J. Pitman and M. Yor. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855­900, 1997.
[15] J. Sethuraman. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639­650, 1994.
[16] Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton. Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4:1235­1260, Dec 2003.
[17] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. This volume, 2007.
[18] R. L. Wolpert and K. Ickstadt. Simulations of le´vy random fields. In Practical Nonparametric and Semiparametric Bayesian Statistics, pages 227­242. Springer-Verlag, 1998.

[19] F. Wood, T. L. Griffiths, and Z. Ghahramani. A non-parametric Bayesian method for inferring hidden causes. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, volume 22, 2006.

APPENDIX

Recall from the construction of µ(k+1:) that it is simply the K   limit of a decreasing ordering of µLk . Since reordering does not affect the probabilities of zil's given
the corresponding µl for each l  Lk,

p(z:,>k = 0|µ(k))

= lim
K 

p(µLk |µ(k))p(z:,Lk = 0|µLk ) dµLk

Given µ(k), µl's and zil's are conditionally i.i.d. across different l's, with cdf of µl as given in (12). Thus we have

= lim
K 

µ(k)

(1

-

µ)N

 K

µ-(k)K

µ

 K

-1

dµ

K -k

0

(34)

Applying change of variables  = µ/µ(k) to the integral,

1

(1

-

 µ(k) )N

 K



 K

-1

d

0

1

=

(1

-



+

(1

-

µ(k)))N

 K



 K

-1

d

0

1N

=

(Ni

)

(1

-



)N

-i(

(1

-

µ(k) ))i

 K



 K

-1

d

0 i=0

N

=

(Ni )

(1

-

µ(k) )i

 K

(

 K

+i)(N

-i+1)

(

 K

+i+N

-i+1)

i=0

N
=

N! (N -i)!i!

(1

-

µ(k) )i

 K

Qi-1
j=0

(

 K

+j)(N

-i)!

QN
j=0

(

 K

+j)

i=0

= N!

QN
j=1

 K

+j

1

+

 K

N i=1

(1Qi-1 j=1

 K

+j

i!

-

µ(k) )i

Finally, plugging the above into (34) and taking K  ,

p(z:,>k = 0|µ(k)) = exp - HN + 

N i=1

1 i

(1

-

µ(k))i

(35)

To obtain (25), we note that the conditional for µ(k) is the posterior conditioned on both z:,k = 0 and z:,>k = 0. The prior given µ(k-1) is (14), the probability of z:,k = 0 is just (1 - µ(k))N , while the probability of z:,>k = 0 is (35);
multiplying all three gives (25).

