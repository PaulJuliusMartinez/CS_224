COMPARISON OF LEARNING ALGORITHMS FOR HANDWRITTEN DIGIT
RECOGNITION
Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J.ED.eSnakcekri,nHge. rD, rPu.cSkiemr,aIr.dG, aunydonV, .UV.aMpnuilkler, Bell Laboratories, Holmdel, NJ 07733, USA Email: yann@research.att.com Abstract
This paper compares the performance of several classi er algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also rejection, training time, recognition time, and memory requirements.
1

COMPHAARNISDOWNROITFTLEENADRINGIINTGRAELCGOOGRNIITTHIOMNS FOR
Y. LeJBC.euEDlnl.e,LSnLaakEb.cemkorJ,iraanaHcigtlk:o.eerrDly,i,aerPLnsu,..ncHSB@kieoomrrlte,mtasoIerd.udaeGr,,lc,AauhnNy..adoJBtnVtr0,.uc7.Uon7Vm.o3at3Mp,,nCuUil.klSeCAro, rtes,

1 Introduction
Tinhgeasligmoruiltthamneso, uasnadvlaairlgaebidliatytaobfaisneesx,pheanvseivceapuosewderrfauplicdopmropgurteesrss,inpohwaenrdfuwlrlietainrngrecognition in the last few years. This paper compares the relative merits of several classi cation algorithms developed at Bell Laboratories and elsewhere fdoirgitthseispounrlpyooseneoforfemcoagnnyizpirnogbhleamnsdwinrvitotlevneddiingidtse.siWgnhinilge arepcroagcntiizcianlgreincodgivniidtiuoanl system, it is an excellent benchmark for comparing shape recognition methods. Though many existing method combine a handcrafted feature extractor and a trainable classi er, this study concentrates on adaptive methods that operate directly on size-normalized images.

2 Database
The database used to train and test the systems described in this paper was constructed from the NIST's Special Database 3 and Special Database 1 conotafin30in,0g0b0inpaartyteirmnsagfreosmofShDa-n3d,warnitdte3n0,d0i0g0itsp.atOteurrnstrfarionmingSDse-t1.waOsucromtesptosseedt was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writewresr.e Wdisejominatd. eAslulrtehethiamtatghees wseetrse osifzewrnioterrmsaolifzetdhetotratiniinnga s2e0tx2a0ndpitxeesltbsoext (while preserving the aspect ratio). For some experiments, the 20x20 images were deslanted using moments of inertia before being presented. For other experiments they were only centered in a larger input eld using center of mass. G(LreaNyescta1leapnidxeTlavnaglueenstwDeirsetaunsceed) tuosereddsuucbestahmepeleedctvserosfioanlisaosifntgh.eTiwmoagmesetthoo1d6s by 16 pixels.

3 The Classi ers

cInomtBhpialsestseeelcidtneieoscnLriiwpnteeioabnrrsiCerelyaadsdseeirsscermrib:aeyPtochosesnisbcullyaltststhiheeesrirsmefupesrleeedsntcicenlsa.ossuir

study. er that

For more one might

consider is a linear classi er. Each input pixel value contributes to a weighted

sum for each output unit. The output unit with the highest sum (including the

cthonistreixbpuetrioimneonfta, wbeiaussceodnsdteasnlat)ntienddi2ca0txe2s0tihmeacglaess.s oTfhteheneitnwpourtkchhaarsa4ct0e1r0. fFroeer

parameters. The de ciencies of the linear classi er are well documented (Duda

& Hart 73) and it is included here simply to form a basis of comparison for more

sophisticated classi ers. The test error rate is 8.4%. Various combinations of

sigmoid units, linear units, gradient descent learning, and learning by directly

solvBinagselilnienaer sNysetaermesstgaNveeisgimhbiloarrrCeslualstss.i er: Another simple classi er is a

K-nearest neighbor classi er with a Euclidean distance measure between input

images. This classi er has the advantage that no training time, and no brain

on the part of the designer, are required. However, the memory requirement

and recognition time are large: the complete 60,000 twenty by twenty pixel

training images (about 24 Megabytes at one byte per pixel, or 12 megabytes at 4

bits/pixel) must be available at run time. Much more compact representations

could be devised with modest increase in recognition time and error rate. As in

the previous case, deslanted 20x20 images were used. The test error for k = 3 is

2.4%. Naturally, a realistic Euclidean distance nearest-neighbor system would

operate on feature vectors rather than directly on the pixels, but since all of

the other systems presented in this paper operate directly on the pixels, this

resuPltaiisrwusiesfeulLfionreaarbaCsellainsesicoemr:paArissoinm.ple improvement of the basic linear

classi er was tested (Guyon et al. 89). The idea is to train each unit of a

single-layer network to classify one class from one other class. In our case this

layer comprises 45 units labelled 0/1, 0/2,...0/9, 1/2....8/9. Unit i=j is trained

to produce +1 on patterns of class i, -1 on patterns of class j, and is not trained

on other patterns. The nal score for class i is the sum of the outputs all the

units labelled i=x minus the sum of the output of all the units labelled y=i, for

alilnlexParracinnldacsisypi.aelEr.CrroormrpatoenoenntthAentaeslyt ssiest

was
and

7.6%, only slightly better
Polynomial Classi er:

than a Follow-

ing (Schurmann 78), a preprocessing stage was constructed which computes

the projection of the input pattern on the 40 principal components of the set of

training vectors. To compute the principal components, the mean of each input

component was rst computed and subtracted from the training vectors. The

covariance matrix of the resulting vectors was then computed, and diagonalized

using Singular Value Decomposition. The 40-dimensional feature vector was

used as the input of a second degree polynomial classi er. This classi er can be

seen as a linear classi er with 821 inputs, preceded by a module that computes

all

pRroadduiactlsBoaf spiasirFsuonfcintipount

vNaeritawbolersk. :EFrroolrloownintghe(Lteeest9s1e)t,

was 3.3%. an RBF network

was constructed. The rst layer was composed of 1,000 Gaussian RBF units

with 400 inputs (20x20), the second layer was a simple 1000-10 linear classi-

er. The RBF units were divided into 10 groups of 100. Each group of units

was trained on all the training examples of one of the 10 classes using the

adaptive K-means algorithm. The second layer weights were computed using

a

reLgualragreizeFdulplsyeuCdoon-innveecrtseedmMetuholtdi.-LEaryroerrrNateeuornalthNeettewstosrekt:

was 3.6% Another

clas-

si er that we tested was a fully connected multi-layer neural network with two

layers of weights (one hidden layer). The network trained with various numbers

of hidden units. Deslanted 20x20 images were used as input. The best result

was 1.6% on the test set, obtained with a 400-300-10 network (approximately

123,300 weights). It remains somewhat of a mystery that networks with such

a large number of free parameters manage to achieve reasonably low testing

errors. We conjecture that the dynamics of gradient descent learning in multi-

INPUT 28x28

feature maps 4@24x24

feature maps feature maps feature maps OUTPUT

4@12x12

12@8x8

12@4x4

10@1x1

Convolution

Subsampling

Convolution

Subsampling

Convolution

Figure 1: Architecture of LeNet 1. Each plane represents a feaure map, i.e. a set of units whose weights are constrained to be identical. Input images are sized to t in a 16 x 16 pixel eld, but enough blank pixels are added around the border of this eld to avoid edge e ects in the convolution calculations.

layer nets has a \self-regularization" e ect. Because the origin of weight space

is a saddle point that is attractive in almost every direction, the weights in-

variably shrink during the rst few epochs (recent theoretical analysis seem to

con rm this (Sara Solla, personal communication)). Small weights cause the

sigmoids to operate in the quasi-linear region, making the network essentially

equivalent to a low-capacity, single-layer network. As the learning proceeds,

the weights grow, which progressively increases the e ective capacity of the

network. A better theoretical understanding of these phenomena, and more

empLireiNcael tev1i:deTnoces,oalvree

de the

nitely needed. dilemma between

small

networks

that

cannot

learn

the training set, and large networks that seem overparameterized, one can de-

sign specialized network architectures that are speci cally designed to recognize

two-dimensional shapes such as digits, while eliminating irrelevant distortions

wanodrkv(aLrieaCbuilnitye.t

These considerations lead us to al. 90). In a convolutional net,

etahcehiduenaitotfackoensviotsluitniopnuatlfrnoemt-

a local \receptive eld" on the layer below, forcing it to extract a local fea-

tpularen.esF,ucratlhleedrmfeoarteu,ruenmitsaplosc, awteitdhaint

di erent places on the image which units are constrained

are grouped in to share a sin-

gle set of weights. This makes the operation performed by a feature map shift

iTnhviasriwaneitg,hat-nsdhaerqinugivtaelechntnitqoueagcroeantvloylurteidounc,esfotllhoewnedumbbyesrqoufafsrheiengpafruanmcteitoenrss..

A single layer is formed of multiple feature maps, extracting di erent features

types. Complete networks are formed of multiple convolutional layers, extracting

features of increasing complexity and abstraction. Sensitivity to shifts and dis-

tortions can be reduced by using lower-resolution feature maps in the higher

layers. This is achieved by inserting subsampling layers between the convolu-

tion layers. It is important to stress that all the weights in such a network

are trained by gradient descent. Computing the gradient can be done with a

slightly modi ed version of the classical backpropagation procedure. The train-

ing process causes convolutional networks to automatically synthesize their own

features. One of our rst convolutional network architecture, LeNet 1, shown

in Figure 3, was trained on the database. Because of LeNet 1's small input

eld, the images were down-sampled to 16x16 pixels and centered in the 28x28

input layer. Although about 100,000 multiply/add steps are required to eval-

uate LeNet 1, its convolutional nature keeps the number of free parameters

vtoerosinolny oafbothuet 3U0S0P0S. TdahteabLaesNeeatn1d airtcshsiitzeectwuraes wtuanseddevtoelompaetdchustihneg aovuariloawblne

dataL.eLNeeNtet4:1

achieved 1.7% test error. Experiments with LeNet

1

made

it

clear

that

a

larger

convolu-

tseiotn. alLneNetewto4rkwwaassdneeseidgendedtotomaadkedroepstsimthailsupseroobfletmhe. laIrtgiessaizne eoxfpthanedteradinvienrg-

sion of LeNet 1 that has a 32x32 input layer in which the 20x20 images (not

deslanted) were centered by center of mass. It includes more feature maps and

an additional layer of hidden units that is fully connected to both the last layer

coofnfneaectutiroenssmanapdshaansdabtooutthe17o,0u0tp0uftreuenpitasr.amLeetNerest. 4Tceosnt tearirnosr awbaosut1.216%0.,00In0

previous experiments with ZIP code data, replacing the last layer of LeNet with

a more complex classi er improved the error rate. We replaced the last layer

loefarLneiNnget"4mweitthhoad EofucBliodtetaonu NanedareVsatpNniekig, hibnowr hcilcahssia elro,caalndlinweaitrhctlahsesi\leorcaisl

retrained each time a new test pattern is shown. Neither of those improve the

rawLeerrNoretra5t:e,LaeltNheotu5g,hhtahseyandiadrcimhiptercotvuerethseimreijleacrtitoonL. eNet 4, but has more

fseeanttuarteionmatopse,nacoldaergtehrefuclalyte-gcoonrineescatetdthlaeyoeru,tpauntd liatyeurs,esraathdeirsttrhibauntethderemporere-

traditional \1 of N" code. LeNet 5 has a total of about 340,000 connections,

and 60,000 free parameters, most of them in the last two layers. Again the non-

dperoslcaendtuerde 2in0xcl2u0diemd aagmesocdeunlteertehdatbdyiscteonrttesrtohfeminapsustwiemreaguesseddu, rbiuntg tthraeintrianigniunsg-

ing small randomly picked a ne transformations (shift, scaling, rotation, and

skewBinogo)s.teItdaLcheiNeveetd40:.9F%olelorwroirn.g theoretical work by R. Schapire, Drucker et

aclla. s(sDi reurcsk.erThetreael L93e)Ndetev4elaorpeecdotmhbei\nbedo:ostthineg"rsmt eotnheodisftorracinoemdbtihneinugsmuaullwtipayle.

the second one is trained on patterns that are ltered by the rst net so that

the second machine sees a mix of patterns, 50% of which the rst net got right,

aonndw5h0i%chotfhwehircsht iatngdotthwerosencgo.nFdinnaeltlsy,dtihsaegtrheier.dDneutriinsgtrtaeisnteindgo,nthneewouptaptuttesrnosf

the three nets are simply added. Because the error rate of LeNet 4 is very low,

it was necessary to arti cially increase the number of training samples with

random distortions (like with LeNet 5) in order to get enough samples to train

cthlaesssieceornsd. Aantd trhstirgdlannectse., bTohoesttiensgt earprporearrastetowbaes 0th.7r%ee, ttihmeebsemstoorfeaenxypeonfsoivuer

as a single net. In fact, when the rst net produces a high con dence answer,

t(hTeDToCtah)neigrsenanettnseDaarrieessttna-onntecicegahlCbleoldar.smTsiheteehrcood(stTwiDhs eaCrbe)o:tuhTteh1de.7is5TtaatinnmcgeeensftuthnDacittsitooanfnacisesimnclgaaldseseinienetr-.

sensitive to small distortions and translations of the input image (Simard et al.

93). If we consider an image as a point in a high dimensional pixel space (where

the dimensionality equals the number of pixels), then an evolving distortion of

atiocnhsardaectenre taralcoews-oduimt eancsuiornvaelimn apnixifeolldspiancep.ixTealksepnacteo.geFthorers,malalllthdeissteodrtiisotonrs-,

in the vicinity of the original image, this manifold can be approximated by a

plane, known as the tangent plane. An excellent measure of "closeness" for

dchisatroarcttieornsimuasegdestios gtheneedriastteanthcee bpelatwneesenintchlueidretsatnrgaennstlaptliaonnes,s,swcahlienreg,tshkeewseitnogf,

400-10 pairwise PCA+quadratic 1000 RBF 400-300-10
LeNet 1 LeNet 4 LeNet 4 / Local LeNet 4 / K-NN LeNet 5 Boosted LeNet 4 K-NN Euclidean Tangent Distance Soft Margin

---- 8.4 ----> ---- 7.6 ----> ---- 3.3 ----> ---- 3.6 ---->

0 0.5

1.1 1.1 1.1 0.9 0.7
1.1 1.1
1

1.6 1.7
1.5 2

2.4 2.5

3

Figure 2: error rate on the test set (%). The uncertainty in the quoted error rates is about 0.1%.

squeezing, rotation, and line thickness variations. A test error rate of 1.1% was achieved using 16x16 pixel images. Pre ltering techniques using simple Euclidean distance at multiple resolutions allowed to reduce the number of necessary Tangent Distance calculations. The gure for storage requirement assumes that the patterns are represented at multiple resolutions at one byte
perOpipxteilm. al Margin Classi er (OMC): Polynomial classi ers are well-
studied methods for generating complex decision surfaces. Unfortunately, they are impractical for high-dimensional problems, because the number of product terms is prohibitive. A particularly interesting subset of decision surfaces is the ones that correspond to hyperplanes that are at a maximum distance from the convex hulls of the two classes in the high-dimensional space of the product terms. Boser, Guyon, and Vapnik (Boser et al. 92) realized that any polynomial of degree k in this \maximum margin" set can be computed by rst computing the dot product of the input image with a subset of the training samples (called the \support vectors"), elevating the result to the k-th power, and linearly combining the numbers thereby obtained. Finding the support vectors and the coe cients amounts to solving a high-dimensional quadratic minimization problem with linear inequality constraints. Using a version of the procedure, known as Soft Margin Classi er (Cortes & Vapnik 95) that is well suited for noisy problems, with a 4-th degree decision surface, a test error of 1.1% was reached. The number of support vectors obtained was around 25,000.

4 Discussion
A summary of the performance of our classi ers is shown in Figures 2 to 5. Figure 2 shows the raw error rate of the classi ers on the 10,000 example test set. Boosted LeNet 4 is clearly the best, achieving a score of 0.7%, closely pfoelrlofowremdabnyceL, e0N.2e%t .5 at 0.9%. This can be compared to our estimate of human
Figure 3 shows the number of patterns in the test set that must be rejected to attain a 0.5% error. In many applications, rejection performance is more signi cant than raw error rate. Again, Boosted LeNet 4 has the best score. The enhanced versions LeNet 4 did better than the original LeNet 4, even though

400-300-10 LeNet 1 LeNet 4
LeNet 4 / Local LeNet 4 / K-NN
Boosted LeNet 4 K-NN Euclidean
Tangent Distance Soft Margin

1.8 1.4
1.6 0.5
1.9 1.8

3.2 3.7

8.1

01 23 45 67 89

Figure 3: Percent of test patterns rejected to achieve 0.5% error on the remaining test examples for some of the systems.

400-10 pairwise PCA+quadratic 1000 RBF 400-300-10
LeNet 1 LeNet 4 LeNet 4 / Local LeNet 4 / K-NN LeNet 5 Boosted LeNet 4 K-NN Euclidean Tangent Distance Soft Margin

0.5 2 2.5
60 10 15 30
40 50

0

500

1000 1000

1000

1500

2000
2000 2000 2000

Figure 4: Time required on a Sparc 10 for recognition of a single character starting with a size-normalized image (in milliseconds).

the Fraigwuraecc4ursahcoiwess wthereetiidmenetriceaqlu.ired on a Sparc 10 for each method to recognize a test pattern, starting with a size-normalized image. Expectedly, memory-based method are much slower than neural networks. Single-board hardware designed with LeNet in mind performs recognition at 1000 charactmeresm/soercy-(bSaascekdintgeecrhn&iquGersafar9e4)m. oCreosetl-uesievcet,ivdeuehatrodwthaerier iemnpolremmoeunstamtieomnsoroyf requirements.
Training time was also measured. K-nearest neighbors and TDC have esPseCntAia+llqyuzaedrroattircainneitngcotuimldeb. eWtrhaiilneetdheinsilnesgslet-hlaayneranneht,outhr,etphaeirmwuisletilnaeyte,rannedt training times were expectedly much longer: 3 days for LeNet 1, 7 days for the fully connected net, 2 weeks for LeNet 4 and 5, and about a month for boosted LeNet 4. Training the Soft Margin classi er took about 10 days. Howiervreerl,evwahnitletothtehetrcauinsitnogmteirm. e is marginally relevant to the designer, it is totally
Figure 5 shows the memory requirements of our various classi ers. Figures are based on 4 bit per pixel representations of the prototypes for K-Nearest bNeeitgahkbeonras,s1ubpypteerpbeorupnidxse,lafosrcSleovfetrMcoamrgpinr,esasniodnToafntgheentdaDtiastaanndc/eo. rTehliemyisnhaotuioldn of redundant training examples can reduce the memory requirements of some

400-10 0.016

pairwise

0.072

PCA+quadratic

0.1

1000 RBF

400-300-10

LeNet 1 0.012

LeNet 4

0.068

LeNet 4 / Local

--- 12 MBytes --->

LeNet 4 / K-NN --- 12 MBytes --->

LeNet 5

Boosted LeNet 4

K-NN Euclidean --- 12 MBytes --->

Tangent Distance --- 25 MBytes --->

Soft Margin --- 11 MBytes --->

0 0.1

0.24 0.21
0.2 0.3

0.44 0.49
0.4 0.5 0.6

Figure 5: Memory requirements for classi cation of test patterns (in MBytes). Numbers are based on 4 bit/pixel for K-NN, 1 byte per pixel for Soft Margin, and Tangent Distance, 4 byte per pixel for the rest.

of the methods. Memory requirements for the neural networks assume 4 bytes per weight (and 4 bytes per prototype component for the LeNet 4 / memorybased hybrids), but experiments show that one-byte weights can be used with rneoqusiigrensi tchaenlteachstanmgeeminoryer.ror rate. Of the high-accuracy classi ers, LeNet 4
5 Conclusions
This paper is a snapshot of ongoing work. Although we expect continued tchhaatngaerse ilnikeallyl taospreemctsainofvarelicdogfonritsioonmetetcihmneo.logy, there are some conclusions
Performance depends on many factors including high accuracy, low run time, and low memory requirements. As computer technology improves, largertcraapiancinitgy sreetcso.gnLiezNerestb1ecwoamseafpeparsoibplrei.atLeartgoetrhreecaovganiliazebrlse itnecthunronlorgeyquirvee lyaergaersr ago, just as LeNet 5 is appropriate now. Five years ago a recognizer as complex as LeNet 5 would have required several months' training, and more data than was available, and was therefore not even considered. locaFlolreaqruniitneg aclalosnsig etri,mteh,eLoepNtiemt a1l mwaasrgcionncsliadsesrieder,thaendsttahtee toafntgheentadrti.staTnhcee classi er were developed to improve upon LeNet 1 { and they succeeded at that. However, they in turn motivated a search for improved neural network vaarcrhioituesctleuarrens.ingThmisacsheianrecsh, dwearsivgeudidfreodminmpeaarsturbeymeesnttismoafttehseotfrtahineincagpaancditytesotf error as a function of the number of training examples. We discovered that more capacity was needed. Through a series of experiments in architecture, combined with an analysis of the characteristics of recognition errors, LeNet 4 andWLeeNentd5thwaetrebocroasfttinedg.gives a substantial improvement in accuracy, with a relatively modest penalty in memory and computing expense. Also, distortion models can be used to increase the e ective size of a data set without actually taking more data.

The optimal margin classi er has excellent accuracy, which is most remarkapbriloer,ibkencaouwsleedugneliakbeotuhtetohtehperrohbiglehmp.eIrnfoframcta,ntcheisclcalsassisieresr,witoduoldesdnoojtuisntcalus dweelal if the image pixels were permuted with a xed mapping. It is still much slower eaxnpdemcteedmoarsythheuntgecrhyntihqaune itshereclaotnivvoelluytnioenwa.l nets. However, improvements are
Convolutional networks are particularly well suited for recognizing or rejecting shapes with widely varying size, position, and orientation, such as the ones typically produced by heuristic segmenters in real-world string recognition systems (see article by L. Jackel in these proceedings).
When plenty of data is available, many methods can attain respectable accuracy. Although the neural-net methods require considerable training time, trained networks run much faster and require much less space than memorybased techniques. The neural nets' advantage will become more striking as training databases continue to increase in size.

References

B. ECm. oaBmloMpseuart,ragtIii.noGnCaullayLsosenia,renarinsn,dginVT.hPerNoo.rcyeVe5dapi1nn4gi4ks-,1o5fA2t,hTPeriaFtitinsftibnhugrAgAhnlng(uo1ar9il9t2hW)m.orfkosrhoOppotin-

L. Bottou and V. Vapnik, Local Learning Algorithms, Neural Computation 4,
888-900 (1992).

C. Cortes and V. Vapnik, The Soft Margin Classi er, Machine Learning, to appear (1995).

R. O. Duda and P. E. Hart, Pattern Classi cation and Scene Analysis, Chapter 4, John Wiley and Sons (1973).

H. Drucker, R. Schapire, and P. Simard, Boosting Performance in Neural Net-
gweonrckes,7In7t0e5r-n7a2t0io(n1a9l9J3o).urnal of Pattern Recognition and Arti cial Intelli-

I. Guyon, I. Poujaud, L. Personnaz, G. Dreyfus, J. Denker, and Y. LeCun, Comparing Di erent Neural Net Architectures for Classifying Handwritten Digits, in Proc. 1989 IJCNN II 127-132, Washington DC. IEEE, (1989).

Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, R. Hubbard, and L. D. Jackel, Handwritten digit recognition with a back-propagation
Sneytswteomrks, 2in, MD.oTrgoaunreKtzakuyfm(eadn),, (A1d9v9a0n).ces in Neural Information Processing

Yuchum Lee, Handwritten Digit Recognition using K-Nearest Neighbor, Radial-
tBaatsioisn,F3u,n3ct,i(o1n9s9,1a)n. d Backpropagation Neural Networks, Neural Compu-

E. Sackinger and H.-P. Graf, A System for High-Speed Pattern Recognition and Image Analysis, Proc of the fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, IEEE (1994).

J. ScRheuardminagn,nI,EAEEMTurlatin=s.F,oGn2t7W, 3or(d19R78ec).ognition System for Postal Address

P.

Simard, Y. LeCun, and J Denker, E New Transformation Distance, Neural

cient Pattern Recognition Using Information Processing Systems

5a,

50-58, Morgan Kaufmann (1993).

