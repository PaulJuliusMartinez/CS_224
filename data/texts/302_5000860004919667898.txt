Computer  Experiments and Global Optimizat ion 

Matthias Schonlau 

A  t hesis 

present ed t O  the University  of  Waterloo 

in fulfilment  of  the 

thesis  requirement for the degree of 

Doctor of  Philosphy 

in 

Statistics 

Waterloo, Ontario, Canada,  1997 

@Matthias Schonlau  1997 

1+1  ,cm, 

National Library 

Bibliothèque nationale 
du Canada 

Acquisitions and 
Bibliographie Services 
395 Wellington Sîreet 
Ottawa ON  K1A O N 4  
canada 

Acquisitions et 
services bibliogaphiques 
395. nie Wellington 
W w a O N  KlAON4 
Canada 

Yourtue  voas nilérrmar 

Our 6le  Notre réUrmœ 

The author has granted a non- 
exclusive licence allowing the 
National L i b r q  of Canada to 
reproduce, loan, distn'bute or sell 
copies of this thesis in microform, 
paper or electronic formats. 

L'auteur  a accordé une licence non 
exclusive permettant à la 
Bibliothèque nationale du Canada de 
reproduire, prêter, distribuer ou 
vendre des copies de cette thèse sous 
la fome de microfiche/nlm, de 
reproduction sur papier ou sur format 
électronique. 

The author retains ownership of the 
L'auteur  conserve la propriété du 
copyright in this thesis. Neither the 
droit d'auteur  qui protège cette thèse. 
thesis nor substantial extracts fiom it  Ni la thèse ni des extraits substantiels 
may be p ~ t e d  or otheMrise 
reproduced without the author's 
permission. 

de celle-ci ne doivent être imprimés 
ou autrement reproduits sans son 
autorisation. 

The University of Waterloo requires the signatures of all persons using  or ph* 

tocopying this thesis.  Please sign below, and give address  and date. 

Abstract 

A  complex  mathematical  mode1 that  produces  output  values  from  input  values 

is  now  commonly  c d e d   a  computer  model.  This  thesis  considers  the  problem 

of  h d i n g  the global  optimum  of  the  response  with  few  function  evaluations.  A 

s m d  number of function evaluations is desirable since the computer rnodel is often 

expensive (time consuming)  to evaluate. 

The function  to be  optimized  is  modeled  as  a  stochastic  process  from  initial 

function evaluations.  Points  are sampled sequentially  according  to a criterion  that 

combines promising  prediction  values with  prediction  uncertainty.  Some grap hical 

tools are given that d o w  early assessrnent about whether the modeling strategy wiU 

work  weIl.  The approach is  generalized  by  introducing  a parameter  that  controls 

how  global versus local  the search strategy is.  Strategies to conduct  the optiiniza- 

tion  in  stages  and  for  optimization  subject  to  constraints  on  additional  response 

variables  are presented. 

Special  consideration  is  given to the stopping  criterion  of  the global  optimiza- 

tion  algorithm.  The problem  of  achieving a tolerance  on  the global  minimum  can 
be  represented  by determining  whether  the  first  order  statistic  of  N  deperdent 

variables  is  greater  than  a  certain  value.  An  algonthm  is  developed  that  quickly 
det ermines bounds  on  the probability  of  this event . 

A  strategy to explore high-dimensional  data informdy through  effect  plots  is 

presented.  The interpretation of the plots is guided by pointwise  standard errors of 

the effects which  are developed.  When used in the context  of  global optimization, 

the graphical analysis  sheds light  on the number and location  of  local optima. 

Acknowledgement s 

My  thesis  would  have been  impossible  without  the invaluable  guidance of  my  su- 
pervisor  William  J. Welch.  His  overflowing wealth  of  ideas, his remarkable  ability 
to teach scientific writing,  and his  insight  into numerical  aspects of  programming 

have greatly influenced both me and my thesis. 

1 thank my thesis cornmittee.  Angela Dean (external examiner)? Jerald  F. Law- 
less,  Lara  J. Wolfson,  and  Henry Wolkowicz  for  their  carefd reading  and  helpful 

suggestions.  Markus  Abt gave valuable comments on an earlier  draft  of  Chapter 3. 

1 am  very gratefd  to Michael Hamada.  University of  Michigan  at Ann  Arbor. 

for  involving  me in  the  solar  collector project  which  now  forrns  Chapter  3  of  niy 

t hesis.  His excitement for research is exceptiondy cont agious. 

1 am  very gratefd to Donald  R.  Jones,  General Motors  RSrD  Center. Warren. 

Michigan, for his initial involvement of what  now forms Chap ter 4, in particiilar  for 

pointing  out  the e q e c t e d  improvement algorithm.  Donald  Joues also went  out of 

his way to provide  us  with  the Piston Data set for Section 5.4.1. 

1 also  thank  S.  Cao  (now  at  the  National  Research  Council  of  Canada)  and 
T. Hollands,  Solar Thermal  Lab, Department of  Mechanical Engineering,  Univer- 
sity of  Waterloo,  who  were  involved  on  the engineering side in  the solar collector 

application. 

A  big  thanks goes also to Robert Zvan, Department  of  Computer  Scieilce. Uiii- 
versity of Waterloo, and the MF'CF cornputer consultants who patiently put iip with 
all my programming questions.  A late night  pub discussion with Edward Cam. De- 

partment of  Computer  Science, University of  Waterloo, led to the formulation and 

proof  of  Theorem 3. 

My parents still think 1 should have done my Ph-D. in  Germany but supported 
me unconditionally nonetheless.  I find that truly remarkable and 1 am most gratefd 

for  their support. 

Rekha  Agrawal  was  a fantastic &end  throughout  all these years.  The German 

saying "someone to steal horses with"  alludes to the tmst and confidence bestowed 
upon someone you would be willing to commit  a crime with.  Rekha, 1 would steal 

horses with you. 

Contents 

1  Introduction 

2  Review 

2.1  The Analysis  of  Cornputer Experiments  . . . . . . . . . . . . . . . .  

1 

3 

3 

2.2  Stochastic Methods of  Spatial Prediction  (Kriging) 

. . . . . . . . .   12 

3  Understanding  Key  Features  of  Computer  Codes  via  Graphical 

Analyses 

14 

3.1 

3.2 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

Introduction 
13 
Identifying Key Features of  Computer Codes  . . . . . . . . . . . . .   17 

3.3  Estimates for  Effects and their  Standard Errors  . . . . . . . . . . .   21 

3.4  Application  to a Solar Collecter  Code 

. . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

3.5  Discussion 

4  A  Data Analytic  Approach  to Bayesian  Global Optimization 

4.1 

Introduction 

4.2  Expected Improvement Algonthm 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . .  

4.2.1  Modeling  Approach 

4.2.2  Expected Improvement 

23 

31 

37 

37 

35 

40 

41 

4.3  Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   44 
4.4  Examples  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   46 
4.4.1  Branin Function  (Br) . . . . . . . . . . . . . . . . . . . . . .   46 
4.4.2  Goldstein-Price  Function  (Gp )  . . . . . . . . . . . . . . . . .   51 
4.4.3  Hartman 6 Function  (H6)  . . . . . . . . . . . . . . . . . . .   58 
4.4.4  ShekellOFunction(Shl0)  . . . . . . . . . . . . . . . . . . .   60 
4.5  Discussion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   65 

5  Extensions to Bayesian Global  Optimization 

67 

5.1  Introduction  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   67 

5.2  Generalized Expected Improvernent  . . . . . . . . . . . . . . . . . .   68 

5.3  Sequential Design in Stages 

5.2.1  Example:  Golds tein-Price  Function  . . . . . . . . . . . . . .   71 
. . . . . . . . . . . . . . . . . . . . . .   73 
5.3.1  Example:  Goldstein-Price  Function  . . . . . . . . . . . . . .   76 

5.4  Minirnization  Subject toconstraints 

. . . . . . . . . . . . . . . . .   77 

5.4.1  Example:  Piston  Application 

. . . . . . . . . . . . . . . . .   79 

6  Fast Evaluation of the CDF of the Minimum of N Dependent Vari- 

ables 

88 

Introduction 

6.1 
6.2  A  Basic  Algorithm 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

58 
. . . . . . . . . . . . . . . . . . . . . . . . . . .   90 
6.2.1  Computing  the CDF of  a h s t  order  statistic  . . . . . . . . .   90 
. . . . . . . . . . . . . . . . . . .   91 
6.2.2  Upper  and  Lower Bounds 
6.2.3  Evaluating  the CDF of  a Multivariate  Distribution 

. . . . .   93 

6.3  Reducing  the Number of  Terms to be Evaluated 

. . . . . . . . . .   94 
6.3.1  Reduction  1 . . . . . . . . . . . . . . . . . . . . . . . . . . .   94 
6.3.2  Reduction  2  . . . . . . . . . . . . . . . . . . . . . . . . . . .   95 

6.4  Algorithmic  Considerations  . . . . . . . . . . . . . . . . .  -  . . .  .  97 
6.5  An Example Based on Hypothetical Data . . . . . . . . . .  . . .  .  .  99 
6.6  A  Stopping Rule Based on a Fkst Order Statistic  . . . . . . . .  . -  101 

6.6.1  Examples: Branin and Goldstein-Pnce function  . . 

Concluding Remarks 

On Programming 

Addendum  to Section 3.3 

Proof of Theorem 3 

A  Splus Function for the Visualization  of High Dimensional  Datai19 

Derivation  of the  Generalized  Expected Improvement 

D eiik's  algorit hm 

121 

124 

List  of Tables 

Data-Analytic  Bayesian Approach:  Function Evaluations and Toler- 
ances for Test  Functions  . . . . . . . . . . . . . . . . . . . . . . . .  

Various  Global  Optimization  Algorithms:  Function  evaluations  for 

Test  Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

Hartman 6 Function:  Coefficients  . . . . . . . . . . . . . . . . . . .  

Shekel 10 Function:  Coefficients  . . . . . . . . . . . . . . . . . . . .  

Ln  Goldstein-Px-ice  Function:  Cornparison  of  Minimizations  where 
g = l . 2 .  5  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

The First Order Statistic S topping Rule Applied  to the Branin Fii~ic- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
tion 
The First Order S t atistic S topping Criterion Applied  to the Golds tein- 
Price Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

B . 1  Examples for the Use of  Theorems 1 and 2  . . . . . . . . . . . . . .   116 

List of Figures 

3.1  Solar Application:  Scatter Plots  . . . . . . . . . . . . . . . . . . . .   15 
3.2  Solar Application:  The Latin  Hypercube Design  . . . . . . . . . . .   25 
3.3  Solar Application:  Main  Effect Plots  . . . . . . . . . . . . . . . . .   26 
. . . . . . . . . . . . . . . . .   27 
3.4  Solar Application:  Joint Effect  Plots 
3.5  Solar  Application:  Main Effect  Plots . Method  2  . . . . . . . . . . .   28 
3.6  Solar Application:  Cross Validation Predictions from the Parametric 

Nonlinear  Model 

. . . . . . . . . . . . . . . . . . . . . . . . . . . .   30 
3.7  Solar Application:  Main Effect Plots from the GAM Approach  . . .   32 
3.8  Solar Application:  Cross Validation  Predictions fkom the GAM Model  33 

3.9  Solar Application:  Cross Vakdation Predictions From  the Stocliastic 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

Processes  mode1 

34 

3.10  Solar  Application:  Added Variable Plot  for x4  . . . . . . . . . . . .   35 

4.1  Branin Function:  Contour Plots of  the Estimated  and True Function  48 

4.2  Branin Function:  Diagnostic  Plots 
4.3  Branin Function:  Final Experimentd Design  . . . . . . . . . . . . .   50 

. . . . . . . . . . . . . . . . . . .  

49 

4.4  Goldstein-Price  Function:  Contour Plots of  the Estimated and Triie 

Function  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   53 
4.5  Goldstein-Price  Function:  Diagnostic Plots  . . . . . . . . . . . . . .   55 

4.6  Ln  Goldstein-Price  Function:  Diagnostic Plots  . . . . . . . . . . . .   56 

57 
4.7  Ln  Goldstein-Price  Function:  F i n d  Experimental Design 
4.8  - In(-y)  Hartman 6 Function:  Main effects  . . . . . . . . . . . . .   59 
4.9  -In(-y)  Hartman 6 Function:  A  Joint  Effect and its Standard Error  60 
4.10  -lu(-y)  Hartman 6 Function:  Final Experimental Design 

. . . . . .  

. . . . .  

61 

4.11  Shekel 10 Function:  Marginal Plot of the "Sharp Welln at the Global 

Minimum 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  

63 

4.12  Inverse Shekel 10 Function:  Final Experimental Design 

. . . . . . .  

64 

Ln  Goldstein-Price  Function:  Final Experirnental Design with g = 2 
Ln  Goldstein-Price  Function:  Final Experimental Design  with g = 5 

72 

73 

Ln  Goldstein-Prïce  Function:  Final  Experimental  Design  in  Stages 
w i t h g = 2   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   76 

Piston  Application:  Contour Plot  of  the True Functions 

. . . . . . .  

80 

Piston  Application:  Perspective Plot  of prnax 

. . . . . . . . . . . .   S 1  

Piston  Application:  Diagnostic Plots  for rnp  . . . . . . . . . . . . .   52 

Piston  Application:  Diagnostic Plots  for p a x  . . . . . . . . . . . .   83 
Piston  Application:  Final Experimental Design for g = 2  . 
Piston  Application:  Final Experimental Design for g = 5  . 

85 

86 

6.1  Generating All  Possible  Subsets of  Four Variables in  a Tree S triicture  98 
. . . . . . . . .  100 

6.2  Algorithm  for Computing  the First  Order Statistic 

Chapter  1 

Introduction 

This thesis is about global op timization of expensive-to-comput e computer models . 

The approach  that  we  take is  closely  Iinked  to  computer  experiments in  that  we 

repeatedly  use methodology developed  for computer experiments for  modeling  the 

unknown  function  to be optimized.  The thesis  is  organized  as follows: 

Chapter 2 @es  a review of  the analysis of computer experiments and corrirnents 

briefly on  the connection with  Kriging,  a stochastic method of spatial  prediction. 

As  well as showing how to identify key features of  computer model;.  Chapter 3 

presents an illustrative exarnple for modeling a computer experiment.  Even though 

Chapter 3 is not about optirnization, some of its aspects (particularly visualization) 

are inherently useful for the data-analytic approach to optirnization  that we adopt 

later  on.  Along  the way  we  introduce novel  methodology  for  attaching  staiidard 

errors for the estimates of  main effects and interactions.  We aIso present  a method 

for finding a  suitable  nonlinear  regression  model  when  the functional  relationship 

between response  and explanatory variables  is  unknown. 

Chapter  4  presents  a  dat a-analytic  approach  for  global  op timization.  Novel 

aspects include the use of  diagnostics  before optimizing  an expensive- to-corriptit e- 

CHAPTER 1.  INTRODUCTION 

2 

function, using methodology for the analysis of computer experiments in the context 

of  optimization, and the a d a b i l i t y  of  a reliable  stopping rule. 

Chapter 5 takes this approach to global optimization a step further adding three 

novel  aspects:  (i)  a  parameter that  controls  the balance  between  local  and  global 

components of  the optimization;  (ii) methodology for optimization in stages rather 

than one-point-at-a-time;  and (iii) optimization subject to constraints on additional 

response variables. 

Chapter 6 gives  an algorithm  for the evduation of the CDF of  the minimuni of 

N  dependent  variables.  The algorithm  is  particularly  fast  when  it  is  sufficient  to 
specify whether  certain bounds  on  the CDF are met.  The algorithm  incorporates 

three ideas  that  make it  fast  and therein  lies  the novelty.  While  this  is  a  topic  in 

its own right,  we show  how  it  can  also be  used  as  an alternative  stopping  rule  for 

the global optimization algorithm  introduced in  Chapter 4. 

This thesis is computationally  intensive.  The examples shown  throughotit  were 
generated wit h  the following software:  ACED  (Algorit hms  for  the Constructioii  of 
Experiment al Designs) , software developed by William J . Welch? was used t hroiigh- 
out  the thesis  for  aIl  design  aspects for  computer experiments.  GASP  ( GAiissiaii 
Stochastic Processes),  also  software by  William  J. Welch,  was used  in  Chapter 3 

for  the  analysis  of  computer  experiments  and for  Figures  4.8  and  4.9.  SPACE 
(Stochastic Processes  Analysis  of  Computer  Experiments) , software  by  Matthias 
Schonlau, was used for the analysis  and optimization throughout  except for Cliap- 
ter 3  and Figures  4.8  and 4.9.  Appendix A  contains  a brief  overview of  the major 

components that a computer program  for the design,  analysis  and optimization  of 

computer models  must contain. 

Chapter 2 

Review 

2.1  The Analysis  of Computer Experiments 

A  complex  mathematical  model  that,  given  a  set  of  input  values,  produces  a  set 

of  output  values  is  now  commonly  referred  to  as  a  computer  model.  The  name 

stems  fiom  the  necessity  to  have cornputers  do  the  extensive  computations.  as 

almost  always  the  mode1 cannot  be  written  in  closed  form  and/or  it  requires  an 

iterative solution.  Computer models are distinct from models of data from physical 

experiments in  that  they are often not  subject to randorn  error.  A  computer (the 

same computer architecture) fed with the same input will  always produce the sarne 

output.  Due to the lack of  random  error, traditional  modeling  approaches  are not 

useful.  For  example,  one  of  the  principles  of  design  of  experiments,  replication. 

leads to redundant information  in computer experiments. 

This section gives a overview of the analysis of computer experiments.  Relevant 
references include Currin et al.  (lggl), Mitchell and Morris (l994), Morris. Mitchell. 

and  Yhisaker  (1993), Sacks, Schiller,  and Welch  (1989),  Sacks  et  al.  (1989). and 

Welch et al.  (1992). 

The Deterministic  Mode1 
The data from a computer experïment consist  of  n  vectors  of  covariate values  (or 
inputs) denoted by xi,. . . , X ,  for d covariates  and the corresponding response val- 
ues (or outputs) y = (yl, . . . , y,)'.  Then the response is modeled  by a linear model 
plus departures from the linear model: 

Response  = Linear model + Systematic Departure. 

One convenient way  of  expressing  the systematic  departure function is  to  view  it 

as  a sample  path  fiom  a suitably  chosen  stochastic  process.  This  point  of  view. 

namely  the resemblance  of  the systematic departure to  a realization  of  a  random 

function, respects  the deterministic  nature of  a  computer code.  since a reakzatioii 

of  a stochastic  process  is  deterministic.  but  provides  a  s tocliastic  frarriework  for 

assessing  uncertainty.  The model can be written forrnally as: 

[ f  (x), - - -  , f ( x ) ]  

are 

k 

where 
functions. 
(Pi, P 2 ,  .  . , ,Bk) are  the  corresponding  parameters.  and  Z(x) is  a  stochastic  pro- 
cess.  As a notational  convention we write vectors and matrices in  bold let ters.  The 
covariance between the 2's  at two inputs x = (xl,. . . , xd), and x'  = (z;. . . . .zd) is 

regression 

known 

Cov(Z(x), Z ( x 1 ) )  = uiR(x, x'), 

(2.2) 

where  R(-, *)  is  a  correlation  function  that  can  be  tuned  to  the data  and  ni  is  a 

scale factor, also called the process  variance. 

We require the stochastic process to be stationary. which implies that E ( Z ( x ) )  = 

CHAPTER 2.  REWEW 

5 

0,  and that  the  covariance  between  the  2's  at  points  x  and  d depends  only  on 
x - x',  that is on  their relative location,  not  on x and x',  that  is  on  the locations 
themselves.  For  computational  reasons  it  is  convenient  to  choose  a  correlation 

function that adopts the so called produet  correlation rule: 

While there are many choices, a sufficiently flexible and commonly used correlation 

family is  the following: 

where Bj  2 O and O  < pj  5 2.  The pj's  can be interpreted as smoothness parameters. 
The response  surface is  smoother with  respect  to  xj as  p j   increases.  In  fact.  the 

correlation  function  and  hence  the  sample  path  Z  is  infinitely  differentiable  for 

p  = 2  in  a  rnean  square sense.  The 6's  indicate how  local  the estimate is.  If  the 

0's  are large, only data points in the immediate vicinity of  a given point  are highly 

correlated  with  that  point  and are thus influentid on  the prediction  of  that  point. 

If  the 0's  are srnall, points further away are still highly correlated and still influence 

the prediction  of  that  point. 

The deterministic nature of  the problem  is  kept  because  R ( x ,  x) = 1.  For  tliis 

reason,  the predictor is an interpolator. 

We use the correlation family (2.3) throughout the thesis.  With two parameters 

for each dimension this family is very flexible yet it is  not  too  costly for parameter 

estimation  (see dso the discussion  in Sacks, Welch,  Mitchell, and Wynn.  1989). 

Measurernent Error 

In the presence of  measurement error equation  (2.1)  can be easily  modified  to 

where Z is  the systematic departure and c  is  the measurement  error.  In that  case. 
the variance  must  reflect  this  change,  Var ( Y ( x ) )  = u:  + oz, and  the covariance 
becomes 

Cov(Y(x),Y(xt)) =  o:R(x,x') 

Cov(Y(x),Y(x))  =  a: 

Var(Y (x))  =  a, + oc. 

2 

2 

where  the  covariances  (2.5)  and  (2.6)  strictly  refer  to  two  distinct  observatioiis 

(which for  (2.6)  are replicates), the variance  (2.7) refers  to  only  one  observatioxi. 

cr:  is  the process  variance  as defined  in  (2.2), a:  is  the  error  variance'  and  R  is 
the  correlation  function  as  defined  in  (2.3).  Equivalently,  the  correlation  in  the 

presence of  measurement error is 

Cor(Y(x), Y (x'))  =  2 R ( x ,  x') 

u 
cr2 

where  the correlations in  (2.8) and (2.9) strictly refer  to two distinct observations 
(which in (2.9)  are replicates), and o2 = o:  + G:. 

The fiaction  01/n2 constitutes  an  additional  correlation  parameter;  its  value 
has to be optimized as well.  Except for this redefinition  the randorn error model is 
treated just  like  the deterministic model. 

CHAPTER 2.  REWEW 

Best  Linear Unbiased Predictor 

We now  introduce some more notation,  before we derive the best  linear predictor 

of  Y at an untried input  x.  Let 

F =  

be the n  x k expanded design matrix, let 

be  the vector  of  correlations between  the 2's at the design  sites XI,  . . . . x,.  let 

be the vector of k known regession functions, let R be the n x n correlation matrix 
with  element i, j d e h e d  by  R ( x i , x j )  in  (2.3) and let  the untried  input  be x.  For 
data y = (x,. . . , Y,)t, the mode1 in  (2.1) is  written  as 

where  F is  the  above  defined  expanded  design  matrix,  P  = (P1,P2,. . . !Ok) the 
correspondhg  parameters,  and  z  = (Z(xl), . . . , Z(x-))'  the  stochastic  process. 

For  any  linear predictor  c i y  of  Y ( x )  the mean squared error of prediction  is  : 

The last equation follows if we impose the unbiasedness constraint F' c,  = f..  Tlùs 

constraint follows from  equating 

and 

for all B. 

Introducing  lz  Lagrange  multipliers  X  for  the k equations  FtcX = fz and taking 

the derivative with respect  to c, in  (2.11) yields 

o : ~ c ,  - afr, - FA  = O 

. 

Together with the unbiasedness constraint we have a system of two sets of equations 
in the two unknown  vectors c,  and X  : 

CHAPTER 2.  REVTEW 

We rewrite this system in matrix form  : 

The best  linear unbiased predictor is  then 

This can also be written as 

- 

where p = (FtR-lF)-lFtR-ly is  the generalized least  squares estimator of P .  It 
turns out it is  also the MLE,  as  will be  derived later. 

The MSE of  the estimate can be derived by  substituting  (2.12) in  (2.11): 

Another  way  of  looking  at  this  is  to  consider  y  and  Y ( x )  together.  assiimirig 

they  are jointly  normally distributed: 

Shen, we can understand  Y(x) as  the conditional  expectation of  I ( z )  = Y(+)ly. 
More precisely, 

where 

Equations  (2.16)  and  (2.14)  are equivalent .  Equation  (2.15) differs  from  (2.17). 
because the estimation  of p is ignored in the latter case. 
Maximum Likelihood Estimation 

We  consider now  the problem  of  finding  maximum  likelihood  estimates  of  the  iin- 
. . . .Od)  in  (2.3).  p  = 
known  parameters:  ,B  in  ( 2 4 ,  O,  in  (2.2). 0  =  (BI,&. 
(pl, p2, . . . , pd) in  (2.3), and  in  the  case  of  random  error  $ in  (2.9).  .4ssiirriing 
the stochastic process is  Gaussian,  the Ioglikelihood  up  to an additive constant  is 

Given the correlation parameters O  and p, by differentiation with  respect to 0. the 
MLE of 

is the generalized least  squares estimator 

The MLE of  cr:  is 

CHAPTER 2.  REVlEW 

If  we substitute 5; and 

back  into (2.18) we obtain 

This  function  of  the  data and  the-correlation parameters  B  and  p  has  to  be  nu- 

merically  maximized.  Direct  maximum likelihood estimation  is  very  expensive to 
cornpute.  Hence, an algorithm which introduces  the parameters sequentially  is  of- 

ten introduced  to cut  down  on  computing  time.  For  example,  see  the algorithm 

described in  Welch  et d. (1992). 

Bayesian Approach 

Alternatively, instead of viewing y as a realization of a stochastic process. one can 

take a Bayesian point  of  view and predict  y(x) by  the posterior  meai 

where y. denotes data at the design points.  Currin et al.  (1991) take this approacli. 

representing prior uncertainty by a Gaussian stochastic process with k e d  mean and 
variance, thus without  a pnor to represent  the uncertainty  in  the mean. û arid  p  . 

Because  of  this,  they  corne  to  the  same result  as  displayed  in  equations  (2.16) 

and (2.17).  Estimation  of  the parameters  is  then also performed  by  the Maximum 

Likelihood  method.  See ais0 Morris, Mitchell, and Ylvisaker  (1993) in  the context 

of  the Bayesian point  of  view. 

2.2  Stochastic  Methods  of  Spatial  Prediction 

(Kriging) 

The stochastic process model presented in the previous section has traditionally 

been  used  in  geostatistics  under  the  name of  Kriging  for  the  exploration  of  gold 

mines,  oil  fields,  etc..  Since  this  stochastic  process  model  is  used  frequently  in 
this  thesis,  a  bnef  overview  of  Kriging  is  given  here.  For  more  extensive  reviews 

the  reader  is  referred  to  Cressie  (1993),  Journel  and Ruijbregts  (1978),  and,  for 

nonlinear Kri,&g, 

also Rivuirard  (1994). 

The  word  "Kriging"  is  synonymous  with  optimal  spatial  prediction. 

It  has 

been  termed  after a South-Afican  mining engineer with the name Krige.  who first 

popularized  stochastic methods for spatial prediction. 

When  the underlying  stochastic process is  Gaussian  and a quadratic Ioss  func- 
tion  is  chosen, then  an  optimal  predictor  is  given  by  E ( Y ( x ) l y , ) .  Because  of  the 

Gaussian assumption, the predictor is  a linear function of  x. 

Usudy the following additional  model  assumption is made: 

where Z(.) is  a random process  with mean  0, and p ( . )  is a parametric  mode1 spec- 

ifying the mean  structure. 
Simple Kriging 
The simplest Kriging models are ones where the mean structure p (x) and the covari- 
ance  structure Cov( Z(x) , Z(YC')) are assumed  known. Furt hermore,  the predict or 
is  assumed  to be a linear  function of  the data. 

CHAPTER 2.  REVIEW 

The optimal predictor  can then be derived as 

Often the mean  and the covariance  structure are not  known.  The ordinary  Krig- 

ing  method  therefore  relaxes  the assumption  of  full  knowledge  of  mean  and  the 

covariance structure. 
Ordinary Kriging 
The  mean  p ( x )  is  unknown,  but  assumed  constant.  The  ruidom  function  Z  is 
stationary.  The predictor is a linear function of  the data and is uniformly unbiased. 
i.e.  E(ij(x)) = p. 

This  method  no  longer  requkes f d  knowledge of  the mean:  however.  it  on?y 

allows  for  stationary  models.  In  the following  method.  a  class  of  non-stationary 

models is introduced  through a  nonst ationary mean structure. 
Universal  Kriging 

The mean structure is  given  by 

The random  function  Z  is  stationary.  Furthermore,  the predictor  is  linear  in  the 

data and uniformly  unbiased. 

The analysis of computer experiments uses the Universal Kriging approach.  Un- 
like Kriging models in geos tatis tics,  however, computer experiments are considered 

t o  be deterministic.  This difference is reflected in the covariance structure.  Another 

clifference is that correlations for Kriging are u s u d y  estirnated by variograms (e.g.. 

Cressie,  1993)  whereas  computer  experiments  typicdy  use  maximum  likelihood 

estimation. 

Chapter 3 

Understanding Key Features of 

Cornputer  Codes via Graphical 

Analyses 

3.1  Introduction 

Cornputer  models  or  codes are now  frequently  used  in  engineering  design,  and  in 

many  other  areas  of  physical  science.  For  instance.  the  main  example  discussed 

here  concerns  the engineering design  of  a solar  collecter.  This  code  computes  an 

increase in heat  transfer effectiveness, y, resulting from  an engineering  innovation. 
The  design  is  charactenzed  by  six  factors  (engineering  parameters)  XI. . . . . xc. 
Further details will be given in Section 3.  As is often the case, the code is expensive 

to  cornpute and  the engineers wanted  to understand  key  features  of  the cornplex 

functional relationships  embodied in their cornputer code.  In particular,  they were 

interested in  possible  nonlinearities  and interactions. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

15 

Figure 3.1 shows scatter plots of the response against each x variable in t iirn for 

Figure 3.1:  Scatter Plots of  y versus  xi, i=l  . . . 6. 

data from  an experiment  on  the solar  collecter  code.  They indicate some trend in 

the relationship  between y  and xz and xs. However, the scatter plots  do not  show. 

for example, the strong relationship  in  2 4 ,  because it is masked by the effects of the 
other covariates.  This would not  matter if  the effects were d linear  and  additive. 
but, as we s h d  see in Section 3, the effect of x4 is highly nonlinear.  With nonlinear 

effects, we need to know the form of  the mode1 to be fitted, and simple plotting of 

the data does not  suggest  a class  of  nonlinear  parametric  models  here.  Moreover. 

CNAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

16 

nonlinearities  are common in computer experiments, because the inputs often cover 

wide ranges. 

There is already some work on the design and analysis of cornpu ter experiments. 

See, for  example,  Currin,  Mitchell,  Moms,  and Ylvisaker  (fggl), Sacks,  Schiller. 

and  Welch  (1989),  Sacks,  Welch,  Mitchell,  and Wynn  (1989), and  Welch,  Buck. 
Sacks,  Wynn,  Mitchell,  and Morris  (1992).  The methods proposed  in  these  refer- 

ences  take into account  the deterministic  nature of  a code like  the solar  collector 

computer model.  Given the same inputs, it always reproduces  the same outptit(s). 
Typically, the code will  be expensive to run, e-g., it solves a large number of  differ- 

ential equations  which may require several hours or more of  computer  time. 

So  far  work  on  the design  and  analysis  of  computer  experiments  has  focused 

on  finding  a good  cheap-to-compute  nonpararnetric  surrogate  (i.e..  predictor)  for 

the computer  model.  In  the solar  collector  example,  however,  expianation rat lier 

than  prediction  is  the overriding objective.  The class  of  nonparametric predictors 

suggested  in  the above references and  (2.14) is  unsuitable  for this  task:  They  are 

computationally  cheap  approximations,  but  t hey  are  nonet lieless  niatlierriat icaily 

complex. 

In this  chapter  we  propose  to  explore  key  features of  computer  codes  such  as 

nonlineari ties and interactions by testing specific hypo t heses about t heir functional 

form.  To  facilit ate hypotheses  generation,  that  is  identifying  key  features.  we  in- 

troduce some new methodology for at t aching standard errors to the nonpararrietric 
estimates of the effects. W e  then construct parametric models that embody the hy- 
~othesized key features.  The parametric framework allows  us  to test  key features. 
and  thus  to  c o n h m  them.  As  will  be  shown,  the  visualization  of  effects  is  fairly 

automatic. 

An overview of the chapter is as follows.  Section 2 first outlines the nonparamet- 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

17 

ric method we  use for analyzing data fiom  a computer experiment.  It has  several 

advantages, but it is by no means the only method that might accomplish this task. 

Section 2 then explains how key features of the nonparametric model can be identi- 
fied graphically  and confirmed by building parametric models.  Section 3 states two 

theorems, special cases of which explain how to estimate efFects and  their standard 

errors.  Section 4 demonstrates these ideas using the solar collecter code.  Section 5 

concludes with some discussion, including  comments on the choice of  experimental 

design and alternative modeling  approaches. 

3.2 

Identieing Key Features of Computer Codes 

Identifying  key features of  the relationship  between  input  and  output  variables  is 
easy  if  there  is  only  one  covariate.  A  simple  scatter  plot  reveals  the functional 

relationship,  which  for  a  computer model is  exact since  the relationship  is  deter- 

ministic.  Then the data analyst often chooses to fit a parametric model to the data. 

where a class of  (possibly nonlinear) models might be siiggested by the scat ter plot. 

This approach was used in a case study presented in Bates and Watts (1988. Section 

3.13) for physical experimental data which  contained random error.  While the data 

from  a computer experiment contain  no  random  error, the objective here rernains 

the same, Le.,  to  summarize the relationship  between  input  and  output  variables 

in  a concise way. 

Scat ter plots are not very usefd for the identification  of functional relat ionships 

where  there  is  more  than  one  covariate,  however.  The relationship  between  the 

response  and  each  covariate  can  be  masked  by  the  relationships  between  the re- 

sponse and  the other  covariates  (e.g., Montgomery and Peck, 1982, Section  4.2.5). 

To overcome the masking problem,  a plot  of a function involving only the covariate 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

of  interest is  needed.  In other words,  the effects of  the other covariates need  to  be 

eliminated.  Such plots  will be considered shortly, after some preliminaries. 

First,  a b&f  overview  of  the nonparametric  predictor  used  in  this  chapter  is 
given because it plays  a key role in  the method proposed  shortly.  The data from a 

cornputer experiment consist of n vectors of covariate values (or inputs) denoted by 
XI,.  . . , x, for the d - dimensional  covariates X I , .  . . , zd as specified by a particular 
experimental design.  The corresponding  response  values  (for  a particular  output 
variable) are denoted y = (yi, . . . , y,)t.  Then, following the approach of, e.g., Welch 
et al.  (1992), the response y is  treated  as  a realization  of  a stochastic process: 

where  E ( Z ( x ) )  = O  and  Cov(Z(w),Z(x)) = cr:R(w,x) for  two  input  vectors  w 
and x.  The conelation  function  R(-. 0 )   can  be tuned  to the data.  and  is  assumed 

here to have the form: 

where  Bj  2  O  and  O  <  pj  5  2.  The pj's  can  be  interpreted  as  smootliness 
response surface is  smoother with respect  to xj as pj increases- 
parameters-the 

and  the Bj's  indicate  how  local  the estimate is.  If  the Bj's  are large, only  data at 
points  in  the immediate  vicinity  of  a given  point  are highly  correlated  with  Y  at 

that  point  and  are thus influential  in  the prediction  at  that  point.  If  the  B j ' s   are 

s m d ,  data at points  further away are still highly  correlated  and still influence the 

prediction  at that point.  Conelation functions other than (3.2) could be chosen. for 

example Matérn  (Yaglom,  1987, p.139).  While Matérn7s correlation function  does 

CHAPTER 3.  KEY FEATURES OF COMPUTER CODES 

19 

give more control over smoothness,  it is  also  more expensive and not  clear  that it 

is a better choice in practice (see also the discussion in  Sacks, Welch, Mitchell,  and 
W p ,  1989). 

The best linear unbiased  predictor of Y at an untried x can be shown to be (see 

(2.14) with F = 1 and f,  = 1): 

where  r(x) is  the n x  1 vector  of  the  correlations  between  Y ( x )  and  y, 
is  the 
generalized  least  squares  estimator  of  P ,  R is  the n x  n correlation  matrix  with 
element  i, j  defined  by  R(xi, x j )  in  (3.2) and  1 is  an n  x  1 vector of  1's.  Except 
for very large n  this predictor is  cheap  to c o m p t e .   The cost  of  one evaluation  of 

the likelihood is of  order n3, but  the evaluation  of  the predictor  is  only of  order n. 

While  this  predictor has  proven  to  be accurate for numerous  applications.  it  does 
not reveal the relationship  between y  and zl, . . . , xd in a readily interpretable way. 

Consequently, this predictor is unsuit able for  ezplaining the functional relationship 

between  the conriates and the response. 

RecaU  that  in order  to identify  the functional  relationship  between  a group of 

covariates and the response, the effect of  these covariates needs to be isolated  froni 

the others.  When we  want  to isolate the effect of  a single  covariate, the true  main 

effect of  the covariate can be defined in the following two ways: 

1.  Integrating  out  the  other factors. The main effects are defined as: 

(Sacks, Welch, Mitchell, and Wynn, 1989). 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

20 

For  simplicity  we assume a hyper-rectangular  integration  region  in x.  Esti- 

mates  of  & ( ~ i )   and  their  standard  errors  are discussed  fwther in  the  next 

section. 

2.  Keepzng  the  other variables f i e d .   For example,  the other variables  might  be 

fixed at their respective midranges.  Standard errors for the estimated effects 
using  this  method  are available  directly  frorn MSE(I,) 
in  (Sacks, Welch, Mitchell, and Wynn,  1989). 

as given  for example 

In  both  calcdations,  the  unknown  y(*)  needs  to  be  replaced  by  Y(x) from 
Equation  (3.3).  The first  approach  may  be  preferred  because  it  is  analogous  to 

analysis of variance in that all the other covariates are averaged out.  Note also tliat 
integrating p(x) is numerically easy to perform if the x region is cuboidal aiid if the 
correlations are in product from  as  in  (3.2).  In a similar  fashion.  the effect of  two 

or  more  covariates  can be investigated  by  integrating  out  all the otlier  covariates 

or fixing the other conriates at specific values. 

Main  effects for each xi and effects of, Say,  two covariates  for each pair  (xi, z j )  

can  then  be displayed  graphically.  By  choosing a  tentative model for  each  of  the 
effect plots which  displays some key feature (i.e., impacts  the response), an o v e r d  
model can be developed by adding up all  the corresponding  candidate models.  Tlie 

standard  errors for the effects are useful here in  that  they may  guide  the choice of 

tentative models.  They are further discussed in  the next  section. 

If  there are no interactions (and hence, additivity holds) the d-dimensional prob- 

If  large  interactions  are 
lem  has  been  reduced  to  d  one-dimensional  problems. 
present , then  the interacting  covariates  need  to  be  considered jointly.  Covariates 
might  then be  grouped  so  that  covariates  in  two  difTerent groups  do  not  interact. 

Provided  that  the groups  contain  no  more  than  two  variables.  candidate  ~tiodels 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

21 

may still be identified from contour plots  of  the response.  For larger sized  groups. 
such plots  will  generally not  be helpful.  In  this case,  when  faced with  many inter- 

actions, transforming  the response rnay help  in reducing  the apparent  complexity. 

Experience with  a number of  computer models,  however,  suggests  the complexity 

of  computer models  tends to arise from additive nonlinearities  rather than tkough 

interactions. 

Subsequently,  the  key  features  summarized  in  the  parametric  mode1  can  be 

confirmed  by  fit ting  it  using  standard  nonlinear  regression  techniques.  S t artirig 

values  for  the parameter estimates can often be estimated from  the effect  plots. 

3.3  Estimates for Effects and their Standard Er- 

rors 

Suppose we want to plot  the estimated effect of  some of  the x  variables.  derioted 
by  XeReor The rernaining  x  variables? denoted  by  x0,,  . have  to  be  ixitegated  out 

of  the predictor.  The effect is 

where  V  is  the  volume of  the xout repion  over  which  we  integrate.  For  example. 

for  the  main  effect  of  r i  in  the  solar  collecter  application  with  six  explanatory 
variables,  xeneCt = x i ,  and  the  plotting  coordinates  for  the estimated mairi  effect 

of  X I ,  ji(xl), require  an integration  over  x,,,  = (x2, . . . , x6)( for  each  value  of  x l  

plotted.  The integral in (3.4) is easy to approximate if  the x-space is  cuboidal. and 

if  the correlation  function is  a product  of  correlation  functions for each  x variable. 
. . . . x,,, 
(ml 

( 1  1 
Numerically, we approximate (3.4) by a sum over a grid of m points x,,,, 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

representing  the x,,,  space.  Thus (3.4) becomes 

We now show how  to estimate (3.5)  and the standard error associated  with  the 

estimate.  In fact, we prove a more general result for estimating linear combinations 
of  K's,  which we show later c m  also be used for estimates of interactions  and their 
standard errors.  Both theorems are new  work. 
Theorem  1: The best  linear  unbiased predictor  (BLUP) of  C biK  is  bi?(xi). 
Theorem  2:  The mean squared error of C b i g  is 

where b = (bi, b 2 , .  . . , b,),  f  = Ci bifzi, and P = Ci biïZi . The standard  error  of 

- 

We  prove both  theorems in  Appendix B. 

AU  effects or linear combinations of effects can be written as C b i x  with suitable 
coefficients bi, i = 1 . . . nz. Then the estimates of  the effects are given by Theorem  1 
and their  standard errors by Theorem  2. 

For  (3.5), Theorem  1 with  bi = llm, i = 1,. . . ,m states  that  the  BLUP  of 
the effects given  by  (3.5)  is  the corresponding  sum  of  estimated  function  values. 

Moreover,  Theorem  2  gives  a  pointwise  standard  error  for  the  estimated  effect. 
Appendix B contains further examples showing how  Theorems 1 and 2 can be used 
and explains why (3.6) is easy to evaluate. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

3.4  Application to a Solar Collector Code 

In  t his  section,  the proposed  method is  applied  to an  expensive-to-compute  com- 

puter  model for  the  heat  exchange effectiveness between  the air  and  an  unglazed 

transpired-plate  solar  collector  with  dot-like  perforations  (henceforth,  referred  to 

as holes).  The use of equally spaced dot-like  holes replaces the unredistic  assump- 

tion  of  infinitesirnally s m d  and infinitesimally close holes  and  thus, represents  an 

engineering  novelty  in  the design  of  unglazed  solar  coUectors.  Golneshan  (1994) 

showed  that  the  heat  exchange  effectiveness  for  these  solar  collectors  is  a  func- 

tion  of  six  covariates,  (1) inverse  wind  velocity,  (2) dimensionless  slot  width.  (3) 

Reynolds  number,  (4) admitt ance,  (5) dimensionless  plate  t hickness.  and  (6) t lie 

radiative  Nusselt  nurnber,  as defined  by  a  system  of  differential  equations.  The 

cornputer  code  (Cao,  1993) solves  the  system  of  differe~itial equations  for  giveii 

covariate  values  and  requires  around  two  hours  of  computing  tirne on  a  worksta- 

tion.  The response  considered  here is  the increase  in  heat  exchange effectiveness 

attributed  to the heat  transfer in the holes from the hole sides and is expressed  as 

a- percentage (0-100).  For further details, see Cao  (1993). For notational sirnplici ty 
the six covariates Iisted  above will  be referred to as XI, x2, . . . . x~ and the response 

Y- 
The mechanical engineers who  had  developed  the solar  collector  code were in- 

terested specifically in explaining  the impact of the six covariates  (which are design 

factors)  on  the response,  heat  exchange  effectiveness; ultimately,  the  explanat ion 

would  help  to  identify  better  solar  collector  designs.  Note  that  such  understand- 

ing  was  not  apparent  from  inspecting  the  system  of  difFerentia1 equations.  The 

engineers  were also interested in developing a surrogate parametric  model  because 

empirical  models  of  this  type existed in  the literature for solar  collectors  based  on 

CHAPTER 3.  KEY FEATURES OF COMPUTER CODES 

24 

older technologies;  they had no  preconceived idea of  what  form  the mode1 should 

take because  the collectors with slot-like holes represented state-of-the-art  t echnol- 

ogy.  Hence,  the need  arose  for  performing  an  experiment  on  the  solar  collecter 
code, ie.,  a computer experiment. 

The experimental design  used  for  the computer  design  was  one that  filled  the 

six  dimensional  cuboidal  region,  a  swcded  space  filling  design.  Specificdy.  a 

Latin  hypercube  design  (McKay,  Beckman,  and  Conover,  1979)  consisting  of  100 

points was chosen in which the minimum distance between points (i.e.'  the covariate 

vectors) in low-dimensional projections was maximized.  The design was found using 
ACED ( Algorithms  for  Constructing  Experiment al  Designs)  which  was  developed 
by Welch.  Ali the two-dimensional  projections  of  the Latin  hypercube design  can 

be seen in Figure 3.2 which shows that  the design  is  indeed space-mng. 

Scatter plots  of  the data (Figure 3.1) indicate a possible  linear  trend in  z2 and 

xg. The remaining  relationships,  if' any,  are masked by  the presence of  the  otlier 
covariates.  In  the following,  the proposed  method  for  identifying  key  featiires  of 

the computer code will  be applied. 

The stochastic process  predictor  (3.3)  with  the  correlation  function  (3.2) was 
fit for  the response  using  the software GaSP (Gaussian  Stochastic Processes), de- 
veloped  by  Welch.  GaSP also  estimates  the  correlation  parameters  Bi  and  pi. 
j  = 1.. . d, as  well  as  u: via the maximum likelihood  approach. 

The predictor  appears  to be reasonably  accurate.  Main  effect  and joint  effects 

plots,  generated  by  integrating  out  the  other  covariates,  are  as  shown  in  Figure 

3.3 for  covariates  X I  through  16  and  Figure 3.4  for  the pairs  ( x 2 ,  xs) and  (x4, xS). 

respectively.  By joint  effect, we mean (3.5) where xenKt includes two  variables and 

xo,t  all other variables. 

The main effect for covariate XG is very flat , and all but two two-way  interactions 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Figure 3.2:  Two Dimensional  Projections  of  the Latin  Hyperciihe Design. 

are  close  to  zero  everywhere. 

These  effects  were  considered  negligible  by  the 

engineers.  The features displayed in the main  effect plots suggest  t hat the effects of 

X I   and x3 are approximately  linear  and  the effects of  x2  and  x5 are approximately 

quadratic. 

The main  effect  plot  for  x4 is  rather  ragged.  Although  the plot  gives  a good 

indication  of  the  apparently  nontinear  x4 effect,  it  is  doubtful  that  the  triie  14 

relationship  is  that  bumpy.  One  possible  explanation  is  that  the  cornputer  code 

may  have some numerical  convergence problems  in certain regions  of  the x  space. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Figure 3.3:  Main effect  plots.  The Middle  Line is  the Estimated  Effect. tlie Upper 
and  Lower  Lines are Approximate 95% Pointwise  Confidence Limits  Based  on  the 
Standard Error Given by  Theorem 2. 

This  possible  erratic  behavior  may  then  be  erroneously  attributed  to  z4 which 

clearly  has  the  most  nonlinear  or  complex  impact  on  the  response.  Engineering 

knowledge  suggests  that  the increase  in  heat  efficiency  is  a  monotone  increasing 

function  of  the admittance rate  of  the plate  x4.  The head  engineer  commented: 

"The slight  blip  in  the curve is  almost  certainly  due to some numerical  probleiri" 

(Hoilands,  1995, personal  communication).  Therefore,  we  do not  mode1 tlie lit tle 
down peak  at xq = 300. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

oadio 

o d i r  

orizo 

O- 

0.a 

Figure  3.4:  Joint  Effects  (left) and  Standard  Error  Plots  (right) for  (x2, 24 aiid 
(xq, 2 5 ) .   The Standard Error is  Given by  Theorem  2. 

Plots of  the main  effects using  the method of fixing the other variables  at their 

respective midranges rather  than averaging them out, result in very sirnilar graplis. 

For  example, Figure  3.5 shows the Method  2  main effect plot  for  z4. 

The nonlinear  shape of  the  z4 main  effect plot  which  appears  to  approach  an 

asymptote can be captured by  a Michaelis-Menten  model  (Bates and Watts,  1985. 

p.  329); the Michaelis-Menten  mode1 has long been  used  to model  the behavior  of 

a limiting  chernical reaction  which  nses  at  a  decreasing rate  to  an  asymptote.  It 

also  arises in the context of  a reciprocal link function in generalized linear models. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Figure 3.5:  Predicted Y(x) versus z4, Keeping All  Other I Variables Fixed at Their 
Midranges  (Method  2).  The Middle  Line is  the Estimated  Effect.  the  Upper  and 
Lower  Lines  are Approximate  95% Pointwise  Confidence Limits. 

where an inverse linear response function is assumed (McCullagh and Nelder.  1989. 

p.  291).  We  reparameterize  the  Michaelis-Menten  mode1  to  make the  nonlinear 

fitting numericdy easier: 

where Po = i/-yo and Pt = 7i/yo 

Both  x2 and  x5  appear  to  be  quadratic.  For  the joint  effect,  we  notice  that 

when x5 increases,  the response rises more rapidly  when  x2 is  low than when  x 2  is 

high.  This points  to  the presence of interaction.  Nonet heless, the interaction  does 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

29 

not seem very complex and we try the simplest f o m  for the interaction te=,  xlxs. 

For  the joint  effect  between  x4 and xs, we  notice  that  for  high  values  of  x5 

the sudden rise due to x4 seems to be more pronounced  than for low  values  of  2 5 .  

Hence we  hypothesize  the existence of  an interaction of  the form  h4(x4) x~ where 

hl(.) is  the main  &ect  model  for  2 4 ,   i.e.  the Michaelis-Menten  model.  For  added 

flexibility, we d o w  dinerent  parameters for the Michaelis-Menten  term in h 4 ( . )  and 

in  the corresponding main  effect term. 

To  confirrn  the key features found,  we  then  fit  the overall  model  consisting  of 

linear effects in  X I ,  xz, x3, and  xs, a quadratic  effect in  xz and  xs, the Michaelis- 

Menten model for x4, the bilinear term for (x2, x5), and the interaction term between 

the main  effect  rnodel  for  x4 and  xs using 

standard  nonlinear  regession  software 

which gave 

AU  of the parameters were significant  at the 0.0001 level, except for the multiplica- 
tive parameter for the main effect for x4 (0.0025),  which  was  margindy significant 

at the 0.05  level.  Also,  adding  xs reveals that  r6 is  not  significant  at  the -10 level. 

Further, when replacing the interaction model h4(x4)x5 with  the bilinear  term x4r5 

the latter is  not  significant.  Although  the data contain  no  random  error  so  that 

significance testing  has  no  theoretical grounds  here,  the results  of  the significance 

tests do indicate the importance of  the various effects relative  to the ability  of  the 

overall model to fit the data.  Alternatively taking  the Bayesian  point  of  view.  one 

could  calculate posterior  model  probabilities.  Note  that  the  model  contains  only 

twelve parameters but  fits  the  100 data points  quite well  as  indicated  by  the cor- 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

30 

responding  cross validation plot  given in Figure  3.6.  The fact  that  the parametric 

nonlinear  model does not  fit  the data quite as  well  as  the nonparametric model is 

not  surprising,  since  the  parametic model is  much  simpler.  The  cross  validated 
root  MSE is defined as 

Cross Validated RootMSE = J 

( ~ i  - $-i(*)12 

72 

- - -   ---  

- - - -  

where tj-i(=) 
is  the cross  validated  prediction  value  at x;  based  on  all  but  the ith 
observation.  The better the fit is, the srnaller is  the cross validated root MSE. Here 

they are .O059  for the nonparametric and .O071 for  the parametric model. 

Figure  3.6:  Cross  Validation  Predictions  from  the  Parametric  Nonlinear  Model. 
The Line  Predicted Response = Actual Response is  Shown.  The Cross Validation 
Root  MSE is  -0071 . 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

3.5  Discussion 

The examples  presented  in  nonlinear  regression  books  typicdy  deal with  only  a 

single covariate x, where the fùnctional relationship  between  x  and  the response  TJ 
is  unknown.  On  the other hand,  the method  proposed  here can  be applied  to  an 
arbitrarily large number of  covarïates. 

Throughout this chapter, we have used model (3.1) for the initial nonpararnetric 
analysis.  O ther nonparametric methods, like  GeneraIized  Additive Models.  codd 

be used.  However, the rnodel we  use  has  three main  theoretical advantages:  first. 

the mode1 is  truthful to the deterministic nature of  the data, second, error bounds 

for  the  effects  are  available,  and  third,  interactions  do  not  need  to  be  modeled 

expli ci t ly. 

For a cornparison in practice, we fit a Generalized  Additive Model (GAM) to the 

data  (see Figure  3.7).  We  choose  Generalized  Additive  Models  for  its  popiilarity 

and because  the algorithm is  readily  available  in  Splus.  The cross  validatioii  plot 

for  GAM in  Fiorne 3.8  shows  a  slight  bias  at  the  upper  and  lower  range  of  the 
response.  The cross  validated  root  MSE is  -0133, more than  twice  as large  as  the 
one for  the stochastic model  (Fiove 3.9)  and a h o s t  twice as large as  the one for 

the parametric model  (Fiorne 3.6).  The effects for the GAM Model  are the sanie 

but  they  are less  obvious.  Due  to  the  smoothing  the sudden  rise  for  low  values 

of  x4 is  not  as  clear.  At  present  GAM software  does  not  support  nonpararrietric 

interactions.  Hastie and Tibshirani (1990, section 9.5) suggest  among other things 

examining the residuals  for interaction.  Due to the lack of  error bounds  it  is more 

difficult to assess, for example, the efTect  of  z6. 

Breieiman  (1991)  criticized  algorithms  for  producing  "only  one  picture"  of  the 

functional  relationship,  thus ignoring the many O t her  "pic t ures7' which  are alrnos t 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

32 

Figure 3.7:  Main  Effect  Plots  from  the Generalized  Additive  Models  (GAM) Ap- 
proach.  Except  for  Scaling  of  the Vertical  Axes,  the Plots  Represent  the Default 
Setting in Splus.  The Rug at the Bottom Indicates Frequencies. 

as good.  The error bounds given for  the effects can  serve here as  an assessrnent  of 

the variability  of the effect fit. 

There are certainly other ways to identify key aspects of input-output  relation- 

ships.  For  example,  clever residual  analyses  in  the  hand  of  a  skilled  data analyst 

may  well  lead  to  the same results.  For  the  solar  collecter  experiment.  an  adcled 

variable  (partial regression) plot  for  x4 based  on  a linear  regression  mode1 for  the 

remaining  covariates  shows  the effect  of  x4 is  nonlinear,  albeit  with  considerable 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Figure 3.8:  Cross Validation Predictions from the GAM Model.  The Line Predicted 
Response = Actual Response is Shown.  The Cross Validation  Root  MSE is  -0133 . 

scatter  as displayed  in  Figure  3.10.  This  success  is  not  surprising  since  the  as- 

sumption  of  a  linear  model  for  the  rernaining  variables  tums  out  to  be  a  good 
approximation.  If  the true model had  cont ained several strong nonlinearities,  t hen 

added variable plots on their own would not have sufficed. It might  also be possible 

to find the interactions  with residual  analysis,  though  with  considerable difficulty. 

Elaborate  residual analyses  are often not  done for  three reasons:  (1) They  are 

hard  to  do,  especially  when  the  "true"  model  contains  more  than  one  nonlinear 

effect. 

(2)  Data  analysts,  especially  inexperienced  ones!  may  not  always  know 

about them.  (3) They  can  take a  lot  of  time to perform.  The method  presented 

here  is  easy  and  fairly automatic for  detecting  nonlinear  effects  and  interac tio~is. 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Fi,we  3.9:  Cross  Validation  Predictions  from  the Nonparametric  S tochas tic  Pro- 
cesses  Model.  The Line  Predicted  Response  = Actual  Response  is  Showii.  The 
Cross Validation  Root MSE is  -0059 . 

It  is  not  a  panacea  for  all  functional  relationships,  however.  If the relatioiiship 

cannot  be  transformed  to near  additivity  with  few  or  no  interaction  effects.  tlien 

identification  of  key  features  with  several  covariates  will still  be  a  challenge.  For 

most  of  these cases, it  is  doubtful whether aiternate methods  will work either. 

The effect  plots  play  a  key  role  in  the  proposed  method  and  their  resoliition 

depends on the experimental design used.  The Latin hypercube design is a desirable 

choice because the design points fill the experimental region well and produce high- 

resolution  plot S. 

Originally, a 4'-' 

fractional factorid design was considered for the solar collecter 

computer experiment.  While the choice of a fractional factorial or even full factorial 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

Figure 3.10:  Added variable  plot  for x4. 

design would lead to estimates that are uncorrelated,  there would have been several 

drawbacks,  however.  First, if'  only  a  few  covariates  (factors) had  an impact.  the 

design  effectively  collapses  into  a  design  in  the  active  factors  with  replications. 

But,  replications  in  a  computer  experiment  are non-informative  because  of  the 

deterministic nature of  the computer code and therefore would  have been  a waste 

of  resources.  Second,  it  could  have  been  easy  to miss  an  unknown  effect  by  only 

experimenting  at  a  few  different  points  for  each  factor.  For  example.  the  exact 

nature  of  the nonlinear  x4 effect  would  have  been  dificult  to  identify  with  oxily 

four levels; in fact, the dramatic nonlinear  behavior  of  x4 surprised  the engineers. 

Analogous  arguments apply for interactions.  Third, the decision of  where to place 

the levels  becomes  much  more  crucial  for  the  factorial  design; lower  dimensional 

CHAPTER 3.  KEY FEATURES  OF COMPUTER CODES 

36 

projections  of  Latin  hypercube  design  typicdy  consist  of  n  distinct  and  spread- 

out  points  so that their  exact position  is less important.  F i n d y ,  a 

fractional 

factorid design would have required 256 nuis.  Contrast this with the 100-run Latin 

hypercube design that was used; even fewer runs might  have been  sufficient. 

Computer  experiments typically  use  such space filling  designs  so the proposed 

method is particularly  suited to computer experiments.  While physical experiments 

typically  collect  much less  data than  computer experiments, in  principle  the pro- 

posed  method  can be  applied  to physical  experiments  by adding  a random  enor 

term to the model. 

Chapter 4 

A  Data Analytic Approach t o  
Bayesian Global  Optimization 

4.1  Introduction 

Global  optimization.  that  is  the  search  for  a  global  extremuili.  is  a  probleiii  fre- 

quently encountered.  Sometimes it  is  extremely  cost!y  to  evaluate  a  functiori  for 

an engineering design.  For example. Frank  (Davis, 1996) says about experiences at 

Boeing: 

"Designing  helicopter  blades  to achieve low  vibration  is  an extreme ex- 

ample of  a  problem  where it  is  prohibitively  expensive to compute re- 

sponses for large  numbers of  design alternatives." 

For  such applications  one is interested in  rninimizing the total nurnber  of  function 

evaluations needed to find the global extremum. 

W h e n  function  evaluations  are  extremely expensive, it  appears  sensible  t O  ex- 

amine previous function evaluations, that is aheady sampled points, very carefdy. 

CHAPTER 4.  BAYESIAN  GLOBAL OPTIMIZATION 

38 

It is of  particular  interest to discover po tential optimization  problems  before large 

amounts of  sampling resources  are spent. 

In this  chapter we introduce a set of  diagnostic plots  which  early on assess the 
likely success of  the global optimization method.  If  a problem is diagnosed, it is of- 

ten possible to overcome it f d y  or partially by optimizing a suitable transformation 

of  the response rather than  the nntransformed response. 

The method  proposed  in  this  chapter  deals  with  the unconstrained  global  op- 
timization  problem,  minimize  f (x) where  x  = (xl, . . . , xd).  This  includes  the 
class  of  problems  with  simple  constraints  like  ai  5  xi  5  bi, i  = 1. . . . . d. since 
these problems can be transfoxmed to unconstrained  global  optimization  protlerns. 

Throughout  we assume without  loss of  generality that  the extremum of  interest  is 

a minimum. 

The outline  of  this  chapter  is  as  follows.  In  Section  2  we  review  briefly  the 

Bayesian  global  optimization  approach  and  introduce  a  more  flexible  stochastic 

mode1 in  that  framework.  Also,  a  theorem  concerning  convergence  of  Bayesian 

global  optimization  is  given.  Section 3  describes  the  diagnostic  plots.  We  show 

how  they are used  to assess and improve the mode1 fit  and hence the effectiveriess 

of  the global  optimization  method.  Section 4  shows by  means of  several examples 

fiom the optimization  literature that this approach is  very  efficient in  terms of  the 

number of function evaluations required.  Section 5 concludes with some discussion. 

4.2  Expected Improvement Algorithm 

This  algorithm  is  based  on  the idea  that  any future  sampled  point  constitutes  a 

potential  improvement  over  the  minimal  sampled  value  up  to  the  present  stage. 

Uncertainty  about  the function value at a point  to be sampled is  dealt with by  cal- 

CHAPTER 4.  BAYESIAN GLOBAL OPTIMIZATION 

39 

culating the expected improvement, based on some statistical model.  The e q e c t e d  

improvement  criterion is equivalent, we show later, to one-step-ahead  optimality in 

Bayesian Global  Optimization. 

The expected improvement algonthm proceeds  in five steps: 

1. 

Choose  a small initial  set  of  sampled  points  spread  over the entire x  space. 

Evaluate the true function at these points. 

2. 

Mode1 the true function  using  all previous function evduations. 

3. 

Search over x for  the maximum expected improvement  in  f .  The location  of 

the maximum is  the next sampled  point. 

4. 

Compute a stopping criterion based on the maximum expected improvelrie~it . 

If  the criterion is  met stop. 

5. 

Evaluate  the true function at  the new sampled  point.  Go  to Step 2. 

Note 

that  after  each  sampling  step  the  predictor  is  updated  (Step  2).  aiid  the 

expected improvement as  a function of  x is redefined in Step 3. 

For S tep 1, Latin hypercube sampling schemes (McKay et al.. 1979) are particii- 

lady useful, because they have a space filling property, i.e.  they uniformly cover the 

x  domain  to explore the function globdy.  The number  of  points  sampled  at  this 

initial  stage is somewhat  arbitrary.  We  choose about  10 points  per  active variable 

because  one needs  at le& 
moderat ely complex functions  ( Welch, personal  communication ) . 

that  many  points  to  obtain  a  reasonably  good  fit  for 

For  the modeling approach in  Step 2  we  use  a  stochastic  process with  a more 

flexible correlation  structure than  has  been  previously  employed  in  the  Bayesian 

global op timization  literature.  This is discussed  f u t  her in  Section 2.1. 

CHAPTER 4.  BAYESIAN  GLOBAL OPTrMIZATION 

40 

The expected improvement aiterion in Step 3 is based on the idea that any addi- 

tional fùnction e d u a t i o n  constitutes a potential reduction of the minimal function 
evaluation found so far.  This is discussed further in Section  2.2. 

For Step 4, we propose to stop when the maximum of the expected improvement 

is smaller than a tolerance value; smaller in absolute value or relative to the current 

minimal function value.  Step 5  consists of  evaluating the next sampled point. 

4.2.1  Modeling  Approach 

Suppose  that,  after  an  initial  experimental design  (set of  sampled  points)  or  at 
some stage of  the  algorithm,  we  have n vectors  xi,. . . ,x,  at  which  the ftinction 
f has  been  evaluated.  Each  vector  x  is  d-dimensional  for  the  d  covariates  (or 
inputs)  X I , .  . . , xb  The  corresponding  response  values  (or outputs)  are  denoted 
y = ( y  , . - . , y ) .  Shen, following the approach of  Chapter  2  or:  e-g.. Welcli et al. 
(1992), the response is treated as a random function or a realization  of  a stochastic 

process: 

where  E ( Z ( x ) )  = O  and  Cov(Z(w), Z(x)) = u2R(w, x) for  two  inputs w  and  x. 
The correlation  function  R ( - ,  a)  can  be  tuned  to  the  data.  Here  it  is  assumed  to 

have the forrn: 

where  Bj  1 O  and O  < pj < 2.  The pj's  can be  interpreted as parameters  which 
indicate the smoothness of  the response  surface (smoother as the p's  increase) and 

CHAPTER 4.  BAYESIAN  GLOBAL OPTrMIZATION 

the 0's  indicate how local the predictor is  (more local  as the 19's  increase). 

The best linear  unbiased predictor of y at an untried x can be shown to be (see 

(2.14) with F = 1 and f,  = 1 ): 

where  r(x) is  the  n x 1 vector of  correlations  R(x, x;)  in  (4.2)  for  i  = 1. . . . . n 
between Z at x and each of the n sampled points, Ris a n x n  correlation matrix with 
dernent  (i. j )  defined by  R(%: x j )  in  (4.2)?  = (ltR-'1)-'lty is  the generalizecl 
least squares estimator of p l  and 1 is  a vector of  1's. 

The mean  squared error  (MSE) of  the predictor  can  be  derived  as  (see (2.15) 

with F = 1 and f,  = 1 ): 

The predictor  based  on  the  correlation  function  (4.2) in  (4.3) Lias  proven  to 

be  accurate for  numerous  applications,  see e.g.  Currin  et  al.  (1991), Sacks et al. 

(1989a), Sacks et al.  (1989b), Welch et  al.  (1992).  Mockus  (1989) used  a Wiener 

field ins tead. 

In practice, uZ defined after (4.1) and the correlation parameters  Oi? . . . . Bd  and 
p l , .  . . ,pd  in  (4.2) have  to  be  tuned  t o  the  data.  We  use  maximum  likeliliood 
estimation;  see, for example, Welch et al.  (1992)  for details. 

4.2.2  Expected Improvement 

We will  now derive the expected improvement  criterion. 

CHAPTER 4.  BAYESIAN  GLOBAL OPTIMIZATION 

42 

If  the function is sampled  at x to determine y  = f (x) then the improvement  1 

over fzin, the minimal sampled function  value after n evaluations, is  defined as 

The expected improvernent is given as 

where 40 is  the ~robability density function representing uncertainty about y. 

Mockus  (1989) ~roposed a  generalization  by  specifying  a  loss  function  on  the 

sequential  n-step  optimization strategy Sn : 

Le.,  loss  is  defined  as  the  difference  between  the  global  minimum  and  the  best 

function  value found  after n steps.  The risk, or 

the average loss is  then given as 

E (L(Sn, f )) = E ( f , i n )  - 

E(*n  f (4). 

(4.6) 

An  optimal  strategy  is  defined  as  one  that  minimizes  the  risk  (4.6).  Computing 

an optimal strategy turns out to be computationally infeasible for even a moderate 

number of  points n.  The standard approach  then is  to relax the n-step  optimality 

to one-step optimality.  The criterion for one-step  optimality is equivalent  to  (4.5). 

To predict  Y(x) at an untried x, we have c(x) from (4.3) with  a mean  sqiiared 

error given by  (4.4).  For  notational  simplicity, we omit  the dependence on  x. and 

denote i ( x )  by  ij  and  the  mean  squared  error  by  s2.  If  we  further  assurue  that 

CHAPTER 4.  BAYESIAN  GLOBAL OPTIMIZATION 

43 

the random function Y ( x )  is  Gaussian,  then  t j  is also normal.  Thus, we represent 
uncertainty  about  the t m e  y  by saying it is  N(& s2). The expected improvement 
in  (4.5) can be expressed  as 

where  4 ( )  and  a() denote  the  probability  density  function  and  the  cumulative 
distribution  function of  the standard normal distribution.  The f i s t  term in  (4.7) is 

the predicted difference between the current minimum and y at x. penalized  by the 
probability of improvement.  The second term is large when y(x) is close to f;.  and 
s is large,  i.e.,  when  there is much uncertainty about  whether y(x) will  beat  f;;,. 

Thus,  the expected  improvement  will  tend  to  be large  at  a  point  with  predicted 

value  s m d e r  than  fmin  and/or  where  there  is  much  uncertainty  associated  with 

the prediction. 

A  practical  problem,  though,  is  findinp  the global  maximum  of  the  expected 

improvement  criterion over  a  continuous region.  Expected  improvement  is  zero at 

sampled points.  As  distance from all sampled points increases, so does S.  one of t lie 

factors leading to large expected improvement.  Random starting points  are cliosen 

such  that  in  each  coordinate  the  randorn  point  is  halfway  between  two  adjacerit 

design  points.  Since the  original  design  was  space  filling,  it  is  ensiired  that  the 

entire x-space  is  covered with local optimization  tries.  This does not  guarantee  to 
find the global maximum, of course.  Mockus (1994) states in this context "[ ...] there 

is no  need for exact minimization  of  the risk function",  because  we only determine 

the point  of  the next observation. 

The following theorem holds for the expected improvement  algori t hm when the 

number of  possible sampling  points is finite: 

CHAPTER 4.  B A M A N  GLOBAL OPTIMIZATION 

44 

Theorem 3 : Suppose we use the Gaussian model (4.1) and the covariance function 

(4.2)  is  such that  the mean  square error of  prediction  in  (4.4)  is  positive  for  any 

unsampled  point  x.  Further,  suppose  the number  of  possible  sampling  points  is 
finite.  Then the expected improvement algonthm will visit  all  the sampling points 

and hence will always find the global minimum. 

Proof: Given in Appendix  C. 

4.3  Diagnostics 

The success  of  the  Bayesian  minimization  algorithm  depends  on  having  a  valid 

model.  The better  the model the more likely the algorithm  will  terminate quickly 

and  with  an  accurate  tolerance  on  the minimum.  For  this  reason  one would  like 

to  assess the  performance  of  the modeling  approacli  as  soon  as  ~ossible. that  is 

after the initial function  evaluations.  When  the model does  not  fit  well  it  is  often 

possible to improve the fit through appropriate transformations of the response.  For 

this  purpose  we  propose  four  diagnostic  plots  to be used after  the initial  functio~i 
evaluations  have  been  obtained.  AU  of  them  are based  on  the  concept  of  cross 

validation. 

Cross  validation  is  a  statistical  technique  often  used  for  assessing  a  model's 

predictive capability, when it is not  convenient to test the mode1 at furtlier sanipled 

points.  It consists of  setting aside  and predicting  a small portion  of  the data from 

a model  based  on  the remaining larger  portion  of  data.  Most  commonly  only  one 

point  at a  time is  set  aside,  and cross validation  is  performed  once for  each  point 

left  out.  In this  chapter we always use leave-one-out cross validation. 

We  remove  case  i  fkom  (4.3)  and  (4.4)  to  obtain  i-;(x;)  and  s - ~ ( x ~ ) .  The 

notation emphasizes  that  case i is removed when predicting  at  xi.  Cross-validatecl 

CHAPTER 4.  BAYESIAN GLOBAL OPTZMïZATION 

st andardized prediction errors (residuals) , for example, can be m i t  ten as 

We propose  the following four diagnostic plots: 

1. A  plot  of  the  cross  validation  predictions  versus  the  true  y's,  i-e.  c-i(xi) 

versus Yil  to indicate prediction  accuracy. 

2.  A  plot  of  the cross  validated  standardized  errors  versus  the  cross  validated 

predictions, Le.,  e;  in (4.8) versus t j - i ( ~ i ) .  This  plot  assesses whether the es- 

timated  uncertainty in prediction  is realistic.  The standardized errors should 
not  lie  far outside  about  [-2,2]  or.  if  many  points  are plotted.  [-3.31.  We 

are particularly  concerned  that estimated prediction  accuracy  is  realistic  for 
smaller predicted  values, i, as  they  are of  most  interest in  minimization. 

3.  A  quantile-quantile  (Q-Q) plot  of  the  ordered  cross  validated  standardized 

m o r s  versus quantiles from the standard normal distribution.  If  the norrnal 

approximation in deriving (4.7)  is d d ,

 we  should see a straight  line through 

the origin  with  slope  1. 

4.  A  plot  of the cross validated  expected improvements versus the true fiiiiction 

values, i.e.,  E ( 1 )  evaluated at x;  based on &;(xi)  and s-;(xi) versus y;.  Thus. 

we  pretend  that  xi was just  introduced  and  compare the expected  irnprove- 

ment with the function value actually achieved.  If  the expected improvement 

criterion is  to find further points  with  good  improvement,  the lowest  y's  to 

date should be associated  with  the highest  expected improvements. 

If  the plots indicate  a poor  fit, a  transformation  of  the data can often iruprove 

CHAPTER 4.  BAYESIAN  GLOBAL OPTIMIZATION 

46 

the fit.  This is possible because the transformed data may more closely resemble a 

realization of  a  Gaussian stochastic process. 

It is  often usefd to  visualize  the estimated function  surface,  as that  may give 

some idea of  the number of local minima or it might  be possible to d e  out certain 

regions  of  the x-space as  a potential location  for  the global  minimum  with  a high 
degree of  confidence.  In  more  than two  dimensions  visualization  of  the  function 

surface is not straightforward.  Instead, we estimate and plot  main and joint  effects. 

i.e.,  the response  as  a function of  only  one or  two  variables  at a tirne.  The main 
effect  of  xi  is  obtained  by  averaging  out  from  the  predictor  i ( x )  all  x  variables 

except xi. Similady, joint  effects of  two variables  are obtained by  averaginp out all 

but  two variables  of  interest  (see e.g.  Welch  et  al-? 1992 or  Section 3.3). 

4.4  Exarnples 

Our methodology is aimed  at optimizing functions  that are very expensive to com- 

pute, for example finite-element  codes.  It  is  convenient,  however.  to  take example 

functions  from  the  optimization  literature.  They  demonstrate  many  qualitative 

features  of  real  functions.  They  are often  highly  nonlinear  and  have  several  Io- 

cal optima.  Moreover, using  t hese well-known  tes t-examples  facilit ates cornparisori 

with  previous methods. 

4.4.1  Branin Function  (Br) 

The Branin function  (Tom and 2ilinska~ 1989) is 

