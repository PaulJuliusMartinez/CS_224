Risk Analysis, Vol. 14, No. 5, 1994

Assessment of Variability and Uncertainty Distributions for Practical Risk Analyses

Dale Hattisl and David E. Burmaste?
Received December 30, 1993

In recent years the U.S.Environmental Protection Agency has been challenged both externally and
internally to move beyond its traditional conservative single-point treatment of various input parameters in risk assessments. In the first section, we assess when more involved distribution-based analyses might be indicated for such common types of risk assessment applications as baseline assessments of Superfund sites. Then in two subsequent sections, we give an overview with some case studies of technical analyses of (A) variabilityheterogeneity and (B) uncertainty. By "interindividual variability" is meant the real variation among individuals in exposure-producing behavior, in exposures, or some other parameter (such as differences among individual municipal solid waste incinerators in emissions). In contrast, "uncertainty" is a description of the imperfection in knowledge of the true value of a particular parameter or its real variability in an individual or a group. In general uncertainty is reducible by additional information-gathering or analysis activities (better data, better models), whereas real variability will not change (although it may be more accurately known) as a result of better or more extensive measurements. The purpose of the rather long-winded exposition of these two final sections is to show the differences between analyses of these two different things, both of which are described using the language of probability distributions.

KEY WORDS Variability; uncertainty; distributional analysis; Monte Carlo simulation.

1. THE NEED FOR DISTRIBUTION-BASED ANALYSIS-WHEN A N D WHY?

If public and private decision makers have long since adopted probabilistic methods, why have these

"proven" methods been adopted so slowly in risk as-

In the early 199Os, many risk assessors analyzing sessments for hazardous waste sites, and for environ-

toxic chemicals in the environment have begun to favor mental and occupational health risks more generally?

probabilistic methods long since adopted in highway

In addition to various kinds of institutional inertia,

planning, in utility load forecasting, in strategic military we suggest that part of the difficulty rests with the fact

studies, and in many other disciplines. Probabilistic that adopting a new paradigm for distributional analysis

methods have worthy roots in the 1940s. Physics and will require USEPA and other agencies to deal with both

quantum mechanics gave us Monte Carlo simulations3 technical and policy issues that are far from trivial. How

and supply logistics for World War I1 gave us Opera- are specific risks likely to be distributed in a mixed pop-

tions Research.

Center for Environment, Technology, and Development (CENTED), Clark University, 950 Main Street, Worcester, Massachusetts 01610.
Alceon Corporation, P.O.Box 2669,Harvard Square Station, Cam-
bridge, Massachusetts 02238-2669. B. Hayes. "The Wheel of Fortune," American Scienrisr 81,114-118 (1993): "The idea behind the Monte Carlo method goes back two centuries or more, but the modem form of the technique was invented at Los Alamos shortly after World War 11. The inventor was Stan-
...islaw Ulam, who was working on a problem in the diffusion of neu-
trons (Metropolis and Ulam, 1949). Ulam refined and developed

his method in collaboration with von Neumann,Nicholas Metropolis and other colleagues at Los Alamos. Within a few years it had been
applied to a variety of problems in physics, statistical mechanics and chemistry. Today it is an indispensable tool in fields ranging from solid-state physics to economics." In each "trial" of a Monte Carlo
simulation, the computer selects a random value from a prespecified distribution for each variable or uncertain parameter, and uses the results to calculate one or more outcome quantities of interest. The distribution of the outcome parameter is thereby determined as the result of the combined effects of multiple sources of variability or multiple sources of uncertainty.

713

M72433y94/1000-0713S07.W/lQ1994 Society for Risk h d y s i s

714 Hattis and Burmaster

ulation with different exposures (that is, what is the variability of risks among exposed people), and how confident we can be that risks to Nth percentile individuals in exposed populations are below specified values (that is what is the uncertainty of our estimates of risk^)?^
Both the variability of risks in a mixed population and the uncertainty of risks are typically the products of multiple factors. For example, the variability of a specific risk in a population might depend on
(1) how many people at various distances from a contamination source are exposed to what concentrations of toxicants,
(2) how many people take in various amounts of which contaminated media (air, water, soil),
(3) how much of the toxicants are absorbed via the different routes in different people, and
(4) how people differ in their individual susceptibilities for different effects, etc.
Similarly, there are multiple sources of our uncertainty in the risks faced by any individual or group, including, for example,
(1) How likely it is that the exposed group's average (or Nth percentile) consumption of various media differs from our standard assumption by various amounts?
(2) How likely is it that the predictions of the air and ground water dispersion models used to calculate air and water concentrations are wrong by various amounts? and
(3) How likely is it that human susceptibilities to the action of a specific toxicant are different by various amounts from the standard values inferred from the relevant animal studies?
`By "interindividual variability" is meant the real variation among
individuals in exposure-producing behavior, in exposures. or some other variable (such as differences among individual municipal solid waste incinerators in emissions, see below). For example, if we were to weigh every individual in a crowd of people with an accurate scale, we would undoubtedly get a distribution of different values, because the different people really do differ in their weights. And, given constant exposure to a given amount of toxicant, those differences would be expected to affect their individual chances of suffering some adverse effect. In contrast, what is meant by "uncertainty" is a description of the imperfection in knowledge of the true value of a particular parameter or its real variability in an individual or a group. In general, uncertainty is reducible by additional informationgathering or analysis activities (better data, better models), whereas real variability will not change (although it may be more accurately known) as a result of better or more extensive measurements.

These are not always easy questions to address in isolation from one another: and their combination poses some additional challenges. Widely available Monte Carlo simulation software can now conveniently handle a substantial number of distributions, and their correlations with one another. However, intelligent use of Monte Carlo simulation (as well as other distributionbased methods of analysis) in the near term will also require some expansion in the normal interaction between risk assessors and risk managers.
0 There is a legitimate concern that if this tool is adopted without careful thought and testing, the protective posture of the agency in implementing various statutes will be compromised in unforeseen ways. Distributional information will offer risk managers the opportunity (perhaps not, for all, a fervently desired opportunity) to make important choices in elaboratingcriteria for judging what is to be a "significant risk," how clean is "clean," and other legal expressions of risk management goals, within the meaning of various statutes and implementing regulations. For example, the criterion for judging the need for a Superfund clean-up due to a cancer risk might
be elaborated as "no more than X risk, or the
upper Yth percentile of the exposed population, with Z degree of confidence, using the following procedures/standards for estimating X , and Z..." 0 There is also concern that the implementation of Monte Carlo analysis not compromise the perceived benefits of procedural consistency achieved by imposing single-point assumptions of different risk related factors (e.g., assumptions about the amount of soil ingested by people, fish consumption, cancer potency factors, and reference doses). However, we suggest that real consistency in assessments can be improved if distributions closer to the real facts of individual
Appropriate analyses of individual distributions involves questions such as
(a) How does one choose among different distributional forms, both on the grounds of basic mechanisms responsible for variability or uncertainty and with the aid of procedures for assessing the goodness-of-fit to different distributional models?
@) How does one determine the parameters that best fit a partic-
ular distribution form to a particular data set? and (c) When is it advisable to adjust the best-fitting parameters of a
distribution in the light of other considerations that are not captured within the available data points (e.g., possible systematic biases of one kind or another in either over- or underestimating variability or uncertainty)?

Variability and Uncertainty Distributions

715

circumstances are substituted for specified point estimates of various factors. For example, it does not seem sensible to us to impose a national uniform assumption of 365-day access to the soil of a waste site, regardless of whether the site is in Texas (where access is possible) or Maine (where snow and ice cover limit access over an appreciableportion of the year). It will of course take some work to actually define appropriate procedures for deriving input distributions in particular applications (see Sections 2 and 3 for the different considerations that can be involved in modifying raw data to derive variability and uncertainty distributions).
1.1. When is a Standard Single-Point Screening Analysis Likely to be Sufficient?
Not every hazardous waste site or other risk problem needs or deserves its own probabilistic assessment. For small sites with inexpensive remediation possibilities, it may make more sense just to clean the property than to undertake a full probabilistic study. Alternatively, of course, if a well-calibrated screening-type assessment procedure indicates that substantial risks are highly unlikely, available resources for analysis may be better directedtoward other problems. [One of the useful applications of a full probabilistic analysis is in fact to check on the appropriateness of the calibration of the screening procedures in not-missing instances where a more elaborate analysis would reveal significant risks (see below).]
1.2 When Is a Standard Analysis Likely Not to be Sufficient?
For large sites with only expensive remedial options, and where a first-cut analysis of risks indicates the potential for substantial hazards, it may well be prudent to undertake a more thorough analysis. Eventually we expect a general appreciationof the limitations of singlenumber presentations of risk information -regardless of whether that single number is called "nominal," "typical," "high end," "conservative," "most exposed individual," or "at the Nth percentile." Appropriately derived and communicated, distributions set a richer context for informed decision making.
The checklist implicit in the following subsections can help identify those projects and sites where decision making might benefit from probabilistic analyses.

1.2.1 When You Want to Think More Clearly than Is Possible with Single-Point Screening Analyses
Any procedure that relies on a combination of point values-some conservative and some typical-yields a point estimate of exposure and risk that falls at an unknown percentile of the full distributions of exposure and risk. For this reason, a risk assessor following current federal or state guidance manuals cannot know the degree of conservatism in her or his calculation--only that the values combine many conservative factors and are likely to overstate actual risk.
In some cases it is likely that the combination of point values-many conservative-yields an estimated risk that is far higher than may have been originally intended for a particular risk management situation. For example, a formula that combines three 95th pecentile values from three lognormal distributions that happen to have the same variance will yield an estimated exposure and risk above the 99.8th percentile of the full distribution of risk. Of course, in the more general case, where the variances due to different factors are unequal, the result will be less extreme than this. How much less extreme is most conveniently determined in a Monte Carlo simulation.
The use of Monte Carlo simulation and other distributional analysis techniques have the potential to clarify greatly the task of risk management and related decision making. Seeing and understanding the full range of exposures and risks put the risk management decisions in a richer context. A simple example helps to illustrate the concept. Let's imagine that the standard (deterministic) risk assessment techniques used by a state culminated in a point estimate of cancer risk equal to 1.1in 1million, just slightly above the upper threshold of acceptable risk commonly in use at that time in that locale. What should a risk manager decide in this case? Consider two possible cases in view of additional information available from a distributional analysis.
0 Case 1. Let's consider that the distributional analysis revealed that the point estimate of 1.1 in one million occurs at the 99.4th percentile of exposure and risk, a long way out a distribution with a long tail to the right. If the simulation further revealed that the 95th percentile of the distribution of exposure and risk occurred at 4 in 10million risk, the risk manager may decide that the proposed facility under consideration falls within the acceptable risk range. Thus, she or he may fairly approve the construction and operation of the facility.

716 Hattis and Burmaster

0 Case 2. Let's consider that the distributional analysis revealed that the point estimate of 1.1 in one million occurs at the 85th percentile of exposure and risk. If the simulation further revealed that the 95th percentile of the distribution of exposure and risk occurred at 2.2 in 1million risk, the risk manager might decide that the proposed facility under consideration falls above the acceptablerisk range. Thus, she or he may fairly disapprove the construction and operation of the facility.
1.2.2. When You Want to Calibrate Procedures for Future Single-Point Screening Analyses
For certain repetitive calculations of risks from common cases, such as the estimation of cleanup targets for gasoline that has spilled or leaked from underground storage tanks at service stations, regulatory agencies may wish to publish standard methods and/or standard values to estimate cleanup targets. In these situations, the regulatory agency, often a state agency, can promulgate methods or cleanup targets after analyzing the proposed method or standards using Monte Carlo simulations and/ or other distributional analyses. The object of such an analysis would be to assess how often a particular screening formulae for quick assessment, with defined but limited information, would and would not turn out to be appropriately protective, given particular articulated risk management standards. In this way, the regulators, the regulated community, and the public can all understand the degree of public health protection incorporated in the policies and procedures for standardized assessments. In this and other cases below it should be stressed that the distributional analysis method needs to be adapted to the specific application-there is no single analytic formula that will efficiently serve a diverse array of information needs.
1.2.3. When You Want to Analyze the Likely Benefits of Gathering Additional Information
With the advent of inexpensive yet powerful desktop computers, we can now undertake probabilistic sensitivity analyses to investigate which of many input distributionsor input variables "drive" an analysis. We now have the ability to do computational experimentslet's use the power to identify and isolate those input variables with these characteristics:The input has a high

leverage on the final risk estimate and the input has a weak foundation in field measurements. Such an input is a strong candidate for additional field studies. Researchers in the "value of information" field use these techniques all the time, and we can readily adopt many of the techniques to public health risk assessment.
1.2.4. When Signijicant Equity Issues Are Raised by Likely Sources of Variability
In the language of welfare economics, most environmental statutes seek to achieve an "equity" (fairness) rather than a societal "efficiency" (maximum social aggregate economic welfare) goal. The question set for analysts by these statutes is not primarily "HOW can we achieve the greatest overall human health benefit?" (although that might be a good second question to ask in some cases-see the subsection below), but "What must we do to avoid an unjust imposition of an unreasonable amount of foreseeable risk on some sector of the exposed population?" Note that we are not talking here about protecting a fictional "porch potato" (in Dr. Michael Callahan of EPA's classic terminology for a person who lives out her or his 70-year lifetime on her or his porch at the fence line of a facility, drinking pure leachate from the site, and eating mostly vegetables grown in contaminated soil), but real people in real neighborhoods including elderly people with osteoporosis and poor kidney function; real two year olds who consume house dust in variable amounts; people who activate or inactivate specific toxicants at rates that are substantially different from other people,(*.2e)tc. Realistic analyses that address the legitimate concerns of vulnerable subpopulations should take into account the readily measurable and estimable differences among people that are likely to cause real differences in exposure and risk for different portions of the population.
1.2.5. When You Wish to Assess the Potential Benefits of Targeting Resources to High-Productivity Categories for Intervention
There has been a great deal of discussion in recent years of ways to get the "biggest bang for the buck" in our use of resources for health protection. One of us has recently presented an extensive analysis of the impact of uncertainty and variability on priority-setting decision~.(B~r)iefly, the argument is that an ideal priority setting system for a particular agency activity will at-

Variability and Uncertainty Distributions

717

tempt to maximize the "net good" that is done in the outside world per unit of limiting agency resources used (where the "limiting resource" for a particular activity is not necessarily dollars, but could be the limited time of people with a particular skill, etc. Because some activities are limited by different noninterconvertible resources, it was suggested that agencies should first sort their activitiesinto groups limited by the same resources and then rank primarily within those groupings).
In the framework, uncertainty and variability in priority scores have different implications in a priority setting system. Other things being equal, the following applies.
0 Great variability (true heterogeneity in the actual results of allocatingeffort to different categories) will enhance the benefits of allocating resources preferentially to relatively high-priority categories. Categories for evaluation should therefore be created which tend to maximize this variability. In contrast, great uncertainty (imperfection in knowledge of the actual results of allocating effort to different categories) will increase the desirability of (1) measures to obtain better information and (2) some spreading of efforts to lower-priority candidates for attention.
2. DISTRIBUTIONAL DESCRIPTION O F VARIABILITY
2.1. Mechanism vs FittingData Description as a Basis for Selecting Distributional Forms
To some statisticians, the "choice" of distributional forms for representing variability boils down to an evaluation of which available computationally convenient distributional form "fits" an observed set of data the best. In this view, which is most compatible with the philosophy of science that goes by the name of "po~itivism,"(~th)e mathematical model is primarily seen as a tool for efficiently summarizing or describing data, not for theorizing about some underlying reality>')
Our own view, of course, is that "theorizing about some underlying reality" is exactly what we need to be doing in (a) assessing the distribution of risks for different portions of our mixed population and (b) assessing differences in likely productivity for alternative uses of specific resources to achieve our goals (see Ref. 3, as

discussed above). In doing this, the available specific data rarely represent, in the phrase of Stanley Kaplan's (of Pickard, Lowe, and Garrick), "the total body of evidence" available to us.
In this subsection we first argue that sometimes our basic understanding of the features of the processes that generate the variability can give rise to at least preliminary ideas (Table I) about what distributional forms are likely to describe the underlying reality. Where data are relatively plentiful these presumptions can sometimesbe tested, but in more common circumstances the data will be too sparse to formally rule in or rule out some conceivable distributionalmodels. In such cases, as in fitting dose response data, our ultimate distributional conclusions will be as strong or as weak as our underlying mechanistic understanding of the causes of the variability in question, and the way the different causes interact to disperse individual values away from each other. Analyses of uncertainty in variability estimates will sometimes benefit from taking into account plausible alternative mechanism-based theories of the sources of variability and their interrelationships.
In Section 2.2 we indicate that if what we want is true variability, we need to make some adjustment in our raw data to subtract out the spreading influences of measurement error (and other short term variability) that is not relevant to the true variation on long-term exposures and consequent risks. (In practice, this subtraction of measurement variability is often neglected either for lack of information or because it is thought to be small in relation to the real variation that appears in the data set.)
Section 2.3 then makes a strong suggestion that if a particular type of exposure variability is to be related to the risk of a specific toxic effect, then the dynamics with which the toxic effect is produced must influence the time units in which the variability of exposures are expressed. For example, information on dietary variability often is derived from cross-sectional samples of what different people were observed to eat over the course of a few days. Without some further analysis/ data, the variability observed over these short time spans cannot be generally assumed to be the same as the interindividual variability in toxicant uptakes from different foods that would be relevant to the effects of a chronic toxicant that builds up over a period of months or years.
Finally Section 2.4 provides another case example in which the combined effects of correlated variability in the emissions of different dioxinldibenzofuran con-

718 Hattis and Burmaster

Distributional form

Table I. Mechanistic Bases for Specific Choices of Distributional Forms of Variability
~

Mechanistic basis

Example(s)

Normal Lognormal

One basic result from probability theory is the "central limit theorem." Essentially, if you add a large number of random variables together (so that no one variahle contributes a substantial amount to the total variation), the result will tend to take on the shape of a normal distribution.
If one multiplies a large number of random variables together, this is the same as adding the logarithms of those numbers, and the result will tend to take on a lognormal shape (the logarithms of the values are normally distributed).

The "Gaussian Plume Model" for the dispersion of air pollutants results from the idea that, at a micro level, individual parcels of air, or molecules of pollutants, are subject to many random kicks from other molecules, etc., that act together as if a large number of random numbers were being addedhbtracted from an initial 3-dimensional description of a position.
Concentrations of chemicals in environmental media, amounts of those media that are consumed, efficiencies of absorption, and rates of elimination of toxicants all can he expected to have a multiplicative influence on the amount of a toxicant that can be observed in the blood stream. And in practice, lognormal distributions often provide good fits to data on toxicant concentrations in a variety of media (Fig. 1-4r-1

Multimodal or mixed distributions

These result from situations in which (a) one, or a small number of contributors to variability accounts for an appreciable proportion of the overall variation, and/ or (b) there are a small number of discrete states of an important contributor to variation that substantially determines the level of the parameter under study. Alternatively, there is a causal connection between some oddly-distributed demographic parameter(s), such as age and gender, and a risk related parameter of interest (e.g. breathing rates).

One common circumstance for this that we have seen is where a particular parameter is largely determined by alternative forms of a single gene-giving rise to bimodal or trimodal distributions, depending on whether the heterozygote is appreciably distinguishable from the two homozygotes. See, for example, the suggestive data of Keterer et al.c31)(Fig. 4). For the demographic correlation case, see the work of Layton on the relationship among age, gender, and breathing rates as determined from metabolic rate informati~n!~~)

Poisson Binomial Geometric

Favored in circumstances where one is observing the frequency of discrete events, the events are independent of one another, and relatively small in number.
Produced by processes that (1) can produce only one or the other of two outcomes (a head or a tail on a coin flip) and (2) are carried out a finite number of "trials."
This is a result of a simple exponential decline.

The number of counts of radiation that occur in a particular time interval.
The frequencies of animals with and without tumors (or some other quanta1 outcome) in a chronic animal bioassay.
The fraction of individuals (or anything else) remaining in a system at various times after the start of a simple linear loss process.

Others (beta, geometric, triangular, uniform, etc.)

Not clearly tied to mechanisms related to variability. Probably of most use in representing subjective uncertainties.

a All the logarithms in these figures, and elsewhere in this paper, are base 10.These can be converted to base e ("natural" logarithms) by multiplying by 2.3026 (the natural logarithm of 10).

geners from municipal solid waste incinerators is assessed with the aid of a Monte Carlo simulation.

2.2. The Effect of Measurement Errors on Estimates of Variability in Parameters Related to Human Exposure or Susceptibility to Toxicity

The issue of measurement (and,

Other

short term) variability arises because, as risk assessors,

we are not simply interested in describing the distribution of a set of observations. We want to use the data to help make inferences about how much different the true delivered doses and risks might be for different people in the same environment. Measurement errors contribute to the variability in our observations but do not reflect real differences in dosage and resulting risks. The distribution of the observations of, for example, breathing rates@)represents the combined influences of variation that results both from the true long-term vari-

Variability and Uncertainty Distributions

15-
-1.4
cB 1.2M -1

./34YurOldr

R"2=0.998

y=O.758S+0.2837a RA2=0.999

0.0 0.5

1.0 1.5
2-Score

2.0 2.5

Fig. 1. Lognormal plots of blood lead levels for Massachusetts children, 1989-1990 (source: Massachusetts Department of Public Health, 1992). To create this type of plot, the measurements are first arranged in order and given ranks i (1 through N). Then one calculates a "percentage score" for each ordered value as lOO*(i-OS)/N. (This differs from the usual definition of a "percentile" in which the highest observation is assigned a score of 100.) Finally, from tables of probits or areas under a cumulative normal distribution, one calculates the number of standard deviations above or below the median of a normal distribution that would be expected to be associated with each "percentage score," if the distribution of values were in fact normal (Gaussian). In the regression lie calculated from this type of plot, the intercept (Z=O) represents the expected median, and the slope represents the standard deviation. This procedure has the advantage of being able to accommodate truncated data (e.g., the presence of "nondetect" values)u or, as in the figure, data that were originally provided only in the form of a histogram of the numbers of children above specific cutoff values. Where there are "nondetect" values (as in Figs. 2 and
3 ) these contribute to the N used for calculation of percentage scores,
but such points are not plotted and therefore do not contribute directly to the fitted regression line. For further information see R. 0. Gilbert. "Characterizing Lognormal Populations," in Statistical Methods for Environmental Pollution Monitoring (Van Nostrand Reinhold, New York, 1987), pp. 164-176.

21 I

1
.nno
v

y = 0.5498 + 0 . 3 2 8 8 ~

0 -2-10 1 2 3 4
Z-Score Fig. 2. Lognormal plot of plasma PCB levels in 738 Southern Californians screened prior to employment [data of Sahl et aL(35a) s Analyzed in lOM(m].

ability we are interested in, and various short-term perturbations, including measurement errors.
In the fortunate special case where both the true long-term variability and the measurement errors are ei-

719

1.o
-
00 0.8
Jz 0.6

0.4

0.2

0.0 - 0 . 8 - 0 . 4 0.0

1.2 1.6 2.0

Fig. 3. Lognormal plot of whole-blood methylmercury levels in South Haven, Michigan [data of Humphrey (personal communication, 1990) as analyzed in IOM(m].

ther normally or lognormally distributed, then we can simply subtract the measurement variance from the total observedvariance to obtain an estimate of true long term variance (where both distributions are lognormal, this must be done after transformation to log values). An accompanying paper(? shows an application of this approach to breathing rate data-where the measurement errors and short-term variability were estimated from replicate measurements of breathing rates on the same people.
In the more general case, of course, measurement errors may not conveniently have the same distributional form as the underlying "true" long-term variation in the parameter under study. For such cases, a "deconvolution" analysis may be done. The analytical questions are (1) what "true" distribution, when combined (convoluted) with the estimated measurement variation, gives a combined distribution that is statistically most compatible the variation seen in the data and/or (2) what range of "true" variability estimates, when combined with measurement variation, gives combined distributions that are not statistically incompatiblewith the data. Conventional estimation procedures which maximize goodness-of-fit as a function of the estimates of distributional parameters seem likely to be straightforwardly applicable here, but we are unaware that any analysis of this type has actually been performed on data related to environmental health risk analyses.

2.3. Temporal Aspects of the Description of Variability in Toxicant U p t a k e T h e Need for Input from Toxicodynamics6
It is of course well appreciatedthat for some effects the toxicologically relevant averaging time is the total
The remarks in this section were originally written as comments on a draft EPA document on the use of exposure scenarios in risk analysis.

720 Hattis and Burmaster

0.0
.
-02-
--0.4 --06
--0.8
--1.0
-1.2 ,

y = - 0.6727 + 0.3398~ R"2 = 0.948
. . .I I 1

.

Fig. 4a. Lognormal plot of Ketterer et 0l.c") data on liver phenacetin 0-deethylase activity (P450 IA2).

t

-.8 -.6 -.4 -.2 0 Log (Phenacetin Odeethylase)
Fig. 4b. Histogram of log values from the same distribution as in Fig. 4a.

-2.54':,

. , . , . , . , .I

-3 -2 - 1

0

1

2

3

Z-Score

Fig. 5a. Lognormal plot of the distribution of aldicarb concentrations in potatoes.

lifetime, whereas for others it can be the several minutes it can take to inhale a damaging dose of chlorine, or the half-hour it takes to ingest a meal with an acute toxic dose of the toxin that causes paralytic shellfish poisoning, or a short-acting cholinesterase-inhibiting insecti-

cide. What seems to have been less clearly appreciated is that defining the toxicologically relevant averaging time is absolutely critical for determining the variability of intake via a particular route of exposure among people. So, for example, it is entirely likely that the interindividual variability of the amount of water drunk and the amount of air breathed is substantially different depending on whether one is speaking of lifetime averages, averages over a few months, averages over the course of a single day, or a set of random 1-hr or 15-min periods in the course of a day. Median intake rates for different time periods may often be similar for different periods, but a 90th, 95th, or 99th percentile of a shorter period will tend to be farther away from the median than would the corresponding percentiles of the same intake for a longer period, depending on the contribution of short-term within-individual variability to the variance for the shorter averaging time. The exposure analyst needs to determine which periods are relevant for which effects and then see how the effective averaging that is relevant for specific toxic effects relates to the averaging time implicit in available data measuring exposure, uptake, or internal dose rates for a particular route.
For example the distribution seen in Fig. 5a, of aldicarb concentrations on potatoes, might be appropriate for figuring the likely variability of exposure for individual meals in which people eat one potato each. However if aldicarb were being evaluated for some chronic toxic effect that depended on the aggregate dose delivered over several months or a lifetime, involving N potato-containing meals (or, alternatively, if groups of N independently selected potatoes were routinely mashed together before consumption), then it would be important to compute the variation expected in N-sample averages. This can be done by simulating the result of averaging N selections from the distribution in Fig. 5a. The results of choosing N = 4 and N = 16 are shown in Fig. 5b (based on plots of every tenth point in simulations of 2,000 means in each case). The lower slopes with increasing N correspond to progressively smaller standard deviations of the logarithms of the distributions of simulated mean aldicarb concentrations. (Note: if the distribution of aldicarb concentrations were normal, rather than lognormal, the standard deviation of N-potato averages could be computed as the standard deviation of individual potatoes divided by the square root of N.) Of course, for a real risk assessment, additionalfactors contributing to variability would be any correlations between the concentrations of pesticide in potatoes consumed at the same time, and interindividual differences in the amount of potatoes consumed over the toxicologicaly relevant time period.

Variability and Uncertainty Distributions

721

`0+ 0 . 3 7 2 ~R Y = 0.993

1

w

-3 -2 - 1

0

12

3

-1 Z-Score

Fig. 5b. Lognormal plots of simulated distributions of 4- and 16-sample arithmetic means from the lognormal distribution of aldicarb concentrations.

The most common case, of course, is where one simply wants a lifetime average daily dose. Something more interestingtakes place, however, when for example one needs to try to calculate the mortality toll from acute injuries caused by exposure to chlorine following a large scale release from a ruptured tank; or when one is interested in exposure of pregnant women to a teratogen, whose effectivepotency varies greatly depending on just when the dosage is delivered relative to key events during gestation. The exposure assessor needs to be encouraged to have some dialog with the toxicologist to understand the following.
(1) The region of dosage in which nonlinearities in dose-time-response relationships can be expected. For example, for short-term high exposure to chlorine and ammonia, it appears that the incidence of acute toxic effects depends on the product of exposure duration and the square of concentration, probably because of the saturation of the capacity of defense mechanisms.@I)n the case of formaldehyde,it is likely that gross irritation of the nasal mucosa at high doses causes cell killing and replacement cell replication that could greatly enhance the expected rate of production of tumors per unit exposure at high dose rates.(9)In the case of methylene chloride, Lawrence Rhomberg of USEPA has used a phannacokinetic model to calculate a series of factors that represent the ratio of active metabolite production per unit of dose for particular combinations of high exposure and duration in the course of a day>9)
(2) The rate of reversal of toxicant accumulationor the accumulation of incipient damage(lOJ1)-in-

dividual exposure events that occur several damage repair half-lives apart can be treated as essentially independent without great error; but if the exposure events are not separated by this amount of time, damage or internal toxicant dosage from more than one event will tend to accumulate.
Cases such as these will generally require the construction of integrated models of exposure and effect, or at the very least, the use of effect information in specifying the exact form of exposure information that will most usefully interface with the effect models.
There is one other implication of relatively short averaging times for specific effects that is relevant to the discussion here. In general, the shorter the toxicologically relevant averaging time, the more such periods will be contained in an individual's lifetime, and therefore the more opportunity there is for rare extreme fluctuations in exposure to exceed tolerable limits. For an air pollution planner worried about exposures to ozone or carbon monoxide, every few hours there is another opportunity for toxicologicallysignificant effects to be produced, if the short term fluctuations in levels are high. Such an analyst may well be almost solely concerned with the region from the 99th to the 99.99th percentiles of total time periods X people exposed. So the "high end" of interest in that type of case will be at a generally larger percentile level than might be the case for an effect with a longer toxicologically relevant averaging time.
2.4. An0ther Example-Plan t-to-Plant Variability of Emissions of Dioxin Congeners from Solid Waste Incinerators7
For access to the data cited in this section, we are indebted to David Cleverly of the USEPA's Office of Research and Development. Dr. Cleverly assembled information on the emissions of dioxin and dibenzofuran congeners from test bums of 10 municipal solid waste incinerat0rs(~=1~a)nd expressed them in standardized form per metric ton of waste burned.
This is an interesting case for the assessment of variability because
(1) The data are incomplete, probably in part because of interference by some congeners in the
Portions of this analysis originally were written as comments on a draft EPA document on exposure to dixoin-like compounds.

722
-
e
c
.U-
L
.-f
0 El
Vc

3-.
-1

y = 0.33835 + 0.57243~ R"2 = 0.963

I

-2 -1

0

1

2

Fig. 6. Distribution of emissions of 2,3,7,8-TCDD from municipal solid waste incinerators-lognormal plot.

3,

2-

1-

-Do
I M
cl

0-
1, -2

-1 0
Z-Score

1

2

Fig. 7. Distribution of 1,2,3,7,8-CDD emissions from municipal solid waste incinerators-lognormal plot.

detection of others-not all congeners were measured in the output of all plants; and (2) emissions of many of the congeners are correlated with one another.
Below in this subsection we first assess the variability of emissions among the sample of 10 plants (expressed as approximate 2,3,7,8-TCDD equivalents, given current ``Toxic Equivalency Factors"-estimates of the relative potency of different dioxin/dibenzofuran congeners for producing effects mediated by receptor binding. This could be relevant, for example, to an assessment of the potential benefit a community might obtain by seeing to it that its plant operated with relatively low vs relatively high emissions, given the worldwide experience. In the following section, we use this same case to illustrate the assessment of uncertainty.
It should be stressed that the object of analysis here is not the variability of emissions from time to time within individual plants (although that might also be of

Hattis and Burmaster
interest for some purposes), but the variability of emissions among different plants. We have available only one summary report of the average concentration of various congeners in stack emissions collected in a single series of runs (our impression is that this was usually over a few days). Because of this, we are unable to utilize the approaches outlined in Section 2.2 to either estimate or subtract out short-term variability and measurement errors from the long-term variability of emissions, which would be of primary interest for estimating variability among communities in long term risks. We are also unable to investigate the possible effects of realistic variations in fuels and plant operating characteristics that could affect long-term emissions in practice. It is possible that the data were often collected under relatively favorable conditions in these respects and, therefore, may not entirely reflect the full variability of important operating parameters that would be seen over a lifetime of commercial use of these facilities.The present analysis, therefore, needs to be seen as very much an illustrative starting point for fuller and more sophisticated studies.
The first issue is the form of the distributions. Figs. 6 and 7 and parallel plots of the untransformed data (not shown) indicate that lognormaldistributionsdescribethe individual congener emissions data better than normal distributions.Correspondingregression plots of the same
data before log transformation indicated R 2 values of
0.786 and 0.447 for 2,3,7,8-TCDD and 1,2,3,7,8-CDDY respectively, and results for other congeners were similar. Lognormal distributions are inherently somewhat more plausible in this case, because one expects that the factors likely to be leading to variability in emissions (e.g., holding time at the relevant temperature for dioxin/ dibenzofuran formation, departure of the temperature distribution in the gas from that which is ideal for formation of specific congeners, amount of chlorine in the feed) are more likely to act multiplicatively than additively in quantitatively influencing emissions.
Given that the data are at least approximately lognormally distributed, it is straightforwardto calculate the geometric means and geometric standard deviations, incorporate them into a spreadsheet-based Monte Carlo simulation system (Crystal Ball, by Decisioneering Corp., was used in this case), and use them to obtain an estimate of the likely variability in toxic equivalents emitted by MSW facilities of the standard size and configuration. Table I1 shows the geometric means and standard deviations, and the estimated contribution of each congener to overall average TEiq emissions. One inference that falls out of the overall mean calculation at the bottom of Table I1 is that 2,3,7,8-TCDD itself does not

Variability and Uncertainty Distributions

723

Table II. Variability of Emissions of Individual Congeners from Municipal Solid Waste Incinerators of Standard Size and Configuration

Congener emissions (pglmetric ton)

Plants with data Mean SD TJW Mean TEqb

2,3,7,8-TCDD log(2,3,7&TCDD)

10 4.33 5.03 1

0.3383

0.5767

4.33

1,2,3,7,8-CDD log(1,2,3,7,8-CDD)

8

39.85

97.63

0.5

0.6486

0.9503

19.92

1,2,3,4,7,8-HxCDD IOg(1,2,3,4,7,8-HxCDD)

8

9.37 16.78

0.1

0.4574

0.7270

0.94

1,2,3,6,7,8-Hx-CDD log(1,2,3,6,7,8-HxCDD)

7

22.85

41.23

0.1

0.7305

0.9272

2.29

1,2,3,7,8,9-HxCDD lOg(1,2,3,7,8,9-HxCDD)

5

11.57

16.61

0.1

0.4730

1.078

1.16

1,2,3,4,6,7,8-HpCDD lOg(l,2,3,4,6,7,8-HpCDD)
OCDD log(0CDD)
2,3,7,8-TCDF log(2,3,7,8-TCDF)
1,2,3,7,8-PCDF lOg(1,2,3,7,8-PCDF)
2,3,4,7.8-PCDF log(2,3,4,7&PCDF)
1,2,3,4,7,8-HxCDF lOg(1,2,3,4,7&HxCDF)
1,2,3,6,7,8-HxCDF log(1,2,3,6,7,8-HxCDF)

4 10 8 8 7 8 7

21.09 1.2616
163.88 1.9361
49.11 1.4616
51.73 0.9218
46.75 1.2728
19.64 0.8744
25.35 0.7062

10.95 0.2942
189.5 0.5554
47.89 0.5170
95.37 1.094
67.21 0.6498
24.04 0.8368
42.30 0.9833

0.01 0.001 0.1 0.05 0.5 0.1 0.1

0.21 0.16 4.91 2.59 23.38 1.96 2.54

1,2,3,7,8,9-HxCDF lOg(1,2,3,7,8,9-HxCDF)
2,3,4,6,7,8-HxCDF log(2.3,4,6,7,8-HxCDF)
2,3,4,6,7,8 HpCDF 1og(2,3,4,6,7,8HpCDF)
2,3,4,7,8,9 HpCDF 1og(2,3,4,7,8,9HpCDF)

6

21.47

41.16

0.1

0.0734

1.290

5

20.53

33.98

0.1

0.9109

0.6132

5

25.94

37.60

0.01

0.8794

0.9372

4

0.8145

0.4805

0.01

-0.1423

0.2455

2.15 2.05 0.26 0.01

OctaCDF log(0ctaCDF)
Total arith mean TJZq

10

74.60

110.1

0.001

0.07

1.0290

1.1044

68.92

a Toxic equivalency factor. This is the estimated potency for each congener in producing receptor-mediatedbiological effects, relative to the potency Of 2,3,7,8-TCDD. Toxic equivalent emissions. This is simply the mean emissions Wmetric ton of waste burned) times the TEF.

make the largest contribution to overall toxic equivalents, at least with the current set of "toxic equivalent factors" as estimated by EPA.
Before proceeding to the results of the Monte Carlo simulation, it is important to assess the possibility of correlations in the emissions of different congeners. The effect of any such correlations would be expected to increase the spread of the distribution of TEq equivalents

emitted by different plants. To assess correlationswe did a series of simple regressions of the log,, of various congeners emitted by various plants vs the log,, of the congener that makes the single largest contribution to the TEq-2,3,4,7,8-PCDF. A couple of these correlations are shown graphically in Figs. 8 and 9. Table I11 summarizes the regression coefficients and associated p values for the correlations for all the congeners. It is

724 Hattis and Burmaster

2

s

un
t

1-

00.

`=

?

c!

M 0-

0

-1

y = - 1.003 + 1.046~ RA2 = 0.932

1
0 12
Log(2,3,4,7,8-PCDF)

3

Fig. 8. Observed correlation between emissions of 2,3,7,8-TCDD and 2,3,4,7,8-PCDF by municipal solid waste incineration plants.

v 00 0
4
Q
1 0 123
Log(2,3,4,7,8- PCDF)
Fig. 9. Observed correlation between emissions of 1,2,3,7,8-CDD and 2,3,4,7,8-PCDF by municipal solid waste incineration plants.
apparent that there are numerous, highly significant associations between the emissions of different congeners and 2,3,4,7,8-PCDF, although not all congeners show such associations.
The most recent versions of Monte Carlo simulation systems allow such associations to be built in to analyses of variability or uncertainty, at least in the form of rank correlations (Crystal Ball, by Decisioneering Corp., was used in this case). The rank correlation procedure allows correlationsto be expressed in a consistent way even if the parameters being correlated have different distributional forms. And the general impression from the designers of the system is that rank and ordinary Pearson correlation coefficients should be similar in most cases.
In response to a reviewer's query, we tested this for our specific application. A simulation of 10,000 trials was done incorporating each of the eleven correlations in Table I11 whose P value was less than 0.1. It can be seen in Table IV and Fig. 10 that there is a modest but

Table III. Apparent Correlations Between the Measured Emissions
of Various Congeners and 2,3,4,7,8-PCDF

Congener/parameter log(pg/metric ton)
log(2,3,7,8-TCDD) Iog(1,2,3,7,8-CDD) Iog(1,2,3,4,7,8-HxCDD) log(1,2,3,6,7,8-HxCDD) log(l,2,3,7,8,9-HxCDD) log(1,2,3,4,6,7,8-HpCDD) log(0CDD) log(2,3,7,8=TCDF) log(1,2,3,7,8-PCDF) log(2,3,4,7,8-PCDF) log(1,2,3,4,7,8-HxCDF) log(1,2,3,6,7,8-HxCDF) log(1,2,3,7,8,9-H~CDF) log(2,3,4,6,7&HxCDF) log(2,3,4,6,7,8 HpCDF) log(2,3,4,7,8,9 HpCDF) log(octaCDF)

R*
correlation with log (2.3,4,7,8-PCDF)
0.9324 0.6813 0.6667 0.4950 0.3308 0.0126 0.8293 0.7145 0.6020 (identity) 0.2448 0.6068 0.7498 0.9349 0.5147 0.2157 0.7507

~~~
P

~

value

for

correlation

~

0.0004

0.022

0.025

0.0777

0.31

0.89

0.0044

0.017

0.04

0.26 0.039 0.026 0.0072 0.28 0.54 0.012

J
1P
0 4 0 . 5 0.6 0.7 0 . 8 0.9 1 0
R A 2 Observed in Data and Input as Rank Correlations in the Simulation Fig. 10. Relationship between the correlations input as rank correlations to crystal ball and ordinary Pearson correlationsof output values (based on 10,000 trials).
consistent tendency for the output Pearson RZvalues in
the simulated correlations to be somewhat less than the
RZvalues that were put into the simulation. This small
but apparently systematic bias is bothersome, and where calculations of high accuracy are desired, it may be worthwhile to test out procedures to inflate the correlation coefficients input to simulations in order to get outputs that are as strongly correlated as desired. Alternatively rank correlations can be calculated from the

Variability and Uncertainty Distributions

725

Table IV.Accuracy of the Representation of Correlations by Crystal Ball's Rank Correlation Procedure When the Input Correlations are Calculated as Ordinary Pearson Correlations

Congenedparameter log(pg/metric ton)

Observed R2 correlation with log(2,3,4,7,8-PCDF emissions) input to the simulation

~ ~~~

~

RZof the correlation

of simulated values of the

log (congener emissions) with

simulated log(2,3,4,7,8-PCDF

emissions)

Ratio, simulated PI
input R1

log(2,3,7,8-TCDD) log(1,2,3,7,8-CDD) log(1,2,3,4,7,8-HxCDD) log(1,2,3,6,7,8-HxCDD) log(0CDD) log(2,3,7,8-TCDF) log(1,2,3,7,8-PCDF) log(1,2,3,6,7,8-HxCDF) log(1,2,3,7,8,9-HxCDF) log(2,3,4,6,7,8-HxCDF) log(octaCDF)

0.9324 0.6813 0.6667 0.4950 0.8293 0.7145 0.6020 0.6068 0.7498 0.9349 0.7507

0.8478 0.6378 0.625 0.439 0.774 0.656 0.558 0.542 0.697 0.869 0.660

0.909 0.936 0.937 0.887 0.933 0.918 0.927 0.893 0.930 0.930 0.879

primary data for use in simulationsdirectly (Crystal Ball has a routine for doing this), but any rank correlation procedure would seem to sacrifice some information contained in the original continuous data. For our illustrative purposes here, the modest departure of simulated
and desired correlations is not of great significance. As
shown below, in the example here we have a secondary means of checking out the degree to which the simulations reproduce the dispersion of the actual TEiq data.
Table V shows the estimated distribution of selected congener TEq contributions and overall summed TEq values (based on all congeners shown in Tables I1 and 111) with and without correlations with P values less than 0.1, based on runs of 10,000 trials each. Minor
differencesbetween the entries for individual congeners, and for the mean values in the two parts of Table V, show the magnitude of purely statistical fluctuations in the results of 10,000-trial runs. It is apparent that including correlations substantially broadens the expected distribution of total TEq variability among plants.
There is another, and some would say more straightforward,approach for analyzing the variability in total plant TEiq emission factors. That is simply to calculate the overall TEq emissions for each plant and derive the distribution directly. This can be done if we prune the database to eliminate the three plants for which the most important congeners were not analyzed. The results are given in Table VI and Fig. 11.
The log(geometric standard deviation) of about 0.66 inferred in this way is comparable to the value of 0.59 derived in the simulation with correlations. However the mean of about 71 pg TEq/metric ton of waste

Table V. Expected Variability in Standard Plant Overall TEq EmissionsMetricTon, Based on 10.000-Trial Monte Carlo
Simulation Runs

Chemical or aggregate

5th percentile

Median (50th percentile

Mean

95th percentile

Results without including correlations in the production of different congeners

2,3,4,7,8-PCDF TEq 1.2,3,7,8-CDD TEq

0.83 9.1 0.06 2.2

29 28

113 80

2,3,7,8-TCDD
2,3,7,8-TCDF TEq

0.26 2.2 0.41 2.9

5.3 19 5.9 21

Total sim. trial TEq 19 64 129 376

Results including correlations in the production of different congeners

2,3,4,7,8-PCDF TEq 1,2,3,7,8-CDD TEq

0.79 9.4 28 0.06 2.0 24

107 78

2,3,7,8-TCDD
2,3,7,8-TCDFTEq

0.24 2.2 0.40 2.9

5.2 19 5.9 20

Totalsim. trialTEq

5.0

35

127

468

burned is somewhat less than the corresponding simulated means of 127-129 TEq/metric ton. It is possible that part of the differencebetween the directly calculated and the simulated means reflects a small downward bias in the seven-plant calculated mean derived from the fact that in the direct procedure, levels of emisions for unmeasured congeners were implicitly assumed to be zero (whereas randomly chosen values for all congenerswere included in the simulation calculation).

Hattis and Burmaster

Table VI. Variability of Total Plant TEq Emissions Inferred
Directly from Data for the Seven Plants with Relatively Complete Congener Emissions Information

Facility No.

Total TEq

log(tota1plant TEq)

2 3 4 5 6 8 10 Mean SD SE Geom. mean Geom. SD Geom. SE

68.46 76.15 6.27 11.60 317.42 6.83 9.61 70.91 112.81 35.67 25.94
4.56 1.62

1.835 1.882 0.797 1.065 2.502 0.835 0.983 1.414 0.659 0.208

04
-2

-1 0
Z-Score

1

I
2

Fig. 11. Distribution of total TEq emissions/metric ton for the seven
plants with relatively complete data.

3. DISTRIBUTIONALDESCRIPTION OF UNCERTAINTY

3.1. Results of a "Classical" Analysis of Uncertainties in National Dioxin Congener TEq Emissions
Having given this extended example of the treatment of expected variability among plants, it is well to show how this contrasts with an analysis of uncertainty in overall national average TEq emissions. For this, we adopt the point of view of a national decision-maker assessing the potential aggregate risks to the country as a whole of emission of dioxin-related compounds from municipal solid waste incinerators. It should be stressed that in this illustrative analysis we focus only on TEq-

normalized emissions, and neglect what may well be the greater uncertainty in the TEq values themselves.
The analysis begins with a similar series of Monte
Carlo simulation trials of individual plants for total TEq
emissions as was derived for Table V above (including the correlations in the emissionsof different congeners). But for this purpose, one must take cognizance of the fact that there are approximately 10 plants worth of information available for estimating the national average TEq emissions. (The available information is actually somewhat less than 10 full plants worth, because not all congeners were analyzed for all facilities.) We therefore grouped the individualplant trials into aggregates of 10plants each, calculating the arithmetic mean TEq for each of these 10-plant groups. The distribution of these TEq averages is approximately, but not exactly, lognormal with a log,,(geometric standard deviation) of about 0.3 (Fig. 12 and Table VII).
A conventional analysis of the portion of the decision-maker's uncertainty arising simply from the reported measurements would usually end at this point. (Of course, in a full analysis of uncertainties facing a decision-maker, there would be several other sources of uncertainty to consider, such as the representativenessof the standard configuration MSW plant, possible future changes in emissions resulting from changes in the national average mix of municipal solid waste burned, etc.). The log(GSD) = 0.3 for national average TEq levels determined in this way essentially represents the implications for uncertainty of the random variability seen in the available dioxin emission measurements.
3.2 The Shlyakhter/KammenInnovation-The Likely Magnitude of Unmeasured Systematic Error
In a publication cited previously{20)one of us proposed as a third candidate "law" of uncertaintylvariability analysis: "The application of standard statistical techniques to a single data set will nearly always reveal only a trivial proportion of the overall uncertainty in a parameter value." The reason for this is that standard statistical techniques as generally applied to a single data set are wonderful at representing random errors in the data but cannot possibly discover anything about systematic errors shared by all the data points that result from a particular study. As a general matter, our experience is that systematic errors tend to be large relative to the random errors that are easily quantified within data
sets. (Aspointed out by a reviewer, another contributor
to error here may be the fact that commonly procedures

Variability and Uncertainty Distributions

727

for estimating confidence limits from data assume that means and standard deviations are known, rather than simply estimated from small data sets. Alternative procedures exist to calculate "tolerance intervals," which include measures of confidence in the means and standard deviations of small data sets.)
Recently, in a brilliant one-page paper in a Russian emigre physicist and a coworker have suggested an approach for making at least some tentative estimates of the likely amount of undiscovered systematic error in experimental measurements. The most telling of three sets of observations analyzed in that paper was the historical changes in the estimated values and stated uncertainties calculated by physicists for 79 elementary particle properties (principally, meson masses
and lifetimes). As has been noted by others in the pastJz2)
the more recent values of these "constants of nature'' fall outside previously stated confidence bounds much more frequently than would be expected by chance. Shlyakhter and Kammen observe that the probability of departures of various sizes can be approximately de-
-scribed by a simple exponential function: P(x) e-Wu
where P(x) is the likelihood of finding a deviation of the new "exact" value of the parameter x stated standard errors or more from the old central tendency estimate of the parameter, and u is the ratio of unmeasured systematic error to measured random error captured by standard statistical procedures? Shlyakhter and Kammen observe that for their physical constants data sets u is approximately 1. (Another type of prediction, forecasts of energy consumption and population size, was found to be markedly worse than this, with u about 3-5.) For example, using the conventionalgaussian distribution,95%
confidence limits are taken as the mean +. 1.96 SE. Us-
ing the compound distribution of both systematic and random errors (for which the simple exponential function quoted above is an approximation)with u = 1,95% confidence limits would be calculated as f 3 . 9 SE. With u = 2, a range of 2 6 SE would be needed to have a 95% probability of encompassing the "true" value of the parameter of interest. Table VIII shows the implications of the compound distribution function for spreading two-tailed confidence bounds for u = 1 and u = 2.
* More recently, on the basis of a more exact mathematical treatment
-of the expected relationship, Shlyakhter et have suggested a
slightly different exponential formula as a good approximation for values of u between 1 and 3: P(x) e-l.Xo.7n+ O.6). Both of these formulas are approximations to a more complex "compound" distribution of the systematic and random errors.

l!. , . , . , . , . I . I

-3 - 2

-1

0

1

23

2-score

Fig. 12. Distribution of average TEq values from simulated groups of 10 standard municipal solid waste incinerators-lognormal plot.

Table W.Results of a "Classical" Uncertainty A n a l y s i e Distribution of Average TEq Values in Simulated Groups of 10
Standard Muncipal Solid Waste Incinerators, in Comparison with a Fitted Lognormal Distribution (Straight Line in Fig. 12)

Percentile of
distribution

Fitted lognormal distribution

Distribution resulting from Monte Carlo
simulation

1 2.5 5
10 25 50 75 90 95 97.5 99 Arith. mean Geom. mean
Geom. SE log(geom SD)

19 25 31 40 61 99 160 245 317 396 514 130 99 2.03 0.307

28 30 33 42 60 98 161 243 317 503 757 130 99 2.03 0.307

In our judgment, the measurements of emissions of dioxins and dibenzofurans reported in Dr. Cleverly's compilation should not be expected to incorporate less unmeasured systematic error, relative to random error, than the measurements of the elementary particle physicists which gave rise to the u = 1 observations. [For example, sources of potential systematic bias in the congener emission measurements might include (A) possible differences in average recovery of individual congeners in the forms collected from the incinerators (mixed particulate, etc.), relative to the recovery of pure compounds used in calibrating runs; (B) possible biases arising from the difficulties in precisely determining the

728

Table VIII. Contrast Between Gaussian Confidence Limits and Confidence Limits Calculated According to the Revised ShlyakhteP) Compound Distribution Function

2-tailed confidence
level

Number of stated SE from the mean needed to achieve the confidence level

Standard gaussian distribution

Compound function with. u=1'

Compound function with u=20

50% 0.674

1.12

80% 1.282

2.27

90% 1.644 95% 1.96 98% 2.326 99% 2.576

3.07 3.87 4.91 5.70

99.9%

3.29

8.3

.Source: A. Shlyakhter, personal communication.

1.51 3.26 4.61 5.99 7.84 9.25 >>lo

Hattis and Burmaster
pared with the previously quoted results derived from the Monte Carlo simulation and the fitted lognormal distribution in Table VII. It can be seen that inclusion of this consideration considerably expands the 1-99% confidence range-from about 27-fold in Table VII to slightly over 1000-foldin Table IX. The 95%confidence limits are similarly altered. In Table VII the 95% confidence range encompasses approximately a 10-fold range of values; including the minimal estimate of un-
suspected systematic errors in Table IX inflates this to
over 200-fold. Thus if the phenomenon of unsuspected systematic errors of comparable magnitude to random errors is in fact generally applicable to environmental measurements, classical procedures for estimating confidence limits are potentially seriously misleading.

Table IX.Estimation of the Uncertainty Distribution for National Average TEq Emissions from Standard Municipal Solid Waste
Incinerators, Based on the ShlyakhterKamrnen Exponential Function with u = 1

Percentile of uncertainty
distribution

Required SD from mean log
(from Table MII, Col. 3)

log (average TEq Emissions) based on 10 plants"

Average TEq emissions based on 10 plants

1 -4.91 2.5 -3.87 5 -3.07 10 -2.27 25 -1.12 50 0.00 75 1.12 90 2.27 95 3.07 97.5 3.87 99 4.91

0.435 0.798 1.07 1.35 1.72 1.995 2.27 2.64 2.92 3.19 3.55

3.1 6.4 11.3 19.9 44.8 98.9 218 492 866 1520 3180

aThe numbers in this column are simply the classical log standard deviation from Table MI (0.307) times the values in the previous column plus the median log (1.995).

areas under different peaks on chromatograms, in the presence of nearby peaks due to other substances; and (C) some possible tendency on the part of the operators of the incinerators to avoid nonideal operating conditions or feed mixes on the days of testing, etc.]
Therefore, we believe that the log(GSD) = 0.3 for groups of 10 simulated incinerators determined above is likely to understate the actual uncertainty that a national decision-maker really faces. Table IX shows how the distribution of overall average TEq emissions would be broadened by an assumption of u = 1.This can be com-

33. The Stopping Problem-Uncertainty of the Uncertainty? Uncertainty of the Uncertainty of the Uncertainty?
The title of this subsection is somewhat facetious. But for those of us "enthusiasts" for quantitative assessment of variability and uncertainty, it poses a final reminder. If, as indicated by the third proposed "law" of uncertainty/variabilityanalysis, we must generally be more uncertain about the magnitude of our uncertainty itself than we are about the uncertainty in the original estimate, there is a point at which our capability to produce potentially useful information disappears into the noise. Some might suspect that in raising the issue of the uncertainty of the uncertainty, we are well past that point.
4. CONCLUSIONS
We have illustrated the different kinds of techniques available for assessing the variability and uncertainty of parameters used as inputs for risk analyses. These techniques have the potential to improve the clarity with which both risk analysts and decision makers understand risk distributions. While analyses based on single-point treatments of inputs are likely to continue to be useful as screening tools in relatively routine circumstances, more sophisticated distributional treatments of data are feasible and likely to be helpful in clarifying the likely consequences of alternative policies for abating various hazards.

Variability and Uncertainty Distributions

729

ACKNOWLEDGMENTS
Dale Hattis is grateful for support from the U.S. Environmental Protection Agency via CooperativeGrant Agreements 818679 and 816974 during the composition of Sections 1.2.4, 1.2.5,2, and 3. Special thanks are also due to Dr. Dave Cleverly of USEPA, who allowed the use of his analyses of normalized dioxin emissions for different municipal waste treatment plants, and to Dr. Alex Shlyakhter, who provided the data for Table VIII. David E. Bumaster thanks Alceon Corporation and General Electric Company for support during his composition of Sections 1.1and 1.2.1-1.2.3.
REFERENCES
1. D. Hattis, L. Erdreich, and T. DiMauro, "Human Variability in Parameters That Are Potentially Related to Susceptibility to Carcinogenesis. I. Preliminary Observations, Report to the Environmental Criteria and Assessment Office, U.S. Environmental Protection Agency (MIT Center for Technology, Policy and Industrial Development, Report No. C P I D 86-4, May, 1986).
2. D. Hattis, "Book Review of Ethnic Differences in Reactions to Drugs and Xenobiotics, W. Kalow, W. Goedde, and D. P. Agarwal," Science, 234, 221-222 (1986).
3. D. Hattis and R. Goble, "Current Priority-setting Methodology: Too much Rationality or too little?" In Worst Things First? The Debate over Risk-Based National Environmental Priorities. A. M. Finkel and D. Golding. John Hopkins University Press, Baltimore, 1994. (In Press)
4. G. Gale, "Science and the Philosophers," Nature 312, 491-495 (1984).
5. F. Suppe, "The Search for Philosophic Understanding of Scientific Theories," in F. Suppe (ed.), The Structure ofScientqc Theories (University of Illinois Pnes, 1977).
6. C. 0.Jones, S. Gauld, J. F. Hurley, and A. M. Rickmann. Personal differences in the breathing patterns and volumes and dust intakes of working miners. Report to the Commission of the European Communities, Report No. TM/81/11, Environmental Branch, Institute of Occupational Medicine, Roxburgh Place, Edinburgh, Scottland (1981).
7. D. Hattis and K. Silver. "Human Interindividual Variability-A Major Source of Uncertainty in Assessing Risks for Non-Cancer Health Effects." Risk Analysis 14,421-431 (1994).
8. D. Hattis, Risk Assessment for Acute Exposures to Chlorine or A m m o n i a 4 Theoretical Toxicological Perspective, Report Prepared for Environmental Resources LTD, Mar. (1984).
9. D. Hattis, PhannacokineticPrinciplesfor Dose Rate Extrapolation of Carcinogenic Risk from Genetically Active Agents," Risk Analysis 10,303-316 (1990).
10. D. Hattis and K. Shapiro, "Analysis of Dose/Time/Response Relationships for Chronic Toxic Effects-The Case of Acrylamide," NeuroToxicology 11, 219-236 (1990).
11. M. P. Wang and S. A. Hanson, "The Acute Toxicity of Chlorine on Freshwater Organisms: Time-Concentration Relationships of Constant and Intermittent Exposures," in R. C. Bahner and D. J. Hanson (eds.), Aquatic Toxicology and Hazard Assessment: Eighth Symposium, ASTM STP 89 (American Society for Testing and Materials, Philadelphia, 1985), pp. 213-232.
12. Entropy, Stationary source sampling report. Signal Resco Pinnellas County resource recovery facility, St. Petenburg, FL, Refer-

ence # 5286-B, (Entropy Environmental Inc., Research Triangle Park, NC, Sept. 1987).
13. D. R. Knisley, C. G. Jamgochian,W. P. Gergen, and D. J. Holder, Draft emissions test report dioxidfurans and total organic chlorides emission testing. Saugus resource recovery facility, Saugus, MA (Radian Corporation for Office of Air Quality Planning and Standards, US. EPA, Research Triangle Park, NC, DCN No. 86223,015-05, OCt. 2, 1986).
14. S. Marklund. L. 0. Kjeller, M. Hansson, M. Tysklind, and C. Rappe, Determination of PCDDs and PCDFs in incineration samples and pyrolytic products. Presented at the American Chemical Society National Meeting, Chlorinated dioxins and dibenzofurans in the total environment, Miami, FL, Apr. (1985).
15. R. Seelinger, J. Hahn,H. P. VonDrnfange, and R. A. Zurlinden, Environmental test report, Ogden Marten Systems of Tulsa, Inc., Compliance with Tulsa City/County health department and U.S. EPA permits. Sept. 9 (1986).
16. P. C. Siebert, D. Alston-Guiden, and K. H. Jones, "An Analysis of Worldwide Resource. Recovery Emissions Data and the Implications for Risk Assessments, in H. A. Hattermer-Frey, and C. Travis (eds.), Health Effects of Municipal Waste Incineration (CRC Press, B o a Raton, FL, 1991). pp. 2-46.
17. U.S. Environmental Protection Agency, Comprehensive Assessment of the Spec@ Compounds Present in Combustion Processes.
VoL 1. Pilot Study of CombustionEmission Variability (Officeof
Toxic Substances, Washington, DC, EPA-560-5-83-004, June
1983). 18. U.S. EnvironmentalProtection Agency, Municipalwaste combus-
tion multipollutant study. Summary report. Signal Environmental Systems, Inc., North Andover RESCO, North Andover, MA (Office of Air Quality Planning and Standards, Research Triangle, Park, NC, EMB Report No. 86-MIN-O2A,Mar. 1988). 19. US.EnvironmentalProtection Agency, Municipal waste combustion multipollutant study. Summary report. Marion County solid waste-to-energy facility, Brooks, OR (Office of Air Quality Planning and Standards, Research Triangle Park, NC, EMB Re-
port No. 86-MIN-03& Sept. 1988).
20. D. Hattis, "Three Candidate `Laws'of Uncertainty Analysis," Risk Analysis 10, 11 (1990).
21. A. I. Shlyakhter and D. M. Kammen, "Sea-Level Rise or Fall?" Nature 253,25 (1992).
22. M. Henrion and B. Fischoff, American Journal of Physics 54, 791-797 (1986).
23. D. M. Murray and D. E. Burmaster, "Estimated Distributions for Total Body Surface. Area of Men and Women in the United States," Journal of Exposure Analysis and Environmental Epidemiology 2, 451461 (1992?).
24. J. Brainard and D. E. Burmaster, "Estimated Bivariate Distributions for Height and Weight of Men and Women in the United States," Risk Analysis 12, 267-275 (1992).
25. A. M. Roseberry and D. E. Burmaster, "Lognormal Distributions for Water Intake by Children and Adults," Risk Analysis 12, 99-
104 (1992). 26. A. M. Roseberry and D. E. Burmaster, "A Note: Estimating Ex-
posure Concentrations of Lipophilic Organic Chemicals to Humans via Raw Fingish Fillets." Journal of Exposure Analysis and Environmental Epidemiology 1,513-521 (1991).
27. K. M.Thompson and D. E. Burmaster, "Parametric Distributions for Soil Ingestion by Children," Risk Analysis 11, 339-342
(1991).
28. D. E. Burmaster, D. M.Murray, and P. D. Anderson, "Distributions for the Daily Consumption Rates of Self-Caught and StoreBought Fish by Respondents to an Ontario Questionnaire," manuscript available from dioxon corp. (see footnote 2).
29. B. Ruffle, D. E. Burmaster, P. D. Anderson, and H.D. Gordon,
"Lognormal Distributions for Fish consumption by the General US Population," Risk Analysis, 1994, in press.

30. K. J. Lloyd and D. E. Burmaster, "Distributions for Body Weights
of Female and Male Children in the United States," manuscript from dioxon co'p. (see footnote 2).
31. B. Ketterer, D.J. Meyer, E. Lalor, P. Johnson, F. P. Guengerich, L M. Distlerath, P. E. B. Reilly, F. f. Kadlubar, T. J. Flammang, Y. Yuamazoe, and P. H. Beaune, "A Comparison of Levels of
glutathione Transferases, Cytochromes P450 and Acetyltransferases in Human Livers,'' Biochemical Pharmacology 41,635637(1991).
32. D.W.Layton, "Metabolically ConsistentBreathing Rates for Use in Dose Assessments," Health Physics 64,23-36 (1993).
33. A. Shlyakhter, "Improved Framework for Uncertainty Analysis: Accounting for Unsuspected Errors," Risk Analysis 14,441447 (1994).
34. C. C. Travisand and M. I. Land, "Estimating the Mean of Data Sets with Nondetectable Values," Environmental Science TechnoZogy 24, 961-962 (1990).

Hattis and Burmaster
35. J. D. Sahl, T. T. Crocker, R.J. Gordon, and E. J. Faeder, "Polychlorinated Biphenyl Concentrations in the Blood Plasma of a Selected Sample of Non-occupationally Exposed Southern California (U.S.A.) Working Adults," Science in the Total Environment 46,9-18 (1985).
36. Institute of Medicine, Seafood Safefy (National Academy Press, Washington, DC, Committee on the Evaluation of the Safety of Fishery Products, Food and Nutrition Board, Instituteof Medicine, Farid E. Ahmed, ed., 1991).
37. A. Shlyakhter, I. Shlyakhter, C. Broido, and R. Wilson, "Esti-
mating Uncertainty in Physical Measurements, Observationaland Environmental Studies: Lessons from Trends in Nuclear Data," Procedures of the Second Internationalsymposium on Uncertainty Modeling and Analysis (ISUMA '93) (University of Maryland, College Park, MD, USA, Apr. 25-28, 1993), pp 310-317 (1993) IEEE Computer Society Press, Los Alamitos, CA.

