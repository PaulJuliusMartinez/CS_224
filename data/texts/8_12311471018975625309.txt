Using Statistical Learning Theory to Rationalize System Model Identification and Validation Part I: Mathematical Foundations
A. A. Guergachi School of Information Technology Management, Ryerson University, Toronto, Ontario, Canada, M5B 2K3
G. G. Patry Department of Civil Engineering, University of Ottawa, Ottawa, Ontario, Canada, K1N 6N5
Existing procedures for model validation have been deemed inadequate for many engineering systems. The reason of this inadequacy is due to the high degree of complexity of the physical mechanisms that govern these systems. It is proposed in this paper to shift the attention from modeling the engineering system itself to modeling the uncertainty that underlies its behavior. A mathematical framework for modeling the uncertainty in complex engineering systems is developed. This framework uses the results of computational learning theory. It is based on the premise that a system model is a learning machine.

1. Introduction
Modeling of engineering systems is traditionally carried out in three sequential steps.
1. Model development. The modeler collects available knowledge about the studied system S in the form of first principles, empirical laws, and/or heuristic hypotheses. Based on this knowledge, the modeler develops a set of mathematical relationships (i.e., the system model ) among the system state variables, which can generally be written in the form of a differential equation:

x f (t, x, p)

(1)

where t is the time, x is the system state vector, p is the model parameter vector, and f is a mathematical function which is generally nonlinear.

Electronic mail address: a2guerga@ryerson.ca. Electronic mail address: patry@uottawa.ca.

Complex Systems, 14 (2003) 63­90; 2003 Complex Systems Publications, Inc.

64 A. A. Guergachi and G. G. Patry

2. Model identification. After the model is developed, the modeler uses a set N (N being a natural number greater than 0) of empirical data:

N xdata(t1), xdata(t2), . . . , xdata(tN)

(2)

collected from the real operation of the system to identify the model parameters. This step usually requires the minimization of an objective function J(p) of the form:

J(p)

N
x(p, tk)
k1

xdata(tk) 2

(3)

where x(p, t) represents the solution to the model equation (1). In most cases, the data set N would actually be divided into two subsets N1 and N2 (N N1 N2). The first subset (called identification sample) is used for the model parameter vector identification and the second (called validation sample) for model validation (step 3).
3. Model validation. In this step, the identified system model is tested on the validation subset N2 that it has never "seen." If the model performs well on this sample, then it is retained. Otherwise, the model structure is adjusted and the validation procedure repeated.
The foregoing model validation procedure (called cross validation) has been criticized in many areas of engineering. In wastewater engineering, for example, in [1] Jeppsson pointed out that, "in a strict sense, model validation is impossible" with the existing validation techniques. Similarly, Zheng and Bennett in [2] noted that, in groundwater engineering, "models, like any scientific hypothesis, cannot be validated in the absolute sense . . . They can only be invalidated." Konikow and Bredehoeft suggested in [3] that terms like "model verification" and "model validation" convey a false sense of truth and accuracy and thus should be abandoned in favor of more realistic assessment descriptors such as history-matching and benchmarking.
The engineering systems for which the cross validation procedure is deemed inadequate all share one similar feature: the mechanisms that govern each one of them are so complex that no single model can be considered to describe these mechanisms in their entirety. The predictions of a model, no matter how sophisticated it is, are not guaranteed to match the reality. In this paper, it is proposed to shift the attention from modeling the system itself to modeling the uncertainty that underlies its behavior. The aim is to answer questions such as: What makes uncertainty high or low? How can it be controlled and to what extent can it be reduced?
A mathematical framework for modeling the uncertainty in complex engineering systems is developed in this paper. This framework is based

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

65

on the premise that a system model is a learning machine. The model

identification procedure is viewed as a learning problem or, equivalently,

an information transfer from a finite set of real data N into the system model. Uncertainty is measured by the deviation between the

system's actual response function (see below for the definition of this

function) and the approximation delivered by the system model (af-

ter identification) for this function. This deviation is also a measure of

the performance of the system model: the smaller the value of , the

higher the performance of the model.

The framework leads to a set of inequalities (called uncertainty mod-

els, see equation (4)) that define bound functions on the deviation .

These inequalities are of the general form

. The bound functions

are dependent on (among others): (1) the amount N of data used to

carry out the system model identification, and (2) the complexity of the

system model structure q. The inequalities

can be utilized to

evaluate the quality of a system model after it has been identified and

are useful for system modelers in many respects.

When a set of values of N and q is fixed (e.g., N N0 and q q0), the inequalities allow the modeler to compute a bound 0 ( 0 being the value of for N N0 and q q0) on the deviation which, as indicated above, represents a measure of the model performance. The modeler then obtains a guarantee on the model quality.

When the model structure complexity (i.e., q) is fixed (e.g., q q0), the inequalities allow the modeler to assess the rate of model performance improvement as the value of N increases. This assessment can be done by computing the partial derivative:

N q0

of the bound .

When the amount of data N is fixed (e.g., N N0), the inequalities allow the modeler to select the optimal model structure complexity qopt that minimizes the bound function , N being set equal to N0.

Consequently, the inequalities

can potentially be used as replace-

ments for the traditional system model validation procedures, since they

provide the system modeler with a method of computing a guarantee

on the model performance.

The development of the framework is based on the extensive research

work by Vapnik in [4, 5, 6] and that of Vapnik and Chervonenkis in

[7, 8, 9] in the area of mathematical statistics and its applications to

computational machine learning theory. Section 2 shows why and how

a system model can be considered as a learning machine. The remainder

of the paper is devoted to the framework development.

Complex Systems, 14 (2003) 63­90

66 A. A. Guergachi and G. G. Patry

2. A system model is a learning machine

Assume that we are interested in the variations of one state variable

xgoi0voerfntshtehesydsytenmamSicasnodf

consider the model this variable:

differential

equation

that

x i0 f (t, x, p) or

dxi0 dt

f (t, x, p)

(4)

where t is the time, x is the process state vector, p is the parameter vector, and f is a real-valued function. This equation represents one component of the vector differential equation:

x f (t, x, p)

of the system model . However, the vectors x and p in equation (4) do not necessarily contain all of their components. Normally, they should
be denoted as xxi0 and pxi0 and equation (4) should become:

dxi0 dt

f (t, xxi0 , pxi0 )

(5)

in order to highlight the fact that x and p contain only those state varTiahbislesstaunddypwarilalmbeetelirms,irteesdpetoctitvheelyc,atsheatoifnafluuteonncoemthoeudsysnyasmteimcsso, ftxhia0t.
is, systems whose models do not depend explicitly on time. In other words, the general model equation that governs xi0 can be written as:

dxi0 dt

f (xxi0 , pxi0 ).

(6)

In addition to assumed to be

xi0 , all state variables; that is, components directly and separately measurable.

of

xxi0 ,

are

Using the Euler method to numerically integrate equation (6), the

time is discretized with a time step of times

t and then xi0 is computed at

t1 t , t2 2 t , . . . , tn n t , . . . using the following equation:

xi0 (tn) xi0 (tn 1)

t f (xxi0 (tn 1), pxi0 ).

Define wtn as the value of xi0 to be predicted by the model

wtn xi0 (tn).

Complex Systems, 14 (2003) 63­90

(7) , that is:

Using Statistical Learning Theory

67

Similarly, define the vector vtn as:

vtn [xi0 (tn 1), xxi0 (tn 1)T]T ,

(8)

where the superscript T denotes takes values from a subset W of multidimensional space V.

a transposed the real line

vector. , and

vTehcteonr uvmtn bferromwtna

Now introduce the real-valued function H defined as:

H(vtn , pxi0 ) xi0 (tn 1)

t f (xxi0 (tn 1), pxi0 ).

(9)

The expression of this function corresponds to that of the right-hand side of equation (7). The latter equation then becomes:

wtn H(vtn , pxi0 ).

(10)

For a fixed parameter function from V to W:

vector

pxi0 ,

H(

.

, pxi0 )

represents

a

mapping

H( . , pxi0 )

V vtn

W wtn H(vtn , pxi0 ).

(11)

The parameter vector denoted here as .

Dpxeifi0 nteaktehsevafulunecstifornoaml

a multidimensional space set of all mappings

H( . , pxi0 ) with pxi0

:

H( . , pxi0 ) pxi0

.

(12)

Now assume that a sequence of actual measurements of the couple (vt, wt):

N (v1, w1), (v2, w2), . . . , (vN, wN)

can be obtained from the real process operation, and consider an algo-

rithm that receives the sequence N as input and produces a parameter

vbeecsttoarp(pprxoi0x)iemmpacteosrrtehseproenadl ipnrgotcoestsherefsupnocntisoen.

IHn(p.r,a(cptxici0e),emthp)is

that algorithm

corresponds to the system model identification procedure which consists

of minimizing an objective function of the form:

J(p)

N
wk
k1

or, equivalently:

H(vk, p) 2

(13)

Remp(p)

1 N

N

wk

k1

H(vk, p) 2 .

(14)
Complex Systems, 14 (2003) 63­90

68 A. A. Guergachi and G. G. Patry

The subscript emp means "empirical" and the number wk H(vk, p) 2 represents a measure of the loss between the desired response wk corresponding to the vector vk and the model prediction represented by H(vk, p).
A set of mapping functions equipped with an algorithm such as is called a learning machine in the area of artificial intelligence and computational learning theory. We have shown that the couple S ( , ), composed of a system model and an identification procedure, can be viewed as a learning machine. On the basis of this result, it is possible to develop a mathematical framework that will allow us to model the uncertainty that underlies the behavior of the engineering system S, and rationalize the procedures of system model identification and validation. The next sections of this paper are about the development of this framework. It should be noted that one of the objectives of the framework is to abstract the basic notions (system, model, model parameter, and objective function) of the traditional system modeling approach (introduced previously), in order to enhance the system model identification and validation procedures. As a result of this abstraction work, several new concepts are introduced in the following sections of the paper. For a full and detailed explanation of the implementation of these concepts in the case of modeling a concrete engineering system, the reader is referred to Part II of this paper [10]. As a transition to section 3, however, the following table presents some preliminary indications regarding the correspondence between the traditional approach's basic notions and the framework's new concepts.

Traditional approach basic notions The system S itself. The system surrounding S. The space ( being the system model). The model parameters p.
The objective function J(p).

Corresponding framework concepts The transformer . The environment . The decision rule space .
The parameters that characterize the elements of . The empirical risk.

Note also that several concepts will be introduced in the framework,
with no corresponding notions in the traditional system modeling approach (e.g., the expected risk, the VC dimension).

Remark 1. Note that training of the machine S ( ,)
associated with the system S is carried out for a specific time tn. This time is arbitrary, but fixed. The examples (v1, w1), (v2, w2), . . . , (vN, wN) to
Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

69

be used for machine training should therefore correspond to a realization of the system at time tn, for a fixed n (i.e., a realization in the ensemble of the stochastic process (vt, wt), and not in time; see pages 372 and 442 of [11]). In practice, this is not possible, because the instance vector vt and the outcome wt are measured only once at any time instant t. And what is obtained from these measurements is actually a time series:

(vt1 , wt1 ), (vt2 , wt2 ), . . . , (vtn , wtn ), . . .

whose terms represent the couples instance/outcome at successive time

instants t1, t2, . . . , tn, . . . . It corresponds to one realization of the system S in time. This realization would usually--if not always--be the only one

that is available for investigating the system's behavior. The property

that allows ergodicity.

us to use the This condition

siserqieusite(vwti ,ewakti

) instead and will

of be

(vi, wi) is called assumed to hold

true for the studied system S. A thorough discussion of this condition

and how it is utilized to implement this framework in the case of a

concrete engineering system is presented in Part II of this paper [10] as

well as in [12].

3. General description of the framework

In a certain environment , at a time instant t, a situation vt arises randomly and a transformer acts and assigns to this situation vt a number wt obtained as a result of the realization of a random trial. Formally, situation vt represents a random vector that takes values from an abstract space V called the instance space. It is generated according

to on

a fixed but unknown probability density V. The number wt, which is dependent

ofunnvctt,iornep(rPeDseFn)tsPavt

defined random

variable that takes values from another space W called the outcome

space. It is generated according W, also fixed but unknown. The

mtoaathceomnadtiitcioalnoabl jPeDctF(vPt,wwt vtt)

defined on arises then

in the product space Z V W (called the sample space) according to

the joint PDF environment

P. (vItn,wtw) haPtvft Polwlot vwt ,s,wthhiechcocuhpalreac(tvetr,iwzets)

the probabilistic is denoted as zt

(meaning that it takes values from the sample space Z). Using this

nboe tiantdioifnfe,rtehnetljyoicnatllPeDdFsiPtu(vatt,wiot)nisorthinenstdanenceotaenddatshPeznt .uTmhbeevr ewcttooruvtct owmilel or transformer's response.

In the context of this paper, the parameter t represents the time;

but, a priori, it could refer to some other continuous parameter such

as distance or angle [11]. It takes values in the set of real numbers .

The family zt, t

of the random variables zt is a stochastic process

in the environment . The set

E(zt), t

of all possible

values of E(zt) can, in theory, cover the entire sample space Z. In

Complex Systems, 14 (2003) 63­90

70 A. A. Guergachi and G. G. Patry

practice, however, is a subset of Z that covers a specific region of

Z. This subset will be designated here as the operating mode of the

transformer .

To illustrate what is meant by "operating mode" consider, for in-

stance, the behavior of an automotive engine. The operating conditions

of such an engine are not the same when the car is climbing a hill as when

it is taking a highway. In the first case, the engine develops a very high

torque and the speed is low, while in the second case, the same engine

operates under opposite conditions: the speed is high but the torque is

low. Another example that illustrates this concept of operating mode is

a wastewater treatment plant using the activated sludge process. The

operation of this plant can use a little return sludge and low solids in

the aeration tank in order to achieve the objective of removing soluble

substrate with relatively low oxygen supply. But this plant could also be

operated with the purpose of aerobically destroying all of the organic

solids in the waste, which can be done by returning all the sludge to

the aeration tank. Thus, the same plant could operate under different

operating conditions.

Associated machine

with the environment

(,

whose objective is to understand

the

b, ezht,aPvzito)risofathleeatrrnainnsg-

former . It receives a finite sequence N of N training examples:

N ((vt)1, (wt)1), ((vt)2, (wt)2), . . . , ((vt)N, (wt)N)

or, using z-notation:

N (zt)1, (zt)2, . . . , (zt)N
generated in the environment , as a result of N distinct random trials in the space Z.
Note. For the sake of simplifying the notations, we shall, in what follows, denote the variables:

((vt)i, (wt)i) and (zt)i as simply:

(vi, wi) and zi respectively.

The elements zi of N are instances of the random vector zt that are obtained by physically measuring the components of zt at the end of each of the N random trials. Based on these training examples, the learning

machine

selects a strategy that specifies the best approximation

w of the transformer's response for each instance vt. Once this strategy is selected, it will be used on all future situations vt arising in the environment , in order to predict the transformer's responses. This

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

71

strategy, which is mathematically a mapping function from V into W,

is called a decision rule and is chosen from a fixed functional space

called the decision rule space.

The goal of

is then to select, from the space , that particular

decision rule which best approximates the transformer's response. The

expression "best approximation of the transformer's response" means

"closeness to the transformer's `general tendency' g ." The latter func-

tion is defined as

g (vt) E(wt vt)

W wt Pw vt (wt vt) dw.

(15)

This function will be indifferently called general tendency or response
function. Closeness is understood in the sense of the metric defined in the following way:

h,

(h, g ) E( l(h(vt), g (vt)) )

l(h(vt), g
V

(vt)) Pvt(vt) dvt

where l is defined throughout this paper as the quadratic loss:

(16)

(a, b) 2, l(a, b) a b 2.

After receiving the sequence N of training examples, the learning

machine

selects that particular decision rule h0 which minimizes

(h, g ) on the space (h designates an element of and g the trans-

former's general tendency). Formally, this means finding the minimum

of the function:

( . ,g ) h

(h, g )

and the decision rule h0 at which this minimum is attained. To do so, implements an algorithm whose ultimate goal is to find h0 on
the basis of the finite sequence N of training examples. Note that wt is related to g (vt) through the following relationship:

w g (vt) 

(17)

where  is the noise associated with the probabilistic environment . By
the properties of conditional expectation, it follows from equation (17) that:

E( vt) 0.

(18)

Remark 2. The decision rule space is considered to be indexed by a

subset of n for some n 1, that is, there exist an integer n 1 and a

subset T n, such that the space can be expressed as T . This is the case for most engineering systems.

hp p

Complex Systems, 14 (2003) 63­90

72 A. A. Guergachi and G. G. Patry

4. Overcoming the first obstacle in minimizing the value of over the space

The objective of the learning machine

( , ) is to minimize the

distance (h, g ) over the entire decision rule space . This distance

involves two functions: h and g . The function h is an element of the

space and, as such, it is well known to : once the components of

vt are measured, the value of h(vt) is readily computable. The problem however is g . Not only is it an unknown function and impossible

to derive from first principles (recall that the systems we are dealing

with are complex ones), but there is no operational way of getting

even sample measurements or any empirical information about it. g

is indeed buried in noise. What we can measure, with respect to the

transformer's response, is the outcome wt, and wt contains in it both the value of g and noise, all mixed up.

So how should

proceed to minimize (h, g ), when the only

information it can get is in the form of noise-corrupted measurements

of the outcome wt and, of course, the instance vt? Theorem 1 will be of great help. Before stating it, we need the following definition.

Definition 1 (Expected risk) Let vironment and, associated with

( it,

, a

lear,nzitn,gPzmt )abcehianeprobabili(stic,

en).

Let h

be a decision rule. The expected risk R(h) of h is defined as

the expected value of the random variable:

l(h(vt), wt) h(vt) wt 2

when the vector zt (vt, wt) is drawn at random in the sample space Z

V .

W according to Formally, it is:

the

PDF

Pzt

P(vt,wt) corresponding to environment

R(h) E( l(h(vt), wt) )

V W l(h(vt), wt) P(vt,wt)(vt, wt) dvt dwt. (19)

Also, to simplify the notations, we need the following definition.

Definition 2 (Simplifying notations) Let tic environment and, associated with

( it,

, a

lea,rznti,nPgzt

) be a probabilismachine

( , ). For every decision rule h , we define the real-valued func-

tion lh on the sample space Z V W as follows:

(vt, wt) V W,

lh(vt, wt) l(h(vt), wt).

(20)

Hence, using z-notation, equations (19) and (20) become:

h

,

R(h) E(lh(zt))

Z lh(zt) Pzt (zt) dzt

zt (vt, wt) Z,

lh(zt) l(h(v), wt).

Complex Systems, 14 (2003) 63­90

(21) (22)

Using Statistical Learning Theory

73

Theorem
abilistic

1 (Transition
environment

(h, g
and,

) R(h)) Let associated with

( it, a

l,earni,nzgt,mPzatc)hbienea

prob-

( , ). Let h0

be a fixed decision rule. Then the function:

h (h, g )

is minimal at h0 if and only if the function: h R(h)

is minimal at h0. Proof. Using equation (15), it can be shown that the equality

R(h)

[w
VW

g (vt)]2 P(vt,wt)(vt, wt) dvt dwt

holds true for all h . Since the integral

[ (h, g )]2

(23)

[w
VW

g (vt)]2 P(vt,w)(vt, w) dvt dw

is independent of h, it follows that (h, g ) is minimal if and only if R(h) is minimal, and that both functions attain their minimum at the
same function h0.

Theorem 1 is very important in simplifying the learning problem that is faced with. What it means is that minimizing (h, g ) or, equivalently, the square of it [ (h, g )]2 over amounts to minimizing R(h) over the decision rule space. Look at the expressions of these two functions [ (h, g )]2 and R(h):

[ (h, g )]2 E( l(h(vt), g (vt))) and

(24)

R(h) E( l(h(vt), wt)).

(25)

From these expressions, it can be seen that, in the course of minimizing

(h, g ), Theorem 1 allows us to replace the unknown and unmeasur-

able noise-free value g (vt) by the measurable noise-corrupted value wt, without losing information on that decision rule h0 at which the minimum of (h, g ) is attained.

The following corollary will be helpful for system uncertainty model

development.

Corollary 1 (First inequality) Let

( , , zt, Pzt ) be a probabilistic en-

vironment and, associated with it, a learning machine

( , ).

Then the inequality:

[ (h, g )]2 R(h)

(26)

holds true for any rule h .

Proof. This inequality is a direct consequence of equation (23).

Complex Systems, 14 (2003) 63­90

74 A. A. Guergachi and G. G. Patry

5. Second obstacle: Pzt is not known to

Theorem 1 is still not enough for to proceed to the determination

of the rule h0 that minimizes (h, g ). This is because R(h) is func-

tion of the PDF the environment

Pzta: ndth, isasPsDuFche, mitbiosdnieost

all sources of uncertainty in known. The objective--and

the power--of the framework developed here consists in avoiding any

strong a priori assumption regarding the sources of uncertainty in .

CmoizNninsoegwqRu, eh(hna)tvloyin,ngintthawekhbeaantstifshoiolslfosowtansnl,ydPaoztfininsPictzeot,nnwsuiemdehbraeevrdeNfitoxoefifdntrbdauiant iwunngakyenxooafwmmnpi.nleis-

z1, z2, . . . , zN. How to do that? By introducing a principle called inductive principle of empirical risk minimization (IPERM). This prin-

ciple emerged in the 1980s as a result of the extensive research work in

[4, 5, 6] and that in [7, 8, 9].

6. Inductive principle of empirical risk minimization

Before we state the IPERM, we need to define the meaning of the empirical risk of a decision rule.

Definition 3 (Empirical risk) Let vironment and, associated with

( it,

, a

lear,nzitn,gPzmt )abcehianeprobabili(stic,

en).

Let h

be a decision rule and N (z1, z2, . . . , zN) a finite sequence

of N training examples generated and measured in the probabilistic en-

vironment as a result of one realization of this same environment.

The empirical risk RemNp(h) of h on the sequence N is defined as the arithmetic mean of the sequence of numbers:

(lh(zi))i 1,2,...,N that is,

RemNp(h)

1 N

N i1

lh(zi).

(27)

Having introduced the concept of empirical risk, we can now define what is meant by an uncertainty model.

Definition 4 (Uncertainty model)
environment and, associated

Let with

it,

( a

le,arnin,gzmt, Paczth)ibnee

a

probabilistic ( , ).

Let N be a finite sequence of N training examples from the environment and  is a fixed real number in the interval ]0,1[. Let hemNp be a decision
rule at which the empirical risk RemNp(h) reaches its minimum. An uncertainty model (or simply uncertainty model) of the transformer

is any inequality of the type:

(hemNp, g ) (e1, e2, . . . , el)

(28)

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

75

that satisfy the following two conditions.

1. Pr (hemNp, g ) (e1, e2, . . . , el) 1 . 2. e1, e2, . . . ,el are a set of uncertainty control variables and
function of these variables that satisfy the following:

is a real-valued

the variables ei and the function are readily determinable/computable.

(29)

Expected and empirical risks, R(h) and RemNp(h), may seem to introduce new concepts in this framework, but they are not if we go back to the concepts of probability theory. To see that, fix a decision rule h in the space . Since zt is a random variable, the number lh(zt) is then also a random variable. Denote it as , that is,

 lh(zt).
(Recall that h is fixed.) From probability theory, we know that there are two measures of the central tendency of a random variable such as .

An empirical measure. Given a series of realizations 1, 2, . . . , N of the variable , this measure is constructed by computing the arithmetic average ( i i)/N of this series.
A mathematical measure. This measure is expressed in terms of the PDF P of , that is: P() d. It is called the expected value.
In this framework, RemNp(h) represents the empirical measure of the central tendency of  lh(zt) and R(h) represents the mathematical one. The former measure is approximate but computable, the latter is exact but unknown. Also note that, under some conditions with respect to the dependency and heterogeneity of the realizations i, the empirical measure converges to the mathematical one when N is made infinitely large [13]. This is known as the Law of Large Numbers in probability theory. Applying this law to the case of the expected and empirical risks, we get that RemNp(h) converges (in probability) to R(h) as N is made infinitely large. That is,

RemNp(h) R(h) as N

.

(30)

The reader should note a very important fact here: the convergence equation (30) is valid for a fixed decision rule h in the space . This is called pointwise convergence, as opposed to another type of convergence (called uniform convergence) that is discussed briefly in the next sections. The term "pointwise" refers to the fact that the convergence equation (30) occurs only for fixed points of and not for all points of this space simultaneously.
Now we state the IPERM. This principle consists of implementing the following two actions.

Complex Systems, 14 (2003) 63­90

76 A. A. Guergachi and G. G. Patry

Action 1. Replace the expected risk R(h) by the empirical risk RemNp(h) computed on the basis of one training sequence N. Action 2. Take the decision rule hemNp at which RemNp(h) reaches its minimum as a good representation of the best rule h0 that minimizes the expected risk R(h).

Therefore, the implementation of the IPERM comes down to minimizing the empirical risk RemNp(h), instead of the expected one R(h), over the space and then choosing that decision rule hemNp at which the minimum of RemNp(h) is reached to describe the transformer's behavior. Engineering systems modelers (in various areas of engineering such as chemical, civil, or environmental) have been using this procedure for system model identification for years. The reader may then wonder: Why are we developing a new mathematical framework, if all we are going to do is turn back to the traditional model identification procedure? What is the point?
This framework is not about inventing new procedures, but rationalizing existing ones and modeling the uncertainty that is associated with them. Engineering systems modelers have been using the traditional identification procedure without being aware of the transitions

(h, g ) R(h) RemNp(h).

(31)

Their decision to rely on empirical risk minimization may be explained by the fact that mechanistic models (as opposed to balck-box models)
are usually assumed to contain adequate a priori information about the
real system and, as a result, very little information would be lost in the transition

R(h) RemNp(h).

(32)

Now we know that this is not true for a complex system, since all existing models represent just a simplified picture of the real system behavior. If the sequence N is a finite one, then there is definitely a loss of information in the transition equation (32), that has always been ignored by engineering systems modelers. The aim of this framework is to rationalize and investigate the validity of this transition. First, we determine in what cases the replacement of R(h) by RemNp(h) can be legitimized and, second, evaluate the loss of information that occurs in the course of this replacement. To do so, we need to examine the applicability of the IPERM, for which Vapnik's results will be of great help.

7. Applicability of the inductive principle of empirical risk minimization
In the transition (h, g ) R(h)
Complex Systems, 14 (2003) 63­90

(33)

Using Statistical Learning Theory

77

there is absolutely no information loss, by virtue of Theorem 1. As a result, R(h) can be considered as an exact measure of the performance of the decision rule h when this rule is selected by as an approximation of g . The transition that is problematic is the second one:
R(h) RemNp(h).
RemNp(h) is indeed just an estimation of R(h). Of course, one may argue that replacing R(h) by RemNp(h), as suggested in Action 1 of the IPERM, can be legitimized by the fact that, according to the Law of Large Numbers, RemNp(h) becomes a perfect estimation of R(h) when the size N of the sequence N is made infinitely large. But, this fact cannot be used to justify Action 2 of the IPERM. Here is indeed the problem.

As was done above, denote the decision rules that minimize R(h) and RemNp(h) as h0 and hemNp, respectively. This is equivalent to writing

RemNp (hemNp)

inf
h

RemNp (h)

and

(34)

R(h0)

inf R(h).
h

(35)

Action 2 of the IPERM stipulates taking hemNp as a good representation of the best rule h0. For this to be justified, we need to ensure that hemNp is very "close" to minimizing the expected risk R(h) which is, as pointed out previously, an exact measure of the rule's performance (meaning the rule's closeness to g in the sense of ). In more concrete terms, we need that the value R(hemNp) of the expected risk at hemNp be close to the minimum one R(h0), for N sufficiently large. That is,

R(hemNp) R(h0) as N

(36)

(convergence is understood in probability).

It has been shown [9] that the pointwise convergence equation (30) does not guarantee the one that is really required for the purpose of the IPERM, that is, convergence equation (36). In other words, it is possible that convergence equation (30) be satisfied, but R(hemNp) remains always far from R(h0)--even for large values of N--meaning that hemNp would never constitute a good approximation to the transformer's behavior. It is therefore important to verify whether the IPERM is applicable or not before using it in any learning problem.
Taking into consideration the foregoing comments, the following definition shall be adopted for the meaning of the applicability of the IPERM.

Complex Systems, 14 (2003) 63­90

78 A. A. Guergachi and G. G. Patry

Definition 5 (Applicability of the IPERM) Let abilistic environment and, associated with

( it, a

, , zt learning

,mPzatc)hbienea

prob-

( , ). Let N be a finite sequence of N training examples from the environment and let hemNp and h0 be two decision rules that minimize the risks RemNp(h) and R(h), respectively (refer to equations (34) and (35)). The IPERM is said to be applicable to ( , ) if, for any > 0, the

following equality holds true:

lim Pr
N

sup 
h

R(h), RemNp(h)

>

0,

with  being a deviation measure defined on the real line.

(37)

Now that the applicability of the IPERM has been defined, we need to develop a simple method of verifying it. In the foregoing discussion,
it has been pointed out that the pointwise convergence equation (30) is not enough to guarantee the applicability of the IPERM. A more
stringent condition regarding the empirical risk convergence needs to be
imposed. Vapnik and Chervonenkis showed in [9] that, for the IPERM to be applicable, it is necessary and sufficient that the empirical risk
RemNp(h) converges uniformly to the expected risk R(h) over the whole space (convergence is understood in probability). Mathematically,
uniform convergence means that equation (37) holds true. Intuitively, it means that, as N is made infinitely large, the whole curve of RemNp(h) converges to that of R(h) over the space . In this presentation, the theoretical part of such questions will not be detailed. Instead, the reader
is referred to Vapnik's book Statistical Learning Theory [6] for details. In
what follows, Vapnik's results are presented in a more practical fashion, allowing direct application to the cases under study in this paper (i.e.,
engineering systems). The mathematical rigor is, however, preserved
throughout the whole presentation. A criterion to verify the applicability of the IPERM is not the only
thing that is needed here. We also want to know how much information is lost when R(h) is replaced by RemNp(h). Here again, to evaluate this information loss, we need to define a measure of the deviation between R(h) and RemNp(h). For this purpose, two deviation relative measures are introduced.

Relative measure 1 defined by:

(a1, a2) 2,

1[a1, a2]

a1

a2 a1

.

(38)

Relative measure 2 defined by:

(a1, a2) 2,

2[a1, a2]

a1 a1 a2 .

(39)

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

79

Each of these measures will be associated with a different weak prior information about ( , ).
Using these measures, Theorem 2 defines sufficient conditions for the applicability of the IPERM and helps evaluate the loss of information that occurs when R(h) is replaced by RemNp(h).

Theorem 2 (Applicability of the IPERM) Let bilistic environment and, associated with

( it,

a

, , zt, learning

Pmzta)cbheinae

proba-

( , ). Let N be a finite sequence of N training examples from the environment and  is a real number in the interval ]0, 1[. Let  be

one of the deviation measures 1 or 2. If it is possible to establish some

weak prior information

about ( , ) and construct a function

dependent on N, the whole set ,

, and the number  such

that both Statements 1 and 2 listed below hold true, then the IPERM is

applicable to ( , ). When such a function

(N, ,

, )

exists, the IPERM is said to be -applicable to ( ,

(N, ,

, ).

) with the bound

Statement 1. For any  ]0, 1[, the inequality

sup  R(h), RemNp(h)
h

(N, ,

is satisfied with probability of at least 1

, ) .

Statement 2. When , , and

are fixed, then:

lim (N, ,
N

, ) 0.

Proof. Let > 0 and  ]0, 1[ be two fixed numbers. From Statement 2, we infer that:

N0 ,

N > N0,

(N, ,

, ) < .

Then, from Statement 1, we get that for N > N0, the inequality

sup  R(h), RemNp(h)
h

is satisfied with probability of at least 1 . That is,

Pr sup  R(h), RemNp(h) > < .
h
Thus, we have shown that, for any > 0:

 ]0, 1[, N0 , N > N0, Pr sup  R(h), RemNp(h) > < 
h Complex Systems, 14 (2003) 63­90

80 A. A. Guergachi and G. G. Patry

which means, by definition, that

lim Pr
N

sup 
h

R(h), RemNp(h)

>

0.

Now recall that the objective of this study is to develop uncertainty models (see Definition 4) for complex engineering systems. The follow-
ing theorem defines a way of developing such models.

Theorem 3 (Uncertainty model) Let environment and, associated with

it,

( a

le,arnin,gztm, Pazcth) ibnee

a

probabilistic ( , ).

Let N be a finite sequence of N training examples from the environment

and  is a real number in the interval ]0, 1[. Let

be some weak

prior information about ( , ) and hemNp a decision rule at which the empirical risk RemNp(h) reaches its minimum.

If the IPERM is 1-applicable to ( , then the inequality
[ (hemNp , g )]2 RemNp (hemNp )

) with the bound (N, ,

2(N, , 2

, )

, ),

1

1

4 RemNp (hemNp )

2(N, ,

, )

(40)

holds true with probability of at least 1 .

If the IPERM is 2-applicable to ( , then the inequality

) with the bound (N, , ,

),

[ (hemNp, g )]2 (1

RemNp (hemNp)

(N, ,

, ))

(41)

holds true with probability of at least 1 , where (a) sup(a, 0).

Proof. If the IPERM is 1-applicable to ( , ) with the bound (N, , , ), then, from Theorem 2, it follows that (all inequalities hold with probability of at least 1 ):

R(hemNp) RemNp(hemNp) R(hemNp)

(N, ,

, ).

Hence:

R(hemNp)

RemNp (hemNp )
2(N, , 2

, ) 1

1

4 RemNp(hemNp)

2(N, ,

, )

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

81

and then, from Theorem 1, it follows that

[ (hemNp, g )]2 RemNp(hemNp)

2(N, , 2

, ) 1

1

4 RemNp(hemNp)

2(N, ,

, )

.

Similarly, if the IPERM is 2-applicable to ( , ) with the bound

(N, , ,

), then, from Theorem 2, it follows that

R(hemNp) RemNp(hemNp) R(hemNp)

(N, ,

, )

and then [ (hemNp, g )]2 R(hemNp) (1

RemNp (hemNp ) (N, ,

, ))

.

The bound on the squared distance [ (hemNp, g )]2, when it exists, is called the guaranteed deviation between hemNp and g , and denoted as or as

(N, , RemNp(hemNp),

, ).

8. The Vapnik­Chervonenkis dimension

One of the objects which the guaranteed deviation is dependent on is the whole set of decision rules. Now we need to know exactly what characteristic of affects and the uncertainty equations (40) and (41). Intuitive analysis of uncertainty in engineering systems shows that this characteristic is the complexity of [12]. The objective of this section is to define a measure of this complexity. This measure is known as the Vapnik­Chervonenkis dimension, or simply VC dimension, named in honor of its originators, Vapnik and Chervonenkis [7]. The definition of this dimension is quite difficult to assimilate from the first reading. Because of this, an intuitive interpretation of the VC dimension will be given first and, at the end of this section, a series of illustrative examples are presented.

8.1 Intuitive introduction
Consider the following concrete example: V1 and W1 ; line is the set of all functions h from V into W such that:
x V, h(x) p1x p2 with p (p1, p2) 2 as the parameter vector.
Complex Systems, 14 (2003) 63­90

82 A. A. Guergachi and G. G. Patry

If we had to assign a number to the complexity of this set of functions, then intuitively the number two, corresponding to the number of parameters, would be the most suitable one. Consider now this second example:

V2 and W2 ; sine is the set of all functions h from V into W such that:

x V, h(x) p1 sin(p2x)

with p (p1, p2) 2 as the parameter vector.
Since the number of parameters that define this set is also two, we may be tempted to again assign the number two to the complexity of this set. If we do so, it would mean that line and sine have the same degree of complexity, which is obviously not correct: the set line is a family of just straight lines, while sine is a complex family of curves that can take many different shapes. The "expressive power" of sine is indeed much higher than that of line. As a result, it should be expected that the complexity of sine be much higher than that of line, and that is what we get when we consider the VC dimension as a measure of the complexity of the decision rule space.
Intuitively, the VC dimension may be considered as equal to the maximum number of points that the curves representing the functions of the decision rule space can pass through simultaneously. Straight lines (functions defined by h(x) p1x p2, space line) can pass through any two points, but not any three points. Parabolas (functions defined by h(x) p1x2 p2x p3, space parab) can pass through any three points, but not any four points. Sine functions (h(x) p1 sin(p2x), space sine) can pass through any number of points. Hence, if the VC dimension of a space is denoted as q( ), then:

q( line) q( parab)
q( sine)

2 3
.

The foregoing intuitive interpretation of the VC dimension is approximate. A more precise definition of it is given in section 8.2.

8.2 Definitions
For every set I, the notation 2I will designate the set of all subsets of I.
Definition 6 (VC dimension of a family of sets) Let G be some space ( n with n > 0, for example, or any other space). Let be a family of subsets of G (examples of in the case of G 2 are the family of all open [or
Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

83

closed] balls of 2 or the family of all half planes of 2) and I is a finite subset of G. Let (I) be the subset of 2I defined as follows:
(I) 2I F , F I .
The finite set I is said to be shattered by the family of sets if (I) 2I. The largest integer q such that some finite subset I G of size q is shattered by is called the Vapnik­Chervonenkis dimension (VC dimension) of the family . It is denoted by q q( ). If such an integer q does not exist, then the VC dimension of is said to be infinite.

Definition 7 (VC dimension of a family of functions) Let be a family of realvalued functions on some space G and I is a finite subset of G. For every function f , define the subset pos(f ) of the space G as follows:
pos(f ) a G f (a) > 0 .
Then define the family pos( ) of subsets of G as follows:

pos( ) pos(f ) f

.

The finite set I is said to be shattered by the family of real-valued functions , if it is shattered by the family of subsets pos( ). The VC
dimension q( ) of the family of real-valued functions is, by definition, equal to the VC dimension of the family of subsets pos( ):

q( ) q(pos( )).

The VC dimension is then a purely combinatorial concept that has, a priori, no connection with the geometric notion of dimension. In most situations, it is difficult to evaluate the VC dimension by analytic means. Usually, all that is possible is to determine a bound on the VC dimension, that is, establish an inequality of the form: q( ) q0 (q0 ). Also in some cases the VC dimension is simply approximated by the free parameters of the family . Theorem 4 shows how to determine it in some particular cases. It also establishes a link with the geometric notion of dimension.

Theorem 4 (VC dimension and vector space) Let be a family of real-valued

functions on some space G. Fix any function f0 from G into and let 0

be the new family of functions defined by 0 f0

f0 f f

. If

is an m-dimensional real vector space, then the VC dimension q( 0)

of 0 is equal to m:

q( 0) m.

Proof. Refer to [14] for the proof of this theorem.

Complex Systems, 14 (2003) 63­90

84 A. A. Guergachi and G. G. Patry

8.3 Examples

Example 1.
G n (n

Consider the family of functions hp defined from the space ) into 0, 1 by:

x (x1, x2, . . . , xn) n,

hp(x)

n
 pixi
i1

where p (p1, p2, . . . , pn, ) defined by (real threshold ):

n 1 is the parameter vector and  is

(a)

1 if a , 0 if a < .

This family of functions is known as the perceptron and is used in pattern recognition. Its VC dimension is equal to n 1 [15].

Example 2.
some space

Consider G by

the

family

of

real-valued

functions

hp

defined

on

x G,

hp(x)

n
pii(x)
i1

where p (p1, p2, . . . , pn) n is the parameter vector and 1, 2, . . . , n is a sequence of n linearly independent real-valued functions. The VC dimension of this family of functions is equal to n [4]. Note that the determination of this VC dimension results directly from Theorem 4.

Example 3. Consider the family of functions hp defined on G 2 by

(x, y) 2,

hp(x, y) (y polyn(x, p))2

where p (p0, p1, p2, . . . , pn) n 1 is the parameter vector and polym(x, p) is a polynomial function of degree n defined by

x,

polyn(x, p) p0 p1x p2x2

pnxn.

The VC dimension of this family of functions hp is at most 2n 2 [5].

Example 4. Consider the family of functions hp defined on G x , hp(x) p1 sin(p2x)

by

where p (p1, p2) 2 is the parameter vector. The VC dimension of this family of functions is infinite [6].

From these examples it can be seen that, generally speaking, the VC
dimension of a family of functions is not always related to the number of parameters. It can be larger (Example 4), equal (Examples 1 and 2), or

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

85

smaller (see [5] where new types of learning machines were constructed) than the number of parameters.

9. Vapnik­Chervonenkis dimension and applicability of the inductive principle of empirical risk minimization

In section 7, the concept of applicability of the IPERM and that of

guaranteed deviation between the decision rule hemNp that minimizes the empirical risk and the transformer's response function g were intro-

duced. However, no methodology has been developed to determine the

expression of the function

(N, ,

, ) (see Theorems 2 and

3), which is the key function in implementing those concepts. In this

section, some fundamental results with respect to determining such a

function are presented. These results make use of the VC dimension

concept defined in section 8 and are due to [6]. Extensive discussion

and application of these results to model identification and quality eval-

uation can be found in [12].

Before stating these results, we need to define a new space l and five

different conditions.

Definition 8 (Space l ) Let ment and, associated with

( it,

, a

learn,iznt,gPmzt )abcheianeprobabili(stic,

environ). For

every decision rule h

and a real number 

, we define the

real-valued functions lh,  on the sample space Z V W as follows:

zt Z,

lh, (zt) lh(zt) .

The functional space of all functions lh,  will be denoted by l :

l lh,  (h, )

.

Now we define the conditions C.1, C .1, C.2, C.3, and C .3.

C.1 Weak prior information (1). There exists a positive number M ]0, [ such that:

sup lh(zt) M.
h ,zt Z

C .1 Weak prior information (2). There exists a pair (s, ) and  < such that:

2 with s > 2

sup
h

E1/s ([lh (zt )]s ) R(h)

<

.

C.2 VC dimension. The VC dimension q q(l ) of the functional space l is finite.

Complex Systems, 14 (2003) 63­90

86 A. A. Guergachi and G. G. Patry

C.3 I.i.d. condition. The training examples z1, z2, . . . , zN of the sequence N are independent and identically distributed (i.i.d.).
C .3 Weaker i.i.d. condition. The real-valued random variables

lh(z1) lh(z2)

lh (zN )

obtained by computing the values of lh at each one of the training examples zi of the sequence N, are i.i.d. for any h .

Corollary 2 (IPERM applicability and
probabilistic environment and,

VC (1)) Let associated

with

( it,

, a

learn, zint,gPmzt )acbheinae

( , ). Let N be a finite sequence of N training examples from the environment and  is a real number in the interval ]0, 1[.

If the conditions C.1, C.2, and C.3 are satisfied, then the IPERM is

1-applicable to ( , ) with the bound:

M  (42)

where the number  is:



q ln 4

2N q

1 N

ln

 4

and q is the VC dimension q(l ) of the space l .

Proof. In [6] Vapnik showed that, for any > 0, the following inequality holds true:

Pr sup 1[R(h), RemNp(h)] >
h

< 4 exp

q

ln

2N q

N

1

2
4M N (43)

when conditions C.1, C.2, and C.3 are satisfied [6], (see inequalities 5.24 and 5.12 at pages 197 and 192 of [6] respectively). Set the righthand side of the above inequality equal to . Then the expression of
is
M
and, therefore, from Vapnik's inequality, it follows that the inequality

sup 1[R(h), RemNp(h)] < M
h
holds true with probability of at least 1

.

Corollary
abilistic

3 (IPERM applicability and VC (2)) Let environment and, associated with it,

a

(, , learning

mzta, Pchzti)nbee

a

prob-

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

87

( , ). Let N be a finite sequence of N training examples from the environment and  is a real number in the interval ]0, 1[. If the conditions C .1, C.2, and C.3 are satisfied, then the IPERM is 2-applicable to ( , ) with the bound

(s)  

(44)

where

(s)

s1 s 2s

1 2

s

1
,

the number  is:



q ln 4

2N q

1 N

ln

 4

,

and q is the VC dimension q(l ) of the space l .

Proof. In [6] Vapnik showed that, for any > 0, the following inequality holds true:

Pr sup 2[R(h), RemNp(h)] > (s)  <
h

4 exp

q

ln

2N q

N

1

2
4N

(45)

when conditions C .1, C.2, and C.3 are satisfied [6], (see inequalities 5.43 and 5.12 at pages 210 and 192 of [6] respectively). Set the righthand side of the above inequality equal to . Then the expression of
is:

and, therefore, the inequality:

sup 2[R(h), RemNp(h)] < (s)  
h
holds true with probability of at least 1 .

Note that

is represented by the number M in Theorem 2 and

by the numbers s and  in Theorem 3.

Theorem 5 uses a weaker i.i.d. condition (C .3).

Theorem 5 (Using condition C .3) If the third condition C.3 in Theorems 2 and 3 is replaced by the condition C .3 and the two other conditions, C.1 and C.2 for Theorem 2 and C .1 and C.2 for Theorem 3, are kept unchanged, then the IPERM is still applicable to ( , ) with respect to the same deviation measures 1 and 2 and with the same bounds of equations (42) and (43), respectively.

Complex Systems, 14 (2003) 63­90

88 A. A. Guergachi and G. G. Patry

Proof. To prove equations (43) and (45), Vapnik [4, 5] made use of the weaker i.i.d. condition only. As a result, these inequalities remain true if condition C.3 is replaced by condition C .3. Consequently, the foregoing proofs of Theorems 2 and 3 are still valid with condition C .3.
Using Theorems 2, 3, and 5, it is now possible to develop uncertainty models for ( , ) with a guaranteed deviation that is readily computable.

Corollary
abilistic

4 (Uncertainty
environment

model and VC) Let and, associated with

( it, a

l,earni,nzgt ,mPzat c) hbienea

prob-

( , ). Let N be a finite sequence of N training examples from the environment and  is a real number in the interval ]0, 1[. Let hemNp be a decision rule at which the empirical risk RemNp(h) reaches its minimum.

If the conditions C.1, C.2, and C .3 are satisfied, then the inequality

[ (hemNp, g )]2

RemNp (hemNp )

M 2

1

1

4 RemNp (hemNp) M

(46)

holds true with probability of at least 1 . If the conditions C .1, C.2, and C .3 are satisfied, then the inequality

[ (hemNp, g )]2

RemNp (hemNp ) (1 (s)  )

(47)

holds true with probability of at least 1 .

(a) sup(a, 0) for any number a

(s)

s1 2

s s

1 2

s

1.

The number  is

.



q ln 4

2N q

1 N

ln

 4

.

q is the VC dimension q(l ) of the space l .

(48)

Proof. This theorem is a direct consequence of Theorems 5 and 3.

Theorem 4 establishes two uncertainty models, 1 and 2, for ( , ). The first one, 1, is based on the weak prior information
(1) and is defined by equation (46). The right-hand side of this

inequality represents the guaranteed deviation 1 between hemNp and

g , developed on the basis of

(1). Using this function 1, the

uncertainty model 1 can be rewritten as follows:

1 [ (hemNp, g )]2

1(N, , RemNp(hemNp),

(1), ) (49)

Complex Systems, 14 (2003) 63­90

Using Statistical Learning Theory

89

with

1(N, , RemNp(hemNp),

RemNp (hemNp )

M 2

1

(1), )

1

4 RemNp(hemNp) M

.

(50)

The second model,

2, is based on the weak prior information

(2) and is defined by equation (47). Denoting the right-hand side

of this inequality as 2 (guaranteed deviation developed on the basis of (2)), the uncertainty model 2 can be rewritten as

2 [ (hemNp, g )]2

2(N, , RemNp(hemNp),

(2), ) (51)

with

2(N, , RemNp(hemNp),

(2), )

RemNp (hemNp ) (1 (s)  )

.

(52)

10. Conclusions
A mathematical framework for modeling the uncertainty in complex engineering systems is developed. This framework uses the results of computational learning theory and is based on the premise that a system model is a learning machine. A definition of an uncertainty model is given and a principle called inductive principle of empirical risk minimization (IPERM) is introduced. The applicability of this principle is examined and the concept of "guaranteed deviation" defined. The system model complexity is measured using the Vapnik­Chervonenkis (VC) dimension. Based on this dimension, two different uncertainty models were developed.

Acknowledgments This work was financially supported by CIDA and NSERC.

References
[1] U. Jeppsson, Modelling Aspects of Wastewater Treatment Processes, Ph.D. Thesis, Lund Institute of Technology, Department of Industrial Electrical Engineering and Automation, Lund, Sweden, 1996.
[2] C. Zheng and G. Bennett, Applied Contaminant Transport Modeling-- Theory and Practice (Van Nostrand Reinhold, New York, 1995).
[3] L. Konikow and J. Bredehoeft, "Ground-water Models Cannot Be Validated," Advances in Water Resources, 19(2) (1992) 75­83.

Complex Systems, 14 (2003) 63­90

90 A. A. Guergachi and G. G. Patry
[4] N. Vapnik, Estimation of Dependencies Based on Empirical Data (Springer-Verlag, New York, 1982).
[5] N. Vapnik, The Nature of Statistical Learning Theory (Springer-Verlag, New York, 1995).
[6] N. Vapnik, Statistical Learning Theory (Wiley, New York, 1998). [7] N. Vapnik and A. Chervonenkis, "On the Uniform Convergence of Rel-
ative Frequencies of Events to their Probabilities," Soviet Mathematics Doklady, 9 (1968) 915­918. [8] N. Vapnik and A. Chervonenkis, "Necessary and Sufficient Conditions for the Uniform Convergence of the Means to their Expectations," Theory of Probability and Its Applications, 26 (1981) 532­553. [9] N. Vapnik and A. Chervonenkis, "The Necessary and Sufficient Conditions for Consistency of the Method of Empirical Risk Minimization," Pattern Recognition and Image Analysis, 1(3) (1991) 284­305. [10] A. A. Guergachi and G. G. Patry, "Using Statistical Learning Theory to Rationalize System Model Identification and Validation. Part II: Application to Biological Wastewater Treatment Systems," Complex Systems, to be submitted. [11] H. Stark and J. W. Woods, Probability, Random Processes, and Estimation Theory for Engineers (Prentice-Hall, New Jersey, 1994). [12] A. Guergachi, Uncertainty Management in the Activated Sludge Process-- Innovative Applications of Computational Learning Theory, Ph.D. Thesis, University of Ottawa, Ottawa, Canada, 2000. [13] H. White, Asymtotic Theory for Econometricians (Academic Press, Orlando, 1984). [14] R. Wenocur and R. Dudley, "Some Special Vapnik­Chervonenkis Classes," Discrete Mathematics, 33 (1981) 313­318. [15] M. Anthony and N. Biggs, Computational Learning Theory: An Introduction (Cambridge University Press, Cambridge, 1992).
Complex Systems, 14 (2003) 63­90

