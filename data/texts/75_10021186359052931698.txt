Sampling Techniques for the Nystro¨m Method

Sanjiv Kumar Google Research sanjivk@google.com

Mehryar Mohri Courant Institute and Google Research
mohri@cs.nyu.edu

Ameet Talwalkar Courant Institute, NYU
ameet@cs.nyu.edu

Abstract
The Nystro¨m method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the distribution according to which columns are sampled from the original matrix. In this work, we present an analysis of different sampling techniques for the Nystro¨m method. Our analysis includes both empirical and theoretical components. We first present novel experiments with several real world datasets, comparing the performance of the Nystro¨m method when used with uniform versus non-uniform sampling distributions. Our results suggest that uniform sampling without replacement, in addition to being more efficient both in time and space, produces more effective approximations. This motivates the theoretical part of our analysis which gives the first performance bounds for the Nystro¨m method precisely when used with uniform sampling without replacement.
1 Introduction
A common problem in many areas of large-scale machine learning involves deriving a useful and efficient approximation of a large matrix. This matrix may be a kernel matrix used with support vector machines (Boser et al., 1992; Cortes and Vapnik, 1995), kernel principal component analysis (Scho¨lkopf et al., 1998) or manifold learning (Platt, 2003; Talwalkar et al., 2008). Large matrices also naturally arise in other applications such as clustering. For these large-scale problems, the number of matrix entries
Appearing in Proceedings of the 12th International Confe-rence on Artificial Intelligence and Statistics (AISTATS) 2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR: W&CP 5. Copyright 2009 by the authors.

can be in the order of tens of thousands to millions, leading to difficulty in operating on, or even storing the matrix.
An attractive solution to this problem involves using the Nystro¨m method to generate a low-rank approximation of the original matrix from a subset of its columns (Williams and Seeger, 2000). A key aspect of the Nystro¨m method is the distribution according to which the columns are sampled. This method was first introduced to the machine learning community (Williams and Seeger, 2000) using uniform sampling without replacement, and this remains the sampling method most commonly used in practice (de Silva and Tenenbaum, 2002; Fowlkes et al., 2004; Platt, 2003; Talwalkar et al., 2008). More recently, the Nystro¨m method has been theoretically analyzed assuming a nonuniform sampling of the columns: Drineas and Mahoney (2005) provided bounds for the Nystro¨m approximation while sampling with replacement from a distribution with weights proportional to the diagonal elements of the input matrix.
This paper presents an analysis of different sampling techniques for the Nystro¨m method. Our analysis includes both empirical and theoretical components. We first present novel experiments with several real-world datasets, comparing the performance of the Nystro¨m method when used with uniform versus non-uniform sampling distributions. Although previous works have compared uniform and nonuniform distributions in a more restrictive setting (Drineas et al., 2001; Zhang et al., 2008), our results are the first to compare uniform sampling with the sampling technique for which the Nystro¨m method has theoretical guarantees. Our results suggest that uniform sampling, in addition to being more efficient both in time and space, produces more effective approximations. We further show the benefits of sampling without replacement. These empirical findings motivate the theoretical part of our analysis. We give the first performance bounds for the Nystro¨m method as it is used in practice, i.e., using uniform sampling without replacement.
The remainder of the paper is organized as follows. Section 2 introduces basic definitions and gives a brief presen-

304

Sampling Techniques for the Nystro¨m Method

tation of the Nystro¨m method. In Section 3, we provide an extensive empirical comparison of various sampling methods used with the Nystro¨m method. Section 4 presents our novel bound for the Nystro¨m method in the scenario of uniform sampling without replacement, and provides an analysis of the bound.

2 Preliminaries

Let G  Rn×n be a symmetric positive semidefinite

(SPSD) Gram (or kernel) matrix. For any such Gram ma-

trix, there exists an X  Rm×n such that G = XX. We define X(j), j = 1 . . . n, as the jth column vector of X and

X(i), i = 1 . . . m, as the ith row vector of X, and denote by · the l2 norm of a vector. Using singular value decomposition (SVD), the Gram matrix can be written as G =

U U , where U is orthogonal and  = diag(1, . . . , n)

is a real diagonal matrix with diagonal entries sorted in de-

creasing order. For r = rank(G), the pseudo-inverse of G

is defined as G+ =

r t=1

t-1U (t)U(t).

Further,

for

k



r,

Gk =

k t=1

tU (t)U(t)

is

the

`best'

rank-k

approximation

to G, or the rank-k matrix with minimal · F distance to

G, where · F denotes the Frobenius norm of a matrix.

The Nystro¨m method generates low-rank approximations

of G using a subset of the columns of the matrix (Williams
and Seeger, 2000). Suppose we randomly sample l  n columns of G uniformly without replacement.1 Let C be

the n × l matrix of these sampled columns, and W be the

l ×l matrix consisting of the intersection of these l columns with the corresponding l rows of G. Since G is SPSD, W

is also SPSD. Without loss of generality, we can rearrange

the columns and rows of G based on this sampling such

that:

G=

W G21 G21 G22

and C =

W G21

.

(1)

The Nystro¨m method uses W and C from (1) to construct a rank-k approximation G~k to G for k  l. When used with uniform sampling, the Nystro¨m approximation is:

G~k = CWk+C  G.

(2)

The Frobenius distance between G and G~k, G - G~k F , is

one standard measurement of the accuracy of the Nystro¨m method. The runtime of this algorithm is O(l3+nlk): O(l3)

for SVD on W and O(nlk) for multiplication with C.

3 Comparison of Sampling Methods
Since the Nystro¨m method operates on a subset of G, i.e., C, the selection of columns can significantly influence the
1Other sampling schemes are also possible as we discuss in Section 3. The formulation of the Nystro¨m method under these sampling schemes is identical to the one presented here, modulo an additional step to normalize the approximation by the probabilities of the selected columns (Drineas and Mahoney, 2005).

Name PIE-2.7K PIE-7K MNIST
ESS ABN

Type faces (profile) faces (front) digit images
proteins abalones

n 2731 7412 4000 4728 4177

d 2304 2304 784
16 8

Kernel linear linear linear RBF RBF

Table 1: Description of the datasets and kernels used in our experiments (Asuncion and Newman, 2007; Gustafson et al., 2006; LeCun and Cortes, 2009; Sim et al., 2002). `d' denotes the number of features in input space.

accuracy of approximation. Thus, in this section we discuss various sampling options used to select columns from G.
3.1 Description of Sampling Methods
The most basic sampling technique involves uniform sampling of the columns. Alternatively, the ith column can be sampled non-uniformly with weight proportional to either its corresponding diagonal element Gii (diagonal sampling) or the l2 norm of the column (column-norm sampling) (Drineas and Mahoney, 2005; Drineas et al., 2006b). There are additional computational costs associated with these non-uniform sampling methods: O(n) time and space requirements for diagonal sampling and O(n2) time and space for column-norm sampling. These non-uniform sampling techniques are often presented using sampling with replacement to simplify theoretical analysis. Column-norm sampling has been used to analyze a general SVD approximation algorithm. Further, diagonal sampling with replacement was used by Drineas and Mahoney (2005) to bound the reconstruction error of the Nystro¨m method,2 though the authors of that work suggest that column-norm sampling would be a better sampling assumption for the analysis of the Nystro¨m method.
Two other techniques have also been introduced for sampling-based techniques to generate low-rank approximations. The first method adaptively samples columns of G while the second performs k-means clustering as a preprocessing step to construct informative columns (Deshpande et al., 2006; Zhang et al., 2008). Although these methods show good empirical accuracy on small datasets, they are both computationally inefficient for large-scale problems. Adaptive sampling requires a full pass through G on each iteration, while k-means clustering quickly becomes intractable for moderately large n. For this reason, in this work we focus on fixed distributions ­ either uniform or non-uniform ­ over the set of columns.
In the remainder of this section we present novel exper-
2Although Drineas and Mahoney (2005) claimed to weight each column proportional to G2ii, they in fact use the diagonal sampling we present in this work, i.e., weights proportional to Gii (Drineas, 2008).

305

Kumar, Mohri and Talwalkar

Relative Accuracy

Uniform vs Non-Uni Sampling: PIE-7K
100
90
80
70
60 Uni+Rep Diag+Rep
50 Col-Norm+Rep
40 10 20 30 40 50
% of Columns Sampled (l / n )
(a)

l/n Dataset Uniform+Rep Diag+Rep Col-Norm+Rep

PIE-2.7K 38.8 (±1.5) 38.3 (±0.9) 37.0 (±0.9)

PIE-7K 55.8 (±1.1) 46.4 (±1.7) 54.2 (±0.9)

5% MNIST 47.4 (±0.8) 46.9 (±0.7) 45.6 (±1.0)

ESS 45.1 (±2.3)

-

41.0 (±2.2)

ABN 47.3 (±3.9)

-

44.2 (±1.2)

PIE-2.7K 72.3 (±0.9) 65.0 (±0.9) 63.4 (±1.4)

PIE-7K 83.5 (±1.1) 69.8 (±2.2) 79.9 (±1.6)

20% MNIST 80.8 (±0.5) 79.4 (±0.5) 78.1 (±0.5)

ESS 80.1 (±0.7)

-

75.5 (±1.1)

ABN 77.1 (±3.0)

-

66.3 (±4.0)

(b)

Figure 1: (a) Nystro¨m relative accuracy for various sampling techniques on PIE-7K. (b) Nystro¨m relative accuracy for various sampling methods for two values of l/n with k = 100. Values in parentheses show standard deviations for 10 different runs for a fixed l. `+Rep' denotes sampling with replacement. No error (`-') is reported for diagonal sampling with RBF kernels since diagonal sampling is equivalent to uniform sampling in this case.

imental results comparing the performance of these sampling methods on several data sets. Previous works have compared uniform and non-uniform in a more restrictive setting, using fewer types of kernels and focusing only on column-norm sampling (Drineas et al., 2001; Zhang et al., 2008). However in this work we provide the first comparison that includes diagonal sampling, the sampling technique for which the Nystro¨m method has theoretical guarantees.
3.2 Datasets
We used 5 datasets from a variety of applications, e.g., computer vision and biology, as described in Table 1. SPSD kernel matrices were generated by mean centering the datasets and applying either a linear kernel or RBF kernel. The diagonals (respectively column norms) of these kernel matrices were used to calculate diagonal (respectively column-norm) distributions. Note that the diagonal distribution equals the uniform distribution for RBF kernels since diagonal entries of RBF kernel matrices always equal one.

the spectral energy for each dataset. We first compared the effectiveness of the three sampling techniques using sampling with replacement. The results for PIE-7K are presented in Figure 1(a) and summarized for all datasets in Figure 1(b). The results across all datasets show that uniform sampling outperforms all other methods, while being much cheaper computationally and space-wise. Thus, while non-uniform sampling techniques might be effective in extreme cases where a few columns of G dominate in terms of · , this situation does not tend to arise with realworld data, where uniform sampling is most effective.

Next, we compared the performance of uniform sampling

with and without replacement. Figure 2(a) illustrates the

effect of replacement for the PIE-7K dataset for different

l n

ratios.

Similar results for the remaining datasets are

summarized in Figure 2(b). The results show that uni-

form sampling without replacement improves the accuracy

of the Nystro¨m method over sampling with replacement,

even when sampling less than 5% of the total columns.

4 Improved Nystro¨m Bound

3.3 Experiments

We used the datasets described in the previous section to
test the approximation accuracy for each sampling method. Low-rank approximations of G were generated using the Nystro¨m method along with these sampling methods, and accuracies were measured relative to the best rank-k approximation (Gk) as follows:

relative accuracy =

G - Gk G - G~k

F.
F

Note that relative accuracy is upper bounded by 1 and approaches 1 for good approximations. We fixed k = 100 for all experiments, a value that captures more than 90% of

The experimental results from Section 3 show that uniform sampling is the cheapest and most efficient sampling technique across several datasets. Further, it is the most commonly used method in practice. However, there does not currently exist a formal analysis of the accuracy of the Nystro¨m approximation when using uniform sampling without replacement. We next present a theoretical analysis of the Nystro¨m method using the more reasonable assumption of uniform sampling without replacement. We first introduce a general concentration bound for sampling without replacement (Section 4.1), and use it to derive a general bound on approximate matrix multiplication in the setting of sampling without replacement (Section 4.2). In Section 4.3, following Drineas and Mahoney (2005), we

306

Sampling Techniques for the Nystro¨m Method

Change in Rel Accuracy

Effect of Replacement: PIE-7K
4
3
2
1
0
-1
-2 10 20 30 40
% of Columns Sampled (l / n )
(a)

Dataset
PIE-2.7K PIE-7K MNIST
ESS ABN

5%
0.8 (±.6) 0.7 (±.3) 1.0 (±.5) 0.9 (±.9) 0.7 (±1.2)

10%
1.7 (±.3) 1.5 (±.3) 1.9 (±.6) 1.8 (±.9) 1.3 (±1.8)

15%
2.3 (±.9) 2.1 (±.6) 2.3 (±.4) 2.2 (±.6) 2.6 (±1.4)

30%
4.4 (±.4) 3.2 (±.3) 3.4 (±.4) 3.7 (±.7) 4.5 (±1.1)

(b)

Figure 2: Comparison of uniform sampling with and without replacement measured by the difference in relative accuracy. (a) Improvement in relative accuracy for PIE-7K when sampling without replacement. (b) Improvement in relative accuracy when sampling without replacement across all datasets for various l/n percentages.

show the connection between the Nystro¨m method and approximate matrix multiplication and present our main result: a general bound for the Nystro¨m method in the scenario of uniform sampling without replacement.

4.1 Concentration Bound for Sampling Without Replacement

We will be using the following concentration bound for sampling without replacement shown by Cortes et al. (2008) which holds for symmetric functions. A function  : X m  R defined over a set X is said to be symmetric if (x1, . . . , xm) = (x(1), . . . , x(m)) for any x1, . . . , xm  X and any permutation  of (1, . . . , m).
Theorem 1. Let m and u be positive integers, x1, . . . , xm a sequence of random variables sampled from an underlying set X of m + u elements without replacement, and let  : X m  R be a symmetric function such that for all i  [1, m] and for all x1, . . . , xm  X and x1 , . . . , xm  X ,
|(x1, . . . , xm) - (x1, . . . , xi-1, xi, xi+1, . . . , xm)|  ,

where  is a positive real number. Then   > 0,

Pr

 - E[]    2 exp

-22 (m, u)2

,

(3)

where

(m, u)

=

mu m+u-1/2

·

1-1/(2

1 max{m,u})

.

4.2 Concentration Bound for Matrix Multiplication
To derive a bound for the Nystro¨m method using uniform sampling without replacement, we first present a generalization of a bound on approximate matrix multiplication given by Drineas et al. (2006a) to the more complex setting of uniform sampling without replacement. This generalization is not trivial since previous inequalities hinge upon a key i.i.d. assumption which clearly does not hold when sampling without replacement.

Theorem 2. Suppose A  Rm×n, B  Rn×p, 1  l  n. Choose a set (S) of size l uniformly at random without replacement from {1 . . . n}, and let C (R) equal the columns of A (rows of B) corresponding to indices in S scaled by
n/l. Then CR is an approximation to AB, i.e.,

AB =

n
A(t)B(t) 

l

C (t) R(t)

=

n l

A(t)B(t) = CR,

t=1 t=1

tS

and,

E AB - CR F 

nn l

A(t) 2 B(t) 2.

t=1

(4)

Further, let   (0, 1), t = argmaxt A(t) B(t) , and

=

log(2/)(l,n-l) l

,

with

(l,

n

-

l)

defined

in

Theorem

1. Then, with probability at least 1 - ,

AB - CR F 

nn l t=1

A(t)

2

B(t)

2+

 2

n

A(t )

l

B(t) .

(5)

We note that for even moderately sized l and n, (l, n - l)  l(1 - l/n) and thus   log(2/)(1 - l/n).
Corollary 1. If A = B and t = argmaxt A(t) , then

E AA - CC F 

nn l t=1

A(t)

4.

(6)

Further, let   (0, 1) and  = with probability at least 1 - ,

log(2/)(l,n-l) l

.

Then,

AA - CC F 

nn l
t=1

A(t)

4 + n l

A(t )

2.

(7)

307

Kumar, Mohri and Talwalkar

In this special case, we use the tighter Lipschitz condition

defined in (26). Further, since

n t=1

A(t)

4n

A(t )

4

we can simplify Corollary 1 as follows:

Corollary 2. If A = B then

E

AA - CC F

 n A(t) 2. l

(8)

Further, let   (0, 1), t = argmaxt A(t) , and  =

log(2/)(l,n-l) l

.

Then,

with

probability

at

least

1

-

,

AA - CC

F



(1 + ) n l

A(t )

2.

(9)

The proof of this theorem and its corollaries involves bounding an expectation, determining a Lipschitz condition and using the concentration bound of Theorem 1. These three steps are presented in detail below.

Bound on Expectation

To obtain a bound for E AB - CR F , we first calculate expressions for the mean and variance of the (i, j)th

component of CR, i.e., (CR)ij. For any set S of distinct elements in {1 . . . n}, |S| = l, we define (S) as the prob-

ability that a randomly chosen subset of l elements equals

S.

There are a total of

n l

distinct sets and in the uni-

form case, (S)

=

1/

n l

.

Furthermore, each element in

{1 . . . n} appears in l/n of these distinct sets. Thus, the

following equalities hold:

(nl ) E[(CR)ij] = (Sk) ·
k=1

n l AitBtj
tSk

=

l

n
n l

n

l

n l

t=1 n

AitBtj

= (AB)ij.

(10)
(11) (12)

Further, we have

E[(CR)ij ]2 = (AB)i2j =

n
AitBtj
t=1

2

(13)

and

(nl ) E[(CR)2ij] = (Sk) ·
k=1

n l

AitBtj

2

tSk

=

n2 l2

(nl )  (Sk )

k=1

AitBtj
tSk

2
.

(14) (15)

Since all sets (Sk) have equal probability and each el-

ement

appears

in

l n

of

these

sets,

when

we

expand

tSk AitBtj 2 we find that the coefficient for each

(AitBtj)2 term is

l n

.

Further, to find the coefficients for

the cross terms, we calculate the probability that two dis-

tinct elements appear in the same set. If we fix elements t

and t with t = t and define set Sk such that t  Sk, then

Pr[t



Sk ]

=

l-1 n-1

.

Thus,

E[(CR)i2j ]

=

n l

n
(AitBtj )2+

t=1

(16)

l-1 n n l n-1

n
AitBtj Ait Btj

t=1 t=t

=

n l

n
(AitBtj )2+
t=1

(17)

l-1 n l n-1

n
(AB)i2j - (AitBtj )2
t=1



n l

n
(AitBtj )2
t=1

+

l

- l

1 (AB)2ij ,

(18)

 where the inequality follows since x 1  n x for x  Rn. We can now bound the variance as:

Var[(CR)ij] = E[(CR)i2j] - E[(CR)ij]2



n l

n
(AitBtj )2
t=1

-

1 l

(AB

)i2j

.

(19) (20)

Now, we can bound the expectation as:

mp

E

AB - CR

2 F

=

E[(AB - CR)i2j]

i=1 j=1

mp

= Var[(CR)ij]

i=1 j=1



n l

n
(
t=1

i

A2it)(
j

Bt2j

)

-

1 l

AB

2 F



n l

n t=1

A(t)

2

B(t)

2.

 By the concavity of · and Jensen's inequality, E AB -

CR F 

E

AB - CR

2 F

. Thus,

E AB - CR F 

nn l

A(t) 2 B(t) 2.

t=1

(21)

Lipschitz Bound
Consider the function  defined by (S) = AB -CR F , where S is the set of l indices chosen uniformly at random without replacement from {1 . . . n} to construct C and R. If we create a new set S of indices by exchanging i  S for some i / S, then we can construct the corresponding

308

Sampling Techniques for the Nystro¨m Method

C and R from this new set of indices. We are interested in finding a  such that

|(S) - (S)|  .

(22)

Using the triangle inequality, we see that

AB - CR F - AB - CR F  CR - CR F .

We next observe that the difference between CR and CR depends only on indices i and i,3 and thus

  CR - CR F

=

n l

A(i)B(i) - A(i)B(i)

F



n l

A(i)

B(i) + A(i)

B(i )



2n l

A(t )

B(t) ,

(23) (24)

where we use the triangle inequality and the identity A(i)B(i) F = A(i) B(i) to obtain (24).

Further, if A = B, we can obtain a tighter bound. If a = A(i) and a = A(i), we have:

aa - aa F = Tr (aa - aa)(aa - aaT )

= a 4 + a 4 - 2(aa)2
 a 4 + a 4.
Combining (23) with (25) we get: 
  2n A(t) 2. l

(25) (26)

Concentration Bound

Using the bound on the expectation and the Lipschitz bound just shown, by Theorem 1, for any  > 0 and  > 0, the following inequality holds:

Pr AB - CR F 

nn l

A(t) 2 B(t) 2 + 

t=1

 2 · exp

-22 (l, n - l)2

.

(27)

Setting  to match the right-hand side and choosing  =



log(2/)(l,n-l) 2

yields

the

statement

of

Theorem

2.

4.3 Bound for Nystro¨m Method

We now present a bound on the accuracy of the Nystro¨m method when columns are chosen uniformly at random without replacement.4
3A similar argument is made in Drineas et al. (2006a) using the assumption of sampling independently and with replacement.
4Bounds for the l2 norm can obtained using similar techniques. They are omitted due to space constraints.

Theorem 3. Let G  Rn×n be an SPSD matrix. Assume

that l columns of G are sampled uniformly at random with-

out replacement, let G~k be the rank-k Nystro¨m approxima-

tion to G as described in approximation to G. For

(2), >

and 0, if

llet G6k4kb/et4he,

best then

rank-k

E ^ G - G~k F ~  G - Gk F +

"



,,n l

X

v «u n

#

1 2

Gii utn X Gi2i ,

iD(l)

i=1

where iD(l) Gii is the sum of the largest l diagonal

entries of G. Further, if  =

log(2/)(l,n-l) l

,

with

(l, n - l) defined in Theorem 1 and if l  64k/4 then

with probability at least 1 - ,

G - G~k F  G - Gk F +

"  ,,n
l

X

v «,,u n

# «

1 2

Gii tun X G2ii +  max `nGii´ .

iD(l)

i=1

Recall that for even moderately sized l and n, (l, n - l)  l(1 - l/n) and thus   log(2/)(1 - l/n). To prove this theorem, we use Corollary 1 (see proof for further details). If we instead use Corollary 2, we obtain the following weaker, yet more intuitive bound.5
Corollary 3. Let G  Rn×n be an SPSD matrix. Assume that l columns of G are sampled uniformly at random without replacement, let G~k be the rank-k Nystro¨m approximation to G as described in (2), and let Gk be the best rank-k approximation to G. For  > 0, if l  64k/4, then
E G - G~k F  G - Gk F +  · max nGii . (28)

Further, if  =

log(2/)(l,n-l) l

,

with

(l,

n-l)

defined

in

Theorem 1 and if l  64k(1 + )2/4 then with probability

at least 1 - ,

G - G~k F  G - Gk F +  · max nGii (29)

Proof. The theorem and its corollary follow from applying

Lemma 2 to Lemma 1 and using Jensen's inequality. Note

that when using these lemmas to prove Theorem 3, we use

the fact that if G = XX then

iD(l) Gii =

X (1:l)

2 F

,

where X(1:l) are the largest l columns of X with respect

to · . We next state and prove these lemmas.

Lemma 1. Let G  Rn×n be an SPSD matrix and define X  Rm×n such that G = XX. Further, let S, |S| =
l be any set of indices chosen without replacement from {1 . . . n}. Let G~k be the rank-k Nystro¨m approximation

5Corollary 3 can also be derived from Theorem 3 by not-

ing

that

P
iD(l)

Gii



l max `Gii´

and

Pn
i=1

Gi2i



n max `G2ii´.

309

Kumar, Mohri and Talwalkar

of G constructed from the columns of G corresponding to indices in S. Define CX  Rm×l as the columns in X
corresponding to the indices in S scaled by n/l. Then

G - G~k

2 F

G  4k

- Gk

2 F

+

XXXX



-

CX

CX

CX

CX

F.

Proof. The proof of this lemma in Drineas and Mahoney (2005) does not require any assumption on the distribution from which the columns are sampled and thus holds in the case of uniform sampling without replacement. Indeed, the proof relies on the ability to decompose G = XX. To make this presentation self-contained, we next review the main steps of the proof of this lemma.
Let X = U V  and CX = U^ ^ V^  denote the the singular value decompositions of X and CX . Further, let U^k denote the top k left singular vectors of CX and define E = XXXX - CX CXCX CX F . Then the following inequalities hold:

G - G~k

2 F

=

=



 =

XX - XU^kU^kX

2 F

XX

2 F

-2

X X  U^k

F 2

+

U^k X X  U^k

2 F

XX

2 F

-

k
X

t4

(CX

)

+

 3k

E

F

t=1

XX

2 F

-

k
X

t2

(X



X

)

+

 4k

E

F

G - Gk

2 F

t=1 +4 k

E

F.

Refer to Drineas and Mahoney (2005) for further details.

Lemma 2. Suppose X  Rm×n, 1  l  n and construct CX from X as described in Theorem 2. Let E = XXXX - CX CXCX CX and define X(1:l)  Rm×l as the largest l columns of X with respect to · . Then,

E

EF



2n l

X (1:l)

2 F

·

nn l

X(t) 4.

t=1

(30)

Using the triangle inequality we have:

E F

X

2 F

+

CX

2 F

· XX - CX CX F



2n l

X (1:l)

2 F

·

XX - CX CX

F.

The lemma now follows by applying Corollary 1.

4.4 Analysis of Bound

In the previous section we presented a new bound for the Nystro¨m method, assuming columns are sampled uniformly without replacement. We now compare this bound with one presented in Drineas and Mahoney (2005), in which columns are sampled non-uniformly with replacement using a diagonal distribution. We compare the relative tightness of the bounds assuming that the diagonal entries of G are uniformly distributed, in which case Theorem 3 reduces to Corollary 3. This is the case for any normalized kernel matrix (K) constructed from an initial kernel matrix (K) as follows:

K(x, y) =

K(x, y)

.

K(x, x)K(y, y)

(32)

The diagonals of kernel matrices are also identical in the case of the RBF kernels, which Williams and Seeger (2000) suggests are particularly amenable to the Nystro¨m method since their eigenvalues decay rapidly. When the diagonals are equal, the form of the bound in Drineas and Mahoney (2005) is identical to that of Corollary 3, and hence we can compare the bounds by measuring the value of the minimal allowable  as a function of the fraction of columns used for approximation, i.e., the l/n ratio. Both bounds are tightest when the inequalities involving l, e.g., l  64k(1 + )2/4 for Corollary 3, are set to equalities, so we use these equalities to solve for the minimal allowable epsilon. In our analysis, we fix the confidence parameter  = 0.1 and set k = .01 × n. The plots displayed in Figure 3 clearly show that the bound from Theorem 3 is tighter than that of Drineas and Mahoney (2005).

Further, let   (0, 1) and  = with probability at least 1 - ,

log(2/)(l,n-l) l

.

Then

5 Conclusion

The Nystro¨m method is used in a variety of large-scale

v

E

F



2n l

X (1:l)

2 F

·

u u

n

n
X

t

X (t)

4+ n

l t=1

l

! X (t) 2 .

learning applications, in particular in dimensionality reduction and image segmentation. This method is commonly used with uniform sampling without replacement, though

(31) non-uniform distributions have been used to theoretically

Proof. We first expand E as follows:

analyze the Nystro¨m method. In this work, we gave a series of clear empirical results sup-

E = XXXX - XXCX CX + XXCX CX- CX CXCX CX
= XX(XX - CX CX) + (XX - CX CX)CX CX.

porting the use of uniform over non-uniform sampling, as uniform sampling tends to be superior in both speed and accuracy in several data sets. We then bridged the gap between theory and practical use of the Nystro¨m method

310

Sampling Techniques for the Nystro¨m Method

Min Epsilon

Comparison between Bounds
4.5 New Bound
Existing Bound 4

3.5

3

2.5

2

1.5 10 20 30 40
% of Columns Sampled (l/n)

50

Figure 3: Comparison of the bound given by Drineas and Mahoney (2005) and our bound based on sampling without replacement.

by providing performance bounds for the Nystro¨m method when used with uniform sampling without replacement. Our analysis gives the first theoretical justification for the use of uniform sampling without replacement in this context. Our experiments and comparisons further demonstrate that the qualitative behavior of our bound matches empirical observations. Our bounds and theoretical analysis are also of independent interest for the analysis of other approximations in this setting.
Acknowledgments
This work was supported in part by the New York State Office of Science Technology and Academic Research (NYSTAR) and by a Google Research Grant.
References
A. Asuncion and D.J. Newman. UC Irvine machine learning repository, http://www.ics.uci.edu/mlearn/MLRepository.html, 2007.
Bernhard E. Boser, Isabelle Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classifiers. In COLT, 1992.
Corinna Cortes and Vladimir N. Vapnik. Support-Vector Networks. Machine Learning, 20(3):273­297, 1995.
Corinna Cortes, Mehryar Mohri, Dmitry Pechyony, and Ashish Rastogi. Stability of transductive regression algorithms. In ICML, 2008.
Vin de Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. In NIPS, 2002.
Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projective clus-

tering via volume sampling. In Symposium on Discrete Algorithms, 2006.
Petros Drineas and Michael W. Mahoney. On the Nystro¨m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning. Journal of Machine Learning Research, 6:2153­2175, 2005.
Petros Drineas, Eleni Drinea, and Patrick S. Huggins. An experimental evaluation of a Monte-Carlo algorithm for SVD. In Panhellenic Conference on Informatics, 2001.
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo Algorithms for Matrices I: Approximating Matrix Multiplication. SIAM J. Comput., 36(1):132­157, 2006.
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. SIAM J. Comput., 36(1), 2006.
Petros Drineas. Personal communication, 2008.
Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping using the Nystro¨m method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2), 2004.
A. Gustafson, E. Snitkin, S. Parker, C. DeLisi, and S. Kasif. Towards the identification of essential genes using targeted genome sequencing and comparative analysis. BMC:Genomics, 7:265, 2006.
Yann LeCun and Corinna Cortes. The MNIST database of handwritten digits, http://yann.lecun.com/exdb/mnist/, 2009.
John C. Platt. Fast embedding of sparse similarity graphs. In NIPS, 2003.
Bernhard Scho¨lkopf, Alexander Smola, and Klaus-Robert Mu¨ller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299­1319, 1998.
Terence Sim, Simon Baker, and Maan Bsat. The CMU pose, illumination, and expression database. In Conference on Automatic Face and Gesture Recognition, 2002.
Ameet Talwalkar, Sanjiv Kumar, and Henry Rowley. Large-scale manifold learning. In CVPR, 2008.
Christopher K. I. Williams and Matthias Seeger. Using the Nystro¨m method to speed up kernel machines. In NIPS, 2000.
Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved Nystro¨m low-rank approximation and error analysis. In ICML, 2008.

311

