On the Dynamics of Boosting

Cynthia Rudin

Ingrid Daubechies

Robert E. Schapire

Princeton University

Princeton University

Progr. Appl. & Comp. Math.

Department of Computer Science

Fine Hall

35 Olden St.

Washington Road

Princeton, NJ 08544

Princeton, NJ 08544-1000

schapire@cs.princeton.edu

{crudin,ingrid}@math.princeton.edu

Abstract
In order to understand AdaBoost's dynamics, especially its ability to maximize margins, we derive an associated simplified nonlinear iterated map and analyze its behavior in low-dimensional cases. We find stable cycles for these cases, which can explicitly be used to solve for AdaBoost's output. By considering AdaBoost as a dynamical system, we are able to prove Ra¨tsch and Warmuth's conjecture that AdaBoost may fail to converge to a maximal-margin combined classifier when given a `nonoptimal' weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv.
1 Introduction
AdaBoost is an algorithm for constructing a "strong" classifier using only a training set and a "weak" learning algorithm. A "weak" classifier produced by the weak learning algorithm has a probability of misclassification that is slightly below 50%. A "strong" classifier has a much smaller probability of error on test data. Hence, AdaBoost "boosts" the weak learning algorithm to achieve a stronger classifier. AdaBoost was the first practical boosting algorithm, and due to its success, a number of similar boosting algorithms have since been introduced (see [1] for an introduction). AdaBoost maintains a distribution (set of weights) over the training examples, and requests a weak classifier from the weak learning algorithm at each iteration. Training examples that were misclassified by the weak classifier at the current iteration then receive higher weights at the following iteration. The end result is a final combined classifier, given by a thresholded linear combination of the weak classifiers.
Often, AdaBoost does not empirically seem to suffer badly from overfitting, even after a large number of iterations. This lack of overfitting has been attributed to AdaBoost's
This research was partially supported by NSF Grants IIS-0325500, CCR-0325463, ANI0085984 and AFOSR Grant F49620-01-1-0099.

ability to generate a large margin, leading to a better guarantee on the generalization per-

formance. When it is possible to achieve a positive margin, AdaBoost has been shown to

approximately maximize the margin [2]. In particular, it is known that AdaBoost achieves a

margin

of

at

least

1 2

,

where



is

the

largest

margin

that

can

possibly

be

attained

by

a

com-

bined classifier (other bounds appear in [3]). Many of the subsequent boosting algorithms

that have emerged (such as AdaBoost [4], and arc-gv [5]) have the same main outline as

AdaBoost but attempt more explicitly to maximize the margin at the expense of lowering

the convergence rate; the trick seems to be to design an update for the combined classifier

that maximizes the margin, has a fast rate of convergence, and is robust.

For all the extensive theoretical and empirical study of AdaBoost, it is still unknown if
AdaBoost achieves a maximal margin solution, and thus the best upper bound on the probability of error (for margin-based bounds). While the limiting dynamics of the linearly inseparable case (i.e.,  = 0) are fully understood [6], other basic questions about the dynamics of AdaBoost in the more common case  > 0 are unknown. For instance, we do not know, in the limit of a large number of rounds, if AdaBoost eventually cycles among
the base classifiers, or if its behavior is more chaotic.

In this paper, we study the dynamics of AdaBoost. First we simplify the algorithm to reveal a nonlinear iterated map for AdaBoost's weight vector. This iterated map gives a direct relation between the weights at time t and the weights at time t + 1, including renormalization, thus providing a much more concise mapping than the original algorithm. We then provide a specific set of examples in which trajectories of this iterated map converge to a limit cycle, allowing us to calculate AdaBoost's output vector directly.

There are two interesting cases governing the dynamics: the case where the optimal weak classifiers are chosen at each iteration (the `optimal' case), and the case where permissible non-optimal weak classifiers may be chosen (the `non-optimal' case). In the optimal case, the weak learning algorithm is required to choose a weak classifier which has the largest edge at every iteration. In the non-optimal case, the weak learning algorithm may choose any weak classifier as long as its edge exceeds , the maximum margin achievable by a combined classifier. This is a natural notion of non-optimality for boosting; thus it provides a natural sense in which to measure robustness.

Based on large scale experiments and a gap in theoretical bounds, Ra¨tsch and Warmuth [3] conjectured that AdaBoost does not necessarily converge to a maximum margin classifier in the non-optimal case, i.e., that AdaBoost is not robust in this sense. In practice, the weak classifiers are generated by CART or another heuristic weak learning algorithm, implying that the choice need not always be optimal. In Section 3, we show this conjecture to be true using a low-dimensional example. Thus, our low-dimensional study provides insight into AdaBoost's large scale dynamical behavior.

AdaBoost, as shown by Breiman [5] and others, is actually a coordinate descent algorithm on a particular exponential loss function. However, minimizing this function in other ways does not necessarily achieve large margins; the process of coordinate descent must be somehow responsible. In Section 4, we introduce a differentiable function that can be maximized to achieve maximal margins; performing coordinate ascent on this function yields a new boosting algorithm that directly maximizes margins. This new algorithm and AdaBoost use the same formula to choose a direction of ascent/descent at each iteration; thus AdaBoost chooses the optimal direction for this new setting. We approximate the update rule for coordinate ascent on this function and derive an algorithm with updates that are slightly more aggressive than those of arc-gv.

We proceed as follows: in Section 2 we introduce some notation and state the AdaBoost algorithm. Then we decouple the dynamics for AdaBoost in the binary case to reveal a nonlinear iterated map. In Section 3, we analyze these dynamics for a simple case: the case where each hypothesis has one misclassified point. In a 3 × 3 example, we find 2 stable

cycles. We use these cycles to show that AdaBoost produces a maximal margin solution in the optimal case; this result generalizes to m × m. Then, we produce the example promised above to show that AdaBoost does not necessarily converge to a maximal margin solution in the non-optimal case. In Section 4 we introduce a differentiable function that can be used to maximize the margin via coordinate ascent, and then approximate the coordinate ascent update step to derive a new algorithm.

2 Simplified Dynamics of AdaBoost

The training set consists of {(xi, yi)}i=1..m, where each example (xi, yi)  X × {-1, 1}. Denote by dt  Rm the distribution (weights) over the training examples at iteration t, expressed as a column vector. (Denote dtT as its transpose.) Denote by n the total number

of classifiers that can be produced by the weak learning algorithm. Since our classifiers are

binary, n is finite (at most 2m), but may be very large. The weak classifiers are denoted

h1, ..., hn, with hj : X  {1, -1}; we assume that for every hj on this list, -hj also

appears. We construct a matrix M so that Mij = yihj(xi), i.e., Mij = +1 if training

example i is classified correctly by hypothesis hj, and -1 otherwise. The (unnormalized)

coefficient of classifier hj for the combined hypothesis is fAda(x)

final =

combined

n j=1

(j

/

hypothesis  1)hj(x)

is denoted where 

j
1

, so =

that the

n j=1

j

final . (In

this paper, either hj or -hj remains unused.) The simplex of n-dimensional vectors with

positive entries that sum to 1 will be denoted n. The margin of training example i is

defined by yifAda(xi), or equivalently (M)i/  1, and the edge of hypothesis j with respect to the training data (weighted by d) is (dT M)j, or 1 - 2×(probability of error of

hj on the training set weighted by d). Our goal is to find a normalized vector ~  n that maximizes mini(M~)i. We call this minimum margin over training examples the margin

of classifier . Here is the AdaBoost algorithm and our reduction to an iterated map.

AdaBoost (`optimal' case):

1. Input: Matrix M, Number of iterations tmax

2. Initialize: 1,j = 0 for j = 1, ..., n

3. Loop for t = 1, ..., tmax

(a) dt,i = e(-Mt)i /

m i=1

e(-Mt )i

for

i

=

1, ..., m

(b) jt = argmaxj(dTt M)j

(c) rt = (dTt M)jt

(d)

t

=

1 2

ln

1+rt 1-rt

(e) t+1 = t + tejt , where ejt is 1 in position jt and 0 elsewhere.

4. Output: combined,j = tmax+1,j / tmax+1 1
Thus at each iteration, the distribution dt is computed (Step 3a), classifier jt with maximum edge is selected (Step 3b), and the weight of that classifier is updated (Step 3c, 3d, 3e).

(Note that wlog one can omit from M all the unused columns.)

AdaBoost can be reduced to the following iterated map for the dt's. This map gives a
direct relationship between dt and dt+1, taking the normalization of Step 3a into account automatically. Initialize d1,i = 1/m for i = 1, ..., m as in the first iteration of AdaBoost.

Reduced Iterated Map:

1. jt = argmaxj(dTt M)j

2. rt = (dtT M)jt

3.

dt+1,i =

dt,i 1+Mijt rt

for i = 1, ..., m

To derive this map, consider the iteration defined by AdaBoost and reduce as follows.

dt+1,i = e-(Mijt t) =
dt+1,i =

e e-(Mt)i -(Mijt t)

m i=1

e-(Mt )i

e-(Mijt t)

where

t

=

1 2

ln

1 - rt 1 + rt

1 2

Mijt

=

1 - Mijt rt 1 + Mijt rt

1 2
, thus

m i=1

dt,i

dt,i
1-Mijt rt 1+Mijt rt

1 2

1+Mijt rt 1-Mijt rt

1.
2

1 + rt 1 - rt

, so

Define d+ =

{i:Mijt =1} dt,i

and d-

=

1 - d+.

Thus,

d+

=

1+rt 2

and d-

=

1-rt 2

.

For

each i such that Mijt = 1, we find:

dt+1,i

=

d+

dt,i + d-

1+rt 1-rt

=

dt,i 1 + rt

Likewise, complete.

for To

each i such check that

that
m i=1

Mijt dt+1,i

= =

-1, we find dt+1,i =

1, we see

m i=1

dt+1,i

.dt,i
1-rt

Our reduction

=

1 1+rt

d+

+

1 1-rt

d-

is =

d+ 2d+

+

d- 2d-

= 1.

3 The Dynamics of Low-Dimensional AdaBoost

First we will introduce a simple 3×3 input matrix and analyze the convergence of AdaBoost in the optimal case. Then we will consider a larger matrix and show that AdaBoost fails to converge to a maximum margin solution in the non-optimal case.

-1 1

1

Consider the following input matrix M = 1 -1 1 corresponding to the case of

1 1 -1

three training examples, where each weak classifier misclassifies one example. (We could

add additional hypotheses to M, but these would never be chosen by AdaBoost.) The max-

imum value of the margin for M is 1/3. How will AdaBoost achieve this result? We are in

the optimal case, where jt = argmaxj(dtT M)j. Consider the dynamical system on the sim-

plex

3 i=1

dt,i

=

1,

dt,i

>

0

i

defined

by

our

reduced

map

above.

In

the

triangular

region

with vertices (0, 0, 1), (1/3, 1/3, 1/3), (0, 1, 0), jt will be 1. Similarly, we have regions for

jt = 2 and jt = 3 (see Figure 1(a)). Since dt+1 will always satisfy (dTt+1M )jt = 0, the

dynamics

are

restricted

to

the

edges

of

a

triangle

with

vertices

(0,

1 2

,

1 2

),

(

1 2

,

0,

1 2

),

(

1 2

,

1 2

,

0)

after the first iteration (see Figure 1(b)).

11

second component of d_t second component of d_t

(dTM)2 =0

jt=1
1/3

jt =3

1/2 (dTM)1 =0

jt =2
1/3
first component of d_t

1

(dTM)3 =0
1/2
first component of d_t

1

Figure 1: (a-Left) Regions of dt-space where classifiers jt = 1, 2, 3 will respectively be selected. (b-Right) All weight vectors d2, ..., dtmax are restricted to lie on the edges of the inner triangle.

(0,.5,.5)

(0,.5,.5)

position along triangle

position along triangle

(.5,.0,.5)

(.5,0,.5)

(.5,.5,0)

(.5,.5,0)

(0,.5,.5) (0,.5,.5)
0.6

(.5,.5,0)

(.5,.0,.5)

position along triangle

(0,.5,.5)

(0,.5,.5) (0,.5,.5)

0.5

(.5,.5,0)

(.5,.0,.5)

position along triangle

(0,.5,.5)

second component of weight vector

second component of weight vector

0.105.15

first component of weight vector

0.55 00

first component of weight vector

0.5

Figure 2: (a-Upper Left) The iterated map on the unfolded triangle. Both axes give coordinates on the edges of the inner triangle in Figure 1(b). The plot shows where dt+1 will be, given dt. (b-Upper Right) The map from (a) iterated twice, showing where dt+3 will be, given dt. There are 6 stable fixed points, 3 for each cycle. (c-Lower Left) 50 timesteps of AdaBoost showing convergence of
dt's to a cycle. Small rings indicate earlier timesteps of AdaBoost, while larger rings indicate later timesteps. There are many concentric rings at positions d(c1y)c, dc(2y)c, and dc(3y)c. (d-Lower Right) 500 timesteps of AdaBoost on a random 11x21 matrix. The axes are dt,1 vs dt,2.

On this reduced 1-dimensional phase space, the iterated map has no stable fixed points or

orbits d(c1y)cT

of =

length 2.However,

(

3- 4

5,

5-1 4

,

1 2

),

consider dc(2y)cT =

t(h12e,f3o-ll4ow5 ,ing54-pe1r)i,oddic(c3y)ocTrb=it

oflength 3:

(

5-1 4

,

1 2

,

 3- 5
4

).

This

is clearly a (d(c1y)cT M)1

=cyc(le,5s-inc1e)/s2ta. rtNinogwf, rcoommpdu(c1tyi)cn,gAdd(ca1yB)c,oi /o(s1t

will choose jt + Mi,1r1) for

= 1. each i

Then yields

r1 = d(c2y)c.

In this in fact


way, AdaBoost another 3-cycle,


wdic(l1yl)cTcy=cle(b3e-t4w5e,en12

,hy5p4-o1th)e, sdesc(2yj)cT

= =

1, (

2,

1 2

,

3, 1, 2, 3, etc.

5-1 4

,

3- 4

5 ),

There d(c3y)cT

is =

(

5-1 4

,

3- 4

5,

1 2

).

To find these cycles, we hypothesized only that a cycle of length 3 exists,

visiting each hypothesis in turn, and used the reduced equations from Section 2 to solve for

the cycle coordinates.

We give the following outline of the proof for global stability: This map is a contraction, so any small perturbation from the cycle will diminish, yielding local stability of the cycles. One only needs to consider the one-dimensional map defined on the unfolded triangle, since within one iteration every trajectory lands on the triangle. This map and its iterates are piecewise continuous and monotonic in each piece, so one can find exactly where each interval will be mapped (see Figure 2(a)). Consider the second iteration of this map (Figure 2(b)). One can break the unfolded triangle into intervals and find the region of attraction of each fixed cycle; in fact the whole triangle is the union of both regions of attraction. The convergence to one of these two 3cycles is very fast; Figure 2(b) shows that the absolute slope of the second iterated map at the fixed points is much less than 1. The combined classifier AdaBoost will output is: combined = ((dc(1y)cT M)1, (dc(2y)cT M)2, (dc(3y)cT M)3)/normaliz. = (1/3, 1/3/1/3), and since mini(Mcombined)i = 1/3 AdaBoost produces a maximal margin solution.

This 3 × 3 case can be generalized to m classifiers, each having one misclassified training example; in this case there will be periodic cycles of length m, and the contraction will also persist (the cycles will be stable). We note that for every low-dimensional case we
tried, periodic cycles of larger lengths seem to exist (such as in Figure 2(d)), but that the
contraction at each iteration does not, so it is harder to show stability.

Now, we give an example to show that non-optimal AdaBoost does not necessarily con-

verge to a maximal margin solution. Consider the following input matrix (again, omitting

-1 1 1 1 -1

unused columns): M =

1 1

-1 1 1 -1

1 1

-1 1

. For this matrix, the maximal mar-

1 1 1 -1 1

gin  is 1/2. In the optimal case, AdaBoost will produce this value by cycling among the

first four columns of M. Recall that in the non-optimal case jt  {j : (dTt M)j  }.

Consider the following initial condition for the dynamics:

d1T

=

(

3- 8

5,

3- 8

5,

1 2

,

5-1 4

).

Since (d1T M)5 > , we are justified in choosingj1 = 5, although here it is not the optimal

choice.

Another iteration yields dT2

=

(

1 4

,

1 4

,

5-1 4

,

3- 4

5 ),

satisfying

(dT1 M)4

>



for

which we choose j2 = 4. At the third iteration, we choose j3 = 3, and at the fourth iteration

we find d4 = d1. This cycle is the same cycle as in our previous example (although there

is one extra dimension). There is actually a whole manifold of 3-cycles in this non-optimal

case,

since d~1T

:=

(

,

 3- 5
4

-



,

1 2

,

5-1 4

)

lies

on

a

cycle

for

any

,0





3- 4

5.

In any

case, the value of the margin produced by this cycle is 1/3, not 1/2.

We have thus established that AdaBoost is not robust in the sense we described; if the
weak learner is not required to choose the optimal hypothesis at each iteration, but is only required to choose a sufficiently good weak classifier jt  {j : (dtT M)j  }, then a maximum margin solution will not necessarily be attained. In practice, it may be possible
for AdaBoost to converge to a maximum margin solution when hypotheses are chosen to be
only slightly non-optimal; however the notion of non-optimal we are using is a very natural notion, and we have shown that AdaBoost may not converge to  here. Note that for some matrices M, a maximum margin solution may still be attained in the non-optimal case (for example the simple 3×3 matrix we analyzed above), but it is not attained in general as shown by our example. We are not saying that the only way for AdaBoost to converge to
a non-optimal solution is to fall into the wrong cycle; there may be many other non-cyclic
ways for the algorithm to fail to converge to a maximum margin solution. Also note that
for the other algorithms mentioned in Section 1 and for the new algorithms in Section 4,
there are fixed points rather than periodic orbits.

4 Coordinate Ascent for Maximum Margins

AdaBoost can be interpreted as an algorithm based on coordinate descent. There are other algorithms such as AdaBoost and arc-gv that attempt to maximize the margin explicitly,
but these are not based on coordinate descent. We now suggest a boosting algorithm that aims to maximize the margin explicitly (like arc-gv and AdaBoost) yet is based on co-
ordinate ascent. An important note is that AdaBoost and our new algorithm choose the direction of descent/ascent (value of jt) using the same formula, jt = argmaxj(dTt M)j. This lends further credence to the conjecture that AdaBoost maximizes the margin in the
optimal case, since the direction AdaBoost chooses is the same direction one would choose
to maximize the margin directly via coordinate ascent.

The function that AdaBoost minimizes via coordinate descent is F () =

m i=1

e-(M)i

.

Consider any  such that (M)i > 0 i. Then lima a will minimize F , yet the origi-

nal normalized  might not yield a maximum margin. So it must be the process of coordi-

nate descent which awards AdaBoost its ability to increase margins, not simply AdaBoost's

ability to minimize F . Now consider a different function (which bears a resemblance to an

-Boosting objective in [7]):

G() = - 1 ln F () = - 1 ln

m
e-(M)i

1

1

i=1

n
where  1 := j .
j=1

It can be verified that G has many nice properties, e.g., G is a concave function for each
fixed value of  1, whose maximum only occurs in the limit as  1  , and more importantly, as  1   we have G()  µ(), where µ() = (mini(M)i)/  1, the margin of . That is,

me-µ()  1  -(ln m)/  1 + µ() 

m i=1

e-(M)i

> e-µ()  1

G()

< µ()

(1) (2)

For (1), the first inequality becomes equality only when all m examples achieve the same
minimal margin, and the second inequality holds since we took only one term. Rather than performing coordinate descent on F as in AdaBoost, let us perform coordinate ascent on G. The choice of direction jt at iteration t is:

argmax
j

dG(t + d

ej

)

=
=0

argmax
j

m i=1

e-(Mt )i

Mij

F (t) t 1

+

1 t

2 ln(F (t)).
1

Of these two terms, the second term does not depend on j, and the first term is proportional to (dtT M)j. Thus the same direction will be chosen here as for AdaBoost.

Now consider the distance to travel along this direction. Ideally, we would like to maximize G(t + ejt ) with respect to , i.e., we would like:

0

=

dG(t + d

ejt )

t+1

1=

m i=1

e-(Mt )i

e-Mijt

 Mijt

F (t + ejt )

- G(t + ejt )

There is not an analytical solution for , but maximization of G(t+ejt ) is 1-dimensional so it can be performed quickly. An approximate coordinate ascent algorithm which avoids
this line search is the following approximation to this maximization problem:

0

m i=1

e-(Mt )i

e-Mijt

 Mijt

F (t + ejt )

- G(t).

We can solve for t analytically:

t

=

1 2

ln

1 + rt 1 - rt

-

1 2

ln

1 + gt 1 - gt

, where gt = max{0, G(t)}.

(3)

Consider some properties of this iteration scheme. The update for t is strictly positive (in the case of positive margins) due to the Von Neumann min-max theorem and equation (2),
that is: rt   = mindm maxj (dT M)j = maxn mini (M)i  mini (Mt)i/ t 1 > G(t), and thus t > 0 t. We have preliminary proofs that the value of G increases at each iteration of our approximate coordinate ascent algorithm, and that our algorithms
converge to a maximum margin solution, even in the non-optimal case.

Our new update (3) is less aggressive than AdaBoost's, but slightly more aggressive than arc-gv's. The other algorithm we mention, AdaBoost, has a different sort of update. It converges to a combined classifier attaining a margin inside the interval [ - , ] within 2(log2 m)/2 steps, but does not guarantee asymptotic convergence to  for a fixed . There are many other boosting algorithms, but some of them require minimization over non-convex functions; here, we choose to compare with the simple updates of AdaBoost (due to its fast convergence rate), AdaBoost, and arc-gv. AdaBoost, arc-gv, and our algorithm have initially large updates, based on a conservative estimate of the margin. AdaBoost's updates are initially small based on an estimate of the edge.

Margin Margin

0.65
arc-gv, approximate coord ascent, and coord ascent
0.5
AdaBoost

AdaBoost
arc-gv approximate coord. ascent and coord. ascent

0.16 0.13

AdaBoost

arc-gv

approximate coordinate ascent

AdaBoost*

0.4 0 Iterations 20

0.1 150 1100 90

400Iteration1s800

10000

Figure 3: (a-Left) Performance of all algorithms in the optimal case on a random 11 × 21 input matrix (b-Right) AdaBoost, arc-gv, and approximate coordinate ascent on synthetic data.

Figure 3(a) shows the performance of AdaBoost, arc-gv, AdaBoost (parameter  set to

.001), approximate coordinate ascent, and coordinate ascent on G (with a line search for

t at every iteration) on a reduced randomly generated 11 × 21 matrix, in the optimal case.

AdaBoost settles into a cycle (as shown in Figure2(d)), so its updates remain consistently

large, causing t 1 to grow faster, thus converge faster with respect to G. The values of rt in the cycle happen to produce an optimal margin solution, so AdaBoost quickly

converges to this solution. The approximate coordinate ascent algorithm has slightly less

aggressive updates than AdaBoost, and is very closely aligned with coordinate ascent; arcgv is slower. AdaBoost has a more methodical convergence rate; convergence is initially

slower but speeds up later. Artificial test data for Figure 3(b) was designed as follows:

50 example points were constructed randomly such that each xi lies on a corner of the

hypercube {-1, 1}100. We set yi = sign(

11 k=1

xi

(k)),

where

xi(k)

indicates

the

kth

component of xi. The jth weak learner is hj(x) = x(j), thus Mij = yixi(j). As expected,

the convergence rate of approximate coordinate ascent falls between AdaBoost and arc-gv.

5 Conclusions
We have used the nonlinear iterated map defined by AdaBoost to understand its update rule in low-dimensional cases and uncover cyclic dynamics. We produced an example to show that AdaBoost does not necessarily maximize the margin in the non-optimal case. Then, we introduced a coordinate ascent algorithm and an approximate coordinate ascent algorithm that aim to maximize the margin directly. Here, the direction of ascent agrees with the direction chosen by AdaBoost and other algorithms. It is an open problem to understand these dynamics in other cases.
References
[1] Robert E. Schapire. A brief introduction to boosting. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 1999.
[2] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651­1686, October 1998.
[3] Gunnar Ra¨tsch and Manfred Warmuth. Maximizing the margin with boosting. In Proceedings of the 15th Annual Conference on Computational Learning Theory, pages 334­350, 2002.
[4] Gunnar Ra¨tsch and Manfred Warmuth. Efficient margin maximizing with boosting. Journal of Machine Learning Research, submitted 2002.
[5] Leo Breiman. Prediction games and arcing classifiers. Neural Computation, 11(7):1493­1517, 1999.
[6] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1/2/3), 2002.
[7] Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classifier. Technical report, Department of Statistics, Stanford University, 2003.

