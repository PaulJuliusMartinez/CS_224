Proceedings of t h e IEEE Conference on Computer Vision and P a t t e r n Recognition Washington, D. C., J u n e , 1983

OPTIMAL PERCEPTUAL INFERENCE

Geoffrey E. Hinton . '
Computer Science Department Carnegie-Mellon University

.Terrence J. Sejnowski
Biophysics Department The J o h n s Hopkins University

-ABSTRACT
When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability
\
associated with a hypotlmis is rcprcscntcd by the probability h a t it is in one of two states, true o r false. Wc give a particular nondeterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~alinferences.
lnt roduction
One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth'value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units.
Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the

following problems: 1.They convcrge slowly.

2. It is bard to analyse what computation is being performed by the relaxation process. For example, in some vcrsions of relaxation there is no explicit global measure which is being optimized.

3. They are unable to integrate, in a principled way, two kinds of

decision. Some systems use rclaxation to make discrete

decisions (e.g. which kind of 3-D edge a line depicts) and the

numbcrs that are modified during relaxation then represent

Other systems choose the most likely values of

continuous physical parameters (e.g. the local surface

orientation) and the numbcrs that are modified then rcprcscnt

current cstimatcs of these

No system integrates

both kinds of decisioil and still guarantees convergence to the

optimal intcrprctation.

4. Systems designed to make discrctc decisions do not always convcrge to a state in which all probabilities for discrete hypotheses are 1or 0,so a subsequent stage is needed to choose a specific pcrccptual interpretation.

S.There is no obvious way for most systems to learn the appropriate values for the weighting coefficients that dctermi,ne how the probabilities of related hypothcses affect each other.
\
In this paper wc present a parallel search technique which overcomes these difficulties by using a different reprcscntation for probabilities. 1\11 the currcnt methods use real llumbcrs to represent the probabilities associated with hypotheses. Our method uses a more dircct encoding in which probabilitics are represented by probabilities. If a hypothcsis has a probability of two thirds of being correct, the unit representing it will have a probability of two thirds of being found in the "true" state and a probability of one third of being in t21e "false" state. Wc first show that this dircct cncoding allows the probability of one hypothcsis to determine the probabilitics of other related hypothcses even though none of the hypothesis units ever has enough information to allow it, for example, to print out its associated probability. We then describe a search method. using this cncoding. that finds plausible combinations of hypothcses. Next we show that, using,our search tcchniquc, thcrc is a Bayesian interpretation of h e weights that determine the effects of one hypothcsis on another, and

CH1891-1/83/0000/0448$1.001@983 IEEE

that the intcrprctation docs not rcquirc thc usual assumption of indcpcndcnce of multiple sourccs of cvidcncc.
F~nallywe give a learning rule that allows an optimal (or ncar optimal) set of wc~ghtsto be learnt from cxpcricncc. ?his learning rule can be uscd even in cases whcrc h e rcprcscntatior~sthat UIC systcm should use havc not been dec~dcdin advance. The rule generates new intcrn,~lrcprcscntations that make explicit thc higher-order statistical regularities in the environment.
R e ~ r e s e n t i n qroba abilities ,
Thcrc arc two vcry different senses of the phrase "communicate a probability". In the strong sense, a unit has communicated a probability to another unit if the second unit has received cnough information to allow it to print out the probability. In this strong sensc, it takes a long timc to communicate a prob'ability using discrcte stochastic states. T o decide whethcr a unit is adopting thc tnlc state 100 times pcr sccond or only 90 times per second, it is necessary to o b s c r x its state for a largc fraction of a sccond. In a tenth of a sccond there is only a difference of 1in the expected number of times thc unit is in the m c statc in the two cases. S o in this strong sensc, a unit that adopts truth valucs with a particular probability can only cornmunicatc thc probability very slowly (or vcry inaccurately). Even if there is little physical transmission dclay, there is still a long "dccoding" dclay bcforc anothcr unit has received cnough information to be able to make an accurate estimate of the probability.
I h c dccoding dclay can be rcduccd by using a Iargc pool of equivalent unlts, and by monitoring the outputs of all of them. If each unit is considcrcd to be a Poisson process, a pool of units is a Poisson process v,hosc rate is just d ~ seum of the individual ratcs, so the dccoding dclay is mversely proportional to the numbcr of units in the pool. However, thc use of population avcragcs is clearly cxpentive in tcrms of the number of units and connections rcquircd, and is thcrcfore only worth
doing if thcre is no more economical alternative. .'
Fortunately, for the kind of scarch we arc proposing it is not necessary to communicate probabilities in the strodg sensc of the term. What we rcquire is that the probability associatcd with unit B depends, in a particular way, on the probability associated with unit A. If these probabilitcs arc related by somc arbitrary function, it is generally ncccssary for unit A to communicate its probability to unit B in the
strong sensc of the term. But thcre is a special class of functions
relating the probabilirics of A and B that can be hnplcmentcd without the units ever having to "know" (ix. having enough information to print out) these probabilities. Thc simplest member of this class is the idenbty function. If B simply adopts thc samc state as A, its probability will be exaclly the same as A's, and there will be no dccoding delay. Whcncver the probability associatcd with A changes, UIC probability associated with B will change after a timc cqual to the transmission dclay alone. Another func:ion that can be implcmcntcd his way is a probabilistic disjunction. T o make the probability that

unit C is in the true state bc equal to thc probability that cithcr A or B is in the true statc, it is sufficient to make C true if cithcr A or B is m e .

Even though the statcs themselves are rcgardcd as probabilistic, the identity and disjunction functions involve a detcnninistic relationship bctween the statc of one unit and the statc of another. A nondetcmlinistic relationship can be uscd, for cxamplc, to make the probability associatcd with B be half the probability associatcd with A. The ntle is simply that B adopts the true statc with a probability of one half if A is in the true state. This is a "doubly-stochastic" proccss
in which onc probability is a probabilistic function of anothc*r. W. e use,
such proccsscs in our model of perceptual inference.

Searchinq for minimum enerqv states of a

-n e-t w o r k

.<

Givcn a perceptual input derived from .some particular world, cach possible combination of hypotheses has a particular probdbility of being the correct intcrprctation of the input We show laicr that the probability can be rclated to a potential cncrgy function, so that the most plausible combination of hypothcscs is the one with lowest potcntial encrgy. First we give an exprcssion lbr the "potential cnergy" of a state of a network and show how thc proccssors have to behave in order to minimizc the energy.

~ o ~ f i c l dc' scribcs a system with a li~rgcnumbcr of binary units. The units arc ~jwrneiticu!(vconnected, wit. the strength of r11c co!incction
being the same in both directions. Hopficld has shown that therc is an exprcssion for the "enc~gy"of a global statc of the network, and with the right assumptions, the individual units act so as to minimize the global cnergy. We use a variation of IHopficld's system in which a particular task is dcfined by suslained inputs from outside thc system. and the interactions bctween units implement constraints bctwccn hypotheses.'lhe cncrgy of a statc can Lhcn be intcrpretcd as the extent to which a combination of hypotheses fails to fit the input daki and
\
violates the condtraints between hypotheses, so in minimizing cnergy ,the system is maximizing the extent to which a perceptual interpretation fits the data and satisfies the constraints.

?'he global potential energy of the. systcm is dcfined as

whcre q, is the cxtcrnal input to the ifh unit, wu is the strcngth of conncction (synaptic weight) from t h c t h to the ifhunit, siis a boolean truth value (0or I),and 0,is a threshold.
A simple algoritflm for finding a copbination of truth values that is a locnl minimum is to switch each hypothesis into whichever of its two skitcs yields the lower total energy given the current sutcs of thc othcr hypothcscs. If hardware units makc their decisions asynchronously. and if transmission times are negligible, Ulcn thc system always settles into a local cncrgy minimum. Because the connections are

symmetrical. the diffcrence between the energy of the whole system with the kth hypothesis false and its energy with the kth hypothesis true can be determined locally8 by the kth unit and is just

(2)

Therefore, the rulc for minimizing the energy contributed by a unit is to adopt the true state if its total input from the other units and from outside the system exceeds its threshold. This is the familiar rule for binary threshold u n i k

Usinq probabilistic decisions to escaoe from local minima

The deterministic algorithm suffers from the standard weakness of

gradient dcsccnt methods: It gets stuck at local minima that are not

globally optimal. This is an inevitable consequence of only allowing

jumps to states of lower energy. If, however, jumps to higher energy

states occasionally occur, it is possiblc to break out of local minima.

An algoritllm with this property was introduced by Metropolis el. 01.'

to study average properties of thermodynamic

ad! has

recently been applied to problems of constraint shsfactionl'. We

adopt a form of the Metropolis algorithm that is suitable for parallel,

computation: If the energy gap betwecn the true and false states of the

kth unit is AEk then regardless of the previous statc set sk=l with

probability

where T is a parameter which acts)ike temperature (see fig. 1).

where Pa is the probability of being in the ah global state, and Ea is
the energy of that state.

At low tcrnpcraturcs there is a strong bias in favor of states with low energy, but the time required to reach equilibriun~may be long. At highcr temperatures the bias is not so favorable but equilibrium is reached faster.
Bavesian inference
Baycsian inference suggests a general paradigm for pcrccptual interpretation problems. Suppose the probability associated with one unit rcprescnts the probability that a particular hypothesis, h, is correct. Suppose, also, that the "true" statc of anothcr unit is used to represent the existcnce of some evidence, e. Daycs theorem prcscribcs a way of updating the probability of the hypothesis A h ) given the existence of new evidence e:

+'

p(hIe)=

p(h)p

(

p(h)
eI 4

p

(

eI h) p(J9~

(

e

I

J9

where 7iis the negation of h.
'I'hc Ilaycs rulc has the same form as the decision rule 111 l:4 (3) if we identify the probability of the unit with the probability of the hypothesis. The threshold implements the a priori likelihood ratio, the extcrnal input implements the effect of the direct evidence in the image, and the synaptic weights implement the cffcct of the evidence providcd by the states of other hypotheses (assuming the temperature is fixed at 1):
\

Figure 1
Probability p(AE) that a unit is in its "true" smte as a function of its energy gap AE plotted for T= 1 (Eq. 3). As the tempcrature
, . is lowered to zero the sigmoid approaches a step function.
This parallel algorithm ensurcs that in thermal equilibrium the relative probability of two global states is determined solely by their energy difference, and follows a Boltvnann distribution.

Bayesian inference with one piece of evidence can therefore be implemented by units of the type we have been considering. There are, however, scvcral difficulties with this simple formulation.
1. It provides no way for the negation of the evidence e to affect the probability of h. '

2. It does not lead to symmetrical weights when two units affcct each other since p(elh)/p(elli) is generally not equal to
p(hle)/p(hl3.

3. Although it can easily be gencralised to cascs where thcrc are

many independent pieces of evidence, it is much harder to

generalisc to cases where the pieces of evidence not independent

of each other.

't,

A diagrammatic representation of the way to solve the first difficulty is

shown below. ' h e diagram uscs a convention in which threshold terms are implemented by weights of the opposite sign on a connection from a permanently true unit This TRUE unit is just a hypothetical device for allowing threshold terms to be treated in the same way as pairwise interactions. ft simplifies the mathematics because it allows all terms in
the energy expression to be treated as pairwise intcractions. (The sustained external inputs that specify the particular data to be interpreted can also be turned into painvise terms by treating them as weights on lines from units that are fixed in the true state for that particular case). The effcct of ;can be implemented by putting it into the threshold term for h, and by subtracting an equal amount from the weighting coefficient from e, so that when e is in the true state the effect of the threshold term on h is cancelled out.

@e!wh
awhere whF=ln P(m ' I l ~ u sthe combined weight from e is:

'
w

Equation 6 is symmetrical in eand h, so in solving Uie problem of how to make the negation of e have the correct effcct on h we have also
solved the second problem -- the required weights arc now
symmetrical. The more complicated weight in Eq. 6 does not alter the fact that the probability of a hypothesis has the form of the Bolhnann distribution for a unit with two energy states.
Systems which use Bayesian inference often make the assumption that pieces of evidence are independent.12s13The main motivation for this assumption is that too much memory would be required to store all the dependencies, even if they were known. The independence assumpdon is hard to justify and it is typically a poor approximation in systems with many mutually interdependent hypotheses. A much better approximation, given some fixed set of variable weights, can be achieved by using whatever weights give the best ovcrall approximation to the correct probabilitics for the various possible combinations of hypotheses. At first sighf it is very hard to derive these wcights, since the correct value for each weight depends on all the others. However, we now show that there are ways to hill-climb towards the optimum combination of weights.

Learninq
When a system is allowed to reach thcrmal equilibrium using the probabilistic decision rule in Eq 3, the probability of finding it in any particular global state depends on the energy of that state (Eq 4). and so the probability can be changed by modifying the weights s o as to
change the energy of the state. d4we describe a learning rule which
assumes that in addition to the input data, the system is given the desired probability ratios for pairs of global states. The rule is guaranteed to converge on a set of wcights that causes the system to behave in accordance with the desired probabilitics (if any such set of weights exists). We now describe a more gencral learning rule that does not require any separate source of information about thc desired probabilities of global states. The rule leads to continual improvcmcnts in the network's model of its environment.

Suppose h a t the environment directly and completely determines the

states of a subset of the units (called the "visible" units), but leaves the

network to determine the states'of the remaining. "hiddcn" units. The

aim of the learning is to use the hidden units to create a model of the

structure implicit in the ensemble of binary state vectors ttrat the

environment determines on the visible units.

't'.

We assume that each of the environmentally determined state vectors persists for long enough to allow the rest of the network to' reach thermal equilibrium, and we ignore any structure that may exist in the

sequence of environmentally determined vectors. The structure of the environment can then be spccified by giving the probability distribution over all 2' states of the v visible units. The network will be said to have a perfect model of the environment if it achieves exactly the same probability distribution over these 2" states when it is running freely at thermal equilibrium with no environmenlal input.

In general, it will be impossible to achieve a perfect model because the
\
1/2(v+h12 wbights among the v visible and h hidden units are
insufficient to model the 2' probabilities of the environmentally determined statcs of the visible units. However, if there are regularities in the environment, and if the network uscs its hiddcn units to capture these regularities, it may achieve a good match to the environmental
probabilities.

An information theoretic measure of the discrepancy between the network's internal model and the environment is

where P(VJ is the probability of the aLhstate of the visible units when their states are determined by the environment, and P'(Va) is the corresponding probability when the network is running freely with no envir~nmental,input.The term P1(Va) depends on the weights. and so G can be altered by changing the weights. T o perform gradient descent in G, it is necessary to know the partial derivative of G with

respect to cach individual wcight In most cross-coupled non-linear networks it is very hard to derive h i s quantity, but bccausc of the simple relationships that hold at thermal equilibrium, the partial derivative of G is fairly simple to derive for our nctworks. The probabilities of global states arc determined by thcir energies (Eq. 4)
and the encrgies arc dctcnnincd by the weights (1q.1).using' these
equations it can'be shown that
where sy is the state of thc thunit in thc n f hglobal statc. P,, is the
probability of the n t h global state (dcfincd over both the visible and hidden unitc) when UE network is bcing driven by the cnvironmcnt so that the statcs of the visible units do not dcpcnd on the weights, and P I v is the probability of the nrh global state whcn thc ne'twork is running freely.
T o minimize G, it is therefore sufficient to increment cach wcight by an amount proportional to the diffcrencc bctwceh two frequencies. The first is the frequency with which the two units that thc wcight connccts are both on whcn the network is being driven by the environmcnt, and the second is Ule corresponding frcqimcy when the nctwork is running freely without cnvironmcntal input. Doth frequencies must be mcasurdd whcn the network is at thermal
equilibrium. A surprising feature of this rulc is that it uses only locall)* availnble infonnation. The change in a wcight dcpcnds only on the bchaviour of the two units it connccts, even though the change optimi~esa global mcasure, and Uie bcst valut for each wcight depends on the values of all the other weights.
Once G has been minimized h e nctwork will be able to gcncrate plausible cornplctions when the environmcnt only determines the states of some of the visiblc units. The network will have captured the best regularitics in the environment and these regularities will be enforced whcn performing complctioh. One way to use this completion ability would be to divide the visiblc units intotwo subsets called "input" and "output". During "training" the environment would consist of pain of inputs and desired outputs. In minimizing G, the network would then be finding weights that allowed it to predict the output whcn given the input alone.
If there are no hidden units, the weight space is concave in G so gradient descent will find the global minimum. When there arc hiddcn units. the same learning rule still performs gradicnt descent in .G, but there are non-global minima in the wcight space, and the sytem can get stuck at one of these sub-optimal values of G.This occurs when the systcm is doing the bcst that it can given thc representations it has learnt in the hiddcn units. T o do better it has to change these
representations which involves a temporary increase in G.O f course, if
the modifications to the weights are probabilistic so that G can

sometimes increase, it is possible to cscapc from local minima and ensure that after enough lcarrling thcre is a b h s in favor of globally optimal or near optimal scts of wcights.
Potential e n e r q v a n d ~ e r c e n t u a iln f e r e n c e
In designing a parallel system for pcrccptual inference, the cncrgy was important for two reasons. It rcprcscnted the dcgrcc of violarion of the constraints between hypotheses, and it also dctcrmincd the dynamics of the search. From a few simple postulates about the energy it is possible to derive the main propertics of the probabilistic systcm.
Postulate 1: There is a "po~etrlinlerrrrgy"Jirrrc/io,t over ~ r a / e sof fire ~vhaiesystern which is a fut~ction,jrPa), of /he probabiliry of a sure. This is equivalent to saying that, given any input, a particular combination of hypotheses has exactly one probability. It docs not, for example, have a probability of 0.3 and alsb a probability of 0.5.
Postulate 2: The porenlial energy is addiliw f i r indepettrienr system Since the probability for a combination of statcs of indcpcndcnt
systems is multiqlicative, it follows that/(P,)-tflPp)=fllh Pp). The
only function that satisfies this equation is/(/',)= k In (P,). To make more probable sutes have lower energy k must be negative.
Postulate 3: The par/ of /he po/enrial urergy corrrriburedby a 5brgle uni/ can be cotnputedfrom bl/onnatiot~available 10 the unil. Only potential cncrgics symmetrical in all pairs of unirs have this property,'since in this case a unit can "deduce" its effect on other units from thcir effect on it.
Discussion
Wc have given a brief and condenscd description of a new relaxation mcthod that overcomes many of the drawbacks of current mcthods. There is'not space for a detailed discussion of the many. interesting qucstions qaised by Lhe new mcthod, and so we shall just mention a few of the more important issues here.
We have ignored the difficult question of how long it takes the system to reach equilibrium. The efficiency of the whole method depends on equilibrium being reached fairly rapidly, so this is a crucial issue. Several methods of speeding the approach to equilibrium are described briefly in14 but more rcscarch is needed. A group at Brown University (Gcman, private communication)' have independently discovered the value of this kind of non-detcrministic search as a model of parallcl computation, and they are deriving bounds on the rate of approach to equilibrium.
It'may seem disadvantageous to have a systcm which docs not always find the most probable interpretation of the perceptual input, but inste'ad produces interpretations with a probability that equals thcir probability of bcing correct However, a system that integrates many different kinds of constraints will almost always pick the corrcct interpretation of a natural scene bccausc with enough information the

corrcct intcrprctation is ovcrwhclmingly morc likcly than any ~ t h c r ? ~ Also, by Iowcring the tcmpcraturc and running the systcm for longer it is possible to cxaggcratc thc probability with which the most plausible intcrprctation will be selected.
The natural way to repi'cscnt continous parameters for our relaxation mcthod is to divide thcir rangcs into a number of ovcrlapping intervals and to set asidc a unit for each intcrval16. The truth-value of a unit then indicates whcther thc continous parameter lies within its interval. By using large overlapping intervals, this rcprcscntation can be made both accuratc and eflicicnt for cncoding'~nultidimensionalvariables.17 An advantagc of using this "mosaic" encoding is that 'it allows dccisions about discrete and continuous variables to be integrated into a singlc scarch in a principlcd way.

information about the highcrwrdcr sLitistics of chc enscmble. In a case like the Necker cube, for cxample, thcrc may be two alternative collcctions of hypotheses that form equally plausible interpretations. and a probabilistic binary machine may occasionally flip betwccn thcse collections. A real-valued machine would assign a valuc of 0.5 to each hypothcsis in eithcr collcction, and would thus fail to reprcsent which hypothesis goes with which.

Acknowledqements
This work was supported by grants from the System Dcvclopment Foundation and by earlier grants from the Sloan Foundation to Don Norman and to Jerry Fcldman. We thank Dana Ballard, Francis Crick, Scott Fahlman, David Rumclhart, and Paul Smolcnsky, for helpful discussions.

i

We havc ignorcd h e fact h a t at flnitc tcmpcraturc the system will inevitably scttle into a "dc.gcncrate" minimum in which it fluctuates among a collcction of similar states. This is actually an advantage since the proportion of the tinic a unit is true within the degcncrate minimum allows it to convcy morc information about thc solution than a single truth value.
Wc havc assumed that the connections arc all symmetrical in ordcr to simplify the analysis. This assumption, however, can bc relaxed. Given the syrnmctry of the potential cncrgy function, it is not
ncccssary to liavc two-way connccdons in Uic pnmllcl hardware. If a ~yrninctricalnctwork is dcgradcd by removing one of the directions for cach pairwise link, its bchavior will still app[oximate the bchavior of thc original network provided each unit has a large numbcr of inputs. and the choice of which direction to remove for each link is random rclativc to the potential encrgy function. If thcse conditions hold, a unit can gct a good, unbiased estimate of what its total input would have bcen if all the conncctions had been synmctrical.
A very common misconception about our relaxation method is that it is just a noisy version of continuous relaxation methods which associate a real-number with cach unit. According to this view, it is the timc average of thc truth valucs that is important in the computation, and this time average can be reprcscntcd by an approximate realnumber. 'Ihis view is wrong for several reasons. First, the coniputation is pcrformed by the non-equilibrium process of reaching equilibrium, and during this process there are major differences bctwcen the ensenible average (taken over a collection of identical non-clctcrministic machincs) and thc time averagc (taken over timc for a single machine). For example, probabilities can be accurately dcfincd over very short time periods using cnsemblc averages and they can also change very rapidly. Second, the behaviour of a large enscmble of identical machincs containing binary units cannot be rnodcllcd adcquatcly by a single machine that contains real-valued units whose values reprcsent the fraction of thc corresponding units
that zre on in the cnscmblc. The singlc rcal-valued machine looses

REFERENCES

1. Davis, L. S. & Rosenfeld, A. Cooperating processes for low-lcvcl

vision: A survey. Arlificial InteNigetice 1981, 17, pp245-261.

2. Hinton, G. E. Relaxation and its role in vision. PhD Thesis,

University of fdinburgh. 1977; Described in: Conlp~iterVision,

D. H. Ballard & C. M. Brown (Eds.) Englewood Cliffs, NJ: Prcntice-Mall, 1982, pp. 408-430.

3. Hummel, R. A. & Zuckcr, S. W. On thc foundations of relaxation labeling processes. 'I'R-80-7, Computer Vision Lab.

McGill University, July 1980.

4. Marr, D. & Poggio, T. Coopcrative computation of stereo disparity. Science, 1976 194, p 283-287.

5. Faugeras, 0.1). & Ucrthod, M. Improving consistency and

reducing ambiguity in stochastic labcling: An optimization ' approach. IEEB Transac~ionson Patrern Analysis and Machine
Inlelligence, 1981, PAMI-3, pp 412-424.

6. Ikeuchi. K. & Horn, 13. K. P. Numerical shape from shading and

occluding boundaries. Arrificial In/elligence 1981, 17, pp 141-184.

7.

Grimson, W. E. L. From MI'T Press. 1981.

images

to

surfaces.

Cambridge

Mass:

.

.

8. Hopfield, J, J. Neural networks and physical systems with ,

cmcrgent collective computational abilities. Proceedings of rhe

Nalional Academy of Sciences USA, 1982, 79 pp 2554-2558.

9. Metropolis, N. Rosenbluth, A. W. Rosenbluth, M. N. Teller, A. H. Tellcr! E. Journal of Chemical Physics. 1953 6, p 1087.

10. Binder, K. (Ed.) The hfonte-Carlo Merhod in Staristical Physics New York: Springer-Verlag, 1978.

11. Kirkpatrick, S. Gelatt, C. D. & Vecci, M. P. optimization by

simulated annealing. Scic~tce(in press)

12. Woods, W. A. Shortfall and dcnsity scoring strategies for specch understanding control. In Procccdings of the Fifth International

Joint Confcrcnce on Artificial Intclligcnce. Cambridge Mass. August 1977, pp 18-26.

13. Yakimovsky, Y. & Feldman, J. A. A semantics-based decision

theory region analyser. In Proceedings of the Third

International Joint Confcrencc on Artificial Intelligence, Menlo Park CA, 1973, pp 580-588.

14. Ilinton, G. E. & Scjnowski, T. J. Analyzing Cooperative Computation. To appear in: Procccdings of thc Fifth Annual

Confcrcnce of the Cognitive Scicnce Society. Rochester NY. May 1983.

15. Gibson, J. I. The perception of the visual world. Boston:

Houghton Mifflin. 1950.

16. Feldman. J. A. & Ballard, D. H. Connectionist modcls and their properties. Cognilive Science, 1982.6. pp 205-254.

17. Ilinton, G. E. Shape rcpresentation in parallcl systems. In Procccdings of the Seventh International Joint Confcrcnce on Artificial Intclligencc, Vol2. Vancouver BC, Canada. 1981.

