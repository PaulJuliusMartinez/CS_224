Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms
Michael Kearns and Satinder Singh
AT&T Labs 180 Park Avenue fmkeaFrlnosr,hbaamvejPaagr@k,reNseJar0c7h9.3a2tt.com
Abstract
In this paper, we address two issues of long-standing interest in the reinforcement learning literature. First, what kinds of performance guarantees can be made for Q-learning after only a nite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based indirect approaches, which use experience to estimate next-state distributions for o -line value iteration? We rst show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed. In particular, on the order of only N log1= = 2logN + log log1=  transitions are su cient for both algorithms to come within of the optimal policy, in an idealized model that assumes the observed transitions are well-mixed" throughout an N-state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution. The result also shows that the amount of memory required by the model-based approach is closer to N than to N2. For either approach, to remove the assumption that the observed transitions are well-mixed, we consider a model in which the transitions are determined by a xed, arbitrary exploration policy. Bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy.
1 Introduction
There are at least two di erent approaches to learning in Markov decision processes: indirect approaches, which use control experience observed transitions and payo s to estimate a model, and then apply dynamic programming to compute policies from the estimated model; and direct approaches such as Q-learning 2 , which use control

experience to directly learn policies through value functions without ever explicitly estimating a model. Both are known to converge asymptotically to the optimal policy 1, 3 . However, little is known about the performance of these two approaches after only a nite amount of experience. A common argument o ered by proponents of direct methods is that it may require much more experience to learn an accurate model than to simply learn a good policy. This argument is predicated on the seemingly reasonable assumption that an indirect method must rst learn an accurate model in order to compute a good policy. On the other hand, proponents of indirect methods argue that such methods can do unlimited o -line computation on the estimated model, which may give an advantage over direct methods, at least if the model is accurate. Learning a good model may also be useful across tasks, permitting the computation of good policies for multiple reward functions 4 . To date, these arguments have lacked a formal framework for analysis and veri cation. In this paper, we provide such a framework, and use it to derive the rst nite-time convergence rates sample size bounds for both Q-learning and the standard indirect algorithm. An important aspect of our analysis is that we separate the quality of the policy generating experience from the quality of the two learning algorithms. In addition to demonstrating that both methods enjoy rather rapid convergence to the optimal policy as a function of the amount of control experience, the convergence rates have a number of speci c and perhaps surprising implications for the hypothetical di erences between the two approaches outlined above. Some of these implications, as well as the rates of convergence we derive, were brie y mentioned in the abstract; in the interests of brevity, we will not repeat them here, but instead proceed directly into the technical material.

2 MDP Basics

GRfVudapaLannnirxMeMaicsodtvedcttabdeoeohMiRcanirruatab.aMnit+aQn=oitbIlmdnreiMtetiadEayPitbn,asxostodnwoiuojaumoefremlPunmad1signccModzea+ohaoeknewicnsfdnnoioastrojtgtnlchrebewkaov2wVeettmtnegoteMa+hvnwo.rpaNeivsdeeluTtjeuerns-a2trenshete.rtnrwtee3eaaet0Ttactodthv+jreheaaeed,idbevvnlMuoeygeerdgpeierdDovynct1a,oeeiPuemlfsnfiwwprvtovoswaaoehitfatmttdlnhieallheteuprat;hofsaeseetoottutrslaArhanriwtircfretieotuentylaxiooringncssespaadttgcssirstaitteuseoihaitormttnnetefioipnesnasig.-raogl,tiaeeplsavWacnwiatpatct=naeeaylirefrdtuoorraeiandouex0lramisi,figtmperunyVmeaandiaPcMin.caertMedtasexiwxivAiaoteheefihni,cdx1ij;pQecuieiahosistaoMcslitutpd:intwotchtteghityQeeined;imemaoaMeaedntsaxcefegslioatopudis.pitss;rcemoetseoaoQcenivlugtetpiMeehcnna=ridyetsst.:;

If M is given, value iteration can be used to compute a good approximation to the

optimal value function. iterate as follows:

Setting our initial guess

as Q0i; a

= 0 for all i; a, we

Q`+1i; a = RaMi + X PMa ijV`j
j

1

wpmuhatexereit;hawefegjQrde`eediyn; aeapV,p`rQojMxim=i;aamtiojagnxbfQto``.tjhG;ebiovgpe.tnimaIntaylcpaaonplpibcryoexsimhoaawstinontihQa=ttoaarQfgtemMraw`xaeiftceQarnait;ciooanmgs-,.

3 The Parallel Sampling Model

In reinforcement good policy must

learning, the transition be learned on the basis

pofroobbasberilvietidesexPpMaeriiejncaeretrnaontsigtiiovnens,

and a in M.

Classical convergence results for algorithms such as Q-learning 1 implicitly assume

that the observed experience is generated by an arbitrary exploration policy" , and

then proceed to prove convergence to the optimal policy if  meets certain mini-

mal conditions | namely,  must try every state-action pair in nitely often, with

probability 1. This approach con ates two distinct issues: the quality of the explo-

ration policy , and the quality of reinforcement learning algorithms using experience

generated by . In contrast, we choose to separate these issues. If the exploration

policy never or only very rarely visits some state-action pair, we would like to have

this re ected as a factor in our bounds that depends only on ; a separate factor

depending only on the learning algorithm will in turn re ect how e ciently a partic-

ular learning algorithm uses the experience generated by . Thus, for a xed , all

learning algorithms are placed on equal footing, and can be directly compared.

There are probably various ways in which this separation can be accomplished; we now introduce one that is particularly clean and simple. We would like a model of the ideal exploration policy | one that produces experiences that are well-mixed", in the sense that every state-action pair is tried with equal frequency. Thus, let us de ne a parallel sampling subroutine PSM that behaves as follows: a single call to PathcSecoMrredsiunlrgteitntugornNPsM,afAoijrne.evxeTtrhysutasstt,aeetsvea-earrecytriseotpnaotrpet-aeaidcr.tioAi;nasipn,agailreriacsaneldlxotemocuPtnSeedxMtsismtiasutltethajenredeofiosutrsreliybr,euaatnlelddy simulating N  A transitions in M, and we must be careful to multiply the number of calls to PSM by this factor if we wish to count the total number of transitions witnessed.

What is PSM modeling? It is modeling the idealized exploration policy that manages to visit every state-action pair in succession, without duplication, and without fail. It should be intuitively obvious that such an exploration policy would be optimal, from the viewpoint of gathering experience everywhere as rapidly as possible.

We shall rst provide an analysis, in Section 5, of both direct and indirect reinforcement learning algorithms, in a setting in which the observed experience is generated by calls to PSM. Of course, in any given MDP M, there may not be any exploration policy that meets the ideal captured by PSM | for instance, there may simply be some states that are very di cult for any policy to reach, and thus the experience generated by any policy will certainly not be equally mixed around the entire MDP. Indeed, a call to PSM will typically return a set of transitions that does not even correspond to a trajectory in M. Furthermore, even if PSM could be simulated by some exploration policy, we would like to provide more general results that express the amount of experience required for reinforcement learning algorithms under any exploration policy where the amount of experience will, of course, depend on properties of the exploration policy.

Thus, in Section 6, we sketch how one can bound the amount of experience required under any  in order to simulate calls to PSM. More detail will be provided in a longer version of this paper. The bound depends on natural properties of , such as its stationary distribution and mixing time. Combined with the results of Section 5, we get the desired two-factor bounds discussed above: for both the direct and indirect approaches, a bound on the total number of transitions required, consisting of one factor that depends only on the algorithm, and another factor that depends only on the exploration policy.

4 The Learning Algorithms

We now explicitly state the two reinforcement learning algorithms we shall analyze and compare. In keeping with the separation between algorithms and exploration policies already discussed, we will phrase these algorithms in the parallel sampling framework, and Section 6 indicates how they generalize to the case of arbitrary exploration policies. We begin with the direct approach.

Rather than directly studying standard Q-learning, we will here instead examine a

variant that is slightly easier to analyze, and is called phased Q-learning. However, we

eBwmaisptihhcaallesliayzr,enraitnhtghaetrraattlehlaonfiuo;paudra=rteinstugi1l;ttash,ecwavnahleubreee

generalized to apply to standard Q-learning ti; a is the number of trials of i; a so far. function with every observed transition from

i; a, phased Q-learning estimates the expected value of the next state from i; a

on the basis of many transitions, and only then makes an update. The memory

requirements for phased Q-learning are essentially the same as those for standard

Q-learning.

Direct Algorithm | Phased Q-Learning: As suggested by the name, the algo-
srtwitahhtmee-raeocpmtieoDrnatwpesaililirnbpie;hadaes.teesAr.mtIntinheeead`cthbhypphthhaaseese,a,nthtahelyeasaligslgo,orirttihhtuhmsmwguailptlhdmearatienksgetmmheDDecstatrilimlaslasttoeodPf SevvaMelurye function as follows: for every i; a,

Qb`+1i; a = RaMi +

1 mD

XmD
k=1

Vb`jk` 

2

where PSM

j1`d;u:r:i:n;gjm`tDhea`rtehthpehamseD.

next The

states policy

observed from i; computed by the

aalgoonritthhme misDthceanllsthtoe

greedy policy determined by the nal value function. Note that phased Q-learning

is quite like standard Q-learning, except that we gather statistics the summation in

Equation 2 before making an update.

We now proceed to describe the standard indirect approach.

npcsIatonerasomxdtbtdeiparesujbsettcaicwelrttidiaetbisAebessradylemgaaitnoschphrelfSeioeetsadlchlloftgoomiwoonrrsn:eit:tah2ThPcbemhhMamoeniiIsai;jtlttahgrhoei=.earilIgtsxthreoetmdifemh!demIiyan;rjoaspbdt,oue.wmlililTcPdhbayhseMkaredeaesaniicjlmtgeaomIftiroepi!crdtiahr`albimIlcysjaptltthhohmiaesesnPoetdSshdn.eeoMlaAenlosgufvvamatatihlonlbueu,eeoettrbfhriuoatteaenfnrpicstnatioittmiloiimioocnennyIs.

pvTTahhhlauuesseed,iriitrneeeqrcpauthtiriaaoeslnsegdmorwQDith-hlicmecahalrlsrnertiqnsotugPi,mrStehsaMekneaosl,gamfodorIrditiahtciamtoolnltsraaulltnodnsauPftmoaSrb,sMeofromor,featanrtnuaodmntsabtiltheinerounn`mDsr`bupDenhrsaosm`feIstD,rpaahnnaNsdsiteeisaoAcnohs.f

smoIthaNt,

 A. with

The question probability at

we now address is: how least 1 , , the resulting

plaorlgiceiems uhsatvemeDx;pmecIt;e`dD;re`tIubrne

within of the optimal policy in M? The answers we give yield perhaps surprisingly

similar bounds on the total number of transitions required for the two approaches in

the parallel sampling model.

5 Bounds on the Number of Transitions

We now state our main result.

Theorem 1 For any MDP M:

FoforcaalnlsatpopProSprMiatercehquoiirceedobfythtehepairnadmireetcetraslmgoIriathnmd ainndo`rId,erthteoteontaslunreumthbaetr, with probability at least 1 , , the expected return of the resulting policy will be within of the optimal policy, is

O1= 2logN=  + log log1= :

3

FcaolrlsatnoaPppSroMpriarteequcihroeidcebyofpthhaesepdarQam-leeaterrnsinmgDinanordd`eDr ,totheentsoutrael

number of that, with

probability at least 1 , , the expected return of the resulting policy will be

within of the optimal policy, is

Olog1= = 2logN=  + log log1= :

4

The bound for phased Q-learning is thus only Olog1=  larger than that for the

indirect case are

algorithm. obtained by

Bounds on multiplying

the the

total given

number bounds

of by

NtransAit.ions

witnessed

in

either

Before sketching some of the ideas behind the proof of this result, we rst discuss some of its implications for the debate on direct versus indirect approaches. First of all, for both approaches, convergence is rather fast: with a total number of transitions only on the order of N logN  xing and for simplicity, near-optimal policies are obtained. This represents a considerable advance over the classical asymptotic results: instead of saying that an in nite number of visits to every state-action pair are required to converge to the optimal policy, we are claiming that a rather small number of visits are required to get close to the optimal policy. Second, by our analysis, the two approaches have similar complexities, with the number of transitions required di ering by only a log1=  factor in favor of the indirect algorithm. Third | and perhaps surprisingly | note that since only OlogN calls are being made to PSM again xing and , and since the number of trials per state-action pair
iatCwmsrroluoeeeredvxdoeeatsnrlcr,latyPPrblbneyMMasapOitrtheiiiNosjjeennnlowpbtugarumitolilNilbbotbaenerbbyiooelrfxifatttPtcihbrehaeMesaelmlr|sinietjtldhoyyiaerwPsnetpcSisaltOtlrMisatlelNhpgup|,2osrtoo.hraadeencFsehutdonolutoittsrahutilugnhinnsh,,tuaotfamaolttactbhdtleeoermroruriingobevlhmfelyenaowOaorenpnyep-ldzroaroeegroqr-xonouNipomierttneiammt.htriaiaeoevInlnnsetpstisoonoptltihhttachhaceyeeert!
to provide any details, if instead of a single reward function, we are provided with L reward functions where the L reward functions are given in advance of observing any experience, then for both algorithms, the number of transitions required to compute near-optimal policies for all L reward functions simultaneously is only a factor of OlogL greater than the bounds given above. Our own view of the result and its implications is:
Both algorithms enjoy rapid convergence to the optimal policy as a function of the amount of experience. In general, neither approach enjoys a signi cant advantage in convergence rate, memory requirements, or handling multiple reward functions. Both are quite e cient on all counts.
We do not have space to provide a detailed proof of Theorem 1, but instead provide some highlights of the main ideas. The proofs for both the indirect algorithm and phased Q-learning are actually quite similar, and have at their heart two slightly

di erent uniform convergence lemmas. For phased Q-learning, it is possible to show wtheatc,anforchaonoysebmouDndso`Dthoant the number of phases to be executed, and for any 0,

1=mD

XmD
k=1

Vb` jk` 

,

X
j

Piaj

Vb`j



5

will hold simultaneously for every words, at the end of every phase,

i; a and for the empirical

every phase estimate of

`= the

e1x;p:e:c:t;e`dD

. In other next-state

value for every is with respect

i; to

athewcilulrbreenctloessetitmoattheedtvrauleueexfpuenccttaitoinonVb,`w. here

here

the

expectation

For the indirect algorithm, a slightly more subtle uniform convergence argument is

ovreafqliuuteeirrmeadtI.iosHnusecrhoeftwhveaaltsuheowitetrhaatXitointPbitsioajpVbo`esjseibx,leecXutotecdPhioaojonVs`et,hjeforPbMaanyijbo, uanndd

`I on the for any

number 0, a 6

jj

for every resulting

i; a from

paenrdfoervmerinygpthrauseev`al=ue1;it:e:r:a;t`iIo,nwhthearet

essentially says that expectations of the true value

itsh,eoVn`thje functions

aaPrrMeea qthiujeitev.asEliumqeuilfaautrniocuntniodn6ers

either the true or estimated model, even though the indirect algorithm never has

access to the true value functions.

In either case, the uniform convergence results allow us to argue that the corre-

sponding algorithms still achieve successive contractions, as in the classical proof

oafs`fvo=allluomwe asi:xteir;aatfiojQnb.`i;Faor

instance, in , Q`i; ajg,

the case of phased Q-learning, if we we can derive a recurrence relation for

de ne `+1

jQb`+1i; a , Q`+1i; aj =


1=m Xm Vb`jk` , X PiajV`j

7

2mf a;,x

k=81 g:

0@X
j

Piaj

j
Vb`j

+

1 A

,

X
j

PiajV`j

=9 ;8

 + `:

9

Here gives

we `

have

m=a1d,e

use 

of for

Eanqyua`t.ioFnrom5.thSisinitceisn0ot=ha0rdQbto0

s=hoQw0th, atthifsorreacnuyrrein; cae

jQb`i; a , Qi; aj   =1 ,  + `:

10

From this computed

it can be shown that by phased Q-Learning

tahfeterre`grpehtaisneseixspaetctmedosrteturn=1su,

ered +

b`y2t=he1p,olicy.

The proof proceeds by setting this regret smaller than the desired , solving for ` and

, and obtaining the resulting algorithm is similar.

bound

on

mD.

The

derivation

of

bounds

for

the

indirect

6 Handling General Exploration Policies

As promised, we conclude our technical results by brie y sketching how we can translate the bounds obtained in Section 5 under the idealized parallel sampling model into

bounds applicable when any xed policy  is guiding the exploration. Such bounds must, of course, depend on properties of . Due to space limitations, we can only outline the main ideas; the formal statements and proofs are deferred to a longer version of the paper.

Let us assume for simplicity that  which may be a stochastic policy de nes an

ergodic Markov process in the MDP M. Thus,  induces a unique stationary distri-

bexueticountinPgMa;ctii;oan

over state-action pairs | a from state i during an

iinntuniittieverlayn,dPoMm;wia;lak

is in

the M

frequency according

of to

. Furthermore, we can introduce the standard notion of the mixing time of  to

witthsiallsttbatehtieovndeairsrytyrcidbliousstteri"oibntuoitniPodnMuc;|ed1i.nofnFoirsnmtaaaltlleyl-,ya,lcettthioiusnsisdpetahirensenbuymTb=e-rmstTienpio;wafasfltkPespMsa;crceioq;ruadirigne.dg

such to 

Armed with these notions, it is not di cult to show that the number of steps we must take under  in order to simulate, with high probability, a call to the oracle PSM, iiTsndpesoptleyepnnsdo,menwitealdoirbnatwtahi,newqaeunahnaatviltemyoaTsttl=ienads.tepTpehrnoedbeiannbttiu"liittdyioranwiosffrsotdrmraaigwPhMitnf;gorawi;naayrd;p:aaarnttdimcuwoliasttrheevia;ecrahy pair. Once we have sampled every i; a pair, we have simulated a call to PSM. The formalization of these intuitions leads to a version of Theorem 1 applicable to any , in which the bound is multiplied by a factor polynomial in T= , as desired.

However, a better would occur when

ressuilmt pislypodsoseisblne.otInevcearseesxewchueteresomme aacytiboensfmroamll

or even 0 which some state, the

ittfcwdnaohaeecgsoltete.smohtseir,IeunngisgTtoitvMpiaess=tnlnibiymgeihntaittil;sle,Tyaprlhaoaterlofliogtocedreryreewmotirnhhnb1ieeicMyn,htamhlcPngeouioMntslretsu;iittbprafh-oulnimMi;rcedatdsDbiootobPuontyr,hMhaabaQ,novfw-deualcenhawtaden,orersrnweiminbphnieoiagtccliyhoyaamlnnn0isopodeimhwoswbatiahstaoeeaelbpaiktiatninahnreioaaddnTrmtirfcverde=ooactenetmct.rvueaceooMptruTfsgpsoeerb.acunoynhrcaIdnesncciihhcmersaolauliplotimclkelsyhyes--,

inates small-probability state-action pairs, but this is a minor detail. By allowing

to become smaller as the amount of experience we receive from  grows, we can

oabs tai!n a0n. anytime" result, since the sub-MDP M  approaches the full MDP M

References
1 Jaakkola, T., Jordan, M. I., Singh, S. On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 66, 1185 1201, 1994.
2 C. J. C. H. Watkins. Learning from Delayed Rewards. Ph.D. thesis, Cambridge University, 1989.
3 R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
4 S. Mahadevan. Enhancing Transfer in Reinforcement Learning by Building Stochastic Models of Robot Actions. In Machine Learning: Proceedings of the Ninth International Conference, 1992.

1Formally, the degree of closeness is measured by the distance between the transient and stationary distributions. For brevity here we will simply assume this parameter is set to a very small, constant value.

