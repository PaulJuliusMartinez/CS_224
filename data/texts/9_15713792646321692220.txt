Ten Ways to Waste a Parallel Computer
Kathy Yelick
NERSC Director, Lawrence Berkeley National Laboratory
EECS Department, UC Berkleey
Berkeley ParLab

Moore's Law is Alive and Well

1.E+07

1.E+06 1.E+05

Transistors (in Thousands)

1.E+04

1.E+03

1.E+02

1.E+01

1.E+00

1.E-01 1970

1975

1980

1985

1990

1995

2000

Berkeley ParLab

Data from Kunle Olukotun, Lance Hammond, Herb Sutter, Burton Smith, Chris Batten, and Krste Asanoviç
2

2005

2010

But Clock Frequency Scaling Has Been Replaced by Scaling Cores / Chip

1.E+07

1.E+06 1.E+05 1.E+04

Transistors (in Thousands) Frequency (MHz) Cores

1.E+03

1.E+02

1.E+01

1.E+00

1.E-01 1970

1975

1980

1985

1990

1995

2000

Berkeley ParLab

Data from Kunle Olukotun, Lance Hammond, Herb Sutter, Burton Smith, Chris Batten, and Krste Asanoviç
3

2005

2010

Performance Has Also Slowed, Along with Power (the Root Cause of All This)

1.E+07

1.E+06 1.E+05 1.E+04 1.E+03

Transistors (in Thousands) Frequency (MHz) Power (W) Perf Cores

1.E+02

1.E+01

1.E+00

1.E-01 1970

1975

1980

1985

1990

1995

2000

Berkeley ParLab

Data from Kunle Olukotun, Lance Hammond, Herb Sutter, Burton Smith, Chris Batten, and Krste Asanoviç
4

2005

2010

This has Also Impacted HPC System Concurrency
Sum of the # of cores in top 15 systems (from top500.org)

Exponential wave of increasing concurrency for forseeable future! 1M cores sooner than you think!

Berkeley ParLab

5

New World Order
· Goal: performance through parallelism · Power is overriding hardware concern:
­ Power density limits clock speed ­ Handheld devices limited by battery life ­ HPC systems may be >100 MW in 10 years
· Performance is now a software concern

· How can we lose performance and therefore lose the case for parallelism?

Berkeley ParLab

6

#1: Build Systems with Insufficient Memory Bandwidth

· Memory Bandwidth Starvation
"Multicore puts us on the wrong side of the memory wall. Will CMP ultimately be asphyxiated by the memory wall?" --Thomas Sterling
· Simple double buffering model uses only
­ Time to fill up all on-chip memory
memory size / bandwidth
­ Time to compute on all on-chip data
memory size bytes * algorithmic intensity / ops-per-sec

DRAM

Processor Chip

Nothing new, except that ops-per-sec on a chip (aggregate) is still going up

Berkeley ParLab

#1: Build Systems with Insufficient Memory Bandwidth
· Under some assumptions about scaling over time · Can determine for a given algorithm class
(constants matter!) when you are bandwidth-limited
Technology to solve this problem if there is market pressure
Berkeley ParLab

#2: Don't Take Advantage of Hardware Performance Features

Nearest-neighbor 7point stencil on a 3D array

Use Autotuning·z+1 !

·+ Z

·y -1

Write

code

generators and·x+1 ·x ,y,z

let

·x -1

·+Y

computers

do

tuning·y+1 ·z -1

·+ X

3D Grid

·7 -point nearest neightbors

Berkeley ParLab

9

#3: Ignore Little's Law
Little's Law: required concurrency = bandwidth * latency
#outstanding_memory_fetches = bandwidth* latency
NERSC application benchmarks Shalf et al
· Experiment: Running on a fixed number of cores · 1 core per socket vs 2 cores per socket
· Only 10% performance drop from sharing (halving) bandwidth
Berkeley ParLab

7 Point Stencil Revisited

· Cell and GTX280 are notable for both performance and energy efficiency

Berkeley ParLab

Joint work with Kaushik Datta, Jonathan Carter,
Shoaib Kamil, Lenny Oliker, John Shalf, and Sam
Williams
11

Why is the STI Cell So Efficient?

· Unit stride access is as important as cache utilization on processors that rely on hardware prefetch
­ Tiling in unit stride direction is counter-productive: improves reuse, but kills prefetch effectiveness
· Software controlled memory gives programmers more control
­ Spend bandwidth on what you use; bulk moves (DMA) hide latency

Berkeley ParLab

Joint work with Shoaib Kamil, Lenny Oliker, John
Shalf, Kaushik Datta

#4: Turn Functional Problems into Performance Problems
· Fault resilience introduces inhomogeneity in execution rates (error correction is not instantaneous)

Berkeley ParLab

Slide source: John Shalf

#5: Over Synchronize Applications
Computations as DAGs
View parallel executions as the directed acyclic graph of the computation

Berkeley ParLab

Slide source: Jack Dongarra
14

Berkeley ParLab

Slide source: Jack Dongarra 15

DAG Scheduling Outperforms Bulk-Synchronous Style
PLASMA on shared memory UPC on partitioned memory

· UPC LU factorization code adds cooperative (nonpreemptive) threads for latency hiding
­ New problem in partitioned memory: allocator deadlock
­ Can run on of memory locally due tounlucky execution order

Berkeley ParLab

PLASMA by Dongarra et al; UPC LU joint with

Parray Husbands

16

#6: Over Synchronize Communication
· Use a programming model in which you can't utilize bandwidth or "low" latency

Berkeley ParLab

Joint work with Berkeley UPC Group

Sharing and Communication Models: Two-sided vs One-sided Communication

·two-sided message

·message id

·d ata payload

·one-sided put message

·address

·data payload

·network · interface

·h ost ·C PU

·m emory

· Two-sided message passing (e.g., MPI) requires matching a send with a receive to identify memory address to put data
­ Wildly popular in HPC, but cumbersome in some applications ­ Couples data transfer with synchronization
· Using global address space decouples synchronization
­ Pay for what you need! ­ Note: Global Addressing  Cache Coherent Shared memory

Berkeley ParLab

Joint work with Dan Bonachea, Paul Hargrove,
Rajesh Nishtala and rest of UPC group

3D FFT on BlueGene/P

Berkeley ParLab

Joint work with Rajesh Nishtala, Dan Bonachea,
Paul Hargrove,and rest of UPC group

#7: Run Bad Algorithms
·Algorithmic gains in last decade have far outstripped Moore's Law
­A daptive meshes rather than uniform
­S parse matrices rather than dense
­R eformulation of problem back to basics
·Example of canonical "Poisson" problem on n points:
­Dense LU: most general, but O(n3) flops on O(n2) data ­Multigrid: fastest/smallest, O(n) flops on O(n) data
Performance results: John Bell et al
Berkeley ParLab

#8: Don't Rethink Your Algorithms

· Consider Sparse Iterative Methods
· Nearest neighbor communication on a mesh
· Dominated by time to read matrix (edges) from DRAM
· And (small) communication and global synchronization events at each step
­ Can we lower data movement costs?
· Take k steps "at once" with one matrix read from DRAM and one communication phase ­ Parallel implementation
· O(log p) messages vs. O(k log p)
­ Serial implementation
· O(1) moves of data moves vs. O(k)
· Performance of Akx operation relative to Ax and upper boun
­ Runs up to 5x faster on SMP

Berkeley ParLab

Joint work with Jim Demmel, Mark Hoemman, Marghoob
Mohiyuddin

But the Numerics have to Change! Need to collaborate

Berkeley ParLab

Work by Jim Demmel and Mark Hoemman

#9: Choose "Hard" Applications 
Examples of such systems include
· Elliptic: steady state, global space dependence · Hyperbolic: time dependent, local space dependence · Parabolic: time dependent, global space dependence Global vs Local Dependence
­ Global means either a lot of communication, or tiny time steps: hard to scale well in parallel
­ Local limits communication, e.g., nearest neighbor Global dependencies are inherent in some problems
­ E.g., incompressible fluids like blood flow (games and medicine), ocean dynamics (climate), ...

Berkeley ParLab

02/11/2009

#10: Use Heavy-Weight Cores Optimized for Serial Performance

TensilicaDP

PPC450

Intel Core2

Power 5

· Power5 (Server)
­ 389 mm2 ­ 120 W @ 1900 MHz
· Intel Core2 sc (Laptop)
­ 130 mm2 ­ 15 W @ 1000 MHz
· PowerPC450 (BlueGene/P)
­ 8 mm2 ­ 3 W @ 850 MHz
· Tensilica DP (cell phones)
­ 0.8 mm2 ­ 0.09 W @ 650 MHz

·E ach core operates at 1/3 to 1/10th efficiency of largest chip, but you ·c an pack 100x more cores onto a chip and consume 1/20 the power!

Berkeley ParLab

John Shalf and the rest of the Green Flash team

Green Flash Summary

· We propose a new approach to scientific computing that enables transformational changes for science
­ Choose the science target first (climate in this case) ­ Design systems for applications (rather than the reverse)
­ Design hardware, software, algorithms together using hardware emulation (RAMP) and auto-tuning

·General Purpose ·A pplication Driven ·Special Purpose ·S ingle Purpose

·Cray XT3

·B lueGene ·D esign for ·Climate

·D.E. Shaw ·A nton

·MD Grape

Berkeley ParLab

John Shalf and the rest of the Green Flash team

A Short List of x86 Opcodes that Science Applications Don't Need!

Berkeley ParLab

John Shalf and the rest of the Green Flash team

More Wasted Opcodes
·W e only need 80 out of the nearly 300 ASM instructions in the x86 instruction set! ·Still have all of the 8087 and 8088 instructions! ·W ide SIMD Doesn't Make Sense with Small Cores ·Neither does Cache Coherence ·N either does HW Divide or Sqrt for loops
·Creates pipeline bubbles ·B etter to unroll it across the loops (like IBM MASS libraries) ·M ove TLB to memory interface because its still too huge (but still get precise exceptions from segmented protection on each core)
Berkeley ParLab

Green Flash Strawman
System Design In 2008
We examined three different approaches: · AMD Opteron: Commodity approach, lower efficiency for
scientific applications offset by cost efficiencies of mass market · BlueGene: Generic embedded processor core and customize
system-on-chip (SoC) services to improve power efficiency for scientific applications · Tensilica XTensa: Customized embedded CPU w/SoC provides further power efficiency benefits but maintains programmability

Processor
AMD Opteron IBM BG/P Green Flash / Tensilica XTensa

Clock
2.8GHz 850MHz 650MHz

Peak/ Cores/ Sockets Cores

Core

Socket

(Gflops)

5.6 2

890K 1.7M

3.4 4

740K 3.0M

2.7 32 120K 4.0M

Power

Cost 2008

179 MW 20 MW 3 MW

$1B+ $1B+ $75M

Berkeley ParLab

John Shalf and the rest of the Green Flash team

Ten Sources of Waste in Parallel Computing

1) Insufficient memory bandwidth (HW) 2) Ignore performance features (SW+HW) 3) Ignore Little's Law (SW+HW) 4) Hide faults in low level (SW+HW) 5) Over synchronization globally (SW) 6) Over synchronize communication (SW) 7) Choose bad algorithms (Alg) 8) Don't rethink algorithms (Alg) 9) Choose "hard" applications (Apps) 10) Use overly-general processors (HW)

Berkeley ParLab

29

Conclusions

· Enable programmers to get performance
­ Expose features for performance ­ Don't hide them
· Go Green
­ Enable energy-efficient computers and software
· Work with experts on software, algorithms, applications

Berkeley ParLab

30

