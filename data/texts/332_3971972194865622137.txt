Prior Knowledge in Support Vector Kernels

Bernhard Scho¨lkopf Max­Planck­Institut fu¨r biologische Kybernetik
Tu¨bingen, Germany

Patrice Simard, Vladimir Vapnik AT&T Research
101 Crawfords Corner Rd. Holmdel, NJ, USA

Alexander J. Smola GMD FIRST
Rudower Chaussee 5 Berlin, Germany

Abstract
We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.

1 INTRODUCTION
When we are trying to extract regularities from data, we often have additional knowledge about functions that we estimate. For instance, in image classification tasks, there exist transformations which leave class membership invariant (e.g. local translations); moreover, it is usually the case that images have a local structure in that not all correlations between image regions carry equal amounts of information. The present study investigates the question how to make use of these two sources of knowledge by designing appropriate Support Vector (SV) kernel functions. To this end, we start by giving a brief introduction to SV machines, following Vapnik & Chervonenkis (1979) and Vapnik (1995) (Sec. 2). Regarding prior knowledge about invariances, we develop an idea of Vapnik (1995) and present a method to design kernel functions which lead to invariant classification hyperplanes (Sec. 3). The method is applicable to invariances under the action of differentiable local 1­parameter groups of local transformations, e.g. translational invariance in pattern recognition. In Sec. 4, we describe kernels which take into account image locality by using localized receptive fields. Sec. 5 presents experimental results on both types of kernels, followed by a discussion (Sec. 6).

2 OPTIMAL MARGIN HYPERPLANES

f bFor linear hyperplane decision functions (x) = sgn ((w x) + ), the VC­dimension

can be controlled by controlling the norm of the weight vector w. Given training data

y : : : y y(x1 1)

(x` `) xi 2 RN i 2 f 1g a separating hyperplane which generalizes

well can be found by minimizing

y b i : : : `1 kwk2 subject to
2

i ((xi w) + )

1 for = 1

(1)

the latter being the conditions for separating the training data with a margin. Nonseparable

cases are dealt with by introducing slack variables (Cortes & Vapnik 1995), but we shall

omit this modification to simplify the exposition. All of the following also applies for the

nonseparable case.

To solve the above convex optimization problem, one introduces a Lagrangian with multi-

pliers

i and derives the dual form of the optimization problem: maximize

``

`

X X y y X y :i=1

1 i ; 2 i k=1

i i k k(xi

xk)

subject to

i

0 i i=0 i=1

(2)

PIt turns out that the solution vector has an expansion in terms of training examples, w =

y`
beci=om1 e

i ixi, where nonzero; the

only those i corresponding to constraints (1) which are met respective examples xi are called Support Vectors. Substituting

can this

X !expansion for w yields the decision function

`

f (x) = sgn

y b :i i(x xi) +

(3)

i=1

It can be shown that minimizing (2) corresponds to minimizing an upper bound on the VC

dimension of separating hyperplanes, or, equivalently, to maximizing the separation margin

between the two classes. In the next section, we shall depart from this and modify the dot

product used such that the minimization of (2) corresponds to enforcing transformation

invariance, while at the same time the constraints (1) still hold.

3 INVARIANT HYPERPLANES

Invariance by a self­consistency argument. We face the following problem: to express

the condition of invariance of the decision function, we already need to know its coeffi-

cients which are found only during the optimization, which in turn should already take into

account the desired invariances. As a way out of this circle, we use the following ansatz:

consider decision functions f = (sgn g), where g is defined as

`
g X y B B b(xj) :=

i i( xj

xi) +

(4)

i=1

with a matrix B to be determined below. This follows Vapnik (1995), who suggested to

incorporate invariances by modifying the dot product used. Any nonsingular B defines a

dot product,
A = B>B.

which

can

equivalently

be

written

as

(xj

Axi), with a positive definite matrix

Clearly, invariance of g under local the local invariance of f , which is

transformations of all xj what we are aiming for.

is a sufficient condition for Strictly speaking, however,

invariance of g is not necessary at points which are not Support Vectors, since these lie in

a region where (sgn g) is constant -- however, before training, it is hard to predict which

examples will turn out to become SVs. In the Virtual SV method (Scho¨lkopf, Burges, &

Vapnik, 1996), a first run of the standard SV algorithm is carried out to obtain an initial SV

set; similar heuristics could be applied in the present case.

Local invariance of g for each pattern xj under transformations of a differentiable local

1­parameter group of local transformations Lt,

@ g(Ltxj ) = 0 @t t=0

(5)

can be approximately enforced by minimizing the regularizer

X`
@1 ` @t g :j=1

2
t=0 (Ltxj )

(6)

Note that the sum may run over labelled as well as unlabelled data, so in principle one could also require the decision function to be invariant with respect to transformations of elements of a test set. Moreover, we could use different transformations for different patterns.

X ! XFor (4), the local invariance term (5) becomes

`

@@t y B B bt=0

i i( Ltxj
i=1

xi) +

` =
i=1

y @ Bi i 1( L0xj

Bxi)

B

@ @t

t=0 Lt xj

(7)

@ B Busing the chain rule. Here, 1( L0xj xi) denotes the gradient of (x y) with respect to

B Bx, evaluated I @that L0 =

at the point (x and 1(x y) =

yy>),=yie(ldLs t0hxej

xi). Substituting regularizer

(7)

into

(6),

using

the

facts

!` X X @ X1
` y B B @t y y B QBj=1

>`
i i( xi)
i=1

t=0 Lt xj

2` = i k=1

i i k k(

xi

xk) (8)

where

X @1 `
Q B:= ` @tj=1

t=0 Lt xj

@@t t=0Ltxj > B>:

(9)

We now choose Q such that (8) reduces to the quadratic term in the standard SV optimiza-

(BtiBo>nxBiprBoobxr,lke)bm:yA(r2ses)quuumitriiilnnizggintBhgattthotheebdexoint soppnraosnidnutghcuetlwacrhh,oois.leee.nstpihnaact(e4n,)ot,hiii.snefc.oosrnumdcaihttiiotohnnabtge(ecBtolxmoisets

QBB>QxkB)

= =

during the

preprocessing, Q = I. This can be satisfied by a preprocessing matrix

B

=

C;

1 2

(10)

the nonnegative square root of the inverse of the nonnegative matrix

@0 X A1C :=

`
@1 ` @tj=1

t=0 Lt xj

>
@@t t=0Ltxj

:

(11)

In practice, we use a matrix

C C I:= (1 ; ) +

(12)

0 < 1 instead of C. As C is nonnegative, C is invertible. For = 1, we recover the

standard SV optimal hyperplane algorithm, other values of determine the trade­off be-

P Ctween invariance and model complexity control. It can be shown that using corresponds

to using an objective function (w) = (1 ; )

i (w

@ @t

jt=0 Lt

xi

)2

+

kwk2.

By choosing the preprocessing matrix B according to (10), we have obtained a formulation

of the problem where the standard SV quadratic optimization technique does in effect min-

imize the tangent regularizer (6): the maximum of (2), using the modified dot product as in

(4), coincides with the minimum
where g is defined as in (4).

of

(6)

subject

to

the

separation

conditions

yi

g(xi)

1,

Note (xj

that preprocessing with B does B>Bxi), we can precompute B

not affect classification
>Bxi for all SVs xi and

B Bspeed: since ( xj xi) =
thus obtain a machine (with

modified SVs) which is as fast as a standard SV machine (cf. (4)).

The nonlinear case. To construct nonlinear SV machines, kernel functions k(x y) are

substituted for every occurence of a dot product (in (2) and (3)), corresponding to a dot

product in some feature space which is nonlinearly related to input space (Boser, Guyon,

& Vapnik, 1992). In that case, the above analysis leads to the regularizer

!` X X @1
` y @ k B B B @t :j=1

`
i i 1 ( xj
i=1

xi )

t=0Ltxj

2

(13)

The derivative
@1k(x y) = d

of (x

ky)mdu;s1t

bye>e:vTaoluoabtetadinfoarkseprneceilfi­scpkeecrifineclcs,ones.gtr.aifnotr

k(x y) = (x y)d, on the matrix B, one

Xhas to equate the result with the quadratic term in the nonlinear objective function,

yi i kykk(Bxi Bxk):

(14)

ik

Relationship to Principal Component Analysis (PCA). Let us presently return to the

linear case, and provide some interpretation of (10) and (11). If we sum over derivatives

odiannfiadbtghoaoetnhdacildaoigizvroeaednrci,atailCnomcnesa=,tmrtihSxaetDrDvixSecc>ootof,nrtwtsahiienthirn@@aagtnnjtdt=hooe0rmtLhEotivxggejeocnnthovaaarlvlumeezas@@et.trrjiTotx=hmS0eLnecatwonxne.ascniBsadtneinCicngogimospfpaoCussti'eatsimvBEepi,l=geCeneCsvcte;aicmn21toab=rtees

SD;

1 2

S

>,

with

D;

1 2

being diagonal and

nonnegative.

Since

the dot

product is

invariant

under orthogonal transformations, we may drop the leading S and (4) becomes

X`
g(xj ) =

yi i

(D;

1 2

S

>xj

D;

1 2

S

>xi

)

+

b:

(15)

i=1

A given pattern x tangent covariance

is thus matrix

Cfir,swt htriacnhsafroermtheedrobwysporfoSje>ct.inTgheitroesnutoltitnhgefEeaigtuernevveecctotorsr

of the is then

rescaled by dividing by the square roots of C's Eigenvalues.1 In other words, the directions

of main variance of the random vector put on features which are less variant

u@@nt jdte=r0

Lt x Lt.

are scaled back, thus more emphasis is For example, in image analysis, if Lt

represent the group of translations, more emphasis is put on the relative proportions of ink

in the image rather than the positions of lines. The PCA interpretation of our preprocessing

matrix suggests the possibility to regularize and reduce dimensionality by discarding part

of the features, as it is common usage when doing PCA.

Going nonlinear by using nonlinear PCA. We are now in a position to describe the way

how to generalize to the nonlinear case. To this end, we use kernel principal component

analysis (Scho¨lkopf, Smola, & Mu¨ller, 1996). This technique allows us to compute prin-
cipal components in a space F nonlinearly related to input space, just as nonlinear SV machines construct decision rules in F . The kernel function k plays the role of the dot F kproduct in , i.e. (x y) = ( (x) (y)). Kernel PCA consists in diagonalizing the maktrix ( (xi xj))ij in order to obtain the Eigenvalues of the covariance matrix of the images F Fof the data in and an implicit expression of its Eigenvectors: since they live in , the Eigenvectors cannot in general be given explicitely (F may be very high­ or even infinite­

dimensional). However, kernel PCA finds coefficients

k i

such

that

`

Xk
ki

(xi

x)

(16)

i=1

1As an aside, note that our goal to build invariant SV machines has thus serendipitously provided us with an approach for an open problem in SV learning, namely the one of scaling: in SV machines, there has so far been no way of automatically assigning different weight to different directions in input space: in a trained SV machine, the weights of the first layer form a subset of the training set. Choosing these Support Vectors from the training set only gives rather limited possibilities for appropriately dealing with different scales in different directions of input space.

is the projection of x onto the kth Eigenvector of the covariance matrix in F . To generalize (15) to the nonlinear case, we thus need to compute the tangent covariance matrix C (Eq. 11) in F and its projection onto the subspace of F given by the linear span of the images of
our data. Here the considerations of the linear case apply. Without going into further detail
on this issue, we state that the whole procedure reduces to computing dot products in F , which can be done using k, without explicitly mapping into F .

4 KERNELS USING LOCAL CORRELATIONS

kBy using a kernel (x y) = (x y)d, one implicitly constructs a decision boundary in the space of all possible products of d pixels. This may not be desirable, since in natural

images, correlations over short distances are much more reliable features than long­range

kcorrelations. To take this into account, we define a kernel

d1 p

d2

as

follows:

1. compute a third image z, defined as the pixel­wise product of x and y

2. sample z with a pyramidal receptive field of diameter p, centered at all locations (i j), to obtain the values zij
3. raise each zij to the power d1, to take into account local correlations within the
range of the pyramid

4.

dssuommezdiljo1 nogvee­rrathnegewchoorlreeliamtiaognes,

and raise the result to the power 2 to between the outputs of the pyramidal

allow for receptive

fields

The resulting
tions of d1d2

kernel pixels.

will

be

of

order

d1d2,

however,

it

will

not

contain

all

possible

correla-

5 EXPERIMENTAL RESULTS
In the experiments, we used a subset of the MNIST data base of handwritten characters (Bottou at al., 1994), consisting of 5000 training examples and 10000 test examples at a
:resolution of 20x20 pixels, with entries in ;1 1]. Using a linear SV machine (i.e. a sep-
arating hyperplane), we obtain a test error rate of 9 8% (training 10 binary classifiers, and
using the maximum value of g (Eq. 4) for 10­class classification); by using a polynomial kernel of degree 4, this drops to :4 0%. In all of the following experiments, we used degree
4 kernels of various types. The number 4 was chosen as it can be written as a product of
two integers, thus we could compare results to a kernel kp with d1 = d2 = 2. For the
considered classification task, results for higher polynomial degrees are very similar.
kIn a series of experiments with a homogeneous polynomial kernel (x y) = (x y)4, using preprocessing with Gaussian smoothing kernels of standard deviation 0:1 0:2 : : : 1:0, we obtained error rates which gradually increased from :4 0% to :4 3%; thus no improvement
of this performance was possible by a simple smoothing operation. Applying the Virtual SV method (retraining the SV machine on translated SVs; Scho¨lkopf, Burges, & Vapnik,
1996) to this problem results in an improved error rate of :2 8%.
Invariant hyperplanes. Table 1 reports results obtained by preprocessing all patterns with
B (cf. (10)), choosing different values of (cf. Eq. (12)). In the experiments, the patterns were first rescaled to have entries in 0 1], then B was computed, using horizontal and
vertical translations, and preprocessing was carried out; finally, the resulting patterns were scaled back again. This was done to ensure that patterns and derivatives lie in comparable regions of RN (note that if the pattern background level is a constant ;1, then its derivative is 0). The results show that even though (11) was derived for the linear case, it can lead to improvements in the nonlinear case (here, for a degree 4 polynomial), too.

kTable 1: Classification error rates for modifying the kernel (x y) = (x y)4 with the

invariant hyperplane preprocessing matrix B

=

C

;

1 2

;

cf.

Eqs.

(10)

­

(12).

Enforcing

invariance with 0:1 < < 1 leads to improvements over the original performance ( = 1).

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

error rate in % 4.2 3.8 3.6 3.6 3.7 3.8 3.8 3.9 3.9 4.0

Dimensionality reduction. The above 0 1] scaling operation is affine rather than linear, hence the argument leading to Eq. 15 does not hold for this case. We thus only report results on dimensionality reduction for the case where the data is kept in 0 1] scaling from the very beginning on. Dropping principal components which are less important leads to substantial improvements (Table 2); cf. the explanation following Eq. (15)).

Table 2: Dropping directions corresponding to small Eigenvalues of C (cf. (15)) leads to substantial improvements. All results given are for the case := 0 4 (cf. Table 1); degree 4

homogeneous polynomial kernel.

principal components discarded 0 50 100 150 200 250 300 350

error rate in %

8.7 5.4 4.9 4.4 4.2 3.9 3.7 3.9

Kernels using local correlations. To exploit locality in images, we used pyramidal re-

k p d dceptive field kernel

d1 p

d2

with

diameter

= 9 (cf. Sec. 4). For 1 = 2 = 2, we ob-

:tained an improved error rate of 3 1%, another degree 4 kernel with only local correlations

d d : :( 1 = 4 2 = 1) led to 3 4%. Albeit significantly better than the 4 0% for the degree 4

homogeneous polynomial (the error rates on the 10000 element test set have an accuracy

of about :0 1%, cf. Bottou at al., 1994), this is still worse than the Virtual SV result of :2 8%.

As the two methods, however, exploit different types of prior knowledge, it could be ex-

pected that combining them leads to still better performance; and indeed, this yielded the
best performance of all :(2 0%).

For the purpose of benchmarking, we also ran our system on the US postal service database

:of 7300+2000 handwritten digits at a resolution of 16 16. In that case, we obtained the
following test error rates: SV with degree 4 polynomial kernel 4 2%, Virtual SV (same ker-

:nel) 3 5%,
to almost

aSlVl kwnoitwh nk72re2s3u.l6ts%o,nVitrhtautaldSaVta

k :with
base,

72an2d3

0%. The is second

latter only

compares favourably to a memory­based

tangent­distance nearest neighbour classifier at :2 6% (Simard, LeCun, & Denker, 1993).

6 DISCUSSION
With its rather general class of admissible kernel functions, the SV algorithm provides ample possibilities for constructing task­specific kernels. We have considered an image classification task and used two forms of domain knowledge: first, pattern classes were required to be locally translationally invariant, and second, local correlations in the images were assumed to be more reliable than long­range correlations. The second requirement can be seen as a more general form of prior knowledge -- it can be thought of as arising partially from the fact that patterns possess a whole variety of transformations; in object recognition, for instance, we have object rotations and deformations. Typically, these transformations are continuous, which implies that local relationships in an image are fairly stable, whereas global relationships are less reliable. We have incorporated both types of domain knowledge into the SV algorithm by construct-

ing appropriate kernel functions, leading to substantial improvements on the considered pattern recognition tasks. Our method for constructing kernels for transformation invariant SV machines, put forward to deal with the first type of domain knowledge, so far has only been applied in the linear case, which partially explains why it only led to moderate improvements (also, we so far only used translational invariance). It is applicable for differentiable transformations -- other types, e.g. for mirror symmetry, have to be dealt with using other techniques (e.g. Scho¨lkopf, Burges, & Vapnik, 1996). Its main advantages compared to the latter technique is that it does not slow down testing speed, and that using more invariances leaves training time almost unchanged. The proposed kernels respecting locality in images led to large improvements; they are applicable not only in image classification but in all cases where the relative importance of subsets of products features can be specified appropriately. They do, however, slow down both training and testing by a constant factor which depends on the specific kernel used. Both described techniques should be directly applicable to other kernel­based methods as SV regression (Vapnik, 1995) and kernel PCA (Scho¨lkopf, Smola, & Mu¨ller, 1996). Future work will include the nonlinear case (cf. our remarks in Sec. 3), the incorporation of invariances other than translation, and the construction of kernels incorporating local feature extractors (e.g. edge detectors) different from the pyramids described in Sec. 4. Acknowledgements. We thank Chris Burges and Le´on Bottou for parts of the code and for helpful discussions.
References
B. E. Boser, I .M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144­152, Pittsburgh, PA, 1992. ACM Press.
L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. D. Jackel, Y. LeCun, U. A. Mu¨ller, E. Sa¨ckinger, P. Simard, and V. Vapnik. Comparison of classifier methods: a case study in handwritten digit recognition. In Proceedings of the 12th International Conference on Pattern Recognition and Neural Networks, Jerusalem, 1994.
C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273 ­ 297, 1995. B. Scho¨lkopf, C. Burges, and V. Vapnik. Incorporating invariances in support vector learn-
ing machines. In C. von der Malsburg, W. von Seelen, J. C. Vorbru¨ggen, and B. Sendhoff, editors, Artificial Neural Networks -- ICANN'96, pages 47 ­ 52, Berlin, 1996. Springer Lecture Notes in Computer Science, Vol. 1112. B. Scho¨lkopf, A. J. Smola, and K. R. Mu¨ller. Nonlinear component analysis as a kernel eigenvalue problem. Technical Report 44, MPI fu¨r biologische Kybernetik, 1996. submitted to Neural Computation. P. Simard, Y. Le Cun, and J. Denker. Efficient pattern recognition using a new transformation distance. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing Systems 5. Proceedings of the 1992 Conference, pages 50­58, San Mateo, CA, 1993. Morgan Kaufmann. P. Simard, B. Victorri, Y. Le Cun, and J. Denker. Tangent prop -- a formalism for specifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, Advances in Neural Information Processing Systems 4, San Mateo, CA, 1992. Morgan Kaufmann. V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995. V. Vapnik and A. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, Moscow, 1974. (German Translation: W. Wapnik & A. Tscherwonenkis, Theorie der Zeichenerkennung, Akademie­Verlag, Berlin, 1979).

