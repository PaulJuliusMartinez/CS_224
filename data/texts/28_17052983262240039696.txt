On the Convergence of Leveraging£

Gunnar Ra¨tschÝ, Sebastian MikaÞ and Manfred K. WarmuthÜ Ý RSISEÞÜ,FUArnauuivsnterhraoslifitaeynr oFNfIaRCtSiaolTinf,aoKlrnUekinauialve´tesSrtsrai.nt7yt,,aC1C2ar4nu8bz9e, rCBraAe,rAl9inC5,0TG600e2,rmU00aSnAAyustralia
raetsch@csl.anu.edu.au, mika@first.fhg.de, manfred@cse.ucsc.edu

Abstract
We give an unified convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes ½-norm regularized cost functions leading to a clean and general way to regularize ensemble learning.

1 Introduction

We show convergence rates of ensemble learning methods such as AdaBoost [10], Logistic

Regression (LR) [11, 5] and the Least-Square (LS) regression algorithm called LS-Boost

[12]. These algorithms have in common that they iteratively call a base learning algorithm
Ä (also called weak learner) on a weighted training sample. The base learner is expected
Àto return in each iteration Ø a hypothesis Ø from some hypothesis set of weak hypotheses that has small weighted training error. This is the weighted number of false predictions

liunisneecsalarthlsyseicfisoicgmantbiooinnfead«ntdofowfroeprimrgehdtthiecedtifioensnat.ilmThhaytepioohntyhpeeorsrtiohsresi«ins´ÜcreoµgerfefiscÈsiieoØnn«t.«ØTØØhi´seÜsdµee; thienyrmpcoliatnhsesedisfieacstaiattireoernatthoioennne Ø, such that a certain objective is minimized or approximately minimized, and is fixed for

later iterations. Here we will work out sufficient conditions on the base learning algorithm

to achieve linear convergence to the minimum of an associated loss function . This means
Çthat for any starting condition the minimum can be reached with precision ¯ ¼ in only
¯´ÐÓ ´½ µµ iterations.

Relation to Previous Work In the original work on AdaBoost it has been shown that

the optimization objective (which is an upper bound on the training error) converges ex-

ponentially fast to zero, if the base learner is consistently better than random guessing, i.e.

its weighted training error ¯ is always smaller than some constant  with 

½ ¾

.

In

this

case the convergence is known to be linear (i.e. exponentially decreasing) [10]. One can

easily show that this is the case when the data is separable:1 If the data is not separable, the

£Supported by DFG grants MU 987/1-1, JA 379/9-1 and NSF grant CCR 9821087; we gratefully

acknowledge help from B. Borchers, P. Spellucci, R. Israel and S. Lemm. This work has been done,

while G. Ra¨tsch was at Fraunhofer FIRST, Berlin.
«1We call the data separable, if there exists such that «´Üµ separates the training examples.

weighted

training

error

¯

cannot

be

upper

bounded

by

a

constant

smaller

½ ¾

,

otherwise

one

could use AdaBoost to find a separation using the aforementioned convergence result.2

For AdaBoost and Logistic Regression it has been shown [5] that they generate a combined

hypothesis asymptotically minimizing a loss functional only depending on the output of
the combined hypothesis «. This holds for the non-separable case; however, the assumed
conditions in [5] on the performance of the base learner are rather strict and can usually

not be satisfied in practice. Although the analysis in [5] holds in principle for any strictly

convex cost function of Legendre-type (e.g. [24], p. 258, and [1]), one needs to show the

existence of a so-called auxiliary function [7, 5] for each cost function other than the expo-

nential or the logistic loss. This can indeed be done [cf. 19, Section 4.2], but in any case

only leads to asymptotical results. In the present work we can also show rates of conver-

gence.

In an earlier attempt to show the convergence of such methods for arbitrary loss functions
[17], one needed to assume that the hypothesis coefficients «Ø are upper bounded by a
rather small constant. For this case it has been shown that the algorithm asymptotically
converges to a combined hypothesis minimizing . However, since the «Ø's need to be
small, the algorithm requires many iterations to achieve this goal.

In [9] it has been shown that for loss functions which are (essentially) exponentially de-

creasing (including the loss functions of AdaBoost and Logistic regression), the loss is
Ç Ô Ç´½ Øµ in the first Ø iterations and afterwards ´ Ø Øµ. This implies linear convergence.

However, this only holds, if the loss reaches zero, i.e. if the data is separable. In our work

we do not need to assume separability.

An equivalent optimization problem for AdaBoost has also been considered in a paper that

predates the formulation of AdaBoost [4]. This optimization problem concerns the likeli-

hood maximization for some exponential family of distributions. In this work convergence

is proven for the general non-separable case, however, only for the exponential loss, i.e. for the case of AdaBoost.3 The framework set up in this paper is more general and we are able

to treat any strictly convex loss function.

In this paper we propose a family of algorithms that are able to generate a combined hy-

pothesis converging to the minimum of  (if it exists), which is a functional depending

on the outputs of the function evaluated on the training set. Special cases are AdaBoost,

Logistic Regression and LS-Boost. While assuming mild conditions on the base learning

algorithm and the loss function
   in the first iteration) of the type

,

we can Ø·½

show

l£inear 

convergence ´ Ø

rat£es µ

[15] for

(beginning some fixed

¾ ¼ ½µ. This means that the difference to the minimum loss converges exponentially

fast to zero (in the number of iterations). A similar convergence has been proven for Ada-

Boost in the special case of separable data [10], although the constant shown in [10] can

be considerable smaller [see also 9]. To prove the convergence of leveraging, we exploit

results of Luo & Tseng [16] for a variant of the Gauss-Southwell method known from nu-

merical optimization.
ÀSince in practice the hypothesis set can be quite large, ensemble learning algorithms

without any regularization often suffer from overfitting [22, 12, 2, 19]. Here, the com-

plexity can only be controlled by the size of the base hypothesis set or by early stopping

after a few iterations. However, it has been shown that shrinkage regularization implied

by penalizing some norm of the hypothesis coefficients is the favorable strategy [6, 12].

We therefore extend our analysis to the case of ½-norm regularized loss functions. With a slight modification this leads to a family of converging algorithms that e.g. includes the

Leveraged Vector Machine [25] and a variant of LASSO [26].

In the following section we briefly review AdaBoost, Logistic Regression, and LS-Boost

and cast them in a common framework. In Sec. 3 we present our main results. After re-

2This can also be seen when analyzing a certain linear program in the dual domain (cf. [23]) 3We will expand on this connection in the full paper (see also [14, 19]).

lating these results to leveraging algorithms, we present an extension to regularized cost functions in Sec. 4 and finally conclude.

2 Leveraging algorithms revisited

We first briefly review some of the most well known leveraging algorithms for classification and regression. For more details see e.g. [10, 11, 12, 8]. We work with Alg. 1 as a template for a generic leveraging algorithm, since these algorithms have the same algorithmical structure. Finally, we will generalize the problem and extend the notation.

AdaBoost & Logistic Regression are designed for classification tasks. In each iteration

¢they call a base learning algorithm on the training set Ë  ½ ·½ (cf. step 3a in Alg. 1). Here a weighting Ø
that is recomputed in each iteration Ø. The base learner

Ø ´Ü½
½

Ý½ØÆµ

is expected

´ÜÒ ÝÒµ
 on the sample is used
to return a hypothesis

ÀØ from some hypothesis space4
has small weighted classification error5 ¯Ø

 ÈÆ Ò½

½ ·½

½ Â that

ØÒ Á´ÝÒ

Ø´ÜÒµµ [10, 11], where

 Á Áand´ØÖÙ µ ½

´ Ð× µ

defined as Ø ½ ¾¯Ø

È¼.ÆIt Ò

is
½

more convenient to work
ØÒ Ø´ÜÒµ. After selecting

with the edge of Ø, which is
the hypothesis, its weight «Ø

is computed such that it minimizes a certain functional (cf. step 3b). For AdaBoost this is

 «´ µ

Æ Ò½

Ò
ÜÔ

ÝÒ

Ó
« Ø´ÜÒµ · Ø ½´ÜÒµ

(1)

and for Logistic Regression it is

ÄÊ «´ µ

 Ý «Æ Ò

Ò

ÐÓ ½

½ · ÜÔ

Ó
 Ò ´ Ø´ÜÒµ · Ø ÜÒ½´ µµ

(2)

ÈwhØe r½e
Ö½

Ø ½ is the «Ö Ö´ÜÒµ.

combined hypothesis of the previous For AdaBoost it has been shown that

iteration given
«Ø minimizing

by (1)

Ø ½´ÜÒµ
can be com-

puted analytically [3]. This is true because we assumed that the hypotheses are binary

valued. Similarly, for LR there exists an analytic solution of (2). The weighting on the

 sample ØÒ·½

is updated
ÝÒ ÜÔ´

ÝbÒaseØ´dÜoÒnµµthaenndewØÒ·c½ombiÝnÒe½d·hÜyÔÜ´pÔ ´o ÝthÒÝeÒØs´iØÜs´ÒÜµÒØµµ´µÜÒfµor

« Ø´ÜÒµ
AdaBoost

· Ø ½´ÜÒµ:
and Logistic

Regression, respectively.

¢ ÀLeast-Square-Boost is an algorithm to solve regression tasks. In this case Ë

ÝÜ´ ½ ½µ

´ÜÒ ÝÒµ

, Ê and

½ Â . It

works in a similar way as AdaBoost and LR. It first selects a hypothesis solving

 ¾ÀØ

½ Ö ÑÒ
¾

Æ Ò½

ÒØ

¾
´ÜÒµ

(3)

and then finds the hypothesis weight «Ø by minimizing the squared error of the new com-

bined hypothesis:

ÄË «´ µ

½ ¾

   Æ
Ò½

ÝÒ

Ø ½´ÜÒµ « Ø´ÜÒµ ¾

(4)

 The "weighting" of the sample is computed as ØÒ·½ ÝÒ Ø´ÜÒµ, which is the residual

of Ø [12]. In a second version of LS-Boost, the base hypothesis and its weight are found

simultaneously by solving [12]:

Ø «Ø

½ Ö ÑÒ

«¾Ê ¾À ¾

   Æ
Ò½

ÝÒ

Ø ½´ÜÒµ « ´ÜÒµ ¾

(5)

Since in (5) one reaches a lower loss function value than with (3) and (4), it might be the favorable strategy.

À4Notice that always contains only a finite number of different hypotheses when evaluated on
the training set and is effectively finite [2].
5Different from common convention, we include the ÝÒ in Ò to make the presentation simpler.

Algorithm 1 ­ A Leveraging algorithm for the loss function .

1. Input: Ë ´Ü½ Ý½µ

´ÜÆ ÝÆ µ , No. of Iterations Ì , Loss function ÊÆ Ê

2. Initialize: ¼ ¼, ½Ò ³´ÝÒ ¼´ÜÒµµ for all Ò ½ Æ

3. Do for Ø ½ Ì ,

(a) Train classifier on Ë Ø and obtain hypothesis Ø

Ê(b) Set «Ø ¾«Ö Ñ Ò

Ø · « Ø

(c) Update Ø·½ Ø · «Ø Ø and ÒØ·½

³

ÝÒ

ÈØ
Ö

½ «Ö

Ö ´ÜÒ µ

4. Output: Ì

The General Case These algorithms can be summarized in Alg. 1 (where case (5) is

   siBnliogohsttly(cdf.eg(1eÈ)n)e,ÒÆra´½teÝd´,Ýc´ÒfÜ. µSµe´cÜ.Ò3ÐµÓ.µ2)a´nf½od·r cshoÜomÔoes´ianpgÝpro´Üparsµiµaµt´efÝloyr

defined functions and ³: plug-

Ü´ µµ

ÝÜÔ´ Ü´ µµ for Ada-

Logistic Regression (cf. (2)) and

 Ý ÝÜ ½
´ ´ µµ ¾ ´

Ü´ µµ¾ for LS-Boost (cf. (4)).

It can easily be verified that the function ³, used for computing the weights , is the deriva-

tive of with respect to the second argument [3, 12].

The Optimization Problem It has been argued in [3, 18, 11, 17] and finally shown in

[5] that AdaBoost and Logistic Regression under certain condition asymptotically con-

verge to a combined hypothesis minimizing the respective loss on the training sam-

¾ À ¾ À ¾ ÀÒplÈe, Âw½he«re

is a linear comÓbination of hypotheses from , i.e. « Ð Ò´ µ « Ê . Thus, they solve the optimization problem:

¾ ÀÑ Ò Ð Ò´ µ

«À«¾Ê Ñ Ò

Â´

µ

(6)

where we defined a matrix À ¾ ÊÆ¢Â with À

´Ü µ.

To avoid confusions, note that hypotheses and coefficients generated during the iterative

algorithm are marked by a hat. In the algorithms discussed so far, the optimization takes

Àplace by employing the leveraging scheme outlined in Alg. 1. The output

rithm is a
With «Ø ÈÂ ½ «Ø

seÈqueÖØ nc½e«oÖfÁ´paÖirs ´«Ø
´Üµ, which is in Ð Ò´

Øµ and a combined hypothesis Ü´ µ
µ, ½ Â, it is easy to verify that
µ (note the missing hat).

oÈÈf sÖØØÖu«c½Öh«aÖÖn´ÜÖa´lµgÜoµ-.

Other Preliminaries Throughout the paper we assume the loss function is of the form

«

´À «µ

ÈÆ Ò½

´ÝÒ

« ´ÜÒ µµ

Although, this assumption is not necessary, the presentation becomes easier. In [7, 5, 19]

a more general case of Legendre-type cost functions is considered. However, note that

additive loss functions are commonly used, if one considers i.i.d.-drawn examples.

We assume that each element ÀÒ and ÝÒ is finite ( ½ Â, Ò ½ Æ ) and À
¡does not contain a zero column. Furthermore, the function ´Ý µ Ê Ê is assumed to ¾be strictly convex for all Ý .

ÀFor simplicity we assume for the rest of the paper that is finite and complementation

¾ À   ¾ Àclosed, i.e. for every

there exists also

. The assumption on the finiteness is

not crucial for classification (cf. footnote 4). For regression problems the hypothesis space

might be infinite. This case has explicitly been analyzed in [20, 19] and goes beyond the

scope of this paper (see also [27]).

3 Main Result
We now state a result known from the field of numerical optimization. Then we show how the reviewed leveraging algorithms fit into this optimization framework.

3.1 Coordinate Descent

The idea of coordinate descent is to iteratively select a coordinate, say the -th, and find

« such that some functional «´ ½ «

«Ì  µ is minimized with respect to « .

There exist several different strategies for selecting the coordinates [e.g. 15]; however, we

are in particular interested in the Gauss-Southwell-type (GS) selection scheme: It selects

¬ Ö «the coordinate that has the largest absolute value in the gradient vector

´ µ, i.e.

Ö Ñ Ü ¼ ½ Â ¬ ¼ . Instead of doing steps in the direction of the negative gradient

as in standard gradient descent methods, one only changes the variable that has the largest

gradient component. This can be efficient, if there are many variables and most of them are

zero at the minimum.

We start with the following general convergence result, which seemed to be fallen into
oblivion even in the optimization community. It will be very useful in the analysis of
leveraging algorithms. Due to a lack of space we omit proofs (see [21, 19]).
Theorem 1 (Convergence of Coordinate Descent [16]). Suppose ÊÆ Ê is twice
Ëcseotnotifnsuooluustiloyndsiffe£rentiÊaÂblteoand strictly convex on ÓÑ . Assume that ÓÑ is open, the

««Ñ¾ÒË ´ µ

« À´ µ · «

(7)

¾ ¾is not empty, where À ÊÆ¢Â is a fixed matrix having no zero column,  ÊÂ fixed

Ëand ÊÂ Ö ¾ Ëthe Hessian

is
¾

a´À(p«os£sµibilsyaupnobsoiutinvdeemd)atbroixx-focor nasltlra«in£ed

se£t..

LFeutrth«erØmobreethaesssuemqeuetnhcaet

generated by coordinate descent, where the coordinate selection ½ ¾ satisfies

   «Ø·½ Ø Ø

«ØØ

¬ «Ñ ½

ÜÂ

Ø·½

«Ø

(8)

¾for some ¬ «´¼ ½, where Ø·½ is the optimal value of «Ø·½ if it would be selected, i.e.

«  «Ø·½

«Ñ¾ËÒ

 À Ø · À ´«

«Ø

¡
µ

·



«

(9)

Then «Ø Ëconverges to an element in £.

The coordinate selection in Thm. 1 is slightly different from the Gauss-Southwell selection

rule described before. We therefore need the following:

Ë Ë ¾ ËProposition
Thm. 1. Let

2

(Convergence of GS be a convex subset of

on

ÊÊÂÂ).

Assume such that

t«heØ

conditions on . Assume

and À as in

«¾ ´À µ
«¾

«¾ ´À µ

Ù and

«¾

Ð

«¾Ë

(10)

¾holds for some fixed Ð Ù ¼. Then a coordinate selection ½
Thm. 1, if there exists a fixed Æ ´¼ ½ such that

«¬¬¬¬¬





´
«ØØ

Ø

µ

¬¬¬¬¬

«Æ

Ñ ½

ÜÂ

¬¬¬¬¬ 

´
«Ø

Øµ ¬¬¬¬¬

Ø ½¾

¾

satisfies (8) of (11)

Thus the approximate Gauss-Southwell method on ÊÂ as described above converges. To

show the convergence of the second variant of LS-Boost (cf. (5)) we need the following

ËProposition 3 (Convergence of the maximal improvement scheme on ÊÂ ). Let À Ëand as in Proposition 2 and assume (10) holds. Then a coordinate selection ½ ¾ ¾satisfies (8), if there exists a fixed Æ ´¼ ½ with

«   « «   «Ø
´µ

´ Ø·½ Ø µ

Æ  Ø

Ñ ½

ÜÂ

´µ

Ø·½ ¡
´µ

Ø ½¾

(12)

Thus the maximal improvement scheme on ÊÂ as above converges in the sense of Thm. 1.

Finally we can also state a rate of convergence, which is surprisingly not worse than the rates for standard gradient descent methods:

ËTheorem 4 (Rate of Convergence of Coordinate Descent, [16]). Assume the conditions
of Thm. 1 hold. Let as in Prop. 2 and assume (10) holds for some Ð ¼. Then we have

«   «   «   «¯Ø·½

Ø·½
´µ

£
´µ

Ø½ ½ ´ ´ µ

£
´ µµ

(13)

« «where Ø is the estimate after the Ø-th coordinate descent step, £ denotes a optimal solu-

½  tion, and ¼

. Especially at iteration Ø: ¯Ø ´½ ¯½ µØ ¼.

ÇFollowing [16] Öconstant of

one can and

show that the constant is is a constant that depends on

´À¾aÄnÆÂ¾dÆth¾eµr,ewfohreereonÄthisethgeeoLmipestrcyhiotzf

the hypothesis set (cf. [16, 13] for details). While the upper bound on can be rather large,

making the convergence slow, it is important to note (i) that this is only a rough estimate

of the true constant and (ii) still guarantees an exponential decrease in the error functional

with the number of iterations.

3.2 Leveraging and Coordinate Descent

We now return from the abstract convergence results in Sec. 3.1 to our examples of lever-
aging algorithms, i.e. we show how to retrieve the Gauss-Southwell algorithm on ÊÂ as a
part of Alg. 1. For now we set ¼. The gradient of with respect to « is given by

 ´«µ
«

ÈÆ Ò½

³´ÝÒ

« ´ÜÒ µµ

´ÜÒ µ

ÈÆ Ò½

Ò

´ÜÒ µ

(14)

where Ò is given as in step 3c of Alg. 1. Thus, the coordinate with maximal absolute gradient corresponds to the hypothesis with largest absolute edge (see definition). However, according to Proposition 2 and 3 we need to assume less on the base learner. It either has
to return a hypothesis that (approximately) maximizes the edge, or alternatively (approximately) minimizes the loss function.
Definition 5 (Æ-Optimality). A base learning algorithm Ä is called Æ-optimal, if it always returns hypotheses that either satisfy condition (11) or (12) for some fixed Æ ¼.
ÀSince we have assumed is closed under complementation, there always exist two hy potheses having the same absolute gradient ( and ). We therefore only need to consider
the hypothesis with maximum edge as opposed to the maximum absolute edge.
For classification it means: if the base learner returns the hypothesis with approximately smallest weighted training error, this condition is satisfied. It is left to show that we can apply the Thm. 1 for the loss functions reviewed in Sec. 2:

Lemma strongly

6. The convex

loss and

functions fulfill the

of AdaBoost, conditions in

Logistic regression and Thm. 1 on any bounded

LS-Boost subset of

ÊarÆe

bounded, .

We can finally state the convergence result for leveraging algorithms:

Theorem 7. Let be a loss function satisfying the conditions in Thm. 1. Suppose Alg. 1

generates a sequence of hypotheses
«base learner. Assume Ø with «Ø «point of Ø is a solution of (6) and

½ È½Ø Ö

and weights «½ «¾ using a Æ-optimal

½ «ÖÁ´ Ö

µ is bounded. Then any limit

converges linearly in the sense of Thm. 4.

Note that this result in particular applies to AdaBoost, Logistic regression and the second
   tfcvhooeerrnsadbiloilatnishoeynoplsfeoiaLtnhrSneD-esBerefisop.norSisettifi.neocrnFse5oÈhrcyatÆÒphnoent½ho´seetslÒbeeesctswiaotinit´shÜfisÒcsehmµdµea¾imnllegÈeoÈnfÒÆeÒÆLraS½l½,-´Buno´Ülo´esÒÜstsµÒ¾gÈµi¾vaÒÆnend¾½bcÒyou(´Òl3dÜ´)ÒÜtahµÒn¾eµdr·iesf(co4Óor)Òe,n×sbsØttoaontµpht,
improving the objective while being not optimal (see [20, Section 4.3] and [19, Section 5] for more details).

4 Regularized Leveraging approaches

We have not yet exploited all features of Thm. 1. It additionally allows for box constraints

and a linear function in terms of the hypothesis coefficients. Here, we are in particular

« « «interested in ½-norm penalized loss functions of the type ´ µ À´ µ·

½, which

are frequently used in machine learning. The LASSO algorithm for regression [26] and the

ÀPBVM algorithm for classification [25] « «closeness of , we can assume without

are examples. Since we loss of generality that a

assumed solution

co£msaptliesmfieesnta£tion

 «¼. We can therefore implement the ½-norm regularization using the linear term

,

where

½ and

¼ is the regularization constant. Clearly, the regularization

Àdefines a structure of nested subsets of , where the hypothesis set is restricted to a smaller

set for larger values of .
«The constraint ¼ causes some minor complications with the assumptions on the base

learning algorithm. However, these can easily be resolved (cf. [21]), while not assuming

more on the base learning algorithm. The first step in solving the problem is to add the
additional constraint «Ø ¼ to the minimization with respect to «Ø in step 3b of Alg. 1.

Roughly speaking, this induces the problem that hypothesis coefficient chosen too large in

a previous iteration, cannot be reduced again. To solve this problem one can check for each

coefficient of a previously selected hypothesis whether not selecting it would violate the
Æ-optimality condition (11) or (12). If so, the

Algorithm 2 ­ A Leveraging algorithm for ½-norm regularized loss .

1. Input: Sample Ë, No. of Iterations Ì , Loss function ÊÆ 2. Initialize: ¼ ¼, Ò½ ³´ÝÒ ¼´ÜÒµµ for all Ò ½ Æ

Ê, Reg. const.

3. Do for Ø ½ Ì ,

Â ¾  (a)
(b) (c)

ÖLT£reatinÖcÖlasÑÈsiÒfiÆÒe¾r½ÂonØÒ

,

Ë Ø and
Ö´ÜÒµ and where

«obÖ tainÈhØ×yp½o«thÖeÁs´is× ½Ø

Ø
½

Öµ for Ö and «

(d) if Ø  

  Ö£ then Ø Ö£ and «Ø «Ö£ else «Ø ¼

½
¼.

Ø

(e) (f)

Set «Ø
Update

 «Ö Ñ Ò «Ø Ø·½ Ø · «Ø

«Ø · Ø · Ø and ØÒ·½

« ³´ÝÒ

Ø·½´ÜÒµµ Ò

½

Æ

4. Output: Ì

¼

algorithm selects such a coordinate for the next iteration instead of calling the base learning

algorithm. This idea leads to Alg. 2 (see [21] for a detailed discussion). For this algorithm

we can show the following:

Theorem 8 (Convergence of ½-norm penalized Leveraging). Assume À are as
Thm. 1, is strictly convex, ¼, and the base learner satisfies

 ´«Øµ
«ØØ

ÆÑ Ü ½

Â



´«Ø µ
«Ø

Ø ½¾

(15)

for Æ ¼. Then Alg. 2 converges linearly to a minimum of the regularized loss function.

This can also be shown for a maximum-improvement like condition on the base learner,
which we have to omit due to space limitation.
In [27] a similar algorithm has been suggested that solves a similar optimization problem
«(keeping ½ fixed). For this algorithm one can show order one convergence (which is
weaker than linear convergence), which also holds if the hypothesis set is infinite.

5 Conclusion
We gave a unifying convergence analysis for a fairly general family of leveraging methods. These convergence results were obtained under rather mild assumptions on the base learner and, additionally, led to linear convergence rates. This was achieved by relating leveraging

algorithms to the Gauss-Southwell method known from numerical optimization. While the main theorem used here was already proven in [16], its applications closes a central gap between existing algorithms and their theoretical understanding in terms of convergence. Future investigations include the generalization to infinite hypotheses spaces and an improvement of the convergence rate . Furthermore, we conjecture that our results can be extended to many other variants of boosting type algorithms proposed recently in the literature (cf. http://www.boosting.org).
References
[1] H.H. Bauschke and J.M. Borwein. Legendre functions and the method of random bregman projections. Journal of Convex Analysis, 4:27­67, 1997.
[2] K.P. Bennett, A. Demiriz, and J. Shawe-Taylor. A column generation algorithm for boosting. In P. Langley, editor, Proceedings, 17th ICML, pages 65­72. Morgan Kaufmann, 2000.
[3] L. Breiman. Prediction games and arcing algorithms. Neural Comp., 11(7):1493­1518, 1999. [4] N. Cesa-Bianchi, A. Krogh, and M. Warmuth. Bounds on approximate steepest descent for
likelihood maximization in exponential families. IEEE Trans. Inf. Th., 40(4):1215­1220, 1994. [5] M. Collins, R.E. Schapire, and Y. Singer. Logistic Regression, Adaboost and Bregman dis-
tances. In Proc. COLT, pages 158­169, San Francisco, 2000. Morgan Kaufmann. [6] J. Copas. Regression, prediction and shrinkage. J.R. Statist. Soc. B, 45:311­354, 1983. [7] S. Della Pietra, V. Della Pietra, and J. Lafferty. Duality and auxiliary functions for bregman
distances. TR CMU-CS-01-109, Carnegie Mellon University, 2001. [8] N. Duffy and D.P. Helmbold. A geometric approach to leveraging weak learners. In P. Fischer
and H. U. Simon, editors, Proc. EuroCOLT '99, pages 18­33, 1999. [9] N. Duffy and D.P. Helmbold. Potential boosters? In S.A. Solla, T.K. Leen, and K.-R. Mu¨ller,
editors, NIPS, volume 12, pages 258­264. MIT Press, 2000. [10] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119­139, 1997. [11] J. Friedman, T. Hastie, and R.J. Tibshirani. Additive Logistic Regression: a statistical view of
boosting. Annals of Statistics, 2:337­374, 2000. [12] J.H. Friedman. Greedy function approximation. Tech. rep., Stanford University, 1999. [13] A.J. Hoffmann. On approximate solutions of systems of linear inequalities. Journal of Research
of the National Bureau of Standards, 49(4):263­265, October 1952. [14] J. Kivinen and M. Warmuth. Boosting as entropy projection. In Proc. 12th Annu. Conference
on Comput. Learning Theory, pages 134­144. ACM Press, New York, NY, 1999. [15] D.G. Luenberger. Linear and Nonlinear Programming. Addison-Wesley Publishing Co., Read-
ing, second edition, May 1984. Reprinted with corrections in May, 1989. [16] Z.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differen-
tiable minimization. Journal of Optimization Theory and Applications, 72(1):7­35, 1992. [17] L. Mason, J. Baxter, P.L. Bartlett, and M. Frean. Functional gradient techniques for combining
hypotheses. In Adv. Large Margin Class., pages 221­247. MIT Press, 2000. [18] T. Onoda, G. Ra¨tsch, and K.-R. Mu¨ller. An asymptotic analysis of AdaBoost in the binary
classification case. In L. Niklasson, M. Bode´n, and T. Ziemke, editors, Proc. of the Int. Conf. on Artificial Neural Networks (ICANN'98), pages 195­200, March 1998. [19] G. Ra¨tsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam, October 2001. http://mlg.anu.edu.au/~raetsch/thesis.ps.gz. [20] G. Ra¨tsch, A. Demiriz, and K. Bennett. Sparse regression ensembles in infinite and finite hypothesis spaces. Machine Learning, 48(1-3):193­221, 2002. [21] G. Ra¨tsch, S. Mika, and M.K. Warmuth. On the convergence of leveraging. NeuroCOLT2 Technical Report 98, Royal Holloway College, London, 2001. [22] G. Ra¨tsch, T. Onoda, and K.-R. Mu¨ller. Soft margins for AdaBoost. Machine Learning, 42(3):287­320, March 2001. also NeuroCOLT Technical Report NC-TR-1998-021. [23] G. Ra¨tsch and M.K. Warmuth. Marginal boosting. NeuroCOLT2 Tech. Rep. 97, 2001. [24] R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970. [25] Y. Singer. Leveraged vector machines. In S.A. Solla, T.K. Leen, and K.-R. Mu¨ller, editors, NIPS, volume 12, pages 610­616. MIT Press, 2000. [26] R.J. Tibshirani. Regression selection and shrinkage via the LASSO. Technical report, Department of Statistics, University of Toronto, June 1994. ftp://utstat.toronto.edu/pub/tibs/lasso.ps. [27] T. Zhang. A general greedy approximation algorithm with applications. In Advances in Neural Information Processing Systems, volume 14. MIT Press, 2002. in press.

