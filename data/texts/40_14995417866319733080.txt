Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, 2000
Barrier Boosting

  ¡   ¢ 

¤  £

Gunnar Ra¨tsch , Manfred Warmuth , Sebastian Mika , Takashi Onoda , Steven Lemm and Klaus-Robert Mu¨ ller

 
¡ GMD FIRST, Kekule´str. 7, 12489 Berlin, Germany Computer ¢ Science Department, UC Santa Cruz, Santa Cruz, CA 95064, USA
£ CRIEPI, 2-11-1, Iwado Kita, Komae-shi, Tokyo, Japan ¥ University of Potsdam, Am Neuen Palais 10, 14469 Potsdam, Germany raetsch, mika, lemm, klaus¦ @first.gmd.de, manfred@cse.ucsc.edu, onoda@criepi.or.jp

Abstract

are used for some linear combination of hypotheses from a given finite hypothesis set. Contrarily, in Section 4 we will

Boosting algorithms like AdaBoost and Arc-GV

consider iterative algorithms like AdaBoost and Arc-GV.

are iterative strategies to minimize a constrained objective function, equivalent to Barrier algorithms. Based on this new understanding it is shown that convergence of Boosting-type algorithms becomes simpler to prove and we outline directions to develop further Boosting schemes. In particular a new Boosting technique for regression ­ § -Boost ­ is proposed.

2.1 Margin, Edge and Linear Programming

We begin by focusing on the following problem. We are

gA9 ive@ n a set of ¨ examples ©  ! ¤"#%$'&)(130 42 054¨ 86 7

U

C 1B D( 6 and a (fiY9 nitXae) sc` eb t of hypotheses EFGIQH PR&S(T0

W0 SV 6 of the form

(#e1d (gf .

Our goal is to find a "good" convex combination of the

hypotheses, i.e.

1 Introduction

r Ch i s
 p$q

P H P p $

(1)

The past years have seen strong interest dedicated to Boosting and ensemble learning algorithms due to their success in practical classification applications (e.g. [13, 26, 28, 43, 2, 11]). Recent research in this field now focuses on the better understanding of these methods and on extensions that are concerned with robustness issues [31, 3, 37, 36, 38] or generalizations of Boosting algorithms to regression [19, 14, 5].
The present work aims to contribute in two respects: (a) we will show an important relation of Boosting to a general class of optimization methods, the so-called barrier optimization ­ a technique to minimize constrained objective functions [20]. We clarify that Boosting can be seen as a special case of barrier optimization, i.e. as an iterative approximation method to a barrier algorithm that also relates to the Gauss-Southwell method [27] of nonlinear optimization. Furthermore, this understanding allows us to outline possible paths going beyond existing Boosting schemes. For example, convergence theorems from the optimization literature can be applied, simplifying convergence proofs for Boosting type algorithms. We choose a particularly interesting path, giving rise to our second contribution: (b) the definition of a new Boosting algorithm for regression and its convergence proof. Experiments on toy examples follow, that show the proof of concept for our regression algorithm. Finally a brief conclusion is given.
2 Boosting and Convex Programming

Putwvyx

r

where 

lies in the

V

-dr imensional

probability
hi

simplex



.

That is, P85 and  uP wt v P ( . If fier thenx the classification ox f instance 

is used as a classi-

Ch i
is u c  

$¤$ .1

Let us now define a measure of goodness for a single

example. The margin of an example  u"#h Q i $ with respect to a given weight vector  is defined as "     $ . A pos-

itive margin corresponds to a correct classification and the

more positive the margin the greater the confidence [39, 40,

49] that the classification is correct. The margin has been

frequently used in the context of Support Vector Machines

(SVMs) [49, 47] and Boosting (e.g. [39, 40, 36]). However,

for the definition of the margin one needs to have a normal-

ization by some norm of  , as otherwise one could arbitrarily

increase the margin by scaling the weight vector  . Different

norms are used for SVMs and Boosting. In SVMs the margin

is normalized by the g -norm of the weight vector in feature

space. In Boosting the v -norm of the weight vector is used

(cf. Footnote 1). Note that for thChei purpose of classification

the normalization of the function is immaterial.

r

For convenience we introduce the matrix adeShf g ,

where   P  "  H P   $ . TheU 2 -th example    u"  U $ corresponds to the 2 -th row and the -th hypothesis to the -th col-

umn of  . We let i denote the 2 -th row ofh  i . With this notation the margin of the 2 hC-ith example is "     $'j   . The margin of a function is defined as the minimum mar-

In this section we will introduce some terminology and notation conventions. We will mainly consider convex optimization problems for finding hypothesis coefficients that

1Note, that we could use an arbitrary mk ol n , e.g. pqrk pgsuxv t w and t{ hen we would need to normalize the function ygz . Here, we use the
s norm for the normalization.

gin over all ¨ examples, i.e.

| }~ $q  #f wt  v i 

(2)

A reasonable choice [29, 21, 1, 49] for a convex combi-

nation is to maximize the minimum margin of the examples,

i.e. choose 

 

r d 

such that | 

  $

i  D C  

|  $

(3)

Roughly speaking, the larger the margin the better the bounds

that can be proven for the generalization error (e.g. [49, 1]).

Also SVMs are based on maximizing a minimum margin.

They use the g -norm to define the margin and the maximum

margin hyperplane maximizes the minimum geometric dis-

tance of the patterns to the hyperplane. In our case we use

the  v -norm to define the margin. Now the maximum margin

hyperplane maximizes the minimum   distance of the pat-

terns to the hyperplane [30]. We assume for convenience

throughout the paper that the hypb otheses class is comple-

mentation problems2

cfloorstehde(cH asde Ethaitm| p l ie  s$ 

Hd E
.

) in order to avoid

Boosting algorithms maintain a distribution W d f on

the examples. What would be a good choice for this distri-

bution for a given set of examples and hypotheses? Assume

for a moment that the labels of the hypothesis are binary, i.e.

H P   $h3d C 1B #( 6 . Then for a distribution  , the dot product

~P    #f twv  "#%HQP  p$ is the expectation that HQP pre-

dicts the D 1B D( 6 label correctly.3 We call this the edge [8] of

the hypothesis H P . Note that a random hyp` bothesis has an ex-

pected edge `ob f zero and since QH P  %h$ d

(ed1(f , the edge

of HP lies in ( ed1(f . We define the edge of a weight vector

 as the maximum edge over the set of hypotheses, i.e.

r

 S q$ 

 D uP wt v

 P  

(4)

Since to  D 

E

r is complementation closed, the above is equivalent Putwv  P    . In the case of Boosting, we want to find a

distribution on the examples such that the maximum edge of

the hypotheses is minimized [8] (a solution always exists):

 
choose  d  f

such that   

  q$ 

  D  

 S$

(5)

The minimax theorem of linear programming (LP) can be used to make the following connection [8, 21, 17, 3] between the above two optimization problems.

Theorem 1.

  D     S$q i  D D   | }r$

(6)

The theorem is proven by considering both sides of (6)

as linear programming problems (Here ¡ and ¢ are vectors

or all zeros and ones, respectively, where the dimension is

understood from the context):

 £ ¤ Di  |

 ¥ ¤   

s.t.   ¡ u1¢ j(
~  | ¢

  ¡ q¢ (    0  ¢

(7)

Margin-LP Problem

Edge-LP Problem

2Alternatively, one can use two non-negative weights per hy-

pothesis C¦ § , one for ¦C§ and one for ¨ C¦ § .

as

©

3Here correct means © w and incorrect as 0,

w and then it

iwnocourldrebcet c¨ omw .eIfªQs c«}o¬ rr§ ec® t

is
©

encoded w ¯ .

Both problems are dual to each other and thus the equality of the theorem follows from the fact that the primal and the dual objective have the same value. Since our hypothesis class is complementation closed, this value is always non-negative. The Margin-LP Problem was introduced in [29] and was first used for Boosting in [8, 21].

2.2 Boosting and Relative Entropy Minimization
We will now use the Edge-LP problem to make a connection to a class of Boosting algorithms that use a relative entropy in the objective function [22, 25, 9]. In the Totally Corrective Algorithm of [22] and in a related algorithm by [9] the edge was forced to be zero for all hypotheses. This essentially corresponds to the Edge-LP Problem with  fixed at  . Furthermore a relative entropy to the uniform distribution was used as the objective function:

  c 

f

 Dtv



 ° Yuv ±e³ ²f

s.t.   ¡ uS¢´(

~q oµ ¡

(8)

Totally Corrective Algorithm

Note, the well-known AdaBoost algorithm [16, 40] can be

motivated as minimizing a relative entropy subject to the

constraint that the edge of only the last hypothesis is zero

[16, 22, 25, 9].

However, how should one choose  when there is no dis-

tribution Theorem


1

for which the implies that if

edges of all the margin |

hyp  otheses } $¶  ,

are zero? then such

distribution  does not exist. In this case the minimal edge is

positive. This question can be answered by adding a relative

entropy to the objective function of the Edge-LP Problem:

 ¥ ¤ c 

 d¸·

f

 Dtwv





°

vu± ³

² f

s.t.   ¡ uS ¢´(
~q¸0  y¢

(9)

Note that we introduced a constant parameter · , which controls the trade-off between keepingX the edge  versus the relative entropy of  minimal. For ·  we recover the EdgeLP Problem. Also note that if    is enforced in the above

problem then we arrive at the optimization problem of the

Totally Corrective Algorithm. We believe that above prob-

tlheme cwasiethwthheentr| ad e-  o$¹ff¶

parameter .

·

is the natural choice for

Before we continue we change to a more convenient vari-

ant of the above problem (9). Using the new problem (called

the Edge-Entropy Problem) will simplify the notation in the

sequel of the paper. Note that constraints of both problems

are the same, but the objective function of the new problem

differs by · S ¢! ° u ¨ºd($ . Since we have the constraint

 »¼¢ ½ ( , it is a constant and both optimization problems

are equivalent:

 ¥ ¤  

 ¾d ·

f 

b   °    

Dwt v

s.t.   ¡ uS  ¢(
~ i¸0  ¢)

(10)

Edge-Entropy Problem

The dual of the above is

 £q¿C ¤ i 

|À b

·

f 

 ÂTÃ

#twvÁ

s.t.   ¡u¢ j(

q£ ¿Ä Å i À²

Æ

(11)

Margin-Exp Problem

In Section 4.2 we will present how this problem is related to the Arc-GV algorithm [8]. Let | À $ be the solution of (11) for a fixed  . We have:

|#À } ~$q

b sf · ° Ç É È

Q Â Ê b q 



Dtwv Á

Ì· Ë Í

Analyzing the behavior of | À }r $ for · X  yields:

À °  Î  Ï | À ~ q$  |   $

(12) (13)

where the convergence in terms of · is linear in the worst case. We can get rid of the variable |#À in the Margin-Exp Problem by plugging in the optimal | À $ given in (12). This results in an equivalent optimization problem with one less variable to optimize:

i

 i  

f 3· Ð ° Ç  

QRÂ Ã b Å ²À Æ d5( Ñ

Dtwv Á

s.t.   ¡ ur ¢ j(

(14)

Note that except for the constraint  ¢ j( (and some constants) the above problem is dual to the optimization problem

(8) of the Totally Corrective Algorithm. Also AdaBoost can

be motivated as optimizing a °Ç  of a sum of exponentials,

twhheesroeluthtieonco nÀs  troafinttherM» ¢WargÒin-(Exisp

absent.4 Problem

Note also converges

that (for

X ·

 ) to a global solution    of the Margin-LP Problem.5

When the examX ples are noisy then the solution found by (3) and (11) for ·  tends to over-fit the data, as all pat-

terns are classified with some non-negative margin [21, 36].

In Section 4.5 we will consider a regularization approach,

where the entropy is used as regularization by keeping 3· ¶  . Then, the parameter · specifies the trade-off between minimizing the edge and keeping the relative entropy small.
Note that the constraints    | ¢ of the Margin-LP problem are absent from the Margin-Exp problem. However,

À °  Î~ Ï b · QÂÔÓ · Ä v  | b iQ  $¤rÕ  b×Ö iff q j0 |
Á

and thus the additional term (which involves a sum of exponentials) in the objective function of the Margin-Exp Problem enforces the constraints q   | for small enough · . This technique for solving a consx traint optimization problem is known as the exponential barrier method (e.g. [10]). Also the constraints   ¡ can be removed when going from the Edge-LP Problem to the Edge-Entropy problem. Here, the

4However, the scaling can easily be done by setting Ø v w Ù «ÛÚ# k¹Ü ¯ , where kÜ is now the unnormalized version of k . This will be worked out in detail in Section 4.2.
5For the case that the solution of the Margin-LP Problem is not unique, (14) prefers solutions which have a small relative entropy in the dual domain.

entropy term works as a barrier for the non-negativity constraints. This barrier function is called the entropic barrier [6].
Before we show some more connections to Boosting in Section 4, we will introduce the numerical method of barrier optimization in the next section.

3 Barrier optimization

3.1 Problem definition

Let us shortly review some basic statements and formulations about barrier optimization techniques. For details, the reader is referred to e.g. [20, 6, 27]. Consider the convex optimization problem

 c 
with

Ý } Þ!$ ß  àyÞ $ ¼ âá!2  q( gu ¨

(15)

We call ã the convex set of feasible solutions described by

ã ´ CÞY&rß  }!Þ $ m  âá2  ( gg  u¸¨ 6 (16)

We assume ã is non-empty, i.e. there exists a feasible solution. If the function Ý!à !Þ $ is strictly convex, then the solution of problem (15) is unique. I  f Ý} Þ!$ is convex only, there exists a set of global solutions ä . Note that from the convexity of !Ý à !Þ $ and ã follows that any local minimum is a global minimum as well.

3.2 Barrier functions
Problem (15) can be solved by finding a sequence of (unconstraint) minimizers of the so called barrier (or penalty) error function å

À àÞ$qxÝ}!Þ $d s f

À } ß }!Þ u$ $

Dwt vpæ

(17)

where, À is a barrier function and ·¶  is a penalty parameter.æ Common choices for À can be found in Table 1. Barrier algorithms use a suitablyæ chosen sequence of · 's that goes to zero. For each · , using the Þ found in the previous iteration as a starting value, (17) is minimized.
For the Log-Barrier and the Entropic Barrier the initial Þ has to be a feasible solutions. The barrier function assures that the sequence of Þ 's corresponding to the decreas  ing sequence of · values remain feasible. Thus the final Þ is approached from the "interior" of the feasible region. Methods based on the Log-Barrier and the Entropic Barrier are therefore called interior point methods. Such methods are often used for finding solutions for SVMs [7].

Table 1: Common barrier functions used in convex optimization

b À ¤ç $ b æ · c° Ç ç

·

·ç Q Â 

°b Ç

qç çuèCw· $

Á

Name Log-Barrier [6, 20, 27] Entropic Barrier [6, 27] Exp-Barrier [10, 24, 12, 33]

In the case of the Exp-Barrier, the initial solution does not have to be feasible [10]. For the Exp-Barrier the minimizers

å

of À }Þ!$ will become feasible automatically, when · be-

comeé s small enough. If a constraint is violated then this will

lead to an exponential growth in the barrier objective (17).

So the barrier has the effect that it keeps Þ in the feasible

region or pulls it closer to a feasible solution. Note that En-

tropic Barrier is the dual of the Exp-Barrier (cf. Section 2.2).

The Exp-Barrier can also be used to solve the feasibility

problem for convex programming:

ê )ë Þ
with ß} !Þ $ ¼

á!2 (qg ¤¨

(18)

In fact, we will see in Section 4.3 that AdaBoost exploits that property. Solving the feasibility problem with interior point methods is more involved [48]. In the rest of the paper we concentrate on the Exp-Barrier only and focus on its connection to Boosting methods. Most of the work in Section 5 can be extended to other barrier functions.

3.3 Convergence

Besides an intuitive reasoning for the barrier function con-
verging to an optimal solution of (1å 5) the following presents some more formal aspects. Let us define

ÞÀ

ì& 

# í



 î

 

À } Þ$

(19)

 
as the optimal solution for some fixed · . Moreover, let ä

be the set of global solutions Xof (15). Then for any barrier function and any sequence ·pï  holds:

ï °Î   

Þ IÀ ð Éd ä å

 

(20)

However, for the Exp-Barrier6 it it turns out to be unnecessary to exactly minimize À ð for each p· ï . The following proposition shows how close an estimate Þ ï to Þ À ð has to be, in order to obtain the desired convergence:

Proposition 2 (along are differentiable and

the lines of [10]). convex functions.

LAestsÞ uïmbee

Ý

and ß an ñ ï5 

minimizeå r of

å

À ð à Þ $qxÝ }Þ!$¾d p· ï s f

Q Â

b 

D wt v Á

ß}!Þ $uCè p· ïò$

(21)

i.e. óô î point of

À C Þ

ð ï

àÞ 6ï

ï C$ö óRõ0 gñ ï

.

Then, for

ñïeup· ï

ï yb Î X 

is a global solution to (15).å

 every limit

In the sequel tion 2, where

we we

will often use a use ñïS3·pï and

simpler version require gó ô î À ð

ofï
} Þ

Proposig$ ×ó 0m·pï .

4 Boosting as Barrier Algorithms
In Section 2 we have considered convex optimization problems for finding the hypothesis coefficients  for a given finite hypothesis set E . Now, we would like to consider iterative algorithms that have to solve two problems: In each iteration one has to select a hypothesis from E and then one assigns a weight to the selected hypothesis. We will show how AdaBoost [16] and Arc-GV [8] can be understood as particular implementations of a barrier optimization approach, asymptotically solving convex optimization problems of the
6For other barrier functions there exist similar results.

type stated in Section 2. It is shown, that Boosting is basi-

cally a Gauss-Southwell method minimizing a barrier func-

tion.

Throughout the paper, we will think of dealing in each

iteration ç of Boosting with a full hypothesis weight vector  ï ofU size V . However, usually we will change only one entry at a time. We denote by ÷ ï the index of the hypothesis that has been chosen in the ç -th iteration and by ð the

hypothesis coefficient that has been updated. Thus,x ÷ ø is a

mapping fromX the hypotheses that have been chosen so far to

E , i.e. ÷ &#ù

E.

4.1 The underlying Optimization Problem
hCi
Let be a convex combination (cf. Section 2) of hypotheses

HdRE

as defined in Section 2
r hDi s P
 $&ì Puwt v óx ó v H#P $

(22)

where we enforce the hypothesis coefficients to sum to one

by dividing by ógóv . For simplicity, we will consider the

unnormalized version of  throughout the rest of the paper and normalize, if it is needed.

Suppose one would like to solve the Margin-LP Problem

(7), where we can omit the  ¢ ( constraint due to of the

normalization introduced in (22):

D 
with

|

Ch i #"  % $ 

|

(23)

 x

Note that the constraint P ú  can always be enforced by selecting the H P with xthe appropriate sign (we have assumed complementation closedness of E ). For convenience, we will in the sequel also often write   ¢ instead of óg1ó v . Usinå g (17) and the definition of i Cï we minimize

À  1 | $  b | d¾· s f

QÉÂ Ê | ó óv b i Q

D wt v Á

· ógóv å

with respect to | and  with   ¡ . Let

` 

À

|À af 

Di í  û

 Ï

¤ c£ 

À } | $

Ë

(24) (25)

`
and let 

 

|   f be a global solution to (23). Note that | À



#| À }r$ introduced in Section 2.2. Using (20) we conclude,

that

À°  Î Ï

` 

À

|À f!

` 

 

|  f

(26)

4.2 Arc-GV ­ A Greedy Algorithm

Let us now consider one particular iterative strategy for min-

imizing (24)Ï which wihlli tü»urý n out to be exactly Arc-GV: We

start with  W ¡ and

 . In each iteration ç we (i) ap-

Uproximate | À and (ii) find a hypothesis H P

)

and update P to get the Now the dx etails: First

new hypothý esis we set ·pï gó 

Rd E (i.e. an index

wï eó igÄ hv t

vector which


is

ï

. a

reasonable choice, as (assuming complementatioþX n cÖlosed hy-

pothesis sets) for AdaBoost and Arc-GV ó 1ó

holds.

In step (i) one approximates the minimum margin | À by

(cf. Eq. (13))

|

|  

ïÄ v $ 

  c 

h i ðÿ¡ 
#"    ) $

In step (ii) one updates the hypothesis weighting ð by

ð xø

D¢¤í £


ð¦¥

Ï 

s f  Â #wt v Á

 | óg1ó v

b

xø   r $

(27)

where one chooses the index ÷ï such that the sum in (27) is minimal. That is, one finds the hypothesis such that some weighted training error  is minimized [16]. Note that for

the case where we use D B1D( 6 -valued weak learners one can

compute a closed form solution [8] for (27):

ð xSø

r§( °Ç 

Ê

(

b 



(

| b

|Ë

(28)

Moreover, note that | is the same as ¨ ©¤ used in [8]. Proposition 2 serves as a useful tool foå r proving the con-

vergence of such kind of algorithms: From the fast convergence of | to |À we can conclude that ô £ À   | $ vanishes fast. Consider the update for the hypothesis weight ð given

by (27) and (28), respectåively. Under rather XFmilÖ dx aø ssump-

tions, one can show that in the limit for óg1ó v

holds:

D í
¢

£

 ð

û

Ï 

À 1 | $q

ð

xø

(29)

where  is the vector of the last iteration  ï Ä v changed in

the
ó ó

÷

Ä v

ï

-v thincformonptoonfenthteosnulym.

minimization of ð as óó

This is because the factor õ· 

iï nÄ
v

(24) loses v becomes

its influence large.

on

the

However, (29x )ø does not imply that the gradient with re-

spect to  becomes ¡ yeå t, as needed to apply Proposition 2.

It needs some more argumentation to show that

i ô

À  1 | $ X

¡

(30)

where one exploits that always the best hypothesis H ð is found. Essentially, a proof can be done in the style of Tøheorem 4 in Section 5.5, where one uses the convergence properties of the Gauss-Southwell method (gradient-descent in coordinate directions with maximal gradient) [27]. One has to show, that óóv grows slow enough while the gradients become smaller. Similar techniques have been used in [15, 14] and (30) has already been shown in [8], so we will not go into further details. Using the property (30), one can finally apply Proposition 2 which shows the convergence.
Based on the argumentation above, we would like to viå ew Arc-GV as a barrier algorithm using a particular strategy (related to the Gauss-Southwell method) for minimizing À and choosing · .

4.3 AdaBoost ­ Finding a Separation

We can argue as before and find that AdaBoost can be interpreted as a barrier algorithm for finding a separation of some training set with margin at least  . For this, we fix | to  and get tåhe barrier function from Eq. (24)) as

À }r$

b  d¾· s f

Q Â

D twv Á

Ê gó 1 ó v  b   



· gó  ó v

Ë

(31)

Originally, the · in front of the sum in (31) down-weights

the constraint of AdaBoost

penalties against the and Arcing we have

o bjý ectivaen. dBu t

iý n

the case const.,

respectivh ei ly, tion ("   

and
$ 

our 

goal is to simply obtain , cf. Eq. (18)). Thus, we

a feasible can omit

soluthe ·

here and drop the  in front. What remains is only the sum

of (31). By using (coordinate­wise

the same simplified descent) and setting

optiý mizatioÄ nv
· óg ó v

strategy in (31)

as before, we obtain the original AdaBoost error function

[8, 18, 31, 36]. least  , if ó óv

TaX huÖs, we will get a solution with margin at . Note that ó1 óv will stay bounded if

and only if there is no solution [16, 36].

4.4 Iterative Boosting Algorithms that always update all previous coefficients

There are two basic approaches for designing boosting algo-

rithms. The first one is to update the coefficient of a single

(possibly new) hypothesis in each iteration. In this approach

we want to do little work per iteration. Typical examples are

AdaBoost and Arc-GV. A second approach is to ignore op-

timization issues and use the optimization problems consid-

ered in Section 2.1 to always find the optimal weights of all

past hypotheses. The Totally Corrective Algorithm of [22]

and the Column Generation Algorithm of [3] are particular

examples using this approach.7 Assume at trial ç we already have a subset E

ï Ä v of ç

hypob theses from the base set E . of ç ( weights for combining

We the

also have a vector hypotheses of E ï

Ä

b
ï v

Ä

( v

in

some optimal way. This vector was found by solving some

scwhohinpevnsexwapoepplatiliewmdaitzyoastthihoeanvspeetraoE bclï oeÄ rmvre. (sNep.oogtn.edthtihneagMt dvaiirsagtrtihinbe-uEdtxiuopanlPr reoïlbaÄ tlvieomonn)-

the examples. from the base

sDetuErintgotEriaï lÄ

ç v

we add one to form E ï

more hypothesis H ï and then find ç new

weights  ï for all hypotheses of E ï .

We have not specified how to choose the new hypothesis

H ï at trial ç . A natural greedy heuristic is choose a hypothesis

such that the value of optimization problem for E ï Ä v  IH ï 6

is optimized or approximately optimized.

Basically, any optimization algorithm for finding a linear

combination for a fixed set of hypotheses (or by the duality

relationship a distribution based on these hypotheses) imme-

diately leads to a boosting algorithm via the above scheme.

4.5 A regularized Version of Boosting
The question is which optimization problem should we use to construct a boosting algorithm (via the scheme of the previous subsection) in the case when the examples is noisy. Before we address this we give some background. It has been shown that solutions found by AdaX Boost, Arc-GV and also for the Margin-Exp problem for ·  , tend to over-fit the data, as all patterns are classified with some non-negative margin [21, 36]. Several approaches have been proposed to deal with this situation. For SVMs slack variables have been frequently used [4, 49] to allow for some soft margin, i.e. violations of the margin constraints similar to (7). In the dual domain the introduction of slack variables leads to an  norm constraint on the dual variables. Therefore, the (dual) variables  of the Edge-LP are restricted to intersection of the probability simplex f and some hypercube only, where the size of the hypercube is controlled by some regularization constant. Basically, the distribution  is kept near to the
7The boosting algorithms of [9] update the weights of all past hypotheses in parallel in an attempt to find a close approximation of the optimal weights of all the past hypotheses.

cent er of the simplex w f . The same idea has been proposed for Boosting in [37].
In view of this discussion, it is natural to keep the · parameter in the Edge-Entropy problem (10) fixed or lower bounded. Now the relative entropy term does not vanish and is traded off with the maximum edge  . The parameter · should be tuned via cross-validation. The more noise in the data, the larger · should be. Large · give the relative entropy to the uniform distribution high importance and the solution is kept close to the center of the simplex f (as in SVMs). It can be shown that this regularization approach is equivalent to the PBVM algorithm proposed in [44].
Note that the trade-off between a relative entropy and some loss has long been used to derive on-line learning algorithms (see the derivation of the Exponentiated Gradient Algorithm in [23]).

5 Boosting for Regression

So far we were mainly concerned with understanding Boosting algorithms for classification from the barrier optimization point of view and pointed out potential extensions. In this section we will now consider one natural generalization in depth: Boosting for regression.

5.1 Problem definition

9X

Let EU be a finite class of base hypotheses ºE & õI HQ~P &

fe un& ctio n8(#hCi ggd g° SV6 

. TheCh i
E $

regr9 essX ion
&e

problem :

is

to

find

some

r

hi  $ 

s

P H P  $

(32)

Putwvyx

with

r9 d e ¤ d

µ9 @
bTiasahsstemheedgaolpolarnlroiibosidkafbt(hitlreia` th yilne×f imanrgena)insdugartg¡pea!rwo"wchevD"isc¤sh"b gviis$sh taog sgsfi¤$ u$nm¡d # eafdfi utuo" unf"Qcb$te»$ i,"ordwneshh peorwenise t# ih-.

ble for the gen"eratib onh function, e.g.  "

of the obserbvedh
$¤$Ò "

data, and is a loss $u$  , depending on

the specific regression estimation problem at hand. Since we

do not know the probability density # and our hypothesih s class might be large, one has to take care that the function

does not overfit the data. Thus, one has to introduh ce capacity control, i.e. one has to bound the complexity of . One way

to obtain a small(er) risk is to minimize a regularized risk

functional

%'$ &)( ` h f& 10

`h
P

f d2%&)354

`h

f

(33)

where  &)53 4 ` h fiì& 

v 

"b
fDwt v #" 

h
p¤$ $ is the empirical

risk, P is a regularizef r penalizing (and therefore limiting) the

model complexity. The parameter 0 defines the trade-off

between complexity and empirical risk. Under rather mild

assumptions it can be shown [46] that there exists a constant

076 such that minimizing (33) produces the same function as

solving the problem P8 A9 C@ BE  GD F  )& 53 4 ` h fà

(34)

This is shown by the following lemma:

8In some cases it might be useful also to have a bias H . This
extension is made in Appendix A.1.

" b hDi

LIe` Dhmi ma 3 (Along the lines oI f [46]). Let  "

 $¤$ and

f be subsets of

convex in  . °   E $ , then

If for

defines a any sample

Q©struP ctu9 re@

of
e

nested there

exists a monotonically decreasing function 70 à6 R ~0 $ , such that

the problems (33) and (34) using 0 and 07à6 S0~$ , respectively,

have the same solution sets.

5.2 The Convex Program

" Let us assume that

and P are chosen such that T $U)& (

`h

i

f

ipsrocgornavmexfoirnm in. imTihzeinngw e$U&)c( a` nhDi

solve the following f with respect to  :

convex

 
with

0

` Ch i
P

fQd

v

f 

"
ñg% $

b f hCi #twv

ñ 5 #" 

 p$

r

2É ( g gu¨ÉWVRde f qd e

(35)

Consider the following optimization problem which is equivalent to (35) in the spirit of Lemma 3.

  
with

v

f 

"
 ñ  $

f
P

`

h

D i twf v X0 b70 6

hCi

ñ 5"#

 p $

r

É2 ( gg uÉ¨ WVRd efqde

(36)

5.3 A Barrier Algorithm

Before we derive two non-negative

vthareiabbalrersieYgro`bY j  ecm tiv e:,

we

substitute

ñ



by

ñ

XY 

b

Y

  



Then each equality constraint in (35) can be replace by two

inequality constraints:
r
bs "

P H P    $Ì0

Y

b #" ~d

uP twyv x r
s
PHP pÌ$ 0

Y

  

Puwt v x

Tushiunå sg,

the the

barrier minimization objective for the problem exponential penalty can be written as:

(35)

À

 
} ba ba $ì& c 0

`h i
P

Qf d

(s

¨

" U Y 

b

Y

  

$ d

sf d×·

Ã

EÄ d ³ À
²

d

ÄEfd e
²

³À

Æ

d

Dt v Á

Á

where ñ 

sf d×·

Ã

gih
²

EÄ d

²qp ³

À

d

gÄ

h
²

EÄ fd e
²

p³ À

Æ

(37)

Dtwv Á

Á

br

& " 

 uP wt v

P H P 'r  $ . To show how to mini-

mize (37) ify P and

u" sing a . For

Boostingx -type algorithm, convenience we use the

we sv

need to spec-norm of the

hyph oi thesis weight vector  as a measure for the complexity of [45]

` Ch i
Pv f ì& ó1 ó vI

(38)

Note that it fulfills the assumptions made in Lemma 3. Fur-

thermore, as frequently and successfully used for regression

with SVMs [49, 41], we consider the § -insensitive loss:

"Ct h b
  $

Q" ¹$ ì& 

D 



h Ió

 $

b

b #"  ó v

§#$

(39)

The ally

§ule-aindsetonssiptiavreselossoslhuatisoanpspoefal(i3n5g)parnodpethrteievs,-tarsicikt w[4il2l,u4s5u]-

can be applied to even optimize § automatically (cf. Ap-

pendix A.2). Note that most of th" e following derivations for this particular choice of P and generalize easily to other

regularizers and cost functions (e.g. for some other norm of

 and for the squared loss usingIthi` ew °Ç  -ba" rrw ier function). Plugging in our definitions of f and  $ to (37) using

Lemma 3 yields: å

À

 1baba

  $¹&ì

(

óxa

b

 
a óiv d

¨

sf d×·

Ã

yÄ d ³ À
²

d

Äyxd e
²

³À

Æ

d

Dtwv Á

Á

sf ×d ·

Ã

g h
²

Ä

t

ÄEd ²qp ³ À

d

gÄ

h
²

Ä

t

Äydxe
²

p³ À

Æ

# twv Á

Á

(40)

withå ó ó Rv 0 076 . Now we can find the optimal slack vari-

ables ô À

by minimizing (40) for   W ¡ and solving for a `a

given · and . We get:



by setting

Y   w· $5· °Ç ) ¤( d

Y

  

·w$5 ·

°Ç )¤ (d

Á

Ä Ä

gt Ä gt £

h h

q² p ²p

³ ³

À À

$ $

(41) (42)

As expected, À °  Î Ï Y   ·w$

 C



Áb ò§

ñ



$

and À°  Î  Ï

Y

  

 w· $ 

 C   ò§ dñ  $ hold.

5.4 How to Optimize in Practice §
Usually, in a barrier algorithm one would optimize all i2 d ¨ parameters directly (up to a certain precision) for some · and then decrease · until the desired precision is reached. But then we would need to know all hypothesis in E in advance in order to optimize their weights (like in SVMs). Thus, we

consider a Boosting-type algorithm that finds one new hypothesis H ð and its weight ð in each iteration. Then there is only oneø parameter, ð  ·x $ ø , to be determined in each iter-

ation.9

xø

In the described setting we can fulfill the constraint óóvr0

70 6 only by stopping the algorithm when the constraint is vi-

olated, because the weights of the previous iterations are already fixed. One way to keep going is to redefine  :

P~&ì P 

c ò #( f0

6

ó¦'ó

Ä v

v $

(43)

x
and to optimize in terms of  . Then the constraint is always

fulfilled and is active, if and only if óx' ó v  076 .10

5.5 Convergence
The critical point in proving the convergenceå of the algorithm is how the base learner selects the next hypothesis, i.e. which index ÷ï must be chosen, such that À is reduced reasonably ­ without knowing the whole hypothesis class E
9To minimize (37) with respect to  ð for a given Ø , one may
employ standard techniques like Brent's line search method which work quite fast.
10There are also other ways, like introducing a scaling variable, to deal with this problem.

Algorithm 1 The § -Boost algorithm

argument: Sample Number

oºf i t ev rag gti¤o nf s6 ` 

 " v gg ¤" f

Regularization constant 70 6

Tube size § x constants: E·  ï'xuï d4  gI( $ , ¶W( .

returns: Linear combination from E .

function § -Boost 'É` `'f70 6 ò#§ $ Set ·É5 ·  'ï x¤ï

for 2ÉÔj (#QgÂg ¤ b q¨ #" 

db o
#§ q$ èI·w$

b

Q Â u "# b D§ q$ èIw· $

endfor Á

Á

for çqj(#gg   å

H ï & su' É `dT$

¹ï & 

Dí
e

ð

¤ f  C! 

À U  ï $

Compute  Set ñ  & 3"



by
b 

(4Pg 3)g
H

g

U r



$

for 2É j&ì (#ggih ²g!Ä u ¨² Ä t dp ³ oxÀ b

gÄ

h
²

Ä

e
²

Ä

t p³ À

h

endfor Á

Á

if  HpïUo$  d  m· , do

· &ì5 ·i

endif

endfor return h å jid



r ïtwv

ï¤pH ï

end x

function À U q $

Compute 
Set ñgR& 5 "# Compute a`a

by
b

(4ïg 3)

g

   twv H

by (4x1)

g U)r %$ and

(42)

Set k8&ì return v

fó ` a yb ld a a  

 d4§Q¢y óvi¸d ·

b 

Sg pldQ Â a



 
bn m

¸d g Q§ )¢ `a
èC·w$



ba

 



f

end f

Á

6

å

in advance. The idea is as follows: The gradient of À with

respect to each å P can be computed as

x
ô ¢¤o À } 8`aba   $ s f  pQH P   p $

å

where  

ig h
²

Ä



²

Ä

tp³À

b

g Ä Dh t Äv
²



e
²

Ä

tp³À

. To reduce

À

iteratively (foÁr some fixed · ) oÁ ne mayå choose the hypothesis

H ð such that

ø

÷ qï 

D í  Puwt v

 C
¤qqp qp òp ¤ sr rr

ô

¢qo

À rr 

(44)

This corresponds to a cåoordinate descent method, the so-

called Gauss-Southwell method, whichå finally converges [27]

notoof rt÷hmï eeongfsloôubrieaslå ,stÀ ohl(auitftitEohneis¹soffi n-inÀteo)­r,mfwooirlflscôoominevefiÀ rxg,eeadntod·

. This choice therefore any zero for some

fixed · , because of the convergence properties of the Gauss-

Southwell method.

Theorem 4. Assume E is finite. Let the base learner be

su' É `dT¹$ ì&  # í t  q u C   H'o$  d  

(45)

Suppose we run § -Boost (see pseudocode) with 076¶  as

r egXuvlarÖ izatthioenoucotpnusttaonf" t

and §   as tube size. theC" talgoriIth`mhDi converges to

Then for a global

solution of (36) using  and

f óg óv .

Proof. Aså sume we would fix ·õ¶  , thåen the sequence of  ï generated in each iteration ç converges to the global minimum of À exploiting the convexity of À and the convergence properties of the Gausså -Southwell method. Thus, after a finite number of steps we have

V Ä uv ³u ógô i À ~ $ó0m·q

bMyotrheeovbears,elelet aH rønð ebreå suth'É e h`yd pï o$ t.hTeshiesnfoundå in the ç -th iteration

Ä ¤v q³  i V óô

À 

ï g$ ó 

0

i ógô

À

} 

ï $gó 

å  H 'o$  d ï 

Thus, line h

i
if ógô

À }

in Algorithm 1 ensures that we decrease · only ï g$ gó 0 ·pï`w V . Hence, the algorithm generates

sequences of  ï and  )· eï ¤p· ï`w V$ fulfilling the conditions in

Proposition 2. This proves the theorem.

Let us analyze (45) as a particular way of finding the next hypothesis: Assume ó H P 'o$gó   const for all hypothesis. Then selecting the hypothesis which minimizes the mean
squared error (MSE)

ÔH 

Dí t  q u 

( ¨

sf   b
Dtv

Hw p$u$ 

will result in the same hypothesis as in (44) and (45).

The definition of the base learner (45) seems to be very restricting. The next corollary uses a weaker condition on s
and is a direct implication of Theorem 4:

Corollary 5. Assume the conditions as in Theorem 4 and a

base learner a hypothesis

s
H

,

swuhcihchthfaotrfsoormsoemweeix ghtinÖg

d

always returns

 t qC u   H'o $  d  x  HwUo $  d  

(46)

Suppose we run § -Boost. Then for  X Ö the output of the algorithm converges to a global solution of (34).

Proof. Using the same argumentation as in the proof of Theorem 4, after a finite numbeå r of steps ç we have a gradient with respect to a sub-vector  of  satisfying:
V Ä uv u³  ógô i À } ï g$ ó  0· 
Moreover, let H ð be the hypothesis found in the ç -th iteration by a base ø learner ×s å U É d ï $ fulfilling condition (46). Suppose there would exist a hypothesis H P such that
 ô ¢qo À   $  ¶ xÉq· 

Then the base learner is enforced to return HQP before · is

decreased. Thus, the coordinate descent iterations will focus

on directions with large gradients. Thus after a finite number

of iterations tåhe gradient is reducedå for such hypothesis as

well, until

i ógô

À  ï g$ ó  y0 ¼x ó ô i

À } ï $ ó 

Hence,

å

å

V Ä vuq³  óô i À } ï $ ó  0

i ógô

À  ï g$ ó 

å 0 xoHwU o $  d ï 

Thui s,
gó ô

line
À 

h

ï

in Algorithm g$ ó  z0 x · ï w

1
V

ensures that we decrease · only if . Hence, the algorithm generates

sequences of  ï and  · ï x · ï w V$ fulfilling the conditions

in Proposition 2. This proves the theorem.

NB: One is now bad compared to

looking the best

for a hypothesis hypothesis in E .

which If x½

isÖ

not too , then

one would allow that the base learner never returns some of

tE heannedewdeodulhdygpeotthx eses.Ö

Thus, .

we could

remove them from

5.6 An Experiment on toy data

To illustrate (i) that the proposed regression algorithm con-

verges to the optimal (i.e. zero error) solution and (ii) is ca-

pable of finding a good fit to noisy data we applied it to a toy

example whose results are shown in Figure 1. For simplic-

ity i.e.

we
E

used
 I H



radial
  $ 

basis
QÂ

fbun§ ctionb
 ó

kernels
  ó$

as base hypothesis,  2#( g gq¨46 .

Á

6 Conclusion

Barrier optimization is a general framework for minimizing a constrained objective function. We have proposed to understand AdaBoost or Arc-GV as special iterative strategies for barrier optimization. This new view suggests that we can use the tool kit of optimization theory for barrier methods quite convenient in the context of Boosting and it allows e.g. simpler convergence proofs and more general Boosting algorithms for classification and regression. The proposed new § -Boost algorithm exemplifies this general claim, as it defines a very natural Boosting scheme for regression. So far the strength of our contribution is to be seen on the theoretical and conceptual side. On the practical side this paper has only shown toy examples, to give the proof of concept. Large scale simulation studies need to follow. Future theoretical research will be dedicated to further exploiting the link between barrier methods and Boosting, in order to obtain extensions of Boosting algorithms that are eventually faster, better, more general and easier to understand.

Acknowledgments: We thank Alex Smola, Bob Williamson and Bernhard Scho¨lkopf for valuable discussions. This work was partially funded by DFG under contract JA 379/91, JA 379/71 and by EU in the NeuroColt2 project. Manfred Warmuth and a visit of Gunnar Ra¨tsch to UC Santa Cruz were partially funded by the NSF grant CCR-9821087. Gunnar Ra¨tsch would like to thank CRIEPI, ANU and UC Santa Cruz for warm hospitality.

A Extensions
A.1 Regression using a Bias
In some cases it might be useful to have a bias term. One way to achieve this is using a hypothesis class that includes the constant function. But then ­ depending on the definition of the regularizer ­ the bias is penalized, too. Therefore, it

1

0.8

0.6

0.4

0.2

0

-0.2 -6 -4 -2 0 2 4 6
1.2 1
0.8 0.6 0.4 0.2
0 -0.2 -0.4
-6 -4 -2 0 2 4 6

Figure 1: Toy example: The left panel shows the fit of the sinc

function without noise (training samples: dots, fit: line) after a large

number of iterations without any regularization. It is almost perfect,

i.e. empirically the algorithm converges to the optimal solution of

the unregularized linear noisy sinc function with

program. The regularization

priagrhatmpeatnerel{}sh| ovws~a

fit of a (train-

ing samples: dots, fit: solid,  -tube: dotted, true function: dashed).

is worth investigating how to explicitely introduce it to § -

Boost.

hi

¤

In fd

reg reEss$iohni

w¤ f it&h9YbiXas

j
e

one has to find some function :

r

h i ¤ f   $ 

s

P H P   $2d j

with

Puwt yv x

r  de u4d

9

f×j d e¹

Thus, we only need to change (32) and all referring equa-

tions. In particular one needs to change the definition of ñg :

r

ñ

b & 5" 

jb

s

P H P   $

Puwt vyx

However, the problem is how to find the minimizing j for (40) in each iteration. One way is to optimize j separately in each iteration, e.g. after finding ð (cf. Algorithm 1). An-
other approach, likely to be mox reø efficient in the number of boosting iterations, would be to find j and ð simultaneously. Certainly, optimizing two variables inxstøead of one
is much more computationally expensive than for one. But

standard methods like the Newton algorithm or Conjugate Gradient algorithms work quite fast for two variables. Note that for both approaches Theorem 4 and Corrolary 5 can be easily extended.

A.2 Adaptive § -Boost

One problem of the § -insensitive loss used in § -Boost is that

it has a free parameter § specifying the tube size. Tuning §

needs some knowledge about the problem at hand. In [42]

a way for automatically tuning § for SVMs was proposed:

given a constant ´v dG  g(I$ , the tube-size is automatically

chosen such that approximately a fraction of v patterns lie

outside the § -tube.

This idea was extended in [45] for SVM regression us-

ing linear programming. It turns out to be a straightforward

extension of (36):

 c
with

vb §'a"" dAaEbd f1v hCh d  ii e¤¤ fff#£f  tw vq% gY $¹$ h00d de'§§'Y r dd  òYgY§ !  m 2É2É´  ((gggg  uu¨ ¨ óg×ó 0X70 6

(47)

Hence the difference between (36) and (47) lies in the
fact that § has become a positively constrained variable of the optimization problem itself. The v property for (47), stated below (see [45, 37]) can be shown straight forward.

Proposition 6 ([45]). Assume §¶  . Suppose we run (47). The following statements hold:

(i) v is an upper bound on the fraction of errors (i.e. points outside the § -tube).
(ii) v is a lower bound on the fraction of points not inside (i.e. outside or on the edge of) the § tube.

References
[1] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999.
[2] E. Bauer and R. Kohavi. An empirical comparison of voting classification algorithm: Bagging, boosting and variants. Machine Learning, pages 105­142, 1999.
[3] K.P. Bennett, A. Demiriz, and J. Shawe-Taylor. A column generation algorithm for boosting. In Prooceedings, 17th ICML, 2000.
[4] K.P. Bennett and O.L. Mangasarian. Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1:23­34, 1992.
[5] A. Bertoni, P. Campadelli, and M. Parodi. A boosting algorithm for regression. In W.Gerstner, A.Germond, M.Hasler, and J.-D. Nicoud, editors, Proceedings ICANN'97, Int. Conf. on Artificial Neural Networks, volume V of LNCS, pages 343­348, Berlin, 1997. Springer.
[6] D.P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1995.
[7] B.E. Boser, I.M. Guyon, and V.N. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144­152, Pittsburgh, PA, 1992.
[8] L. Breiman. Prediction games and arcing algorithms. Technical Report 504, Statistics Department, University of California, 1997.
[9] M. Collins, R.E. Schapire, and Y. Singer. Adaboost and logistic regression unified in the context of information geometry. In Colt'00, 2000.

[10] R. Cominetti and J.-P. Dussault. A stable exponential penalty algorithm with superlinear convergence. J.O.T.A., 83(2), 1994.
[11] T.G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning, 1999.
[12] M. Doljansky and M. Teboulle. An interior proximal algorithm and the exponential multiplier method for semidefinite programming. SIAM J. Optim., 9(1):1­13, 1998.
[13] H. Drucker, R. Schapire, and P. Simard. Boosting performance in neural networks. International Journal of Pattern Recognition and Artificial Intelligence, 7:705 ­ 719, 1993.
[14] N. Duffy and D. Helmbold. Leveraging for regression. In Colt'00, 2000.
[15] N. Duffy and D. Helmbold. Potential boosters? In S.A. Solla, T.K. Leen, and K.-R. Mu¨ller, editors, Advances in Neural Information Processing Systems 12, pages 258­264. MIT Press, 2000.
[16] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. In EuroCOLT: European Conference on Computational Learning Theory. LNCS, 1994.
[17] Y. Freund and R.E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 2000. to appear.
[18] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Technical report, Department of Statistics, Sequoia Hall, Stanford Univerity, 1998.
[19] J.H. Friedman. Greedy function approximation. Technical report, Department of Statistics, Stanford University, 1999.
[20] K.R. Frisch. The logarithmic potential method of convex programming. Memorandum, University Institute of Economics, Oslo, May 13 1955.
[21] A.J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In Proceedings of the Fifteenth National Conference on Artifical Intelligence, 1998.
[22] J. Kivinen and M. Warmuth. Boosting as entropy projection. In Colt'99, 1999.
[23] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient updates for linear prediction. Information and Computation, 132(1):1­64, 1997.
[24] B.W. Kort and D.P. Bertsekas. Multiplier methods for convex programming. In Proc 1073 IEEE Conf. Decision Control, San-Diego, Calif., pages 428­432, 1973.
[25] J. Lafferty. Additive models, boosting, and inference for generalized divergences. In Proc. 12th Annu. Conf. on Comput. Learning Theory, pages 125­133, New York, NY, 1999. ACM Press.
[26] Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. Mu¨ller, E. Sa¨ckinger, P. Simard, and V. Vapnik. Comparison of learning algorithms for handwritten digit recognition. In F. Fogelman-Soulie´ and P. Gallinari, editors, Proceedings ICANN'95 -- International Conference on Artificial Neural Networks, volume II, pages 53 ­ 60, Nanterre, France, 1995. EC2.
[27] D.G. Luenberger. Linear and Nonlinear Programming. Addison-Wesley Publishing Co., Reading, second edition, May 1984. Reprinted with corrections in May, 1989.
[28] R. Maclin and D. Opitz. An empirical evaluation of bagging and boosting. In Proc. of AAAI, 1997.
[29] O.L. Mangasarian. Linear and nonlinear separation of patterns by linear programming. Operations Research, 13:444­452, 1965.
[30] O.L. Mangasarian. Mathematical programming in data mining. Data Mining and Knowledge Discovery, 42(1):183­201, 1997.
[31] L. Mason, P. Bartlett, and J. Baxter. Improved generalization

through explicit optimization of margins. Technical report, Deparment of Systems Engineering, Australian National University, 1998. [32] L. Mason, J. Baxter, P.L. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In A.J. Smola, P.L. Bartlett, B. Scho¨lkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 221­247. MIT Press, Cambridge, MA, 1999. [33] L. Mosheyev and M. Zibulevsky. Penalty/barrier multiplier algorithm for semidefinite programming. Optimization Methods and Software, 1999. Accepted. [34] T. Onoda, G. Ra¨tsch, and K.-R. Mu¨ller. An asymptotical analysis and improvement of adaboost in the binary classification case. Journal of Japanese Society for AI, 15(2):287­296, 2000. In japanese. [35] G. Ra¨tsch, T. Onoda, and K.-R. Mu¨ller. Regularizing AdaBoost. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing Systems 11, pages 564­570. MIT Press, 1999. [36] G. Ra¨tsch, T. Onoda, and K.-R. Mu¨ller. Soft margins for AdaBoost. Machine Learning, 2000. In press. [37] G. Ra¨tsch, B. Scho¨kopf, A. Smola, S. Mika, T. Onoda, and K.-R. Mu¨ller. Robust ensemble learning. In A.J. Smola, P.L. Bartlett, B. Scho¨lkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 207­219. MIT Press, Cambridge, MA, 1999. [38] G. Ra¨tsch, B. Scho¨lkopf, S. Mika, and K.-R. Mu¨ller. Svm and boosting: One class. Submitted to NIPS'00, May 2000. [39] R.E. Schapire, Y. Freund, P. Bartlett, and W.S. Lee. Boosting the margin: a new explanation for the effectiveness of voting methods. In Proc. 14th International Conference on Machine Learning, pages 322­330. Morgan Kaufmann, 1997. [40] R.E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. In Colt'98, pages 80­91, 1998. [41] B. Scho¨lkopf, C.J.C. Burges, and A.J. Smola, editors. Advances in Kernel Methods ­ Support Vector Learning. MIT Press, 1999. [42] B. Scho¨lkopf, A. Smola, R. C. Williamson, and P.L. Bartlett. New support vector algorithms. Neural Computation, 12:1083 ­ 1121, 2000. [43] H. Schwenk and Y. Bengio. AdaBoosting neural networks. In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors, Proc. of the Int. Conf. on Artificial Neural Networks (ICANN'97), volume 1327 of LNCS, pages 967­972, Berlin, 1997. Springer. [44] Y. Singer. Leveraged vector machines. In S.A. Solla, T.K. Leen, and K.-R. Mu¨ller, editors, Advances in Neural Information Processing Systems 12, pages 610­616. MIT Press, 2000. [45] A. Smola, B. Scho¨lkopf, and G. Ra¨tsch. Linear programs for automatic accuracy control in regression. In Proceedings ICANN'99, Int. Conf. on Artificial Neural Networks, Berlin, 1999. Springer. [46] A. J. Smola. Learning with Kernels. PhD thesis, Technische Universita¨t Berlin, 1998. [47] A.J. Smola, P.L. Bartlett, B. Scho¨lkopf, and D. Schuurmans, editors. Advances in Large Margin Classifiers. MIT Press, 2000. [48] R.J. Vanderbei and D.F. Shanno. An interior point algorithm for nonconvex nonlinear porgramming. Technical Report SOR-97-21, Statistics and Operations Research,, Princeton University, 1997. [49] V.N. Vapnik. The nature of statistical learning theory. Springer Verlag, New York, 1995.

