Classiﬁcation of Hand Shape with Similar Contour for Sign Language

Recognition

Yutaka Yamada, Tadashi Matsuo, Nobutaka Shimada, and Yoshiaki Shirai

Ritsumeikan University, Computer Vision Lab

{yamada,matsuo}@i.ci.ritsumei.ac.jp, {shimada,shirai}@ci.ritsumei.ac.jp

1-1-1 Noji-higashi Kusatsu, Shiga, Japan

Abstract

with similar contour.

Recogntion of a sign language image sequence is chal-
lengeable because of the variety of hand shapes and hand
motions.

This paper proposes a method to classify hand shapes
for sign language recognition. To classify hand shapes
with similar contour, we use Histogram of Orientated Gra-
dient(HOG). We build binary hand shape classiﬁers. The
classiﬁers consist of weak classiﬁers with threshold. To se-
lect effective weak classiﬁers, we use adaboost algorithm.
Experimental result shows effectiveness of our method.

With HOG feature, we make binary classiﬁers. They
are linear combination consisting of weak classiﬁers. Weak
classiﬁers simply classify 2 classes by threshold.

In the second section, we describe how to extract hand
regions. In the third section, we describe how to calculate
HOG feature. In the fourth section, we describe classiﬁer.
In the ﬁfth section, experiment are described.

2 Feature Extraction

To classify hand shapes, hand regions must be extracted.

This section describe how hand regions are extracted.

1 Introduction

2.1 Extraction of Hand Regions

Sign language is used for communicating to people with
hearing difﬁculties. Recogntion of a sign language image
sequence is challengeable because of the variety of hand
shapes and hand motions. Sign language recognition from
a image sequence requires feature extraction and feature in-
terpretation. Generally features consist of the position, ve-
locity, and other data(ex. facial expression, nodding)[1]. To
extract these features from a sign language image sequence,
many ways have been proposed.

Okazawa et al proposed optical ﬂow for recognition of
sign language words[2]. Lee et al used motion vector to
recognize the words[3]. However, these methods don’t use
hand shape and orientation features of the words.

Kawahigashi et al used hand contour to represent hand
shape features[4]. However, their method uses only hand
orientation and stretched ﬁnger number. These features are
not enough to represent hand shape.

To begin with, we extract hand regions by Kawahigashi’s
method[4].
Histogram of Orientated Gradient(HOG)
feature[5] is calculated from the hand region. HOG repre-
sents not only edges of contour of hand but also inner edges
of hand. This is because HOG is effective to classify shapes

Face and hand regions are ﬁrst extracted using a model
of skin color. Since the colors of the face and hands change
depending on the environment and the subject, the range of
the skin color is determined from the initial image, where
the person region is extracted by background subtraction
and skin regions are extracted at ﬁxed positions of the per-
son region.

When hands and face regions overlap, they are separated
using the context (the previous an the succeeding frames).
First, the image of the face and hands just before overlap-
ping are saved as templates. Assuming that those images
do not change, hands and face regions are extracted by tem-
plate matching. This process is repeated until the overlap
is terminated. Next, assuming that hand shape changes dur-
ing overlapping, the hand region is extracted. The image of
face and hands just after overlapping are saved as templates
and the regions are similarly extracted by template match-
ing. This process is repeated backward to the beginning
of the overlapping. Then the timing of the shape change
is determined comparing the degree of matching in the for-
ward backward template matching. The process of template
matching is illustrated in Figure 1.

Figure 1. Template Matching

Figure 2. Example of HOG Feature. Left:Input
Image, Center:Gradient Image, Right:HOG
Image

3 HOG feature

From the hand region extracted in Section 2, we calcu-
late HOG feature. HOG feature was proposed by Dalal et
al[5]. It is calculated by counting the occurrences of gra-
dient orientation in locarized portion in an image. The oc-
currences are repeatedly normalized with some parts over-
lapped. How to calculate HOG feature is described below.
First,let L(x, y) the luminance of a point of an image.
Magnitude of gradient m(x, y) and orientation of it θ(x, y)
is given by,

m(x, y) =

(cid:113)
fx(x, y)2 + fy(x, y)2
θ(x, y) = tan−1 fy(x, y)
fx(x, y)
fx(x, y) = L(x + 1, y) − L(x − 1, y)
fy(x, y) = L(x, y + 1) − L(x, y − 1)

(1)

(2)

(3)
(4)

Second, from m(x, y) and θ(x, y), we make an orientation
histogram. An input image is divided into N × N pixel
patches. We call this image patch cell. The orientation his-
togram is calculated for each cell. Each histogram has 9
bins( 0◦ ≤ θ ≤ 180◦).
Third process is normalizing process. Each of M ×
M cells,this unit is called block, is normalized. When
9 dimension feature of cell(i, j) is expressed as Fij =
(f1, f2, ..., f9) and feature vector of k-th block Vk is ex-
pressed as Vk = (Fij, Fi+1,j, ..., Fi+M,j+M ). Normalized
feature vk is given by,

Vk(cid:112)(cid:107)Vk(cid:107)2 + 

vk =

( = 1.0)

(5)

In normalizing process, cell(i, j) is normalized more than
once by different blocks. We deﬁne feature vector x as
(v1, v2, ..., vM 2). When N = 5, M = 3, and an input image
is 50×50 pixel image, x has 8block× 8block× 81 = 5, 184
dimensions. We use this 5,184 dimension vector to clas-
sify hands. An example of Visualization of HOG feature is
shown in Figure 2

4 Classiﬁer with HOG Feature

To classify hand shape, we make binary classiﬁers us-
ing HOG features. We make a classiﬁer for each hand
shape. Final classiﬁer Hs(x) for hand shape s made by
AdaBoost algorithm is expressed as linear combination of
T weak classiﬁers :

T(cid:88)

Hs(x) =

αs,tgs,t(x)

(6)

t=1

where gs,t(x) is t th weak classiﬁer for hand shape s and
αs,t is a weight of gs,t(x), which is given by
1 − err rate + 
err rate + 

αs,t = ln

(7)

where err rate is weighted error rate of training data. The
ﬁnal result is sign(Hs(x)).

4.1 Generate Classiﬁer

By AdaBoosting algorithm, the ﬁnal classiﬁer is built.

How to make Hs(x) is shown below.
for t = 1,···
1. Generate 5,184 weak classiﬁer candidates, corre-
sponding to each dimension of x. g(i), the candidate
for i-th value of x is:

g(i) = sign(xi − thi)
where thi is a threshold for i-th value.
lowest error rate at the interval of 0.01.

(8)

thi gives the

2. Calculate evaluate value for each weak classiﬁer. Eval-

uate value Es,t(i) for g(i) in t th training is:

Es,t(i) =

n Ds,t(n)(g(i)yn < 0)

,

(9)

(cid:80)N

(cid:80)N

n Ds,t(i)

where N is number of training samples, Ds,t(n) is the
weight of n-th sample for t-th training of shape s, and
yn is the teacher signal of the n-th training sample.

Figure 3. Histogram of xi of Positive Samples
and Negative Samples for Training Images.

3. Deﬁne gs,t(x) as g(i),

where i = arg min

Es,t(i).

i

4. Calculate Ds,t+1(n). When Hs(x) makes wrong clas-
siﬁcation for n-th sample, Ds,t+1(i) gets bigger than
Ds,t(i).

5. Calculate error rate of Hs(x). when the error rate is
lower than δ, we stop training. δ is experimentally set
to 0.0003.

To make Hs(x), we need weight for each training sample.
Using yn,teacher signal of n-th sample, and Ds,t(n), weight
of i-th sample of (t)-th training for hand shape s, Ds,t+1(n)
is calculated by

Ds,t+1(n) = Ds,t(n) exp(−sign(yngs,t(x))αs,t)

(10)

Equation 10 makes it possible to select gs,t+1(x) which fo-
cuses on samples with false recognition. When t = 1 , every
Dt(n) has the same weight.

In this paper, we use HOG feature as x. If we select ap-
propriate HOG feature xi and set appropriate thi, gs,t(x)
works well. An example of histogram of xi for training im-
ages is shown in Figure 3. In this case, negative samples are
concentrated on from 0.0 to 0.1, so both classes are separa-
ble when thi is set around by 0.1.

5 Experiment

To show effectiveness of our method, we had a classi-
ﬁcation experiment. We use 28 hand shape classes for the
experiment. Figure 4 shows examples of hand shapes. Each
classiﬁer for the shape is trained with 100-300 positive sam-
ples and 5300-5500 negative samples. Classiﬁers consist of
20-40 weak classiﬁers.

We had an experiment of classifying 1500 test images.
The test images included words ”ten”, ”silk”, and ”you”,
which are difﬁcult to classify by the contour. An image
of ”ten” and other similar hand shape images are shown in
Figure 5. Each image has a similar contour but belongs to a
different class.

Figure 4. Examples of Each Class

5.1 Result

Experimental result is shown in Table 1. TP in the table
represents True Positive. This means only corresponding
classﬁer returns positive output. FP represents False Posi-
tive. This means another classiﬁer returns positive output
even if corresponding classiﬁer returns positive output. FN
represents False Negative. This means no classiﬁer returns
positive output. Images with false recognition are shown in
Figure 6.

5.2 Discussion

From the Table 1, difference of experimental results be-
tween the words is very little. This proves that our method
is effective to classify the shapes with similar contour.

We can also say that FN occurs much more often than
FP. This is caused by following reasons. One of the biggest

Figure 5. Hand shape with similar contour.
Right:”sister”,Center:”tomorrow”,Left:”ten”

(a)input

(b)correct class

False Positive Images

(c) result

False Negative Images Caused by Blur and Low Resolution

image A

image B

image C

False Negative Images Caused by Displacement of Hand

Figure 6. False Recognition

[3] M.Lee et al, ”Classiﬁcation of Sign Language using
Motion Vector”, Technical Report of IEICE , Vol.103
No.746 pp.65-70(2004)

[4] K. Kawahigashi, et al, ”Automatic Synthesis of
Training Data for Sign Language Recognition Us-
ing HMM”, Proc. 10th International Conference on
Compters Helping People with Special Needs, pp.623-
626(2006)

[5] N.Dalal et al,”Histogram of Oriented Gradient for Hu-
man Detection”, IEEE Computer Vision and Pattern
Recognition, Vol.1 pp.886-893(2005)

Table 1. recognition rate

input word

ten
silk
you
others

TP
0.758
0.773
0.866
0.759

FP

0.040
0.042
0.042
0.044

FN
0.202
0.185
0.091
0.197

reason is blur and low resolution like the second row of the
Figure 6. The other reason is displacement of hand regions
in test images.
In the third row of Figure 6, Image A is
very similar to B and C. These images are a little different
in the position of hand. However, this difference makes big
difference in HOG feature. Image A is TP and image B and
C are FN. To overcome this, we should use more samples
in training.

FP occurs in 4% of input images, but in almost all of
these cases, correct classiﬁer gives the biggest Hs(x), so
it is effective to regard the class with highest Hs(x) as a
classiﬁcation result.

6 Conclusions and Future Work

We proposed a method for classiﬁcation of hand shape.
Classiﬁers are made of coupling of weak learners with Ad-
aBoost training. From experimental result, our classiﬁers
classify images which have similar contour for other class.
However, our classiﬁer is not good at displacement of
hand in an object image. This problem has to be improved.
Now, our method is applied just for the classiﬁcation of
hand shapes, but it can be applied to detect and track hand.
We’ll make hand detector using these classiﬁers.

References

[1] Y.Nagashima, et al,”Computer Processing of Sign
the Journal of IEICE, Vol.984, No.4

Language”,
pp.320-324(2001)

[2] Y.Okazawa et al, ”Recognition of Global Motion
in Sign Language Based on Optical Flow”, Techni-
cal Report of IEICE PRMU Vol.104 No.317 pp.39-
44(200)

