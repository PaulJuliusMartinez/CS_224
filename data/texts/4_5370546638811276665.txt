Dynamic Programming Algorithms for Maximum Likelihood Decoding
by Kevin Geoffrey Kochanek A.B. in Physics, Cornell University, 1991 M.S. in Physics, University of Illinois at Urbana-Champaign, 1993 Sc.M. in Applied Mathematics, Brown University, 1995
Thesis Submitted in partial fulfillment of the requirements for
the Degree of Doctor of Philosophy in Division of Applied Mathematics at Brown University
May 1998

Abstract of "Dynamic Programming Algorithms for Maximum Likelihood Decoding," by Kevin Geoffrey Kochanek, Ph.D., Brown University, May 1998
The Viterbi algorithm is the traditional prototype dynamic programming algorithm for maximum likelihood decoding. Seen from the perspective of formal language theory, this algorithm recursively parses a trellis code's regular grammar. This thesis discusses generalized Viterbi algorithms for the maximum likelihood decoding of codes generated by context-free grammars and transmitted across either memoryless or Markov communications channels. Among the codes representable by context-free grammars are iterated squaring constructions--including the Reed­Muller codes. Two additional strategies are introduced for handling large Reed­Muller-like codes. First, by systematically discarding information bits, a code's grammatical and decoding complexities can be reduced to manageable levels without seriously reducing its information capacity. Second, a coarse-to-fine dynamic programming algorithm for the maximum likelihood decoding of Reed­Muller-like codes is presented; this algorithm almost uniformly outperforms the Viterbi algorithm.

c Copyright by
Kevin Geoffrey Kochanek 1998

This dissertation by Kevin Geoffrey Kochanek is accepted in its present form by Division of Applied Mathematics as satisfying the dissertation requirement for the degree of Doctor of Philosophy

Date . . . . . . . . . . . . . . . . . . . . . . . . .

..................................................... Stuart Geman

Recommended to the Graduate Council

Date . . . . . . . . . . . . . . . . . . . . . . . . .

..................................................... David Mumford

Date . . . . . . . . . . . . . . . . . . . . . . . . .

..................................................... Donald McClure

Approved by the Graduate Council

Date . . . . . . . . . . . . . . . . . . . . . . . . .

.....................................................

ii

The Vita of Kevin Geoffrey Kochanek Kevin Geoffrey Kochanek was born in State College, PA on March 31, 1970. He attended Georgetown Day High School in Washington, DC from 1983 to 1987. In 1991, he graduated from Cornell University summa cum laude in physics. After earning a masters in physics from the University of Illinois at Urbana-Champaign in 1993, he transferred into the applied mathematics program at Brown University. He received a masters in applied mathematics in 1995 and defended this Ph.D. thesis on October 16, 1997.
iii

Preface
Coding theorists have long recognized dynamic programming as a powerful tool for performing exact maximum likelihood decoding. The most widely examined dynamic programming application, known as the Viterbi algorithm, decodes a given code by computing the shortest length path through its associated trellis diagram. Seen from the perspective of formal language theory, the Viterbi algorithm recursively parses a code's regular grammar. By further exploring the relationship between codes and formal grammars, this thesis aims to extend the applicability of dynamic programming techniques within coding theory.
Chapter 1 provides a brief introduction to the fundamental concepts from coding theory and formal language theory that underpin the remainder of the thesis. After discussing the structure of error correcting codes and the optimality of maximum likelihood decoding, we introduce the Viterbi algorithm and its grammatical interpretation. We also introduce the family of Reed­Muller codes, our canonical example of codes derivable from context-free grammars.
Chapter 2 presents generalized Viterbi algorithms for the class of codes derived from context-free grammars. Here we discuss maximum likelihood decoding algorithms for both memoryless and Markov communications channels, simultaneously introducing the posterior probability as a useful reliability statistic.
In chapter 3, we construct codes generated by context-free grammars to which these algorithms may be applied. Reinterpreting Forney's iterated squaring construction in grammatical terms, we develop a large class of such codes, including the widely known Reed­Muller codes. Moreover, we relate the computational complexity of our decoding algorithms to the corresponding grammatical complexity of the given codes. Since many of the larger Reed­Muller codes are effectively undecodable (even using dynamic programming methods), we construct a family of thinned Reed­Muller codes whose grammatical and decoding complexities are strictly controlled.
Chapter 4 presents a coarse-to-fine dynamic programming algorithm for the maximum likelihood decoding of thinned Reed­Muller codes. This coarse-to-fine procedure computes the maximum likelihood codeword by applying the standard dynamic programming approach to a sequence of codes that in some sense approximate the original code. Its implementation is highly dependent on the particular grammatical structure of these thinned
iv

Reed­Muller codes. Finally, Chapter 5 is a conclusion that combines an analysis of simulated decoding trials
with a discussion of the important unifying themes of this thesis.
v

Acknowledgments
Although the process of researching and writing a Ph.D. thesis is largely an individual effort, the substance and character of the final product in fact reveal the contributions of the many people who made it possible. First and foremost among them are my parents who have always fostered my intellectual and creative development. I dedicate this thesis to you, though you will find it exceedingly difficult to read.
I have also benefited enormously from the informal yet rigorous atmosphere of the Division of Applied Mathematics. In particular, I would like to thank my adviser Stuart Geman who provided a near perfect combination of guidance and autonomy. My committee members Donald McClure, David Mumford, and Basilis Gidas were also an invaluable source of constructive criticism and advice. Finally, I am grateful to Wendell Fleming for introducing me to probability theory and tutoring me in stochastic processes.
In addition, there are a number of people from outside the Division who have influenced the course of my graduate studies. Chief among them is my undergraduate adviser Neil Ashcroft who has for many years been a role model and mentor. Paul Goldbart and Eduardo Fradkin from Illinois kindly aided my transfer from physics to applied mathematics. Sidney Winter generously supported a memorable summer of studying management theory at the Wharton School. Finally, I would also like to thank Ronel Elul and Robert Ashcroft for introducing me to financial economics.
Of course, without the generous financial support of the Division and Brown University, this thesis could not have been written. Finally, I am grateful to Jean Radican, Laura Leddy, Roselyn Winterbottom, and Trudee Trudell for cheerfully helping me manage a host of administrative details.
vi

Contents

Preface

iv

Acknowledgments

vi

1 Preliminaries 1.1 Error Correcting Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.2 Reed­Muller Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.3 Convolutional Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Maximum Likelihood Decoding . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 The Viterbi Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1 2 2 4 5 6 8 9

2 Maximum Likelihood Decoding of CFG Representable Codes 2.1 A Grammatical Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Memoryless Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Markov Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11 12 13 15

3 Construction of CFG Representable Codes 3.1 The Squaring Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Iterated Squaring Constructions . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Reed­Muller Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Bent Reed­Muller Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Counting States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Thinned Reed­Muller Grammars . . . . . . . . . . . . . . . . . . . . . . . .

20 21 22 24 29 33 38

vii

4 A CTFDP Algorithm for Maximum Likelihood Decoding 4.1 Coarse-to-Fine Dynamic Programming . . . . . . . . . . . . . . . . . . . . . 4.2 Super-States for Thinned Reed­Muller Codes . . . . . . . . . . . . . . . . . 4.3 A CTFDP Decoding Algorithm for Thinned Reed­Muller Codes . . . . . .

42 43 44 49

5 Discussion 5.1 Synopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 DP and CTFDP Decoding Performance . . . . . . . . . . . . . . . . . . . . 5.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54 55 56 58

A Decoding Simulations

60

viii

Chapter 1
Preliminaries
1

1.1 Error Correcting Codes
1.1.1 Introduction
A code is simply a mathematical structure designed to store information. More formally, a binary (N, M ) code C is a set of M binary strings of length N . A given stream of source data (perhaps a satellite image) represented by a finite string of information bits is encoded as a sequence of codewords with each successive substring of length log2 M selecting an associated codeword from C according to a known rule. As we shall see in Section 1.2, noisy communications channels will typically corrupt transmitted bit streams; thus, any receiver must use a decoding scheme to infer the transmitted codeword from a channel's potentially garbled output. To facilitate both efficient and reliable decoding, commonly used codes exhibit ample algebraic and geometric structure.
In order to understand the role of geometry in error correcting codes, we introduce the following norm and metric on the space ZN2 of binary N-tuples. For strings x and y, the (Hamming) weight w(x) of x is the number of non-zero bits in x and the (Hamming) distance d(x, y) between x and y is the weight of the string x - y. If S and T are sets of strings, the subset distance d(S, T ) is the minimum distance between two elements of S and T . Finally, the minimum distance d(S) of a set S is the minimum distance between two distinct elements of S (which can in general differ from the minimum weight w(S) of S, the smallest non-zero weight of any string in S). An (N, M, d) code is an (N, M ) code with minimum distance d.
Often the minimum distance of a code alone determines its capacity to detect and correct transmission errors [10]. Consider a communications channel (such as the binary symmetric channel introduced in Section 1.2) that can corrupt a transmitted codeword by flipping its component bits from 0 to 1 or vice-versa. Imagine sending a codeword c from an (N, M, d) code C across this channel. If the channel flips between 1 and d - 1 bits of c, the received word will not be a codeword--the codeword closest to c differs in at least d bits. In other words, C detects up to d - 1 errors. Now suppose we adopt the following intuitive decoding scheme (seen in Section 1.2 as optimal in some circumstances): upon receiving the string d (not necessarily a codeword), infer that the transmitted codeword is that element of C which is closest to d (in Hamming distance). If the channel flips between 1 and (d - 1)/2 bits, this decoding scheme correctly yields the sent codeword c. Thus, C also corrects up to
2

(d - 1)/2 errors.

Among the most widespread algebraic structures occurring in coding theory are groups [10].

A linear binary [N, K, d] code C is a set of 2K binary N-tuples with minimum distance d (i.e.

an (N, 2K, d) code) that forms a group under mod-2 vector addition. Being a K-dimensional

subspace of the linear space Z2N , C is uniquely characterized by a K element basis of generators GC = {gk|1  k  K} in the sense that any element of C can be expressed as a linear combination of these generators. The generator matrix GC may be equivalently viewed as either a set of K generators or as a K × N matrix with the generators ordered by row.

Symbolically,

K
C = L(GC ) = { akgk|a Z2K } = {aGC |a Z2K }.
k=1

Generator matrices inherit an algebra from their associated linear codes [4]. The direct sum

C1 + C2 of two codes C1 and C2, generated respectively by G1 and G2, has the generator matrix G1+G2 = G1 G2. In this algebra, subtraction is not defined and the zero generator
matrix is the empty set.

Codes that also happen to be groups exhibit a rich structure. If C is a subgroup of C,

the distinct cosets of C partition the group C itself:

C=

C + c = C + [C/C ],

c [C/C ]

where [C/C ] is a complete set of coset representatives and + denotes a direct sum. If C is generated by GC  GC and [C/C ] is chosen to be generated by GC/C = GC \GC , then the corresponding partition of C's generator matrix is

GC = GC + GC/C .
Furthermore, since C is itself linear, the minimum distance of each coset C + c in this partition equals d(C ). In general, if C1 > C2 > · · · > Cm is a nested sequence of subgroups of C having minimum distances d(C)  d(C1)  · · ·  d(Cm), then C can be successively partitioned into cosets--all sharing the same minimum distance at any given level of refinement.
We now introduce two examples of error correcting codes that feature prominently in our discussion of maximum likelihood decoding.
3

1.1.2 Reed­Muller Codes

The family of Reed­Muller codes will emerge in Chapter 3 as the canonical example of

codes derivable from context-free grammars. The following construction was introduced by

Forney [4]. Consider the following alternative basis for Z22: G(2,2) = {g0, g1} where g0 = [1, 0] and
g1 = [1, 1]. Recall that G(2,2) can be considered to be a 2×2 generator matrix for the code Z22 with rows g0 and g1. Now define the N × N (N = 2n) matrix G(N,N) = G(n2,2) = in=1G(2,2), the n-fold Kronecker product of G(2,2) with itself. (If A and B are binary matrices of
dimensions m × n and p × q respectively, then A  B is the mp × nq matrix obtained by

replacing every element aij in A with the matrix aijB.) As in the case N = 2, G(N,N) is both a basis and generator matrix for Z2N ; its rows are simply all 2n n-fold Kronecker products of g0 and g1. Since w(g1  x) = 2w(x) and w(g0  x) = w(x) for any vector x, the weight of such a row is 2n-r where r (n - r) is the number of g0's (g1's) in the

corresponding Kronecker product.

The generator matrices defining the Reed­Muller codes are constructed by selecting

specific subsets of rows from G(N,N).

Let GRM (r, n) be the set of

n r

rows of weight

2n-r (i.e. all n-fold Kronecker products of r g0's and (n - r) g1's) drawn from G(N,N).

Moreover, let GRM (r, n) =

r s=0

GRM (s, n)

be

the

set

of

all

rows

from

G(N,N )

of

weight

2n-r or greater. The Reed­Muller code RM(r,n) is defined to be the binary block code (of

length N = 2n, dimension K =

r s=0

n s

, and (as shown in section 3.2) minimum distance

d = 2n-r) generated by the matrix GRM (r, n):

RM (r, n) = L(GRM (r, n)).

Furthermore, GRM (r, n) can be chosen as the generator matrix for the group of coset representatives [RM (r, n)/RM (r - 1, n)]:

[RM (r, n)/RM (r - 1, n)] = L(GRM (r, n)). For completeness, define RM (-1, n) = {0}.

4

1.1.3 Convolutional Codes
Convolutional codes provide the second example of a grammatically derived family of codes that is amenable to decoding by dynamic programming.
A convolutional code is most easily defined as the output of a deterministic finite automaton [3]. At discrete time intervals, a rate k/n binary convolutional encoder accepts a k-bit input sequence and generates an n-bit (n  k) output sequence depending only on the current state of the machine and the input string. Upon accepting an input string, the automaton makes a transition from it's current state to one of M = 2k input-determined successor states. Typically, a convolutional code is terminated at time T -  by requiring the machine to accept a fixed input sequence for  time units, thus forcing its state to a given end-state at time T . The resulting set of output strings, an (nT, M T -) code, is a rate k/n convolutional code.
The output of such a convolutional encoder--a convolutional codeword-- can be represented as a path through a branching graph [3]. For a code of length nT , the graph has n sections of branches connecting n + 1 time-indexed sets of nodes ordered from left to right (0 to n). A node at time t is labeled according to the corresponding state of the machine, whereas a branch between two nodes at successive times is labeled by the n-bit output sequence associated with the allowed transition. At the extreme left lies the sole start-node (representing the encoder in its start-state at time 0) from which M branches emerge, each corresponding to a possible input string of k-bits. These M branches terminate in M time 1 nodes which in turn branch M times, and so on. Thus, at time t  T -  there are M t nodes in one-to-one correspondence with the set of all possible inputs to date; for T -   t  T , the number of nodes remains constant at M T - and the graph branches singly--reflecting the uniform terminal input stream.
The standard graphical representation of a convolutional code, called a trellis diagram, eliminates the inherent redundancy of the above branching graph [3]. Since the number of nodes grows exponentially with time, at some point the number of nodes exceeds the cardinality S of the automaton's state space; thus, many nodes, while representing different sequences of inputs, correspond to identical states of the encoder. Since the action of the machine depends only on its current state (and a sequence of future inputs), the subtrees emerging from different nodes in identical states may be merged. The graph resulting from
5

this consolidation, a trellis diagram, has at most 2 + (n - 1)S interconnected nodes and terminates at a single end-node at time n.

1.2 Maximum Likelihood Decoding

Having discussed various static features of error correcting codes, consider now the problem

of transmission. Physically, a communications channel is a device that accepts codewords

from a code C and emits potentially garbled words belonging to an output set D. If C is

an (N, M ) code, D will typically be a subset of binary or real-valued N-tuples. One can

therefore model the channel as a conditional probability distribution on the output set with

the channel probability p(d|c) denoting the conditional probability that d D is received,

given that c C is sent. If D is not a discrete space, assume that the channel probability

distribution admits a conditional density of the form p(d|c). The conclusions of this section

do not depend on this distinction.

A channel is termed memoryless if it independently corrupts the bits of a transmitted

codeword:

N
p(d|c) = p(di|ci)
i=1

The coding theory literature features a number of memoryless channels of which the most

common are:

1. The binary symmetric channel. D = ZN2 : p(1|0) = p(0|1) = p. 2. The binary asymmetric channel. D = ZN2 : p(1|0) = p, p(0|1) = q. 3. The additive gaussian noise channel. D = RN : di = ci + ni, ni  N (0, 2).

4. The bipolar channel with additive gaussian noise. D = RN : di = 2ci - 1 + ni, ni  N (0, 2).

Given any output d D of a noisy communications channel, a decoding scheme f : D  C is a function that infers the corresponding transmitted codeword f (d). If f (d) is indeed the transmitted codeword, the scheme has corrected any transmission errors; otherwise, it has made a decoding error. An optimal decoding scheme minimizes the probability of making decoding errors, thereby maximizing the probability of correcting errors.

6

Following the standard Bayesian approach [10], introduce a prior probability distribution p(c) on the channel's input--the codebook C. By Bayes rule, the posterior probability that c C was in fact sent is

p(c|d)

=

p(d, c) p(d)

=

c

p(d|c)p(c) C p(d|c)p(c)

.

P (c|d) is also called the backwards channel probability. The optimal decoding scheme that maximizes the probability of correcting errors is clearly

c^(d) = arg max p(c|d).
cC

Having no particular knowledge concerning the channel inputs, impose the uniform prior p(c) = 1/M on the codebook C. The posterior probability becomes

p(c|d) =

c

p(d|c) C p(d|c)

,

simplifying the decoding scheme accordingly:

(1.2.1)

c^(d) = arg max p(d|c).
cC
This procedure is called maximum likelihood decoding (or soft decoding), for the codeword c^(d) is the channel input that makes the output d most likely.
To illustrate this decoding approach, consider a binary symmetric channel with bit-error probability p  1/2. If the codeword c is transmitted across this channel, producing the output word d, the number of bit-errors incurred during transmission equals the distance d(c, d). The resulting likelihood function,

p(d|c) = pd(c,d)(1 - p)N-d(c,d),

is maximized when d(c, d) is a minimum [10]; to decode the received word d, one simply selects the nearest codeword c^(d). This common scheme is termed minimum distance decoding (or hard decoding). However, for most channels maximum likelihood and minimum distance decoding do not coincide.
For all but the smallest codes, sequentially searching a codebook for the optimal element

7

under a decoding scheme is impossible. However, for a variety of important codes and channels, maximum likelihood decoding can be formulated as a dynamic programming problem.

1.3 The Viterbi Algorithm

The Viterbi algorithm [11] was introduced in 1967 to expedite the decoding of convolutional

codes. In fact, it can be applied to all trellis-based codes (including linear block codes [12]).

Consider the maximum likelihood decoding of a length N = nT convolutional codeword

transmitted through a memoryless channel. Taking the negative logarithm of the likelihood

converts the decoding problem into the following minimization of a bit-wise additive cost

function:

N

c^(d)

=

arg

min
cC

i=1

-

ln

p(di|ci).

Now introduce a metric on the branches of the code's trellis diagram [3]. If {di|n(t - 1) < i  nt} is the length n output of the channel in the time interval (t - 1, t] and {ci|n(t - 1) < i  nt} is a possible channel input associated with a particular branch in the t-th section

of the trellis diagram, assign that branch a "length" equal to

nt
- ln p(di|ci).
i=n(t-1)+1

Maximum likelihood decoding is thereby reduced to finding the shortest "length" path through the code's trellis [2].
Viterbi suggested the following dynamic programming approach for finding this shortest path [3].

1. Set t = 0 and initialize the length of the start node to zero.

2. For each node (state) s at depth (time) t + 1, consider the set of predecessors at depth t--nodes for which there is a connecting branch to s. For each predecessor, compute the sum of the predecessor's length and the length of the branch connecting it to s. Label the node s with a length equal to the smallest of these sums and by the bit sequence corresponding to the shortest length path to s--the shortest path label of the minimizing predecessor concatenated with the label of its connecting branch.

8

3. If t < T , increment t and goto 2; otherwise stop computing and return the shortest length path to the final node, the maximum likelihood estimate for the transmitted codeword.
The validity of this method--called the Viterbi algorithm--is evident once one realizes that the shortest path to any given node must be the extension of the shortest length path to one of its predecessor nodes.
The computational complexity of Viterbi's algorithm is easily established. For a trellis diagram with n sections and at most S nodes at each time step, 2nS2 is a loose upper bound on the number of real number operations involved. For each node at a given depth, computing the optimal extension requires p additions and p - 1 comparisons, where p--the number of predecessor nodes--is necessarily less than or equal to S. Provided M t exceeds S for most values of t, the Viterbi algorithm without question outperforms a sequential search of the size M T - convolutional codebook.
1.4 Context-Free Grammars
In this section, we introduce some concepts from the theory of formal languages [6] and reformulate the Viterbi algorithm accordingly, replacing unwieldy trellis diagrams with sets of simple grammatical rules.
A context-free grammar (CFG) is a four-tuple G = (V, T, P, S) consisting of two finite sets of symbols--T of terminals and V of variables (including S, the start symbol )--and a finite set P of productions, grammatical rules of the form A  , where A is a variable and  is a string of symbols from V T . G generates a context-free language L(G), a set of strings over the alphabet T of terminals each element of which can be derived from S (i.e. constructed by applying a finite sequence of productions to the start symbol). Context-free languages lacking the empty string (i.e. prohibiting any null productions) can be generated by grammars in Chomsky normal form with all productions of the type A  BC or A   (A,B,C being variable and  being terminal). Thus, the derivation of any string in such a language can be viewed as a binary tree with each leaf extended by a single additional branch and vertex; each interior vertex represents a variable (the tree root being S itself), each exterior vertex a terminal, and each branching a production.
A special subclass of context-free languages within Chomsky's linguistic hierarchy is
9

the class of regular languages. A regular language is generated by a regular grammar, a context-free grammar that is right-linear (having productions of the form A  wB or A  w where A and B are variables and w is a string of terminals). The derivation of a string belonging to a regular language can be viewed as a linear graph, a finite sequence of connected node-branch pairs in which each node represents a variable, each branch a string of terminals, and each pair a production.
Without rigorously demonstrating the equivalence of finite automata and regular languages, we clearly see that trellis-codes (e.g. convolutional codes) are generated by regular grammars over the binary alphabet. Each path through a trellis diagram is a right linear derivation; state labels of trellis nodes correspond to grammatical variables, whereas nodebranch pairs correspond to productions. The codebook, the aggregate of all possible paths through the trellis diagram--a collection of right-linearly derived strings--is thus a regular language. Similarly, from the grammatical point of view, the Viterbi algorithm performs maximum likelihood decoding by recursively parsing the code's regular grammar. To find the minimal "length" string ending in the variable A at time t+1, it computes the minimum "length" of all strings ending in the variable A that are derivable (in a single production) from the set of minimum "length" strings terminating in an arbitrary variable at time t.
Having recognized the equivalence of maximum likelihood decoding by the Viterbi algorithm and parsing regular grammars by dynamic programming, in the following chapters we generalize Viterbi's approach to the class of codes derivable from context-free grammars.
10

Chapter 2
Maximum Likelihood Decoding of CFG Representable Codes
11

In this chapter, we develop generalized Viterbi algorithms for the maximum likelihood decoding of codes derived from context-free grammars and transmitted across either memoryless or Markov communications channels. Moreover, we introduce similar algorithms for computing an important reliability statistic--the posterior probability that a decoded word indeed matches the transmitted codeword.
2.1 A Grammatical Template
A length N = 2p binary block code C generated by a context-free grammar in Chomsky normal form can be viewed as a collection of binary derivation trees each having nodes occupied by symbols and branches representing productions. Our general template [5] for such codes is therefore a binary tree with levels labeled 0  l  p each containing 2p-l nodes. Each node at level l may exist in any one of the states {0, . . . , N (l) - 1} representing allowed grammatical symbols. At level 0, the state of the i-th node represents the value of a codeword's i-th bit--N (0) = 2. Moreover, the uniqueness of a grammar's start symbol requires N (p) = 1. Within this framework, the set of allowed productions from a level l node in state j is Ij(l)  {0, . . . , N (l-1) - 1}2, a subset of state-pairs permitted its level (l - 1) daughter nodes. Obviously, Ij(0) =  for j = 0, 1. The collections {N (l)|0  l  p} and {Ij(l)|0  l  p, 0  j  N (l) - 1} uniquely determine both the code C and its associated context-free grammar.
To encode a stream of source data, one systematically selects productions (to be applied to the start symbol) according to successive subsequences of information bits. For the family of codes constructed in Chapter 3, the number of productions available at at node depends only on its level, taking the form
|I(l)| = |Ij(l)| = 2Q(l)
for 0  j  N (l) - 1. The first Q(p) information bits select a single production from the root node at level p in state 0, the state-pair (c(1p-1), c2(p-1)) at level p - 1. In general, 2p-lQ(l) bits select 2p-l independent productions from a 2p-l-tuple c(l) of states at level l, yielding a 2p-l+1-tuple c(l-1) of states at level l - 1. At each level of the tree-template, there is a grammatically permitted subset C(l) containing 2p-l-tuples of level l states; C(0) is itself the
12

codebook C. Thus, the number of information bits encoded within a single codeword is
p
I(p, {Q(l)}) = 2p-lQ(l).
l=1
2.2 Memoryless Channels

Suppose a codeword from C is transmitted across a memoryless communications channel (in

which bits are independently corrupted). The maximum likelihood decoding of the received

word d is the codeword

2p

c^

=

arg

min
cC

i=1

-

ln

p(di|ci)

that minimizes a bit-wise additive cost function. As in the regular grammar (or Viterbi)

case, this optimization problem admits a dynamic programming solution [5].

Each leaf--or level 0 node--of the tree-template for C contributes a state-dependent

cost to the total cost of a codeword:

Ci0(j) = - ln p(di|ci = j),
for 1  i  2p, 0  j  1. Now consider a permissible assignment of states c(1) C(1) at level 1 and the corresponding subset C(0)(c(1)) of codewords derivable from c(1) in 2p-1 independent productions. Evidently, only the minimum cost codeword c(0)(c(1)) within this restricted subset could possibly be a candidate for the overall cost minimum. Moreover, c(0)(c(1)) can be computed in 2p-1 independent minimizations; for each level 1 node, there is a production that minimizes the sum of the costs of its daughter nodes. Assigning this cost sum to the level 1 node in its given state defines an additive cost function on C(1) which can in turn be minimized for any sequence of level 2 states. Iterating this procedure ultimately yields c^.
With the nodal level 0 cost functions as initial data, the dynamic programming algorithm for maximum likelihood decoding proceeds as follows. For each level 1  l  p, recursively define the optimal productions

Pil(j) = arg (k,km) inIj(l)[C2l-i-11(k) + C2l-i 1(k )]

13

and the optimal costs

Cil(j)

=

min
(k,k ) Ij(l)

[C2l-i-11

(k)

+

C2l-i 1(k

)]

= C2l-i-11(Pil(j)1) + C2l-i 1(Pil(j)2)

for 1  i  2p-l and 0  j  N (l) - 1. Thus, Pil(j) is the leading production in the derivation of the minimum cost (Cil(j)) substring derived from the ith level l node in state j. The maximum likelihood codeword c^ has cost (negative log-likelihood) C1p(0) and is derived by applying the set {Pil(j)|1  i  2p-l, 0  j  N (l) - 1, 1  l  p} of optimal
productions to the level p start state 0:

c^(p) = 0  · · ·  c^(l)  · · ·  c^(0) = c^,

where the lth optimal state sequence c^(l) C(l) is given by the relation

(c^2(li)-1, c^2(li)) = Pil+1(c^(il+1))

for 1  i  2p-l-1. Like Viterbi's algorithm, this dynamic programming procedure is significantly faster
than a sequential search of the codebook. For every possible state at each positive level node in the tree, the algorithm performs an optimization over |I(l)| productions (requiring |I(l)| real number additions and |I(l)| - 1 real number comparisons--hereafter referred to as real number operations). Thus, the number of decoding operations for a CFG representable code with parameters {N (l), |I(l)||0  l  p} is

p
O(p, {N (l)}, {|I(l)|}) = 2p-lN (l)(2|I(l)| - 1).
l=1

(2.2.1)

For example, the thinned Reed­Muller code RM (8)(6, 10), a linear [1024, 440, 16] code introduced in Chapter 3, is decodable in 221 real number operations.
The dynamic programming approach can also be used to determine the reliability of a decoding scheme [5]. Assuming a uniform prior on the codebook C, the posterior probability p(c^|d) that the decoded word c^ was in fact transmitted, given that d was received is

14

expressed in equation 1.2.1. For a memoryless channel, it is more conveniently written:

p(c^|d)-1 =
c

C

2p i=1

p(di|ci) p(di|c^i)

.

Now observe that the right-hand sum can be computed by a dynamic programming algorithm virtually identical to the decoding procedure previously discussed. The nodal state-dependent level 0 contributions to the inverse posterior probability are

Si0(j)

=

p(di|ci = j) p(di|c^i)

for 1  i  2p, 0  j  1. For each level 1  l  p, recursively define

Sil(j) =

S2l-i-11(k)S2l-i 1(k ).

(k,k ) Ij(l)

The posterior probability

p(c^|d) = S1p(0)-1

is therefore computable in as many real number operations (2.2.1) as the maximum likelihood codeword itself.

2.3 Markov Channels
Although it is ubiquitous in the coding theory literature, the memoryless channel, which corrupts individual bits independently, is an unrealistic model for many actual communications channels. Often, real channel errors appear in bursts, corrupting entire sequences of bits [10]. The simplest mathematical model of a transmission line subject to burst noise is a Markov channel in which bit-errors are more likely than non-errors to succeed a given error. Having accepted an input c C, such a channel produces the Markov process d as output, according to the probability
2p
p(d|c) = pi(di|ci, ci-1, di-1).
i=1
Note that in the i = 1 term the dummy parameters c0 and d0 should be ignored. For clarity, we propose a general class of Markov channel models for which the channel proba-
15

bility necessarily factors as above: given a channel input c, invertible observation functions Oi(ci, ·), and a hidden Markov process {ei|1  i  2p}, we define the channel output d by the relation di = Oi(ci, ei). The maximum likelihood decoding of d is the codeword

2p

c^

=

arg

min
cC

i=1

-

ln

pi(di|ci,

ci-1,

di-1).

In contrast to the memoryless channel problem addressed in section 2.2, each level 0 adjacent pair of leaves in the tree-template contributes a state-dependent cost to the total cost of a codeword. However, by again minimizing over the state-spaces of judiciously chosen subsets of nodes, one can systematically reduce the problem level by level.
Consider a peculiar subset C of the code C, those codewords derivable from a permissible assignment of level 2 states c(2) C(2), yet sharing a given fixed set of bits {ci|i mod 4 = 0, 1}. Codewords within C are composed of 2p-2 concatenated 4-bit substrings in which the first and fourth flanking bits are fixed while the second and third internal bits remain variable. The total cost of such a codeword is analogously partitioned into 2p-2 variable contributions--each the sum of the costs associated with the three adjacent state-pairs contained in a substring--and a fixed contribution--the sum of the interaction costs of adjacent pairs of flanking bits. Having fixed the cost of interactions between substrings by fixing their flanking bits, the minimum cost codeword in C --and candidate for the overall minimizer--can be computed in 2p-2 independent minimizations; for each trio, consisting of a level 2 node and the two bits flanking its associated substring, there is a derivation of interior bits that minimizes the variable substring cost. Assigning this minimum cost to the trio and defining the interaction cost of adjacent trios to be the interaction cost of their corresponding flanking bits effectively reduces the original problem over C to a formally identical problem over the significantly smaller set of flanking bits. Iterating this procedure ultimately yields c^.
The dynamic programming algorithm for the maximum likelihood decoding of the output d of a Markov communications channel is formally presented as follows. First, for each state 0  j  N (l) - 1 at level 2  l  p, establish a set of auxiliary productions,

I~j(l) = {(k, k ) Z22|(k, , , k )

I~p(l1-1) × I~p(l2-1),

p Ij(l)

for some ,  Z2}

16

with I~j(1) = Ij(1) for 0  j  N (1) - 1. I~j(l) represents the set of flanking bits that can be grammatically derived from the level l state j. Second, assemble the algorithm's initial data--level 0 and 1 state-dependent costs--respectively defined by:

Q0i (j, j ) = - ln pi(di|ci = j , ci-1 = j, di-1) for 1  i  2p, 0  j, j  1, and

Q1i (j, a) = Q02i(a1, a2)
for a I~j(1), 0  j  N (1) - 1, and 1  i  2p-1. Third, for each level 2  l  p, each node 1  i  2p-l, each state 0  j  N (l) - 1, each
auxiliary production a I~j(l), each ordinary production q Ij(l), and each interior sequence b Z22, recursively define the objective function
Kil(j, a, q, b) = Ql2-i-11(q1, (a1, b1)) + Ql2-i 1(q2, (b2, a2)) +Q(0i-1)2l+2l-1+1(b1, b2),
the optimal production

Pil(j, a) = arg
{(q,b)

min
Ij(l) ×Z22 |(a1 ,b1 ,b2 ,a2 )

I~q(1l-1)×I~q(2l-1)} Kil(j, a, q, b),

and the optimal cost

Qli(j, a)

=

{(q,b)

min
Ij(l)×Z22|(a1,b1,b2,a2)

I~q(1l-1)×I~q(2l-1)} Kil(j, a, q, b)

= Kil(j, a, q, b)|Pil(j,a).

Fourth, compute the optimal flanking bits for the level p start state

a(p-1)

=

arg

min
a I~0(p)

Q10(0,

a1)

+

Q1p(0,

a)

17

and the overall minimum cost

Q

=

min
a I~0(p)

Q10(0,

a1

)

+

Q1p(0,

a)

= Q10(0, a1(p-1)) + Q1p(0, a(p-1)).

Finally, apply the set {Pil(j, a)|2  l  p, 1  i  2p-l, 0  j  N (l) - 1, a I~j(l)} of
optimal productions to the optimal start trio (0, a(p-1)) to generate the maximum likelihood
codeword c^ of cost (negative log-likelihood) Q. Given the optimal state sequence {c(il)|1  i  2p-l} at level l and the associated optimal sequence {ai(l-1)|1  i  2p-l+1} of flanking bits--with the pair (a2(li--11), a(2li-1)) flanking ci(l), the optimal state and flanking sequences {ci(l-1)|1  i  2p-l+1} and {a(il-2)|1  i  2p-l+2} are computed for p  l  2 as follows. If for 1  i  2p-l Pil(ci(l), (a2(li--11), a2(li-1))) equals (q1, q2, b1, b2) Ic(i(ll)) × Z22, assign:

c(2li--11) = q1,

c2(li-1) = q2,

a(4li--22) = b1,

a4(li--21) = b2,

a4(li--23) = a(2li--11), a(4li-2) = a2(li-1).

At level 0, the optimal flanking sequence is c^ itself: c^i = a(i0), 1  i  2p. As before, the computational complexity of this dynamic programming algorithm for

decoding the output of a Markov communications channel can be expressed in terms of the

grammar's parameters {N (l), |I(l)||0  l  p}. Comparing the Markov algorithm to the

memoryless one, we see that the effective sizes of the state and production spaces in the

former case are four times that of the latter. Moreover, each step requires two (rather than

one) real number additions. Thus, the number of decoding operations for a Markov channel

is

p

OMarkov(p, {N (l)}, {|I(l)|}) = 2p-l4N (l)(12|I(l)| - 1),

(2.3.2)

l=2

or approximately 24 times the number required for a memoryless channel.

We close this chapter by presenting an analogous dynamic programming algorithm for

computing the reliability of the above Markov channel decoding scheme. The posterior

probability p(c^|d) that the maximum likelihood codeword c^ was in fact transmitted, given

18

that d was received, can be expressed for the Markov channel as

p(c^|d)-1 =
c

C

2p i=1

pi(di|ci, pi(di|c^i,

ci-1, c^i-1,

di-1) di-1)

.

Assemble the algorithm's initial data at the respective levels 0 and 1:

Si0(j, j

)

=

pi(di|ci = j , ci-1 = j, di-1) pi(di|c^i, c^i-1, di-1)

for 1  i  2p and 0  j, j  1; and

Si1(j, a) = S20i(a1, a2)

for 1  i  2p-1, 0  j  N (1) - 1, and a I~j(1). At the successive levels 2  l  p, compute

Sil(j, a) =

Til(j, a, q, b),

q Ij(l) {b Z22|(a1,b1,b2,a2) I~q(1l-1)×I~q(2l-1)}

where

Til(j, a, q, b) = S2l-i-11(q1, (a1, b1))S2l-i 1(q2, (b2, a2))S(0i-1)2l+2l-1+1(b1, b2)

for 1  i  2p-l, 0  j  N (l) - 1, a I~j(l), q Ij(l), and b Z22. The posterior probability is

then given by

p(c^|d)-1 =

S10(0, a1)S1p(0, a),

a I~0(p)

and is computed in as many real number operations (2.3.2) as the maximum likelihood

codeword itself.

19

Chapter 3
Construction of CFG Representable Codes
20

In 1988, Forney [4] introduced a general technique known as the squaring construction to provide a unified framework for analyzing coset codes. In this chapter, we reinterpret the squaring construction as a grammatical device, deriving in the process context-free grammars for a large family of useful codes--including the Reed­Muller codes introduced in Section 1.1.2.

3.1 The Squaring Construction

If a code S is partitioned by M subsets Ti, 1  i  M , with minimum distances d(S)  d(T ) = mini d(Ti), the (true) squaring construction is defined to be the code U = |S/T |2 =

M i=1

Ti

×

Ti

of

minimum

distance

d(U )

= min[d(T ), 2d(S)].

Alternatively,

given

any

non-

identity permutation  on the set {1  i  M }, a (twisted ) squaring construction can

be defined as the code U  =

M i=1

T(i)

×

Ti

of

minimum

distance

d(T )



d(U )



min[d(T ), 2d(S)]. True or twisted, the squaring construction is simply a technique for

creating larger, greater distance codes from a given code.

When the relevant codes are linear, the squaring construction is particularly elegant [4].

If S is a group partitioned by the cosets of a subgroup T having index |S/T | = M , the true squaring construction, U = |S/T |2 = c [S/T ] T 2 + (c, c) is itself a group nested between the groups S2 and T 2 (S2 > U > T 2). If S is an (N, |S|, d(S)) code and T is an (N, |T |, d(T ))

code, then U is a (2N, |S||T |, min[d(T ), 2d(S)]) code. Invoking the notation of Section 1.1.2,

we choose the following convenient sets of cosets representatives:

[U/T 2] = g1  [S/T ] = {(c, c)|c [S/T ]}

[S2/U ] = g0  [S/T ] = {(c, 0)|c [S/T ]}

[S2/T 2] = [S2/U ] + [U/T 2] = (g0 + g1)  [S/T ] = [S/T ]2.

(Note that since G(2,2) = {g0, g1} is a basis for Z22, (g0 + g1)  A = A2 is an identity for any set A.) The group squaring construction can thus be succinctly expressed as either of

the direct sums

|S/T |2 = T 2 + g1  [S/T ]

21

or |S/T |2 = g1  S + g0  T .
The constructions |S/S|2 = S2 and |S/{0}|2 = g1  S correspond to trivial partitions of the set S.
Now suppose each subset Ti of S is generated by a context-free grammar. In other words, there exist M grammatical symbols ti from which the elements of Ti are derived. It immediately follows that the squaring construction U  itself is generated by a contextfree grammar having start symbol u and associated productions u  t(i)ti, 1  i  M. By induction, any iterated squaring construction (in which the Ti are themselves squaring constructions on squaring constructions . . . ) on a set of CFG representable codes is also generated by a context-free grammar [5]. This notion will be clarified in the following sections, wherein we will construct a large family of CFG representable codes by iterating the squaring construction.
3.2 Iterated Squaring Constructions
Iterating the group squaring construction is relatively straightforward. Suppose S0 > S1 > · · · > Sm is a nested sequence of subgroups with minimum distances d(S0)  d(S1)  · · ·  d(Sm). Since Sj2 > |Sj/Sj+1|2 > Sj2+1, the m group squaring constructions |Sj/Sj+1|2, 0  j  m - 1, derived from this chain also form a nested sequence of subgroups, |S0/S1|2 > |S1/S2|2 > · · · > |Sm-1/Sm|2. Repeating this procedure m times, successively squaring adjacent pairs of subgroups in the current chain to create the next chain, yields a single group of 2m-tuples over S0--the m-level iterated group squaring construction denoted by |S0/S1/ · · · /Sm|M (M = 2m) [4].
In general, it is more convenient [4] to analyze the iterated squaring construction on an infinite subgroup chain composed of the original chain S0 > S1 > · · · > Sm extended to the left and right by the respective dummy sequences · · · > S0 > S0 > and > Sm > Sm > · · · (S-j = S0 and Sm+j = Sm for j  1). Successively squaring adjacent pairs of subgroups yields a tableaux {Snj|n  0, j Z} of codes defined by the recursion:
S0,j = Sj ,
22

Sn+1,j = |Sn,j /Sn,j+1|2 = g1  Sn,j + g0  Sn,j+1

for n  0, j Z. The group Sn,j = |Sj/Sj+1/ · · · /Sj+n|N (N = 2n) of 2n-tuples over S0 has minimum distance
d(Sn,j) = min[d(Sn-1,j+1), 2d(Sn-1,j)]

and generator matrix

Gn+1,j = g1  Gn,j + g0  Gn,j+1

for n  0, j Z. Iterating these relations n times, we find that

d(Sn,j) = min[d(Sj+n), 2d(Sj+n-1), · · · , 2nd(Sj)]

(3.2.1)

and Sn,j = L(Gn,j )

where

n
Gn,j = GRM (r, n)  Gj+r
r=0

(Gj being the generator matrix of Sj) for n  0, j Z. Reed­Muller codes are the principal examples of iterated group squaring construc-

tions [4]. Applying the above construction to the subgroup chain S0 = Z2 = RM (0, 0) > S1 = {0} = RM (-1, 0) with generator matrices G-j = G0 = {1} and Gj = Gj = {0}, j  1, yields the family {Sn,-j|n  0, 0  j  n} of codes generated by

nj
Gn,-j = GRM (r, n)  Gr-j = GRM (r, n) = GRM (j, n);
r=0 r=0

hence, Sn,-j = RM (j, n) for n  0, 0  j  n. Moreover, by evaluating equation 3.2.1 with d(S0) = 1 and d(S1) = , we find that d(RM (j, n)) = d(Sn,-j) = 2n-j.

Iterated squaring constructions on sets that are not groups are far more cumbersome

to describe. Suppose the set S is partitioned by the M sets Ti which are each in turn partitioned by the N sets Vij. The true squaring construction U = |S/T |2 is partitioned

by sets of the form Ti × Ti each of which can itself be partitioned by N twisted squaring

constructions over the sets Vij--for N permutations ki on the set {1  j  N } satisfying

ki (j) = ki (j)  k = k , Ti × Ti =

N k=1

N j=1

Viki

(j

)

×

Vij

.

If

these

MN

component

squaring

23

constructions are labeled |T /V |2ik, the 2-level iterated set squaring construction |S/T /V |4 is

defined to be ||S/T |2/|T /V |2|2 =

M i=1

N k=1

|T /V

|i2k

×

|T /V

|2ik .

As

with

the

iterated

group

squaring construction, extending the initial partition chain analogously increases the number

of possible iterations [4]. Whereas all the partitions of iterated group squaring constructions

are induced by given subgroups, the permutations required to partition iterated set squaring

constructions must be carefully chosen at each stage of the iteration. We therefore postpone

further discussion of iterated set squaring constructions until the labeling system of the next

section is introduced.

3.3 Reed­Muller Grammars
As iterated group squaring constructions, the Reed­Muller codes of length 2p form a nested sequence of subgroups,
RM (p, p) = Z22p > RM (p - 1, p) > · · · > RM (0, p) > RM (-1, p) = {0}.
The space of binary 2p-tuples therefore admits the coset decomposition:

RM (p, p) = [RM (p, p)/RM (p - 1, p)] + [RM (p - 1, p)/RM (p - 2, p)] + · · · +[RM (0, p)/RM (-1, p)] + RM (-1, p).

In other words, each binary string in Z22p can be uniquely expressed as a sum of coset

representatives as follows:

p

Bip0,i1,...,ip =

c(ikk,p)

k=0

where 0  ik  mkp - 1, 0  k  p,

mkp = 2(kp) = |RM (p - k, p)/RM (p - k - 1, p)|,

and ci(kk,p) [RM (p - k, p)/RM (p - k - 1, p)] = L(GRM (p - k, p)).
However, there remains some ambiguity in the labeling of the coset representatives c(ikk,p).

24

In order to resolve this ambiguity, we define

c(ikk,p) = ikGRM (p - k, p)

where ik is the

p k

-bit binary representation of

the integer 0



ik



mpk-1 and GRM (p-k, p)

is the

p k

×2p matrix of rows g(j1, j2, . . . , jp) = gj1 gj2 · · ·gjp from the generator matrix

GRM (p-k, p) ordered lexicographically by label j1j2 · · · jp--largest first. Alternatively, the

matrices {GRM (p - k, p)|0  k  p, p  0} can be constructed by the recursions:





1 g0



GRM (p - 1, p - 1)

GRM (p - k, p)

=



  g1  GRM (p - k, p - 1)
g0  GRM (p - k - 1, p - 1)
g1  GRM (0, p - 1)

 

k=p=0 k = 0, p  1 1  k  p - 1, p  1 k = p, p  1.

Evidently, the coset representatives inherit a similar recursive structure:

c(i00,0) = i0,

0  i0  1;

ci(00,p) = g0  c(i00,p-1),

0  i0  1, p  1;

ci(kk,p) = g1  c(kik-/1m,pkp--11) + g0  ci(kk,mp-o1d)mpk-1 , 1  k  p, 0  ik  mkp - 1, p  1;

c(ipp,p) = g1  c(ipp-1,p-1),

0  ip  1, p  1.

In addition, with this choice of coset representatives the B-symbols can themselves be constructed iteratively:

Bi00 = i0

Bip0,i1,...,ip

=

g1



B

p-1 i1 /m1p-1

,...,

ip-1 /mpp--11

,ip

+

g0



Bp-1
i0 mod

mp0-1 ,...,ip-2

mod mpp--21,ip-1

mod

mpp--11

for 0  ik  mkp - 1, 0  k  p, p  0. Since coset representatives are linearly related to their labels, the mod-2 vector sum of

25

B-symbols is simply

Bip0,i1,...,ip + Bjp0,j1,...,jp = Bfp(i0,j0),f (i1,j1),...,f (ip,jp)

where the function f performs mod-2 vector addition on the binary expansions of its integer

arguments


f (i, j) = [( i/2l + j/2l ) mod 2] 2l.
l=0

In other words, f is the bit-wise exclusive OR operator. For fixed 0  i, j  mkp - 1, f

exhibits the following general properties:

1. f (i, ·) is a permutation on the set {0  j  mkp - 1};

2. f (0, j) = j;

3. f is symmetric: f (i, j) = f (j, i);

4. f (j, j) = 0.

Having precisely defined the addition of B-symbols, their iterative construction can be reexpressed in the explicitly grammatical form

Bi00 = i0

B = Bp
i0,i1,...,ip

p-1 f ( i1/m1p-1 ,i0 mod m0p-1),...,f ( ip-1/mpp--11 ,ip-2 mod mpp--12),f (ip,ip-1 mod mpp--11)

×

B pi-1 /1m1p-1

,...,

ip-1 /mpp--11

.
,ip

(3.3.2)

If the B-symbols are interpreted as variables in a context-free grammar, these expressions are equivalent to the following grammatical production rules:

Bi00  i0;

B  Bp
i0,i1,...,ip

p-1 f ( i1/m1p-1 ,i0 mod mp0-1),...,f ( ip-1/mpp--11 ,ip-2 mod mpp--12),f (ip,ip-1 mod mpp--11)

B .p-1
i1/mp1-1 ,..., ip-1/mpp--11 ,ip

Individual cosets of Reed­Muller codes, aggregates of binary strings, can also be expressed symbolically. For 1    p, the cosets of RM (p - , p) are

Bip0,i1,...,i-1 = {Bip0,i1,...,ip |0  ik  mpk - 1,   k  p} 26

-1

= RM (p - , p) +

c(ikk,p)

k=0

(0  ik  mkp - 1, 0  k   - 1); similarly for  = 0,

Bp = {Bip0,i1,...,ip |0  ik  mkp - 1, 0  k  p} = RM (p, p) = Z22p.

The Reed­Muller codes RM (p - , p), 0    p + 1, are indexed by exactly  zeros:

RM (p - , p) = B0p, 0, . . . , 0.

Reed­Muller cosets inherit an iterative structure from that of their component binary strings. Introducing the auxiliary parameters xk = ik+1/mpk-+11 and zk = ik mod mkp-1 (with mpp-1 = 1) for 0  k  p - 1, the grammatical production 3.3.2 can be rewritten as:
Bip0,i1,...,ip = Bfp-(x10,z0),f (x1,z1),...,f (xp-1,zp-1) × Bxp0-,x11,...,xp-1 .
Since the set {0  ik  mkp - 1} can be put in one-to-one correspondence with the product set {0  xk-1  mkp--11 - 1} × {0  zk  mpk-1 - 1} by the relation ik = mpk-1xk-1 + zk, any coset of RM (p - , p) can be expressed as

Bip0,i1,...,i-1 =

Bip0,i1,...,ip

i,i+1,...,ip

= Bfp-(x10,z0),f (x1,z1),...,f (xp-1,zp-1) × Bxp0-,x11,...,xp-1
x-1,x,...,xp-1 z,z+1,...,zp-1

mp--11 -1
= Bfp-(x10,z0),f (x1,z1),...,f (x-1,z-1) × Bxp0-,x11,...,x-1
x-1 =0

This last equality follows from performing the z-unions before the x-unions, using the fact that f (x, ·) is a permutation for any fixed x and recognizing the resulting Reed­Muller cosets. When  = 0, we recover the obvious result:

Bp = Bp-1 × Bp-1.

27

As with the string B-symbols, these coset B-symbols can be equivalently viewed as variables in a context-free grammar with production rules:

B0  0|1;

Bp  Bp-1Bp-1;

B  Bp
i0,i1,...,i-1

i

p-1 f ( i1/mp1-1 ,i0 mod mp0-1),...,f ( i-1/mp--11 ,i-2 mod mp--12),f (i,i-1 mod mp--11)

Bp-1
i1/m1p-1 ,..., i-1/mp--11 ,i

for 0  i  mp--11 - 1. We summarize the results of this section in the following propositions.

Proposition 3.3.1 The linear [2p, 2p, 1] code Z22p = RM (p, p) is generated by a contextfree grammar with start symbol Bp and productions:

Bl = Bl-1 × Bl-1 B0 = {0, 1}

for 1  l  p.

Proposition 3.3.2

The linear [2p,

p- s=0

p s

, 2]

code

RM (p - , p)

and

its

nonlinear

(2p,

p- s=0

msp,

2)

cosets

(p

+1







1)

are

generated

by

context-free

grammars

with

start symbols {Bip0,i1,...,i-1 |0  ik  mpk - 1, 0  k   - 1} and productions:



Bjl0,j1,...,jK(l)

=



ml--11 -1 x=0

Bfl-(x10,z0),...,f (x-2,z-2),f (x,z-1)

×

Bxl-0,1...,x-2,x

 Bfl-(x10,z0),...,f (xl-2,zl-2),f (xl-1,zl-1) × Bxl-0,1...,xl-2,xl-1

lp 1  l  -1

Bj00 = j0

where

K(l) = min(l,  - 1), xk = jk+1/mkl-+11 , zk = jk mod mkl-1,
and mll-1 = 1 for 0  jk  mlk - 1, 0  k  K(l), and 1  l  p.

28

3.4 Bent Reed­Muller Grammars

The analytic techniques developed in Section 3 provide an essential framework for examining more general partitions of the space Z22p. In particular, the labeling system induced by the coset decomposition of RM(p,p) can be generalized to describe iterated set squaring constructions and their associated context-free grammars [5].
Consider a grammatical system of binary strings constructed according to the production rules:

Ai00 = i0

Aip0,i1,...,ip

=

A × Ap-1
C0p-1 (x0 ,z0 ),...,Cpp--21 (xp-2 ,zp-2 ),Cpp--11 (xp-1 ,zp-1 )

p-1 x0,...,xp-2,xp-1

for 0  ik  mlk - 1, 0  k  p, and p  1. As in proposition 3.3.2, we define the following auxiliary parameters: xk = ik+1/mkp-+11 and zk = ik mod mkp-1. Such a system is considered to be complete if for each 0  k  p and p  0, the function Ckp exhibits the
properties

1. Ckp(·, z) is a permutation on the set {0  x  mpk - 1},
2. Ckp(x, 0) = x (i.e. Ckp(·, 0) is the identity permutation), and
3. Ckp(x, z) = Ckp(x, z ) whenever z = z (i.e. Ckp(x, ·) is a permutation on the set {0  z  mpk - 1})
for 0  x, z, z  mpk -1. Property (1) will ensure that the codes constructed by aggregating A-symbols are indeed iterated set squaring constructions (proposition 3.4.2). As a result of property (2), these codes will be similar to Reed­Muller cosets (propositions 3.4.1,2), having particularly compact grammatical representations (lemma 3.5.2). And perhaps most important of all, property (3) guarantees that the set of all Ap-symbols completely partitions Z22p. Two particularly simple complete grammatical systems are the Reed­Muller system (Ckp = f ) of Section 3 and the cyclic system [5] in which symbol indices are twisted by the cyclic permutations
Ckp(i, j) = (i + j) mod mkp.
As demonstrated in the following lemma, complete grammatical systems partition Z22p into strings with distinctive distance properties.

29

Lemma 3.4.1 If {Api0,i1,...,ip|0  ik  mpk - 1, 0  k  p, p  0} is a complete grammatical system, then for each p  0:

1. Z22p = {Api0,i1,...,ip |0  ik  mpk - 1, 0  k  p}. 2. d(Aip0,i1,...,ip , Aip0,i1,...,il-1,jl,...,jp )  2l whenever jl = il, 0  l  p.

Proof . We use induction. Since the case p = 0 is trivial, assume the validity of (1) and (2)

at level p - 1.

(1) At level p, there are exactly

p k=0

mkp

=

22p

symbols.

Moreover,

symbols

are

distinct

if and only if the lemma is true. So suppose there are two symbols with distinct labels

(i0, i1, . . . , ip) = (j0, j1, . . . , jp) that both represent the same string Aip0,i1,...,ip = Ajp0,j1,...,jp . Applying productions to this equality, we find that

and

A = Ap-1
i1/m1p-1 ,..., ip-1/mpp--11 ,ip

p-1 j1/mp1-1 ,..., jp-1/mpp--11 ,jp

Ap-1
C0p-1( i1/m1p-1 ,i0 mod mp0-1),...,Cpp--21( ip-1/mpp--11 ,ip-2 mod mpp--21),Cpp--11(ip,ip-1 mod mpp--11) p-1
A .C0p-1( j1/mp1-1 ,j0 mod m0p-1),...,Cpp--21( jp-1/mpp--11 ,jp-2 mod mpp--12),Cpp--11(jp,jp-1 mod mpp--11)

=

Since the level p - 1 symbols are necessarily distinct, the corresponding labels must satisfy

ik/mkp-1 = jk/mkp-1

and Ckp--11( ik/mpk-1 , ik-1 mod mkp--11) = Ckp--11( jk/mkp-1 , jk-1 mod mkp--11)
for 1  k  p (with the caveat mpp-1 = 1). Whereupon,
ik-1 mod mpk--11 = jk-1 mod mkp--11
by the completeness of the grammatical system: by property (3), Ckp--11( ik/mkp-1 , ·) = Ckp--11( jk/mpk-1 , ·) is a one-to-one function. Therefore, for 0  k  p,
ik = mpk-1 ik/mpk-1 + ik mod mkp-1 = mkp-1 jk/mkp-1 + jk mod mkp-1 = jk, 30

which contradicts the distinct labels assumption. (2) Suppose the labels (i0, i1, . . . , ip) and (j0, j1, . . . , jp) first differ in the lth index.
Applying productions and the induction hypothesis, we deduce that:

d(Api0,i1,...,ip , Apj0,j1,...,jp ) = d(Axp-0,1x1,...,xp-1 , Ayp0-,y11,...,yp-1 ) + d(Apz0-,z11,...,zp-1 , Apw-0,1w1,...,wp-1 )  2l-1 + 2l-1 = 2l.

Clearly, the respective pairs of indices (xk = ik+1/mkp-+11 , yk = jk+1/mkp-+11 ) and (zk = Ckp-1(xk, ik mod mpk-1), wk = Ckp-1(yk, jk mod mkp-1)) can differ only if k  l - 1. 2
Given a complete grammatical system, we construct the corresponding family of iterated
set squaring constructions by aggregating symbols in the manner of Section 3. For each
1    p, the sets

Api0,i1,...,i-1 = {Aip0,i1,...,ip |0  ik  mpk - 1,   k  p} (0  ik  mkp - 1, 0  k   - 1) partition the space of all binary 2p-tuples,
Ap = {Aip0,i1,...,ip |0  ik  mkp - 1, 0  k  p} = Z22p .

Like Reed­Muller cosets, these codes inherit the iterative structure of the underlying grammatical system. Unioning over the indices i, i+1, . . . , ip (using completeness property (3)), we find that

mp--11 -1

Aip0,i1,...,i-1 =

x=0

A × Ap-1
C0p-1 (x0 ,z0 ),...,Cp--12 (x-2 ,z-2 ),Cp--11 (x,z-1 )

p-1 x0,...,x-2,x

for 1    p, where xk = ik+1/mpk-+11 and zk = ik mod mkp-1. Similarly,

Ap = Ap-1 × Ap-1.

Thus, the codes {Aip0,i1,...,i-1|0  ik  mkp-1, 0  k  -1} are clearly iterated set squaring constructions generated by context-free grammars. Since these codes formally differ from Reed­Muller cosets only in the nature of their associated twisting permutations, they will be referred to as bent (i.e. unnaturally twisted) Reed­Muller codes. Their properties are

31

summarized in the following two propositions.

Proposition 3.4.1 For 1    p:

1. The bent Reed­Muller code Api0,i1,...,i-1 is a (2p, d(Aip0,i1,...,i-1 )  2.

p- s=0

msp)

code

of

minimum

distance

2. The codes {Aip0,i1,...,i-1 |0  ik  mpk - 1, 0  k   - 1} partition Ap = Z22p .

3. d(A0p , 0, . . . , 0) = 2 (including  = 0).


Proof . (1) and (2) These statements follow immediately from lemma 3.4.1.

(3) We use induction. The claim is clearly true when p = 0 or  = 0. Suppose it is valid

at level p - 1. Since the underlying grammatical system is complete, A0p , 0, . . . , 0 is a true

squaring construction:



Therefore,

mp--11 -1

Ap0, 0, . . . , 0 =

Ap0-, .1. . , 0, i × A0p-, .1. . , 0, i.

i=0

 

d(Ap0, 0, . . . , 0)

=

min[min
i

d(A0p-, .1.

.

,

0,

i),

2d(A0p-, 01,

.

.

.

,

0)]


 2d(Ap0-, 01, . . . , 0) = 2. 2

-1

-1

The grammatical features of bent Reed­Muller codes are summarized in proposition 3.4.2.

Proposition 3.4.2 The bent Reed­Muller code Api0,i1,...,i-1 is generated by a context-free

32

grammar with productions:

Alj0,j1,...,jK(l) =





A × Aml--11-1
x=0

l-1 C0l-1 (x0 ,z0 ),...,Cl--12 (x-2 ,z-2 ),Cl--11 (x,z-1 )

l-1 x0,...,x-2,x

A × A

l-1 C0l-1 (x0 ,z0 ),...,Cll--21 (xl-2 ,zl-2 ),Cll--11 (xl-1 ,zl-1 )

l-1 x0,...,xl-2,xl-1

lp 1  l  -1

A0j0 = j0 where

K(l) = min(l,  - 1), xk = jk+1/mlk-+11 , zk = jk mod mlk-1,
and mll-1 = 1 for 0  jk  mkl - 1, 0  k  K(l), and 1  l  p.

3.5 Counting States

In order to implement the maximum likelihood decoding algorithms of Chapter 2, we must

reformulate Reed­Muller codes according to the grammatical template introduced in Section

2.1. In the process, we shall derive the important parameters {|I(l)|, N (l)|0  l  p}

that determine (for a given code) the computational complexity of our decoding schemes.

Emphasizing the primacy of the grammatical rather than the algebraic character of iterated

squaring constructions, we analyze the more general case of bent Reed­Muller codes.

By proposition 3.4.2, the context-free grammar that generates the (2p,

p- s=0

mps ,



2)

bent Reed­Muller code Api0,i1,...,i-1 contains symbols of the form Ajl0,j1,...,jK(l) (K(l) =

min(l,  - 1)) at the levels 0  l  p. We put these grammatical symbols in one-to-one

correspondence with the set of states {0  j  M (l) - 1}, M (l) =

the rule Ajl0,j1,...,jK  j, where

KK

j = jk

mil

k=0 i=k+1

K k=0

mkl ,

according

to

33

and

K

jk = j/

mil mod mlk

i=k+1

(with the understanding that

K i=K

+1

mil

=

1).

Within

the

general

framework

of

this

state-

symbol correspondence, we can count the productions and states generated from a variety

of different start symbols [5].

Lemma 3.5.1 The CFG associated with the start symbol Api0,i1,...,i-1 (1    p) has |I(l)| productions from any level l state, satisfying:

1.



 ml--11   l  p

|I (l) |

=



1 0

1  l  -1 l = 0.

2. For 0  l  p, |I(l)|  mp--11.
Proof . (1) This statement follows immediately from proposition 3.4.2. (2) {mlk|k  l  } is monotone in l.

Lemma 3.5.2 Consider the CFG associated with the start symbol A0p , . . . , 0, i (0  i 

mp -1 - 1, 1    p).



1. The set of grammatically allowed symbols appearing at level 0  l  p consists of symbols of the form Alj0,j1,...,jK(l), labeled by the rightmost K(l) + 1 indices from each of the sequences in the set

{. . . , 0, 0, . . . , 0, i(l), jl+-p, jl+-p+1, . . . , jK(l)|0  jk  mlk - 1, l +  - p  k  K(l)}, where

K(l) = min(l,  - 1),

i(l) =

p-1
i/ mkp- ,
k=l

34

and



mkl

=

mll-k

 = 

2(kl ) 1

0kl k < 0, k > l.

2. At level 0  l  p, the state space can be expressed as {0, . . . , N (l) - 1}, where the

number of states is



N (l)

=

 1



K (l) k=max(0,l+-p)

mkl

l=p 0  l  p - 1.

3. Moreover, N (l)  mp--11, for 0  l  p.

Proof . (1) By proposition 3.4.2, allowed symbols at level l are of the general form

Ajl0,j1,...,jK(l) , labeled by K(l) + 1 indices. However, not all

K (l) k=0

mkl

such

symbols

are

derivable from the given start symbol. Indeed, a symbol is grammatically allowed at level

l if and only if it is the left or right element of a production from an allowed level l + 1

symbol.

We begin by slightly modifying our system of grammatical labels. For convenience, the

level l symbol Ajl0,j1,...,jK(l) will be represented by the semiinfinite sequence . . . , j-1, j0, . . . , jK(l) with 0  jk  mlk - 1 for k  K(l). Productions generalize accordingly: when k is negative, define the spurious twisting permutations Ckl (i, j) = 0, 0  i, j  mlk - 1 = 0.
We proceed by induction. Clearly, the case l = p is a degenerate case of the lemma with

i(l) = i and no subsequent variable indices jk--the resulting set of sequences is a singleton.

Now suppose the lemma holds at level l. Consider first the case of single productions,

1  l   - 1. The labels of all allowed right and left produced symbols are respectively

{. . . , 0, 0, . . . , 0, i(l)/mll+-1-p-1 , jl+-p/mll+-1-p , . . . , jK(l)/mlK-(1l) | 0  jk  mlk - 1, l +  - p  k  K(l)}
and (suppressing C's subscripts and superscripts)

{. . . , 0, 0, . . . , 0, i(l)/mll-+1-p-1 , C( jl+-p/mll-+1 -p , i(l) mod mll-+1-p-1), . . . , C( jK(l)/mlK-(1l) , jK(l)-1 mod mKl-(1l)-1)|0  jk  mlk - 1, l +  - p  k  K(l)}.

35

In contrast, for the case of multiple productions   l  p, these right and left produced
sequences are augmented on the right by the respective single indices j and C(j, jK(l) mod mKl-(1l)) with the variable j assuming the values 0  j  ml--11 - 1. In either case, the set of right or left produced labels simplifies, becoming (for some n)

{. . . , 0, 0, . . . , 0, i(l)/mll-+1 -p-1 , in, . . . , iK(l-1)|0  ik  mkl-1 - 1, n  k  K(l - 1)}.

This occurs because mkl = mlk-1mlk--11 and each function C is a permutation. Two final observations complete the proof. First, n = l +  - p - 1. In the single production case, the

number of variable indices is unchanged and they are left-shifted by one position, whereas

in the multiple production case, the number of variable indices increases by one and they

are not left-shifted. Second, if the expression i(l-1) = i(l)/mll-+1-p-1 is iterated (recalling that mkl = mll-k), the sequences' leading nontrivial index is seen to have the correct form.
(2) N (l) is the cardinality of the set of grammatically allowed symbols at level l. Ex-

amining the set of allowed labels in (1), we see that all indices preceding jl+-p are fixed; moreover, if l +  - p  0, the variable indices {jk|l +  - p  k < 0} are identically zero.

Therefore, the only nontrivial variable indices are {jk| max(0, l +  - p)  k  K(l)} each of which ranges over exactly mkl values. In addition, the grammar's level l state space can be expressed as {0, . . . , N (l) - 1}. Simply use the above state-symbol correspondence (while

simultaneously suppressing or restoring the leading index i(l) if it is nonzero).

(3) Clearly, N (p) = 1 and N (p-1) = mp--11 satisfy the inequality. Since mkl = 1 for

negative k, we can write,

K (l)

N (l) =

mkl

k=l+-p

for 0  l  p - 1. If  - 1  l  p - 2,

-1

-3

N (l) =

mkl  (

mlk++11)ml -2ml-1

k=l+-p

k=l+-p

-2

=(

mkl+1)ml+-11 = N (l+1).

k=l+1+-p

This inequality follows from the relations mlk++11 = mkl +1mkl and mlk  mkl++11. Similarly, if

36

0  l   - 2,

ll

N (l) =

mkl 

mkl++11 = N (l+1).

k=l+-p

k=l+-p

Thus, N (l) increases monotonically from 2 at (l = 0) to mp--11 (at l = p - 1). 2 The parameters {|I(l)|, N (l)|0  l  p} derived in the previous two lemmas solely deter-
mine the computational complexity of our maximum likelihood decoding algorithms for the code RM (p - , p). For the memoryless communications channel case, table 3.1 tabulates the number of operations ( 2.2.1) required to decode the nontrivial Reed­Muller codes of intermediate length. Although the loose upper bound O(RM (p-, p)) 2p(mp--11)2 is overly pessimistic, it does suggest that the logarithm of the decoding complexity is a polynomial function of the code's log-length p. This hypothesis is effectively substantiated by the data presented in table 3.1. Since each million real number operations requires roughly half a second of real-time computation (on an average workstation), we observe that RM(3,7) and almost all the Reed­Muller codes of length 256 or greater are practically undecodable--even by dynamic programming methods.



12

3

4 5 67

4 87 255

127

5 183 1,215

3,007

319

p 6 375 5,375

327,039

79,231

767

7 759 22,783

151,116,543 4,425,388,799 4,606,719

1,791

8 1,527 94,207 292,116,477,439

> 257

> 257 562,599,423 4,095

Table 3.1: Operations to decode RM (p - , p).

Although many Reed­Muller grammars may be too complex for practical decoding, perhaps there exist sub-grammars with more strictly limited state spaces. The following lemma examines a class of context-free grammars generated by certain restricted subsets of Reed­Muller symbols [5].
Lemma 3.5.3 Given the nonnegative integers l and n satisfying ml-1  M = 2n  ml-+11, consider a CFG that uses the symbols {Al0,+.1. . , 0, i|0  i  M - 1} at level l + 1.

1. At level 0  l  l + 1, the above symbol-state correspondence yields the state space

37

{0, . . . , N (l) - 1} of size



N (l)

=

 M  max(1, M/

l k=l

mkl+1-

)

K (l) k=max(0,l+-l-1)

mkl

l = l + 1 0  l  l.

2. For 0  l  l + 1, N (l)  M and |I(l)|  M .

Proof . (1) With a single exception, this follows directly from statements 1 and 2 of lemma 3.5.2. Instead of being fixed for each level l, the index i(l) assumes
l l
(M - 1)/ mlk+1- + 1 = max(1, M/ mlk+1- )
k=l k=l
values. (Suppose a and b are powers of 2. If b divides a, then (a - 1)/b = a/b - 1  0; otherwise b > a and (a - 1)/b = 0.)
(2) That |I(l)|  M follows from lemma 3.5.1. By lemma 3.5.2, N (l) is monotonically increasing in the range 0  l  l. Thus,

N (l)  N (l) = M/ml-1 ml-1  M

for 0  l  l. 2

3.6 Thinned Reed­Muller Grammars
As demonstrated in Section 5, the state spaces of most Reed­Muller codes of length 256 or greater are too large for practical decoding--even by dynamic programming. Fortunately, however, there exists an enormous family of subgroups of Reed­Muller codes that are readily decodable by the exact maximum likelihood decoding algorithms of Chapter 2. Such codes are constructed by systematically thinning the Reed­Muller grammars of Section 3, discarding extraneous productions (or information bits) and thereby strictly limiting the cardinality of each level's state space [5].
Definition 3.6.1 Given 1    p and n  0, the thinned Reed­Muller code RM (n)(p - , p) is the length 2p code generated by the following CFG with start symbol B0p,,n0, . . . , 0

38

and productions:



Bjl,0n,j1,...,jK(l)

=



min(2n,ml--11)-1 x=0

Bfl-(x10,n,z0),...,f (x-2,z-2),f (x,z-1)

×

Bxl-0,1..,.n,x-2,x

 Bfl-(x10,n,z0),...,f (xl-2,zl-2),f (xl-1,zl-1) × Bxl-0,1..,.n,xl-2,xl-1

lp 1  l  -1

Bj00,n

= j0

where

K(l) = min(l,  - 1), xk = jk+1/mkl-+11 , zk = jk mod mlk-1,

and



mkl

= mll-k

 2(kl)  1

0kl otherwise

for 0  jk  mkl - 1, 0  k  K(l), and 1  l  p.
The key properties of thinned Reed­Muller codes are presented in the following proposition. Note that thinned bent Reed­Muller codes (defined as above with the substitutions f  Ckl ) share all of these features except linearity.
Proposition 3.6.1 Consider the thinned Reed­Muller code RM (n)(p-, p) for 1    p and n  0. Define the particular level:

l = max{0  l  p - 1|ml-1  2n}.

1. If n 

p-1 -1

,

then

l

=

p-1

and

RM (n)(p - , p)

=

RM (p - , p);

otherwise,

0  l  p - 2 and RM (n)(p - , p)  RM (p - , p).

2. RM (n)(p - , p) is a linear [2p,

p l=1

2p-lQ(l),

2]

code

with

grammatical

parameters:

|I (l) |

= 2Q(l) =

  min(2n, ml--11)



1 0

lp 1  l  -1 l=0

39

and



 1 l = p

N (l)

=



2n max(1,

N l+1/

l k=l

mlk+1-

)

K(l) k=max(0,l+-l-1)

mkl

l + 1  l  p - 1 0  l  l.

The state-symbol correspondence of Section 5 produces a state space of the form {0, . . . , N (l) - 1}.

3. For 0  l  p, the number of states and productions is bounded: N (l)  2n and |I(l)|  2n.

Proof . (1) If n 

p-1 -1

,

then 2n

 ml -1

and

min(2n, ml-1)

=

ml -1

for

each

0

l

 p - 1;

the thinned Reed­Muller grammar coincides with the original Reed­Muller grammar of

proposition 3.3.2. Otherwise, 2n < mp--11 and

2n-1

2n-1

RM (n)(p-, p) =

B0p-, .1.,n. , 0, i×B0p-, .1.,n. , 0, i 

B0p-, .1. . , 0, i×B0p-, .1. . , 0, i  RM (p-, p).

i=0 i=0





By induction, the sets associated with thinned symbols are subsets of those sets associated with the original symbols of the same label.
(2) The case l = p - 1 is considered in lemmas 3.5.1 and 3.5.2. Now suppose 0  l  p - 2. By the definition of l, ml-1 > 2n for l > l. Thus, if l - 1  l + 1, the productions of the thinned Reed­Muller grammar assume the form

2n-1

B0l,,n. . . , 0, j =

B0l-, 1.,.n. , 0, f (i, j) × B0l-, 1.,.n. , 0, i

i=0

 

for 0  j  N (l) - 1. If N (l)  2n, then j/ml--11 = 0 j implies productions of this form and N (l-1) = 2n; or N (l) = 2n for l + 1  l  p - 2. In contrast, at and below level l + 1,

the thinned Reed­Muller grammar coincides with the original Reed­Muller grammar. The

remainder of the formula for N (l) follows directly from lemma 3.5.3.

Both the linearity and minimum distance of RM (n)(p - , p) are deduced by induction. By proposition 3.3.2, the code B0l,+.1.,.n, 0, 0 = B0l,+.1. . , 0, 0 is linear with minimum

distance 2.


Moreover, since the operation f is mod-2 vector addition of binary strings,

40

2n-1 i=0

B0l,+. 1.,.n,

0,

i

is

linear.

Therefore,

because

B0l,+.2.,.n, 0, 0

is

the

group

squaring

construc-



tion |

2n-1 i=0

B0l,+. 1.,.n,

0,

i /B0l,+. 1.,.n,

0,

0

|2

,

it

too

is

linear

with

minimum

distance

2.

By the


distance properties of the squaring construction, d(B0l,+.2.,.n, 0, 0)  d(B0l,+.1.,.n, 0, 0) = 2;


however, since B0l,+.2.,.n, 0, 0  B0l,+.2. . , 0, 0, d(B0l,+.2.,.n, 0, 0)  d(B0l,+.2. . , 0, 0) = 2. In ad-





dition,

2n-1 i=0

B0l,+. 2.,.n,

0,

i

=

(

2n-1 i=0

B0l,+. 1.,.n,

0,

i

)2

is

linear.

Clearly, this argument can be

iterated

for

the


successive

levels

l

+

3




l

p.

(3) This statement follows from (2) and lemma 3.5.3. 2

As a consequence of lemma 3.6.1, the decoding complexity of thinned Reed­Muller codes

is strictly controlled by the limiting parameter n. The corresponding loose upper bound on

the number of decoding operations is O(RM (n)(p-, p)) 2p+2n, which for fixed n is simply

a multiple of the code length 2p. For example, RM (8)(6, 10), a linear [1024, 440, 16] code

roughly half the size of the undecodable linear [1024, 848, 16] code RM(6,10), is decodable in

221 226 real number operations. Appendix A assembles the results of simulated decoding

trials for six representative thinned Reed­Muller codes. We discuss these results in Chapter

5 after developing an alternative coarse-to-fine decoding algorithm in Chapter 4.

41

Chapter 4
A CTFDP Algorithm for Maximum Likelihood Decoding
42

4.1 Coarse-to-Fine Dynamic Programming
In Section 2.2, we described the standard dynamic programming (DP) approach for minimizing a bit-wise additive cost function over a CFG representable code. One iteratively computes a hierarchy of induced costs, assigning to each state at each node of the code's tree-template the minimum sum of the costs associated with the given state's productions. However, for many codes with large states spaces, the computational cost of standard dynamic programming exceeds practical bounds. Therefore, in this section we consider a variant of the standard approach called coarse-to-fine dynamic programming (CTFDP) in which the original DP problem is replaced by an approximating sequence of simpler DP problems ([8], [9]).
Consider a code C formulated according to the grammatical template of Section 2.1. To implement CTFDP, we begin by constructing a family of codes that in some sense approximate C. First, at each level of the tree-template, we select a sequence of successively finer partitions of the state space {0, 1, . . . , N (l) - 1} beginning with the state space itself and ending with its singlet decomposition. A subset in one of these partition chains, being an aggregate of states, is called a super-state. The coarseness of a super-state specifies the particular partition to which it belongs. Moreover, refinement is the act of splitting a super-state into (typically two) super-states at the adjacent level of coarseness. Second, for any given set of state space partitions, we define generalized super-state productions. If A,B, and C are allowed super-states, respectively occupying a node and its left and right daughter nodes in the tree-template, then A  BC is an allowed super-state production if there exist states x A, y B, and z C such that x  yz is an allowed state production.
With this choice of super-state productions, any given set of state space partitions-- possibly consisting of super-states of varying degrees of coarseness and possibly differing from node to node at any level--uniquely determines a super-state grammar corresponding to a super-code containing the original code C. To ensure that a super-state grammar generates bit-strings (of length equal to that of C), we define the following terminal productions from level 0 super-states: {0}  0, {1}  1, and {0, 1}  0|1. Furthermore, any codeword in C can be derived from a super-state grammar; given the codeword c C, there exists a super-state derivation tree that corresponds (by the definition of super-state productions) to the codeword's own state derivation tree and has the bits of c as its terminal assignments.
43

Thus, each super-state grammar generates a super-code containing C. The heart of the CTFDP minimization algorithm involves a series of standard DP com-
putations over successively finer super-state grammars. We begin by applying the standard DP algorithm to the coarsest possible super-state grammar, the grammar consisting of a single super-state (the state space itself) at each node of the tree-template. Of course, the nodal costs of the level-0 super-states are initialized by 2p independent minimizations over the nodal costs of the states 0 and 1. The output of this first DP step is merely the minimum cost string in the super-code Z22p.
The CTFDP algorithm proceeds by progressively refining the super-state grammar. Given the solution of the the previous DP problem--an optimal derivation tree corresponding to a minimum cost super-codeword, we determine whether the optimal derivation tree contains any non-singlet super-states. If so, we refine these super-states, recompute the super-state productions, solve the new DP problem, and again examine the optimal derivation tree. If not, we stop: the current optimal derivation tree represents the minimum cost codeword. Since the terminal optimal derivation tree contains only states from C's underlying grammar, it certainly generates a codeword (in C). Moreover, this codeword is by definition the minimum cost super-codeword in the terminal super-code--a code that contains C itself.
Although this CTFDP algorithm must eventually produce the solution to a given minimization problem, it need not necessarily outperform standard DP. For the procedure to converge rapidly, the number of refinements and subsequent DP computations must be minimal. This suggests that super-states should consist of aggregations of "similar" states so that their costs more closely reflect those of their constituents [9]. In addition, the determination of super-state productions must not be too computationally demanding. These considerations thus prompt the following question: is there an efficient CTFDP implementation of maximum likelihood decoding for the grammatical codes of Chapter 3?
4.2 Super-States for Thinned Reed­Muller Codes
To implement a CTFDP version of a given DP problem, one must first partition the problem's state spaces into clusters of super-states. In this section, we construct coarsened grammatical symbols and associated super-states for thinned Reed­Muller codes. The fact that
44

these coarsened symbols exhibit considerable grammatical structure suggests that thinned Reed­Muller codes are particularly amenable to coarse-to-fine decoding.
We begin by reformulating thinned Reed­Muller grammars according to the grammatical template introduced Section 2.1. This approach deliberately blurs the distinction between grammatical symbols and their associated numerical states. By proposition 3.6.1, the set of grammatically allowed symbols at level l for a given code RM (n)(p-, p) can be reexpressed as {Bil,n|0  i  N (l) - 1}, where the integer label i denotes the state corresponding to an allowed symbol Bil0,n,i1,...,iK(l). The associated grammatical productions are presented in the following lemma.
Lemma 4.2.1 Consider the thinned Reed­Muller code RM (n)(p - , p) for 1    p and n  0.
1. The set of allowed productions from the state 0  i  N (l) - 1 at level 1  l  p is

Ii(l) = {(f (x(i)  j, z(i)), x(i)  j)|0  j  |I(l)| - 1}

where

|I(l)| = min(2n, mlK-(1l))

and the binary operator  (introduced solely for notational convenience) equals f (i.e. a  b = f (a, b)). The auxiliary integers x(i) and z(i) are defined by the relations

x(i)  x(i) = x0x1 · · · xK(l)

and z(i)  z(i) = z0z1 · · · zK(l).

In these expressions, x(i) (z(i)) is the binary expansion of the integer x(i) (z(i)); zk is

the

l-1 k

-bit

binary

expansion

of

the

integer

zk

=

ik

mod

mlk-1;

and

xk

is

the

l-1 k

-bit

binary expansion of the integer



xk

=

 ik+1/mkl-+11  0

1  k  K(l) - 1 k = K(l).

(Note that for single productions (i.e. K(l) = l), the strings xK(l) and zK(l) have 45

length

l-1 l

= 0 and can therefore be ignored; however, for multiple productions,

they are not negligible.)

2. The corresponding symbolic productions are:

|I (l) |-1

Bil,n =

Bfl-(x1(,in)j,z(i)) × Bxl-(i1),nj .

j=0

Proof . (1) and (2). The state symbol correspondence, originally defined in Section 3.5,

is more intuitively described by the relation i  Bil0,n,i1,...,iK(l) if and only if the binary

expansion i of the integer i equals the concatenation i0i1 · · · iK(l) (where ik is the

l k

-bit

binary expansion of the integer ik). Now observe that for single productions (i.e. K(l) = l

and |I(l)| = mll-1 = 1), x(i)  j  x0x1 · · · xl-1 and z(i)  z0z1 · · · zl-1, whereas for

multiple productions (i.e. K(l) =  - 1 and 1 < |I(l)|  ml--11), x(i)  j  x0x1 · · · x-2j

and z(i)  z0z1 · · · z-1.

(Note that for each k, the strings xk and zk have length

l-1 k

;

and for consistency the binary expansion j of the integer j must share the length of x-1

(and z-1).) Moreover, since the function f performs mod-2 vector addition on the binary

expansions of its arguments, its action is separable in the following sense:

f (a, b)  f (a0, b0)f (a1, b1) · · · f (aK , bK )

(4.2.1)

whenever a  a0a1 · · · aK , b  b0b1 · · · bK , and the substrings ak and bk share the same length. Of course, in this expression, consistency demands that the length of the substring

f (ak, bk), the binary expansion of f (ak, bk), equal the given length of the substrings ak and bk associated with its arguments. Thus, since the function f is separable, lemma 4.2.1 reproduces both the single and multiple productions of definition 3.6.1. 2

Lemma 4.2.1 has two important practical consequences. First, the multitude of produc-

tions for the thinned Reed­Muller code RM (n)(p - , p) can be readily computed from a

comparatively small set of stored integers--the 2(p + 1) parameters {N (l), |I(l)||0  l  p}

and the set of 2

p l=1

N

(l)

p2n+1 auxiliary x's and z's, one pair of integers for each state

at each non-zero level. Second, as we now demonstrate, there exists a hierarchy of coarsened

symbols that retains the essential structure of the original thinned Reed­Muller grammar.

Definition 4.2.1 Consider the thinned Reed­Muller code RM (n)(p - , p) for 1    p

46

and n  0. Fixing the level 0  l  p, coarseness 0  q  log2 N (l), and label 0  i  N (l,q) - 1 (with N (l,q) = N (l)/2q ), define the coarse symbol

2q -1

Bil,n,q =

B(l,in<<q)j = {Bkl,n| k/2q = i}.

j=0

The corresponding level l super-state is denoted by the pair (q, i). (A comment on notation: the coarsening or refinement of a grammatical symbol is most intuitively represented by respectively right- or left-shifting its integer label. The left-shift operation i << q, which shifts the binary representation of i q bits to the left and inserts q zeros, is equivalent to multiplication by 2q; similarly, the right-shift operation i >> q, which shifts the binary representation of i q bits to the right, is equivalent to i/2q .)

The recursive structure of this hierarchy of coarsened symbols is analyzed in the following proposition. Notice that the uniform coarseness productions defined by equation 4.2.2 below retain the simple Reed­Muller structure and are easily computed from the set of stored auxiliary x's and z's that underpin the original grammar.

Proposition 4.2.1 The coarse symbols {Bil,n,q|0  i  N (l,q) - 1, 0  q  log2 N (l), 1  l  p} for the thinned Reed­Muller code RM (n)(p - , p) (with 1    p and n  0)
satisfy the recursive set identity:

where the parameters

I (l,q) -1

Bil,n,q =

Bfl-(x1,nj,,qz) × Bxl-1j,n,q

j=0

(4.2.2)

I (l,q) = 2qx-qz , q = q + qz, z = z(i << q) >> (q + qz),

and

x = [x(i << q) >> (q + qx)] << (qx - qz)

47

are defined in terms of the quantities



 0 k = K(l) + 1

q

=  min(n, 

K(l) k=k

l-1 k

)

0  k  K(l),

 q k = K(l) + 1

qz

=

 max(0, q - rk -

l-1 k-1

)

0  k  K(l),

and



qx

=



min(n,

l-1 K (l)

)



min(q - rk,

l-1 k-1

)

k = K(l) + 1 0  k  K(l).

Given the array



 0 k = K(l) + 1

rk

=



min(n,

l-1 K (l)

)

rk+1

+2

l-1 k

k = K(l) 0  k  K(l) - 1,

the integer k is defined by the expression

k = min{0  k  K(l) + 1|q  rk}.

(Recall that we define

l-1 l

= 0.)

Proof . Applying the productions of lemma 4.2.1 to the coarse symbols of definition 4.2.1, we find that

2q -1

Bil,n,q =

B(l,in<<q)j

j=0

= Bfl-(x1,,zn) × Bxl-1,n

(4.2.3)

where the integers (i << q)  j, z, and x have the binary expansions:

(i << q)  j  z0|x0z1| · · · |xK(l)-2zK(l)-1|xK(l)-1zK(l) z  z0z1 · · · zK(l) x  x0x1 · · · xK(l).
48

(4.2.4)

In the notation of lemma 4.2.1, x and z are the auxiliary integers x((i << q)  j)  xK(l)

and z((i << q)  j) constructed from the length

l-1 k

substrings xk and zk. However, in

contrast to the situation of lemma 4.2.1, xK(l) is not identically zero, but rather a parameter

assuming values in the set {0, 1, . . . , min(2n, mKl-(1l)) - 1}. The heretofore unspecified union

in equation 4.2.3 is over both the min(n,

l-1 K(l)

) bits of xK(l) (labeling the underlying state

productions) and the rightmost q bits of 4.2.4 (representing the coarsening). By successively

unioning over first the z-bits and then the x-bits--using the separability of the function f

(equation 4.2.1), we can independently coarsen first the left-hand and then the right-hand

symbols in 4.2.3.

Following this approach, the remainder of the proof is straightforward. By the definition

of k, all auxiliary x's and z's indexed by integers greater than or equal to k are to be

unioned over. This coarsens both the left and right produced symbols in equation 4.2.3 to

level q . The remaining bits in the union are the rightmost qx and qz bits of xk-1 and zk-1 respectively. If qz > 0, the left and right produced symbols can be further coarsened to level q = q + qz, leaving exactly qx - qz bits of the substring xk-1 to be unioned over. The number of super-state productions is then I(l,q). 2

The uniform coarseness productions (4.2.2) defined in proposition 4.2.1 can be used to

determine the super-state productions corresponding to a given set of state space partitions.

If (q, i), (qL, L), and (qR, R) are super-states that respectively belong to the current state space partitions of a level l node and its left and right daughter nodes, then (q, i) 

[(qL, L), (qR, R)] is an allowed super-state production (in the sense of Section 4.1) if and only if L shares a binary prefix with f (x  j, z) and R shares a binary prefix with x  j for

some 0  j  I(l,q) - 1; for if this condition is met, there is an allowed state production

contained within the postulated super-state production. Since proposition 4.2.1 thus allows

us to compute super-state productions by inspection, an efficient CTFDP implementation

of maximum likelihood decoding might indeed exist for thinned Reed­Muller codes.

4.3 A CTFDP Decoding Algorithm for Thinned Reed­Muller Codes
Suppose a codeword from the thinned Reed­Muller code RM (n)(p - , p) is transmitted across a memoryless communications channel. The maximum likelihood decoding of the
49

received word d is the codeword

2p

c^ = arg

min

- ln p(di|ci).

c RM (n)(p-,p) i=1

In this section, we formulate an explicit coarse-to-fine dynamic programming algorithm for performing this minimization that employs the thinned Reed­Muller super-states defined in Section 4.2.
As discussed in Section 4.1, a CTFDP decoding algorithm iterates the standard DP procedure (of Section 2.2) over a sequence of progressively finer super-state grammars. Since these grammars are induced by state space partitions, the effective level l state space for coarse-to-fine decoding is the entire tree of super-states {(q, i)|0  i  N (l,q) - 1, 0  q  log2 N (l)} rather than the original linear array of ordinary states {0, 1, . . . , N (l) - 1}. Furthermore, at any node of the tree-template during a given DP iteration, we distinguish three disjoint sets of super-states. Leaf super-states are defined to be those super-states currently partitioning the node's state space; the cost of a leaf super-state is defined to be the minimum cost of all corresponding allowed super-state productions. Interior superstates strictly contain leaf super-states; their cost is the minimum cost of their constituent leaf super-states. Exterior super-states are strictly contained in leaf super-states and are not assigned costs. Despite, this apparent doubling in size of the problem's effective state spaces, only a small fraction of all super-states will be explored during a successful coarseto-fine decoding trial.
The maximum likelihood decoding of the received word d is accomplished by the following CTFDP algorithm.

1. Compute the parameters {|I(l)|, N (l)|0  l  p} (proposition 3.6.1) and the auxiliary x's and z's (lemma 4.2.1) that together determine the thinned Reed­Muller grammar corresponding to RM (n)(p - , p).

2. Initialize the level 0 costs. The cost of the state j (super-state (0, j)) at the i-th level 0 node is - ln p(di|ci = j) for 1  i  2p and 0  j  1. The super-state (1, 0) (i.e. the state set {0, 1}) at each level 0 node is assigned the minimum nodal state cost and a pointer to the respective minimizing state.

3. Initialize the state space partitions. For each node at each level 0  l  p of the

50

tree-template, choose the super-state (log2 N (l), 0) as the sole leaf super-state. All other super-states (i.e. those of coarseness less than log2 N (l)) are exterior.
4. Perform DP on the current set of leaf super-states. Proceeding sequentially from level 1 to level p, assign optimal costs and associated pointers first to the leaf super-states and then to the interior super-states at each of the 2p-l nodes in level l:
· Leaf super-states. If (q, i) is a leaf super-state at a level l node, compute the parameters q, x, z and I(l,q) that determine the uniform coarseness productions of proposition 4.2.1:
(q, i)  {[(q, f (x  j, z)), (q, x  j)]|0  j  I(l,q) - 1}.
Recall that these productions are not necessarily allowed super-state productions in the sense of Section 4.1, for the coarseness q super-states may not be leaf superstates at level l - 1. However, the allowed super-state productions from (q, i) are readily obtained. For each uniform coarseness production, identify its left and right elements--respectively (q, f (x  j, z)) and (q, x  j)--as leaf, interior, or exterior super-states. If an element is a leaf super-state, it must belong to an allowed super-state production. If it is exterior, there is a leaf super-state strictly containing it that belongs to an allowed super-state production. And if it is interior, it contains several constituent leaf super-states each of which belong to an allowed super-state production. Simultaneously applying these rules to both the left and right elements of a uniform coarseness production generates all the corresponding allowed super-state productions. By implicitly employing these rules and utilizing the interior super-state cost structure, the following procedure computes the minimum cost super-state production from (q, i) without explicitly determining the set of all such productions. Construct the subset {j0, j1, . . . , jN-1} of {0, 1, . . . , I(l,q) - 1} by the following procedure: beginning with j0 = 0, define jk+1 = jk + 2t(jk) where t(jk) = max(tL(jk), tR(jk)). tL(jk) is the smallest nonnegative integer t for which there exists a non-exterior super-state of coarseness q + t (at the left daughter node) that contains the super-state (q, f (x  jk, z)). Similarly, tR(jk) is the smallest
51

nonnegative integer t for which there exists a non-exterior super-state of coarseness q + t (at the right daughter node) that contains the super-state (q, x  jk). Naturally, the index sequence terminates at N - 1 = max{k  0|jk < I(l,q)}. For each 0  k  N - 1, define the cost c(jk) to be the sum of the costs of the super-states (q + t(jk), f (x  j, z) >> t(jk)) and (q + t(jk), x  j >> t(jk)) at the respective left and right level l - 1 daughter nodes. c(jk) is the minimum cost of all allowed super-state productions associated with those uniform coarseness productions indexed by {jk, jk + 1, . . . , jk+1 - 1}. Therefore, the minimum cost of all allowed super-state productions from (q, i) is c(j) = mink c(jk). Clearly, its determination requires 2N - 1 real number operations (N additions and N - 1 comparisons). The corresponding optimal super-state production is the minimum cost pair of leaf super-states contained in the optimal pair:
[(q + t(j), f (x  j, z) >> t(j)), (q + t(j), x  j >> t(j))].
Assign to the leaf super-state (q, i) the minimum cost c(j) and a pointer to the optimal right super-state (q + t(j), x  j >> t(j)).
· Interior super-states. Having calculated the optimal costs of a node's leaf superstates, we now recursively compute costs and pointers for its interior super-states. The interior super-state (q, i) is assigned the cost of and a pointer to its minimum cost component super-state--(q - 1, 2i) or (q - 1, 2i + 1). Computing such a minimum requires a single real number operation (one comparison).
5. Determine the optimal derivation tree by applying the set of optimal superstate productions to the start super-state (0,0) (the start state 0) at the sole level p node. Suppose the leaf super-state (q, i) appears at a given level l node of the optimal derivation tree. We use the super-state pointers defined in step (4) above to construct the optimal super-state production [(qL, L), (qR, R)] that assigns super-states to the given node's daughter nodes in the optimal derivation tree. If (q, i)'s associated pointer points to the optimal right super-state (qR, R ), then the corresponding optimal left super-state (qL, L ) is by definition (qR, f (R , z(i << q) >> qR). However, it is important to recognize that the pair [(qL, L ), (qR, R )] is not necessarily a super-state production in the sense of Section 4.1, for these super-states may be
52

interior. In fact, the optimal left-produced super-state (qL, L) is the minimum cost leaf super-state contained in (qL, L ); if (qL, L ) is an interior super-state, (qL, L) is obtained by following the sequence of pointers assigned to (qL, L ) and its constituent interior super-states. (qR, R) is defined analogously. 6. If the optimal derivation tree contains super-states of coarseness greater than zero, refine them and return to step (4). Otherwise, stop iterating and generate the maximum likelihood codeword c^ (from the level 0 states of the optimal derivation tree). In the terminology of this section, we refine a super-state (q, i) by transferring it to the list of interior super-states, while replacing it (in the list of leaf super-states) with its components (q - 1, 2i) and (q - 1, 2i + 1). We discuss the relative performance of CTFDP and DP maximum likelihood decoding algorithms in Chapter 5.
53

Chapter 5
Discussion
54

5.1 Synopsis
As proved in Chapter 1, the maximum likelihood decoding scheme is optimal in the sense that it maximizes the probability of correcting channel errors. Of course, for all but the smallest codes, a sequential search for the maximum likelihood codeword is effectively impossible. Therefore, for those communications channels with factorizable likelihoods, dynamic programming is typically the only feasible strategy for exact maximum likelihood decoding.
The Viterbi algorithm is the traditional prototype dynamic programming algorithm for maximum likelihood decoding. For any trellis (e.g. convolutional or linear block) code, maximum likelihood decoding can be formulated as a shortest path search on the code's trellis diagram. As a graphical search, the Viterbi algorithm exploits the elementary dynamic programming principle that the shortest length path to a trellis node must be an extension of the shortest length path to some predecessor node. From the perspective of formal language theory, however, a trellis diagram is simply the graphical representation of a regular language and the Viterbi algorithm recursively parses a trellis code's regular grammar.
By further exploring the relationship between codes and formal languages, we aim to expand the range and applicability of dynamic programming decoding algorithms. The fundamental insight that informs our aim is expressed as follows. Given a factorizable likelihood function, only the existence of considerable grammatical structure within a code can facilitate decoding by dynamic programming. Just as hard decoding algorithms typically rely on underlying algebraic features, exact maximum likelihood decoding algorithms for general communications channels must necessarily exploit grammatical structures.
Following this strategy, we have constructed a large family of codes generated by contextfree grammars and have presented three generalized Viterbi algorithms for their maximum likelihood decoding. The theoretical development proceeded in three successive stages. First, in Chapter 2 we designed dynamic programming algorithms for the maximum likelihood decoding of codes derived from context-free grammars and transmitted across either memoryless or Markov communications channels. In addition, we introduced similar algorithms to compute a useful reliability statistic--the posterior probability that a decoded word in fact matches the transmitted codeword. Second, by interpreting Forney's iterated squaring construction grammatically, we constructed in Chapter 3 a large class of CFG
55

representable codes, notably including the Reed­Muller codes. Moreover, by thinning these grammars, we created a family of codes having readily controllable decoding complexities. Finally, by exploiting the grammatical structure of thinned Reed­Muller codes, we derived a coarse-to-fine dynamic programming algorithm for maximum likelihood decoding in Chapter 4.
Having reviewed the theoretical content of this thesis, we now turn to the question of algorithmic performance.
5.2 DP and CTFDP Decoding Performance
We test our dynamic programming algorithms for maximum likelihood decoding on six representative thinned Reed­Muller codes in a series of simulated decoding trials. For each of the six codes--RM(2,5), RM(2,6), RM(2,7), RM(3,7), RM (10)(4, 8), and RM (8)(6, 10), the same fifty randomly selected codewords are transmitted across four (memoryless) bipolar communications channels with different degrees of additive gaussian noise. In other words, the bits 0 and 1 of each codeword are first converted to the respective signals -1 and 1 which are in turn corrupted by adding noise independently drawn from the distributions {N (0, 2)| = 0.3, 0.5, 0.8, 1.0}. A cross section of the decoding results is displayed in Appendix A; for each code, we select a sufficiently large value of  (typically 0.8 or 1.0) so that there is some variability in decoding performance but avoid overwhelming noise levels.
Consider first the performance of the ordinary DP algorithm for maximum likelihood decoding (Section 2.2). Since DP decoding is infeasible for RM(3,7), the appendix details the outcome of 250 separate decoding trials for five different codes. Although the performance varies from code to code, overall the maximum likelihood decoding scheme recovers the transmitted codeword in 96% (240/250) of the trials. In contrast, consider the performance of a common approximation to this scheme, the thresholded hard decoding scheme (in which c^(d) is the minimum distance decoding of the binary string whose bits are independently most likely): the transmitted codeword is recovered in only 54% (136/250) of these trials. Thus, the theoretically optimal maximum likelihood decoding scheme performs extraordinarily well by absolute or relative empirical measures at moderate noise levels (i.e.  = 0.8 or 1.0).
Furthermore, the posterior probability that the maximum likelihood codeword matches
56

the transmitted codeword provides an excellent measure of decoding reliability. In those trials for which the posterior probability exceeded 90%, the transmitted codeword was recovered 99% (220/222) of the time. When the posterior probability fell between 70% and 90%, the decoding success rate fell to 81% (13/16). And in those trials yielding posterior probabilities less than 70%, the success rate fell further to 75% (15/20).
At noise levels below those considered in the cross section, our maximum likelihood decoding algorithm's success rate is uniformly perfect with posterior probabilities exceeding 99%. In contrast, as  increases past this threshold range of 0.8    1.0, the noise tends to overwhelm the signal producing increasingly mediocre decoding results.
Now consider the alternative CTFDP algorithm for maximum likelihood decoding (Section 4.3). Table 5.1 compares the performance of the CTFDP algorithm with that of the standard DP algorithm; for each set of simulated decoding trials, it displays the average ratio of CTFDP decoding operations to DP decoding operations. The results are striking. For all but the smallest code RM(2,5), the CTFDP algorithm computes the maximum likelihood codeword on average 5 to 100,000 times faster, depending on the particular code and noise level. Thus, even the tremendously complex code RM(3,7)--practically undecodable by the standard DP algorithm--becomes tractable at low to moderate noise levels (i.e.   0.8).

Code RM(2,5) RM(2,6) RM(2,7) RM(3,7) RM (10)(4, 8) RM (8)(6, 10)

DP Operations 3,007 79,231
4,606,719 4,425,388,799
12,887,551 4,236,287

AVG(CTFDP/DP) Operations

 = 0.3  = 0.5  = 0.8  = 1.0

0.8134 0.8136 1.0266 1.693

0.1199 0.1199

0.123 0.2127

0.006903 0.006904 0.007029 0.0099

1.136e-5 1.137e-5 1.214e-4

--

0.003262 0.003263 0.0052 0.0811

0.03261 0.03262 0.0663 0.3306

Table 5.1: The performance of CTFDP relative to DP.

The variation in CTFDP decoding performance is evident in the cross section of decoding

trials displayed in the appendix. The immediate determinant of the number of CTFDP

decoding operations is the number of dynamic programming iterations performed in any

given trial. This statistic is particularly interesting because it also equals the number of level

p - 1 super-states participating in the terminal DP iteration. In principle, it can range from

log2 N (p-1) +1 = min(n,

p-1 -1

)+1 to N (p-1) = min(2n, mp--11) for the code RM (n)(p-, p).

57

At low noise levels, this statistic is close to its lower bound, indicating that each successive optimal derivation tree tends to be a refinement of its predecessor. However, at higher noise levels successive optimal derivation trees are unlikely to be related, creating a surfeit of shattered super-states. For the moderate noise levels considered in our cross section of individual trials, the number of DP iterations remains well below its maximum and the CTFDP algorithm almost uniformly outperforms the standard DP algorithm.
5.3 Conclusion
In this thesis, we have demonstrated that efficient dynamic programming algorithms for maximum likelihood decoding can be designed for codes exhibiting considerable grammatical structure. Of course, much of the substance of this claim is not new. Viterbi [11] introduced his eponymous algorithm for decoding convolutional codes in 1967; Wolf [12] generalized it to the class of linear block codes in 1978. And Forney's [4] original strategy for decoding iterated squaring constructions is ultimately equivalent to our own algorithm (of Section 2.2) for decoding bent Reed­Muller codes. However, the grammatical approach to codes and decoding algorithms provides a powerful tool for both unifying and extending the range of dynamic programming techniques used in coding theory.
Code design is one important area that our grammatical approach might transform. Much of the current work ([1], [4], [7]) in this field involves the construction of minimal trellis representations for codes with specific properties (e.g. length, size, minimum distance, state space cardinality, etc.). Since a trellis diagram is simply the graphical representation of a formal language, one should focus on its underlying grammatical structure. Often grammatical production rules are far more easily specified and manipulated than their graphical analogs. For example, most trellis diagrams for iterated squaring constructions are too complicated to visualize in detail [4]. Moreover, the grammatical method transcends traditional algebraic techniques, facilitating the construction of new classes of nonlinear codes.
In addition, the grammatical approach is essential for designing efficient dynamic programming algorithms for exact maximum likelihood decoding. For communications channels with factorizable likelihoods, one can directly construct generalized Viterbi algorithms that exploit a code's grammatical structure. For example, in Chapter 2 we presented grammati-
58

cal algorithms to compute maximum likelihood codewords and their posterior probabilities for both memoryless and Markov channels. Moreover, within our grammatical framework, the computational complexity of these algorithms was effortlessly established. Finally, the most striking argument for the adoption of a grammatical approach to coding theory must be the performance of our coarse-to-fine decoding algorithm for thinned Reed­Muller codes; for a wide range of noise levels and all but the smallest of codes, this essentially grammatical algorithm outperforms the standard Viterbi (DP) algorithm by a wide margin.
59

Appendix A
Decoding Simulations
60

In this appendix, we present the results of simulated decoding trials for six representative thinned Reed­Muller codes: RM(2,5), RM(2,6), RM(2,7), RM(3,7), RM (10)(4, 8), and RM (8)(6, 10) (ordered by size). For each of these codes, fifty randomly selected codewords are transmitted across a simulated (memoryless) bipolar communications channel with additive gaussian noise. In other words, the bits 0 and 1 of each codeword are mapped first to the respective signals -1 and 1 which are in turn corrupted by adding noise independently drawn from the distribution N (0, 2). The resulting channel outputs are thereupon decoded by three different procedures: DP maximum likelihood decoding (Section 2.2); CTFDP maximum likelihood decoding (Section 4.3); and thresholded hard decoding (in which c^(d) is the minimum distance decoding of the binary string whose bits are independently most likely). This third procedure is commonly used to approximate maximum likelihood decoding.
For each code, we display a table of the simulated decoding results headed by the code name, length, dimension, and minimum distance. We also display the number of required DP decoding operations (equation 2.2.1) and the fixed noise level . Column (A) simply labels the trials. Column (B) presents the distance d(c, c^) between the sent codeword c and the maximum likelihood codeword c^ (computed by DP or CTFDP); if this distance is zero, the decoding scheme has corrected any transmission errors. Column (C) displays the posterior probability p(c^|d), providing a measure of decoding reliability. Column (D) shows the distance d(c, c^ ) between the sent codeword and the result c^ of thresholded hard decoding. Column (E) presents the ratio CTFDP decoding operations to DP decoding operations. Column (F) displays the number of DP iterations required by a given CTFDP decoding trial. The average ratio of CTFDP decoding operations to DP decoding operations is displayed in each table's final line. Of course, since DP decoding is infeasible for RM(3,7), columns (C) and (D) remain blank.
In each case, the noise parameter  is deliberately chosen to produce a sample of trials in which maximum likelihood decoding is beginning to diminish in reliability and the number of CTFDP operations is beginning to fluctuate. At noise levels below this threshold, maximum likelihood decoding performs perfectly and the CTFDP decoding algorithm is uniformly rapid. In contrast, above this threshold the performance of the maximum likelihood decoding scheme and the speed of its CTFDP implementation diminish rapidly as noise overwhelms the signal.
61

RM(2,5) [32,16,8]  = 0.80 DP decoding operations = 3,007

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0

(C) p(c^|d) 0.9998 0.9999 0.9567 0.8953 0.8325 0.9999 0.9998 0.5862 0.9125 0.4875 0.4144 0.9998 1.0000 0.9991 0.6174 0.9918 1.0000 0.7403 0.9981 0.9949 0.9999 0.7528 1.0000 1.0000 0.9774

(D) d(c, c^ )
8 0 8 8 8 0 0 8 0 8 8 0 0 0 8 0 0 0 8 8 0 12 0 0 8

(E) CTFDP/DP
0.8188 0.8134 1.0589 1.2551 1.5284 0.8134 0.8134 1.7918 0.8380 2.4360 3.3818 0.8161 0.8161 0.8134 2.3788 0.8354 0.8134 1.3119 0.8134 0.8161 0.8134 1.8224 0.8161 0.8161 0.8168

(F) DPIs
7 7 8 9 10 7 7 11 7 13 16 7 7 7 13 7 7 9 7 7 7 11 7 7 7

62

RM(2,5) [32,16,8]  = 0.80 DP decoding operations = 3,007

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 0.9534 1.0000 1.0000 1.0000 0.9998 0.9998 0.9984 0.9944 0.9894 0.9988 0.9926 1.0000 0.9998 0.9611 0.9354 0.9999 1.0000 1.0000 1.0000 1.0000 1.0000 0.9998 0.9999 0.8855 0.9999

(D) d(c, c^ )
8 0 0 0 0 0 8 0 8 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0

(E) CTFDP/DP
0.8161 0.8134 0.8134 0.8134 0.8161 0.8134 0.8134 0.8188 0.8161 0.8161 0.8188 0.8134 0.8134 1.0229 1.0216 0.8161 0.8134 0.8134 0.8134 0.8161 0.8134 0.8161 0.8161 1.3089 0.8134 1.0266

(F) DPIs
7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 7 7 7 7 7 7 7 7 9 7

63

RM(2,6) [64,22,16]  = 1.00 DP decoding operations = 79,231

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 1.0000 0.9880 0.3790 1.0000 0.9990 0.9974 1.0000 0.9844 0.9548 0.9981 0.9997 0.9971 0.9948 1.0000 1.0000 0.9744 0.9995 1.0000 1.0000 0.9999 0.9954 0.9962 0.9990 0.9927 1.0000

(D) d(c, c^ )
16 16 16 0 0 0 0 24 16 0 16 16 0 0 0 16 0 0 0 0 16 0 0 16 0

(E) CTFDP/DP
0.1426 0.2834 0.9728 0.1439 0.1425 0.1941 0.1201 0.2870 0.2238 0.1649 0.2015 0.2241 0.1207 0.1201 0.1203 0.3068 0.1405 0.1203 0.1205 0.1379 0.1201 0.1706 0.1224 0.3211 0.1221

(F) DPIs
12 17 38 12 12 14 11 17 15 13 14 15 11 11 11 18 12 11 11 12 11 13 11 19 11

64

RM(2,6) [64,22,16]  = 1.00 DP decoding operations = 79,231

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 1.0000 1.0000 1.0000 0.9910 1.0000 0.9979 0.9994 1.0000 0.5866 0.9983 1.0000 1.0000 0.9512 0.8996 0.9994 1.0000 0.8790 0.9999 0.9968 0.9998 0.9997 0.9999 0.9987 1.0000 0.9688

(D) d(c, c^ )
0 0 0 0 0 0 0 0 24 0 0 0 24 16 16 0 16 0 0 0 16 0 16 0 0

(E) CTFDP/DP
0.1220 0.1203 0.1199 0.1405 0.1203 0.1438 0.1970 0.1223 1.0048 0.1260 0.1205 0.1223 0.4098 0.4592 0.2888 0.1217 0.3968 0.1711 0.1953 0.1434 0.1446 0.1260 0.1423 0.1222 0.3105 0.2127

(F) DPIs
11 11 11 12 11 12 14 11 38 11 11 11 21 23 17 11 21 13 14 12 12 11 12 11 18

65

RM(2,7) [128,29,32]  = 1.00 DP decoding operations = 4,606,719

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000

(D) d(c, c^ )
0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 32 0 0 0 0 0 0 0

(E) CTFDP/DP
0.0092 0.0375 0.0080 0.0069 0.0070 0.0095 0.0070 0.0069 0.0095 0.0178 0.0069 0.0070 0.0080 0.0093 0.0080 0.0133 0.0069 0.0093 0.0110 0.0154 0.0111 0.0079 0.0080 0.0069 0.0070

(F) DPIs
18 39 17 16 16 18 16 16 18 25 16 16 17 18 17 21 16 18 19 23 19 17 17 16 16

66

RM(2,7) [128,29,32]  = 1.00 DP decoding operations = 4,606,719

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000

(D) d(c, c^ )
0 0 0 0 0 0 0 0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(E) CTFDP/DP
0.0080 0.0081 0.0081 0.0104 0.0125 0.0081 0.0070 0.0070 0.0227 0.0070 0.0080 0.0071 0.0144 0.0106 0.0091 0.0069 0.0095 0.0071 0.0123 0.0072 0.0157 0.0071 0.0095 0.0071 0.0099 0.0099

(F) DPIs
17 17 17 19 20 17 16 16 29 16 17 16 22 19 18 16 18 16 20 16 23 16 18 16 19

67

RM(3,7) [128,64,16]  = 0.80 DP decoding operations = 4,425,388,799

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d)

(D) d(c, c^ )

(E) CTFDP/DP
0.0001048 0.0009635 0.0003482 1.579e-05 2.306e-05 9.181e-05 2.292e-05 1.377e-05 1.198e-05 9.823e-05 1.18e-05 1.458e-05 1.539e-05 1.739e-05 1.26e-05 1.538e-05 0.0001487 3.172e-05 1.251e-05 0.0001047 4.471e-05 1.876e-05 3.494e-05 1.169e-05 3.169e-05

(F) DPIs
72 255 144 24 30 66 29 23 21 67 21 23 24 25 22 24 89 35 22 70 42 26 37 21 34

68

RM(3,7) [128,64,16]  = 0.80 DP decoding operations = 4,425,388,799

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d)

(D) d(c, c^ )

(E) CTFDP/DP
0.0004268 8.96e-05 5.287e-05 4.906e-05 3.303e-05 2.125e-05 0.0001984 2.507e-05 2.147e-05 1.863e-05 5.773e-05 1.154e-05 0.002489 3.066e-05 3.537e-05 1.239e-05 7.26e-05 1.819e-05 1.575e-05 1.531e-05 1.687e-05 1.974e-05 1.966e-05 2.668e-05 6.965e-05 0.0001214

(F) DPIs 161
64 48 45 36 28 103 30 28 26 50 21 411 34 37 22 57 26 24 24 25 27 27 32 56

69

RM (10)(4, 8) [256,118,16]  = 0.80 DP decoding operations = 12,887,551

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 0.9489 1.0000 1.0000 0.9976 0.9999 1.0000 1.0000 1.0000 1.0000 0.9838 0.9957 0.9996 0.9827 0.7814 0.9993 1.0000 1.0000 0.9989 0.9997 0.9993 1.0000 1.0000 1.0000 1.0000 0.9664

(D) d(c, c^ )
60 0 24 0 32 0 0 16 0 0 0 0 28 60 24 16 0 0 36 0 28 0 0 0 16

(E) CTFDP/DP
0.0085 0.0043 0.0037 0.0043 0.0048 0.0033 0.0033 0.0033 0.0038 0.0097 0.0052 0.0048 0.0089 0.0124 0.0049 0.0043 0.0033 0.0033 0.0060 0.0082 0.0033 0.0038 0.0038 0.0033 0.0128

(F) DPIs
20 13 12 13 14 11 11 11 12 23 15 14 21 25 14 13 11 11 16 20 11 12 12 11 27

70

RM (10)(4, 8) [256,118,16]  = 0.80 DP decoding operations = 12,887,551

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 1.0000 1.0000 1.0000 0.9989 0.9995 0.6291 1.0000 1.0000 1.0000 1.0000 0.8970 0.9988 1.0000 1.0000 0.9972 0.9999 0.9926 1.0000 1.0000 0.9948 1.0000 1.0000 1.0000 0.9929 1.0000

(D) d(c, c^ )
24 16 0 16 36 40 24 0 32 32 44 16 36 0 44 0 16 0 0 40 36 0 40 40 0

(E) CTFDP/DP
0.0033 0.0037 0.0033 0.0052 0.0071 0.0129 0.0033 0.0037 0.0065 0.0038 0.0120 0.0048 0.0048 0.0033 0.0054 0.0043 0.0054 0.0033 0.0033 0.0033 0.0038 0.0033 0.0033 0.0060 0.0033 0.0052

(F) DPIs
11 12 11 15 18 27 11 12 17 12 26 14 14 11 15 13 15 11 11 11 12 11 11 16 11

71

RM (8)(6, 10) [1024,440,16]  = 0.80 DP decoding operations = 4,236,287

(A) Trial
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

(B) d(c, c^)
0 0 0 0 24 0 24 0 0 0 0 16 0 0 16 0 0 16 0 0 0 0 0 0 0

(C) p(c^|d) 0.9961 0.9968 0.9999 0.9960 0.8850 0.9736 0.4450 0.9953 0.9948 0.7854 0.9999 0.3720 0.9999 0.9838 0.8022 0.9997 0.6635 0.9576 0.9996 0.9993 0.9923 0.3587 0.9994 0.9710 0.9997

(D) d(c, c^ )
52 40 24 88 52 80 64 68 64 64 52 112 56 56 92 52 68 56 64 84 88 48 64 108 60

(E) CTFDP/DP
0.0329 0.0327 0.0328 0.0330 0.0327 0.0425 0.1926 0.0426 0.0327 0.0525 0.0328 0.1104 0.0328 0.0423 0.0889 0.0328 0.1308 0.0677 0.0327 0.0329 0.0526 0.1445 0.0329 0.0330 0.0379

(F) DPIs
10 10 10 10 10 12 39 12 10 14 10 25 10 12 21 10 29 17 10 10 14 31 10 10 11

72

RM (8)(6, 10) [1024,440,16]  = 0.80 DP decoding operations = 4,236,287

(A) Trial
26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 AVG

(B) d(c, c^)
0 0 0 0 16 0 0 0 0 0 0 16 0 0 0 0 0 0 0 0 0 0 0 0 0

(C) p(c^|d) 0.2748 0.9937 0.4483 0.9352 0.4291 0.9979 0.8818 0.9988 0.9990 0.6631 0.8959 0.5427 0.9998 0.9927 0.9936 0.9237 0.4745 0.5098 0.8833 0.9745 0.5249 0.7052 0.9491 0.5803 0.9656

(D) d(c, c^ )
64 40 80 80 84 76 48 60 44 68 68 80 64 104 44 48 76 72 32 60 72 52 64 52 60

(E) CTFDP/DP
0.2516 0.0425 0.1155 0.0576 0.0887 0.0327 0.0329 0.0475 0.0377 0.1367 0.0775 0.0623 0.0328 0.0473 0.0520 0.0627 0.0779 0.0832 0.1030 0.0476 0.0729 0.0476 0.0676 0.1680 0.0376 0.0663

(F) DPIs
48 12 26 15 21 10 10 13 11 30 19 16 10 13 14 16 19 20 24 13 18 13 17 35 11

73

Bibliography
[1] Esmaeli, M., Gulliver, A., and Secord, N.. "Quasi-cyclic structure of Reed­Muller codes and their smallest regular trellis diagram." IEEE Trans. Inform. Theory 43(1997):1040­52.
[2] Forney, G. D.. "The Viterbi algorithm." Proc. IEEE 61(1973): 168­78. [3] Forney, G. D.. "Convolutional codes II: maximum likelihood decoding." Information
and Control 25(1974): 222­66. [4] Forney, G. D.. "Coset codes--Part II: binary lattices and related codes." IEEE Trans.
Inform. Theory 34(1988):1152­87. [5] Geman, S.. "Codes from production rules, and their maximum-likelihood decoding."
In preparation (1997). [6] Hopcroft, J. and Ullman, J.. Introduction to Automata Theory, Languages, and Com-
putation. Reading, Mass.: Addison-Wesley,1979. [7] Muder, D. J.. "Minimal trellises for block codes." IEEE Trans. Inform. Theory
34(1988):1049­53. [8] Raphael, C.. "Coarse-to-fine dynamic programming." In preparation (1997). [9] Raphael, C. and Geman, S.. "A grammatical approach to mine detection." In Detection
and Remediation Technologies for Mines and Minelike Targets II. Eds. Dubey, A. C. and Barnard, R. L.. Proceedings of SPIE 3079(1997):316­332. [10] Roman, S.. Coding and Information Theory. New York: Springer-Verlag, 1992. [11] Viterbi, A. J.. "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm." IEEE Trans. Inform. Theory 13(1967):260­9.
74

[12] Wolf, J. K.. "Efficient maximum likelihood decoding of linear block codes using a trellis." IEEE Trans. Inform. Theory 24(1978):76­80.
75

