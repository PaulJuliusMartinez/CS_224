Theory of Support Vector Machines
M. O. Stitson, J. A. E. Weston A. Gammerman, V. Vovk, V. Vapnik
Technical Report CSD-TR-96-17 December 31, 1996
!2-3(.4/)05+,61
Department of Computer Science Egham, Surrey TW20 0EX, England

The main text of this document is set in Computer Modern 11pt. This document was prepared with LATEX using a PC running Linux. It was printed on a 600 dpi Hewlett Packard laser printer.

Contents

Introduction
1 Introduction 1.1 The problem 1.2 Neural networks as a solution 1.3 Support Vector machines as a solution
2 Pattern Recognition: The Optimal Seperating Hyperplane
3 Pattern Recognition: The Support Vector Machine 3.1 Kernel Functions 3.2 Non-Separable Cases
4 Regression Estimation 4.1 Using a convolved inner product 4.2 Controlling the generalisation ability 4.3 Calculating the value of an unseen example
A Useful formulae A.1 SV Machines for Pattern Recognition A.1.1 Equation 2.17 A.1.2 Equation 2.24 A.2 SV Machines for Regression Estimation A.2.1 Equation 4.10
Bibliography

4
7 7 9 9
11
17 19 20
21 23 23 24
25 25 25 25 26 26
27

Abstract
Support Vector Machines perform structural risk minimisation. They create a classi er with minimised VC Dimension. If the VC Dimension is low, the expected probability of error is low as well, which means good generalisation. Support Vector Machines use a linear separating hyperplane to create a classier, yet some problems cannot be linearly separated in the original input space. Support Vector Machines can non-linearly transform the original input space into a higher dimensional feature space. In this feature space it is trivial 1 to nd a linear optimal separating hyperplane. This hyperplane is optimal in the sense of being a maximal margin classi er with respect to the training data. Open problems with this approach lie in two areas:
The theoretical problem of which non-linear transformation to use. The practical problem of creating an e cient implementation as the basic algorithm has memory requirements which are squared with respect to the number of training examples and computational complexity which is also related to the number of training examples.
1Trivial only in theory, in practice this is still a computationally hard problem.

Chapter 1
Introduction
This chapter will introduce the basic idea behind support vector machines and why we use them. It starts with a general description of the problem and then gives several solution approaches.
1.1 The problem
Learning is a wide eld of which we will only concentrate on a small sub eld here. The problem we look at initially is the problem of nding binary classi ers. Say we are given the weight and height of a person, we want to nd a way of determining their gender. If we are given a set of examples with height, weight and gender, we can come up with a hypothesis which will enable us to determine a persons gender from their weight and height. This hypothesis might be wrong, but it hopefully is the best we can acheive from the data. This learning from examples can also be applied to function approximation or regression estimation. In mathematical terms this can be described by plotting the weights and heights in a two-dimensional coordinate system and then drawing a dividing line or separating hyperplane which divides our weight/height points into two regions, one female and one male.
No. Height Weight Gender 1 180 80 m 2 173 66 m 3 170 80 m 4 176 70 m 5 160 65 m 6 160 61 f
continued on next page

8 INTRODUCTION

continued from previous page

No. Height Weight Gender

7 162 62

f

8 168 64

f

9 164 63

f

10 175 65

f

If we plot this table we get the following picture, where the black dots are classi ed as female and the white dats as male:
82 81 80 79 78 77 76 75 74 73 72 71 70 69 98 67 66 65 64 63 62
01
02 150 154 158 162 166 170 174 178 182
It can be seen that there are a number of lines we can draw to divide the space into two regions. From an intuitive viewpoint the best line would be the line that has the furthest distance to any of our training points. This is because a point near a male point is probably male and a point near a female point is probably female, i.e. two people with the same weight and height are probably of the same sex. The best line or optimal separating hyperplane would then roughly be:
82 81 80 79 78 77 76 75 74 73 72 71 70 69 98 67 66 65 64 63 62
01
02 150 154 158 162 166 170 174 178 182

Neural networks as a solution 9
If we can nd such a separating hyperplane automatically from the training data, as a mathematical function, it is possible using basic geometry to determine which side of the line any given point lies and so to make a classi cation of unseen examples.
1.2 Neural networks as a solution
Most neural networks are designed to nd a separating hyperplane. This is not necessarily optimal. In fact many neural networks start with a random line and move it, until all training points are on the right side of the line. This inevatibly leaves training points very close to the line in a non-optimal way.
There are however now approaches in which a large margin classi er, i.e. a line approaching the optimal is sought.
1.3 Support Vector machines as a solution
Support Vector Machines use geometric properties to exactly calculate the optimal separating hyperplane directly from the training data. They also introduce methods to deal with non-linearly separable cases, i.e. where no straight line can be found, and cases in which there is noise in the training data, i.d. some of the training examples are wrong. With the ideas of chunking SV machines can also be used for certain forms of online learning.

Chapter 2 Pattern Recognition: The Optimal
Seperating Hyperplane

In gure 2 it is easy to determine a separating hyperplane given the training data

manually.

( ) ( ) +1 11 1x y

:::

xl yl

x

2

n
R

y

2f

;g

The hyperplane (w x) + b = 0 satis es the conditions:

which is equivalent to1

(w )xi + b > 0 if yi = 1 (w )xi + b < 0 if yi = ;1

(w xi) + b 1 if yi = 1 (w xi) + b ;1 if yi = ;1
1given some constant factor, i.e. a di erent w and b

(2.1) (2.2)

12 PATTERN RECOGNITION: THE OPTIMAL SEPERATING HYPERPLANE which is equivalent to

( ) + ] 1 = 1yi w xi

b

i :::l

This however is not a good hyperplane as some points are very near the hyperplane and points near them might be classi ed di erently although intuitively they should be the same class. Therefore it would be preferable to have a hyperplane with a large margin or even the largest possible margin. This maximum margin hyperplane is known as the optimal separating hyperplane.
In our example it would look like this:

To nd the optimal separating hyperplane we have to nd the hyperplane which satis es the above condition2 and maximises the minimum distance between the hyperplane and any sample of the training data.

(w b) = fximjyiin=1g w

+xi
jwj

b

;

fxjmjyja=x;1g

w

+xj b
jwj

(2.3)

is the sum of the distances from the nearest two points to the separating hyperplane, which has to be maximised.
From equation 2.3 and equations 2.1 follows:

(w b)

=

fximjyiin=1g

1
jwj

;

fxjmjyja=x;1g

;1
jwj

,

(w b)

= 1 ; ;1
jwj jwj

,

(w b)

=2
jwj

, (w b) = p 2

ww

The optimal seperating hyperplane therefore minimises

(w)

=

1 2w

w

2This is true for all separating hyperplanes.

(2.4) (2.5) (2.6) (2.7)

13
with respect to both the vector w and the scalar b. The solution to this problem can be found by nding the saddle point of the Lagrange functional:

X(L w b

)

=

1 2

w

l
w;
i=1

(i f xi

w) + ]b yi ; 1g

(2.8)

where i are Lagrange multipliers. The Lagrangian has to be minimised with respect to w, b and maximised with respect to i 0.
In the saddle point the solutions w0, b0 and 0 should satisfy the following conditions:

P = 0@L(w0 b0 0) @b

,

li=1

0
i

yi

=0

(2.9) (2.10)

P@L(w0 b0 0) @w

=0

, w0 ;

il=1

0
i

xi

yi

=0

(2.11) (2.12)

This results in the following properties of the optimal seperating hyperplane:

1. Constraint on the coe

cients

0
i

from

equation

2.10:

Xl

0
i yi

=

0

i=1

0 = 10
i i ::: l

(2.13)

2. The vector w0 is a linear combination of the vecors in the training set (from equation 2.12):

= Xw0

l0
i yixi

i=1

0 = 10
i i ::: l

(2.14)

3. Only the so-called support vectors can have non-zero coe

cients

0
i

in

the

expansion of w0:

14 PATTERN RECOGNITION: THE OPTIMAL SEPERATING HYPERPLANE

w0 = X
support vectors

0
i yixi

0
i

>

0

(2.15)

The Kuhn-Tucker Theorem states that the necessary and su cient con-
ditions for the Optimal hyperplane are that the separating hyperplane satis es the conditions:

0
i

f

(xi

w0) + b0]yi ; 1g = 0 i = 1 : : :

l

as well as being a saddle point.

(2.16)

Substituting these results into the Lagrangian, the following functional is obtained:

W( ) = = = =

P21 w w ; li=1 (i f xi w) + ]b yi ; 1g

P P P( ) +12 w w ; li=1 iyi xi w ; b li=1 iyi

il=1

P0 +21 w w ; w w ;

li=1 i

P P ( )il=1 i ; 12

l i

j=1

i j yiyj xi

xj

i

This functional has to be maximised in the non-negative quadrant,

(2.17) (2.18) (2.19) (2.20)

under the constraint

0 = 1i i : : : l

(2.21)

Xl
iyi = 0
i=1

(2.22)

Once the solution to this problem has been found in form of a vector 0 = ( 10 : : : 0l ), an indicator function can be constructed:

f(x) = sign;

X

( )yi

0
i xi

x ; b0

support vectors

(2.23)

where xi are the support vectors,

0
i

are

the

Lagrange

multipliers

and

b0

is

the

threshold

15

b0

=

1 2

(w0

x (1)) + (w0

x (;1))]

(2.24)

where x (1) is any support vector belonging to the rst class and x (;1) is any support vector belongign to the second class.
This solution only holds for separable data, but has to be slightly modi ed for non-separable data.
For non-separable data the i0's have to be bounded:

0

0
iC

where C is a constant choosen a priori. For more information on this see Vap95] pages 131-133.

(2.25)

Chapter 3
Pattern Recognition: The Support Vector Machine
Support Vector machines form an extension to the optimal separating hyperplane method. They map the input space into a high-dimensional feature space through some non-linear mapping choosen a priori and then construct the optimal separating hyperplane in the feature space. This makes it possible to construct linear decision surfaces in feature space which correspond to nonlinear decision surfaces in input space. Taking the example from the previous section with one extra point
it is obvious that no linear separating hyperplane can be found, yet if we could use a non-linear separating hyperplane, which is equivalent to mapping into a high dimensional space, a separation is possible:
The main problem that arises here is how to handle the problem of mapping all vectors into the feature space. This mapping would generate vectors with high dimensionality entailing large storage and computational expense. This curse of dimensionality however turns out not to be a problem as all we ever need is the dot product of the vectors in feature space. If we choose a mapping from the original space X to the higher dimensional

18 PATTERN RECOGNITION: THE SUPPORT VECTOR MACHINE space Z (k : X ! Z):
k(x) = z where x 2 X and z 2 Z then we would have to maximise1

X XW (

l
)=
i=1

1l i ; 2 i j=1

i

( ( )j yiyj k xi

k(xj ))

(3.1)

to nd the Lagrangian multipliers under the constraints from the previous chapter and calculate the value of the classi er

f(x) = sign( X yi
support vectors

0
i

(k(xi

)

k(x) ; b0)

(3.2)

as before.
We do not have to know k(x) as long as we know (K xi )xj = k(xi) k(xj).
According to Hilbert-Schmidt theory (K xi )xj can be any symetric function satisfying the following conditions CH53]:

Theorem 3.1 (Mercer) To garantuee that the symetric function (K xi )xj has
the expansion

X1

(K xi xj) =

k k(xi) k(xj)

k=1

(3.3)

with positive coe cients k > 0 (i.e. (K xi )xj describes an inner product in

some feature space), it is neccessary and su cient that the condition

ZZ

( ) ( ) ( ) 0K xi xj g xi g xj dxidxj >

(3.4)

be valid for all g =6 0 for which
Z
( )2g xi dxi < 1

(3.5)

A simple example of this is a mapping from a two dimensional space into a six dimensional space:
1l is the number of training examples.

Kernel Functions 19
Say there exists a transformation k : x ! z such that the inner product in the six dimensional space can be described by ((xi xj) + 1)2. This can be simply proven:

((xi )xj ; 1)2 =

( + ) + 2( + ) + 1xi1xj1

2
xi2 xj2

xi1 xj1 xi2 xj2

(3.6)

,

=

( ) + 2( )( ) + ( ) +2
xi1 xj1

xi1 xj1 xi2 xj2

2
xi2 xj2

(3.7)

2(xi1 xj1) + 2(xi2 xj2) + 1

(3.8)

)k

x1 x2

!

=

BB0B@BBBBppp2xx22x1222xx1x21 2

1 CACCCCCC

1

3.1 Kernel Functions

Depending on the kernel function used di erent type of SV machine are constructed:

A polynomial machine is constructed using:

( ) = ; = 1K xi xj

d xi xj d

:::

(3.9)

An alternative polynomial machine, which avoids some problems encountered with the above2, is constructed using:

;(K xi )xj = (xi xj) + 1 d d = 1 : : :

(3.10)

A radial basis function machine with convolution function:

(K xi xj) = exp

;

(xi

; xj)2
2

(3.11)

A two layer neural network machine with convolution function:

;(K xi )xj = tanh (b xi )xj ; c

(3.12)

2The determinant of the hessian can become zero, which make solving the Lagrangian impossible.

20 PATTERN RECOGNITION: THE SUPPORT VECTOR MACHINE

This is an interesting experimental kernel function, which might work

Xl
( 1K x x2) = (x1 ; tk)(x2 ; )tk
k=1

(3.13)

where stk are from the samples. This however is not a function determined a priori and thus is not strictly a proper dot product.

Finding a suitable kernel function is the most ill-de ned area of using SV machines.

3.2 Non-Separable Cases

Again non-separable case can be treated very much like the separable case, except that there are further constraints on the i:

0 iC

(3.14)

For erroneously classi ed vectors the following holds true:

i C (3.15)

For more information on this see Vap95].

Chapter 4 Regression Estimation

Huber de ned the least modulus method for regression estimation (1964). This is the so called robust regression function. It is the best function if nothing speci c is known about the model of noise. This provides the loss-function:

( (L y f x )) = jy ; f (x )j

(4.1)

We will consider a slightly more general type of loss function than 4.1, the so-called linear loss-function with -insensitive zone:

(jy ; f x

)j = (jy ; f x

)j

if jy ; (f x )j otherwise

(4.2)

Huber's loss-function 4.1 is a particular case of the loss-function 4.2 with = 0. Again it is possible to use the SRM principle, where

( )w w

cn

(4.3)

Given the training data (x1 y1) : : : (xl )yl the problem is to minimise the empirical risk

XRemp(w b) = 1 l (jyi ; w )xi ; bj
l i=1

(4.4)

22 REGRESSION ESTIMATION

under the constraint 4.3 which is equivalent to nding the pair w b that min-

imises the quantity de ned by the slack variables

i

= 1i i

:::

l

F ( ) = ; Xl i + Xl i
i=1 i=1

(4.5)

under the constraints

(w xi) + b (w xi) + b
i i

y;
y+

;
+

i i

0

0

= 1i : : : = 1i : : : = 1i : : : = 1i : : :

l l l l

(4.6)

and constraint 4.3.
As in the pattern recognition case the solution can be found by nding the saddle point of the Lagrange functional

(L w C ) = Xl ( i + i) ;

i=1

Xl
(i yi ; w xi) ; b + + i] ;

i=1

Xl
(i w )xi + ;yib + + i ] ;
i=1

(4.7)

C
2

(cn ; (w

w)) ;

Xl ( i i + i i)

i=1

This has to be minimised with respect to w, b, i and i and maximised with respect to the Lagrange multipliers C 0 , i 0 , i 0 , i 0 and i 0 for i = 1 : : : l.
As this leads to a complicated optimisation procedure, a simpler approach can be used, if instead of minimising the functional 4.5 under the appropriate constraints, one minimises

(x

)

=

1 2

(w

w) + C; Xl
i=1

i + Xl
i=1

i

(4.8)

Using a convolved inner product 23
for a given value of C and subject to the constraints 4.6. To nd the desired vector

Xl
w = ( i ; )i xi
i=1

(4.9)

one has to nd the coe cients i and i, such that the quadratic form

W(

) = ; Xl (
i=1

i+

i) + Xl (yi
i=1

i;

i)

;

1 2

Xl (
i j=1

i;

i)(

j;

j)(xi )xj (4.10)

is maximised subject to the constraints

Xl i = Xl i
i=1 i=1

0 = 1i C i

::: l

0 = 1i C i

::: l

(4.11)

The simpli ed approach coincides with the full approach when C = C . The solution of this problem has some nice properties:

Only a small number of i s and is will be non-zero. i =6 0 ) i = 0 i =6 0 ) i = 0

4.1 Using a convolved inner product

As with the pattern recognition case we can use a convolved inner product rather than the standard inner product with the same type of kernel functions as well as splines which are very good at handling low dimensional cases.

4.2 Controlling the generalisation ability
By controlling the two parameters cn and (or C and in the quadratic optimisation approach) it is possible to control the generalisation ability of the SV machine.

24 REGRESSION ESTIMATION
4.3 Calculating the value of an unseen example
When all i s and is have been found the function is approximated by

Xl
(f x ) = ( ( i ; ) (i K xi x)) + b
i=1

(4.12)

where b is a unique constant which minimises the error over the training set:

X Xl l

2

min
b i=1

yi ;

( i;
i=1

) (i K xi x)

;b

(4.13)

or

1 2

(min(;yi

+

f

(xi

) ; b) + max(;yj + f (xj

) ; b) (4.14)

or

X Xb0

=

1 2

xjmj ijn>0(;yj

+

l
(
i=1

i;

l

) (i K xj

xi))

+

xkmj ikn>0(;yk

+

(
i=1

i;

) (i K xk xi)) (4.15)

where (yi )xi and (yj )xj are support vectors from the training set.

Appendix A Useful formulae

A.1 SV Machines for Pattern Recognition
A.1.1 Equation 2.17
For many quadratic optimisation packages the rst and second derivative of the objective function 2.17 are needed:

= 1 X ( )@W

l
;

j yiyj K xi xj

@ i j=1

(A.1)

2

= ( )@ W

;yiyj K xi xj

@ i@ j

(A.2)

A.1.2 Equation 2.24
Equation 2.24 can also be written as:

b0

=

X ; (K xi aiyi
support vectors

x

(1))

+ K(xi 2

x

(;1))

(A.3)

26 USEFUL FORMULAE
A.2 SV Machines for Regression Estimation
A.2.1 Equation 4.10
For many quadratic optimisation packages the rst and second derivative of the objective function 4.10 are needed: The rst derivative with respect to the i's:

Xl

= +@W

; ; yi

( j ; ) ( )j K xi xj

@ i j=1

The rst derivative with respect to the i 's:

(A.4)

Xl
= +@W ; yi ; ( j ; ) ( )j K xi xj
@ i j=1

The second derivative with respect to i and j

2

= ( )@ W

;K xi xj

@ i@ j

The second derivative with respect to i and j

2

= ( )@ W

;K xi xj

@ i@ j

The second derivative with respect to i and j

2

= ( )@ W

K xi xj

@ i@ j

The second derivative with respect to i and j

2

= ( )@ W

K xi xj

@ i@ j

(A.5) (A.6) (A.7) (A.8) (A.9)

Bibliography
CH53] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume Volume I. Springer Verlag, Berlin, rst English edition, 1953.
LS92] George F. Luger and William A. Stubble eld. Arti cial Intelligence: Structures and Strategies for Complex Problem Solving. Benjamin / Cummings, Redwood City, California, second edition, 1992.
Sch91] Detlef Lind / Harald Scheid. Abiturwissen Stochastik. Ernst Klett Verlag fur Wissen und Bildung, Stuttgart, fourth edition, 1991.
SW96] M. O. Stitson and J. A. E. Weston. Implementational issues of support vector machines. Technical Report CSD-TR-96-18, Royal Holloway, University of London, December 1996.
Vap82] Vladimir N. Vapnik. Estimation of Dependences Baed on Empirical Data. Springer, New York, 1982.
Vap95] Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.
VC79] Vladimir N. Vapnik and A. J. Cervonenkis. Theorie der Zeichenerkennung. Akademie-Verlag, Berlin, 1979. Translated from the Russian original (1974).
Wal86] Ing. Rudolf Walther, editor. Worterbuch der Technik EnglischDeutsch. Cornelsen Verlag Schwann-Giradet, Dusseldorf, fth edition, 1986.
Wal88] Ing. Rudolf Walther, editor. Worterbuch der Technik DeutschEnglisch. Cornelsen Verlag Schwann-Giradet, Dusseldorf, fourth edition, 1988.
WSG+96] J. A. E. Weston, M. O. Stitson, A. Gammerman, V. Vovk, and V. Vapnik. Implementational issues of support vector machines.

28 BIBLIOGRAPHY
Technical Report CSD-TR-96-18, Royal Holloway, University of London, December 1996.

