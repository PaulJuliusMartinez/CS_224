1 Support Vector Density Estimation

J. Weston, A. Gammerman, M. O. Stitson, V. Vapnik, V. Vovk, C. Watkins Royal Holloway, University of London J.Weston@dcs.rhbnc.ac.uk

We describe how the SV technique of solving linear operator equations can be applied to the problem of density estimation and how this method makes use of a special type of problem-speci c regularization. We present a new optimization procedure and set of kernels that guarantee the estimate to be a density (be nonnegative everywhere and have an integral of 1). We introduce a dictionary of kernel functions to nd approximations using kernels of di erent widths adaptively. A method of SV regression using square loss is introduced and it is shown how this technique is useful for density estimation. Finally, a way of compressing density estimates from classical kernel based methods is described, and all these algorithms are compared to classical kernel density estimates (Parzen's windows).

1.1 The density estimation problem

We wish to approximate the density function p(x) from data where the correspond-

ing distribution functZionx is
F(x) = P(X x) = p(t)dt:
?1

(If not speci ed otherwise our densities are with respect to the usual Lebesgue

Zme1asure.) Finding the required density means solving the linear operator equation1

(x ? t)p(t)dt = F(x);
?1

(1.1)

(1.

(x) =

1; 0;

x>0
otherwise

Generic author design sample pages 1999/07/12 15:50

2 Support Vector Density Estimation

where instead of knowing the distribution function F(x) we are given the iid (independently and identically distributed) data

x1; : : :; x`

(1.2)

generated by F. The problem of density estimation is known to be ill-posed. \Ill-posed" means
that when nding p that satis es the equality Ap = F, where A is a linear operator, we can have large deviations in solution p corresponding to small deviations in F. In our terms, a small change in the distribution function of the continuous random variable X can cause large changes in the derivative, the density function. To solve ill-posed problems, regularization techniques can be used.

1.2 SV method of estimating densities

Using the data (1.2) we construct the empirical distribution function

F`(x) =

1 X`
` i=1

(x ? xi)

instead of the right hand side of (1.1), which is unknown. We use the SV method to solve the regression problem of approximating the right hand side, using the data

(x1; F`(x1)); : : :; (x`; F`(x`)):

Applying the SV method of solving linear operator equations Vapnik et al. (1997)

(using yi = F`(xi)), the parameters of the regression function can then be used to express the corresponding density. Regularization is controlled with the parameters

" and C.

This approach can be re ned by further control of the regularization Vapnik

(1998). For any xed point x the random value Fl(x) is an unbiased estimate of

F

(=x)ran1`dFh(xa)s(t1h?e

standard F (x))

deviation

so "i

we =

cain=charra1`cFte`r(ixzei)(t1he?aFcc`(uxrai)c)y;

of

our

approximation

at

the

data

points

with
(1.3)

where is usually chosen to be 1.

Therefore we consider triples

(x1; F`(x1); "1); : : :; (xl; Fl(x`); "`):

(1.4)

We will use a generalization of the usual support vector regression technique (SVR) to allow the value "i to de ne the loss at the training point xi , i = 1; : : :; `; in the usual SV technique "1 = : : : = "`.

Generic author design sample pages 1999/07/12 15:50

1.3 SV density estimation by solving the linear operator equation

3

In the next section we will review how the SV method is used to solve linear operator equations and then use this technique to construct kernels speci cally for density estimation.

1.3 SV density estimation by solving the linear operator equation

To solve the density estimation problem we use the SV method for solving linear operator equations

Ap(t) = F(x);

where operator A is a linear mapping from a Hilbert space of functions p(t) to a Hilbert space of functions F(x). We solve a regression problem in the image space (F (x; w)) and this solution, which is an expansion on the support vectors, can be used to describe the solution in the pre-image space (p(t; w)). The method is as follows. Choose a set of density functions p(t; w) to solve the problem in the pre-image space that are linear in the attening space:
p(t; w) = X1 wr r(t) = (w (t));
r=0
that is, p(t; w) are linear combinations of the functions

(t) = ( 0(t); : : : ; n(t); : : :):

Each p(t; w) can be thought of as a hyperplane in this attening space, where w = (w0; : : :; wn; : : :) are the coe cients to the hyperplane. The result of the mapping from the pre-image to the image space by the operator A can then be

expressed as a linear combination of functions in the image Hilbert space de ned

thus:
F (x; w) = Ap(t; w) = X1 wr r(x) = (w (x));

r=0

where (x) = ( 0(x); : : :; n(x); : : :) and r is the rth function from our set of functions after the linear operator A has been applied, i.e r(x) = A r(t).
The problem of nding the required density ( nding the vector w in the preimage space) is equivalent to nding the vector of coe cients w in the image space,

where w is an expansion on the support vectors

X`
w=

i

(xi);

i=1

giving the approximation to the desired density

p(t; w) = X` i (xi) (t):

i=1

Generic author design sample pages 1999/07/12 15:50

4 Support Vector Density Estimation

To nd the required density we solve a linear regression problem in the image

space by minimizing the same functional we used to solve standard regression

problems Vapnik (1995); Vapnik et al. (1997). Instead of directly nding the in nite

dimensional vector w which is equivalent to nding the parameters which describe the density function, we use kernel functions to describe the mapping from input

space to the image and pre-image Hilbert spaces.

We use the kernel
k(xi; xj) = X1 r(xi) r(xj)

r=0

to represent the inner product in the image space de ned by the set of functions

. We solve the corresponding regression problem in the image space, using the

coe cients to de ne the density function in the pre-image space:

p(t; ) = X` iK(xi; t)

i=1

where K is the so-called cross kernel:
K(xi; t) = X1 r(xi) r(t):

(1.5)

r=0

1.4 Spline approximation of a density

We can look for the solution to equation (1.1) in any set of functions where we can

construct a corresponding kernel and cross kernel. For example, consider the set

of constant splines with in nite number of nodes, similar to Vapnik et al. (1997).

Tp(ht)at=isZw1ega(pp) r(otx?ima)tde

the unknown + a0

density

by

the

function:

0

(which is assumed to be concentrated on 0,1]) where function g( ) and parameter

a0 are toZ
F(x) =

b1ege(st)imZ axted(t.

We ?

thus )dt]d

de +

Znexthe regression
a0dt

problem

in

image

space

=

Z

1

g(

0
)

(x

?

0
)+]d

+ a0x:

0

(1.6)

0

kS(oxtih; xejc)o=rreZsp1o(nxdii?ng

kernel is )+(xj ?

)+d

+ xixj

0

=

(xi

^

xj )2(xi

_

xj )

?

1 2

(xi

^

xj)3

?

1 2

(xi

^

xj)2(xi

_

xj)

+

1 3

(xi

^

xj)3

+

x xi j(1.7)

Generic author design sample pages 1999/07/12 15:50

1.5 Considering a monotonic set of functions

5

SV regression with generalized "-insensitive loss function

where we denoted by (xi^xj) the minimumand (xi_xj) the maximumof two values

xi and K(x; t)

xj Z.
=

The corresponding cross kernel 1 (t ? )(x ? )+d + x

is

(notice

that

d dt

(t

?

)+ =

(t ?

))

0

=

x(x

^

t)

?

(x

^ 2

t)2

+

x:

Using kernel (1.7) and triples (1.4) we can obtain the support vector coe cients

SVi =regire?ssioi,noanplypsrooxmime oaftwiohnicVhaaprneiknoetn-azle.r(o1.9T97h)iswisitahchgeienveerdalbizyedus"in-ignstehnessittaivnedlaorsds

function, by maximizing the quadratic form

W(

;

) = ? X` "i( i +
i=1

i) + X` yi( i ?
i=1

Xi)

?

1 2

`
(
i;j=1

i?

i)( j ?

j)k(xi; xj)

subject to the constraints

X` i = X` i
i=1 i=1
0 i C; i = 1; : : :; `;

(1.8)

0 i C; i = 1; : : :; `:

(1.9)

For density estimation constraint (1.8) can be removed as the threshold b is not used. These coe cients de ne the approximation to the density

p(t) = X`0 i0K(xi0; t)
i=1

where

x0 i

are

the

`0

` support vectors with corresponding non zero coe

cients

.0
i

1.5 Considering a monotonic set of functions
Unfortunately, the described technique does not guarantee the chosen density will always be positive (recall that a probability is always nonnegative, and the distribution function monotonically increases). This is because the set of functions F (x; w) from which we choose our regression in the image space can contain nonmonotonic functions.
We can choose a set of monotonic regression functions and require that the coe cients i, i = 1; : : :; `, are positive. However, many sets of monotonic functions expressed with Mercer Kernels are too weak in their expressive power to nd the desired regression | for example if we choose from the set of polynomials with only positive coe cients.
Using kernels from classical density estimation theory (for example, see ?)) in the SV method means solving a regression problem using a non-Mercer Kernel. In

Generic author design sample pages 1999/07/12 15:50

6 Support Vector Density Estimation
the next section we consider a slightly di erent method of SV regression estimation that allows us to use kernels of this form.

1.6 Linear programming (LP) approach to SV regression estimation

SV regression with L1 norm regularization

In the SV approach, regression estimation problems are solved as a quadratic optimization problem (1.9), giving the approximation

F (x) = X`0 i0k(x0i ; x):

i=1

If we choose as our regularizing term the L1 norm of w (in the usual approach we choose the L2 norm) we are only required to solve a linear program Vapnik (1998). In this alternative approach our regularizing term is the sum of the support vector

weights. This approach is justi ed by bounds obtained in the problem of Pattern

Recognition that the probability of test error is less than the minimum of three

terms, one of which is a function of the number of support vectors.

This gives us the linear program:
min X` i + X` i + C X` i + C X`

!
i

i=1 i=1

i=1 i=1

with yi ?

"co?nstirain@0tsX` (

j?

1 j)k(xi; xj)A + b

yi + " + i ;

i = 1; : : :; `;

j=1

i 0; i 0; i 0; i 0; i = 1; : : :; `:

Minimizing the sum of coe cients is a (convex) approximation to minimizing the number of support vectors. This regularizing term can be seen as a measure of smoothness in input space; a small number of support vectors will mean a less complex decision function. Note that in this approach k(x; x0) does not have to satisfy Mercer's condition. We shall use this freedom to construct kernels to estimate densities from a mixture of Gaussian-like shapes. In general we will consider kernels of the following form: K(x; x0) is a density function, and k(x; x0) is its integral, for any xed x0.

1.7 Gaussian-like approximation of a density

Sigmoid kernel

A common approximation to an unknown density is a mixture of bumps (Gaussianlike shapes). Using the SV method this means approximating the regression in the image space (approximating the unknown distribution function) with a mixture of sigmoidal functions. Considering sigmoids of the form

Generic author design sample pages 1999/07/12 15:50

1.8 SV density estimation using a dictionary of kernels

7

Gaussian-like cross kernel

k(x;

x0)

=

1

+

1 e?

?(x x0)

(1.10)

the approximation of the density becomes

p(x) = X` iK(xi; x)
i=1
where

K(x; x0) = ? 2 + e ?(x x0) + e? :?(x x0)

(1.11)

The chosen centres for the bumps are de ned by the support vectors, and their heights by the size of their corresponding weights. Note that the kernel (1.10) is non-symmetrical and can only be used with the approach to SV regression described in section 1:6.
In fact, we can consider any kernel function from classical density estimation theory ?) (Uniform, Cosine arch,. . . ) which has a known integral in order to construct both kernel and cross kernel functions.
Typically, these kernels have a width parameter which is used to choose the smoothness of the density estimate. We would like to remove this free parameter by considering a set of functions which contains functions of di erent widths. This can be achieved by considering a dictionary of kernel functions.

1.8 SV density estimation using a dictionary of kernels

Density estimate with dictionary of kernels

We would like to estimate the density with kernels of varying widths, allowing the technique to choose the best widths at di erent training points. This can be achieved by considering a dictionary of kernel functions giving the decision function

p(x) = X` ( i1K1(xi; x) + 2i K2(xi; x) + : : : + i K (xi; x))

i=1

where each positive to

vector xi , guarantee

i= our

1; : : :; ` density

, has coe estimate

isciennotns-nejigati0ve,

j = 1; : : :; everywhere

, which are . We then

have a corresponding dictionary of cross kernels, where Ki has the width i, for

example

1 = 21; 2 = 31; : : :

As usual we want many of these coe cients to be zero. As the sum of coe cients is 1 (to make a density) the regularization described in section 1:6 is not suitable for density estimation. Instead, we choose the regularizer

X` X 1 n
i=1 n=1 n i

Generic author design sample pages 1999/07/12 15:50

8
Generalized LP SV regression with dictionary of kernels

Support Vector Density Estimation

to penalize kernels with small width. This results in a linear program. It is possible

to choose other regularizers, and it is not yet clear whether there exist more

appropriate measures of smoothness.

We can thus transform the LP SV regression technique (section 1:6) to the

following optimization problem:

min

X` X 1

n i

+

C

X`

i + C X`

!
i

i=1 n=1 n

i=1 i=1

(1.12)

with constraints

yi ? "i ? i X` X nj kn(xi; xj) yi + "i + i ; i = 1; : : :; `;

j=1 n=1

X` X

n i

=

1;

(1.13)

i=1 n=1

i 0; i 0; i 0; i = 1; : : :; `:

1.9 One more method of SV density estimation

Square loss SVR with L1 norm regularization Generalized square loss SVR with dictionary of kernels

Densities can also be estimated using square loss instead of absolute loss using a

linear regularizer (as introduced in section 1:6). This method gives a quadratic

mopitnimB0@iXz`ati0@onyip?ro42blXe`m(: j ?

312 j)k(xi; xj) + b5A +

X`
( i+

1 i)AC

i=1 j=1

i=1

with constraints

i 0; i 0; i = 1; : : :; `:

The sparsity again comes from the regularizing term. Using this method of

mreginre@0ssXii=o`1n(yfoi r?dXje=n` 1sinXt=y1estjnikmna(txiio;nxwj)i)t2h+kernXi=e`l1snXo=f1di1n

ere1nt An
i

width

we

obtain:

with constraint (1.13) and the constraints

i 0; i = 1; : : :; `:

This solution cannot take advantage of the special regularization that we gained from the "-insensitive loss function. However, the optimization problem we obtain is more suitable to training with a large number of data points as we can employ decomposition methods (as in Osuna et al. (1996, 1997)). This becomes feasible due to the simplicity of the constraints.

Generic author design sample pages 1999/07/12 15:50

1.10 Parzen's windows

9

1.10 Parzen's windows

Classical kernel based density estimates use the decision function

pest(x)

=

1 `

X`
i=1

K(x; xi;

):

We choose the kernel

K(x; xi;

)=

1
N

K

x ? xi

; x 2 RN

where K(u) is a symmetric unimodal density function. The decision function is an expansion on all ` training vectors, rather than just the support vectors. In our experiments we compare density approximations from this classical technique with our techniques.

1.11 Approximating density estimates using SV regression techniques

Approximating density estimates using SVR

The support vector approach can also be used to compress the description of density estimates that are generated by some other method, for example the Parzen's windows estimator. Parzen's windows estimation is an expansion on all of the training data. An approximation to this estimate that has a sparse representation (that uses only some vectors in the training set) can be found using a special kind of regression estimation.
This can be done in the following way: construct the pairs (x1; y1); : : :; (x`; y`) where yi = pest(xi), and xi, i = 1; : : :; `, are the original training data. We then approximate this data with a regression function. If we restrict our set of functions to be densities (that are non-negative everywhere and have an integral of 1) we try to nd a sparse approximation to the density estimate. Accuracy vs complexity of the approximation is controlled by the regularization. This can be achieved by using the regression techniques described in section 1:8 or section 1:9, replacing the kernel k(x; y) with the set of functions you wish to approximate with, for example the radial basis function (RBF) kernel (1.11).

1.12 Multi-dimensional density estimation
To estimate multi-dimensional densities the generalization of the SV method is straightforward. We estimate the density p(x) which has the corresponding distri-
bution function Z Zx1 xN
F(x) = P(X x) = : : : p(t)dt : : :dt
?1 ?1

Generic author design sample pages 1999/07/12 15:50

10 Support Vector Density Estimation

where x = (x1; : : :; xN) 2 RN using the multi-dimensional empirical distribution

function

F`(x)

=

1 `

X`
i=1

(x1 ? xi1) : : :

(xN ? xNi ):

Multi-dimensional kernels can be chosen to be tensor products of one dimensional

kernels, or other kernels can be chosen directly.

1.13 Experiments

We considered a density generated from a mixture of two normal distributions

p(x; ; ) = 2 p1(2 )exp

?

(x

? 2

2

)2

+ 2p1(2 )exp

?

x2 2

(1.14)

where = ?4 , distribution, and

e=stim21 .aWteed

drew 50 , 75, 100 and 200 examples generated by this the density by choosing the best value of parameters

for each of the following four techniques:

Linear Programming SVM with "-insensitive loss (EL-SVM) (section 1:8). Here = 1 (which xes ") and C is a free parameter (although it is typically close to 1). Square loss SVM (SL-SVM) (section 1:9). This has the free parameter . Approximating Parzen's windows estimates using SV regression (section 1:11). We xed C = 1 and adjusted the free parameter " (we do not use di erent values of " at di erent training points in this case). Parzen's windows (section 1:10), controlling the kernel width .

In all techniques we used the RBF kernel (1.11). In all three SV techniques a

dictionary of four kernel widths was used: n = For each method we chose the values of the

2fnre,enp=ara1m; :e:t:e;r4(.s)

which

gave

the

lowest value of ISE (integrated squared error estimated using Simpson's method)

given knowledge of the true density. In practice, of course, the true density is

unknown and the best parameter(s) cannot be selected; we only use this information

to nd how close the best case prediction of an estimator can possibly get to the

true density.

Generic author design sample pages 1999/07/12 15:50

1.13 Experiments

11

EL-SVM SL-SVM PE-SVM Parzen pts SVs ISE SVs ISE SVs ISE SVs ISE 50 5 0.045 6 0.031 10 0.050 50 0.056 75 7 0.095 6 0.036 13 0.086 75 0.087 100 7 0.105 7 0.064 10 0.079 100 0.091 200 5 0.072 7 0.056 20 0.053 200 0.053
Table 1.1 A comparison of the SV density estimator with "-insensitive loss (ELSVM), square loss (SL-SVM), a SV approximation of the Parzen's estimator (PESVM) and the Parzen's windows estimator (Parzen). Each estimator was picked with the best possible value of parameters given knowledge of the true density.

The results shown in Table 1.1 indicate that all three SV techniques are competitive with the Parzen's estimator, whilst possessing less complex (sparse) decision functions. EL-SVM performed worst but has the advantage of less parameter selection (only C was chosen, and typically C = 1). PE-SVM approximations of Parzen's estimates obtained signi cantly reduced numbers of support vectors (as Parzen's windows is an expansion on all the training examples) whilst slightly reducing ISE. This reduction is probably due to the decision function being marginally smoother. Trade o between accuracy and complexity when controlling " in this method can be seen in Figure 1.1.
SL-SVM performed best of all, providing functions very close to the true density with small numbers of support vectors. It is worth pointing out this is not because the loss function is square loss instead of absolute loss; setting " = 0 and controlling the free parameter C in EL-SVM gives as good results (or better) than SL-SVM (results not reported here). However, SL-SVM is better at dealing with a large number of data points (one can employ more e cient decomposition techniques.)

0.14 0.12
0.1 0.08 0.06

SV approximation Parzen estimate

30 SV approximation
25
20
15
10
5

ISE SVs

0.04 0

0.1 0.2 0.3 0.4 0.5 epsilon

0 0 0.1 0.2 0.3 0.4 0.5
epsilon

Figure 1.1 Approximating Parzen estimates with the SV method (PE-SVM) gives the same or lower ISE using only a small number of support vectors. Here " is plotted against ISE (left) and the number of support vectors (right).

Generic author design sample pages 1999/07/12 15:50

12 Support Vector Density Estimation
1.14 Conclusions and further research
We have described a new SV technique for estimating multi-dimensional densities. Although this results in di erent optimization problems to normal SVMs, the algorithms have the common feature of possessing sparse decision functions.
We have shown how to solve the density estimation problem as a linear operator equation with a special type of problem-speci c regularization using the "-insensitive loss function.
The integrals of powerful density estimation kernels are not typically symmetrical kernel functions. We show two methods of using non-Mercer kernels with SVM: one with an "-insensitive loss function and one with square loss. We show how both of these methods can use a dictionary of kernel functions to choose the density estimate from a wide class of functions (for example a mixture of Gaussian-like shapes of di erent widths).
The results suggest that these methods could obtain good results in real applications. Multi-dimensional problems remain untested; however results in other domains suggest the SV techniques will work well in the multi-dimensional case (the SV kernel regression method is well suited to multi-dimensional problems). Particularly, the square loss method of SV regression is expected to work well as decomposition techniques can be used for dealing with large numbers of training points while using non-Mercer kernels.
Further research lies in the following areas: assessing the performance of the square loss SVM, using dictionaries of kernels in ordinary regression problems and obtaining results in real density estimation applications, in particular in the multidimensional case.
Generic author design sample pages 1999/07/12 15:50

1.14 Conclusions and further research

13

true density 3.5

3

2.5

p(x) p(x) p(x) p(x) p(x) p(x)
p(x) p(x) p(x) p(x) p(x) p(x) p(x)

2

1.5

1

0.5

training set 3.5 estimated density
3 2.5
2 1.5
1 0.5
0 0.2 0.4 0.6 0.8 1 x

0 0 0.2 0.4 0.6 0.8 1
x

3.5 training set
estimated density 3
2.5
2
1.5
1

training set estimated density 3.5
3
2.5
2
1.5
1

0.5 0.5

0 0.2 0.4 0.6 0.8 1 x

0 0 0.2 0.4 0.6 0.8 1
x

training set 4 estimated density 3.5
3
2.5
2
1.5
1
0.5
0 0 0.2 0.4 0.6 0.8 1 x

training set 3.5 estimated density
3 2.5
2 1.5
1 0.5
0 0.2 0.4 0.6 0.8 1 x

training set estimated density 3.5 3 2.5 2 1.5 1 0.5
0 0.2 0.4 0.6 0.8 1 x

3.5 training set estimated density
3
2.5
2
1.5
1
0.5
0 0 0.2 0.4 0.6 0.8 1 x

4

training set estimated density

3.5

3

2.5

2

1.5

1

0.5

0 0 0.2 0.4 0.6 0.8 1
x

3.5 training set estimated density
3
2.5
2
1.5
1
0.5

training set 3 estimated density 2.5 2 1.5 1 0.5

training set estimated density 3 2.5 2 1.5 1 0.5

4 training set estimated density
3.5
3
2.5
2
1.5
1
0.5

0 0 0.2 0.4 0.6 0.8 1
x

0 0 0.2 0.4 0.6 0.8 1
x

0 0 0.2 0.4 0.6 0.8 1
x

0 0 0.2 0.4 0.6 0.8 1
x

Figure 1.2 The true density (top row) and density estimates by the methods "insensitive loss SVM (EL-SVM) (row two), square loss SVM (SL-SVM) (row three), and Parzen's windows estimate (bottom row) generated from (from left to right) 50 , 75 , 100 and 200 points.

Generic author design sample pages 1999/07/12 15:50

14 Support Vector Density Estimation
Generic author design sample pages 1999/07/12 15:50

References
E. Osuna, R. Freund, and F. Girosi. Support vector machines: Training and applications. A.I. Memo (in press), MIT A. I. Lab., 1996.
E. Osuna, R. Freund, and F. Girosi. An improved training algorithm for support vector machines. In J. Principe, L. Gile, N. Morgan, and E. Wilson, editors, Neural Networks for Signal Processing VII | Proceedings of the 1997 IEEE Workshop, pages 276 { 285, New York, 1997. IEEE.
V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, New York, 1995.
V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. forthcoming. V. Vapnik, S. Golowich, and A. Smola. Support vector method for function
approximation, regression estimation, and signal processing. In M. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9, pages 281{287, Cambridge, MA, 1997. MIT Press.
Generic author design sample pages 1999/07/12 15:50

