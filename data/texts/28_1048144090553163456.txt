FMDistance: A fast and effective distance function for motion capture data
Kensuke Onuma Christos Faloutsos Jessica K. Hodgins
April 2008 CMU-CS-07-164
School of Computer Science Carnegie Mellon University
Pittsburgh, PA 15213
Sony Corporation, Tokyo, Japan School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
Abstract Given several motion capture sequences, of similar (but not identical) length, what is a good distance function? We want to find similar sequences, to spot outliers, to create clusters, and to visualize the (large) set of motion capture sequences at our disposal. We propose a set of new features for motion capture sequences. We experiment with numerous variations (112 feature-sets in total, using variations of weights, logarithms, dimensionality reduction), and we show that the appropriate combination leads to near-perfect classification on a database of 226 actions with twelve different categories, and it enables visualization of the whole database as well as outlier detection.

Keywords: data mining, motion capture, distance function, classification, human animation

1 Introduction
Motion capture data is often used to create human animations for video games, movies and other applications. Large databases of motion now exist both on the web (see http://mocap.cs.cmu.edu/, for example) and as proprietary resources within entertainment companies. These databases are not easily searchable. In this paper, we present a distance function that represents the characteristics of the actions in a motion capture sequence. This distance metric is suitable for classifying motions, searching for similar motions, and for detecting outlier motions.
We would like a distance function that is both fast and effective. The distance function should be fast to compute, even on long motion capture sequences. Ideally, it should be independent (O(1)) on the length Nframe of the sequences. The distance function should be also meaningful, so that it is useful for clustering, classification, and anomaly detection (see Figure 1).
Figure 1: Visualization of classification by three new features. Notice the homophily: same motions cluster together; also notice that we can visually spot outliers, like ballet dancing which has noisy capture frames, in the red circle.
Specifically, we propose F M Distance, a method which is independent on the sequence length, and only depends on K, the number of joint angles we track. Our idea is to calculate the approximation of the total kinetic energy of each joint as a preprocessing step, thus compressing each motion capture sequence of K × Nframe numbers into K numbers (and possibly, even fewer, if we do dimensionality reduction). The proposed method is also effective, as we illustrate by results of the accuracy of motion classification and visualization of the relation among motion capture data in Section 4.
1

The rest of the paper is organized as follows: we first review the related work in Section 2; the proposed strategies are presented in Section 3; the experimental results are presented in Section 4; and we conclude the paper in Section 5.

2 Related work
To our knowledge, there is little work in computer graphics that focuses on a distance function for expressing the characteristics of a motion capture sequence. The work of Ren et al.[7], who explored methods for verifying the naturalness of a motion capture sequence, is close to our problem. Troje [9] investigated gender classification of walking motions by analyzing motion capture data. Researchers in computer vision also proposed distance functions for human activity classification [1].
Many distance functions have been proposed for finding candidates of "frame to frame" transition [3, 5, 6, 8, 10] or indexing for segmentation [4]. However, the distance function in [3, 5, 6, 8, 10] requires K × Nframe computations; Our proposed method does not depend on Nframe and is much simpler and faster (O(K)).

3 Proposed method
In this section, we describe details of F M Distance for the classification of motion capture data. First we describe the way to make the data independent of time in preprocessing step, and then we introduce how to calculate the distance between two motion capture sequences.

3.1 Preprocessing step : Transformation from motion capture data to kinetic energy-based parameters

We assume that motion capture data have K DOF in total and a series of motion capture data for
one action has Nframe frames. Specifically, every frame has joint angles, the root orientation and the root positions (coordinates of the root). A set of motion capture data is denoted by {xi|i = 1, ..., Nframe}. Each frame xi = [xi,1, xi,2, ..., xi,K]T represents a point in K dimensional space.
The main idea is to use the average of the approximate kinetic energy of each joint angle (or carefully selected groups of joints), as features. This way, the K × Nframe numbers of a motion capture sequence are condensed into at most K values as a preprocessing step, achieving our first
goal, speed.
We compute the approximate kinetic energy as the sum of squares of velocities. Specifically, the velocity of the root and the angular velocity of each joint, vi = [vi,1, vi,2, ..., vi,K-1]T are calculated by the first derivatives:

vi

=

xi+1 - xi tf rame

(1)

2

where tframe is the period between frames. For the root velocity, we calculate the velocity and the energy across the plane from the x + z position. Thus, the dimensionality of vi decreases by 1, to K - 1. We note that the vertical velocity of the root is included as one of parameters.
To compute the kinetic energy, we also need to consider the moment of inertia mj of each joint, (and body mass, for each position-coordinate). The kinetic energy Ei,j of joint j at time i is

Ei,j = mj × vi2,j

(2)

Although the moment of inertia varies depending on the body part, we assume that they are constant with respect to time.

3.2 Distance function for classification
We calculate the mean of the kinetic energy at each dimension (joint angle/position/orientation), j:

1 Nframe

Ej

=

Nf rame

Ei,j
i=1

(3)

The kinetic energy is bursty: some joints have a high kinetic energy while others do not. We propose to treat the burstiness, by taking logarithms, specifically log(x + 1) (to handle the joints of zero energy). Thus:

ej = log Ej + 1

(4)

The vector e = [e1, e2, ..., eK-1]T is our proposed feature vector. Then, the distance between two motion capture sequences N and M is the Euclidean distance of their feature vectors eN and eM .

4 Experimental results
In this section, we evaluate the effectiveness of our approach. The motion capture data we use for the experiments is http://mocap.cs.cmu.edu/. Figure 2 shows the human figure, the number of DOF, and the value of mj for each joint. The value of mj heuristically models the moment of inertia: hip joints get high values, shoulders get a bit smaller, knees are next, elbows are next, etc.
We used 226 sequences of motion capture data and they are categorized into the twelve actions described in Figure 6. We also carefully examined 112 feature sets, from the cross products of [{log, lin} × {original data, transformed data from quaternion} × {weighted mj, constant mj} × {normalized, unnormalized} × {seven feature sets}].
Implementation details: The distance function for each feature set was the Euclidean distance. Before calculating velocities, we followed standard practice and removed noise with a linear lowpass filter spanning five frames. Finally, when we normalized the data in order that they have zero mean and unit standard deviation.

3

Figure 2: Human figure model, DOFs and the distribution of mj. It has 29 joints, 56 joint angles, three angles for the root orientation and three position coordinates for the root.
Effectiveness measure: To measure the effectiveness of our feature sets, we use the classification accuracy, and specifically, a 1-nearest neighbor (1-NN) classifier. We chose this classifier because its accuracy is directly related to the effectiveness of the feature set, and it needs no training. Moreover, as we show next, it gives excellent classification accuracy.
4.1 Accuracy of various feature sets
We use the feature sets described in Section 3, as well as some other, simpler feature sets, for comparison. The nomenclature for a feature set is as follows: For example, 61-LOG-cons-quanorm stands for 61 features, with the log transform, constant values for the moments/weights mj, converted by quaternion, and normalized. Similarly, 61-LIN-est-nqua-unnorm stands for the same 61 features, without the log transform, with the estimated values of the mj weights as shown in Figure 2, not converted by quaternion, and unnormalized. Figure 3 gives the list of feature sets we tried, and their descriptions.
Tables 1-4 show results of our experiments. They show that 61-LOG-cons-nqua-norm is the best feature set for classifying motion capture data (the accuracy is same as 62-LOG-cons-nquanorm, but we take the feature set with fewer parameters).
4

3-LOG The features shown in Equation (6).

3-LIN The average of the total approximate kinetic energy, the ratio of the approximate kinetic

energy

between

upper

body

vs

lower

body,

limbs

vs

trunk

(E total ,

E upper E lower

,

E limbs E trunk

).

4-LOG The features shown in Equation (6) plus the logarithm of the ratio of the approximate

kinetic energy between right body and left body (rr/l = log

E right +1 E lef t +1

).

4-LIN The average of the total approximate kinetic energy, the ratio of the approximate kinetic energy between upper body vs lower body, limbs vs trunk, and right body vs left body (E , , , ).Eupper Elimbs Eright
total Elower Etrunk Elef t

29-LOG The logarithms of the mean of the approximate kinetic energy for each joint.

29-LIN The mean of the approximate kinetic energy for each joint.

31-LOG The logarithms of the root's energy in the horizontal and vertical direction, as well as of the approximate kinetic energy of each joint.

31-LIN The root's energy in the horizontal and vertical direction, as well as the approximate kinetic energy of each joint.

59-LOG Log of the mean of the approximate kinetic energy for each joint angle (not including kinetic energy of root).

59-LIN Mean of the approximate kinetic energy for each joint angle (not including kinetic energy of root).

61-LOG Log of the mean of the approximate kinetic energy for each joint angle plus vertical and horizontal kinetic energy of root. See Equation (4).

61-LIN Mean of the approximate kinetic energy for each joint angle plus vertical and horizontal kinetic energy of root. See Equation (3).

62-LOG Log of the mean of the approximate kinetic energy for each joint angle plus kinetic energy of root for x, y, z axes.

62-LIN Mean of the approximate kinetic energy for each joint angle plus kinetic energy of root for x, y, z axes.

62-POS The mean of xi with respect to time.

Figure 3: Feature sets: names and descriptions

5

features
3-LOG-cons-nqua-norm 3-LOG-est-nqua-norm 3-LIN-cons-nqua-norm 3-LIN-est-nqua-norm 3-LOG-cons-qua-norm 3-LOG-est-qua-norm 3-LIN-cons-qua-norm 3-LIN-est-qua-norm 3-LOG-cons-nqua-unnorm 3-LOG-est-nqua-unnorm 3-LIN-cons-nqua-unnorm 3-LIN-est-nqua-unnorm 3-LOG-cons-qua-unnorm 3-LOG-est-qua-unnorm 3-LIN-cons-qua-unnorm 3-LIN-est-qua-unnorm 4-LOG-cons-nqua-norm 4-LOG-est-nqua-norm 4-LIN-cons-nqua-norm 4-LIN-est-nqua-norm 4-LOG-cons-qua-norm 4-LOG-est-qua-norm 4-LIN-cons-qua-norm 4-LIN-est-qua-norm 4-LOG-cons-nqua-unnorm 4-LOG-est-nqua-unnorm 4-LIN-cons-nqua-unnorm 4-LIN-est-nqua-unnorm 4-LOG-cons-qua-unnorm 4-LOG-est-qua-unnorm 4-LIN-cons-qua-unnorm 4-LIN-est-qua-unnorm

mj
constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated

quaternion
N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y

normalized
Y Y Y Y Y Y Y Y N N N N N N N N Y Y Y Y Y Y Y Y N N N N N N N N

% error
16.37% 11.50% 22.57% 16.81% 19.47% 15.93% 21.68% 22.12% 16.37% 11.95% 29.20% 37.17% 21.68% 16.37% 34.96% 29.20% 15.49% 10.62% 20.80% 17.70% 15.04% 14.60% 23.45% 20.80% 13.27% 9.73% 29.20% 37.17% 15.49% 12.83% 34.51% 29.65%

Table 1: Feature sets and their classification error - part A

6

features
29-LOG-cons-nqua-norm 29-LOG-est-nqua-norm 29-LIN-cons-nqua-norm 29-LIN-est-nqua-norm 29-LOG-cons-qua-norm 29-LOG-est-qua-norm 29-LIN-cons-qua-norm 29-LIN-est-qua-norm 29-LOG-cons-nqua-unnorm 29-LOG-est-nqua-unnorm 29-LIN-cons-nqua-unnorm 29-LIN-est-nqua-unnorm 29-LOG-cons-qua-unnorm 29-LOG-est-qua-unnorm 29-LIN-cons-qua-unnorm 29-LIN-est-qua-unnorm 31-LOG-cons-nqua-norm 31-LOG-est-nqua-norm 31-LIN-cons-nqua-norm 31-LIN-est-nqua-norm 31-LOG-cons-qua-norm 31-LOG-est-qua-norm 31-LIN-cons-qua-norm 31-LIN-est-qua-norm 31-LOG-cons-nqua-unnorm 31-LOG-est-nqua-unnorm 31-LIN-cons-nqua-unnorm 31-LIN-est-nqua-unnorm 31-LOG-cons-qua-unnorm 31-LOG-est-qua-unnorm 31-LIN-cons-qua-unnorm 31-LIN-est-qua-unnorm

mj
constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated

quaternion
N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y

normalized
Y Y Y Y Y Y Y Y N N N N N N N N Y Y Y Y Y Y Y Y N N N N N N N N

% error
4.87% 4.42% 8.85% 5.31% 4.42% 5.31% 7.52% 4.42% 3.98% 3.98% 8.85% 15.04% 4.87% 5.31% 9.29% 10.62% 3.10% 3.10% 6.19% 4.42% 3.10% 3.98% 6.64% 3.98% 4.87% 3.54% 11.95% 15.04% 3.98% 3.10% 12.39% 11.95%

Table 2: Feature sets and their classification error - part B

7

features
59-LOG-cons-nqua-norm 59-LOG-est-nqua-norm 59-LIN-cons-nqua-norm 59-LIN-est-nqua-norm 59-LOG-cons-qua-norm 59-LOG-est-qua-norm 59-LIN-cons-qua-norm 59-LIN-est-qua-norm 59-LOG-cons-nqua-unnorm 59-LOG-est-nqua-unnorm 59-LIN-cons-nqua-unnorm 59-LIN-est-nqua-unnorm 59-LOG-cons-qua-unnorm 59-LOG-est-qua-unnorm 59-LIN-cons-qua-unnorm 59-LIN-est-qua-unnorm 61-LOG-cons-nqua-norm 61-LOG-est-nqua-norm 61-LIN-cons-nqua-norm 61-LIN-est-nqua-norm 61-LOG-cons-qua-norm 61-LOG-est-qua-norm 61-LIN-cons-qua-norm 61-LIN-est-qua-norm 61-LOG-cons-nqua-unnorm 61-LOG-est-nqua-unnorm 61-LIN-cons-nqua-unnorm 61-LIN-est-nqua-unnorm 61-LOG-cons-qua-unnorm 61-LOG-est-qua-unnorm 61-LIN-cons-qua-unnorm 61-LIN-est-qua-unnorm

mj
constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated

quaternion
N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y N N N N Y Y Y Y

normalized
Y Y Y Y Y Y Y Y N N N N N N N N Y Y Y Y Y Y Y Y N N N N N N N N

% error
3.54% 3.10% 5.75% 4.87% 4.42% 2.65% 5.75% 4.42% 3.98% 3.54% 7.96% 13.27% 4.42% 3.54% 10.62% 4.87% 2.21% 2.65% 3.98% 4.42% 3.54% 2.65% 4.87% 3.98% 3.10% 2.65% 13.27% 16.37% 2.65% 2.65% 13.72% 12.39%

Table 3: Feature sets and their classification error - part C. The feature set with the lowest classification error, 61-LOG-cons-nqua-norm, is in bold.

8

features
62-LOG-cons-nqua-norm 62-LOG-est-nqua-norm 62-LIN-cons-nqua-norm 62-LIN-est-nqua-norm 62-LOG-cons-qua-norm 62-LOG-est-qua-norm 62-LIN-cons-qua-norm 62-LIN-est-qua-norm 62-LOG-cons-nqua-unnorm 62-LOG-est-nqua-unnorm 62-LIN-cons-nqua-unnorm 62-LIN-est-nqua-unnorm 62-LOG-cons-qua-unnorm 62-LOG-est-qua-unnorm 62-LIN-cons-qua-unnorm 62-LIN-est-qua-unnorm
62-POS-norm 62-POS-unnorm

mj
constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated constant estimated
N/A N/A

quaternion
N N N N Y Y Y Y N N N N Y Y Y Y N/A N/A

normalized
Y Y Y Y Y Y Y Y N N N N N N N N Y N

% error
2.21% 2.65% 4.87% 4.42% 3.54% 2.65% 4.87% 3.54% 3.10% 3.10% 12.83% 15.49% 3.10% 3.54% 14.60% 12.83% 5.75% 20.80%

Table 4: Feature sets and their classification error - part D (last).

We report the conclusions and observations:
· Figure 4 illustrates how the features improve. The classification error reduces steeply at the lower rank features and it still declines gradually at the higher rank features. It implies that there might still have a space to increase classification accuracy.
· Taking the logarithms (log(x + 1)) almost always improves performance. Figure 5 provides the result that all features, except one feature (29-***-est-qua-norm), which take the logarithm have lower classification error than those which do not take the logarithm.
· There is no clear winner with respect to the sets of mj weights. Both "cons" and "est" perform about the same.
Figure 6 shows the confusion matrix for our best performer, the 61-LOG-cons-nqua-norm feature set. Columns correspond to the predicted labels, and rows to the actual label. In a perfect classifier, the matrix would be diagonal. Notice that 61-LOG-cons-nqua-norm gives a near-diagonal matrix. The sequences it confused were all "Walking" sequences, ("Walking", "Walking slowly", and "Walking on uneven terrain").

9

error [%]

rank vs. error 40 35 30 25 20 15 10
5 0
0 20 40 60 80 100 120 rank
Figure 4: The distribution of the classification error among feature set. X-axis represents a rank of the features sorted by percentage of the classification error and y-axis means percentage of the classification error.
log vs. lin 40
log lin 35
30
25
20
15
10
5
0 0 10 20 30 40 50 60 feature
Figure 5: Comparison of the classification error by taking the logarithm and that by not taking the logarithm.
10

error [%]

Figure 6: Confusion Matrix of condition 61-LOG-cons-nqua-norm in Table 4. Notice that (a) it is near-diagonal and (b) the confused motions are very similar (A01-A03), all being variations of walking motions.
We also examine the effect of dimensionality reduction by PCA. We did PCA on the 61-LOGcons-nqua-norm feature set (Figure 7), and we observed that the classification accuracy was preserved, as long as we retain the first 18 components (or more).
4.2 Visualization
The features described in the Section 3.2 have dimensionality higher than three. For visualization, we have to reduce the dimensionality to three. Although it is common to use principal component analysis (PCA) [2] in order to reduce dimensionality for classification, we propose a more intuitive method.
The intuition is that different motions will exercise different body parts: for example, "walking" will have a balance between upper body and lower body, while "golf swing" will have more energy on the upper body. We propose to capture these differences with new features, namely, the ratio r of the approximate kinetic energy of groups of body parts.
First we sum up the average of the approximate kinetic energy of the joint rotation in order to
11

The effect of dimensionality reduction by PCA

error

0.6
0.5
0.4
0.3
0.2
0.1
0 0 10 20 30 40 50 60 components
Figure 7: The error rate versus number of retained Principal Components. Notice the flattening, at about 10 components.

estimate the kinetic energy of the whole body, upper body, lower body, limbs and trunk.

Eparts =

Ej

jparts

(5)

Then we take the ratio of the kinetic energy between symmetrical body parts. As in Equation (4), we also use the log(x + 1) transform:

eall = log Etotal + 1

ru/l = log rl/t = log

Eupper + 1 Elower + 1 Elimbs + 1 Etrunk + 1

(6)

We propose to use the 3-d vector  = [eall, ru/l, rl/t]T as a feature vector of a motion capture sequence, and, again, we use the corresponding Euclidean distance between two such feature vectors Ni, Mi.
Here we show that, even with 3-d feature sets, we can still have a useful visualization. We use the features which we showed in the previous subsection (3-LOG, 3-LIN), as well as the first three principal components of PCA from the 61-LOG-cons-nqua-norm feature set (3-PCA).
Tables 1-4 show that 3-LOG-est-nqua-norm led to 11.50% classification error, outperforming the other 3-LOG, 3-LIN, and 3-PCA features (the latter with 15.49% as shown in Figure 7). This

12

result was another pleasant surprise: the human intuition behind 3-LOG-est-nqua-norm won over PCA, which is mathematically optimal under the L2 norm.
Thus, we use the 3-LOG-est-nqua-norm feature set for visualization. Figure 1 shows the scatter-plot of motion capture sequences in this 3-d space. The scatter-plot leads to observations that agree with our intuition, underlining the effectiveness of our chosen feature sets:
· The trunk energy during walking on uneven terrain is higher than during a normal walk. · Frame-climbing requires roughly the same energy of all body parts. · Running and walking have similar proportions of energy (upper body vs. lower body and
limbs vs. trunk). Moreover, the scatter-plot can help us spot outlier motions. For example, the points inside the red circle of Figure 1, correspond to actions with high energy; closer inspection shows that they are noisier, and the first derivatives skyrocket. The offenders correspond to ballet dancing motions, labeled as #5-6 and #5-8 in http://mocap.cs.cmu.edu/. Figure 8 shows the energy versus time for motion #5-6 (each line corresponds to a DOF). Notice that there are some large changes around frame number 150.
Figure 8: 'Outlier' motion capture sequences detected by Figure 1
13

5 Conclusions
The goal of this paper is to find an effective and fast-to-compute distance function between two unequal-length motion capture sequences. Our main contribution is that we proposed a lowdimensionality feature-set for each motion capture sequence. After extensive experiments of 112 possible variations on a large real motion capture dataset, we propose two methods, 61-LOG-cons (most accurate) and 3-MAN (best for visualization). In all variations, the idea is to consider the total approximate kinetic energy expended by each of the approximately 70 angles in the data. The resulting feature sets achieve the original design goals:
· Speed: Our proposed distance function is fast to compute, independent of the duration of the motion capture sequences.
· Effectiveness: 61-LOG-cons gives good classification accuracy, and provides an excellent starting point for dimensionality reduction (with PCA, or 3-MAN), for visualization, clustering, and outlier detection (see Figure 1).
A promising direction for future work is to extend this approach to subsequence search. The distance functions we proposed in this paper are applicable only to whole sequences, but if we could also have a distance function for subsequences of motion capture data, we could apply this method to various tasks, such as finding stitching point between two motion sequences, detecting the exact frame which is noisy, and so on.
References
[1] Raffay Hamid, Amos Johnson, Samir Batta, Aaron Bobick, Charles Isbell, and Graham Coleman. Detection and explanation of anomalous activities: Representing activities as bags of event n-grams. In Proc. of CVPR, volume 1, pages 1031­1038, 2005.
[2] I. T. Jolliffe. Principal Component Analysis. Springer, 1986.
[3] Eamonn Keogh. Exact indexing of dynamic time warping. In Proc. of VLDB, pages 406­417, 2002.
[4] Eamonn Keogh, Themistoklis Palpanas, Victor B. Zordan, Dimitrios Gunopulos, and Marc Cardle. Indexing large human-motion databases. In Proc. of VLDB, pages 780­791, 2004.
[5] Lucas Kovar, Michael Gleicher, and Fre´de´ric Pighin. Motion graphs. ACM Trans. Graph., 21(3):473­482, 2002.
[6] Jehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, and Nancy S. Pollard. Interactive control of avatars animated with human motion data. In Proc. of SIGGRAPH, pages 491­500, 2002.
14

[7] Liu Ren, Alton Patrick, Alexei A. Efros, Jessica K. Hodgins, and James M. Rehg. A datadriven approach to quantifying natural human motion. ACM Trans. Graph., 24(3):1090­1097, August 2005.
[8] Arno Scho¨dl, Richard Szeliski, David Salesin, and Irfan A. Essa. Video textures. In Proc. of SIGGRAPH, pages 489­498, 2000.
[9] Nikolaus F. Troje. Decomposing biological motion: A framework for analysis and synthesis of human gait patterns. Journal of Vision, 2(5):371­387, 2002.
[10] Victor B. Zordan, Anna Majkowska, Bill Chiu, and Matthew Fast. Dynamic response for motion capture animation. ACM Trans. Graph., 24(3):697­701, 2005.
15

A Conversion of Euler angles to quaternion
Because the original motion capture data are expressed by Euler angles, it contains the problem of "gimbal lock". To avoid this problem, we calculate a set of anglar velocities by converting Euler angles to quaternions. If the original motion capture data are described as XYZ system of Euler angles, the corresponding quaternion can be obtained as:

 cos(/2) cos(/2) cos(/2) + sin(/2) sin(/2) sin(/2) 

q

=

 



sin(/2) cos(/2) cos(/2) cos(/2) sin(/2) cos(/2)

- +

cos(/2) sin(/2) sin(/2) 

sin(/2) cos(/2) sin(/2)

 

cos(/2) cos(/2) sin(/2) - sin(/2) sin(/2) cos(/2)

where q indicates a quaternion. And the relation between the quaternion and angular velocity is defined as:

dq

=

1 (t)



q

dt 2

(7)

where (t) = [0 x(t) y(t) z(t)]T indicates an angular velocity vector represented as a quaternion and  denotes quaternion multiplication. The quaternion multiplication is defined as:

 -q1 -q2 -q3 q0   q^0 

q^



q

=

  

q0 -q3

q3 q0

-q2 q1   q^1 

q1

q2

 

q^2

 

q2 -q1 q0 q3

q^3

where q = [q0 q1 q2 q3]T and q^ = [q^0 q^1 q^2 q^3]T . We can obtain the estimate of angular velocity((t)) by solving the formula of (7), where dq/dt
and q are calculated from the original motion capture data by converting to quaternions. Notice that the formula of (7) is overdetermined because it has four equations and the number of unknown variables are three (x(t), y(t), z(t)). So we calculate (t) using least squares.

16

