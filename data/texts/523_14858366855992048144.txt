From: AAAI-82 Proceedings. Copyright ©1982, AAAI (www.aaai.org). All rights reserved.
REVEREND BAYES ON INFERENCE ENGINES: A DISTRIBUTED HIERARCHICAL APPROACH(*)(**)

Judea Pearl Cognitive Systems Laboratory School of Engineering and Applied Science University of California, Los Angeles
90024

ABSTRACT
This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.
Introduction
This paper addresses the issue ofefficiently propagating the impact of new evidence and beliefs through a complex network of hierarchically organized inference rules. Such networks find wide applications in expert-systems Cl], [2],[3],speech recognition [4], situation assessment [5], the modelling of reading comprehension [6] and judicial reasoning [7].
Many AI researchers have accepted the myth that a respectable computational model of inexact reasoning must distort, modify or ignore at least some principles of probability calculus. Consequently, most AI systems currently employ ad-hoc belief propagation rules which may hinder both the inferential power of these systems and their acceptance by their intended users. The primary purpose of this paper is to examine what computational procedures are dictated by traditional probabilistic doctrines and whether modern requirements of local asynchronous processing render these doctrines obsolete.
We shall assume that beliefs are expressed in probabilistic terms and that the propagation of beliefs is governed by the traditional Bayes transformations on the relation P(DIH), which stands for the judgmental probability of data D (e.g., a tombination of symptoms) given the hypothesis H (e.g., the existence of a certain disease). The unique
(*)The paper "An Essay Towards Solving a Problemin the Doctrine of Chances by the late Rev. Mr. Bayes", Phil. Trans. of Royal Sot., 1763,marks the begining of the science of inductive reasoning.
(**) Supported in part by the National Science Foundation, Grant IST 80 19045.

feature of hierarchical inference systems is that the relation P(DIH) is computable as a cascade of local, more elementary probability relations involving intervening variables. Intervening variables, (e.g., organisms causing a disease) may or may not be directly observable. Their computational role, however, is to provide a conceptual summarization for loosely coupled subsets of obser vational data so that the computation of P(HID)can be performed by local processes, each employing a relatively small number of data sources.
The belief maintenance architecture proposed in this paper is based on a distributed asynchronous interaction between cooperating knowledge sources without central supervision similar to that used in the HEARSAY system [4]. We assume that each variable (i.e., a set of hypotheses) is repro sented by a separate processor which bothmaintains the parameters of belief for the host variable and manages the communication links to and from theset of neighboring, logically related variables, The communication lines are assumed to be open at all times, i.e., each processor may at any time interrogateits message-board forrevisionsmade by its neighbors,update its own belief parameters and post newmessages on its neighbors' boards. In this fashion the impact of new evidence may propagate up and down the network until equilibrium is reached.
The asynchronous nature of this model requires a solution to an instability problem. If a stronger belief in a given hypothesis means a greater expectation for the occurrenceof a certain supporting evidence and if, in turn, a greater certainty in the occurrenceof that evidence adds further credence to the hypothesis, how can one avoid an infinite updating loop when the two processors begin to communicate with one another? Thus, a second objective of this paper is to present an appropriate set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.
A third objective is to demonstrate that proper Bayes inference can be accomplished amongmultivalued variables and that, contrary to the claims made by Pednault, Zucker and Muresan [8], this does not render conditional independence incompatible with the assumption of mutual exclusivity and exhaustivity.

133

Definitions and Nomenclature

A node in an inference net represents a variable name. Each variable represents a finite partition of the world given by the variable values or states. It may be a name for a collection of hypotheses (e.g., identity of organism: ORG1, ORG2, ... ..) or for a collection of possible observations (e.g., patient's temperature: high, medium, low). Let a variable be labeled by a capital letter,e.g., A,B,C ,-*., and its various states subscripted, e.g., A1,A2,... .

An inference net is a directed acyclical graph where each branch @ - @ represents a family of rules of the form: if Ai then Bi. The uncertainties in these rules are quantified by a conditional probability matrix, I(BIA),with entries: M(BlA)ij= P(BjlAi). The presence of a branch between A and B signifies the existence of a direct communication line between the two variables. The directionality of the arrow designates A as theset of hypotheses and B as the set of indicators or manifestations for these hypotheses. We shall say that B is a son of A and confine our attention to trees, where every node has onlyonemulti-hypotheses father and where the leaf nodes represent observable variables.

In principle, the model can also be generalized to include some graphs (multiple parents), keeping in mind that the states of each variable in the tree may represent the power set of multi-parent groups in the corresponding graph,

Structural Assumptions

Consider the following segment of the tree:

The likelihood of the

various states of B

would, in general, depend on the entire

D

data observed so far, i.e., data from the tree

k/ A

rooted at B, the tree rooted at C and the tree

C

B

above A. However, the fact that B can communi-

'I F ' 4iY42 , ,

E

cate directly only with its father (A) and its

*t

; '4

sons (F and E) means that the influence of the en-

tire network above B on B is completely summarized

by the likelihood it induces on the states of A.

More formally, let Dd(B) stand for thedataobtained

fromthetree rooted atB,and D"(B) for the data ob-

tained fromthe network above B. The presenceofonly

onelinkconnectingDU(B)and

(B)implies:

P(BjlAi,DU(B)) = P(BjlAi)

(1)

This structural assumption of local communication immediately dictates what is normally called "Conditional Independence"; if C and B are siblings and A is their parent, then

P(Bj,CkIAi) = P(BjIAi) * P(CkIAi)

(2)

because the data C=Ckis part of D"(B) and hence (7) implies P(Bj/Ck,Ai) = P(BjlAi), from which (2)follows.

Note the difference between the weak form of conditional independence in (2) and the overrestrictive form adapted by Pednault et al. [8], who also asserted independence with respect to the complements Ai.

Combining Top and Bottom Evidences

Our structural assumption (1) also dictates how evidences above and below somevariable B should be combined. Assume we wish to find the likelihood of the states of B induced by some data D, part of which, D"(B), comes from above B and part, Dd(B), from below. Bayes theorem, together with (l),yields the product rule:

P(BilDU(B),Dd(B))=UPCDd(B)IBiI*PIBiIDu(B)I,

(3)

where a is a normalization constant. This is a generalization of the celebrated Bayes formula for binary variables:

O(HI E) = X(E) O(H)

(4)

where A(E)=P(E[H)/P(EIn) is known as the likelihood ratio, and O(H)=P(H)/P(fl) as the prior odds [2].

Equation (3) generalizes (4) in two ways.

First, it permits the treatment of non-binary vari-

ables where the mental task of estimating P(EIR) is

often unnatural, and where conditional independence

with respect to the negations of the hypotheses is

normally violated (i.e., P(El,E21R)fP(El/R)P(E2l~)).

Second, it identifies a surrogate to the prior

probability term for any intermediate node in the

tree, even after obtaining some evidential data.

According to,

the multiplicative role of the

prior probability in Equation (4) is taken over by

the conditional probability of a variable based

only on the evidence gathered by the network above

it, excluding the data collected from below. Thus,

the product rule (3) can be applied to any node in

the network, without requiring prior probability

assessments.

The root is the only node which requires a prior probability estimation. Since it has no network above, D"(B) should be interpreted as the available background knowledge which remains unexplicated by the network below. This interpretation renders P(BilD'(B)) identical to the classical notion of subjective prior probability. The probabilities of all other nodes in the tree are uniquely determined by the arc-matrices, thedataobserved and the prior probability of the root.

Equation (3) suggests that the probability distribution of every variable in the networkcan be computed if the node corresponding to that variable contains the parameters

x(Bi) a, P(Dd(B)IBi) and
q(Bi) 4 P(BilD'(B)).

(5)
(6)

q(Bi) represents the anticipatory support attributed to Bi by its ancestors and X(Bi) represents the evidential support received byBi from its diagnostic descendants. The total strength of belief in Bi

134

would be given by the product

P(Bi) = aX(Bi) q(Bi).

(7)

Whereas only two parameters, x(E) and O(H),were sufficient for binary variables, an n-state variable needs to be characterized by two n-tuples:

Propagation of Information Through the Network

Assuming that the vectors h and 9 are stored with each node of the network, our task is now to prescribe how the influence of new information spreads through the network. Traditional probability theory, together with some efficiency considerations [9], dictate the following propagation scheme which we first report without proofs.

1. Each processor computes two message vectors: P and r. P is sent to every son while r is delivered to-the- father. The message p is identical to
the probability distribution of the sender and is computed from h and 4 using Equation (7). r is computed from -x using the matrix multiplication:

-r=M=aA -

(8)

where fi is the matrix quantifying the link to the father. Thus, the dimensionality of r is equal to the number of hypotheses managed by the father. Each component of r represents the diagnostic contribution of the data below the host processor to the belief in one of the father's hypotheses.

2. When processor B is called to update its

parameters, it simultaneously inspects the P(A)

message communicated by the father A and the mes-

sages g,~2,

.. ., communicated by each of its sons

and acknowledges receiving the latter. Using

these inputs, it thenupdates -h and 4 as follows:

3. Bottom-up propagation: h is computed using a term-by-term multiplication of the vectors ~1,
9, . . . :

X(Bi) = (VJ)i X (Q)i X ... = n(ck)j

(9)

k

4. Top-down propagation: 4 is computed using:

q(Bi) = B 1 P(BilAj)P(Aj)/(~`)j

(10)

j

where B is a normalization constant and r' is the

last message from B to A acknowledged by-the father

A. (The division by c' amounts to removing from

P(A) the contribution due to Dd(B) as dictated by

The definition of q in Equation (6)).

5. Using the updated values of 1 and 4, the messages c and r are then recomputed as in step 1 and are posted on the message-boards dedicated for the sons and the father, respectively. This updating scheme is shown schematically in the diagram below, where multiplications and divisions of any
two vectors stand for term-by-term operations.

The terminal nodes in boundary conditions. Here between the two cases:

CURRENTMES%lCE

TO

ALL SONS

the tree require

special

we have to distinguish

1. Anticipatory node: an observable variable whose state is still unknown. For such variables,
therefore, we should set

2. Data-node: an observable variable with a known state. Following Equation (5), if the jth state of B was observed to be true, set x = (O,O...O,l,O...) with 1 at the jth position.
Similarly, the boundary conditions for the root node is obtained by substituting the prior probability instead of the message P-(A) expected from the father.
A Token Game Illustration
Figure 2 shows six successive stages of belief propagation through a simple binary tree, assuming that updating is activated by changes in the belief parameters of neighboring processes. Initially (Figure 2a), the tree is in equilibrium and all terminal nodes are anticipatory. As soon as two data nodes are activated (Figure 2b), white tokens are placed on their links, directed towards their fathers. In the next phase, the fathers, activated by these tokens, absorb the latter and manufacture the appropriate number of tokens for their neighbors (Figure 2c), white tokens for their fathers and black ones for the children (the linksthroughwhich the absorbed tokens have entered do not receive new tokens, thus reflecting the division of P by rl), The root node now receives two white tokens, one from each of itsdescendants. That triggers the production of two black tokens for top-down delivery (Figure 2d). The process continues in this fashion until , after six cycles, all tokens are absorbed and the network reaches a new equilibrium.

135

Figure 2 Properties of the Updating Scheme

1. The local computations required by the proposed scheme are efficient in both storage andtimp. For an m-ary tree with n states per node, each processor should store n2+mnt2n real numbers, and perform 2n2tmn+2n multiplications per update. These expressions are on the order of the number of rules which each variable invokes.

2. The local computations are entirelyindependent of the control mechanism which activates the updating sequence. They can be activated by either data-driven or goal driven (e.g., requests for evidence) control strategies, by a clock or at random.

3. New information diffuses through the network in a single pass. Infinite relaxations have been eliminated by maintaining a two-parameter system (4 and r) to decouple top and bottom evidences. The time required for completing the diffusion (in parallel) is equal to thediameterof the network.

A Summary of Proofs

From the fact that X is only influenced by

changes propagating from the bottom and 9 only by

changes from the top, it is clear that the tree

will reach equilibrium after a finite number of updating steps. It remains to showthat,atequilibrium,

the updated parameters P(Vi), in every node V, corre-

spondtothecorrectprobabilities

P(VilDU(V),Dd(V))

or (see Equation (3)),thatthe equilibriumvalues of

h(Vi) and q(Vi)actuallyequal the probabilities

P(Dd(V)IVi)and P(VilD'(V)) This can be shown byinduc-

tion bottom-up for&and then top-down for 4.

Validity of A: x is certainly valid for leaf nodes, as was explained above in setting the boundary conditions. Assumming that theX's are valid at all children of node B, the validity of x(B) computed through steps (8) and (9) followsJirectly from the conditional independence of the data beneath B's children (Equation (2)).

Validity of q: if all the X's are valid, then P is valid for the root node. `iissuming now that P(A) is valid, let us examine the validity of q(B), where B is any child of A. By definition (equation (6)), q(B) should satisfy:

q(Bi)=P(BiID'(B))=

CP(BilAj)P(AjID'(A),Dd(S)) j

where S denotes the set of B's siblings. The second factor in the summation differs from P(Aj) = P(AjjD'(A),Dd(A)) in that the latter has also incorporated B's message (r')j in the formation of
When we divide P(Aj) by (lo), the correct proba-
bility ensues.

Conclusions

The paper demonstrates that the centuries-old
Bayes formula still retains its potency for serving
as the basic belief revising rule in large, multihypotheses, inference systems. It is proposed, therefore, as a standard point of departure formore
sophisticated models of belief maintenance and inexact reasoning.

REFERENCES

m Shortliffe, E.H., and Buchanan,B.G.,"AModel

of

Inexact Reasoning in Medicine". Math.Biosci.,

23 (1975), 351-379.

PI Duda, R.O., Hart, P.E. and Nilsson, N. J., "Subjective Bayesian Methods for Rule-Based Inference Systems". Tech. Note 124, AI Center, SRI
International, Menlo Park, CA; also Proc. 1976
NCC (AFIPs Press).

c31 Duda, R., Hart, P., Barrett, P., Gashnig, J., Konolige, K., Reboh, R. and Slocum J., "Devel-
opment of the Prospector Consultation System for Mineral Exploration". AI Center, SRI In-
ternational, Menlo Park, CA, Sept. 1976.

c41 Lesser, V.R. and Erman, L.D., "A Retrospective View of HEARSAY II Architecture". Proc. 5th Int. Joint Conf. AI, Cambridge, MA,l977, 790-800.
[51 DDI Handbook for Decision Analysis, Decision and Design Inc., McLean, VA, 1973.
[61 Rumelhart, D.E., "Toward an Interactive Model of Reading". Center for Human Info. Proc.CHIP56, UC La Jolla, March 1976.

[71 Schum, D. and Martin, A., "Empirical Studies of Cascaded Inference in Jurisprudence: Methodo-
logical Consideration". Rice Univ., Psychology
Research Report, #80-01, May 1980.

I31 Pednault, E.P.D., Zucker, S.W. and Muresan, "On the Independence Assumption Underlying Subjective Bayesian Updating". Art. Intel.,
Vol. 16, No. 2, May 1981, 213-222.

L.V.,

L-91Pearl, J., "Belief Propagation in Hierarchical Inference Structures". UCLA-ENG-CSL-8211, UC Los Angeles, January 1982.

136

