Controlling the Sensitivity of Support Vector Machines
K. Veropoulos, C. Campbell, N. Cristianini
Department of Engineering Mathematics, Bristol University, Bristol BS8 1TR, United Kingdom

Abstract
For many applications it is important to accurately distinguish false negative results from false positives. This is particularly important for medical diagnosis where the correct balance between sensitivity and speci city plays an important role in evaluating the performance of a classi er. In this paper we discuss two schemes for adjusting the sensitivity and speci city of Support Vector Machines and the description of their performance using receiver operating characteristic (ROC) curves. We then illustrate their use on real-life medical diagnostic tasks.

1 Introduction.

Since their introduction by Vapnik and coworkers Vap-

nik, 1995; Cortes and Vapnik, 1995], Support Vector Ma-

chines (SVMs) have been successfully applied to a num-

ber of real world problems such as handwritten charac-

ter and digit recognition Scholkopf, 1997; Cortes, 1995;

LeteCalu.,n1e9t9a7l].,a1n9d9s5p;eVaakperniikd,e1n9ti95c]a, tfiaocne

detection Schmidt,

1O9s9u6n]a.

They exhibit a remarkable resistance to over tting, a fea-

ture explained by the fact that they directly implement

the principle of Structural Risk Minimization Vapnik,

1995]. For noise-free classi cation tasks they work by

mapping the training points into a high-dimensional fea-

ture space where a separating hyperplane (w; b) is found

which maximises the margin or distance from the closest

data points. Hyperplanes can be represented in feature

space (a Hilbert space) by means of kernel functions (dot

products between mapped pairs of input points xi):

K(x0; x) = X i(x0) i(x)

i

Gaussian kernels are an example:

K(x0; x) = e jjx x0jj2=2 2

For 1; : : : ;

input p), the

pdoeicnitssionxifumncatpiopningis

to targets formulated

yini

(i = terms

of these kernels:

f(x) = sign

Xp

!
iyiK(x; xi) + b

i=1

where b is the bias and the coe maximising the Lagrangian:

cients

i are found by

L = Xp i
i=1

1 Xp
2 i;j=1

i

jyiyjK(xi; xj)

subject to constraints:

(1)

i 0 Xp iyi = 0
i=1

(2)

Only those points which lie closest to the hyperplane

have In

thie>p0re(stehnecesuopfpnoortisve,ecttworos

). techniques

can

be

used

to allow for, and control, a trade o between training

accuracy and simplicity of the hypothesis (equivalent to

a small norm of w). One consists of setting an upper

ibsoruendducoend.thTehseizoethoefrthcoensiis,tssointhaedidninugeancdeiaogfoonuatlleielers-

ment to the matrix K, e ectively mapping the data into

a \separation space" where a separating hyperplane is

found. Both these soft margin techniques were intro-

duced by (Cortes and Vapnik 1995) and result from the

follosmwuibinnjiegmctoipzteotimiCyzia(Pthiwon;ikxp+irioh+bwlbe;)mwsi: 1 i, with i 0

(3)

with k = 1 and k = 2. A theoretical analysis of both alagnordithCmrissthiaansinrieceSnhtalyweb-TeeanyloprroavniddedCbriystiSahnainwie,-T1a9y9l9o]r, based on the concept of \margin distributions".
For many decision support systems it is important to distinguish the two types of errors that can arise: a false alarm is usually not as expensive as a missed correct alarm. For example, for the detection of tumours from MRI scans it is important to avoid false negative results,

beruatteadsimf tahllensucamnbsearroefsfuablsseeqpuoesnittilvyerere-sscurletesnmedaybybemteodl-ical personnel. Similarly, for the condition monitoring of machinery, occasional false alarms may be less expensive than missing a correct alarm signalling imminant machine failure.
In this paper we will present two new techniques for controlling this trade-o between false positives and false negatives with Support Vector Machines. In section 3 we will compare these methods on real life medical datasets.

2 Sensitivity versus speci city

The performance of a binary classi er is usually quantied by its accuracy during the test phase, i.e. the fraction of misclassi ed points on the test set. However, as we have just remarked, the signi cance of the two types of misclassi cations may well be di erent. Consequently, the performance of such systems are best described in terms of their sensitivity and speci city quantifying their performance for false positive and false negatives. Systems can then be compared by using a ROC (Receiver Operating Characteristic) analysis. These techniques are based on the consideration that a test point always falls into one of the following 4 categories: False Positive (FP) if the systems labels it as a positive while it is a negative; False Negative (FN) if the system labels it as a negative while it is a positive; True Positive (TP) and True Negative (TN) if the system correctly predicts the label. In the following we will use T P , T N, F P , F N to denote the number of true positives, true negatives, false positives and false negatives, respectively. Note that with this notation the number of positive points in the test set can be written as T P +F N, the number of negative points as T N +F P , and the test set size as T P +F P +T N +F N: A confusion matrix can be used to summarize the performance of a learning machine:

Expected PN Machine P TP FP N FN TN

and thus a perfect predictor would have a diagonal confusion matrix.
We then de ne the sensitivity of a learning machine as the ratio between the number of true positive predictions T P and the number of positive instances in the test set:

sensitivity

=

T

TP
P +F

N

the speci city as the ratio between the number of true negative predictions T N and the number of negative instances in the test set:

speci

city

=

TN
TN +FP

the accuracy is the ratio between the number of correctly identi ed examples and the test set size:

accuracy

=

T

P

+

TP TN

+ +

T F

N P

+

F

N

For medical diagnosis sensitivity gives the percentage

of correctly classi ed diseased individuals and the speci-

city the percentage of correctly classi ed individuals

without the disease.

tsitoaRntiOTstChicesoa,rnmyaleySdswiicseatlissdaianadgcnlPaosiscsikisceatClt,emn1t9eot8hr2,o],1da9ni9nd1]Siasingundsaemdl Doarlesetoericen--

ccoenmtlpyarininMg laecahrinniengLesayrsnteinmgsasParonvaolstterentaatli.v,e1m99e8t]h.oRdOfoCr

space denotes a coordinate system used for visualizing

the performance of a classi er, where the true positive

rate is plotted on the y-axis, and the false positive rate

on the x-axis. In this way classifers are compared not by

a number, but by a point in a plane. For classi ers ob-

tained by thresholding a real valued function or depend-

ing on a real parameter, this produces a curve, called a

ROC curve, describing the trade-o between sensitivity

and speci city. Two systems can therefore be compared

with the better one being the highest and leftmost one.

The objective of this paper is to outline methods for

controlling the balance between the o -diagonal terms

in the confusion matrix. In the following we outline and

compare two schemes for doing this. The basic idea is

to introduce di erent loss functions for positively and

negatively labelled points, which translates into a bias

for larger misclassi

cmautilotnipliiserhseaviiefro.r

the class In turn,

where the cost of this induces a de-

cision boundary which is much more distant from the

`critical' class than from the other. In Shawe-Taylor,

1998] it is shown that the distance of a test point from

the boundary is related to its probability of misclassi-

cation (test points further away from the hyperplane

are less likely to be misclassi ed). This observation mo-

tivated a related technique to the one proposed in this

paper. Studying the case of very imbalanced datasets

(where points of one class are much more numerous than

pShoianwtes-oTfatyhleoro,th1e9r99cl]aspsr)o,ptohseedauathnoraslgoofriKthamrakwohuelares

and the

labels are changed in such a way as to obtain a larger

margin on the side of the smaller class.

.2.1 We can readily generalise the soft margin approach (3):

where:

w xi + b 1 i

i0 so that the primal formulation of the Lagrangian has two loss functions for the two types of errors:

Lp

=

jjwjj2 2

+ C+ Xp
fijyi=+1g

i+C

Xp
i
fijyi= 1g

Xp Xp

i yi(w xi + b) 1 + i]

ii

i=1 i=1

wtohesrheow ithat0thanedduail for0m. uIlattiisonthgeinvesstrtahieghstafmorewLaradgrangian as in (1) but with the i constrained as follows:

if yi = +1 and:

C+ i 0

C i0

if yi = 1.

.2.2 Instead of using also use the square of Vapnik, 1995]. Thus:

atnheLL1 2nnoromrmfoirnsttheeadlosCseosrtwese

can and

Lp

=

jjwjj2 2

+ C+ Xp
fijyi=+1g

2
i

+

C

Xp 2 i
fijyi= 1g

Xp i yi(w xi + b)

1 + i]

Xp ii

i=1 i=1

Using
@Lp=@ i

t=he0,daenridvatthieveKs u@hLnp-=T@uwck=er

0co, n@dLitpi=o@nbs:=

0,

and

i yi(w xi + b) 1 + i] = 0 and:

i i=0 we get the dual formulation:

LD

=

Xp
i

i=1

1

1 Xp 2Xi;j=1

i jyiyjK(xi; xj)
21X

2

4C+ fijyi=+1g i 4C fijyi= 1g i

Let:

+

=

1 4C+

=

1 4C

then the balance between sensitivity and speci city can be controlled using the following scheme, namely the diagonal components of the kernel matrices are supplemented by xed positive contributions:

K(xi; xi) K(xi; xi) + + for yi = +1 and:

K(xi; xi) K(xi; xi) + for yi = 1.
The addition of diagonal elements to the covariance function is a widely used in Bayesian regression theory,

where the amount of noise present in the data dictates the size of the regularising diagonal elements added to the covariance function. The use of diagonal terms applies to those cases in which the amount of noise is not the same throughout the data. By analogy, the above method amounts to assuming a noise which is dependent on the class. In a sense one class requiries more regularization than the other. From the perspective of large margin classi ers we have simply enforced an asymmetric margin to minimize the risk of misclassifying the elements of one of the two classes. Algorithmically, we have achieved this by changing the covariance function, e ectively choosing a special kernel which maps the data into a space where the standard hard margin algorithm can be used.

3 Numerical Experiments

Since our motivation for studying this problem comes from medical decision support we will compare the performance of these two methods on 4 medical datasets:

1. Heart .Disease Diagnosis In Figures 1 and 2 we

illustrate the performance of these two methods on data

from the UCI (Cleveland) heart disease dataset Blake

et al., 1998]. This consists of a binary classi cation task

with 13 input attributes and 270 examples. With test

sets of 27 examples performance for C and similarly with

(10==-fo10ldancadrnodvsasv-rvayarinyligidnag+tiCionn+)FwiigneusFrheiog2wu. rFteho1er

the ROC curves in Figure 3 the former method performs

better on the average though the di erence between the

two methods is not statistically signi cant.

70

60

50

40

30

20

10

0 0 5 10 15 20 25 30

Figure versus kernels

C1w:+itHh(exa-rat=xids4)a:t9fao:(rCtaheS=VgeM1ne)rt.aralAiisnasetdCio+nusedinrergcorreGa(asyeu-sassxtiiahsne)

number of false positives decreases (dashed curve) but

at the expense of an increase in the number of false neg-

atives (solid curve).

80

70

60

50

40

30

20

10

0

-10 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure versus

2+:

nels with

Heart data: the generalisation error (y-axis)

(x-axis) = 4:9

for (

a=SV0:M0).trAaisned+

using Gaussian kerincreases the num-

ber of false positives decreases (dashed curve) but at the

expense of an increase in the number of false negatives

(solid curve).

This2. Diabetes Dataset. UCI repository Blake et al.,

d19a9ta8s]eatnadlsococnasmisetsfroofma

the bi-

nary classi cation task with 8 input attributes. We used

768 examples and approximate 10-fold cross-validation

(76 test examples). From the ROC curves (Figure 4) we

see that the better on the

rst method with box constraints on average but not at a statistically signi

cai nits

level.

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 0 0.05 0.1 0.15 0.2 0.25 0.3

Figure 4: Diabetes data: true positives (y-axis) vs false

p2a.to2esdi(tviuvaseriysnign(xgm-ae+xtihs-o)dd(as2h.1e=d(vc1au:1rry)v.ien)g.TChe+

ROC curve gener- solid curve) and

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2

Figure 3: Heart data: true positives (y-axis) vs false

positives (x-axis) using method 2.1

( = 4:9). (varying C

+T-hseoRlidOcCurcvuer)veisgveenreyrasitmed-

il+ar-tdoatshheedROcuCrvceu)r.ve generated by method 2.2 (varying

3. .Liver Disorders Dataset The BUPA liver disorders dataset has 6 inputs and we used approximate 10-fold cross validation (34 test examples). There is little to distinguish the two methods from the ROC curves (Figure 5) though prohibitive training times for the second method meant only part of the latter curve could be plotted though this feature would depend on the training algorithm used Friess et al., 1998].

4.
our

TB
own

.dataset This dataset derives projects Veropoulos et al., 1998;

fr1o9m99]o. neThoef

task involves classi cation of image objects (TB bacilli

or non-bacilli) on images captured using a microscope.

It is intended as part of a system being developed for

semi-automated diagnosis to increase the volume of sam-

ples investigated and improve accuracy (current screen-

ing programmes may miss up to 30-50% of active cases

WHO, 1998]). A total of 1140 examples were used (each

with 14 input attributes) together with 10-fold cross val-

idation. Again (Figure 6) there is little to distinguish the

two methods though we were not able to complete the

ROC curve for the second method for the same reason

as occured in experiment 3.

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6
Figure 5: Liver data: true positives (y-axis) vs false positmi+veet-shdo(axds-ha2xe.1dis)(cvu(arrvy=ei)n0g:7C).+T-hseoRlidOCcucruverv) eagnedne2r.a2t(evdaurysiinngg

4 Conclusion.

In this paper we have outlined 2 schemes for controlling

the balance between false positives and false negatives.

bInoxoucronnustmraeirnitcsal(eCx+p;erCim)enistsbtehtetemr eotnhotdhebaasveedraogne

using for 2

datasets (heart and diabetes). However, the di erence

between the two methods is not statistically signi cant

for any of the used Friess et

4ald.,a1ta9s9e8t]stchoenmsidetehreodd.

For the algorithm based on additions

to for

tlhaergkeevrnaleul eds ioafgo(n+al;

proved to be ). However,

prohibitively the extent of

slow this

problem is likely to be related to the type of QP routine

used.

For many data mining applications the problem of im-

balanced data sets or asymmetric loss functions is com-

mon (with sensitivity and speci city more important

than overall performance). The techniques proposed

here are su ciently simple to be promptly implemented

while adding little further computational load to the al-

gorithm. As demonstrated in the above case studies,

they have the power to e ectively control the sensitivity

of the learning machine.

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11
Figure 6: TB data: true positives (y-axis) vs false positmi+veet-shdo(axds-ha2xe.1dis)(cvu(arrvy=ei)n0g:3C).+T-hseoRlidOCcucruverv) eagnedne2r.a2t(evdaurysiinngg

References
Blake et al., 1998] C. Blake, E. Keogh, and C.J. Merz. UCI Repository of Machine Learning Databases. Irvine, CA: University of California, Department of Information and Computer Science, 1998.
Centor, 1991] R.M. Centor. Signal detectability: The use of roc curves and their analyses. Medical Decision Making, 11:102{6, 1991.
Cortes and Vapnik, 1995] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273{ 297, 1995.
Cortes, 1995] C. Cortes. Prediction of Generalisation Ability in Learning Machines. PhD Thesis, Department of Computer Science, University of Rochester, 1995.
Friess et al., 1998] T. Friess, N. Cristianini, and C. Campbell. The kernel-adatron: a fast and simple learning procedure for support vector machines. In J. Shavlik, editor, Proceedings of the Fifteenth International Conference on Machine Learning, pages 188{196, 1998.
Karakoulas and Shawe-Taylor, 1999] G. Karakoulas and J. Shawe-Taylor. Optimizing classi ers for imbalanced training sets. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11. MIT Press, 1999.
LeCun et al., 1995] Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J.S. Denker, H. Drucker, I. Guyon, U.A. Muller, E. Sackinger, P. Simard, and V. Vapnik. Comparison of learning algorithms for handwritten digit recognition. In F. Fogelman and P. Gallinari, editors, International Conference on Arti cial Neural Networks, pages 53{60, 1995.
Osuna et al., 1997] E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An application to face detection. In Proc. Computer Vision and Pattern Recognition '97, pages 130{136, 1997.
Provost et al., 1998] F. Provost, T. Fawcett, and R. Kohavi. The case against accuracy estimation for comapring induction algorithms. In J. Shavlik, editor, Proceedings of the Fifteenth International Conference on Machine Learning (ICML98), 1998.
Schmidt, 1996] M. Schmidt. Identifying speakers with support vector machines. In Proceedings of Interface '96, Sydney, 1996.
Scholkopf, 1997] B. Scholkopf. Support Vector learning. R. Oldenbourg Verlag, Munich, 1997.
Shawe-Taylor and Cristianini, 1999] J. Shawe-Taylor and N. Cristianini. Further results on the margin distribution. In Proceedings of the 12th Conference on Computational Learning Theory, 1999.
Shawe-Taylor, 1998] J. Shawe-Taylor. Algorithmica, 22:157{172, 1998.

Swets and Pickett, 1982] J.A. Swets and R.M. Pickett. Evaluation of Diagnostic Systems: Methods from Signal Detection Theory. Academic Press, New York, 1982.
Vapnik, 1995] V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New York, 1995.
Veropoulos et al., 1998] K. Veropoulos, C. Campbell, G. Learmonth, B. Knight, and J. Simpson. The automated identi cation of tubercle bacilli using image processing and recognition techniques. In L. Niklasson, M. Boden, and T. Ziemke, editors, Perspectives in Neural Computing, ICANN98, volume 2, pages 797{ 802. ICANN98, Skovde, Sweden, (Springer), 1998.
Veropoulos et al., 1999] K. Veropoulos, G. Learmonth, C. Campbell, B. Knight, and J. Simpson. The automated identi cation of tubercle bacilli in sputum: A preliminary investigation. Analytical and Quantitative Cytology and Histology, to appear, 1999.
WHO, 1998] WHO. Fact sheet no 104: Tuberculosis, 1998.

