Estimating DNA Sequence Entropy

J. Kevin Lanctot University of Waterloo

Ming Liy University of Waterloo

En-hui Yangz University of Waterloo

Abstract

fold up on themselves and become active structures that

This paper presents the rst entropy estimator for DNA sequences that has both proven properties and excellent entropy estimates. Additionally our algorithm, the Grammar Transform Analysis and Compression (GTAC) entropy estimator, uses a novel data structure to repeatedly solve the longest non-overlapping pattern problem in linear time. GTAC beats all known competitors in running time, in the low values of its entropy estimates, and in the properties that have been proven about it.

control or in uence a chemical reaction in the cell. No all of the sequence information that gets copied
from the DNA ends up specifying a protein. In higher eukaryotes (such as plants and animals) much of the mRNA is cut out before the cell translates it into protein. The portions that are translated are called exons and the portions that are removed are called introns. Random changes in a sequence are thought to be more deleterious if they take place in an exon rather than in an intron so these two regions should have di erent information theoretic entropy.

1 Introduction
With the complete DNA sequences of many organisms already known, and the goal of completely sequencing the human genome making steady progress, the challenge of using this wealth of genetic information benecially presents a new series of problems to be solved. One such problem is recognizing and characterizing the di erent regions of DNA and their di erent functions. Information theoretic entropy is one tool that can be used to help solve these types of problems.
DNA stores the information for creating and organizing an organism. It can be thought of as a string over the alphabet fA, C, G, T g, which represents the four chemical components that make it up. In order to synthesize protein, the cell creates a transient copy of a portion of DNA, called mRNA. The sequence of mRNA

Since the alphabet of DNA, and mRNA have four symbols, if these sequences were totally random, it would take two bits per symbol to represent the sequence. However, only a small fraction of DNA sequences result in a viable organism, therefore there are constraints on those sequences which appear in a living organism. Common compression algorithms, such as Hu man, and various Lempel-Ziv based algorithms fail to compress DNA sequences at all (for an example using UNIX compress, see Table 2), and low order arithmetic encoding algorithms only compress to around 1:96 bits per base 4]. With only 61 of the 64 possible mRNA triplet combinations being used to code for protein, that property alone suggests an encoding of 1.977 bits per symbol for exons, so the arithmetic encoding is providing only a slight improvement.

is read, three symbols at a time, and this triplet speci es a single component of the protein called an amino acid. The components are strung together in the order their code appears in the mRNA sequence. Although made as a linear structure, many types of protein can

2 Previous Work
There have been several previous attempts to characterize the entropy of DNA. One of the most common approaches is to estimate the probability of n-tuples for large n, and use this value to compute the block

Address: Dept. of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada. E-mail: jklanctot@neumann.uwaterloo.ca.

entropy. The problem with this approach is that it converges too slowly, so even though genome databases are large and growing larger, the values that are ob-

ySupported in part by the NSERC Research Grant tained systematically underestimate the entropy due to

OGP0046506 and the Steacie Fellowship. Address: Dept. of Computer Science, University of Waterloo, Waterloo, Ont. N2L 3G1, Canada. E-mail: mli@math.uwaterloo.ca.
zSupported by NSERC Research Grant RGPIN203035-98. Address: Dept. of Electrical & Computer Engineering, University

the nite sample e ect, and must be corrected. Several researchers address this problem and have developed methods to correct it, such as Lio et al. 10], and Schmitt and Herzel 13].

of Waterloo, Waterloo, Ont. N2L 3G1, Canada. E-mail:

ehyang@bbcr.uwaterloo.ca

1

2.1 Biocompress Another approach is to compress In the cross validation approach, the sequence is parti-

the sequence in order to determine an upper bound on tioned into 20 equal segments, the algorithm is trained

the entropy. Such a method has been used by Grumbach on 19 of the segments and predicts the remaining seg-

and Tahi with their two algorithms Biocompress 3] ment. This approach is repeated 20 times using a di er-

and Biocompress-2 4]. Biocompress-2 is actually a ent segment as the test segment each time. The value

combination of three approaches: 1) Literal encoding, reported is the average of the 20 iterations.

where each symbol is coded as a two bit number; 2) While the cross validation estimate is used to

Arithmetic encoding, where a substring of symbols is remove the overhead associated with compression, a

encoded using second order arithmetic encoding; 3) simple example will illustrate how this approach can

Lempel-Ziv style encoding, where a substring is encoded severely underestimate the entropy of a genetic se-

as a pair of integers, one representing the length of the quence. Let the input be a tandem repeat, say rr, where

match, and the second representing the position of the r is a random genetic sequence and hence has an entropy

match somewhere to the left of the current position. of two bits per symbol. The cross validation approach

Biocompress-2 checks to see which method is more will report an entropy of close to zero, yet the entropy

e cient to encode a small portion of the input, and of rr is one bit per base. How likely is such an example?

then encodes the type of encoding, the length, followed Both tandem repeats, such as the one above, and dis-

by the actual encoding. The entropy estimates provided persed repeats are known to occur in DNA and comprise

by Biocompress-2 are presented in Table 2.

a substantial fraction of the human genome 9].

The second method that Loewenstern and Yianilos

2.2 Match Length Entropy Estimator Farach et use is called CDNA compress, in which the algorithm

al. 2] developed a novel algorithm to estimate the uses everything to the left of a nucleotide as the learning

entropy of DNA sequences called a match length entropy set to predict its value. The average over all positions

ttehhsteeimlietanhtgotcrhh. aorLfaecttthteeirnlgoinnLgiaesrtseepsquruebsesentnrctien,gthttehheamtvaaotlccuhceulreosnfgintLhitwaiots

is calculated and that is the value that is recorded in Table 2.

places: 1) starting at position i + 1, and 2) a sliding 2.4 Our Results In this paper, we develop a novel

window consisting of the previous algorithm was used to test the di

Nerwenccheasrbacettewrese. nTthhies

entropy estimator of DNA sequences, GTAC, which is based on the idea of Kie er and Yang 7] regarding

entropy of introns and exons, and contrary to what was the design and analysis of grammar based codes, and

expected, they found that the average entropy of exons which recognizes the reverse complement property of

was larger 73 % of the time and that the variability of DNA sequences. Our entropy estimator is universal

introns was larger 80% of the time. Farach et al. also in the sense that it does not assume any source model

proved that their algorithm was universal, that is, that and works for any individual sequence. Moreover, our

the entropy estimate will approach the true entropy as entropy estimator is well justi ed from the information

the size of the sequence increases, but only under the theoretic point of view.

assumption that the sequence is generated by a Markov In Table 1, we compare GTAC with the best known

process.

entropy estimators using three criteria:

2.3 CDNA Loewenstern and Yianilos 11, 17] de- Is the code universal with respect to any station-

veloped CDNA, a program that estimates the entropy of DNA sequences. The motivation for CDNA comes from the observation that naturally occurring DNA sequences contain many more near repeats then would be expected by chance. Two parameters that CDNA uses

ary source? That is, will the entropy estimate converge to the actual entropy if the sequence is long enough? A limited one, such as the Match Length entropy estimator must make the addition assumption that the source is a Markov process.

to capture the inexact matches are w, which represents the substring size, and h, which represents the Ham-

Is the run time linear?

ming distance. These parameters are used to create a

upeasneolfowf parenddichti.veCeDxpNeArtsthpewn;hl,eeaarcnhs

with di ering valthe weightings of

these various experts, using Expectation Maximization,

so that their predictive ability is maximized when com-

bined into a single prediction.

CDNA has been implemented in two di erent ways.

How good are the algorithm's entropy estimates? In summary, algorithms such as Unix compress and Biocompress-2 were designed as compression algorithms, so they tend to overestimate the entropy because they include overhead necessary for compression. CDNA was designed as an entropy estimator, but no

2

Algorithm Unix compress Match Length Biocompress-2 CDNA GTAC

Universal yes limited yes no yes

Linear Run Time yes yes yes no yes

Entropy Estimate worst { 3rd best 2nd best best

Gprxocheasss

no useless of deriving

symbols. x from the

That is, during production rule

the cor-

responding to the start symbol S, each production

rule in Gx is used at lease once.

Such a grammar is called an admissible grammar.

Example 1: Below is an admissible with x = aataaatgcaatatatatgc.

grammar

Gx

Table 1: Features of Various Entropy Estimators
convergence properties have been proven about it and its entropy estimates are inferior to GTAC's.
3 Our Approach

S ! BADBCCD A ! aa B ! At C ! at D ! Cgc

Before presenting our entropy estimator, we rst brie y Suppose that the original sequence x is converted

review grammar based codes, and then discuss how well the corresponding entropy estimators are.

into an admissible To compress x, the

CcoFrGresGpoxnbdyinga

grammar grammar

transform. based code

3.1 Grammar Based Codes A context-free gram-
mar (CFG) is a quadruple G = (V; T; P; S) where V is a nite non-empty set of variables, T is a nite non-empty

then uses an zero order or its equivalent form. its equivalent form and form, is not important

tator(oiTtuhehsnmeaceotwtditacehyicGsotodxpeoocitornonti.vtcseoTrmethqpeGurierxvseasaildneGnteoxrt

set of terminal symbols that is disjoint from V , S is a can see 7, 15, 16] for details. What is important to us

distinguished element of V called the start symbol, and here, is the characterization of the resulting compression

P is a set of production rules which map elements of V rate, which can lead to e cient entropy estimators.) To

onto (V T) . Using the concept of CFG, Kie er and Yang

7,

characterize the be the sequence

resulting obtained

compression rate, by concatenating

lteht e!r(iGghx)t

15, 16] recently put forth a new type of lossless source code called a grammar based code, and developed a

hananddthseidnedoefleatilnl gprtohdeucrtsitonaprpueleasraonfceGoxf

in some order each variable.

new universal lossless source coding theory. In this Let

theory, a grammar based code has the structure shown in Figure 1. The original data sequence x is rst

(3.1)

H

(Gx)

=

X

n(s)

log

j!(Gx)j n(s)

s

Input data Grammar Context-free Arithmetic Binary X transform grammar Gx coder codeword

where the summation is taken over all variables and

tteimrmesintahl esyvmarbioalbslei(norGtxe,rmn(ins)al

denotes the symbol) s

number appears

of in

Figure 1: Structure of a grammar-based code.

!!an((GGd xxt))h.,eaAcnoldsnovj,!einn(tG(io3xn.)1j)0d,letonhgoe1tleosg=athr0ietihslmeandgisothpretloeafdt.tivhIeentsoteeqbrumaessenco2ef,

transformed into a context-free grammar (or simply

aangdratmhemnacro)mGpxrefsrsoemd

which x can indirectly by

be fully using a

recovered, zero order

arithmetic code to compress

Gx,

string Since

matching is one has to

often fully

required that Gx satisfy the

Gusxe.dTino

get an appropriate some manner.

froelcloovweirngxcofnrosmtraiGntxs,: it is

The language generated by Gx consists of only x.

iaGtunhnnxedn7ots,Herhr1mom(5wGa]i,lnnxiczo)heilndao=rgeaEync3xtit4nrea:orm2pi76zyp]e.,lsoetfTthh1tehh,eeqer!ugfeao(srGnlualtmloxtiwt)imnying=aHgrc(GotBGmhxCxe.po)DrFrieeosasmrsacitaA,ohlntlpeearrdCtoaCvFtteehgGd.ce Theorem 3.1. According to arithmetic coding or enumerative coding, one can assign a uniquely decodable binary codeword B(Gx) to each admissible CFG Gx(or its equivalent form) such that

Gapxpeiasrsdeotnerlymionniscteic,onthtahte

is, any variable in V left hand side of the

(3.2)

jB(Gx)j = f(Gx) + H(Gx)

production rules, P. P does not contain the empty string on the right hand side of any rule.

wBunh(iGevrexer)js,Baal(inGtydxo)ffj(dgGreaxnm)otrmeesaptrrhebesaelsneetndsgtcthohdeoefost.vheIernhbei(an3da.2r)py,acifod(dGteowx)othrides

3

negligible as compared to H(Gx) and is upper bounded, in the worst case scenario, by

f(Gx) 5jGxj +

where of all of the

jpGsrooxudjurdcceetniooantlpeshruathlbeeesttooaftnaGdl exin,stiae4nsdiinn

the is the

right hand side the cardinality case of DNA

where jxj denotes the length of x.
(b) For any stationary, ergodic source entropy H, the quantity

f gX 1i i=1

with

max

jB(G)j n

?

H

: G 2 G(X1

Xn)

goes to 0 with probability one as n ! 1.

sequences.

>From Theorem 3.1, it follows that in order for a

grammar based code to be e cient, the corresponding

gtotyhvrapaemrtehmotehfaaedgrrufatn(mrGnamnoxrs)amforarartmelriazanesxdsfsome!rnmatlrlGocapaxsyllpesHdohso(isurGirbledxlde).ubocAefibnGdleeixnsgitgareanrnemedsdtmtinhsagoer

transform has been identi ed in 7]. An admissible

grammar properties

Ghxoldis:

said

to

be

irreducible

if

the

following

P1

EapvpereyarvsaaritalbelaesotftGwxiceotihnerthteharnigthhtehsatnadrt

symbol S side of all

P2 TprhoedruecitsionnorruelpeseaotfedGxp.attern of length 2 in the

P3

right hand side of all production rules of

Each distinct subtring of x.

variable

of

Gx

represents

aGdxi.stinct

dTuhceibaldem. Aissgibralemgmraamr tmraanrsGfoxrmshoiswsnaiidntEoxbaemirprleed1ucisibilrerei-f

it converts every sequence x into an irreducible grammar

Ginx

. 7]

Several reduction rules to reduce any reducible

have also grammar

been proposed transforms into

irreducible ones. Starting with the grammar consisting

of only one production rule S ! x and applying repeat-

edly these reduction rules in di erent orders, one can

design, in principle, many irreducible grammar trans-

forms. Among them are the longest matching substring

grammar transform 7], which will be analyzed and ex-

tended to DNA sequences, and the Yang-Kie er greedy

sequential grammar transform 15], which gives rise to

universal data compression algorithms signi cantly out-

performing the Lempel-Ziv type of algorithms such as

the Unix Compress and Gzip algorithms. Interestingly

enough, no matter how one gets irreducible grammar

transforms, they all give rise to e cient universal com-

pression algorithms, as shown in the following theorem

(see 7] and 8] for its proof).

Remark 1. Part (b) of Theorem 3.2 represents the

worst case scenario. The actual convergence rate at

which

jB(GX n

n

)j

?

H

XfanGwfoXcohXrntreiumngara.e=i1nal=yAle0iGznlXiesatrXodrnr1n,oeddpgfruytriohascXmHmiebnaimln,Trweraheiggtirdeorhrouraeeercspnmedirtmbumotrloocbseaipab3rgy0bl.re1italHrdimaatgey(nnprmGsadeofmanXno3rdrenm.mst2)ar=,aasonrixntnnsrgff!e!ootoplhrelrmeos1eGwstsXex.soon,utnthtitrnhh!cageeet

3.2 Grammar Based Codes Entropy Estimators

As suggested by Theorem 3.2 and Remark 1, we can

associate an entropy estimator with any grammar based

code. Given a grammar based code with grammar

twrhanicshforsmimpxly!useGsx

, we de ne an the normalized

entropy estimator grammar entropy

bHit(sGxp)e=rjxljettoefr Goxf

as x.

an estimate The reason

to we

the entropy in do not include

the of

normalized overhead f the associated entropy

(eGstxim)=ajxtojr

in is

the de nition obvious|the

inclusion generally results in an overestimate. When

the underlying grammar transform satis es Property

P2 and is asymptotically compact in the sense that

jsGucxhj=ejxntjrgoopeys

to 0 as jxj estimators

! 1, some ergodic behavior has been investigated in 8].

of In

this paper, we focus on a more restricted case in which

the underlying grammar transform is irreducible.

In terms of the terminology in estimation theory,

Theorem 3.1, Theorem 3.2, and Remark 1 imply that

whenever the underlying grammar transform is irre-

ducible, the corresponding associated entropy estimator

is consistent and asymptotically unbiased for the class

of stationary, ergodic sources. Although it would be

nice to determine the convergence rate at which the es-

Theorem 3.2. For any sequence x, let G(x) be the set timate provided by such an entropy estimator goes to

consisting of all irreducible grammars G representing x. the actual entropy H, this problem is in general very

Then the following hold.

di cult and thus left open for future research. Further-

(a) There is a constant c, which depends only the more, as indicated in Remark 1, the solution to this

cardinality of the source alphabet, such that for any problem depends on the actual source model and the

sequence x

Gm2Ga(xx) jGj

cjxj log jxj

underlying irreducible grammar transform. In our case of DNA sequences, we have no idea what kind of source model DNA sequences would obey. To get around these

4

di culties, in the following, we shall take a di erent Theorem 3.3 suggests that among all entropy esti-

approach. Instead, we shall show that no matter what mators associated with grammar based codes with irre-

the underlying irreducible grammar transform and the ducible grammar transforms, the best one correspond to

source model are, with a high probability these entropy the irreducible grammar transform which gives the least

estimators will never severely underestimate the actual entropy.

grnadminmgatrheenitrrroepdyucHib(leGgxr)a. mHmoawrewveitrh,

we the

believe that least gram-

Theorem 3.3.
afotrleaacsot n1s?tannt?dd

>Le0t ,ftXheigfi1o=l1lowbeinagnhyodldastawistohuprcreo.baTbihlietny
:

imnatrheenftorlolopwy inHg(Gsexc)tioisn,anweNsPh-ahlal rpdrepsreonbtleamli.neIanrstteiamde, algorithm for constructing an irreducible grammar with a good entropy estimate.

jH (GXn )j n

?

1 n

log

P

(X

)n

?

f (GX n n

)

?

d

log n

n

4 The Algorithm

for any grammar transform x ! Gx, where Xn = X1 Xn and P (X )n denotes the probability of Xn.

The Grammar Transform Analysis and Compression (or GTAC) is an example of a Grammar Based Code. The core of GTAC is to repeatedly solve the longest

Remark 2. >From information theory, one can interpret (? log P (Xn))=n as the entropy in bits per letter of Xn. From Theorems 3.1 and 3.2, it follows that for irreducible grammar transforms, f(GXn)=n is quite small and upper bounded, in the worst case scenario, by O(1= logn). Therefore, Theorem 3.3 says that with a high probability, the entropy estimators associated with grammar based codes with irreducible grammar transforms will never severely underestimate the actual entropy.

non-overlapping pattern (LNP) problem. The LNP problem is as follows: Given a set of strings, P, nd the longest substring such that occurs in at least two non-overlapping positions somewhere in P. The LNP problems can appear in the context of grammar, G = (V; T; P; S), when we let P be the set of all right hand sides of the production rules P, and we add the additional constraint that the length of is at least two. GTAC's goal is to repeatedly nd the LNP and reduce it, creating a new rule. If an LNP appears in the following form (both in the same string),

Proof of Theorem 3.3: Let Fn denote the set consisting of all sequences x of length n for which there

!A 1 2 3

is a grammar transform x ! Gx such that

rewrite the previous rule as two rules.

H(Gx) ? log P (x) ? f(Gx) ? d log n or equivalently,
log P (x) ?H(Gx) ? f(Gx) ? d log n

A! 1 B 2 B 3
B! If an LNP appears in di erent rules,

Then we have PrfXn 2 Fng = X P (x)
2x Fn

A! 1 B! 3

2 4

2? ? ?X

H(Gx) f (Gx ) d log n

then rewrite the previous rules and introduce a new one,

2x Fn

=

n 2? X d

?jB(Gx)j

as follows.

A! 1 C 2

2x Fn
(3.3) n?d

B! 3 C 4 C!

where the last inequality is due to the fact that the bi-

nthaeryKcoradfetwionredquBa(lGityx)

is uniquely among all

decodable, admissible

and hence grammars

holds:

2 1?j jX

B(G)

G is admissible

GTAC can optionally recognize reverse complements. In DNA sequences, the symbols a and t are the complement of each other, and the symbols g and c are the complement each other. A string r is the reverse complement of if r is the reverse of with each character complemented. For example, the reverse

>From (3.3), Theorem 3.3 follows.

complement of aaacgt is acgttt. As Section 5 makes

5

clear, the ability to detect reverse complements is an Trivial implementations of this algorithm requires

important feature of a DNA entropy estimator.

O(n3) time, where n is often in the order of a million or

The GTAC algorithm deals with reverse comple- more. For this size, even an O(n2) algorithm becomes

ments by having two sets of non-terminals, regular ones intolerable. Since a key feature of this approach is to

A1; A2; : : : and reverse complement ones R1; R2; : : :. repeatedly look for the LNP, the generalized su x tree

These non-terminals come into play as follows. Given (a su x tree that contains more than one string) is a

an input, x, the algorithm rst creates the trivial gram- natural data structure to consider because it can nd

mar S ! x. Next GTAC nds the LNP. If there is two the LNP in time linear in the total length of the right

or more occurrences any occurrences of

of .r

, create If there is

aonrulyleoAnei

! ignoring occurrence of

hand side of the grammar. However, GTAC continually rewrites the rules, reducing the size of the grammar, so

and one of r, then create a rule using one of the re- a key challenge is keeping the su x tree up-to-date.

vinetresrepcroetmtphleemseecnotnndono-ctceurrmreinnacels,oRf ith!e

, which means non-terminal in

Consider the following example where catactag is a substring in the input with cat; tact and tag appearing

the right hand side of a rule as the reverse complement elsewhere in the input. When an LNP is discovered,

of the rule.

in this case tact, rewriting it not only a ects the LNP,

For example, given the input aatactgagtaaa, GTAC it also a ects any patterns that overlap with the LNP,

rst creates the trivial grammar.

such as cat and tag. If the algorithm nds an LNP that

S ! aatactgatgaaa

occurs n times, and is l characters long, then rewriting it can a ect O(nl) other su xes.

Next GTAC nds the largest LNP, which is tact and its reverse complement agta. GTAC reduces this substring and creates a new rule

Another complicating factor is that a generalized su x tree directly reports the longest pattern (LP), but our algorithm requires the longest non-overlapping pattern, hence we must be able to obtain one from the

S ! aaR0gR0aa

other. The following lemma gives an algorithm that obtains this result, and provides a bound for the size of

R0 ! tact

the LNP given the LP.

Next, any remaining LNPs are rewritten, in this case Lemma 4.1. If the length of the LP is l, then an LNP

there is one, aa.

can be found with length at least dl=2e.

S ! A0R0gR0A0 R0 ! tact

Proof. Let the reported LP start at positions k and k+i. If i dl=2e we are done. If not, then the LP is a periodic string, and one can nd the start of the second

A0 ! aa Next, relabel the non-terminals in order of appearance

string at or beyond k+dl=2e and it will be at least dl=2e characters long. 2

with the reverse complement rules starting with symbol The preceding lemma characterizes the situation

R0, and the normal rules with symbol A0. A0 ! A1R0gR0A1 R0 ! tact

when the LP occurs twice, and the next lemmais needed when the LP occurs three or more times. Lemma 4.2. Given an LP that occurs three times in a string, at least two of those occurrences do not overlap

A1 ! aa

with each other.

The right hand sides of the rules are concatenated together in the following order: the start rule is rst,

Proof. Proof by contradiction. Assume that any two of the three substrings overlap with each other. Let the

followed by any reverse complement rules, followed by any normal rules and the rst occurrence of each non-

substrings start at k, k+i, and k+j with 0 < i < j < l, where l is the length of the LP. The LP is periodic, and

terminal is deleted.

one can use that property with the substring starting at k + j to show that the other two substrings match at

gR0A1aatact

k + l and k + l + i contradicting the fact that they are

The entropy is then calculated based on the fre- LPs. 2

quency of appearance of each symbol using Equa- With these two lemmas a subroutine for dealing

tion 3.1.

with overlapping matches can be outlined. If an LP has

6

just two occurrences, check for overlap, and if necessary create non-overlapping substrings. Given three or more matches, that LP will also be an LNP, so keep it as is.

4.1 Observations The data structure for GTAC is a
su x tree, along with a copy of the original input, and an array of queues. In order to understand how they interrelate, a few observations are necessary rst.

Observation 1. Since GTAC always considers the longest pattern at each iteration, if it is currently looking at an LNP of length l, the longest branch in the tree is at most 2l long, re ecting the fact the tree may contain a length 2l LP that corresponds to a length l LNP. Hence the most number of characters that the algorithm will have to search down the tree before uniquely identifying a su x is 2l + 1, because at that point it is guaranteed to be in a leaf. So if the LNP starts at position k, only su xes in the range k?2l; k+l?1] needs to be checked to see if they contains a pattern that overlaps the LNP.

Observation 2. No rule will contain a non-terminal

from a previous iteration. For example, if a substring

twch,1aefsro2eut:hn:we:dillllloanintsgeerbevseeotirnnpgbainertetpetarhlnaepcaeagdttrtaebtmrhynamatcanApro,oinibn-oettrecraomAufsiitenci,amlf1eAo.ri2,S:sto:ihnm:ecneel

an LNP will never straddle over the beginning of non-

terminal after it's been introduced, the su x can be

considered to end at that point in the su x tree. For

example, if a path from the root is p1p2p3 1 2 : : :, then

that path could be edited as p1p2p3$. For convenience, this

pla1tpt2epr3aApiprooracshimwpillyl

as be

followed. to be re

However the rewrite ected somewhere, so

ftrhoemor1igi2n:a:l

:inlptuot

Astirhinags

is rewritten, rather than the su x tree.

4.2 A Linear Time Algorithm With the above
observations in mind, a more relaxed version of the generalized su x can be used to implement the GTAC algorithm. In all, GTAC uses three data structures: a generalized su x tree T, which provides information about the LPs, an array of queues Q which bucket sorts the LNP information by size, and an array, x, holding the original input which provides information about the substrings that occur to the left of a given LNP.
First T is built from the original input, and then as it is traversed information about the LNPs are stored in Q. Then the LNPs are read from Q in descending order of size, and Q, x and T are kept up-to-date as the LNPs get removed. The whole algorithm is outlined and explained below.

GTAC(x) begin

1. create rule: S ! x;

T = new su x tree(x);

md = max depth(T);

Q ] = new array of queues(size = md);

2.

forife(ancihs

interior node an LNP)

n

of

T

do

add n to Q depth(n)];

end while;

3. for l = md downTo 2 do

while (Q l] is non-empty) do

n = pop(Q l]);

B = new non terminal;

4. for each ] = path to node n do

p ] = 2l chars to left of ] in y;

for i = 1 to 2l do

if (su x(p i]) contains 1] in T)

5. remove su x in T after p 2l];

end for;

6. for i = 1 to l do

if (su x( i]) goes beyond l] in T)

end rfeomr;ove su x in T after l];

7. replace with B in rules;

end for;

8. create rule: B ! ;

enednfdorw; hile;

estimate entropy based on grammar;

end alg;

1. Initialize Data Structures: Given an input string, x, create the trivial rule S ! x, a gener-
alized su x tree T, and an array of queues Q, with Q i] representing a list of LNPs of size i.

2. Fill Q: Traverse the tree, T, keeping track of the
depth. At each interior node n, which represents a pattern, check to see if it also represents an LNP and if so create a pointer to that node, and add it to the queue that corresponds to it's depth, namely Q depth(n)]. Also include a back-pointer from the interior node to its corresponding entry in the queue.

3. Get p s: Work through the Q array starting at the

largest value md. Given an LNP, say , from Q l],

for each occurrence of , consider the substring

that extends up to 2l characters on either side of

, namely represents

pth1ep2p:r:e: px2,l

a1nd2

:: s

:thles1ssu2

:

:: x

so2fl

where p . This

substring can be determined by consulting T to get

the list of locations of , and then consulting the

input string x to get p and s for that occurrence.

7

4.

DFaniendscdeennSdduindgoxweants:thFleosr1tsre2eae:c:ho:,nsuptehrxefosprtmaatrhtSinptge1ppas2t:4p:1:

pto2

:

:: 7.

with

two pointers, d-ptr and i-ptr. The d-ptr will point

to the leaf that corresponds to the unique su x

we are looking for and will eventually delete; the

entry in Q eliminated. The nal su x to be considered is ls1s2 : : :.
7. Edit Rules: Finally the rule containing is
updated by deleting that occurrence of and adding the appropriate non-terminal in its place.

i-ptr will point to the node where we insert the

end-of-string marker (which is always the node

bfOoebrtswtehereevnabteipog2niln?1n1ianabgnoodvfe,tha1e)s.eLaNrScPeha, rwci1hl.l

down this path Consistent with only go as far as

8. Create New Rule: A new rule is created and the
right hand side of that rule, is added to the su x tree (actually the last occurrence of in the tree is converted to this new entry).

the rst character in the leaf.

With only a few modi cations this algorithm also

5. Remove Su xes Starting in p: If, while search-

ing down a path, 1 is encountered, then the algo-

rithm will begin modifying the tree. First the i-ptr

stays may

at the have to

nboedcerebaettewdeehnerpe2la?n1d

aandcorr1e.spAonndoindge

su x link and entry made in the Q array. The

d-ptr continues down the path to the leaf corre-

sponding to this unique su x. If this leaf has more

than one sibling, then just delete the leaf that d-ptr

points to. If the parent node only has only two child

leaves, then delete the two leaves, and convert the

deals with reverse complements. In the rst step both the string x and xr are added to the su x tree, and when removing a su x both the forward and the reverse complement occurrences must be removed. As well, two sets of non-terminals (for normal and reverse complement patterns) with the decision about which to use take place between steps 3 and 4. The source code for this algorthm will be available at 1].
With a description of the algorithm complete the next step is to characterize the running time, which is as follows.

parent node to a leaf corresponding to the sibling

of the leaf that is being deleted. For example, if

the the the

pptwaatrohenlitenatnvooedtsheaebrpeeaclroaembnetelsnedtohdeeil+ies2alf:a:cb:oerlaernedsdpoci n:id:+:i1ntgahnetndo

Theorem 4.1. The GTAC entropy estimator runs in time linear in the size of its input. Proof. Assume the the input has m characters in it. In step 1, each statement can be performed in linear

the the

sbuackx-poiinit+e1rcf:r:o:m.

Wherever this node

a node is deleted, are followed, if it

time, such as building the su x tree 14]. Step 2 is to build and maintain Q, the array of queues. There are

exists, and it's corresponding entry in the Q array two aspects, the size of the array, and the number of

is removed. As well, the end-of-string marker, $, elements in a queue. The size of the array is at most

is added to where the i-ptr points to (representing m=2. The number of entries is bounded by the number

the new place where this su x ends, as explained of interior nodes in T, because an interior node could

in Observation 2). When nished with the cur- correspond to an LP, but not an LNP. Since each interior

rent su x and moving to the next one, su x links node in T has between two and ve children, and a su x

are used. A su x link is a pointer from the inte- tree built from a string of m characters has m leaves,

rior node that represents the su x starting with then picking the worst case, when each node has only

c to the one representing the su x starting with two children, T will have at most m nodes. Placing and

, where c is a single character and is a possibly traversing at most m entries in at most m=2 queues can

empty string. Both the i-ptr and the d-ptr inde- be done in linear time.

pendently take the su x links from their current Steps 3-7 are removing an LNP and taking care of

node to move on to the next su x, or go up a most all the patterns that overlap the LNP. For this situation,

one node, and use that su x link instead.

we have the following lemma.

6. Remove Su xes Starting in : A similar Lemma 4.3. Given an occurrence of an LNP of length

process is followed for the su xes that start in l, GTAC removes all possible patterns that overlap that

, except that the entire su x is eliminated with occurrence in O(l).

no need to insert a end-of-string marker anywhere.

If the path is corresponding

toi

tih+a1t:

:: su

l

sx1

sis2

e:l:i:msjin, attheednfrtohme

leaf the

Proof. GTAC removes the all the overlapping substrings for a given occurrence of an LNP in steps 4 to 7. During

tree entirely, and if necessary the parent node of these steps, the i-ptr and d-ptr go down a few nodes in

that leaf becomes a leaf, and the corresponding the tree, possibly a leaf is deleted, an internal node is

8

Sequence name PANMTPACGA MPOMTCG CHNTXX CHMPXX SCCHRIII HUMGHCSA HUMHBB HUMHDABCD HUMDYSTROP HUMHPRTB VACCG HEHCMVCG

Sequence length 100314 186609 155844 121124 315339 66495 73308 58864 38770 56737 191737 229354

Unix
compress 2.12 2.20 2.19 2.09 2.18 2.19 2.20 2.21 2.23 2.20 2.14 2.20

Biocompress-2 1.88 1.94 1.62 1.68 1.92 1.31 1.88 1.88 1.93 1.91 1.76 1.85

CDNA compress 1.85 1.87 1.65 { 1.94 0.95 1.77 1.67 1.93 1.72 1.81 {

GTAC 1.74 1.78 1.53 1.58 1.82 1.10 1.73 1.70 1.81 1.72 1.67 1.74

Table 2: Comparison of Entropy Values in Bits Per Symbol

converted to a leaf, and an entry in a queue is deleted. quences to compare their estimates. These standard se-

The i and d pointers may go up a node before following quences,(available at 12]) come from a variety of sources

a su x link, and then begin dealing with the next su x. and include the complete genomes of two mitochondria:

Of these operations, the only one that may take more MPOMTCG, PANMTPACGA (also called MIPACGA);

than constant time is when the i-ptr and d-ptr go down two chloroplasts: CHNTXX and CHMPXX (also called

the tree a few nodes.

MPOCPCG); the complete chromosome III from yeast:

We will give an argument for the d-ptr, with the SCCHRIII (also called YSCCHRIII); ve sequences

argument for the i-ptr being similar. While the d- from humans: HUMGHCSA, HUMHBB, HUMHD-

ptr can travel as much as 2l + 1 nodes to get to the ABCD, HUMDYSTROP, HUMHPRTB; and nally the

leaf representing a single su x, it's amortized cost for complete genome from the two viruses: VACCG and

dealing with all 3l su xes is constant. When the d-ptr HEHCMVCG (also called HS5HCMVCG). On these

moves up one node, and over one su x link, it loses test sequences, GTAC always beats Biocompress-2. As

height at most two nodes in the su x tree. This is well, GTAC beats CDNA on eight out of the ten se-

because the node depth of the su x c is at most one quence results that are available for CDNA. The en-

more that the node-depth of and a pointer can travel tropy estimates of all three algorithms are presented in

from one node to the next in constant time 5]. So Table 2.

in order to look for 3l su xes, it loses up to 6l nodes When GTAC completely ignores reverse comple-

due to following su x links, moves forward at most 5l ments, the values are only slightly worse (about 0.01-

characters to cover the all the overlapping substrings on 0.02 bits/symbol) for eight of the twelve sequences,

2l characters on either side of LNP. Thus GTAC moves but dramatically di erent for four sequences: the two

forward at most 11l nodes to remove 3l su xes. 2 chloroplasts, CHNTXX and CHMPXX, and the two

For a single iteration of step 3, say an LNP of length l with n occurrences is found. It takes O(l) time to remove each one of them. After n ? 1 of them are removed, a new rule is created and so the remaining occurrence of the LNP is converted to correspond to this rule. So to reduce the size of the original input by O(nl) characters takes O(nl), and the amount that gets removed can never exceed to original size of the input m, so this phase is O(m) as well. Thus the theorem is proved. 2

viruses, VACCG and HEHCMVCG. The results get worse by between 0.09 - 0.30 bits per symbol. This is because these sequences are known to contain long reverse complements. Two of these sequences are the ones that Biocompress-2 beats CDNA, and values were not available from CDNA for the other two.
We have performed some preliminary experiments with interesting consequences. The rst result concerns coding and noncoding regions in E. coli. Around 90% of the genome of higher eukaryotes is noncoding whereas about 15% of the genome of E. coli is noncoding. If

5 Implementation and Simulation Results
Other work in the area of estimating the entropy of genetic sequences have used the same benchmark se-

noncoding regions have a de nite role, they may be more regular than coding regions which would support the conjecture that noncoding regions in prokaryotes are

9

not junk. Our results con rmed this hypothesis. When comparing coding and noncoding regions of E. coli we found the following entropy values:
1.85 bits/symbol for coding regions (4,090,525 bases) 1.80 bits/symbol for noncoding regions (640,039 bases) These results are consistent with Farach et al. 2] who did a similar experiment comparing the entropy of introns and exons from human sequences. The second hypothesis we wanted to check was to verify if highly expressed essential genes have lower entropy than normal genes in E. coli because random mutations in normal genes are less likely to be deleterious. The results are as follows:
69 highly expressed essential genes: Mean: 1.7521 and sample variance: 0.0043 bits/symbol. 244 normal genes: Mean: 1.785 and sample variance: 0.0031 bits/symbol.
By statistical tests, with over 99% con dence, our hypothesis is supported.
6 Conclusion
While the idea of using Context Free Grammars in compression algorithms has been around for a while, the recent results have shown that if these grammars have the addition property that they are asymptotically compact then they are universal. This result has created a whole new family of approaches. One such algorithm in this family, namely GTAC, beats all known competitors for estimating the entropy of on a set of standard genetic sequences, and has the additional property that it has linear running time, and has been proven to be universal without assuming an ergodic source.
We are continuing to work in this area by modifying GTAC to include some of the approaches that other methods use, such as recognizing inexact matches which CDNA does.
7 Acknowledgments
We would like to thank Jonathan Badger, Paul Kearney and Huaichun Wang for helpful discussion and useful data.

2] M. Farach, M. Noordewier, S. Savari, L. Shepp, A. Wyner and A. Ziv, On the entropy of DNA: Algorithms and measurements based on memory and rapid convergence. Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms pp. 48-57, 1994.
3] S. Grumbach and F. Tahi, Compression of DNA sequences. Proceedings of the IEEE Symposium on Data Compression, 340-350, 1993
4] S. Grumbach and F. Tahi, A New Challenge for Compression Algorithms: Genetic Sequences, Information Processing & Management 30 (1994), 875-886.
5] D. Gus eld, Algorithms on Strings, Tress, and Sequences, Cambridge University Press, Cambridge, 1997.
6] J. Hopcroft and J. Ullman, Introduction to Automata Theory, Languages, and Computation, AddisonWesley, Reading, 1979.
7] J. Kie er and E. Yang, Grammar Based Codes: A New Class of Universal Lossless Source Codes, submitted for journal publication.
8] J. Kie er and E. Yang, Ergodic Behavior of Graph Entropy, ERA Amer. Math. Society, Vol. 3, no. 1, pp. 11-16, 1997.
9] B. Lewin, Genes VI, Oxford University Press, Oxford, 1997.
10] P. Lio, A. Politi, M. Buiatti, and S. Ru o, High Statistics Block Entropy Measures of DNA Sequences, Journal of Theoretical Biology 180 (1996), 151-160.
11] D. Loewenstern and P. Yianilos, Signi cantly Lower Entropy Estimates for Natural DNA Sequences, accepted for publication in the Journal of Computational Biology.
12] National Center for Biotechnology Information, Entrez Nucleotide Query, http://www.ncbi.nlm.nih.gov/ .htbin-post/Entrez/query?db=n s
13] A. Schmitt and H. Herzel, Estimating the Entropy of DNA Sequences, Journal of Theoretical Biology 188 (1997), 369-377.
14] E. Ukkonen, On-Line Construction of Su x Trees, Algorithmica 14 (1995) 249-260.
15] E. Yang and J. Kie er, E cient universal lossless compression algorithms based on a greedy sequential grammar transform{Part one: Without context models, to appear in IEEE Trans. Inform. Theory.
16] E. Yang and J. Kie er, Universal source coding theory based on grammar transforms, Proc. of the 1999 IEEE Information Theory and Communications Workshop, Kruger National Park, South Africa, June 20-25, pp. 75{77.
17] P. Yianilos, CDNA source code, http://
www.neci.nj.nec.com/homepages/pny/software/
.cdna/main.html

References

1] BioInformatics Group Homepage, .wh.math.uwaterloo.ca

http://
10

