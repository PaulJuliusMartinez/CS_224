From: AAAI Technical Report WS-98-05. Compilation copyright © 1998, AAAI (www.aaai.org). All rights reserved.
SpamCopA: SpamClassification & Organization Program
Patrick Pantel and Dekang Lin Department of ComputerScience University of Manitoba Winnipeg, Manitoba Canada R3T 2N2

Abstract
Wepresenta simple,yet highlyaccurate,spamfiltering program,called SpamCowp,hichis able to identify about92% of the spamswhilemisclassifyingonly about1.16%of the nonspame-mails. SpamCotpreats an e-mail messageas a multiset of wordsandemploysa na'fveBayesalgorithmto determinewhetheror not a messageis likely to be a spam. Comparewdith keyword-spottinrugles, the probabilistic approachtaken in SpamConpot onlyoffers high accuracy,but also overcomethse brittleness sufferedbythe keyworsdpotting approach.
Introduction
Withthe explosive growth of the Internet, so too comes the proliferation of spams. Spammerscollect a plethora of e-mail addresses without the consent of the ownersof these addresses. Then,unsolicited advertising or even offensive messages are sent to them in mass-mailings. As a result, manyindividuals suffer from mailboxesflooded with spams. Manye-mail packages contain mechanisms that attempt to filter out spamsby comparingthe sender address of the e-mails to predefinedlists of knownspammers. Such programs have had limited success since spammers often changetheir address and newspammerscontinuously appear. Furthermore, spammershave found ways to send messageswith forged headers. For example, the sender address can be madethe sameas the receiver address. Amore general and effective approachis obviously needed.
In (Cohen 1996a), Cohenpresented an approach to mail classification in whicha learning program,called RIPPER(Cohen 1995; 1996b), was used to obtain a set keyword-spottingrules. If all the keywordsin a rule are found in a message, the conclusion in the rule is drawn. For example,RIPPERcreated the following set of rules to recognize talk announcements:

Amessageis a talk announcemenift it contains one of the following:
¯ 'talk' and'talk' in 'Subject:'field; ¯ '2d416'and 'the'; ¯ 'applications' and 'commai'n 'Subject:' field; ¯ 'visual'; ¯ 'design' and'transfer'; ¯ 'place' and 'colon' and 'commai'n 'To:' field; ¯ 'doug'in 'From:'field and'specification'; ¯ 'presentation'
Otherwise, the messageis not a talk announcement.
Cohenreported that the rules generated by RIPPERhave similar accuracy as manuallygenerated rules.
Spams form a semantically muchbroader class than the categories in Cohen's experiments. Their subjects often range from advertising products to "make money fast" schemes to WEBsite for "fun-loving adults". The keyword-spottinrgules appearto be too brittle for this purpose. The inadequacyof this methodfor spamfiltering is also evidenced by the fact that even experiencedcomputational linguists are not able to comeup with a goodset of keywordcombinationsfor this purpose.
In this paper, wepresent a spam-filtering program,called SpamCop,which employs a naive Bayes algorithm to detect spams. The remainder of this paper is organized as follows. The next section describes the SpamCopprogram. Wethen present someexperimental results and a comparison with RIPPER.
Description of SpamCop
SpamCopuses a naive Bayes algorithm to classify messages as regular spam or as nonspam. A message M is classified as a spam if P(Y;pamlM) is greater than P(NonSpam)lM. In most probabilistic approaches to text classification, the attributes of a messageare defined as the set or the multiset of wordsin the message.Howevert,his is not the only viable alternative. For instance, one can

also define all the three consecutiveletter sequences(trigrams)as the attributes. OnceMis represented as a set of attributes (al,..., an), the classification problembecomes that of finding the larger one of P(Spamlal, ..., an) and
P( NonSpamlal,..., an). Since

P(Spamlal,...,

an) P(Spam, al,..., an ) P(al,..., an)

P(NonSpam[al,..., an) P(NonSpam, al,..., an P(al, ...,an)
the problembecomesdetermining which is the larger one between P(Spam,al, ..., an) and P(NonSpama, l,..., an), whichcan be rewritten as:
P( Spam,al, ..., an) P (al, ..., an I Spam)P(Spam)
P(NonSparn,al, ..., an)
P(al, ..., anlNonSpam)P(NonSpam)
If wefurther assumethat the attributes in a messageare conditionally independent given the class of the message, the right handside of the above equations become:
P(Spam,al, ..., an) P(al ISpam) . . . P(anlSpam)P(
P(NonSpam, al,..., an) P(aa [Non@am)... P(an[NonSpam)P( NonSpam)
We now describe how to estimate the probabilities P(ailSpam), P(ailNonSpam), P(Spam), P(NonSpam).Oncethese probabilities becomeavailable, the above formulas will allow us to determine whichclass has the higher conditionalprobability.
Wefirst tokenize the message.Atokenis either a consecutive sequenceof letters or digits, or a consecutivesequence of non-space,non-letter and non-digit characters (we limit the length of the second kind of token to be at mostthree characters long). The spaces are ignored. Wethen remove the suffixes fromthe tokens using an implementationof the Porter stemmer(Porter 1980) by Frakes and Cox. The frequencycounts of the suffix-removedtokens are then accumulatedin a frequencycount table. For each wordWin the training messages, the frequency table contains the count N(W, Spare), and N(W, NonSpam), which is the number of times the word Woccurred in the documentsthat belongto class C. Thefrequencytable also records the total numberof words(not necessarily unique) in the spamand nonspam messages: N(Spam) and N(NonSpam). Table 1 illustrates a subsetof the frequencytable.
Once the frequency table is created, we use the mestimate method(Mitchell 1997)to estimate the conditional

Table 1: Sampleentries fromthe frequency table

word
III $ adult 000 million order ### bulk monei

nonspam spam 2183 703
60 0 716 295
52 0 178 26 69 2 253 60
44 0
43 0
127 19

word report mail ship : you /// email address your
busi

nonspam spam
215 64
358 167
36 0
36 0 1165 1210 251 103 212 77 239 99 581 458 122 30

and prior probabilities of the words. M-estimate can be viewed as mixing the sample population in the frequency table with muniformlydistributed virtual examples.In our experiments, weused m=land the probability of a wordin the virtual exampleis ~ whereK is the numberof unique wordsin the training messages.In other words,
N (W, C)+ -k P(WIC) = N(C)
where C is Spamor NonSpamand Wranges over the set wordsin the training messages.
Somewordsare not goodindicators of the classification of the message in which they occur. Weemployeda feature selection algorithm to removesuch wordsfromthe frequencytable so that the classification of a messagewill not be affected by the accumulation of noise. A word Wis removedfrom the frequency table if one of the following conditions are met:
¯ N(W, Spare) + N(W, NonSpam) < 4; or
¯ P(WlSpam)P+(PW(W[SlpNaomn)Spam) [0.45,0.55].
Experimental Results Setup
Ourtraining data consists of 160spamsthat weresent to one of the authors (DL) and 466 nonspammessagesin DL's mailbox. The testing messages consist of 277 spares obtained from the Internet 1 and 346 NonSpame-mails in DL's mailboxfrom a different (but adjacent) time period. The header information is removed from the messages. The classification is completelybased on the bodyof the messages.
There are a total of 230449wordsin the training messages with 60434in spamsand 170015in nonspams.There
i http://pantheon.cis.yale.edguf/ojot/junk.html

are 12228entries in the frequencytable. Applyingthe feature selection rules from the previous section reduces the numberof entries to 3848.
Evaluation Measures Let
¯ TrueCount(Spam) and TrueCount(NonSpam) denote the numberof spamand nonspammessages in the testing data.
¯ CorCount(Spam) and CorCount(NonSpam) denote the numberof messagesthat are correctly classified as Spam and NonSpamby SpamCop.
Weuse three measures to evaluate the performance of SpamCopf:alse positive rate Rfp, false negative rate Ryn, anderror rate Re:
Ryp = 1 - CorCount(NonSpam) TrueC ount( N onSpam
Rfn = 1 - CTroureCCoouunnt(tS(Sppaamm))
CorCount(Spam) + CorCount(NonSpam)
Re:l--
TrueCount(Spam) + TrueCount(NonSpam) The false positive rate is the percentage of nonspammessages that are incorrectly classified as spam. It measures howsafe the filter is. Thefalse negativerate is the percentage of spare messagesthat pass through the filter as nonspams.It measureshoweffective the filter is. Theerror rate measuresthe overall performance.
Results Table2 summarizeosur results. It can be seen that although naive Bayesalgorithm is extremelysimple, it achievedvery high accuracy, especially with respect to the nonspammessages. Feature selection reducedthe frequencytable to 1/3 of its original size and resulted in a slightly higher false positive rate, a muchlower false negative rate and a lower overall error rate.

spam/nonspamratio as the wholeset. The results are presented in Table3. Thefirst columnis the data size in terms of the numberof partitions. For each data size werandomly selected 5 configurations. Theaveragerates of the 5 configurations are shownin last three columnsin Table3. The second columnindicates whetherthe feature selection algorithm wasused or not.

Table 3: Effects of the numberof training examples

Size

Feature Selection

Rfp

Rfn

Re

1/5 yes 1.68% 12.35% 6.42%

1/5 no 1.33% 14.08% 7.00%

2/5 yes 1.68% 10.97% 5.81%

2/5 no 1.10% 12.64% 6.23%

3/5 yes 1.21% 8.66% 4.53%

3/5 no 0.92% 11.84% 5.78%

4/5 yes 1.01% 8.94% 4.53%

4/5 no 0.79% 11.64% 5.62%

SpamCopachieves good performance with as few as 32 spammessages and 91 nonspammessages as training examples. Applyingfeature selection consistently produced the sameeffect: slight increase of false positives, a decrease of false negatives, anda moderatedecreaseof the error rate.
Wealso experimented with varying the ratios between the numberof spamand nonspammessages. The first two columnsin Table 4 represent the numberof spamsand nonspamsused in training. Comparedwith the results in Table 2, it appears that a higher ratio of training examplesin a category increases the performancein that category. However, it significantly decreasesthe performancoef the other category.

Table 4: Effects of varying ratios of spamand nonspams

spams nonspams
32 466 160 91

Rip Rfn

Re

0.06% 53.07% 23.63%

12.60% 1.44% 7.64%

Table 2: Testing results with 277 spamsand 346 nonspams

Feature Selection
yes no

Rfp
1.16% 0.58%

Rfn
8.30% 13.36%

Re
4.33% 6.26%

Wealso investigatedthe effects of the size of the training data on the performanceof SpamCopW. edivided the training data into 5 even partitions. Eachpartition has the same

Using trigrams
Instead of suffix-stripped words, wealso used trigrams extracted from wordsas features. Atrigram in a wordis a consecutive sequenceof three letters in the word.Table 5 illustrates the results of the use of trigramin spam-filtering, using the sametraining and testing data as the experiment described in Table 2. Considering the amountinformation that gets lost whenusing trigrams over words,the values in
Table 5 are remarkablyclose to the values in Table 2. This might be attributed to the fact that since there are much

fewer unique trigrams than unique words, the probability estimations for trigrams are moreaccurate.

Table 5: SpamCoperformance using trigrams

Feature Selection
yes no

Rfp
4.91% 2.89%

Rfn Re
6.50% 5.62% 9.03% 5.61%

Comparison with RIPPER
RIPPERis a rare symbolic learning programthat is able to deal with texts. Weran RIPPERwith the sametraining and testing data as used in testing SpamCopW. eused the Porter stemmearnd treated all the suffix-stripped roots as the features. RIPPERgenerated 9 rules with 24 conditions and achievedan error rate of 8.67%on the 623testing messages.The use of the stemmersignificantly influenced the performance. Without the stemmer, RIPPERgenerated 19 rules with 50 conditions and achieved an error rate of 13.64%on the 623testing messages.
Thetop ranked rule in RIPPERis that "if a messagecontains both the dollar sign and the exclaimation markthen it is classified as spam."This rule correctly classified 54 spamsand misclassified 5 out of 466nonspamisn the training messages. Althoughthis rule performed very well, it will misclassify long nonspamswhich happen to contain these twowords. In contrast, our probabilistic algorithmis muchmore robust.
Another examplethat demonstrates the advantage of a probabilistic classification over a rule-based classification is the word"you". The word"you" has one of the highest ratio betweenits conditional probability in spamand nonspammessages. In an extreme case, one of the spams in the training examplecontained 99 occurrences of "you" or "your" in 112 lines of text. Therefore, a high frequency of "you"is a definitely goodindicator of spams. However, "you" is also a commonword in nonspams. A keywordspottingrule will not be able to usethis in classification.
Conclusion
Wepresented a simple, yet highly accurate, spam-filtering program,called SpamCopI.t treats an e-mail messageas a multiset of wordsand employsa naive Bayes algorithm to determine whetheror not a messageis likely to be a spam. Our experiments show that SpamCopis able to identify about 92%of the spams while misclassifying only about
1.16%of the nonspame-mails. Our experiments also show that high classification accuracy can be achieved with as few as 32 spamexamples. Comparedwith symbolic learning programs such as RIPPER,SpamCopproduced higher

accuracyanddoes not suffer fromthe brittleness associated with keyword-spottingrules.
References
Cohen,W.W.1995. Fast effective rule induction. In Machine Learing: Proceedingsof the Twelfth International Conference. Lake Taho, California: MorganKaufmann.
Cohen,W.W.1996a. Learningrules that classify e-mails. In AAAISpring Symposiumon MachineLearning for Information Access. AAAI.
Cohen,W. W. 1996b. Learning with set-valued features. In Proceedings of AAAI-96.
Mitchell, T. M. 1997. MachineLearning. McGraw-Hill.
Porter, M. E 1980. Analgorithm for suffix stripping. Program 14(3):130-137.

