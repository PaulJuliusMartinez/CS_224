The PQCC experience reveals that-contrary to common practice and belief-retargetability and a high level of optimization are not incompatible.

An Overview of the Production-Quality Compiler-Compiler ProjectBruce W. Leverett, Carnegie-Mellon University
Roderic G.G. Cattell, Xerox Palo Alto Research Center Steven 0. Hobbs, Digital Equipment Corporation Joseph M. Newcomer, Carnegie-Mellon University Andrew H. Reiner, Carnegie-Mellon University
IBruce R. Schatz, Bell Laboratories
William A. Wulf, Carnegie-Mellon University

I

The Production-Quality Compiler-Compiler project, an investigation of the code generation process, has as its practical goal the building of a truly automatic compilerwriting system. Compilers built with this system will be competitive in every respect with the best of today's handgenerated compilers, generating highly optimized object code and meeting high standards of reliability and reasonable standards of performance. The system must operate from descriptions of both the source language and the target computer. The cost of bringing up a new
compiler, given suitable languake and target architecture
descriptions, must be small-on the order of three manmonths, without assistance from builders, maintainers, or other persons deeply involved in the original system.
The PQCC project builds on previous (and current) research in two areas-code optimization and compilerwriting systems (compiler-compilers)-and, in broad terms, relates to it as follows:
(1) With some notable exceptions, previous work in compiler development tools has focused on the parsing and lexical analysis phases of compilation. Thus, "compiler-compiler" has become almost synonymous with "parser generator." We would like to extend the function of compiler-compilers to include the production of optimizers and code generators. When this is done, we believe that compiler-compilers will become far more popular as commercial software development tools.
(2) A great many code optimization techniques are known and have appeared in the literature. Nevertheless, construction of optimizing compilers is still very nearly a black art. As a result, such compilers tend to be expensive both to build and use. They also tend to be unreliable, producing "optimized" object code that is often worse than the unoptimized code-or altogether wrong. We

would like to organize, even formalize, the huge "bag of tricks" associated with code optimization. Ultimately, optimization should be done as routinely, cheaply, and reliably as parsing.
POCC objectives
Standards and limitations. The success of the compilercompiler will be judged partly by the cost of bringing up a new compiler to produce code for a different target machine and/or compile a different language, and partly by the quality of such a compiler. The cost, possibly including the time to write language and machine descriptions, should be fewer than three man-months, but it is harder to state specific goals for quality. Measuring overall "quality" is many-faceted and is still very judgmental. With regard to the quality of the code produced, we have set up the Bliss- Il compiler' as a subjective standard,2-4 but we are unwilling to set particular limits on the size and speed of the compiler itself without a thorough (quantitative) understanding of the costs of various code generation and optimization techniques. When this information is complete, we can choose a collection of techniques that meet acceptable performance standards.
To keep the size of our research task finite, we limited the range of languages to be compiled, the range of target machines, and the set of phases to be included. We have excluded languages which provide the programmer with a well-supported and nontrivial data abstraction-e.g., list processing (Lisp), array processing (APL), and string processing (Snobol). While these languages present interesting and worthwhile optimization problems, op-

38

0018-9162/80/0800-0038$00.75 ( 1980 IEEE

COM PUTER

timization and code generation must be done in terms of run-time subroutines rather than instructions.
We are considering a wide but still limited range of target machine architectures, including one-address (PDP-8), two-address (Nova, PDP-1 1), general-register (PDP-10, 1108, S/370), and three-address (Cray- 1, VAX) architectures, and mixtures of these (almost all real com-

to parameterize a technique so that it can be moved from one target machine to another by changing only the set of
ttmaiebmtlihezosadt;idoenstchtreeicbbhiunnligkquotefhseobuumrtacwthaooirnfkeor.misaTlnhioaztettmoiosdreoevuiorsrelpnersiesnwcsitoappna--l dard techniques, so that they can be easily parameterized.

puters are mixtures). We also excluded pure stack

machines, and other designs even more specialized toward high-level languages, for reasons similar to those for excluding special-purpose languages. For these architectures, many of the usual techniques for code generation and optimization, such as global and local register allocation and targeting, are partly or even wholly irrelevant.

The
begins

literature on code optimization at least as early as the Fortran

I

compiler. Since then, there have been

dozens, perhaps hundreds, of papers about individual optimization techniques.

Note that poorly designed architectures have not been ex-

cluded. Thus, we are prepared to deal with very asym-

metric instruction sets or systems in which supposedly general-purpose registers or other locations must be very nearly dedicated to particular purposes. (This latter category includes instruction sets in which common arithmetic operations may only read their operands, or write their results, in highly restricted categories of registers. The 8080/Z80 machines are well-known examples.)
We are not studying parsing or lexical analysis; the art of parser generation is fairly well advanced already. Nor are we studying loading and relocation; we can provide instructions formatted for an assembler, or even for binary files, using information present in our target machine descriptions.
Related research. Many compiler-writing systems pro-

At least three other promising research or development

efforts, with goals similar to those of PQCC, have been

described recently. The Experimental Compiling Systems

project20-22 is addressing problems of target machine and

language independence in optimizing compilers. The ECS

and PQCC points on

projects treat optimization from divergent the philosophical spectrum, representing

"knowledge-impoverished" and "knowledge-based"

systems (to use terms borrowed from
research23). The ECS approach

artificial intelligence
avoids special-case

analysis, using instead a few powerful and general tech-

niques-such as global flow analysis, constant folding

and propagation, procedure integration, and dead code

elimination-to
discovered only

subsume
by more

optimizations that are usually specialized, or even ad hoc,

methods. The PQCC methodology, by contrast, is

vide little or no support, or even a well-defined framework, for code generation. Typical of the usual ap-

knowledge-based; we are willing known optimization techniques

to use all the (dozens of) and subtechniques, sub-

proach is Yacc,5 a parser generator which works from a ject only to the condition that we can parameterize them

language description in the form of a context-free gram- to use easily understood machine and language despripmar or a set ofproductions. For each production, the pro- tions. Other projects of interest include MUG2,24-26 an ef-

grammer can write an associated subroutine and indicate fort to produce (multipass) optimizing compilers from

what arguments are passed to it. These subroutines must
do all the processing necessary for semnantic analysis and

language and machine descriptions, and the portable C compiler,27'28 which attempts to isolate the machine-

code generation, and the programmer must devise all the dependent portions of the compiler.

data structures and algorithms necessary for processing.

In other systems, such as GCL,6 the programmer receives some support from a built-in data structure (a tree)

Project strategy and methodology

representing the program and from a few primitives fdr

manipulating that structure and/or generating instructions. More recent efforts to formalize code generation, summarized by Cattell,7 have been closer in their aims to

System organization. Figure l is a crude box diagram of the PQCC system and a compiler generated by it. The

the PQCC project; some of these are discussed in later

sections.

The literature on code optimization begins at least as

early as the Fortran I compiler.8 Since then, there have

been several famous (or notorious) optimizing compilers

about which landmark papers (or books) have been writ-

ten,1'-49 and dozens, perhaps hundreds, of papers about

individual optimization techniques. 10-13 Some significant

optimization techniques are almost wholly independent

of the architecture of the target machine. These include

data and control flow analysis,14"5 evaluation order rear-

rangement,'6 and the "packing" phase of global register

allocation. 17,18 In a few other areas, such as code selection

and local register allocation,'9 attempts have been made Figure 1. Simplified block diagram of the POCC system.

August 1980

39

compiler consists of a skeleton compiler PQC and a set of tables which it uses as input (in addition to the user program); the tables contain all the necessary language and target machine-dependent information. Note, however, that these tables are not themselves written or modified by the compiler writer-that is, they are not the original descriptions of the language or target machine. They are instead output from the compiler-compiler system PQCC, which takes as input the original descriptions.
Figure 2 shows some of the structure of the PQC and PQCC. The division of the compiler into phases has two important consequences to our system development methodology:
(1) The phases operate serially (at least in the experimental versions of the system), each taking as its input the entire output of the previous phase and performing some intellectually manageable subtask of the compilation process. This allows the PQCC itself to be decomposed into intellectually manageable portions: each phase has an associated phase generator which creates the language/machine environment information required by that phase.
(2) Work on each phase can proceed independently of work on (almost) any of the other phases. Thus, if some phase does not work, or even exist, progress can still be made on the phases following it in the compiler. This has been very difficult to achieve since it requires that any phase be debuggable in stand-alone mode-that is, without being plugged in to the entire compiler. It has been achieved by the use of a uniform syntax for input and output of all data structures by every phase. On top of this, there are uniform semantics, described in the next section, for the representation of user programs.
It is important to note that target-dependent optimizations are neither avoided nor isolated in phases separate from target-independent optimizations. Rather, target dependence itself is isolated from each phase so that it can be organized as target-independent code operating with

target-dependent tables. This principle is just as important for a humble single-pass compiler as for an ambitious, complex, multipass optimizing compiler. If a compiler is to be adjusted from producing code for one target machine to producing code for a different one, the emount of human intervention required is one or more orders of magnitude greater, if any part of the compiler code must be rewritten, than it would be if modifications were confined to fixed-format tables.
Intermediate representations. The input-output syntax-Linear Graph Notation-is fully described in an earlier report.29 Here, we discuss only our internal representation ofprograms, because it is important to our style and philosophy of code generation and optimization. The initial choice is between various linear representations, such as triples or quadruples (two-address and three-address "instructions"), and a two-dimensional representation, such as a parse tree. We chose the latter. A number of arguments for this choice have been presented elsewhere6'30; our principal reasons can be summarized as follows:
(1) Linear representations are too confining in their prescriptions for the storage and retrieval of operation results. It is difficult to generate good code for threeaddress machines from triples, or for two-address machines from quadruples.
(2) Linear representations require breaking down source-level control constructs into tests and jumps. It becomes more difficult to detect instances when special loop-control instructions and other optimizations are applicable.
(3) The tree representation has a good deal of psychological naturalness. It seems to be much easier for a human reader to grasp the data dependencies of a program from a parse tree than from a list of triples or quadruples; the latter has all the readability problems of assembly code.

Figure 2. Detailed block diagram of the POCC system. 40

COMPUTER

We consider the representation of programs to be a notation or language, called TCOL. * A TCOL tree is not exactly a parse tree, but a tree that reflects abstract syntax. Operator precedence rules, and parenthesization by the programmer to get around them, are no longer evident in the tree; terminals in the grammar are not necessarily represented by leaves in the tree. (In particular, a binary operator expression is represented by a node with two operands; the value of the op attribute is the identity of the operator. In a parse tree, the node would have had three operands, of which the middle one would be the operator.)
More importantly, TCOL follows a number of semantic rules. All operations, including coercions and dynamic checking of subscript ranges and types, must be explicitly represented in the tree. This includes the "dereferencing" coercion, so effectively TCOL uses the Bliss "dot" notation.32 Figure 3 shows some typical examples of this policy for various programming languages. Different operators must be represented by different names; thus integer and real addition are just as distinct from each other as are addition and multiplication. Individual control constructs are represented as individual tree nodes, not broken down into tests and jumps. This means that the control structures in TCOL vary somewhat depending on what language is being implemented. Array and record accesses, on the other hand, are broken down into their components; for instance, the address arithmetic necessary to access an array element, which may involve scaling the index and adding the address of the base of the array, is represented explicitly.

tical statements into one. It may also involve manipulating expressions and their values. If an optimization involves moving the evaluation of an expression, or getting rid of it in favor of reusing the value of an identical expression, a CCV- compiler-created variable- guaranteed to have a name distinct from those of all other variables must be used. When the expression is evaluated, its value is left in that variable, and when the value is to be used, it is read from that variable.
The most popular example of a data flow optimization is moving code out of loops. Consider this example of the initialization of an array:
for ifromlwbatoupbadoa[i] :=b + cod The value of the expression b + c does not change during the loop. It would be legal, therefore, to compute it once, before the loop begins, and store the computed value in some location (a CCV); each time around the loop, this location would be read and its contents stored into the next element of the array a. Thus, the above loop would be coded as if it had been written this way:
t:= b + c; for i from lwb a to upb a do a [i]: = t od
Here, t denotes the CCV. If a statement, rather than an expression, is involved in some optimization like this one, it is generally not necessary to use a CCV.

The code generation process: its phases and proposed organization

STATEMENT (SOURCE LANGUAGE)

TCOL TREE

Flow analysis. The FLOWAN, TNBIND, and FINAL phases all do some sort of data and control flow analysis of the program. The first of these, FLOWAN, detects optimizations involving code motion and elimination of redundant computations.
Crucial to the action of this phase, and later phases, is the construction of a control flow graph, a graph whose nodes represent basic blocks. A basic block8 is a program section without branches, except at its end, or labels, except at its beginning. In our representation, of course, a basic block is an ordered set of nodes from the program tree. Links in the graph are directed and represent possible transfers of control between basic blocks. This data structure is basic, in that it is common to almost all flow analysis systems'5; other structures will be built on top of it, including the execution list, a linked list of all the program tree nodes threaded in (approximate) execution
order. We will not try to catalogue all the optimizations that
are or will be done by this phase; instead, we refer the reader to other texts on optimization." 29 In general, an optimization discovered by this phase may involve moving the execution of a statement or combining two iden-

i-i (FORTRAN) i: =j (ALGOL 68) iS j (BLISS)
a=j ~(FORTRAN)
a: =j (ALGOL 68)
or
a: = (REAL) (j)

*This acronym is historically motivated; the last three letters are from the earlier acronym UNCOL,31 and the first letter stands for "tree."

Figure 3. Examples of TCOL trees.

SYMBOL
"i"
SYMBOL

DEREF
- SYMBOL
~ SYMB
FLOAT DEREF
SI
ITl

August 1980

41

Optimizations requiring CCVs are frequently unprofitable in situations where they happen to be feasible. Sometimes the expressions involved are so easy to compute that the savings of optimization are swamped by the cost of manipulating the CCVs. This may happen in any of three situations:
(1) The expression may cost nothing to compute, because it is an operand of another expression which "subsumes" it. For instapce, consider the expression a[x + 1], in which a is a static array and x is a variable. The "computation" of x + 1 has no cost, because a linker can avoid it by adding 1 to the address of a.
(2) The expression might be computed cheaply by using indirection or indexing on the target machine. An example of this is
a : = b [x] b[x] : = c
Here, it is feasible to compute the address b [x] in a CCV (by moving the contents of x into it and adding the address of b to it), but this is wasteful on almost any target machine. The address can be computed more cheaply by an indexing operation.
(3) An expression may be easy to compute because its value will not have to be saved, or used as an operand. For instance, in
if a > b then c else d fi
it is likely to be more costly to compute the value of a > b and save it in a CCV than to simply test it. Thus, even if the expression a > b appears at several points like this one in the program, but the values of a and b do not change from one point to the other, it may be wiser to compare the two values with each other at each point, rather than to compare them with each other only once, using the result of the comparison to set a boolean CCV and testing the CCV at each point.
The FLOWAN phase, which is entirely target machineindependent, does not make decisions about the desirability of feasible optimizations; decisions regarding desirability analysis are left to a subsequent phase.
The DELAY phases. The name "DELAY," a misnomer from ancient history, is given collectively to a group of phases which do source-to-source transformations of the program tree before the register allocation and code generation. With some exceptions, these transformations are target machine-dependent. Some of these phases do not, strictly speaking, transform the tree; they decorate each node-that is, associate semantic information with it. It is reasonable to expect that some of these phases could be run concurrently with each other. But for our research, clarity is more important than efficiency, and we have kept them separate.
Constant folding refers to three different tree transformations, each of which is simple and local in its effect:
(1) operations with only constant operands: for instance, 3 + 4 becomes 7;
(2) operations involving identities: x + 0 becomes x, and x * 0 becomes 0 (if x has no side effects); and

(3) address arithmetic: the address calculation in a[4,5] is done by the compiler, whether the array is static or on the run-time stack. We distinguish this from constant propagation, or using knowledge about what the contents of a variable will be at some particular point in the program, which requires flow analysis.
Context determination. The context determination phase writes in each tree node information, which will be needed in later phases, about how it is used as an operand. A good example of this was mentioned earlier in the discussion of flow optimizations: a reasonable decision about whether a boolean expression, such as a > b, should be computed and stored in a CCV depends on how it is used. If it must already be computed anyway, for instance to be stored in a boolean variable, the extra cost of storing it in a CCV is minimal. We distinguish four important contexts:
(1) statement or no-value context, which applies to statements, or, in expression languages, expressions whose values are not used;
(2) flow context, which applies to boolean expressions
which are only tested, not stored or used as operands; (3) operand context, which applies to ordinary
operands; and (4) address context, which applies to expressions
which are dereferenced or assigned. Further subcategories are also useful.
Three related techniques. Unary complement operator propagation, targeting, and evaluation order determination should be considered together. The first uses rules of arithmetic and logic to transform expressions s that the unary complement operators (negation of integers and reals, complementation of booleans, etc.) are evaluated as late, and therefore as seldom, as possible. For instance, -a* - bbecomesa* band(- a)+ (- b)becomes - (a+ b).
The second technique uses commutativity to avoid loading and storing operands. This is especially useful on two-address architectures; for instance, a : b + a becomes a : = a + b. It is also relevant on singleaccurnulator architectures: after 1 + b * c becomes b * c + 1, it is no longer necessary to store the value b * c out to memory after computing it, just to make room in the accumulator for "1." Unary complement operator propagation and targeting together are used to talke account of various target machine asymmetries. For instance, some machines have a "load negative" instruction and some do not; some have a "reverse subtract" instruction and some do not.
The third technique, evaluation order determination, has generally been considered part of targeting and even register allocation,'6 but it greatly simplifies matters to treat them all separately. Given a particular targeting, if code must be generated to evaluate both operands of an operator, the one which will be the destination operand (or the one which should be in the accumulator when the operation is performed on a single-accumulator machine) should be evaluated last. If no targeting is better than any other, on the other hand, the targeting is determined from

42 COMPUTER

the evaluation order; the choice of which operand to 6(SP)

if the whole expression, . . ( . x + 4), were a

evaluate first depends on which order requires the fewest

CCV loaded into a memory location six

registers and/or temporary memory locations. (See

bytes from the top of the stack.

Figure 4.) Note that, in the targeting examples given @SP
above, evaluation order was irrelevant because most

same as the above using the top of the
stack.

operands were simple constants or variables, which involve no code for evaluation (only code for loading and storing).

(SP) + R3

same as the above at the last reference to the CCV. if ..(.x + 4) were a CCV loaded into

AMD. The access mode determination phase takes maximum advantage of any special features of the operand address computations of the target machine. Many target architectures allow address computations that do more than simply indexing and indirection. A standard example is the S/370, which frequently allows indexing with two index registers. More recent examples are the DEC VAX- 1 1/78033 and the LLL S-134; these both allow indexing combined with (implicit) scaling of the index register and double indexing combined with indirection. For these architectures, the question of how much of the program calculation can be performed by the addressing hardware is nontrivial and is compounded by the possibility of using CCVs to assist with address computation. Even for the DEC PDP- I 1, with a simple addressing architecture allowing simple indexing and indirection (possibly combined), the use of CCVs makes the problem sufficiently difficult so that the simple algorithms presented in Wulf et al.' for solving it are sometimes inadequate.
The AMD phase assigns to each program tree node a

register R3.
This. phase is parameterized by a list of the possible operand access modes, AMs, each associated with the skeleton program subtree that it represents. The phase generator preprocesses the list so that, using a simple bottom-up tree walk, access modes can be associated with nodes without the use of pattern matching. This involves creation of something like a finite state machine, in which the "state" of a node depends on its operator and the states of its two operand nodes.
There are more DELAY phases than these, and we have plans for several different kinds of tree transformation phases. One of these is the desirability analysis phase mentioned earlier. There are also phases to take advantage of the associative and distributive laws of addition and multiplication; to determine the right coding for multiple assignments, such as a: = b: = c; and to perform miscellaneous transformations, such as changing multiplications by powers of two into shifts. A notation and support system for describing tree transformations, called Bonsai, is under development.

list of the ways in which it could be represented as an instruction operand. For instance, using the DEC PDP- 11 as a target machine, consider the Bliss expression

Register allocation. Register allocation includes a broad range of activities. Data items of all kinds, ranging from those which are very local (such as a register required

. .(.x + 4)For readers unfamiliar with Bliss, "." is the explicit dereferencing operator. Thus x is being used as a variable which holds a pointer to an array of pointers. .x is the value of the variable, that is, an address; . (. x + 4) is the value retrieved by an array access, that is, another address (one of the pointers); and the value of the whole expression, . . (. x + 4), is the contents of the location pointed to (an integer, perhaps). Suppose it appeared in an operand context, such as the source of an assignment. The assign-

to do a particular multiplication) to those which are very global (such as variables and CCVs) are associated with storage locations of all kinds, including accumulators, index registers, and so on. Our register allocation phase, called TNBIND for historical reasons, is divided into two subphases, temporary name assignment and packing, which occur just before the code selection phase. This organization is not so straightforward since, in order to know what the best allocation of registers is, it is necessary to know what code sequences will be used, and vice versa.

ment would eventually result in the generation of a MOV

instruction, of which the source operand could take any

of 10 forms:

@4(R3) if . x were loaded into register R3.

Straightforward evaluation order Better evaluation order using two temporaries (tl and t2): using one temporary (tl):

@0(R3) @(R3) +

if . x + 4 were a CCV loaded into register R3. same as the above, but this is the last

load a add b store to tl

load c multiply by d store to tl

reference to the CCV.

load c

load e

@6(SP)

if . (.x + 4) were loaded into a memory location six bytes distant from the top of
the run-time stack.

multiply by d store to t2 load e multiply by f

multiply by f add tl store to tl load a

@(SP) + same as the above, but the memory location is at the top of the stack, and this is

add t2 multiply by tl

add b multiply by tl

the last reference to the expression.

@R3 if .(.x + 4) were a CCV loaded into Figure 4. Evaluation order for (a + b*((c*) + (ef)) on a

register R3.

single-accumulator machine.

August 1980

43

Temporary names. The TN data structure is used for the TN assignment, packing, and code selection phases. TNs are records used to hold all the information about the results of register allocation that must be available to the code selector. They represent entities that must be allocated storage, such as variables or the results of expressions. Since a TN can correspond to only one storage location, it may be necessary to represent a single variable with more than one TN. TN assignment determines the correspondence between TNs and program entities; packing establishes the correspondence between TNs and target machine storage locations.
To determine where TNs must be attached to the program tree and what properties they must have, the TN assignment phase does a pseudo code selection, using the same library of code selection templates (described in the next section) as the real code selector. Consider the coding of a typical binary operator tree node. There may be several alternative instructions (or instruction sequences) which could be used for it, each with different restrictions on the locations of the operands and the location of the result. A TN -will be associated with the operator node, and there will be restrictions on where the TN can be packed so that at least one of the code sequences can maintain the result. An attempt will be made to assign this TN to the target operand (described above), but if there are different restrictions on that operand, it must be assigned a different TN. The other, nontarget operand will have its own TN. The relation between this TN and the original (operator node) TN is complex. Packing the latter may, in effect, choose ahead of time the course Figure 5. An interference graph. Each node represents a temporary name; links indicate overlapping lifetimes. Figure 6. A preference graph superimposed on an interference graph. Solid lines are conflict arcs; dashed lines are preference arcs, labeled with counts of the loads and stores involved.

which the code selector will follow at that node, and this choice affects the choice of locations for packing the former TN. In addition, the nontarget operand may be associated with another TN and, if it is likely to be useful, saved between the time it is computed and the time it is used. This is only necessary when there are severe restrictions on the range of locations where it can be computed-e.g., when the target machine has a very small number of accumulators.
For instance, consider integer addition on the DEC PDP- 10. There are a number of instructions which can be used to perform this, but if nothing is known about either operand value at compilation time, the number of available instructions is reduced to three. Each of the three instructions (ADD, ADDM, ADDB) has two operands; the first operand must be an accumulator, while the second may be an accumulator or a memory location. The first of the three instructions leaves the sum in the accumulator; the second leaves the result in the other operand; and the third leaves copies of the result in both operands. It is desirable to postpone the choice between these instructions until the real code selector runs; thus, when an addition in the program is to be implemented using one of them, the pseudo code selector selects a "combined" code sequence. It is assumed that both the target and nontarget TNs may be in either an accumulator or memory. However, this assumption is not quite right, and it is corrected by recording the following
relationship, between the two TNs: either one may be in
memory, but if both are in memory, an extra cost (to load the nontarget into an accumulator) will be incurred. This cost information is used in the later packing phase, as a factor in decisions about which TNs are assigned to accumulators and which are not.
During the pseudo code selection, this phase keeps track of how and how often each TN will be used in an operand, and this information later determines a TN importance ranking. A later subphase of TN assignment determines, through program flow analysis, the lifetime of each TN-that is, the set of program points or intervals during which it must have a location to itself. The data structure built from this information, by the packing phase, is an interference graph (see Figure 5). Each graph node represents a TN; a link between two nodes indicates that the two TNs have overlapping lifetimes (i.e., may not be allocated to the same location). In Figure 6 a preference graph is superimposed on the interference graph. Each link in the preference graph indicates that the two TNs "should" be allocated to the same location to avoid loads or stores. These links are labeled with counts of the loads and stores involved, possibly weighted to give more importance to those which occur in loops.
The most common use of preference links is in arithmetic expressions. Consider the coding of the expression
a *b + c on a target machine, such as the DEC PDP-1 1 or the IBM S/370, in which the restrictions on what locations may be used for addition are quite different from the restrictions on multiplication. (For instance, on the PDP-11, multiplication can only be done in an odd-numbered

44 COM PUTER

register, while addition can be done in any register or in memory.) Different TNs must be created to represent the results of the addition and the multiplication; especially if a, b, and c are complicated expressions rather than simple variables, it must be anticipated that both operations cannot be done in the same location. However, these two TNs are connected by a preference link-that is, an arc of the preference graph-labeled by the cost of moving data between the two locations, possibly weighted if the whole expression is within a loop.
Packing. The packing phase must satisfy the rule that no two TNs which are connected by an interference arc may be packed in the same storage location. In addition, it seeks to minimize four cost measures, each of which contributes to the cost (in code size or speed) of the object
program:
(1) Access cost, the sum of the costs of all necessary accesses to all TNs, is minimized if TNs which are used most frequently are packed in the storage locations which are easiest to access (e.g., fast registers).
(2) Code sequence cost is the cost of the instructions necessary to move operands into place to meet restrictions on them, as described earlier.
(3) Preference cost is the cost of loads and stores between TNs which are packed in different locations, as described by the preference arcs.
(4) Opening cost (if any) is the cost of setting up storage locations-for instance, saving and restoring a register or making room on the stack for one or more local variables. Packing is a particular kind of graph coloring problem. Hence, we do not expect to find an algorithm that is both fast and guaranteed to give optimal results, but are primarily concerned with heuristics similar to those described by Johnsson. 18

Figure 7a shows a template which can be used to implement integer addition. The code sequence consists of one of the PDP-10 instructions which implements integer addition; the tree pattern describes the semantics of that instruction. The tree is an assignment whose source is an addition; however, the pattern can be applied to any addition. The labels al and el describe the operands of the instruction. An operand labeled al must be a direct reference to an accumulator, while an operand labeled el uses the full PDP-10 effective address computation and so can either be in an accumulator or in memory and be addressed by indexing and/or indirection. The ADD instruction takes two operands, as shown, and leaves their sum in the first operand (the accumulator). Thus, in the tree pattern, both of the leaves labeled al refer to the same location (this is not clear from the diagram). During code selection, the operand labels al and el are compared with the operands of the addition in the program; for instance, if this template is to be used to implement some addition, the left operand of that addition must be already in, or brought into, an accumulator. (The actions of the register allocation phases have determined with certainty what location each operand is to be computed in.) Alternative templates to perform the same task can be compared with each other on this basis; that is, the one which requires the least loading of operands and storing of results can be chosen. On the PDP-1O, this means choosing between the ADD, ADDM, and ADDB instructions, as discussed above. However, templates whose pattern trees are of different sizes or shapes cannot be exactly compared with each other in this way; we will say more about this below.
The MD machine description representation is genealogically related to ISP.37'38 There are descriptions of the storage classes, instruction formats, effective address calculations, and the computations performed by each instruction on the machine. Given descriptions of

Code selection. The code selector is directly descended from the code generator described by Cattell.35 The difference is that it fits into the PQCC system; thus, register allocation and access mode determination are in separate phases. Generation of template libraries (see below) is done by a code generator-generator.36
The code selection phase is driven by a library of code selection templates. Each template consists of a subtree pattern and a code sequence. Examples of templates for the PDP-10 are given in Figure 7. Briefly, if a subtree of the program tree matches the subtree pattern in the obvious way, and various other conditions (not shown) are met, the template is instantiated-that is, the instructions given in the code sequence are emitted, their operands being determined from the values of the pattern variables (the program tree nodes which matched the leaves of the subtree). The template library, organized for fast retrieval, is similar to those mentioned by Johnson28 and Ripken.26 It is constructed by a preprocessor GEN (the code generator-generator) from a more primitive machine description, called the MD table. *

TREE PATTERN
(a)

al: +

al: el:

(b)

1 "%|TEST

(TRUE)

/1:

AND
a/: el:
al: el:

CONSTANT
0I
O

CODE SEQUENCE
ADD al, el
TDNN al, el JRST /1

*MD stands for machine description; in Cattell35 this is called the MOP Figure 7. Simplified code generation templates for the table. PDP-1 0.

August 1980

45

the instructions in a TCOL-like notation, GEN finds instruction sequences for evaluation of TCOL trees, using heuristic search methods borrowed from artificial intelligence systems. This is too slow to be used in the compiler itself; instead, GEN is used to create code generation templates for a predetermined set of "interesting" subtrees. This set is generated using a number of rules. Two rules in particular serve to ensure that some kind of code can be generated for any program:
(1) There is one template for every TCOL operator. Its pattern tree is simple enough so that it is always applicable wherever that operator occurs. The example in Figure 7a is one of these. (More than one template may be useful if different code is to be generated in different contexts.)
(2) There is one template for every possible data move (load or store); that is, if there are n different kinds of storage on the target machine, there are n2 templates for generating code to move a data item from one type of storage to another. In addition, for every instruction there is at least one template in which the pattern tree describes the effects of that instruction (Figure 7b). Other rules encourage the discovery of "clever" code sequences. However, there can be no guarantee that these heuristics will find all interesting cases, and the number of such sequences that are ultimately represented by templates depends on how long the template search is allowed to run. Templates are also generated for TCOL operators (such as control constructs) that do not appear in the MD table, or for operators (such as exclusive OR) that do not appear in every machine's instruction set, and to take account of information about the run-time environment of object programs.
All templates make some requirements on the forms of their operands. The AMs associated with them by the AMD phase must include some which are suitable in the instruction operands in the template's code sequence. However, the pattern match with a template does not fail if the operands do not meet these requirements; instead, load or store code is generated so that they do meet them. This is the use of the fetch decomposition rule; the corollary store decomposition rule is used when the pattern imposes requirements on the destination of the principal operator. The top node in a pattern tree for matching nodes in operand context is always assignment (": = "), but due to fetch and store decomposition, an actual assignment need not be present in the program tree in order to match such a pattern. In the TN assignment phase, the use of fetch or store decomposition causes a new local TN to be created and associated with the appropriate node; in the code generation phase, the TN is used as the destination of the load or store code that is generated.
Figure 7b is a pattern for one special case of testing for equality. There is a requirement not only on its right operand (it must be a constant of zero value) but also on the operator of the node which is its left operand. For such special-case patterns to be used, the library must be sorted to test them before testing the general-case patterns. (Of course, several patterns of equal generality may match, in which case the "best" one is estimated.) In ad-

dition, and most importantly, the order in which the program tree nodes are treated must be top-down. In this example, for instance, the code generator must not reach the AND node before reaching the "= " node-otherwise, code would be generated for the logical AND operation, and the special case would no longer be applicable. This reverse execution-order traversal of the program automatically subsumes a lot of the special-case testing characteristic of good code generators.
The code generation templates described by Ripken26 are very close, in their design and in their overall approach to the problem of target machine independence, to the templates we have described here. The principal difference is in the control structure of the code generation algorithm itself. Ripken's algorithm first walks the tree from the bottom up. This phase addresses the problem, described earlier, of multiple templates for the same tree pattern. All the templates applicable to a given node are recorded (that is, their costs are recorded, though no code is generated). The next phase uses the cost information to allocate registers locally, walking back down the tree and choosing code sequences. This has the flavor of a dynamic programming algorithm; while there is no provision for any interaction between local and global register allocation, the local allocation may be optimal for a given global allocation because of the exhaustive enumeration of possible code sequences in the first (bottom-up) phase. (Optimality may be lost in some circumstances because of some simplifications in the cost recording mechanism, but in the usual cases it is achieved.) This kind of algorithm, also presented by Aho and Johnson,39 has considerable promise, and it would be desirable to integrate it into a system such as ours in which there is interaction between local and global register allocation.
The FINAL phase. The output of the code generation phase is a doubly linked list of intermixed instructions and labels. These are in a format that retains only minimal target machine independence. The instruction operands are lists of actual parameters for the subtree skeletons of the AMs; these are constants or references to storage locations. In principle the effect of any instruction could be reconstructed, forming a TCOL tree, from the MD table entry for it and from its operands. The reason for this is that further optimizations are done on the program while it is in this format.
Notwithstanding previous arguments for the use of tree-structured intermediate representations, some optimizations are more easily done on the final linear form of the object code. These may arise from any of several circumstances:
(1) Instructions may have effects other than those for which they were emitted. For instance, on a machine with condition codes, an instruction which was emitted by the code generator because it performs some arithmetic operation may, incidentally, set the condition codes. Instructions whose sole purpose is to set the condition codes can sometimes be subsumed in instructions which accidentally set them. It is not desirable for the code generation phase to "know" all the effects, or indeed any of the effects, of the instructions it emits; this knowledge is more appropriately delegated to a separate phase.

46 COM PUTER

(2) Reverse-hoisting code motion (referred to as crossjumping by Wulf et al. ') that was not evident at the TCOL level may become evident at the instruction level. This is because operations which are single nodes in the TCOL tree (or single instructions'in triple or quadruple program representations) may produce multiple instructions, and frequently the cleanup instruction sequences for nonidentical TCOL nodes may be identical. This is especially true for operations such as subroutine call's, which are likely to involve highly stereotyped instructions-e.g., instructions to cut back the stack-to maintain the run-time environment.
(3) The code generator is more intellectually manageable if it does not have to be too careful about the senses and destinations of conditional and unconditional branch instructions. That is, it may emit a branch instruction whose destination is another (unconditional) branch instruction, or a branch whose destination is the next instruction, or a conditional branch which only skips over an unconditional branch-code that' is correct but wasteful in obvious ways. It is easiest to leave cleanup of glitches like this to a conceptually separate phase.
(4) Some optimizations can only be performed with a detailed knowledge, not only of the effects of instructions but of their sizes as well. A classic example of this is the resolution of short and long branch instructions in the PDP-l 1. 1,40 A branch instruction-can take two forms: the short form if its destination is known at assembly time and is within 256 bytes of the origin; otherwise, the long (costlier) form. The algorithm determining the form for each branch must consider the number of bytes required by each instruction, including, of course, the branch instruction itself and other branch instructions.
FINAL performs some optimizations that are usually called "peephole" optimizations,41 However, since the whole list of instructions is kept in core, that term is inappropriate; in fact, for any optimizing'transformation which involves two or more related intructions, FINAL allows many intervening instructions, provided that they are irrelevant to the optimization.
Parts of FINAL present nontrivial problems for parameterizing target machine dependence. These are primarily problems of dealing with the peephole optimizations: generating their library, determining their applicability conditions, and finding the conditions under which they improve, rather than worsen, code. We have chosen to allow the compiler builder to "suggest" optimizing transformations; this is reasonable if there are
not too many of them. (The current Bliss- II compiler' uses about 30.) A program, the legality checker, determines the conditions under which a transformation is correct-i.e., it lists a set of restrictions on the instruction operands and other parameters to the transformation pattern, such that it does not change the meaning of the program. Experience has shown that the human compiler writer is very likely to do this task incorrectly. Sometimes there are errors due to neglect of special cases, or even outright misunderstandings of the effect of the transfor-
mation; sometimes there are clerical errors. The last component of the parameterization is a cost function that, given an instruction in FINAL format, returns some

suitable measure of its cost (for example, its size in bytes or its execution time in cycles). This function is easily derivable from information in the MD table. Whenever a peephole transformation is found to be applicable at some point in a program, the costs of the instruction sequences it creates and destroys are compared to determine whether it should in fact be applied.
Summary and evaluation
Readers familiar with the optimiztion literature will undoubtedly think of a number of optimization techniques not mentioned in the previous pages. In most cases, we are interested in these techniques, but simply have not integrated them into our framework at this writing. These include strength reduction, removal of "tail recursion," ordering of the basic blocks to minimize branches from one block to another, and various interprocedural optimizations and analyses.
The thorniest problems have been in the design of a truly general model of code selection.
At this writing, many phases of the skeleton compiler have been implemented and tested; some are close to final form. In particular, some of the DELAY phases were developed quickly, for the target machine-dependent data which they use are easily characterized. The thorniest problems have been in the design of a truly general model of code selection; thus, design of the register allocation and code generation phases has been slow, and implementation and testing are not complete. A preliminary version of the phase generator for code generation was designed and implemented by Cattell35; two other phase generators, for FLOWAN and for the tree transformation phases of DELAY, are being developed. Work on the phase generators in general, however, has taken a back seat to the task of determining how the phases themselves ought to work (and, from this, what data they require from the phase generators). The phases and- their generators, however, are only part of the complete PQCC system. The parts of the system which a programmer must use in building a compiler must be defined precisely and documented fully. These include the semantics of TCOL, the Linear Graph Notation syntax, and the rules for constructing MD tables. We have had some success with teaching uninitiated programmers how to use these.
Historically, there has been a conflict between the need for optimization in compilers, and the need for compilers which are easily retargetable (modifiable to produce code for different target machines). The most successful retargetable compilers have been those with minimal processing in code generation, while the most successful optimizing compilers have been designed around particular target machines. A compiler can achieve both goals only
if it relies onf optimization and code generation algorithms
which are themselves easily retargetable. Suppose that

August 1980

47

some optimization technique achieves a 15 percent saving in object code size for some target machine. That technique is too successful, too important to be left in the hands of the person whose task is to bring up the compiler on a new machine; the compiler code which performs the optimization must be usable, without rewriting by that person, for any target machine.
Common practice reflects the belief that only a few optimization algorithms, such as those for data flow analysis, are easily retargetable. (The compiler organization given by Gyllstrom42 is a recent example of this.) On the contrary, it has been our experience that nearly all the important techniques can be cast in an easily retargetable form. Especially important examples from the previous sections include code selection using a library of templates, register allocation (in particular, packing), and many of the post-optimizations performed in the FINAL phase. The important lesson of the PQCC effort is that retargetability and a high level of optimization are not incompatible. E
Acknowledgments
In addition to the authors, a number of people have contributed-some substantially--to the PQCC project. Gary Feldman and Paul Hilfinger, who are primarily responsible for the success of the LGN design and the support package, deserve special mention. Among the other participants are (in alphabetical order) Mario Barbacci, Ron Brender, Ben Brosgol, Steve Byrne, Reidar Conradi, Frank DeRemer, David Dill, Peter Hibbard, Andy Hisgen, Regis Hoffman, John Nestor, David Stryker, Lee Szewerenko, Franco Turini, and Gideon Yuval.
This research was sponsored by the Defense Advanced Research Projects Agency (DoD), ARPA Order No. 3597, monitored by the Air Force Avionics Laboratory under contract F33615-78-C-1551. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the US Government.
References
1. W. Wulf, R. K. Johnsson, C. B. Weinstock, S. 0. Hobbs, and C. M. Geschke, The Design of an Optimizing Compiler, Elsevier-North Holland, New York, 1975.
2. B. A. Wichmann, "Basic Statement Times for Algol 60," Tech. Report NAC 42, National Physical Laboratory, Teddington, Middlesex, U.K., 1973.
3. B. A. Wichmann, "How to Call Procedures, or Second Thoughts on Ackermann's Function," Software-Practice and Experience, Vol. 7, June-July 1977, pp. 317-329.
4. W. Wulf, P. Feiler, J. Zinnikas, and R. Brender, "A Quantitative Technique for Comparing the Quality of Compiler Code Generation," Tech. Report, Carnegie-Mellon Univ., Pittsburgh, Pa., (to be published).
5. S. C. Johnson and M.E. Lesk, "Language Development Tools," Bell System Technical J., Vol. 57, No. 6, JulyAug. 1978, pp. 255-2175.
6. M. Elson and S. T. Rake, "Code-generation Technique for Large-language Compilers," IBM Systems J., Vol. 9, No. 3, 1970, pp. 166-188.

7. R. G. G. Catell, "A Survey and Critique of Some Models of Code Generation," Tech. Report, Carnegie-Mellon University, Pittsburgh, Pa., 1977.
8. J. W. Backus et al., "The FORTRAN Automatic Coding System," Proc. Western Joint Computer Conf., AFIPS, Feb. 1957, pp. 188-198. Also in Programming Systems and Languages, S. Rosen, ed., McGraw-Hill, 1967, pp. 29-47.
9. E. S. Lowry and C. W. Medlock, "Object Code Optimization," Comm. ACM, Vol. 12, No. 1, Jan. 1969, pp. 13-22.
10. F. E. Allen, "Annotated Bibliography of Selected Papers on Programl Optimization," Tech. Report RC 5889, IBM Thomas J. Watson Research Center, Yorktown Heights, N.Y., Mar. 1976.
11. F. E. Allen and J. Cocke, "A Catalogue of Optimizing Transformations, " Design and Optimization ofCompilers (Courant Computer Symposium 5), Prentice-Hall, Englewood Cliffs, N.J., 1972, pp. 1-30.
12. T. A. Standish, D. C. Harriman, D. F. Kibler, and J. M. Neighbors, "The Irvine Program Transformation Catalogue," Univ. of Calif., Irvine, Dept. of Information and Computer Science, 1976.
13. D. B. Loveman, "Program Improvement by Source-toSource Transformation," J. ACM, Vol. 24, No. 1, Jan. 1977, pp. 121-145.
14. C. M. Geschke, Global Program Optimizations, PhD dissertation, Carnegie-Mellon Univ., Pittsburgh, Pa., Oct. 1972.
15. F. E. Allen and J. Cocke, "A Program Data Flow Analysis Procedure," Comm. ACM, Vol. 19, No. 3, Mar. 1976, pp. 137-147.
16. J. Bruno and R. Sethi, "Code Generation for a OneRegister Machine," J. A CM, Vol. 23, No. 3, July 1976, pp. 502-510.
17. W. H. E. Day, "Compiler Assignment of Data Items to Registers," IBM Systems J., Vol. 9, No. 4, 1970, pp. 281-317.
18. R. K. Johnsson, An Approach to Global Register Allocation, PhD dissertation, Carnegie-Mellon Univ. Pittsburgh, Pa., Dec. 1975.
19. A. Snyder, "A Portable Compiler for the Language C," master's thesis, MIT, Cambridge, Mass., May 1975.
20. F. E. Allen et al., "The Experimental Compiling Systems Project," Tech. Report RC 6718, IBM Thomas J. Watson Research Center, Yorktown Heights, N.Y., Sept. 1977.
21. W. Harrison, "A New Strategy for Code Generation-the General Purpose Optimizing Compiler," Fourth ACM Symp. Principles ofProgramming Languages, SIGPLANSIGACT, Jan. 1977, pp. 29-37.
22. J. L. Carter, "A Case Study of a New Code Generation Technique for Compilers," Comm. ACM, Vol. 20, No. 12, Dec. 1977, pp. 914-920.
23. Nils J. Nilsson, "Artificial Intelligence," Tech. Report 89, Stanford Research Institute, Artificial Intelligence Center, Menlo Park, Calif., Apr. 1974. Also in IFIP Congress Proc., (invited paper), Stockholm, Sweden, Aug., 1974.
24. K. Ripken, "Generating an Intermediate-code Generator in a Compiler-Writing System," International Computing Symp., Elsevier-North Holland, New York, 1975, pp. 121-127.
25. H. Ganzinger, K. Ripken, and R. Wilhelm, "Automatic Generation of Optimizing Multipass Compilers," Information Processing 77, Elsevier-North Holland, New York, 1977, pp. 535-540.
26. Knut Ripken, Formale Beschreibung von Maschinen, Implementierungen und optimierender Maschinecode-erzeugung aus attributierten Programmgraphen, (in German) PhD dissertation, Technische Universitat Munchen, July 1977.
27. S. C. Johnson and D. M. Ritchie, "Portability of C Programs and the UNIX System," Bell System Technical J., Vol. 57, No. 6, July-Aug. 1978, pp. 2021.

48 COM PUTER

28. S. C. Johnson, "A Portable Compiler: Theory and Practice," Fifth ACM Symp. Principles of Programming Languages, SIGPLAN-SIGACT, Jan. 1978, pp. 97-104.

29. B. W. Leverett, R. G. G. Cattell, S. 0. Hobbs, J. M. New-

comer, A. H. Reiner, B. R. Schatz, and W. A. Wulf, "An

Overview Project,"

oCafrtnheegiPer-oMdeulcltoinonbJnQiuva.l,iPtiyttCsobmupriglhe,rP-aC.o,mp1i97l8e.r

30. S. Warshall and R. M. Shapiro, "A General-Purpose Table-Driven Compiler," Proc. Spring Joint Computer Conf., AFIPS, 1964, pp. 59-65. Also in Programming Systems andLanguages, S. Rosen, ed., McGraw-Hill, New York, 1967, pp. 332-341.

31. T. B. Steel, "A First Version of UNCOL," Proc. Western Joint Computer Conf., AFIPS, 1961, pp. 371-377.

32. W. A. Wulf, D. B. Russell, and A. N. Habermann, "BLISS: a Language for Systems Programming," Comm. ACM, Vol. 14, No. 12, Dec. 1971, pp. 780-790.

33. Digital Equipment, VAX-11/78OArchitectureHandbook, - Digital Equipment Corp., Maynard, Mass., 1977.

34. B. Hailparn and B. Hitson, S-l Processor Manual, Lawrence Livermore Laboratories, Livermore, Calif., 1978.

35. R. G. G. Cattell, Formalization andAutomaticDerivation of Code Generators, PhD dissertation, Carnegie-Mellon Univ., Pittsburgh, Pa., Apr. 1978.

36. R. G. G. Catell, J. M. Newcomer, and B. W. Leverett, "Code Generation in a Machine-Independent Compiler," SIGPLAN Conf. Compiler Construction, ACM, 1979, pp. 65-75.

37. C. G. Bell and A. Newell, Computer Structures: Readings and Examples, McGraw-Hill, New York, 1971.

38. R. G. G. Cattell, "Using Machine Descriptions for Automatic Generation of Code Generators," Proc. 3rd Jerusalem Conf. Information Technology, Elsevier-North Holland, New York, 1978, pp. 503-507.

39. A. V. Aho and S. C. Johnson, "Optimal Code Generation for Expression Trees," J. ACM, Vol. 23, No. 3, Aug. 1976, pp. 488-501.

40. T. G. Szymanski, "Assembling Code for Machines with Span-Dependent Instructions," Comm. ACM, Vol. 21., No. 4, Apr. 1978, pp. 300-308.

41. W. McKeeman, "PeepholeOptimization," Comm. ACM, Vol. 8, No. 7, July 1965, pp. 443-444.

42. H. C. Gyllstrom, R. C. Knippel, L. C. Ragland, and K. E. Spackman, "The Universal Compiling System," SIGPLANNotices, Vol. 14, No. 12, Dec. 1979, pp. 64-70.

Bruce W. Leverett is a doctoral student in the Department of Computer Science at Carnegie-Mellon University. His research interests include programming language design and implementation, and in particular, compiling techniques. He participated in the Bliss-l optimizing compiler implementation effort from 1973 to 1976, in the Carnegie-Mellon multiprocessor implementation of Algol 68 from 1974 to 1977, and in the PQCC project from 1976 to the present. He holds an AB from Harvard in physics and chemistry. R. G. G. CatteH is a member of the Computer Science Laboratory at Xerox Palo Alto Research Center. His research work has been in the areas of code generation and machine description languages, and more recently in data base systems and semantic data models. Cattell received his PhD in computer science from CarnegieMellon University in 1978 and his BS from the University of Illinois in 1974.

Steven 0. Hobbs is a software engineer with the Technical Languages group at
Digital Equipment Corporation. He is
concurrently completing his PhD at ' Carnegie-Mellon University, where his
research interests center around optimiz-
ing compilers. He holds memberships in the ACM and the IEEE. Hobbs received a BA degree in mathematics from Dartmouth College in 1969 and an MA degree in mathematics from the University of Michigan in 1972.

m

Joseph, M. Newcomer is a research SacsiseofciicaetaetiCnatrhneegDieep-aMretlmleonntUnoifveCrosmitpyu.tHeer

received his PhD from CMU in 1975; his

dissertation was in the area of automation

of construction of optimizing compilers.

From 1975 to 1977 he was associated with

the C.mmp/Hydra project and from 1978

to the present with the PQCC project.

Andrew Reiner is a graduate student in the
Computer Science Department at Carnegie-Mellon University. His interests in-
clude compilers, programming languages,
and human factors considerations in computer systems. He completed his
undergraduate work in mathematics and
computer,science at Alfred University and
CBaSrndeeggrieee-MferlolmonCUMniUverinsi1t9y7,9.receiving his

Bruce R. Schatz is a member of the

technical staff in the Customer Systems

Laboratory at ies, Holmrdel,

BNelelwTeJleerpsehyo,newhLearboerahteori-s

g working on software design for business communications products. Previously he

was a graduate student in the Computer

Science Department at Carnegie-Mellon

University, where he held a National

Science Foundation fellowship. He re-

ceived a BA from Rice University in 1975 and an MS from MIT

in 1977.

William A. Wulf is a professor of computer science at Carnegie-Mellon University. Prior to joining CMU in 1968, he was an instructor of applied mathematics and computer science at the University of Virginia. His research interests span the fields traditionally called "programming
_-5<_ systems" and "computer architecture." He is especially interested in the construc-
tion of large systems, notably compilers and operating systems, and in the way the construction of these systems interacts with the architecture of the machine on which they run.
Wulf holds the BS in physics and the MSEE from the University of Illinois and the PhD from the University of Virginia. The author of over 70 papers and technical reports, he serves on the editorial board of the IEEE Transactions on Software Engineering, Acta Informatica, and ACM Transactions on Programming Languages and Systems.

August 1980

49

