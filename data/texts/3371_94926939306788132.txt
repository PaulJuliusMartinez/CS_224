An Information-Theoretic Definition of Similarity

Dekang Lin Department of Computer Science
University of Manitoba Winnipeg, Manitoba, Canada R3T 2N2

Abstract
Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.
1 Introduction
Similarity is a fundamental and widely used concept. Many similarity measures have been proposed, such as information content [Resnik, 1995b], mutual information [Hindle, 1990], Dice coefficient [Frakes and Baeza-Yates, 1992], cosine coefficient [Frakes and Baeza-Yates, 1992], distance-based measurements [Lee et al., 1989; Rada et al., 1989], and feature contrast model [Tversky, 1977]. McGill etc. surveyed and compared 67 similarity measures used in information retrieval [McGill et al., 1979].
A problem with previous similarity measures is that each of them is tied to a particular application or assumes a particular domain model. For example, distance-based measures of concept similarity (e.g., [Lee et al., 1989; Rada et al., 1989]) assume that the domain is represented in a network. If a collection of documents is not represented as a network, the distance-based measures do not apply. The Dice and cosine coefficients are applicable only when the objects are represented as numerical feature vectors.
Another problem with the previous similarity measures is that their underlying assumptions are often not explicitly stated. Without knowing those assumptions, it is impossible to make theoretical arguments for or against any par-

ticular measure. Almost all of the comparisons and evaluations of previous similarity measures have been based on empirical results.
This paper presents a definition of similarity that achieves two goals:
Universality: We define similarity in informationtheoretic terms. It is applicable as long as the domain has a probabilistic model. Since probability theory can be integrated with many kinds of knowledge representations, such as first order logic [Bacchus, 1988] and semantic networks [Pearl, 1988], our definition of similarity can be applied to many different domains where very different similarity measures had previously been proposed. Moreover, the universality of the definition also allows the measure to be used in domains where no similarity measure has previously been proposed, such as the similarity between ordinal values.
Theoretical Justification: The similarity measure is not defined directly by a formula. Rather, it is derived from a set of assumptions about similarity. In other words, if the assumptions are deemed reasonable, the similarity measure necessarily follows.
The remainder of this paper is organized as follows. The next section presents the derivation of a similarity measure from a set of assumptions about similarity. Sections 3 through 6 demonstrate the universality of our proposal by applying it to different domains. The properties of different similarity measures are compared in Section 7.
2 Definition of Similarity
Since our goal is to provide a formal definition of the intuitive concept of similarity, we first clarify our intuitions about similarity.

Intuition 1: The similarity between A and B is related to their commonality. The more commonality they share, the more similar they are.
Intuition 2: The similarity between A and B is related to the differences between them. The more differences they have, the less similar they are.
Intuition 3: The maximum similarity between A and B is reached when A and B are identical, no matter how much commonality they share.

Our goal is to arrive at a definition of similarity that captures the above intuitions. However, there are many alternative ways to define similarity that would be consistent with the intuitions. In this section, we first make a set of additional assumptions about similarity that we believe to be reasonable. A similarity measure can then be derived from those assumptions.

In order to capture the intuition that the similarity of two objects are related to their commonality, we need a measure of commonality. Our first assumption is:

Assumption 1: The commonality between A and B is mea-

sured by

 ¢¡ ¤¡ ¦£ ¥¨§ © ©

¡ ¦£ ¥§ © common

where common

is a pro ¢po¡ s©ition that states the com-

monalities between A and B;  is the amount of infor-

mation contained in a proposition .

For example, if A is an orange and B is an apple. The

proposition that states the commonality between A and B

is "fruit(A) and fruit(B)". In information theory [Cover and

Thomas, 1991], the information contained in a statement

is measured by the negative logarithm of the probability of

the  s¡ tatement.¡ T£¦h¥e§ re© f© o!re,#"%'$ &)( ¡ ¡ £0©

¤¡ §©©

common

fruit and fruit

We also need a measure of the differences between two objects. Since knowing both the commonalities and the differences between A and B means knowing what A and B are, we assume:

Assumption 2: The differences between A and B is mea-

sured by¢  ¡

¡¤1£ ¥ § © ©23 ¢¡

¤¡ 1£ ¥ § ©©

descriptio¡ ¦£ n ¥ § ©

common

where description

is a proposition that describes

what A and B are.

Intuition 1 and 2 state that the similarity between two objects are related to their commonalities and differences. We assume that commonalities and differences are the only factors.

Assu¡¤£¦m¨¥ p§ ti© on 3: The similarity between A and B,
sim , is a function of their commonalities and dif-

fere¤¡n£¦ce¨¥ s§. ©Th5 at64 is¡¤ ¢, ¡

¡ £¦¥§© ©7¥ ¢  ¡

¡¤£1¥ § ©©©

sim 4
The domain of

isco8 m¤¡ 9@m¥ ABoD© nC F9 HE IG ¥ QA RP dGIe¥ sAQcrEHipt@9 ioS n.

Intuition 3 states that the similarity measure reaches a constant maximum when the two objects are identical. We assume the constant is 1.

Assumption 4: The similarity between a pair of identical objects is 1.

When A and B are identical, knowin g¡ their com¡m¦£ o¥¨n§ a©li©)tie s

m¢  ¡ eans knowin¡¤£1g ¥w§ h©a© t they are, i.e., common4

description
the property: T

9URP GV¥¨. 64 T¤¡ h9@e¥r9WefX© o!re,Y the function
.

must have

When there is no commonality between A and B, we assume their similarity is 0, no matter how different they are.
For example, the similarity between "depth-first search"
and "leather sofa" is neither higher nor lower than the similarity between "rectangle" and "interest rate".
Assumption 5: T A`PaGIb¥ 64 ¡¤VG ¥IA ©cdG .

Suppose two objects A and B can be viewed from two independent perspectives. Their similarity can be computed separately from each perspective. For example, the similarity between two documents can be calculated by comparing the sets of words in the documents or by comparing their stylistic parameter values, such as average word length, average sentence length, average number of verbs per sentence, etc. We assume that the overall similarity of the two documents is a weighted average of their similarities computed from different perspectives. The weights are the amounts of information in the descriptions. In other words, we make the following assumption:

AssuT m9fe0p7thgi7o  nAib ep646¥:9 u¡ f9¦q epgH¥ AiAretq ©@vis 467¡uW9be bvw 64 9¢u¡ xq9 ¥ 'q Ay¥Xe 'A Dqwv © A'qDX© 

From the above assumptions, we can proved the following

theorem:

Similarity Theorem: The similarity between A and B is

measured by the ratio between the amount of information

needed to state the commonality of A and B and the infor-

mation needed to fully describe what A and B are:

¡£¦¥¨§©
sim

" r$ &)( ¡

¡¦£ ¥§© ©

" r$ & ()¡ common ¡¤¦£ ¨¥ §©©

description

Proof64: ¡u9¥ AI©

 46¡u9hv GI¥ 9 v ¡¤A ¢9 ©©

 

# 

46u¡ 9@¥ ¢9 c© v Y v  W 



 GW  

46¡ GI¥ A1¢9 ©
(Assumption 6) (Assumption 4 and 5)

Q.E.D.

Since similarity is the ratio between the amount of information in the commonality and the amount of information in the description of the two objects, if we know the commonality of the two objects, their similarity tells us how much more information is needed to determine what these two objects are.
In the next 4 sections, we demonstrate how the above definition can be applied in different domains.
3 Similarity between Ordinal Values

probability

50%

10% 5%

20% 15%

excellent good average bad awful

Quality

Many features have ordinal values. For example, the "quality" attribute can take one of the following values "excellent", "good", "average", "bad", or "awful". None of the previous definitions of similarity provides a measure for the similarity between two ordinal values. We now show how our definition can be applied here.

If "the quality of X is excellent" and "the quality of Y is average", the maximally specific statement that can be said of both X and Y is that "the quality of X and Y are between "average" and "excellent". Therefore, the commonality between two ordinal values is the interval delimited by them.

Suppose the distribution of the "quality" attribute is known

(Figure 1). The following are four examples of similarity

calculations:
¡ 9 ¥ y6© ¡  £¢¤¦  §¥ ¥¡ ¦¨©   
sim
 ¡ ¥ p©X  BY9`9 ¦a Yb  
sim
 ¡ 9 ¥ p©6¡  £¢¤¦  ¥§¥¡¦  ¨ © BY9` ¦0a bY   
sim
 ¡X V¥EscYi©
sim


q  "!$&# %(' 2' 31 1('546¡97 8AB@ "@ CED  "!$&# %(' '2131F'04) G6 D  "!£#&H% 8A"@ B@ CED q  e   "!I%F¤P 0Q )PER P¤Q P"D

 e d G B!SP¤Q EP R  "! IP Q 5P

$3T WU V

q  "!$#&%H8A@"@BcC 7cd Ae '"f Ed A8 '5D

 "$! #&%(cd Ae '5fEEd A8 5' D  "£! #g%H8A@B@"CED

q e  "!I%FP¤Q P P¤Q BR P"D

e  d G B!SP¤Q P  "!PIQ RhP

$T iWp

q  "$! #&(% ' 2' 13F1 ' 46¡97 8AB@ "@ cC 7 cd eA'5Ef Ed 8A5' D  B!$#g(% ' 02' )131F'5W4 G6 D  B!q&# (% cd eA'f"dEA8 5' D q  e   "I! F% P¤Q0EP ) R P¤Q P ¤P Q RBP"D

  dG B!S¤P Q PER  "! IP Q RhP

$rT WV i

q   "!$#&H% A8 B@ "@ cC 7 cd Ae '5Ef Ed 8A'72t Ad CED

 "!S&# H% A8 "@ @BEC D  "$! #&(% 2t Ad CED

q e  q "I! %FP¤Q P P¤Q BR P Q hPED

e    q  G rY Y B! PIQ 5P  "!P¤Q BP

$T

The results show that, given the probability distribution in Figure 1, the similarity between "excellent" and "good" is much higher than the similarity between "good" and "average"; the similarity between "excellent" and "average" is much higher than the similarity between "good" and "bad".

4 Feature Vectors
Feature vectors are one of the simplest and most commonly used forms of knowledge representation, especially in casebased reasoning [Aha et al., 1991; Stanfill and Waltz, 1986] and machine learning. Weights are often assigned to features to account for the fact that the dissimilarity caused by more important features is greater than the dissimilarity

Figure 1: Example Distribution of Ordinal Values

caused by less important features. The assignment of the weight parameters is generally heuristic in nature in previous approaches. Our definition of similarity provides a more principled approach, as demonstrated in the following case study.

4.1 String Similarity--A case study

Consider the task of retrieving from a word list the words that are derived from the same root as a given word. For example, given the word "eloquently", our objective is to retrieve the other related words such as "ineloquent", "ineloquently", "eloquent", and "eloquence". To do so, assuming that a morphological analyzer is not available, one can define a similarity measure between two strings and rank the words in the word list in descending order of their similarity to the given word. The similarity measure should be such that words derived from the same root as the given word should appear early in the ranking.

We experimented with three similarity measures. The first

one is defined as follows:
u¡ 9¥ IA X© 
simedit

Y Y v ¡u9¥IA ©
editDist

¡¤@9 ¥ BA ©
where editDist is the minimum number of character

insertion and deletion operations needed to transform one

string to the other.

The second similarity measure is based on the number of

different trigrams in the two strings:

¡¤9@¥AIX© 
simtri

Y

Y dv C ¤¡ 9¢D© CD5v C u¡ IA © CxuV
tri tri



C ¡uW9 ©wv
tri

¡¤AI© C
tri

¤¡ 9¢© 9

where tri tri(eloquent)

is
=8

the elo,

set loq,

of trigrams iSn
oqu, que, ent .

.

For example,

Rank 1 2 3 4 5 6 7 8 9 10

Table 1: Top-10 Most Similar Words to "grandiloquent'

simedit

simtri

sim

grandiloquently 1/3 grandiloquently 1/2 grandiloquently

grandiloquence 1/4 grandiloquence 1/4 grandiloquence

magniloquent 1/6 eloquent

1/8 eloquent

gradient

1/6 grand

1/9 magniloquent

grandaunt

1/7 grande

1/10 ineloquent

gradients

1/7 rand

1/10 eloquently

grandiose

1/7 magniloquent 1/10 ineloquently

diluent

1/7 ineloquent

1/10 magniloquence

ineloquent

1/8 grands

1/10 eloquence

grandson

1/8 eloquently

1/10 ventriloquy

0.92 0.89 0.61 0.59 0.55 0.55 0.50 0.50 0.50 0.42

Root agog cardi circum gress loqu

Table 2: Evaluation of String Similarity Measures

Meaning leader, leading, bring

 C 7CEf @B@h6
23

11-point average precisions simedit simtri sim 37% 40% 70%

heart

56 18% 21% 47%

around, surrounding 58

24% 19% 68%

to step, to walk, to go 84

22% 31% 52%

to speak

39 19% 20% 57%

The third similarity measure is based on our proposed defi-

nition of similarity under the assumption that the probabil-

ity of a trigram occurring in a word is independent of other

trigrams in the word:

¢¡ ¤£ ¦¥¤¡ 9@¥ BA © ¡ ¤£ ¡ ¤£sim

6

V

%
tri



D

"$r&6 ()tr¡¡i©% ©@D v

%
tri



D

6

" r$ &()¡¡©©

%
tri



D

" $r&()¡¡ ©©

Table 1 shows the top 10 most similar words to "grandiloquent" according to the above three similarity measures.

To determine which similarity measure ranks higher the

words that are derived from the same root as the given

word, we adopted the evaluation metrics used in the Text

Retrieval Conference [Harman, 1993]. We used a 109,582-

word list from the AI Repository.1 The probabilities of

trigrams are estimated by their frequencies in the words.

    fE"@ @6

Let denote the set of words in the word list and

  a 0 ©

¨§ ©§   §d¡ enexo¤¥ tTIeT¤Tth¥ e

I4 su© bset of
denote

that are derived the ordering of

from
8

S

. Let in de-

¨§ § ©§scending similarity¡ 0toxe ¤¥ T¤ITaTc¥cor4Id© ing to a similarity measure.

¥! #" " %'$ &The precision of

at recall lQrerQ Qvel N% is de-

¥©  ¤  $ & ( fined 1) 0 ¨§      © §¤2"  "  43658

aY's¥¤tTIh¤T e7T mB¥ ¨6aSximum and ¡

value of
e ¥¤IT T¤7T ¥



4

QrrQ Q
©

E

such that . The qual-

ity of the sequence

can be measured by the

1http://www.cs.cmu.edu/afs/cs/project/ai-repository

11-point average of its precisions at recall levels 0%, 10%,

 20%, ..., and 100%. The average precision values are then u"f "@ @h6

averaged over all the words in

. The results on 5

roots are shown in Table 2. It can be seen that much better

results were achieved with sim than with the other similar-

ity measures. The reason for this is that simedit and simtri treat all characters or trigrams equally, whereas sim is able

to automatically take into account the varied importance in

different trigrams.

5 Word Similarity
In this section, we show how to measure similarities between words according to their distribution in a text corpus [Pereira et al., 1993]. Similar to [Alshawi and Carter, 1994; Grishman and Sterling, 1994; Ruge, 1992], we use a parser to extract dependency triples from the text corpus. A dependency triple consists of a head, a dependency type and a modifier. For example, the dependency triples in "I have a brown dog" consist of:
(1) (have subj I), (have obj dog), (dog adj-mod brown), (dog det a)
where "subj" is the relationship between a verb and its subject; "obj" is the relationship between a verb and its object;

"adj-mod" is the relationship between a noun and its adjective modifier and "det" is the relationship between a noun and its determiner.
We can view dependency triples extracted from a corpus as features of the heads and modifiers in the triples. Suppose (avert obj duty) is a dependency triple, we say that "duty" has the feature obj-of(avert) and "avert" has the feature obj(duty). Other words that also possess the feature obj-of(avert) include "default", "crisis", "eye", "panic", "strike", "war", etc., which are also used as objects of "avert" in the corpus.
Table 3 shows a subset of the features of "duty" and "sanction". Each row corresponds to a feature. A `x' in the "duty" or "sanction" column means that the word possesses that feature.

Table 3: Features of "duty" and "sanction"

Feature

duty sanction  ¢¡¤¦£ ¥¨§

£© : subj-of(include) x x 3.15

£ : obj-of(assume)

x

5.43

£¦ : obj-of(avert)

x x 5.88

£¦ : obj-of(ease)

x 4.99

£ : obj-of(impose)

x x 4.97

¦£  : adj-mod(fiduciary) x

7.76

£  : adj-mod(punitive)

x

x 7.10

£¦ : adj-mod(economic)

x 3.70

¦§ § ¨§Let 

¡ 0©
be the set of features possessed by

.

¡

©
can

§be viewed as § § ¦§ ¦§ties between

a descriptio0ne
two words

of the and

wq ord .
is then

T h¡ e

ceDo©bmv m o¡ naq li© -.

The similarity between two words is defined as follows:

¨§ !§ ¥¡
(2) sim

ex¥

q ©X

q ¢ I"% !&% I%"g! %  DGD



D

#!&% I"% g! %

GD D
 GD D

¢  %¡ X$ ©

where $ is the amount of information contained in a set

¡ £of features
one another,

 ¢. ¡%$AX© ss!umi4 n'g &th)a( t

" fr$ e&at)( ur¡ ef4 s©

are inde()pe¡ nf4 d© ent of
, where is the

probability of feature . When two words have identical

sets of features, their similarity reaches the maximum value

of 1. The minimum similarity 0 is reached when two words

do not have any common feature.

()¡f4 ©

The probability

can b4 e estimated by the percentage

of words that have feature among the set of words that

have the same part of speech. For example, there are 32868

unique nouns in a corpus, 1405 of which were used as sub-

j22 e1e 4qq4c"01e 3636"P0t4545sR 33

of "include". The probability of subj-of(include) is . The probability of the feature adj-mod(fiduciary) is because only 14 (unique) nouns were modified by

"fiduciary". The amount of information in the feature adj-

mod(fiduciary), 7.76, is greater than the amount of infor-

mation in subj-of(include), 3.15. This agrees with our intuition that saying that a word can be modified by "fiduciary" is more informative than saying that the word can be the subject of "include".

The fourth column in Table 3 shows the amount of infor-

mation contained in each feature. If the features in Table 3

were all the features of "duty" and "sanction", the similar-

ity between "duty" and "sanction" would be:

¢  ¡

8

4re ¥b4ppq ¥b4

2

¥b4 RxV ¥b4 5x¥ ¢b4 ¡ 78

4 e ¥¨4 S©v

2 ¥¨4 R ¥¨4 x7 S©  ¢¡ 8 r4 e ¥¨4 2 ¨¥ 4r0 ¥¨4WxR ¥¨4

7

¨¥ 84 3S©

which is equal to 0.66.

We parsed a 22-million-word corpus consisting of Wall Street Journal and San Jose Mercury with a principle-based broad-coverage parser, called PRINCIPAR [Lin, 1993; Lin, 1994]. Parsing took about 72 hours on a Pentium 200 with 80MB memory. From these parse trees we extracted about 14 million dependency triples. The frequency counts of the dependency triples are stored and indexed in a 62MB dependency database, which constitutes the set of feature descriptions of all the words in the corpus. Using this dependency database, we computed pairwise similarity between 5230 nouns that occurred at least 50 times in the corpus.

The words with similarity to "duty" greater than 0.04 are listed in (3) in descending order of their similarity.

(3) responsibility, position, sanction, tariff, obligation, fee, post, job, role, tax, penalty, condition, function, assignment, power, expense, task, deadline, training, work, standard, ban, restriction, authority, commitment, award, liability, requirement, staff, membership, limit, pledge, right, chore, mission, care, title, capability, patrol, fine, faith, seat, levy, violation, load, salary, attitude, bonus, schedule, instruction, rank, purpose, personnel, worth, jurisdiction, presidency, exercise.

The following is the entry for "duty" in the Random House Thesaurus [Stein and Flexner, 1984].

(4) duty n. 1. obligation , responsibility ; onus; business, province; 2. function , task , assignment , charge. 3. tax , tariff , customs, excise, levy .

The shadowed words in (4) also appear in (3). It can be seen that our program captured all three senses of "duty" in [Stein and Flexner, 1984].
Two words are a pair of respective nearest neighbors (RNNs) if each is the other's most similar word. Our program found 622 pairs of RNNs among the 5230 nouns that

Table 4: Respective Nearest Neighbors

Rank
1 11 21 31 41 51 61 71 81 91 101 111 121 131 141 151 161 171 181 191 201 211 221 231 241 251 261 271 281 291 301 311 321 331 341 351 361 371 381 391 401 411 421 431 441 451 461 471 481 491 501 511 521 531 541 551 561 571 581 591 601 611 621

RNN
earnings profit revenue sale acquisition merger attorney lawyer data information amount number downturn slump there way fear worry jacket shirt film movie felony misdemeanor importance significance reaction response heroin marijuana championship tournament consequence implication rape robbery dinner lunch turmoil upheaval biggest largest blaze fire captive westerner imprisonment probation apparel clothing comment elaboration disadvantage drawback infringement negligence angler fishermen emission pollution granite marble gourmet vegetarian publicist stockbroker maternity outpatient artillery warplanes psychiatrist psychologist blunder fiasco door window counseling therapy austerity stimulus ours yours procurement zoning neither none briefcase wallet audition rite nylon silk columnist commentator avalanche raft herb olive distance length interruption pause ocean sea flying watching ladder spectrum lotto poker camping skiing lip mouth mounting reducing pill tablet choir troupe conservatism nationalism bone flesh powder spray

Sim
0.50 0.39 0.34 0.32 0.30 0.27 0.26 0.24 0.23 0.22 0.21 0.21 0.20 0.19 0.19 0.18 0.18 0.17 0.17 0.17 0.17 0.16 0.16 0.16 0.15 0.15 0.15 0.15 0.14 0.14 0.14 0.14 0.14 0.13 0.13 0.13 0.13 0.13 0.12 0.12 0.12 0.12 0.12 0.11 0.11 0.11 0.11 0.11 0.11 0.10 0.10 0.10 0.10 0.09 0.09 0.09 0.09 0.09 0.08 0.08 0.08 0.07 0.06

occurred at least 50 times in the parsed corpus. Table 4 shows every 10th RNN.

Some of the pairs may look peculiar. Detailed examination actually reveals that they are quite reasonable. For example, the 221 ranked pair is "captive" and "westerner". It is very unlikely that any manually created thesaurus will list them as near-synonyms. We manually examined all 274 occurrences of "westerner" in the corpus and found that 55% of them refer to westerners in captivity. Some of the bad RNNs, such as (avalanche, raft), (audition, rite), were due to their relative low frequencies,2 which make them susceptible to accidental commonalities, such as:

(5)

The
To 8

8 avalanche,S
hold, attend

An uninhibited 8

S
raft the

8

8

S drifted, hit .S ... auditioSn, rite .

audition, rite .

6 Semantic Similarity in a Taxonomy

Semantic similarity [Resnik, 1995b] refers to similarity be-

tween two concepts in a taxonomy such as the WordNet [Miller, 1990] or CYC upper ontology. The semantic simi-
   ¢¡larity between two classes and is not about the classes

themselves. When we say "rivers and ditches are simi-

lar", we are not comparing the set of rivers with the set

of ditches. Instead, we are comparing a g¡ en¥ eric© river and

    ¡a generic ditch. The9 refore9 , we define sim

t9 o be th9 e

0 0 ¡ ¡similar9ity between9 and if all we know about and

  ¡ £  ¡is that

and

.

0 09
The two statements "

 

9
" and "

¡

£  ¡ " are indepen-

dent (instead of being assumed to be independent) because

 the selection of a generic is not related to the selection

0 0o9f
"

a

generic
  and

9

£  ¡ ¡

.

The amount of information contained in
 £¡ " is

" $rX& )( ¡   6© "%'$ & )( ¡   ¡ ©

where )( ¡   © and )( ¡ £  ¡ © are probabilities that a randomly

  ¢  ¡selected object belongs to and , respectively.

0 09@e 0 § 0 Asq suming that the taxonomy is9ea tree,¢9 iq f 9fe
, the commonality between and is

 

9q
  aP¥n¤d q

PP
   e , whereq is the most specific class that subsumes both    and . Therefore,

¡u9fex¥9 qX© 
sim

V
%" $'&()¡


 

De %" ©$'&wv )( " $r¡ & 

P©
)( ¡  

Dq ©

   For example, Figure 2 is a fragment)( of¡ th© e WordNet. The

number attached to each node is

. The similarity

2They all occurred 50­60 times in the parsed corpus.

entity 0.395 inanimate-object 0.167 natural-object 0.0163

geological-formation 0.00176

0.000113 natural-elevation shore 0.0000836

0.0000189

hill

coast 0.0000216

Figure 2: A Fragment of WordNet

between the concepts of Hill and Coast is:

¡ ¥ ©
sim Hill Coast

V



"%'$ & )( ¡ " $r&()¡

© Ge@©olv og"%i$'c&al)( -F¡ ormati© on

Hill Coast

which is equal to 0.59.

There have been many proposals to use the distance be-

tween two concepts in a taxonomy as the basis for their

similarity [Lee et al., 1989; Rada et al., 1989]. Resnik

[Resnik, 1995b] showed that the distance-based similar-

ity measures do not correlate to human judgments as

qwqe u eil¡tlceoacmslom¡sheoisnto¡ ¥£¦mth¥e§aes©ou© nrX©.ee!. pr#oRFp"$reoos&rsne)( idke¡'xhsaemrseipm: leisl,iamriRtiynesnimkF¡ ei£¦gasu¨¥ u§rere© ©

is
2,

simResnik Hill Coast

Geological-Formation .

Wu and Palmer [Wu and Palmer, 1994] proposed a measure

for semantic¡£¦si¥m§i© larity that could be regarded as a special

case of sim

:

3¡ £¦¨¥ §© 3 3 3simWu&Palmer

ev

V

q v

2
V



2

3 3e 1q

where and are the number of IS-A links from A and

3B to their most specific common superclass C; 2 is the

 number of IS-A links from to the root of the taxonomy.

3 3For example, the most specific common supeerclasVs of Hq i ll

and
3V
,

C2 o asi t

is Geological-For¡ mati¥ on. Th©us ,
and simWu&Palmer Hill Coast

GST  

.

,

    ¡)( ¡ C ©

Interestingly, if

is the same for all pairs of con-

 cepts such that there is ¡ a¦£ n¨¥ I§S©-A link from to   ¡¤£1¡ i¥ n§ t© he

taxonomy, simWu&Palmer

coincides with sim

.

Resnik [Resnik, 1995a] evaluated three different similarity measures by correlating their similarity scores on 28 pairs of concepts in the WordNet with assessments made by human subjects [Miller and Charles, 1991]. We adopted

Table 5: Results of Comparison between Semantic Similarity Measures

Word Pair
car, automobile gem, jewel journey, voyage boy, lad coast, shore asylum, madhouse magician, wizard midday, noon furnace, stove food, fruit bird, cock bird, crane tool, implement brother, monk crane, implement lad, brother journey, car monk, oracle food, rooster coast, hill forest, graveyard monk, slave coast, forest lad, wizard chord, smile glass, magician noon, string rooster, voyage Correlation with Miller & Charles

Miller& Charles
3.92 3.84 3.84 3.76 3.70 3.61 3.50 3.42 3.11 3.08 3.05 2.97 2.95 2.82 1.68 1.66 1.16 1.10 0.89 0.87 0.84 0.55 0.42 0.42 0.13 0.11 0.08 0.08 1.00

Resnik
11.630 15.634 11.806 7.003 9.375 13.517 8.744 11.773 2.246 1.703 8.202 8.202 6.136 1.722 3.263 1.722
0 1.722 .538 6.329
0 1.722 1.703 1.722 2.947 .538
0 0 0.795

Wu & Palmer
1.00 1.00
.91 .90 .90 .93 1.00 1.00 .41 .33 .91 .78 .90 .50 .63 .55
0 .41 .7 .63
0 .55 .33 .55 .41 .11
0 0 0.803

sim
1.00 1.00
.89 .85 .93 .97 1.00 1.00 .18 .24 .83 .67 .80 .16 .39 .20
0 .14 .04 .58
0 .18 .16 .20 .20 .06
0 0 0.834

the same data set and evaluation methodology to compare simResnik, simWu&Palmer and sim. Table 5 shows the similarities between 28 pairs of concepts, using three different similarity measures. Column Miller&Charles lists the average similarity scores (on a scale of 0 to 4) assigned by human subjects in Miller&Charles's experiments [Miller and Charles, 1991]. Our definition of similarity yielded slightly higher correlation with human judgments than the other two measures.

7 Comparison between Different Similarity Measures

One of the most commonly used similarity measure is

call Dice coefficient. Suppose two §¡ Yobe j"¥ eiY cqrt¥¤sIT c¤T aT n"¥ Y I4be© de-

scribed with two numerical vectors

and

Table 6: Comparison between Similarity Measures

Property increase with commonality decrease with difference triangle inequality Assumption 6 max value=1 semantic similarity word similarity ordinal values

Similarity Measures:
WP: simWu&Palmer R: simResnik Dice: simdice sim WP R Dice simdist yes yes yes yes no

yes yes no yes yes

no no no no yes

yes yes no yes yes yes no yes yes yes yes no

no yes yes

yes no no yes yes

yes no no no no

§¡ s e "¥ s q ¤¥ IT ¤T TD¥"s 4 ©
, their Dice coefficient is defined as

¡ £¢    ¡ "¡£¦¥¨§ ©X  ¤¢    ¤¢  ¡ " ¡ "simdice

V e4

Y

q

e4 v

Y

s
e4

qT
s

¦¥Another class of  simI ©7i¤¡ l¦£ar¨¥it§y © measures is based a distance

metric. Suppose

is a distance metric between

two objects, simdist can be defined as follows:

¦¥¡ ¦£ ¥§©
simdist

Y Yv  ¤7© ¡ £¦¨¥ §©

their shades, B and C are similar in their shape, but A and
C are no©©©¨¨¨t si©¨©¨¨©mil¨¨¨ ar. ©©©© ©©©© ©©©© 

AB

C

Figure 3: Counter-example of Triangle Inequality

Assumption 6: The strongest assumption that we made in

Section 2 is Assumption 6. However, this assumption is

not unique to our proposal. Both simWu&Palmer and simdice

also satisfy Assumption 6. Suppose tw¡¡Yoe ¥"oyY bxq je¤¥ cIT tT¤sDT A¥BY4Ian© d B

) )a§¡ Dsree¥"rs7exqp¤¥reTIs¤T eDT n"¥ tsce4Bd© by two feature vectors

and

, respectively. Without loss of generality,

¨

suppose the first features and the rest

features rep-

resent two independent perspectives of the objects.

¡ ¦£ ¥¨§ © 

s¡¡¡im¡£££ d ic$£e$

d d






 
d
d



¡¡

 

££ 
¡¡

 

¡$


££ 

t
t
$  

q0

¤ t¡

 

¡¡d ¤  q0
t £ 
¡

¡$

 
¡
d
£

d
¤  ¤ 
q 
$



t $ 
¡
¡ 

 

d

t
 £  £
d


t


$ 


$





¡

t   



v
£

d

$

t




 



t



which is a weighted average of the similarity between A and B in each of the two perspectives.
Maximum Similarity Values: With most similarity measures, the maximum similarity is 1, except simResnik, which have no upper bound for similarity values.
Application Domains: The similarity measure proposed in this paper can be applied in all the domains listed in Table 6, including the similarity of ordinal values, where none of the other similarity measures is applicable.

Table 6 summarizes the comparison among 5 similarity measures.

Commonality and Difference: While most similarity measures increase with commonality and decrease with difference, simdist only decreases with difference and simResnik only takes commonality into account.

Triangle Inequality: A distance metrics must satisfy the

¦¥ ¦¥ §¥triangle in eq¤©7u¡a£¦lit¥ y: © g  ¤7© ¡£¦¨¥ §©@v  I ©7¡¤§Q¥ ©
    . ¡ ¦£ ¥  Consequently, simdist has the property that sim¤¡ £1dis¥t§ ©

©

 cannot¤¡ b§`e¥ arb© itrarily close to 0 if none of simdist

and

simdist

is 0. This can be counter-intuitive in some

situations. For example, in Figure 3, A and B are similar in

8 Conclusion
Similarity is an important and fundamental concept in AI and many other fields. Previous proposals for similarity measures are heuristic in nature and tied to a particular domain or form of knowledge representation. In this paper, we present a universal definition of similarity in terms of information theory. The similarity measure is not directly stated as in earlier definitions, rather, it is derived from a set of assumptions. In other words, if one accepts the assumptions, the similarity measure necessarily follows. The universality of the definition is demonstrated by its applications in different domains where different similarity measures have been employed before.

Acknowledgment
The author wishes to thank the anonymous reviewers for their valuable comments. This research has been partially supported by NSERC Research Grant OGP121338.
References
[Aha et al., 1991] Aha, D., Kibler, D., and Albert, M. (1991). Instance-Based Learning Algorithms. Machine Learning, 6(1):37­66.
[Alshawi and Carter, 1994] Alshawi, H. and Carter, D. (1994). Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635­ 648.
[Bacchus, 1988] Bacchus, F. (1988). Representing and Reasoning with Probabilistic Knowledge. PhD thesis, University of Alberta, Edmonton, Alberta, Canada.
[Cover and Thomas, 1991] Cover, T. M. and Thomas, J. A. (1991). Elements of information theory. Wiley series in telecommunications. Wiley, New York.
[Frakes and Baeza-Yates, 1992] Frakes, W. B. and BaezaYates, R., editors (1992). Information Retrieval, Data Structure and Algorithms. Prentice Hall.
[Grishman and Sterling, 1994] Grishman, R. and Sterling, J. (1994). Generalizing automatically generated selectional patterns. In Proceedings of COLING-94, pages 742­747, Kyoto, Japan.
[Harman, 1993] Harman, D. (1993). Overview of the first text retrieval conference. In Proceedings of SIGIR'93, pages 191­202.
[Hindle, 1990] Hindle, D. (1990). Noun classification from predicate-argument structures. In Proceedings of ACL-90, pages 268­275, Pittsburg, Pennsylvania.
[Lee et al., 1989] Lee, J. H., Kim, M. H., and Lee, Y. J. (1989). Information retrieval based on conceptual distance in is-a hierarchies. Journal of Documentation, 49(2):188­207.
[Lin, 1993] Lin, D. (1993). Principle-based parsing without overgeneration. In Proceedings of ACL­93, pages 112­120, Columbus, Ohio.
[Lin, 1994] Lin, D. (1994). Principar--an efficient, broadcoverage, principle-based parser. In Proceedings of COLING­94, pages 482­488. Kyoto, Japan.

[McGill et al., 1979] McGill et al., M. (1979). An evaluation of factors affecting document ranking by information retrieval systems. Project report, Syracuse University School of Information Studies.
[Miller, 1990] Miller, G. A. (1990). WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235­312.
[Miller and Charles, 1991] Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1­28.
[Pearl, 1988] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.
[Pereira et al., 1993] Pereira, F., Tishby, N., and Lee, L. (1993). Distributional Clustering of English Words. In Proceedings of ACL-93, pages 183­190, Ohio State University, Columbus, Ohio.
[Rada et al., 1989] Rada, R., Mili, H., Bicknell, E., and Blettner, M. (1989). Development and application ofa metric on semantic nets. IEEE Transaction on Systems, Man, and Cybernetics, 19(1):17­30.
[Resnik, 1995a] Resnik, P. (1995a). Disambiguating noun groupings with respect to wordnet senses. In Third Workshop on Very Large Corpora. Association for Computational Linguistics.
[Resnik, 1995b] Resnik, P. (1995b). Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI-95, pages 448­453, Montreal, Canada.
[Ruge, 1992] Ruge, G. (1992). Experiments on linguistically based term associations. Information Processing & Management, 28(3):317­332.
[Stanfill and Waltz, 1986] Stanfill, C. and Waltz, D. (1986). Toward Memory-based Reasoning. Communications of ACM, 29:1213­1228.
[Stein and Flexner, 1984] Stein, J. and Flexner, S. B., editors (1984). Random House College Thesaurus. Random House, New York.
[Tversky, 1977] Tversky, A. (1977). Features of similarity. Psychological Review, 84:327­352.
[Wu and Palmer, 1994] Wu, Z. and Palmer, M. (1994). Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Associations for Computational Linguistics, pages 133­138, Las Cruces, New Mexico.

