Support Vector and Kernel Machines
Nello Cristianini BIOwulf Technologies nello@support-vector.net http://www.support-vector.net/tutorial.html
ICML 2001

A Little History
z SVMs introduced in COLT-92 by Boser, Guyon, Vapnik. Greatly developed ever since.
z Initially popularized in the NIPS community, now an important and active field of all Machine Learning research.
z Special issues of Machine Learning Journal, and Journal of Machine Learning Research.
z Kernel Machines: large class of learning algorithms, SVMs a particular instance.
www.support-vector.net

A Little History
z Annual workshop at NIPS z Centralized website: www.kernel-machines.org z Textbook (2000): see www.support-vector.net z Now: a large and diverse community: from machine
learning, optimization, statistics, neural networks, functional analysis, etc. etc z Successful applications in many fields (bioinformatics, text, handwriting recognition, etc) z Fast expanding field, EVERYBODY WELCOME ! -
www.support-vector.net

Preliminaries
z Task of this class of algorithms: detect and exploit complex patterns in data (eg: by clustering, classifying, ranking, cleaning, etc. the data)
z Typical problems: how to represent complex patterns; and how to exclude spurious (unstable) patterns (= overfitting)
z The first is a computational problem; the second a statistical problem.
www.support-vector.net

Very Informal Reasoning
z The class of kernel methods implicitly defines the class of possible patterns by introducing a notion of similarity between data
z Example: similarity between documents
z By length z By topic z By language ...
z Choice of similarity Î Choice of relevant features
www.support-vector.net

More formal reasoning
z Kernel methods exploit information about the inner products between data items
z Many standard algorithms can be rewritten so that they only require inner products between data (inputs)
z Kernel functions = inner products in some feature space (potentially very complex)
z If kernel given, no need to specify what features of the data are being used
www.support-vector.net

Just in case ...

z Inner product between vectors

x,z = xizi

z Hyperplane:

i

w, x + b = 0

x x
x ox
x
w
ox

bo

x

o o

o

www.support-vector.net

Overview of the Tutorial
z Introduce basic concepts with extended example of Kernel Perceptron
z Derive Support Vector Machines z Other kernel based algorithms z Properties and Limitations of Kernels z On Kernel Alignment z On Optimizing Kernel Alignment
www.support-vector.net

Parts I and II: overview
z Linear Learning Machines (LLM) z Kernel Induced Feature Spaces z Generalization Theory z Optimization Theory z Support Vector Machines (SVM)
www.support-vector.net

Modularity

IMPORTANT CONCEPT

z Any kernel-based learning algorithm composed of two modules:
­ A general purpose learning machine ­ A problem specific kernel function
z Any K-B algorithm can be fitted with any kernel
z Kernels themselves can be constructed in a modular way
z Great for software engineering (and for analysis)

www.support-vector.net

1-Linear Learning Machines
z Simplest case: classification. Decision function is a hyperplane in input space
z The Perceptron Algorithm (Rosenblatt, 57)
z Useful to analyze the Perceptron algorithm, before looking at SVMs and Kernel Methods in general
www.support-vector.net

Basic Notation

z Input space z Output space z Hypothesis z Real-valued: z Training Set z Test error z Dot product

x X y Y = {-1,+1} h H f:X  R S = {(x1, y1),...,(xi, yi),....} 
x, z

www.support-vector.net

Perceptron

o
b

x
x x

x

x
w
ox

o
o o

x o

z Linear Separation of the input space
f (x) = w, x + b
h(x) = sign( f (x))

www.support-vector.net

Perceptron Algorithm

Update rule (ignoring threshold):
z if yi( wk, xi )  0 then
wk + 1  wk + yixi
k  k +1

o
b

x x
x
w
o
o o
o

x
x x
x o

www.support-vector.net

Observations
z Solution is a linear combination of training
points w =  iy ix i
i  0
z Only used informative points (mistake driven) z The coefficient of a point in combination
reflects its `difficulty'
www.support-vector.net

Observations - 2

z Mistake bound:

M





R 



2

o

x x
gx

o o
o

o

x x x
o

x

z coefficients are non-negative z possible to rewrite the algorithm using this alternative
representation
www.support-vector.net

Dual Representation

IMPORTANT CONCEPT

The decision function can be re-written as follows:
f(x) = w,x +b = iyi xi,x +b
w = iyixi

www.support-vector.net

Dual Representation
z And also the update rule can be rewritten as follows:
3 8z if yi jyj xj, xi + b  0 then i  i + 
z Note: in dual representation, data appears only inside dot products
www.support-vector.net

Duality: First Property of SVMs
z DUALITY is the first feature of Support Vector Machines
z SVMs are Linear Learning Machines represented in a dual fashion
f(x) = w,x +b = iyi xi,x +b
z Data appear only within dot products (in decision function and in training algorithm)
www.support-vector.net

Limitations of LLMs
Linear classifiers cannot deal with z Non-linearly separable data z Noisy data z + this formulation only deals with vectorial data
www.support-vector.net

Non-Linear Classifiers
z One solution: creating a net of simple linear classifiers (neurons): a Neural Network (problems: local minima; many parameters; heuristics needed to train; etc)
z Other solution: map data into a richer feature space including non-linear features, then use a linear classifier
www.support-vector.net

Learning in the Feature Space

z Map data into a feature space where they are

linearly separable

x  (x)

f

x ox

f (x) f (o) f (x)

ox o

f (o) f (o)

o x f (o)

X
www.support-vector.net

f (x)
f (x) F

Problems with Feature Space
z Working in high dimensional feature spaces solves the problem of expressing complex functions
BUT: z There is a computational problem (working with
very large vectors) z And a generalization theory problem (curse of
dimensionality)
www.support-vector.net

Implicit Mapping to Feature Space
We will introduce Kernels:
z Solve the computational problem of working with many dimensions
z Can make it possible to use infinite dimensions ­ efficiently in time / space
z Other advantages, both practical and conceptual
www.support-vector.net

Kernel-Induced Feature Spaces
z In the dual representation, the data points only appear inside dot products:
f(x) = iyi (xi),(x) +b
z The dimensionality of space F not necessarily
important. May not even know the map 
www.support-vector.net

Kernels

IMPORTANT CONCEPT

z A function that returns the value of the dot product between the images of the two arguments
K(x1, x2) = (x1),(x2)

z Given a function K, it is possible to verify that it is a kernel

www.support-vector.net

Kernels
z One can use LLMs in a feature space by simply rewriting it in dual representation and replacing dot products with kernels:
x1, x2  K(x1, x2) = (x1),(x2)
www.support-vector.net

The Kernel Matrix
z (aka the Gram matrix):
K(1,1) K(1,2) K(1,3) ... K(2,1) K(2,2) K(2,3) ...
K=
...... ... ... K(m,1) K(m,2) K(m,3) ...

IMPORTANT CONCEPT
K(1,m) K(2,m)
... K(m,m)

www.support-vector.net

The Kernel Matrix
z The central structure in kernel machines z Information `bottleneck': contains all necessary
information for the learning algorithm z Fuses information about the data AND the
kernel z Many interesting properties:
www.support-vector.net

Mercer's Theorem
z The kernel matrix is Symmetric Positive Definite
z Any symmetric positive definite matrix can be regarded as a kernel matrix, that is as an inner product matrix in some space
www.support-vector.net

More Formally: Mercer's Theorem
z Every (semi) positive definite, symmetric function is a kernel: i.e. there exists a mapping

such that it is possible to write:
K(x1, x2) = (x1),(x2)
IPos. Def. K ( x , z ) f ( x ) f ( z ) d x d z  0
f  L2
www.support-vector.net

Mercer's Theorem
z Eigenvalues expansion of Mercer's Kernels:
K(x1, x2) = ii(x1)i(x2) i
z That is: the eigenfunctions act as features !
www.support-vector.net

Examples of Kernels
z Simple examples of kernels are:
K(x, z) = x, z d K(x, z) = e- x-z 2 /2
www.support-vector.net

Example: Polynomial Kernels
x = (x1, x2); z = (z1, z2);

x, z 2 = (x1z1 + x2z2)2 =

= x12z12 + x22z22 + 2 x1z1x2z2 =

= (x12 , x22 , 2 x1x2), (z12 , z22 , 2z1z2) =

= (x),(z)

www.support-vector.net

Example: Polynomial Kernels
www.support-vector.net

Example: the two spirals
z Separated by a hyperplane in feature space (gaussian kernels)
www.support-vector.net

Making Kernels

IMPORTANT CONCEPT

z The set of kernels is closed under some operations. If K, K' are kernels, then:
z K+K' is a kernel
z cK is a kernel, if c>0
z aK+bK' is a kernel, for a,b >0
z Etc etc etc......
z can make complex kernels from simple ones: modularity !

www.support-vector.net

Second Property of SVMs:

SVMs are Linear Learning Machines, that

z Use a dual representation

AND

z Operate in a kernel induced feature space

(that is:

f(x) = iyi (xi),(x) +b

is a linear function in the feature space implicitely defined by K)

www.support-vector.net

Kernels over General Structures
z Haussler, Watkins, etc: kernels over sets, over sequences, over trees, etc.
z Applied in text categorization, bioinformatics, etc
www.support-vector.net

A bad kernel ...
z ... would be a kernel whose kernel matrix is mostly diagonal: all points orthogonal to each other, no clusters, no structure ...
10 0 ...0 01 0 ...0
1 ...... ... ... ... 00 0 ...1
www.support-vector.net

No Free Kernel

IMPORTANT CONCEPT

z If mapping in a space with too many irrelevant features, kernel matrix becomes diagonal

z Need some prior knowledge of target so choose a good kernel

www.support-vector.net

Other Kernel-based algorithms
z Note: other algorithms can use kernels, not just LLMs (e.g. clustering; PCA; etc). Dual representation often possible (in optimization problems, by Representer's theorem).
www.support-vector.net

%5($.
www.support-vector.net

NEW TOPIC
The Generalization Problem
z The curse of dimensionality: easy to overfit in high dimensional spaces
(=regularities could be found in the training set that are accidental, that is that would not be found again in a test set)
z The SVM problem is ill posed (finding one hyperplane that separates the data: many such hyperplanes exist)
z Need principled way to choose the best possible hyperplane
www.support-vector.net

The Generalization Problem
z Many methods exist to choose a good hyperplane (inductive principles)
z Bayes, statistical learning theory / pac, MDL, ...
z Each can be used, we will focus on a simple case motivated by statistical learning theory (will give the basic SVM)
www.support-vector.net

Statistical (Computational) Learning Theory
z Generalization bounds on the risk of overfitting (in a p.a.c. setting: assumption of I.I.d. data; etc)
z Standard bounds from VC theory give upper and lower bound proportional to VC dimension
z VC dimension of LLMs proportional to dimension of space (can be huge)
www.support-vector.net

Assumptions and Definitions
z distribution D over input space X z train and test points drawn randomly (I.I.d.) from D z training error of h: fraction of points in S misclassifed
by h z test error of h: probability under D to misclassify a point
x z VC dimension: size of largest subset of X shattered by
H (every dichotomy implemented)
www.support-vector.net

VC Bounds

 = O~ VC 
m

VC = (number of dimensions of X) +1 Typically VC >> m, so not useful Does not tell us which hyperplane to choose

www.support-vector.net

Margin Based Bounds



=

O~ 

(R

/ m

)2



 = min

y if ( x i )
i
f

Note: also compression bounds exist; and online bounds.
www.support-vector.net

Margin Based Bounds

IMPORTANT CONCEPT

z (The worst case bound still holds, but if lucky (margin is large)) the other bound can be applied and better generalization can be achieved:



=

O~

(R / m

)2



z Best hyperplane: the maximal margin one

z Margin is large is kernel chosen well

www.support-vector.net

Maximal Margin Classifier
z Minimize the risk of overfitting by choosing the maximal margin hyperplane in feature space
z Third feature of SVMs: maximize the margin z SVMs control capacity by increasing the
margin, not by reducing the number of degrees of freedom (dimension free capacity control).
www.support-vector.net

Two kinds of margin

z Functional and geometric margin:
funct = min yif (xi)

o

x x
gx

x geom = min yif (xi)

x x

f

ox

o
o o

o

www.support-vector.net

Two kinds of margin
www.support-vector.net

Max Margin = Minimal Norm
z If we fix the functional margin to 1, the geometric margin equal 1/||w||
z Hence, maximize the margin by minimizing the norm
www.support-vector.net

Max Margin = Minimal Norm

Distance between The two convex hulls

x

x
o gx

x

x x

ox

o
o o

o

w, x+ + b = +1 w, x- + b = -1 w,(x + - x- ) = 2
w ,(x+ - x-) = 2 ww

www.support-vector.net

The primal problem

IMPORTANT STEP

z Minimize: subject to:

w, w
yi4 w, xi + b9  1

www.support-vector.net

Optimization Theory

z The problem of finding the maximal margin hyperplane: constrained optimization (quadratic programming)

z Use Lagrange theory (or Kuhn-Tucker Theory)

z Lagrangian:

1 61
2

w, w

-



iyi

 !

w, xi

+b

- 1"$#

 0

www.support-vector.net

From Primal to Dual

1 6L(w) = 1 2

w, w

- iyi!

w, xi

+b

- 1"$#

i  0
Differentiate and substitute:
L = 0 b L = 0 w

www.support-vector.net

The Dual Problem

IMPORTANT STEP

 z Maximize: W( ) = i - 1
i2

ijyiyj xi,xj
i, j

z Subject to: i  0

iyi = 0 i

z The duality again ! Can use kernels !

www.support-vector.net

Convexity

IMPORTANT CONCEPT

z This is a Quadratic Optimization problem: convex, no local minima (second effect of Mercer's conditions)
z Solvable in polynomial time ...
z (convexity is another fundamental property of SVMs)

www.support-vector.net

Kuhn-Tucker Theorem

Properties of the solution: z Duality: can use kernels
1 6z KKT conditions: i yi w, xi + b -1 = 0

z Sparseness: only the ipoints nearest to the hyperplane

w =   y x(margin = 1) have positive weight

iii

z They are called support vectors

www.support-vector.net

KKT Conditions Imply Sparseness

Sparseness:

another fundamental property of SVMs

x

x
o gx

x

x x

ox

o
o o

o

www.support-vector.net

Properties of SVMs - Summary
9 Duality 9 Kernels 9 Margin 9 Convexity 9 Sparseness
www.support-vector.net

Dealing with noise

In the case of non-separable data

in feature space, the margin distribution

( )can be optimized

 1 m

R+ 2

2 2

yi4 w, xi + b9  1 - i
www.support-vector.net

The Soft-Margin Classifier

Minimize: Or:

1 w, w + C i
2i
1 w, w + C i2
2i

Subject to:

yi4 w, xi + b9  1 - i
www.support-vector.net

Slack Variables

( )  1 m

R+ 2

2

2

yi4 w, xi + b9  1 - i
www.support-vector.net

Soft Margin-Dual Lagrangian

 z Box constraints

W() =

i

i - 1 2

ijyiyj xi,xj
i, j

0  i  C

iyi = 0 i

z Diagonal

  i - 1
i2

ijyiyj xi,xj - 1 i, j 2C

jj

0  i

iyi  0 i
www.support-vector.net

The regression case
z For regression, all the above properties are retained, introducing epsilon-insensitive loss:
L

e 0
www.support-vector.net

x yi-<w,xi>+b

Regression: the -tube
www.support-vector.net

Implementation Techniques

z Maximizing a quadratic function, subject to a

linear equality constraint (and inequalities as

well)

 W( ) = i - 1 ijyiyjK(xi, xj)
2 i,j
i

i  0

iyi = 0 i

www.support-vector.net

Simple Approximation

z Initially complex QP pachages were used.

z Stochastic Gradient Ascent (sequentially update 1 weight at the time) gives excellent approximation in most cases

 i  i + 1

1 - yi iyiK(xi, xj)

 K(xi, xi)

www.support-vector.net

Full Solution: S.M.O.
z SMO: update two weights simultaneously z Realizes gradient descent without leaving the
linear constraint (J. Platt).
z Online versions exist (Li-Long; Gentile)
www.support-vector.net

Other "kernelized" Algorithms
z Adatron, nearest neighbour, fisher discriminant, bayes classifier, ridge regression, etc. etc
z Much work in past years into designing kernel based algorithms
z Now: more work on designing good kernels (for any algorithm)
www.support-vector.net

On Combining Kernels
z When is it advantageous to combine kernels ? z Too many features leads to overfitting also in
kernel methods z Kernel combination needs to be based on
principles z Alignment
www.support-vector.net

Kernel Alignment

IMPORTANT CONCEPT

z Notion of similarity between kernels: Alignment (= similarity between Gram matrices)

A(K1, K2) =

K1, K2

K1, K1 K2, K2

www.support-vector.net

Many interpretations
z As measure of clustering in data z As Correlation coefficient between `oracles'
z Basic idea: the `ultimate' kernel should be YY', that is should be given by the labels vector (after all: target is the only relevant feature !)
www.support-vector.net

The ideal kernel

11

-1 ... -1

11

-1 ... -1

YY'= -1 -1 1

1

...... ... ... ...

-1 -1 1 ... 1

www.support-vector.net

Combining Kernels

z Alignment in increased by combining kernels that are aligned to the target and not aligned to each other.

z
A(K1, YY' ) =

K1, YY'

K1, K1 YY' , YY'

www.support-vector.net

Spectral Machines
z Can (approximately) maximize the alignment of a set of labels to a given kernel
z By solving this problem: y = arg max yKy yy'
yi {-1,+1}
z Approximated by principal eigenvector (thresholded) (see courant-hilbert theorem)
www.support-vector.net

Courant-Hilbert theorem
z A: symmetric and positive definite, z Principal Eigenvalue / Eigenvector
characterized by:
 = max vAv
v vv'
www.support-vector.net

Optimizing Kernel Alignment
z One can either adapt the kernel to the labels or vice versa
z In the first case: model selection method z Second case: clustering / transduction method
www.support-vector.net

Applications of SVMs
z Bioinformatics z Machine Vision z Text Categorization z Handwritten Character Recognition z Time series analysis
www.support-vector.net

Text Kernels
z Joachims (bag of words) z Latent semantic kernels (icml2001) z String matching kernels z... z See KerMIT project ...
www.support-vector.net

Bioinformatics
z Gene Expression z Protein sequences z Phylogenetic Information z Promoters z...
www.support-vector.net

Conclusions:
z Much more than just a replacement for neural networks. -
z General and rich class of pattern recognition methods
z %RR RQ 690VZZZVXSSRUWYHFWRUQHW
z Kernel machines website www.kernel-machines.org
z www.NeuroCOLT.org
www.support-vector.net

