From: AAAI-99 Proceedings. Copyright © 1999, AAAI (www.aaai.org). All rights reserved. 

Hybrid Neural Plausibility Networks for News Agents

Stefan Wermter, Christo Panchev and Garen Arevian

The Informatics Center

School of Computing, Engineering & Technology

University of Sunderland

St. Peter’s Way, Sunderland SR6 0DD, United Kingdom

Email: stefan.wermter@sunderland.ac.uk

Abstract

This paper describes a learning news agent HyNeT
which uses hybrid neural network techniques for clas-
sifying news titles as they appear on an internet
newswire. Recurrent plausibility networks with local
memory are developed and examined for learning ro-
bust text routing. HyNeT is described for the (cid:12)rst time
in this paper. We show that a careful hybrid integra-
tion of techniques from neural network architectures,
learning and information retrieval can reach consist-
ent recall and precision rates of more than 92% on an
82 000 word corpus; this is demonstrated for 10 000 un-
known news titles from the Reuters newswire. This
new synthesis of neural networks, learning and inform-
ation retrieval techniques allows us to scale up to a
real-world task and demonstrates a lot of potential for
hybrid plausibility networks for semantic text routing
agents on the internet.

Introduction

In the last decade, a lot of work on neural networks
in arti(cid:12)cial
intelligence has focused on fundamental
issues of connectionist representations (Hendler 1991;
Miikkulainen 1993; Giles & Omlin 1993; Sun 1994;
Honavar 1995). Recently, there has been a new focus on
neural network learning techniques and text processing-
such as for newswires and world wide web documents
(Papka, Callan, & Barto 1997; Lawrence & Giles 1998;
Craven et al. 1998; Joachims 1998).

However, it has been an open research question (Wer-
mter & Sun 1998) as to whether hybrid neural architec-
tures will be able to learn large-scale real-world tasks,
such as learning the classi(cid:12)cation of noisy newswire
titles. Neural networks with their properties of robust-
ness, learning and adaptiveness are good candidates for
weighted rankings and weighted routing of ambiguous
or corrupted messages, in addition to well-established
techniques from information retrieval, symbolic pro-
cessing, and statistics (Lewis 1994).

In this paper, we develop and examine new Hybrid
Neural/symbolic agents for Text routing on the in-
ternet (HyNeT). We integrate di(cid:11)erent preprocessing

Copyright c(cid:13)1998, American Association for Arti(cid:12)cial Intel-
ligence (www.aaai.org). All rights reserved.

strategies based on information retrieval with recurrent
neural network architectures, including variable-length
short-term memories. In particular, we explore simple
recurrent networks and new more sophisticated recur-
rent plausibility networks. Recurrent plausibility net-
works are extended with a dynamic short-term memory
which allows the processing of sequences in a robust
manner. As a real-world testbed, we describe extensive
experiments with learning news agents.

Recurrent Plausibility Networks

Recurrent neural networks introduce previous states
and extend feedforward networks with short-term incre-
mental memories. In fully recurrent networks, all the
information is processed and fed back into one single
layer. Partially recurrent networks, such as simple re-
current networks, have recurrent connections between
the hidden layer and context layer (Elman 1990) or
Jordan networks have connections between the output
and context layer (Jordan 1986).

In other research (Wermter 1995), di(cid:11)erent decay
memories were introduced by using distributed recur-
rent delays over the separate context layers representing
the contexts at di(cid:11)erent time steps. At a given time
step, the network with n hidden layers processes the
current input as well as the incremental contexts from
the n − 1 previous time steps.

Figure 1 shows the general structure of our recur-
rent plausibility network. It combines the features of
recurrent networks with distributed context layers and
self-recurrent connections of the context layers. The
input to a hidden layer Ln is constrained by the un-
derlying layer Ln−1 as well as the incremental context
layer Cn. The activation of a unit Lni(t) at time t is
computed on the basis of the weighted activation of the
units in the previous layer L(n−1)i(t) and the units in
the current context of this layer Cni(t) limited by the
logistic function f .

X

X

Lni(t) = f (

wkiL(n−1)i(t) +

wliCni(t))

k

l

The units in the context layers perform a time-

averaging of the information using the equation
Cni(t) = (1 − ’n)Lni(t − 1) + ’nCni(t − 1)

where Cni(t) is the activation of a unit in the context
layer at time t. We represent the self-recurrent con-
nections using the hysteresis value ’n. The hysteresis
value of the context layer Cn−1 is lower than the hys-
teresis value of the next context layer Cn. This ensures
that the context layers closer to the input layer will per-
form as memory that represents a more dynamic con-
text. Having higher hysteresis values, the context layers
closer to the output layer will incrementally build more
stable sequential memory. Therefore the larger context
is built on the more recent, dynamic one.

Output 
layer

O(t)

Hidden 
layer

Ln(t)

Hidden 
layer

Ln−1(t)

M : N feedforward 
connections

1 : 1 recurrent 
connections

Context 

layer

Cn(t)

Context 

layer

Cn−1(t)

Input
layer

I(t)
L  (t)
0

Figure 1: Recurrent plausibility network.

The Reuters News Corpus

For learning the subtask of text routing, we used
the Reuters text categorization test collection (Lewis
1997). This corpus contains documents which ap-
peared on the Reuters newswire. All news titles in
the Reuters corpus belong to one or more of eight
main categories1: Money/Foreign Exchange (money-
fx, MF), Shipping (ship, SH), Interest Rates (in-
terest, IN), Economic Indicators (economic, EC),
Currency (currency, CR), Corporate (corporate,
CO), Commodity (commodity, CM), Energy (en-
ergy, EN).

Examples of typical titles from these categories are
shown in Table 1. As we can see, there are abbrevi-
ated phrases or sentences, speci(cid:12)c characters, and terms
which would make it di(cid:14)cult to manually encode a tra-

1There is also a second level categorization into 135 spe-
ci(cid:12)c categories, but since we wanted to study the learning
of di(cid:14)cult and possibly ambiguous classi(cid:12)cations, we used
the 8 main categories from the (cid:12)rst level.

ditional semantic grammar. We can (cid:12)nd incomplete-
ness or ambiguity in the category assignments. For
example, the title \U.K. money market o(cid:11)ered early
assistance" occurs six times in the corpus belonging to
di(cid:11)erent semantic categories. Three times it is clas-
si(cid:12)ed into the \money-fx" category, and three times
into both \money-fx" and \interest" categories. There-
fore there is an inherent ambiguity in the corpus and
such examples pose challenges to learning algorithms
and necessarily increase the classi(cid:12)cation error, since
there is no perfect unambiguous category assignment
in such cases. However, since we wanted to study hy-
brid neural agents under hard, real-world conditions,
we did not perform any cleaning up of the underlying
corpus data.

We use exactly all 10 733 titles of the so-called Mod-
Apte split whose documents have a title and at least
one topic. The total number of words is 82 339 and the
number of di(cid:11)erent words in the titles is 11 104. For our
training set, we use 1 040 news titles, the (cid:12)rst 130 of
each of the 8 categories. All the other 9 693 news titles
are used for testing the generalization to new and un-
seen examples. The description of this corpus is shown
in Table 2. Since categories may overlap, i.e. one news
title can be in exactly one or several semantic categor-
ies, the actual distribution of the titles over the training
set is not even.

Category

Training titles

Test titles

Number Average Number Average
length
8.13
7.32
8.50
8.50
8.42
7.43
7.54
8.08
7.64

length
8.26
7.04
8.79
8.43
8.54
7.10
7.35
7.80
7.96

427
139
288
1 099
92
6 163
2 815
645
9 693

money-fx
ship
interest
economic
currency
corporate
commodity
energy
All titles

286
139
188
198
203
144
321
176
1 040

Table 2: The distribution of the titles from the Reu-
ters news corpus over the semantic categories. Note
that since one title can be classi(cid:12)ed in more than one
semantic category, 1 040 titles represent the total num-
ber of 1 655 category occurrences for the training set
and 9 693 titles represent the total number of 11 668
category occurrences in the test set.

Hybrid Neural News Routing

In this section, we will describe di(cid:11)erent architectures
for learning news title classi(cid:12)cation. Each news title is
presented to the network as a sequence of word input
representations and category output representations,
one such pair for each word. At the beginning of a
news title, the context layers are initialized with 0 val-
ues. Each unit in the output layer corresponds to a
particular semantic category. Those output units (one

Semantic Category

money-fx
ship
interest
economic
currency
corporate
commodity
energy
ship&energy
money-fx&interest&currency

Title

Bankers trust issuing STG/DLR currency warrants
U.S. cargo preference squabble continues
Volcker says FED policy not responsible for prime rate rise
German net currency reserves rise 400 mln marks to 87.0 billion - Bundesbank
Stoltenberg not surprised by dollar reaction
First Granite Bancorp Inc agrees to be acquired by Magna Group Inc for stock
Indonesian co(cid:11)ee production may fall this year
U.S. oil dependency seen rising to record level
Kuwait may re-register gulf tankers - Newspaper
J.P. Morgan <JPM> says DLR may prevent FED easing

Table 1: Example titles from the corpus.

or more) which represent the desired semantic categor-
ies are set to 1. All other output units are set to 0. We
de(cid:12)ne a news title as classi(cid:12)ed to a particular semantic
category if at the end of the sequence the value of the
output unit for the desired category is higher than 0.5.
Using this output classi(cid:12)cation, we compute the recall
and precision values for each title. These values are
used to compute the average recall and precision rates
for each semantic category, as well as for the overall
training and test sets, which all determine the network
performance.

Supervised learning techniques based on plausibility
networks were used for the training (Rumelhart et al.
1995). The training regime forces the recurrent plaus-
ibility network to assign the desired category, starting
from the beginning of the news title as early as possible.
Supervised training is continued until the error over the
training set stops decreasing. Typically, between 700
and 900 epochs through the training set were necessary.
In one epoch, we present all titles from the training set.
Weights were adjusted at the end of each title.

Complete Titles and Signi(cid:12)cance Vectors
In our (cid:12)rst set of experiments, we use signi(cid:12)cance vec-
tors as the basis for representing words for semantic
category routing. Signi(cid:12)cance vectors are determined
based on the frequency of a word in di(cid:11)erent semantic
categories. Each word w is represented with a vec-
tor (c1c2 (cid:1) (cid:1) (cid:1) cn), where ci represents a certain semantic
category. A value v(w; ci) is computed for each di-
mension of the vector as the frequency of occurrences
of word w in semantic category ci (the category fre-
quency), divided by the frequency of occurrences of
word w in the corpus (the corpus frequency). That
is:

P

v(w; ci) =

F requency of w in ci
F requency f or w in cj

f or j 2 f1; (cid:1) (cid:1) (cid:1) ng

j

The same signi(cid:12)cance vector can represent two dif-
ferent words if they actually occur with the same fre-
quency across all semantic categories in the whole cor-
pus. However, a news title will be represented by a se-
quence of signi(cid:12)cance vectors so that phrases with the

same sequence of signi(cid:12)cance vectors are less likely. Fig-
ure 2 shows the signi(cid:12)cance vectors of words from the
lexicon. As we can see, words like \mortgage", \bur-
eau" or \parker" have clear semantic preferences for
speci(cid:12)c semantic categories, while domain-independent
words like \a", \of", \and" have more distributed pref-
erences.

Word

 a
 and
 bureau
 mortgage
 of
 parker
 the

MF

SH

.07
.06
.02
.01
.07
.13
.09

.02
.02
.00
.00
.02
.00
.03

IN
.04
.03
.00
.30
.03
.00
.05

EC

CR

CO CM EN

.17
.15
.38
.18
.14
.00
.18

.04
.03
.02
.00
.03
.13
.05

.28
.25
.02
.51
.28
.73
.20

.27
.34
.56
.00
.31
.00
.31

.10
.09
.01
.00
.09
.00
.09

Figure 2: Examples of signi(cid:12)cance vectors.

Simple recurrent networks were trained using signi-
(cid:12)cance vector representations as the input word rep-
resentation of the input layer. The performance of the
best trained network in terms of recall and precision is
shown in Table 3. The table contains detailed results for
each semantic category as well as for the whole training
and test sets. The two categories \money-fx" and \eco-
nomic" are fairly common, which explains their lower
recall and precision rates, but in general, 91.23% recall
and 90.73% precision are reached for the 9 693 unknown
test titles.

Complete Titles and Semantic Vectors

In our second set of experiments, we use a di(cid:11)erent se-
mantic vector representation of the words in the lexicon.
These vectors mainly represent the plausibility of a par-
ticular word occurring in a semantic category and they
are independent of the number of examples observed in
each category. A value v(w; ci) is computed for each
element of the semantic vector as the normalized fre-
quency of occurrences of word w in semantic category
ci (the normalized category frequency), divided by the

Category

Training set

Test set

Category

Training set

Test set

money-fx
ship
interest
economic
currency
corporate
commodity
energy
Total

recall
84.36
81.06
77.93
71.70
85.75
88.81
86.27
81.88
85.15

precision
84.62
93.17
82.71
80.79
91.46
92.31
94.77
92.26
86.99

recall
84.74
77.34
85.42
74.74
85.36
94.87
86.47
85.22
91.23

precision
69.56
94.96
83.45
77.82
87.16
95.10
88.31
91.65
90.73

money-fx
ship
interest
economic
currency
corporate
commodity
energy
Total

recall
87.78
81.65
85.33
76.22
87.76
89.16
86.27
89.19
88.57

precision
88.60
88.13
86.97
83.54
92.59
91.96
90.52
95.48
88.59

recall
84.07
82.73
88.25
78.36
89.64
95.90
86.20
86.58
92.47

precision
69.59
93.88
88.19
80.30
89.86
95.98
87.22
91.56
91.61

Table 3: Results using signi(cid:12)cance vectors

Table 4: Results using semantic vectors

normalized frequency of occurrences of word w in the
corpus (the normalized corpus frequency). That is:

P

v(w; ci) =

N orm: f req: of w in ci
N orm: f req: f or w in cj

; j 2 f1; (cid:1) (cid:1) (cid:1) ng

j

where:

N orm: f req: of w in ci =

F req: of w in ci

N umber of titles in ci

Some examples of semantic vectors are shown in Fig-
ure 3. As we can see, the domain-independent words
like \a", \of", \and" have fairly even distributions,
while the domain-dependent words \bureau", \parker"
have more speci(cid:12)c preferences. An example of the dif-
ference between the two types of representation is the
word \mortgage". Compared to the signi(cid:12)cance repres-
entation, here the preference entirely changes from the
category \corporate" to the category \interest".

Word

 a
 and
 bureau
 mortgage
 of
 parker
 the

MF

SH

.13
.12
.00
.02
.13
.25
.15

.12
.12
.00
.00
.11
.00
.13

IN
.11
.09
.00
.72
.11
.00
.12

EC

CR

CO CM EN

.16
.16
.50
.15
.15
.00
.15

.16
.14
.11
.02
.15
.56
.18

.06
.05
.00
.09
.06
.17
.04

.11
.15
.32
.00
.14
.00
.11

.15
.15
.01
.00
.14
.00
.12

Figure 3: Examples of semantic vectors

Simple recurrent networks were trained using the
same training parameters and network architecture as
in the previous experiment, but the titles were pro-
cessed using the semantic vector representations. The
performance of the trained network in terms of recall
and precision is shown in Table 4. Using the semantic
vectors rather than the signi(cid:12)cance vectors, we im-
proved the test recall and precision rates to 92.47% and
91.61% respectively.

News Titles without Insigni(cid:12)cant Words
So far we have examined complete phrases in a simple
recurrent network. However, a symbolic preprocessing

strategy from information retrieval could be useful, one
that emphasizes signi(cid:12)cant domain-dependent words.
To investigate such a strategy from information re-
trieval, we removed insigni(cid:12)cant words (sometimes also
called \stop words") from the unrestricted title phrases.
We de(cid:12)ned the set of insigni(cid:12)cant words as equally fre-
quent, domain-independent words that belong to any
of the following syntactic categories: determiners, pre-
positions and conjunctions. Using the semantic vector
representation, we extracted a set of 19 insigni(cid:12)cant
words of which the di(cid:11)erence between the highest and
lowest values in their vector was less than 0.2. For in-
stance, examples of these words are: a, an, and, as, at,
but, by, for, from. These words occur 7 094 times in the
training and test corpus. After removing these words,
the average length of a title in the training set is 7.11
and average length of a title in the test set is 7.00.

The experiment was conducted with the same learn-
ing conditions as for the complete unrestricted titles.
The test results are shown in Table 5. We can see
that the removal of insigni(cid:12)cant words improves the
recall and precision rates slightly over the last two ex-
periments, but the improvement is small. However, by
removing the insigni(cid:12)cant stop words, we have also de-
leted some domain-independent words; hence this ex-
periment also suggests that noisy corrupted titles can
also be processed.

Category

Training set

Test set

money-fx
ship
interest
economic
currency
corporate
commodity
energy
Total

recall
87.40
79.14
86.61
79.12
85.55
89.51
87.25
85.32
88.47

precision
89.12
88.85
93.09
87.80
92.59
93.71
91.18
94.19
90.05

recall
84.97
78.42
88.89
79.28
87.39
96.47
86.74
83.86
92.88

precision
69.36
93.53
88.77
82.64
87.16
96.48
86.51
90.54
91.92

Table 5: Results without insigni(cid:12)cant words

Complete Titles with Plausibility Networks
In this experiment, we use the recurrent plausibility
network described in Figure 1 with two hidden and two
context layers. Di(cid:11)erent networks have been trained us-
ing di(cid:11)erent combinations of the hysteresis value for the
(cid:12)rst and second context layers. The best results were
achieved with the network having a hysteresis value of
0.2 for the (cid:12)rst context layer, and 0.8 for the second.
In order to be able to compare the results of this ar-
chitecture with the previous ones, the rest of the train-
ing and testing environment parameters were set to be
the same as in the experiment of the complete titles
with the semantic vector representation of the words.
Table 6 shows the recall and precision rates obtained
with the recurrent plausibility network. There was
an improvement in the classi(cid:12)cation, especially for the
longer titles, as the two context layers and recurrent
hysteresis connections support a larger and dynamic
short-term memory.

Category

Training set

Test set

money-fx
ship
interest
economic
currency
corporate
commodity
energy
Total

recall
87.34
84.65
85.24
90.24
88.89
92.31
92.81
85.27
89.05

precision
89.47
89.21
87.77
91.77
91.36
92.66
93.14
87.74
90.24

recall
86.03
82.37
88.19
81.89
89.64
95.55
88.84
87.69
93.05

precision
76.70
90.29
86.05
83.80
89.86
95.43
90.29
92.95
92.29

Example (1)
BANK
OF
JAPAN
INTERVENES
SHORTLY
AFTER
TOKYO
OPENS

Example (2)
BANK
OF
JAPAN
DETERMINED
TO
KEEP
EASY
MONEY
POLICY

Example (3)
IRAN
SOVIET
UNION
TO
SWAP
CRUDE
REFINED
PRODUCTS

.11

.22

.33

.44

.56

.67

.78

.89

1.00

MF

SH

IN

EC

CR

CO

CM EN

.65
.58
.74
.97
.98
.96
.97
.96

.48
.40
.21

.19
.20
.23

.36
.30
.60
.97
.98
.95
.96
.95

MF

SH

IN

EC

CR

CO

CM EN

.19
.20
.23

.36
.30
.60

.14

.18

.65
.58
.74
.15
.26
.22

.17
.43

.48
.40
.21
.56
.50
.31
.94
.92
.90

MF

SH

IN

EC

CR

CO

CM EN

.72
.90
.97
.95
.92
.69
.23
.15

.16

.22
.14
.20
.34

.57
.35
.20
.19
.26
.84
.98
.96

Figure 4: Examples and their category preferences

is associated with oil shipping. A recognizable num-
ber of news titles in the corpus are about the Soviet
Union commodity crisis and are classi(cid:12)ed in the \ship-
ping" and \commodity" categories. In the middle of the
title however, when \crude" is processed as a signi(cid:12)cant
term for the \energy" category, the output preference is
changed to this category and subsequent words con(cid:12)rm
this category assignment.

Table 6: Results using recurrent plausibility network
with semantic vectors

Discussion

Examples of the Network Output

Figure 4 presents the output representations after
processing three unknown example titles from the test
corpus with the recurrent plausibility network from
the last experiment. Here we show a representat-
ive behavior of the network, and in all these ex-
amples, the computed (cid:12)nal categories are correct. The
(cid:12)rst two examples start with the same word sequence
\Bank of Japan", but later, when a sequence of more
domain-dependent words has been processed, the net-
work changes its output preference according to the se-
mantic meaning of the di(cid:11)erent input sequences: \in-
tervenes shortly after Tokyo opens" provides a \money-
fx" and \currency" category assignment, while \easy
money policy" leads to an \interest" category assign-
ment. Both these assignments are correct according to
the Reuters classi(cid:12)cation.

In the third example in Figure 4, we illustrate how
the network, based on the incrementally built context,
can turn the strong output preference from the category
\ship" to \energy". The network starts with prefer-
ence for the \ship" and \energy\ categories because, ac-
cording to all news documents from the corpus, \Iran"

We have described several hybrid neural architectures
for classifying news headlines. In general, the recall and
precision rates for simple recurrent networks and recur-
rent plausibility networks were fairly high given the de-
gree of ambiguity of title/category and word/category
assignments (for instance, see Table 1). The generaliz-
ation performance for new and unseen news titles has
been even better than the performance on the training
data (for instance, see experiment 1). This is a very de-
sirable e(cid:11)ect and demonstrates that over(cid:12)tting on the
training set does not exist.

In other related work on text classi(cid:12)cation, whole
documents rather than titles are often used to classify
a news story. Obviously whole documents contain more
information than titles, but we wanted to examine how
far we can get in terms of classi(cid:12)cation based only on
the title. However, since a news title contains on av-
erage around 8 words, it is unlikely that most of the
words in the sequence are ambiguous and that they
would lead to an incorrect category. Our good recall
and precision results of at least 92% con(cid:12)rm that the
inﬂuence of misleading word representations is limited.
Other related work on a di(cid:11)erent book title classi(cid:12)c-
ation task using neural networks (Wermter 1995) has
reached 95% recall and precision, but the library titles

are much less ambiguous and only 1 000 test titles have
been used in that approach while we used 10 000 cor-
rupted and ambiguous titles. Also, the titles in our task
are about 25% longer and more di(cid:14)cult to classify.

Our integration of new variations of the vector space
model (signi(cid:12)cance vectors, semantic vectors) with non-
linear incremental classi(cid:12)cation from neural networks
was shown as a viable new way for classi(cid:12)cation and
routing tasks. The removal of insigni(cid:12)cant stop words
was shown to provide better results in the simple recur-
rent network as well. Best results were obtained with a
plausibility network with two hidden layers, two context
layers and recurrent connections for the context layer.
Although the performance was in general fairly high for
di(cid:11)erent architectures, an additional 2% improvement
in the test set would mean another 200 correctly clas-
si(cid:12)ed titles. The architecture with plausibility network
has more possibilities to encode the preceding context
and reaches at least 92% minimum value for recall and
precision.

There has been interesting related work on text cat-
egorization on the Reuters corpus using whole docu-
ments (Joachims 1998). For the ten most frequently oc-
curring categories, the recall/precision breakeven point
was 86% for a Support Vector Machine, 82% for k-
Nearest Neighbor, 72% for Naive Bayes. However, a
di(cid:11)erent set of categories and whole documents rather
than titles are used and therefore the results are not
directly comparable to ours. Nevertheless, they give
some indication of document classi(cid:12)cation performance
on this corpus. In particular, for medium text data sets
or when only titles are available, our approach produces
very good performance.

Conclusions

We have described and analyzed HyNeT, a novel news
agent for real-world headline routing which learns to
classify news titles. A neural network architecture with
several context layers and recurrent hysteresis connec-
tions is proposed which particularly supports a larger
and dynamic short-term memory. Di(cid:11)erent from most
related approaches for language processing, we have
used hybrid neural learning techniques.

HyNeT is robust, classi(cid:12)es noisy arbitrary real-world
titles, processes titles incrementally from left to right,
and shows better classi(cid:12)cation reliability towards the
ends of titles. This synthesis of techniques and con-
straints from plausibility neural network architectures,
information retrieval and learning holds a lot of poten-
tial for building robust hybrid neural architectures of
semantic text routing agents for the internet in the fu-
ture.

References

Craven, M.; DiPasquo, D.; Freitag, D.; McCallum, A.;
Mitchell, T.; Nigam, K.; and Slattery, S. 1998. Learn-
ing to extract symbolic knowledge from the world wide

1991.

web. In Proceedings of the 15th National Conference
on Arti(cid:12)cial Intelligence.
Elman, J. L. 1990. Finding structure in time. Cognit-
ive Science 14:179{211.
Giles, C. L., and Omlin, C. W. 1993. Extraction, inser-
tion and re(cid:12)nement of symbolic rules in dynamically
driven recurrent neural networks. Connection Science
5:307{337.
Developing hybrid sym-
Hendler, J.
bolic/connectionist models.
In Barnden, J. A., and
Pollack, J. B., eds., Advances in Connectionist and
Neural Computation Theory, Vol.1: High Level Con-
nectionist Models. Norwood, NJ: Ablex Publishing
Corporation. 165{179.
Honavar, V. 1995. Symbolic arti(cid:12)cial intelligence and
numeric arti(cid:12)cial neural networks: towards a resolu-
tion of the dichotomy. In Sun, R., and Bookman, L. A.,
eds., Computational Architectures integrating Neural
and Symbolic Processes. Boston: Kluwer. 351{388.
Joachims, T. 1998. Text categorization with support
vector machines: learning with many relevant features.
In Proceedings of the European Conference on Machine
Learning.
Jordan, M. I. 1986. Attractor dynamics and parallel-
ism in a connectionist sequential machine. In Proceed-
ings of the Eighth Conference of the Cognitive Science
Society, 531{546.
Lawrence, S., and Giles, C. L. 1998. Searching the
world wide web. Science 280.
Lewis, D. D. 1994. A sequential algorithm for training
text classi(cid:12)ers. In Proceedings of the Seventeenth An-
nual SIGIR Conference on Research and Development
in Information Retrieval.
Lewis, D. D. 1997. Reuters-21578 text categorization
test collection, http://www.research.att.com/ lewis.
Miikkulainen, R. 1993. Subsymbolic Natural Language
Processing. Cambridge, MA: MIT Press.
Papka, R.; Callan, J. P.; and Barto, A. G. 1997.
Text-based information retrieval using exponentiated
gradient descent.
In Mozer, M. C.; Jordan, M. I.;
and Petsche, T., eds., Advances in Neural Information
Processing Systems, volume 9, 3. The MIT Press.
Rumelhart, D. E.; Durbin, R.; Golden, R.; and Chau-
vin, Y. 1995. Backpropagation: the basic theory.
In Chauvin, Y., and Rumelhart, D. E., eds., Back-
propagation:
theory, architectures and applications.
Hillsdale, NJ: Lawrence Erlbaum Associates. 1{34.
Sun, R. 1994. Integrating Rules and Connectionism for
Robust Commonsense Reasoning. New York: Wiley.
Wermter, S., and Sun, R. 1998. Nips Workshop on Hy-
brid Neural Symbolic Integration. Breckenridge, CO:
Nips.
Wermter, S. 1995. Hybrid Connectionist Natural Lan-
guage Processing. London, UK: Chapman and Hall,
Thomson International.

