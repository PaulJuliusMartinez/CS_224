Toward Off-Policy Learning Control with Function Approximation

Hamid Reza Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, Richard S. Sutton Department of Computing Science, University of Alberta, Edmonton, Canada T6G 2E8 Department of Computer Science and Automation, Indian Institute of Science, Bangalore-560012, India

Abstract
We present the first temporal-difference learning algorithm for off-policy control with unrestricted linear function approximation whose per-time-step complexity is linear in the number of features. Our algorithm, Greedy-GQ, is an extension of recent work on gradient temporal-difference learning, which has hitherto been restricted to a prediction (policy evaluation) setting, to a control setting in which the target policy is greedy with respect to a linear approximation to the optimal action-value function. A limitation of our control setting is that we require the behavior policy to be stationary. We call this setting latent learning because the optimal policy, though learned, is not manifest in behavior. Popular off-policy algorithms such as Q-learning are known to be unstable in this setting when used with linear function approximation.
In reinforcement learning, the term "off-policy learning" refers to learning about one way of behaving, called the target policy, from data generated by another way of selecting actions, called the behavior policy. The target policy is often an approximation to the optimal policy, which is typically deterministic, whereas the behavior policy is often stochastic, exploring all possible actions in each state as part of finding the optimal policy. Freeing the behavior policy from the target policy enables a greater variety of exploration strategies to be used. It also enables learning from training data generated by unrelated controllers, including manual human control, and from previously collected data. A third reason for interest in off-policy learning is that it permits learning about multiple target policies (e.g., optimal policies for multiple subgoals) from a single stream of data generated by a
Appearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).

single behavior policy. Off-policy learning for tabular (non-approximate) settings is well understood; there exist simple, online algorithms such as Q-learning (Watkins & Dayan, 1992) which converge to the optimal target policy under minimal conditions. For approximation settings, however, results are much weaker. One promising recent development is gradient-based temporal-difference (TD) learning methods, which have been proven stable under off-policy learning for linear (Sutton et al., 2009a) and nonlinear (Maei et al., 2010) function approximators. However, so far this work has only applied to prediction settings, in which both the target and behavior policy are stationary. In this paper we generalize prior work with gradient TD methods by allowing changes in the target policy. In particular, we consider learning an approximation to the optimal action-value function (thereby finding an approximately optimal target policy) from data generated by an arbitrary stationary behavior policy. We call this problem setting latent learning because the optimal policy is learned but remains latent; it is not allowed to be overtly expressed in behavior. Our latent learning result could be extended further, for example to allow the behavior policy to change slowly as long as it remained sufficiently exploratory, but it is already a significant step. Our results build on ideas from prior work with gradient TD methods but require substantially different techniques to deal with the control case. We present a new latent learning algorithm, GreedyGQ, which possesses a number of properties that we find desirable: 1) Linear function approximation; 2) No restriction on the features used; 3) Online, incremental, with memory and per-time-step computation costs that are linear in the number of features; and 4) Convergent to a local optimum or equilibrium point. Alternative ways of solving the latent learning problem include using non-incremental methods that are more computationally expensive (e.g., Lagoudakis & Parr, 2003; Antos et al., 2008; 2007), possibly with nonlinear value function approximation methods (e.g., Antos et al., 2008; 2007); putting restrictions on the linear function approximation method (Gordon, 1995; Szepesv´ari & Smart, 2004), or on the interaction of

Toward Off-Policy Learning Control with Function Approximation

the sample and the features (Melo et al., 2008). Nonincremental methods that allow non-linear value function approximation are an interesting alternative. Because they are non-incremental, there are no stability issues arising. The price is that their computational complexity is harder to control. For a discussion of the relative merits of (non-)incremental methods the reader is referred to Section 2.2.3 of (Szepesv´ari, 2009). Previous theoretical attempts to construct incremental methods with the above properties include that of (Szepesv´ari & Smart, 2004) and (Melo et al., 2008), which also discuss relevant prior literature. The first of these works suggests to use interpolative function approximation techniques (restricting the features), the second work proves convergence only in the case when the sample distribution and the features are matched in some sense. Both works prove convergence to a fixed point of a suitably defined operator. In contrast, our algorithm is not restricted in the choice of the features. However, we are able to prove only convergence to the equilibria of a suitably defined cost function. The cost function that our algorithm attempts to minimize is the projected Bellman error (Sutton et al., 2009a) which is extended to the control setting in this paper.
1. The learning problem
We assume that the reader is familiar with basic concepts of MDPs (for a refreshment of these concepts, we refer the reader to Sutton & Barto (1998)). The purpose of this section is to define the learning problem and to define our notation. We consider the following latent learning scenario: An agent interacts with its environment. The interaction results in a sequence S0, A0, R1, S1, A1, . . . of random variables, where for t  0, St  S are states, At  A are actions, Rt+1  R are rewards.1 Fix t  0 and let Ht = (S0, A0, R1, . . . , St) be the history up to time t. It is assumed that a fixed behavior policy b is used to generate the actions: At  b(·|St), independently of the history Ht given St. Thus, here for any s  S, b(·|s) is a probability distribution over A. It is also assumed that (St+1, Rt+1)  P (·, ·|St, At), independently of Ht given St, At. Here P is the joint nextstate and reward distribution kernel. For simplicity, we assume that (St, At) is in its steady-state and we use µ to denote the underlying distribution. The goal of the agent is to learn an optimal policy for the MDP, M = (S, A, P ), with respect to the total expected discounted reward criterion. The optimal action-value function under this criterion shall be de-
1To avoid measurability issues assume that S, A are at most countably infinite. However, the results extend to more general spaces with some additional assumptions.

noted by Q. As it is well known, acting greedily w.r.t. Q leads to an optimal policy. Remember that a policy  is greedy w.r.t. an action-value function Q if for every state s,  selects an action (possibly random) amongst the maximizers of Q(s, ·). The Bellman operator acting on action-value functions underlying a stationary policy  shall be denoted by T , and is defined by T Q (s, a) =  {r(s, a, s) + Q(s, b)} (db|s)PS (ds|s, a), where r(s, a, s) is the expected immediate reward of transition (s, a, s), PS (·|s, a) is the next-state distribution (the marginal of P (·, ·|s, a)) and we are slightly abusing notation by using integral signs to denote both sums and integrals, depending on whether the respective spaces are discrete or continuous.
2. Derivation of Greedy-GQ
The purpose of this section is to derive the new algorithm. We use linear value function approximation of the form Q(s, a) = (s, a), (s, a)  S × A, to approximate Q. Here (s, a)  Rd are the features,   Rd are the parameters to be tuned. We also employ a class of stationary policies, (;   Rd). For each   Rd,  is a stationary policy (possibly stochastic). We will use (·|s) to denote the actionselection probability distribution chosen by  at state s. Two choices of particular interest are the greedy class and the (truncated) Gibbs class: For the greedy class, for any   Rd, (·|s) is a greedy policy w.r.t. Q. For the Gibbs class, the set A is assumed to be countable and (a|s)  e(Q(s,a)), where (e.g.) (x) = c/(1 + exp(-x)) with some c > 0. The main idea of the algorithm is to minimize the projected Bellman error
J () = T  Q - Qµ2 elauirQrnsagientmaogµ2rrin(=sawfpphapFcircQeoh2QxF^(ipsm-r,=oaafjt)eeµc)µ(Qtd.sstaoa,:ccdhtsiao)snta-inRvcdadgluraedwifis.uernna.ttc.ptdiroeosn·jceseµcn:itntio.tonHQ^otehpr=eeWe aim at an algorithm that works both in the case when (;   Rd) is the greedy class, or when (;   Rd) is a smooth class. Note that in the former case  is non-differentiable w.r.t. , implying the lack of differentiability of J. In this case we will use subdifferentials and our method becomes an approximate stochastic subgradient method. The motivation to minimize J is twofold: (Approximate) gradient descent lets us avoid divergence issues. When () is the greedy class, T  Q = T Q,

Toward Off-Policy Learning Control with Function Approximation

where T  is the Bellman optimality operator acting on action-value functions. It can be shown that if Q-learning converged, then it would converge to the solution of T Q = Q, which defines the global optima to J. Further, J has no other global maxima. Thus, if our algorithm converged to a global maximizer of J then the limit would be the same as the limit that Q-learning would choose. We note in passing that although our objective function resembles that of (Sutton et al., 2009a) and we use some ideas of this previous work, our problem and techniques are substantially different from those of (Sutton et al., 2009a) (and similar other works), who deal with prediction problems only, while we focus on control learning.

Since we will deal with non-differentiable functions, we have to work with sub-gradients. The sub-gradient of a non-convex function is defined as:

Definition 1. (Fr´echet sub-gradient): The Fr´echet sub-gradient of f : Rd  R, at x  Rd, denoted by f (x) is the set of all u  Rd such that

lim

inf

h-1

 f

(x

+ h) - f (x)

- hu



0.

h0 h= 0

Although when the greedy policy class is used, J() is not differentiable, it is still a piece-wise quadratic, continuous function which is differentiable everywhere except the boundaries between the regions defining the pieces (J is not convex, unfortunately). In order to derive a gradient for J, we notice that we can rewrite J as 2

J () = E[t+1()t]E[tt]-1E[t+1()t], where t = (St, At) is the feature at time t,

t+1() = Rt+1 +  V t+1() - t

is the temporal difference error, and V t+1() = V (St+1) is the expected value of the next state under : 

V (s) = (s, a)(da|s).

(1)

Due to the chain-rule of subdifferentials (e.g., Kruger, 2003), it follows that if ^t+1() is an unbiased estimate of the subgradient of V t+1() (given St+1), then bt+1() = ^t+1() - t is a subdifferential to t+1() and thus

E[ bt+1()t

]

E

 t

t

-1

E[ t+1()t

]

=

= -E[ t+1()t ] +  E[ ^t+1()t ] w()

2The derivation of this follows identical steps to the derivation of the analogous identity derived for prediction problems earlier and is thus omitted. The interested reader is referred to e.g. (Sutton et al., 2009a) for the details.

is

a

subdifferential

to

1 2

J

().

Here,

w()

=

E

 t

t

-1

E[t+1()t].

Making use of the weight-doubling trick of Sutton et al.

(2009b), we introduce a new set of weights wt  Rd to estimate w(t). The update equations, which aim at following a negated subgradient to J(·), then become

t+1 wt+1

= =

t + t wt + t

tt++11((tt))-t -tw(wtttt

)^t+1(t), ,

(2) (3)

which define our algorithm Greedy-GQ.

Note that if the greedy class is used, an appropriate tcAhhtio+si1cheoislfdossromf^otle+lom1w(asxtf)irmoismiz^itnth+ge1a(dcettfi)ion=nitioof(nSQot+fts1(u,SAbt+dt+i1ff,1e·))r,.ewnTthiheaarlest immediately (see, e.g., Kruger 2003).

When V (s)

=(a|s)[(iss,

and V t+1() =

differentiable a) +VQ((Sst,+a1))

w.r.t.  ln (a|s)]  , i.e., the

then (da|s)
subd-

ifferential set is a singleton. Note that when

the action set is large, the algorithm can just

sample (St+1,  (a|s)

A=Att++11)ln+Q(a(|Sst)t(+·i|1sS,ttA+ht1e+)1s)oa-ncda(lAleutd+se1s|cSo^tr+te+11)f,(unwtc)htieor=ne

underlying the policy .

Greedy-GQ uses an update-rule for parameter  analogous to that of Q-learning with function approximation except that we have a correction term. The update of the second set of weights, wt, follows the least mean square (LMS) rule. These weights are normally initialized to zero. As promised, the computation of an update takes linear time in the dimension of the features, d.

The update rules of Greedy-GQ are similar to GQ() with  = 0 (Maei & Sutton, 2010). However, GQ() is restricted to prediction problems, whereas the present paper considers control learning.

3. Convergence analysis
We prove our results under the following conditions. The first set of conditions concerns the data ((St, At, Rt+1); t  0).
(M1) (St+1, Rt+1)  P (·, ·|St, At); (M2) R^max s.t. Var [Rt+1|St]  R^max holds almost
surely (a.s.); (M3) At  b(·|St); (M4) The Markov process ((St, At); t  0) is in steady-
state.3 3 Note that (M4) could be replaced by a weaker con-

Toward Off-Policy Learning Control with Function Approximation

We also make the following assumption on the features  : S × A  Rd:

(P1)

max =  < +;

(P2) The matrix C = E tt  is non-singular.

For s  S, let (·|s) be a probability distribution. We assume that ^t+1() is an unbiased estimate of the subgradient of V t+1() = V (St+1) (cf. (1) for the definition of V ):

(G1)

E [^t+1()|St+1]  V t+1().

This is a technical condition that we believe can be relaxed. It is used only in the proof of the boundedness of the iterates. Condition (L3) is similar to the feature-independence condition. If it is not satisfied, the equilibrium set of J in fact can be unbounded (which does not affect value convergence, but affects the boundedness of parameters). Now, write the algorithm in the form

where

t+1 = t + t Gt+1(t, wt), wt+1 = wt + t Ht+1(t, wt),

(4a) (4b)

We need the following additional assumption:

(B1)

Tbohuendseedco: nsdupmoRmd Eent^ot+f 1(^t)+12(<)

is uniformly +.

Under this condition and (P1), it immediately follows that the norm of the matrix
B() = E ^t+1()t 
is uniformly bounded, too. Note that when the greedy policy is used this assumption is automatically satisfied under (P1). When the policy  is differentiable then it will be satisfied under (P1) provided that supRd,(s,a)S×A  log (a|s) < + also holds. We also need the following assumption on the limiting behavior of the parametric family :

Gt+1(, w) = t+1()t - ^t+1()t w, Ht+1(, w) = t+1()t - ttw,
t+1() = Rt+1 +  V t+1() - t, t = (St, At).
We will use the following assumptions on the step-size sequences:

(S1) t, t > 0 t and are deterministic;

(S2)


t=0

t

=


t=0

t

= +;

(S3) t=0(t2 + t2) < +;

(S4) t/t  0.

The last assumption puts the update into the class of two timescale stochastic approximation algorithms.

(L1) For any , the policy () defined by

()(a|s)

=

lim
c

c (a|s),

(s, a)  S × A

exists and the convergence is uniform on compact sets.
 (L2) The set L = () :   Rd is finite.

Define the mean update directions g(, w) = E [Gt+1(, w)] and h(, w) = E [Ht+1(, w)] 4 and the noise sequences Vt+1 = Gt+1(t, wt) - g(t, wt), Ut+1 = Ht+1(t, wt) - h(t, wt), t  0. With these choices the algorithm takes the form

t+1 wt+1

= =

wtt++ttgh((tt,,wwtt))++VUt+t+11,.

(L3) CTh-em atr(isce, sb)(s, a)(db|s)PS (ds|s, a)µ(ds, da) are non-singular, for any   L.
Condition (L1) is satisfied for the typical choices of policy classes. Note that if  is the greedy policy then (L1) is automatically satisfied. Condition (L2) will be naturally satisfied in finite state-action MDPs.
dition on the Harris recurrence of this Markov process. The modifications to our analysis would be standard (Szepesv´ari & Smart (2004) used this condition in a similar context). The reason for relying on (M4) is to keep matters relatively simple.

We need results on such stochastic approximation algorithms when the mean update direction is discontinuous because g depends on E [^t+1()|St = s, At = a], which might be a discontinuous function of  (for (s, a)  S × A fixed). These results are listed in Appendix A. The main result of this paper is the following theorem: Theorem 1. Under the conditions listed in this section the iterates updated by Greedy-GQ stay bounded. Further, t converges to M0 = {  : 0  J() } with probability one.
4These are well defined thanks to (M4).

Toward Off-Policy Learning Control with Function Approximation

The plan of the analysis of the algorithm is as follows: We make the working hypothesis that the parameters updated by the algorithm remain bounded almost surely:

sup(t + wt) < +, a.s.
t

(6)

Then, under this assumption we show that the limiting behavior of the iterates can be reduced to that of an appropriately defined differential equation. Next, we study the limiting behavior of this differential equation. The analysis is finished by showing that (6) indeed holds. Note that by assuming further structure on J (i.e., when () is the greedy class) and that the "noise" is sufficiently rich (i.e., it "excites" every direction), one can show that t will converge to local minima of J.5

In what follows we will always assume that (M1)­ (M4), (P1)­(P2), (G1), (L1)­(L3), (B1), (S1)­(S4) hold, so these conditions will be omitted from the results that follow.

3.1. Convergence to an invariant set We have the following result: Proposition 2. Under (6), we have
(t, wt)  { (, w()) :   M } , a.s.
Here the set M = M ()  Rd is a possibly random set for which it holds almost surely that it is a compact, connected invariant set to the differential inclusion (t)  J().

Proof. We apply Theorem 5, identifying the master equation with the update of t and the slave equation with the update of wt. We need to verify that the conditions (D5­1)­(D5­3), (S5­1)-(S5­3) and (A5­1) of Theorem 5 hold.

CbFor=-this,r(nso(,stae,,fibs)r)st((stsh, ,aaa)t)hP(S,((wddsb)||s=s,)bPa-)SµA((dd(ss)|,sd,-aa)C),µw(Ad,(sw,h)dear=)e, and C was defined in (P2). Here µ denotes the stationary distribution underlying (St, At).

Now, let us verify if Condition (D5­2), which is a linear

growth condition, holds for h. We have h(, w) 

b+A() +Cw. Thus, the condition follows

since A() Further, for 

fixesudp, h(A,(w))is<Lip+sch,ittzhwanitkhs

to (P1). Lipschitz

constant C. Hence, it satisfies (D5­3).

5 Such assumptions are in fact necessary in the analysis of stochastic gradient descent when the objective function is non-convex (cf. Section 4.3 "Avoidance of traps" of (Borkar, 2008)). Note that the standard way to deal with this is to add noise to the updates, which would also work in our case.

With the help of b, A() and B(), g can be written as g(, w) = b - A() - B()w. We see that the growth condition (D5­1) is satisfied thanks to Assumption (B1).

Now, (S5­1) is verified thanks to (S1)­

(S4). To verify (S5­2), note that if Ft =

(s, ws; s  t) then, thanks to their construc-

tions, E [Vt+1|Ft] = 0, E [Ut+1|Ft] = 0. Finally,

(S5­3) is verified as follows: Gt+1(, w) 

|t+1()| t + ^t+1() w t



max (|t+1()| + ^t+1() w), where we

used (P1). Thanks to the definition of t+1(),

|t+1()|  |Rt+1| + |V t+1()| +  t. From the

definition of V (s), we also get V (s)  max. Hence, |t+1()|  |Rt+1| + 2max. By chaint2Ti(nhagakm2ti2i=anEx1tgh(a|eeiRx)G2pt+tie+n1ce1|t2q(aut2tia+o,lwnitsti2i4)e=as1n22mda|aF2ioxu,btstiwanige2neK(gdM+e(t21)a+na^Gdntt+d+1t1(t(B(h2)e1,+n)w2, )wwwue2stig2n2e)gt). with a suitable constant K > 0.

It remains to check (A5­1), i.e., if w (t) = h(, w(t))

admits a unique, globally asymptotically stable equi-

librium, w(), given any fixed value of . Since C

is a positive definite matrix, this is immediate. Fur-

ther, w() = C-1(b - A()), where C-1 exists

thanks to (P2). It is immediate that w(·) is a Lip-

schitz continuous function since, as discussed before,

sthuepcoAnd(it)ion<s

+. This of Theorem

finishes 5.

the

verification

of

Thus, we conclude that there exists a set M =

M ()  Rd, which is (almost surely) a compact, con-

nected invariant set to (t) = g((t), w((t))) and

(t, wt)  { (, w()) :   M }, a.s. Since by con-

struction g(, w())  the statement follows.

-

1 2

 J ()

holds

for

any





Rd,

3.2. The study of the invariant set tMPorotihpseoasdiitsffiuoebrnseent3ti.oaflLtiehntecMlusseitobenofas(ttba)otuionndae-dry21inpJvoa(inr)ita.snTSt hsee=nt {  : 0  J() } to J.

Proof. The statement is immediate when J is differentiable. When J is not differentiable, the solutions are defined in the sense of Filippov (1988) and a more careful analysis is needed. This is however omitted due to the lack of space.
3.3. Boundedness Proposition 4. The iterates remain bounded, that is (6) holds.

Toward Off-Policy Learning Control with Function Approximation

Proof. We use Theorem 6. Since we have already verified the conditions of Theorem 5, it remains to show that the extra conditions of Theorem 6 hold.

Consider the function g. We need to show the existence of g such that

lim
c

g(c, w(c)) c



g()

where the convergence is uniform. We also need that g() is such that zero is the unique global exponentially stable equilibrium to the differential inclusion

  g().

(7)

We cJ

know (c),

that f (, c-1 f (c,

w())  w(c))

- =

12c-J2(J)(.cS)in. ceUsJin(gc)th=e

definition of J, we get

 J (c) c2

=

 

b c

- A(c) 

2 .
C -1

(8)

Let Note that

A()

=

lim
c

A(c).

(9)

A()=(sC, b-)(s, a)()(db|s)PS (ds|s, a)µ(ds, da) exists and the convergence in (9) is uniform on compact sets thanks to (L1). Now, take the limit of c   in (8). Thanks to (L2), the interchange of limit and subdifferentials is justified and we have

lim
c

 J (c) c2

=

 A() C2 -1

= co { N () : N () = limt N() } ,

where N() = 2A()A(). Note that

A {N1

,

is ..

.

,pNieKce}w=ise

constant, N() :



henRcde

so is N. and partition

Let Rd

into non-overlapping regions Ri, i = 1, . . . , K, such

that Ni = N() for all   Ri.

Notice that the matrices Ni are all normal. Further,

they some

are positive nonsingular

definite, because Ni matrix Mi, thanks to

(=L3M). iFuMrtihfeorr,

by definition, the solutions to (7) are exactly the same

as that of the switched linear system with dynamics

 = - K I{Ri}Ni.
i=1

(10)

Thus, it suffices to study the latter system. By Lemma 2 of (Zhai et al., 2006),  > 0 s.t.

Ni + Ni  2I,

(11)

holds for i = 1, . . . , K. Consider the Lyapunov func-

tion candidate to (10). Take t

V () such

=

1 2

that

. Let (t) (t) exists. By

be a solution the definition

oHfeiFn:ci(leti)p,pRoivµsio(tl)ut=ion1sa, ntdher(et)ex=ists iµ:i((tt))Ri

0, µi

such (t)Ni

that (t).

V

=

1 2

 (t)

(t)

+

(t)(t)

=

-

1 2



µi(t) (t)Ni(t) + (t)Ni(t)

i:(t)Ri

 -(t)2,

where the last inequality follows from (11). Hence, V is a Lyapunov function to (10). From (10) it is clear that zero is the only equilibrium point. Further, because V (t)  -2V (t), integrating both sides yields (t)2 = 2V (t)  C exp(-2t) for some C > 0. Therefore, zero is the unique globally exponentially asymptotically stable equilibrium to (10) and thus also to (7).

Hence, we have verified all the conditions of Theorem 6 and it follows that the parameters stay uniformly bounded with probability one.

 J

4. Solving Baird's counterexample on Q-learning
In this section, we illustrate the convergence result of Greedy-GQ on a well known off-policy example; Baird's counterexample (Baird, 1995), for which Qlearning diverges. This has been demonstrated in Fig.1. Here, we have used the 7-star version of the
20
15
Qïlearning
10
5 GreedyïGQ
0 5 10 15 20 25 30 35 40 45 50
Sweeps
Figure 1. Empirical illustration for Baird's counterexample. The graph shows that Greedy-GQ converges to the true solution, while Q-learning diverges. "star" counterexample. The MDP consists of 7 states and 2 actions for each state. The reward is always zero and the discount factor is  = 0.99. In this problem, the true action value is zero for all state­ action pairs. The initial value of  parameters for the action that causes transition to the 7th state is (1, 1, 1, 1, 1, 1, 10, 1) and the rest are 1. The initial values for auxiliary weights w were set to zero. Updating

Toward Off-Policy Learning Control with Function Approximation

was done synchronously in dynamic-programming-like

sweeps through the state-action space. The step-size

parameter  = 0.1 was used for Q-learning, and for

Ghorweedthye-GpQerwfoermusaendcem=ea0s.u05re, ,=J

0.25. Fig.1 shows , evolves with re-

spect to the number of updates. Both algorithms used

expected updates. The graph shows that Greedy-GQ

finds the optimal weights, while Q-learning diverges.

Here, the choice of step-sizes goes beyond our theoretical conditions, testifying that our results are robust beyond what we can prove. For ,  converging to zero according to our theorem statement, the graphs would not differ in their behavior from the one that we presented here.

A. Results on stochastic approximation
The results here are extensions of various results in (Borkar, 2008) and can be proved using the same techniques as developed there. For brevity, the proofs of these technical results are omitted. The first result is an extension of previous two timescale stochastic approximation results where the update functions on the right-hand side (RHS) might be discontinuous. Consider the stochastic approximation algorithm
 xn+1 = xn + a(n) h(xn, yn) + Mn(1+)1 , (12a)
 yn+1 = yn + b(n) g(xn, yn) + Mn(2+)1 , (12b)

5. Conclusions and future work

where xn  Rd, yn  Rk and x0  Rd, y0  Rk are fixed (non-random), (a(n), b(n); n  0) are step-size

In this paper we have made significant progress to- sequences, h, g : Rd × Rk  Rd are possibly discon-

ward solving a long-standing open problem in reinforcement learning: the problem of off-policy learn-

tinuous, functions. As before, (Mn(1), Mn(2); n  1) is a noise sequence.

ing control. Our new algorithm, Greedy-GQ, achieves the four desirable properties identified in the introduction (linear approximation, unrestricted features, an online, incremental, linear-complexity implementation, and convergence to an optimum or equilibria) in the latent-learning setting. On the other hand, our result is limited in several ways. First, we focused on the case when the behavior policy is fixed. Although this is an important case, better performance can be expected if one is allowed to actively change the way the data is sampled. Next, the algorithm might converge to local optima. This follows from the nature of the objective function considered. Unfortunately, convergence to local optima might make it difficult to derive performance bounds on the resulting policy. Nevertheless, we think that the approach considered here is a significant step towards a practical, incremental algorithm to learn a good control policy in the difficult off-policy setting. Our future plans involve extensive testing of the algorithm on various test domains and its possible extensions to prevent convergence to local minima and to handle the case when the behavior policy is allowed to change sufficiently slowly.

We shall assume that b(n) = o(a(n)), separating the speed at which xn is updated from that of the update of yn, making the algorithm a two timescale algorithm. In fact, because of this assumption the update of yn is much smaller than the update of xn. In the limit, we can think of that by the time yn is updated xn has already converged. For this reason, the update equation for yn is called the master update equation, while the update for xn is called the slave update equation. Analogously, yn (xn) is called the master (resp., slave) parameter. The above intuition suggests that if for any fixed value of yn = y the slave equation converges to some point (y) fast enough then it will be sufficient to analyze the ordinary differential equation y = g((y), y) to understand the limiting behavior of yn. The following theorem makes this intuition precise. We shall need the following notation for this theorem: Let f : Rp  Rq. Then, for x  Rp let Limf (x) = >0co ({f (x) | x - x < }) be the closed convex set spanned by the limit-values of f at x. Here co(H) denotes the closed convex hull of set H  Rd.

Theorem 5. Consider the coupled equations (12a)­

Acknowledgements

(12b). For (x, y)  Rd ×Rk, let G(x, y) = Limg(x,·)(y), H(x, y) = Limh(·,y)(x). Let the following assumptions

The authors gratefully acknowledge the insights and hold: K > 0 s.t. for all (x, y)  Rd × Rk,

assistance they have received from Doina Precup, Eric

Wiewiora, and David Silver. They also thank the (D5­1) anonymous reviewers for their helpful comments. This

supgG(x,y) g  K(1 + x + y);

research was supported by iCORE and Alberta In- (D5­2) genuity, both part of Alberta Innovates ­Technology

suphH(x,y) h  K(1 + x + y);

Futures, NSERC, MITACS and the PASCAL2 Net- (D5­3) h is Lipschitz in its first argument, uniformly

work of Excellence under EC grant no. 216886. Cs.

w.r.t. the second.

Szepesva´ri is on leave from MTA SZTAKI.

Further, assume that the step-size and noise sequences

satisfy:

Toward Off-Policy Learning Control with Function Approximation

(S5­1)


n=0

a(n)

=


n=0

b(n)

=

,

n=0(a(n)2 +

b(n)2)

<

+,

b(n) a(n)



0,

n





and

(a(n)),

(b(n)) are eventually decreasing;

 (S5­2) for all n  0, i = 1, 2, E Mn(i+)1|Fn = 0, where

Fn = (xm, ym, Mm(1), Mm(2); m  n);

(S5­3) K  > 0 s.t. for all n  0, i = 1, 2, E Mn(i+) 12|Fn  K(1 + xn2 + yn2).

Antos, A., Szepesva´ri, Cs., and Munos, R. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89­129, April 2008.
Baird, L. C. Residual algorithms: Reinforcement learning with function approximation. In Prieditis & Russell (1995), pp. 30­37.
Borkar, V. S. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.
Filippov, A.F. Differential equations with discontinuous righthand sides. Kluwer Academic Press, 1988.

In addition, assume that

(A5­1) there exists a Lipschitz map  : Rk  Rd such that for any y  Rk, (y) is the globally asymptotically (uniformly) stable equilibrium to x (t) = h(x(t), y).

Tthhaetnthuenredeerxissutspna((rxannd+om,yin.e).,

< +, a.s., it holds path-dependent) sub-

set M of Rd such that with probability one M is a

compact, connected and internally chain transitive in-

variant set to the differential equation

y = g((y), y),

(13)

such that (xn, yn)  M^ = { ((y), y) : y  M } a.s.
Note that M^ is a random set. Also, the RHS of (13) is possibly discontinuous. When this is the case then the solutions are understood in the Filippov sense (see Filippov 1988). The final general result concerns the boundedness of the iterates of two timescale algorithms. Theorem 6. Consider the update equations (12a)­ (12b). Assume that in addition to the conditions (D5­ 1)­(D5­3), (S5­1)-(S5­3), (A5­1) of Theorem 5,

1. G : Rk  2Rk s.t. for any y  Rk, G(y) is closed convex and limc infyG(y) y - ogn((ccocym),cpya)cta=; 0 and the convergence is uniform
2. Zero is the unique, globally asymptotically exponentially stable equilibrium to
y  G(y).
Then, supn(xn + yn) < + holds almost surely.

Gordon, G. J. Stable function approximation in dynamic programming. In Prieditis & Russell (1995), pp. 261­ 268.
Kruger, A.Ya. On Fr´echet subdifferentials. J. of Math. Sciences, 116:3325­3558, 2003.
Lagoudakis, M. and Parr, R. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107­ 1149, 2003.
Maei, H. R. and Sutton, R. S. GQ(): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Baum, E., Hutter, M., and Kitzelmann, E. (eds.), AGI 2010, pp. 91­96. Atlantis Press, 2010.
Maei, H.R., Szepesva´ri, Cs., Bhatnagar, S., Silver, D., Precup, D., and Sutton, R.S. Convergent temporaldifference learning with arbitrary smooth function approximation. In NIPS-22, pp. 1204­1212, 2010.
Melo, F. S., Meyn, S. P., and Ribeiro, M. I. An analysis of reinforcement learning with function approximation. In Cohen, W. W., McCallum, A., and Roweis, S. T. (eds.), ICML 2008, pp. 664­671. ACM, 2008.
Prieditis, A. and Russell, S.J. (eds.). ICML 1995, 1995. IMLS, Morgan Kaufmann.
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. Bradford Book. MIT Press, 1998.
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesva´ri, Cs., and Wiewiora, E. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Bottou, L. and Littman, M. (eds.), ICML 2009, pp. 993--1000. ACM, 2009a.
Sutton, R. S., Szepesva´ri, Cs., and Maei, H. R. A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L. (eds.), NIPS-21, pp. 1609­1616. MIT Press, 2009b.
Szepesva´ri, Cs. Reinforcement learning algorithms for MDPs ­ a survey. Technical Report TR09-13, Department of Computing Science, University of Alberta, 2009.
Szepesva´ri, Cs. and Smart, W. D. Interpolation-based Qlearning. In Brodley, Carla E. (ed.), ICML 2004, pp. 791­798. ACM, 2004.

References

Watkins, C. J. C. H. and Dayan, P. Q-learning. Machine Learning, 3(8):279­292, 1992.

Antos, A., Munos, R., and Szepesva´ri, Cs. Fitted Qiteration in continuous action-space MDPs. In NIPS-20, pp. 9­16. MIT Press, 2007.

Zhai, G., Xu, X., Lin, H., and Michel, A. Analysis and design of switched normal systems. Nonlinear Analysis, 65:2248--2259, 2006.

