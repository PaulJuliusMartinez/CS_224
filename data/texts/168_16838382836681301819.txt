CS 124 .i MATRIX DECOMPOSITIONS AND STATISTICAL CALCULATIONS
GENE H. GOLUB
TECHN ICAL REPORT NO. CS 124 MARCH 10, 1969
COMPUTER SCIENCE DEPARTMENT School of Humanities and Sciences STANFORD UNIVERS ITY

v
i
LI r L
!
IL
L Matrix Decompositions and
L Statistical Calculations*
L L L BY L Gene H. Golub 1 L Computer Science Department L\ Stanford University L
L Reproduction in whole or in part is permitted for any purpose of the United States Government.
L *Invited paper to be presented at the Conference on Statistical Computation, University of Wisconsin, Madison, Wisconsin, April 28-30, 1969* This work
L was in part supported by the National Science Foundation and Office of Naval Research.
t

Several matrix decompositions which are of some interest in statistical calculations are presented. An accurate method for calculating the canonical correlation is given.
i i-

Table of Contents

Section

Page

0. Introduction

. . . . . . . .l l

l lllllllololl

1

1. Cholesky

decomposition..

a .l l l l e l l l

ollllll

2

2. Accuracy of the Cholesky decomposition 0 0l l l l l l l l

4

3. Solution

Of linear

equations

0 0 0 0 0 0l l l l l l 0

le

6

4.

Conditioning

of

matrices

. . .l l

lllllll

lllll

9

5e Iterative

refinement

0 . 0 . . . 0l l

l

lllll

ll0

l

l6e Partial correlations .l l

- . . . . . 0l l l l l l l

0e

7e Least

squares

0 0 0l l l l l l l

llllllll

llll

8e A

matrix

decomposition

. o 0 0l l l l l

llllll

l

ll

11
13 15 17

90 Statistical

calculations

0 . k .l l l l l l l l l l

l le

10. Gram-Schmidt

orthogonalization

.l l l l l l

ellllel

21
25

II.

Sensitivity

of

the

solution

. . .l l l l l l l

lll

ll

12. Iterative refinement for least squares problems l l l l l

27 31

13. Singular 14, Singular

SySteIIlS 0 l 0 l l l l l l l l l l l e l l l l l l
value decomposition l l l l l l l l l l l l l l l

34 36

150

Applications

of

the

SVD

. .l l l l l l

llllll

lll

38

16, Calculationofthe

SVD

.l l l l l

llllllllllll

42

i

L 17*

Canonical

correlations

. . . .e-e

ll

llll

llllll

44

Acknowledgements 461 l .e l l l l l l l l l l l l l l l l l l l

i

References . . 47L

llllllllllllllllllll

le

1

0. Introduction With the advent of modern digital computers, many of the well
known hand calculator methods for making statistical calculations have been revised. For example, Hotelling [19J proposed a number of methods for solving matrix problems. Yet today almost none of these methods are in current use. In this paper, we shall present several well known matrix decompositions and show their relevance to statistical calculation. Some of the properties of the numerical algorithms shall be discussed.
1

1. Cholesky decomposition
Let A be a real, symmetric, positive definite matrix of order n . It is well known that we may factor A so that

A = RTR

(1.1)

where R is an upper triangular matrix (7). The decomposition (1.1) is known as the Cholesky decomposition. The calculation of R may be performed in two ways.
a) Complete Cholesky Decomposition Algorithm (CCDA) Let

rll = bll)1'2 and r = a /r (j=2,...,n) .
13 lj 11 Then for i= 2,..., n,

r ii

=

( a ii

i-1 2 l/2 - gl 'li)

'

r ij =

aij

i-l -
_ Ll rkirkj)/rii(j=i+19

ll l

,n> l

b) Sequential Cholesky Decomposition Algorithm (SCDA)
i
i Let

L6

a

(1) ij

-= a ij

'

!.

Then for k = 1,2,..., n ,
i c
2

r& = ( `kwk 1 9 04rkj = akj / rkk 9

(Pk)

f

i, 3 =k+l,..., n.
I

(1,2)

Since the Cholesky decomposition is unique when rii > 0 , each of the algorithms produces the same R . Each of these methods require n3 /6 + C(n2) multiplications plus n square roots. The CCDA has the advantage that if the it-h row of the matrix R is being computed, then it is only necessary to have available the it-h row of the matrix
A, and the (i-l) previously computed rows of R . This is especially advantageous when the matrix A is so large that it is necessary to
store it in auxilliary storage.

3

2. Accuracy of the Cholesky decomposition
J. Wilkinson[311 has given an error analysis of the SCCA. He assumes that the error in the basic operations are as follows:

fr(aLb) = a(l+al) 2 b(l+Eg) fa(aXb) = ab(l+a3) fa(a/b) = (a/b)(l+~~)

I& iI -< 20t

where a mantissa of t binary digits is used. The notation f1(a 'op'b) indicates the result of the operation with two floating point numbers a and b when standard floating point arithmetic is used. Furthermore, it is assumed that if

x = fl(sqrt a)

then
X2 = a(l+a) with 1~1 5 2 X 2 -5
i
where

When the SCDA is used, arithmetic errors are introduced at each
stage of the calculation. Indeed it is possible that for some positive
! definite matrices it will be impossible to complete the algorithm because i
4

--

of the roundoff error. Wilkinson has given sufficient conditions for

which it is possible to complete the SCDA. In addition, he has shown

that the computed Cholesky decomposition will be exact for some

perturbed matrix A + E . Let II...II 2 indicate the spectral norm,

and let E be the computed Cholesky factor.

/

Theorem 1. (Wilkinson): If A is a positive definite matrix of order n > 10 , then provided

h m.in(A) >- 20n-3/22-t1jlA/j2

the Cholesky factor E can be computed without breakdown and the computed E satisfies the relation
ETE = A + E ,
11311, 5 2.5n 3'22-tljjAil, .

Thus the relative perturbation viz l/~ll,/ll~ll 2 ' is but a few units of the mantissa for the Cholesky factorization. The above result is
independent of the choice of pivots.

- - -- - -- -

1

3. Solution of linear equations
Given the Cholesky decomposition, it is a simple matter to solve a system of linear equations or to compute the inverse. To solve Ax,=;, the most convenient procedure is to first solve

RTx =k

(3.1)

e and then solve

Since R is \1 and RT is I\, this requires a total of n2 + O(n)

multiplications. To compute the inverse, compute R-l which is \I

and then compute R-lR-T , taking advantage of the triangular form

-1 of R

and the symmetry of

A-1 ;

this requires

n2 + O(n2)

7

multiplications. Thus to invert a positive definite matrix requires

n*/2 + O(n 2) multiplications which is fewer multiplications than

multiplying two matrices by the usual algorithm! Because of the roundoff error, equations (3.1) and (3.2) can be

replaced by a perturbed system of equations. Thus, in reality we have

(-R T+Y6R? )u =k

I
and

(R+Gi)z = u ,
6

and this implies that

where

&A

-Jr -T =E+6R XE+R

x61i+6$?&.

Using Theorem 1 and Wilkinson's bounds for solving triangular systems [3OtPg 991, it can be shown that

IbAil 2 < 5n3/22-tl
lrAll,-

(3 03)

when the conditions of Theorem 1 are satisfied.
' ii . The bound given by (3.3) is quite gross but it does indicate that
solving equations using the Cholesky decomposition leads to a relatively
small perturbation in the original data. Note also that we can determine
I a bound on the residual vector ; = 2 - .AZ Since = 2 ,(A+SA)z
rE = 6A2 and thus
Ic
I llgll, 5 5n3'22dt111Al12 . c
i Of course, if the norm of the residual vector is small it is not true
i
an accurate solution has been determined since

X

-

z

-1 =A 2

7

and hence

II&l, 5 llA-111211& .

It is possible to bound the norm of the relative error providing an upper bound for the condition nwllber K. (A) = I(AI/,J(A-~I~~ is known. Since E-&=-A -1RAE, a shQrt manipulation shows that when

I L

(3 04)

1 where p = 116A(Id/1A112 l This bound is independent of the method used.

8
L

4. Conditioning of matrices
Since the bound (3.4) is dependent upon the 'condition number, it is frequently desirable to replace the original system of equations Ax, = 2 by a new system .

where D is a diagonal matrix with non-zero diagonal elements. Let *n be the set of all n X n diagonal matrices with non-zero diagonal elements. We wish to choose fi so that
~(fid) < K(DAD) for all DE&, .
A symmetric matrix is said to have Property & if there exists a permutation matrix II such that
nTArI =

where A eB

a n d p + q = n . All tri-diagonal matrices

1 P and A2E% -

have Property A. /
Let &On and {tlii = l/Jr . Forsythe and Straus [IO] have ii

shown that for matrices that possess Property A, 8 =fj . More

generally, for all positive definite matrices A , van der Sluis [$I

has shown that

K(h$) < nK(fi& l
9

(4.1)

Therefore in the absence of other information, it would appear that it is best to precondition the matrix A so that all the diagonal elements are equal, e.g. the covariance matrix should be replaced by the correlation matrix.
The problem of precondkioning symmetric positive definite matrices arise in the other statistical contexts (cf. [l$j).
10
I i i

5. Iterative refinement
Once an approximate solution to & = Nb has been obtained, it is frequently possible to improve the accuracy of the approximate solution. Let i be an approximate solution, and let r = 2 - 4 , Then if i`= & + 5 j i satisfies the equation

Ai = 2 .

(5 4

Equation (5.1) can be solved approximately once the Cholesky decomposition of A is known; indeed, it requires but n2 + O(n) multiplications
to solve for the correction i . Of course, it is not possible to solve
precisely for i so that the process may be repeated, Thus for NX (0) given, the algorithm proceeds as follows:
1) compute z(k) = B - 4 (k) ; 2) solve Ai04 = g(k) ; 3) compute 5 (k+Q = Hx(k) + Hg(k) . The process continues until

_ 11 25(k+ly I

<s'

a predetermined constant or some other criterion is satisfied. The above algorithm is known as iterative refinement and has been extensively discussed in the literature (Cfe [ 24, 3211) .
There are three sources of error in the process: (1) computation of the residual vector ; (k), (2) solutionof the system of equations

11

for the correction vector i04 j and (3) addition of the correction vector to the approximation Nx (k) . It is absolutely necessary to compute the. components of the residual vector using double precision inner products and then to round to single precision accuracy. The convergence of the iterative refinement process has been discussed in detail by Moler [2&l. Generally speaking, for a large class of matrices for k>-kg all components of -x (k) are the correctly rounded single precision approximations to the components of z . There are exceptions to this, however, (cf,[21]). Experimentally, it has been observed,
in most instances, that if 11_6 (O) ll,/ll~(") lr, 5 KP where I xIm= max IYil Y
l-<i<-n then k. 2 [t/p] . We shall return to the subject of iterative refinement when we discuss the solution of linear least squares problem.
12

i L

6. Partial Correlation

/L Again let A be a positive definite matrix and we partition the matrix in the following form:
1

1 A=

L where All is p X p , A22 is -q X q , and Al2 T= A2l . Suppose 1 the SCDA is used but the algorithm is stopped after p steps, Then

1

L

L here Rl is the Cholesky factor of 91 so that RTRl = Al1 .

Equating matrix blocks, we see

'

L -..

L Al2 = R$ LI A22 -= STS + w '.

L Thus

t W = A22 - A21!?-R;TA12
L -1
L = A22 - A21AllA12 l t 13

The matrix W 4.s denoted by a.. 1J
Consider the covariance matrix

in (1.2).

where xl1 is P X P l

The partial covariance matrix

when the first p variables are held fixed, and the regression function is defined by

k (2)

+I c -1(xo) _jto)
21 11 N

whe=k (1) ,j& (2) are the corresponding vectors of expected values.

(Id Thus if we apply the first p steps of the SCDA, oij

corresp' onds

to the partial covariance when the first p variables are eliminated.

We can eliminate the effect of the first (ptl) variables by simply

performing one more step of the SCDA. It is a simple matter to compute

the regression function since

corresponds to STRol 1l

14

7. Least squares

Let A be a given m X n real matrix of rank r and Nb a

given vector. We wish to determine $ such that

/

m n2

iC=L

bi -

c i=l

aijxj)

= min .

or using matrix notation

1Ik-+l/, = min .

(7.1)

If m >, n and r < n , then there is no unique solution. Under these conditions, we require amongst those vectors 5 which satisfy (7.1) that
II5AII2 = min .

I For r=n, $ satisfies the normal equations I c

ATA-?N = ATb .

(7.2)

Unfortunately, the matrix ATA is frequently ill-conditioned and influenced greatly by roundoff errors. The following example illustrates this well. Suppose

15

1111 E000 A= 0 E 0 0 00&0 000E

which is clearly of rank 4. Then

ATA =

1+e 2 1
1 1+E2

1 1

1

1 1 1+s2 1

m 1 1 1 1+s2

and the eigenvalues of ATA are 4+s 2, s2, E 2, E2 . Assume that the T
elements of A'A are'computed using double precision arithmetic, and
then rounded to single precision accuracy. Now if s < J2 -t ,

fe(ATA) =

m
1111 1111 1111 .1 1 1 1

a matrix of rank one, and consequently, no matter how accurate the linear equation solver it will be impossible to solve the normal equations (7.2).
Longley [23] has given examples in which the solution of the normal
equations leads to almost no digits of accuracy of the least squares problem.
16

8. A matrix decomposition NOW 11$1, = (E~J$-'~ so that k@112 = 11x11, when Q is an
orthogonal matrix, viz, QTQ = I . Thus
lip&II, = Ilk-Qpx,ll 2
where Nc=e and Q is an orthogonal matrix. We choose Q so that
Q,.A=R=
where ii is an upper triangular matrix. Let

then

Ill+& = (c -r1

llx1-r12x2

-...-

2 rlnxn)

+ (c2 -r2fZx2 -

l

**

-

2 '2nxn)

+

...

2 + ( cn-rmxn>

2 + cn+l + cn2+2 +

...+

cm2

'

/

17

Thus j(b-px,l(22 is minimized when

rll ii1 + r12xA2 + . . . + rlnhxn = cl

rS28 2 + . . . + r2Anxn = c2

..

rn' nAxn

=c n

i.e., $=E where

NF = ( cpcp . ..YCJ ,

and

llp$y~ = c;+l + c;+2 + m2. . . + c l
Then

RTR = (iT.:o)T(rii:.o) = @JaTuuJ = ATA ,

(8.3)

and thus iiri"ii is simply the Cholesky decomposition of ATA . There are a number of ways to achieve the decomposition (8.1);
e.g., one could apply a sequence of plane rotations to annihilate the elements below the diagonal of A . A very effective method to realize the decomposition (8.1) is via Householder transformations. A matrix P is said to be a Householder transformation if

18

P=I
Note that 1) P = PT and 2) pPT = I -2%T - 2uuT- + -4uu-Tuu=T I so
that P is a symmetric, orthogonal transformation. Let A (1) = A and let A c2), A(3) ,... Ah-l+') be defined as
follows:

where P w = chosen so that transformations

w(k)Twb) NN

1 . The matrix P (k) is

= . . . C a(n;Iy+k1) = 0 . Thus after k

(2) (2) . (2)

all al2 l .

l aln

0

(3) a22 l l

.

. (3)
a2n

0

.. .

.

Ack+l) = .

ag+1) .

. ak(nk+l)

2. 0
.0 ..

aLzY +l . .
..
. ..

0

0

(k+l)
am,k+l

.

The details of the computation are given in [5] and[13].

19

Clearly,

R = Ab+l)

and Q = PWpb1) . . . pm

although one need not compute Q- explicitly. The number of multiplica-

tions required to produce R is roughly mn2 - Fn3 whereas approximately

TM2 2

multiplications are required to form the noMna1 equations (7.2).

Ii 20

9. Statistical calculations

In many statistical calculations, it is necessary to compute certain auxiliary information associated with A TA. These can readily be obtained from the orthogonal decomposition. Thus
det(ATA) = (rll X r22 x . . . x rnn) 2 .
Since

ATA = Pr'Tw , (ADA)-' = fi-$YT

.

The inverse of fi can be readily obtained since # is an upper triangular matrix. It is possible to calculate (ATA)-' directly from '#. Let
(ATA> -1 = x = (zl, ~,*+g*
Then from the relationship

and by noting that {rTjii = l/rii , it is possible to compute wxn'2n-1y

c,
I

\. . .-.1X . The number of operations are roughly the same as in the first

i- method but more accurate bounds may be established for this method

\ provided all inner products are accumulated to double precision. L
I In some applications, the original set of observations are augmented
by an additional set of observations. In this case, it is not necessary

to begin the calculation from the beginning again if the method of ortho\i, c gonalization is used. Let fil,'N?l correspond to the original data after it
has been reduced by orthogonal transformations and let A2,s correspond to
i
4
the additional observations. Then the up-dated least squares solution can

be obtained directly from

21

A = [;),, b = @

This follows immediately from the fact that the product of two orthogonal transformations is an orthogonal transformation.
The above observation has another implication. One of the arguments frequently advanced for using normal equations is that only n(n+1)/2 memory locations are required. By partitioning the matrix A by rows, however, then similarly only n(n+1)/2 locations are needed when the method of orthogonalization is used.
In certain statistical applications, it is desirable to remove a row of the matrix A after the least squares solution has been obtained. This can be done in a very simple manner. Consider the matrix

AC!

and:=@

.

where NQ/ is the row of A which one wishes to remove, p is the corresponding element of Nb, and i = J-1. Note that

Let z l,n+l =

STS = RTR - rcyVTcNY = ATA - NcyTN,,
r cos 8 msin 8 01 .

0L '1

sin 8

-cos e

.

I

I

S (1) = S , and S (2)

(1)

i = `l,n+lS

l

22

We choose cos 8 so that {S (2) ]n+l 1 = 0. Thus Y

{SC')3 1,j =

j = 2,3,...,n

{SC2)j n+l,j

3 = 2,3,...,n .

Note no complex arithmetic is really necessary. The process is continued as follows: Let
k

n+l I I

zk,n+l L:
0
-

.
I
*I
II_ sih 8k

sin 0k
1' : .
1 0 Q . :I ' -CA 8k`
-

k n+l

Then
s b+1) = zk-,n+l S (k)

k = ly2,...,n ,

23

and cos 0 k is determined so that {S (k+l)]

0.

k,n+l =

Thus roughly 3n2

multiplications and divisions and n square roots are required to form

the new #.

Suppose it is desirable to add an additional variable so that the

matrix A is augmented by a vector 5 (say). The first n columns of Rn are unchanged. Now.one computes

Chv =P"( .> . . p(2)p(l) & .
From Nh one can compute P b+l> andapplyittoPn( > . . .P (1) Nb. It is also possible to drop one of the variables in a simple fashion
after x has been computed. For example, suppose we wish to drop variable 1, then

I0r12 rln
r22 I rnn nx (n-l)

By using plane rotations, similar to those given by (g.l), it is possible to reduce fiT to the triangular form again.

i
L 24

10. Gra@$chmidt orthogonalization

In section 8, it was shown that it is possible to write

QA=R*

(10.1)

The matrix Q is constructed as a product of Householder transformation.

From (lO.l), we see that

\
A = QTR =, PS

where PTP = I n'

sm

l

Each row of S and each column of P is uniquely

determined up to a scalar factor of modulus one. In order to avoid computing

square roots, we modify the algorithms so that S is an upper traingular

matrix with ones on the diagonal. Thus PTP = D, a diagonal matrix. The

calculation of P and $ may be calculated in two ways.

4 Classical Gram-Schmidt Algorithm (CGSA)

The elements of S are computed one column at a time. Let

and assume

T 24 2j

=

Gijdi

,

15 i,j < k-l.

At step k, we compute

1 -< i -< k-l

25

b) Modified Gram-Schmidt Algorithm (MGSA)
Here the elements of S are computed one row at a time. We define

and assume

T x-i zj

= 'ij

diY

piT E0f4

=

0

, 15 i, j 5 k-l, k < 12 n.

e At step k, we take & = &04, and compute

k+l<- a <a nti
In both procedures, skk = 1. The two procedures in the absence of roundoff errors , produce the same decomposition. However, they have completely different numerical properties when n> 2. If A is at all "ill-conditioned", then using the CGSA, the computed columns of P will soon lose their orthogonality. Consequently, one should never use the CGSA without reorthogonalization , which greatly increases the amount of computation. Reorthogonalization is never needed when using the !&GSA. A careful roundoff analysis is given by Bjb'rck in [2]* Rice [27] has shown experimentally that the MGSA produces excellent results.
The MXA hasthe advantages that it is relatively easy to program, and experimentally (cf. [2O]), it seems to be slightly more accurate than the Householder procedure. However, it requires roughly $ operations which is slightly more than that necessary to the Householder procedure. Furthermore, it is not as simple as the Householder procedure to add observations.
i/ 26

11. Sensitivity of the solution
We consider first the inherent sensitivity of the solution of the least squaresproblem. For this purpose it is convenient to introduce the condition number K(A) of a non-square matrix A . This is defined by
K(A) = Olj"n " O1 = $lA ~141 2 II ' `n = $$lA jlJl z l(e

so that al2 and CJ~ are the greatest and the least eigenvalues of A TA. From its definition it is clear that K(A) is invariant with respect to unitary transformations. If g is defined as in (8.1) then
p> = al(A) , CT n (fir) = o,(A) , K(z). = K(A),
while
al(K) = \I fi' \I2 and on(`R) = l/II PT' -' \I2 .
The commonest method of solving least squares problems is via the normal equation

ATA Nx =ATbN . The matrix ATA is square and we-have

(11.1)

K(ATA) = K2(A) .
1 -This means that if A has a condition number of the order of 29 then ATA has a condition number of order 2t and it will not be possible
f using t -digit arithmetic to solve (11.1). The method of orthogonal
i
transformations replaces the least squares problem by the solution of

I 27
t

.7

the equations xNx = E and K(x) = K(A). It would therefore seem to have substantial advantages since we avoid working with a .matrix with condition
1 number K~(A).
We now show that this last remark is an oversimplification. To this
end, we compare the solution of the original system (A 1 b) with that of a
l-
perturbed system. It is convenient to assume that
01 = IPII, = I 2 112 = 1 ;

this is not in any sense a restriction since we can make IIAI\~ and II b- 11, of order unity merely by scaling by an appropriate power of two. We now have

K(A) = K(CiT) = 11 F -' II, = l/On .
I- Consider the perturbed system

L: (A + eE ;l b + E$ Y ll~ll, = \I z II, = 1 Y

1 where E is to be arbitrarily small. The solution -z of the perturbed

1 system satisfies the equation

L (A + EE)~(A + EE) GN = (A + eE)T(bN + ENe) .

(11.2)

If ^xNis the exact solution of the original system and Q is the exact orthogonal transformation corresponding to A we have
Ls

37QA = ..a
tL [I 1 [$I0

-Y Q(A + sE) =

F + sF ....... ,
EG

Qy .;

L L 28

and

r = b-A% , ATr 5 8 l

N

N

N

N

N

Equation (11.2) therefore becomes

giving

(A +-EE)~(A + EE) = (AT + sET)(A xN+ rN + seN)

Neglecting e 2 where advantageous

.(z + EF) T (w + EF) 5-; = (g + ,F)%& + s(fzI + EF)~L + sETz + O(s2)

Nx = (?t + sF)-l'iT ^Nx + e((# + ,F)-'fN +

+ E(XT w,-+ET; + O(E2)

=gOE~-
N

5

^x

+

sR

+ ,(pir w,-lET~ + O(s2)

We observe that the bounds include a term EK~(A)\\~J\~ . It is easy to - verify by means of a 3 X 2 matrix A that this bound is realistic and
that an error of this order of magnitude does indeed result from almost any such perturbation EofA. We conclude that although the use of the orthogonal transformation avoids some of the ill effects inherent in
I
29

the use of the normal equations the value of K~(A) is still relevant to some extent.
When the equations are compatible l\rJ\ = 0 and the term in K~(A) disappears. In the non-singular linear equation case Nr is always null and hence it is always K(A) rather than K2(A) which is relevant. Since the sensity of the solution depends on the condition number, in the absence of other information, one should normalize each column of A so that its length is one in accordance with (4.1).
.
30

12. Iterative refinement for least squares problems The iterative refinement method may also be used for improving the
solution to linear least squares problems. Let

so that

T cYA P

=

ANTb- ATA?N-=2 .

When cy = 1,

I 2the

vector p N

is

simply

the

residual

vector Nr

.

Thus

b

i'

... ...

Y

(12.1)

K-li m.^-x ft.

or

1 One of the standard methods for solving linear equations may now be used to solve (12.1). However, this is quite wasteful of memory space since the dimension of the system to be solved is (m + n).
1 We may simplify this problem somewhat by noting with the aid of i, (8.3) that
i
L a1 A
AT 0
1
1
L 31

We are now in a position to use the iterative refinement method for solving linear equations.
Thus one might proceed as follows:
1) Solve for Nxt O > using one of the orthogonalization procedures outlined
in section 8 or 10. ft(must be saved but it is not necessary to retain Q. Then

NP (`I

=

.$

(b N

-

A x(O)) N

.

2) The vector Hy (s+u is determined from the relationship

g( s+l) =g( 1 +zs( >

where

BeNS( 1 N( )=&-ByS =hNS( > This calculation is simplified by solving

.

(12.2)

Lzs( > =hS( >
NN
UNP( > =Nz(s1

.

a
The vector-Nh( ' > must be' calculated using double precision accuracy and
then rounding to single precision.

3) Terminate the iteration when NI\8(s)II/l\Ny(s)II is less than a prescribed number.
Note that the computed residual vector is an approximation to the
residual vector when the exact solution N2 is known. This may differ from the residual vector computed from the approximate solution to the

32

least squares problem. A variant of the above procedure has been analyzed by Bjb'rck [3],
and he has also given an ALGOL procedure. This has proved to be a very effective method for obtaining highly accurate solutions to linear least squares problems. Bj&ck and Golub [4] have described a similar iterative refinement method for solving least squares problems with linear constraints.
33

I I.
13* Singular Systems If the rank of A is less than n and if column interchanges are
performed to maximize the diagonal elements of R, then
1
when rank (A) = r. A sequence of Householder transformations may now 'be applied on the right of A b-+1) so that the elements of S (n-r)Xr become annihilated. Thus dropping subscripts and superscripts, we have
Q A Z = T = ;; C+l
where T is an rxr upper triangular matrix. Now
where Nc = QNb and Ny = ZTNx . Since T is of rank r, there is no unique solution so that we impose the condition that I\ 5 \I2 = min. But I\ g \I2 = I\xI12 since T is orthogonal and \I ; I\, = min. when
Thus
34

This solution has been given by Fadeev, & 3. [7] and Hanson and
Lawson [183. The problem still remains how to numerically determine
the rank which will be discussed in section 15.
35

14. Singular value decomposition

Let A be a real, m x n matrix (for notational convenience we assume that m 2 n). It is well known (cf. [22])that

A= mVT

(14.1)

where

i
L

us = Im ., v-VT = In

L and

( m- n)x n.
i

AATtThe matrix U consists of the orthonormalized eigenvectors of

and the

i matrix V consists of the orthonormalized eigenvectors of A TA. The
Lr diagonal elements of C are the non-negative square roots of the eigen,
c\- values of ATA; they are called singular values or principal values of A. We assume

L

,L Thus if rank (A) = r, or+1 = org2 = .*a =cJn =o. The decomposition (14.1) I is called the singular value decomposition (SVL).
Let
I (14.2)
I It can be shown[22]that the non-zero eigenvalues of 'zi always occur in

L + pairs, viz

L 36

Xj (x> = + oj (A) (j = l&...,r).

(14.3)

iL
37
i

15. Applications of the SVD
The singular value decomposition plays an important role in a number of least squares problems, and we will illustrate this with some examples.
I Throughout this discussion , we use the Euclidean or Frobenius norm of a matrix, viz.

A) Let Un be the set of all n x n orthogonal matrices. For an arbitrary
n x n real matrix A, determine C&U, such that
IIA - QII 5 \\A - XI\ for any XsU,.
It has been shown by Fan and Hoffman [83 that if

A = UCVT , then Q =wT .
B) An important generalization of problem A occurs in factor analysis.
For arbitrary n X n real matrices A and B, determine &EUn such that I
II A - BQII 5 \\A - BX\\ for any X&U, .
It has been shown by Green [171and by Sch&emann[28] that if

BTA = aVT, then Q =WT .

_ c)

Let

04 ?rn >n

be

the

set

of

all

m

x

n

matrices

of

rank

k.

Assume

A E%l( rtn> l Determine BEs,(k.) (k $ r) such that

II A 7 B)\ 2 \\A - XI\ for all xs$i .

c! 38

It has been shown by Eckart and Young 163 that if

where

A = &, then B = 9sVT

(15.1) (15.2)

Note that
05.3)
D) An n x m matrix X is said to be the psuedo-inverse of an m X n
matrix A if X satisfies the following four properties:

ii) X&X = XI iii) (AX)T = Ax,
i v ) (XA)T = x./l.

We denote the psuedo-inverse by A+ . We wish to determine A' numerically. -It can be shown [26J that A+ dan always be determined and is unique. It is easy to verify that

A+ = VM?

(15*4)

39

where

/-1 a1

0

1

A = O2 . . 7

In recent years there have been a number of algorithms proposed for computing the pseudo-inverse of a matrix. These algorithms usually depend upon a knowledge of the rank of the matrix or upon some suitable chosen parameter. For example in the latter case, if one uses (15*4) to compute the pseudo-inverse, then after one has computed the singular value decomposition numerically it is necessary to determine which of the singular values are zero by testing against some tolerance.
Alternatively, suppose we know that the given matrix A can be represented as
A = B + 6B
where 6B is a matrix of perturbations and
iI Now, we wish to construct a matrix ?3 such that
L I A - "B\ g 7 ! and
rank (^B) = minimum.
i
40
i

This can be accomplished with the aid of the solution to problem (C). Let Bk = rnkVT

where Rk is defined as in (15.2).

Then using (15.3),
^B = B P

if

1

2

(i

Op+l

+

CT2 pf2 + l

.*

+

0;)' r r\

L

tL and

L

(

2
OP

+cJ 2+
p+l

...

Since rank (?3) = p by*construction,

fc A$ = vsl+vT . P Thus, we take hI3+ as our approximation to A+ .

1 El Let A be a given matrix, and b be a known vector. Determine ^x so

1N

that amongst

all x for which \Ib- - ", II N NN

= min, \I ^Nx I\

= min.

It is easy

to verify that

^xN = A+-b .

41

16. Calculation of the SVTL
In [14]it was shown by Golub and Kahan that it is possible to construct a sequence of orthogonal matrices

via Householder transformation so that
,b>,b-11 . . . P(`)AQ(`)QO pea~(nT1) =, P~AQ = J

and J is an m X n bi-diagonal matr-i- x of the form

ok B, 0 . 0
a2 B, . 0
.J = 0 . aBn-l
n
0 (m-n)Xn .

,

The singular values of J are the same as those of A. Thus if the singular value decomposition of

J = XCYT

i L

then

iI A = plccYTQT

! so that

,

i u=Px,v=QT .
L-

In [16], an algorithm is given for computing the SVD of J; the algorithm

i is based on the highly effective algorithm of Francis [ll] for computing

L the eigenvalues.

42

It is not necessary to compute the complete SVD when a vector Nb is given. Since N^x = V&JTNb, it is oiy necessary to compute V,C and $2; note, this has a strong flavor of principal component analysis. An AJXOL procedure for the SVp will soon be published by Golub asd Reinsch and a complex FORTRAN procedure for. the SVD by Businger and Golub.
43

179 Canonical correlations
It is well-known (cf. [l]) that in order to solve for canonical correlations,, it is necessary to solve the matrix equation

where

Cl1

is

a

p

x

p

positive

definite

matrix

and

C 22

is

a

q

x

q

positive

definite matrix. The eigenvalues of (17.1) correspond to the canonical

correlations. Since Cii is positive definite we have

cii = r;ri . A short manipulation shows we may rewrite (17.1) as

(!IT i) (3 =A (:)
where R

(17.2)

Thus by (14.2) and (14..3) , we s_ee that the canonical correlations, A., are J
the singular values of Q.

Suppose

we

have

two

sets

of

data

X nxp

and

Yn

x

q'

We assume that

the mean of each variable is zero. Then

^cll

=

cxTx,

i22

=

cYTY,

i,,

= cXTY(c > 0)

l

44

Using the Householder algorithm described in section 8 or the Gram-Schmidt algorithm described in section 10, we may write
X = QR,QT Q=LR: TJ Y P
Y = PS, PTP = Iq' s:Y .
Hence by (17.2)
ri = R-TRTQTpSS-l

= $P .

(17m3)

Therefore the canonical correlations are the singular values of Q'P. Note

`i(`) L II fi II2 I II QT 1.12 ' lIPI 5 1 l

A short calculation shows that s= R-ki and &= S-"L~ where zi and Li are the ith columns of U and V, respectively. This method of characterizing the canonical correlations has been observed previously (cf. [25] ). An algorithm using these techniques will soon be published by Bj&ck and Golub.

i L
45

Acknowledgements
I am very pleased to acknowledge the many helpful comments made by Dr.
0
Ake Bj'brck and Dr. Peter &singer.
I t i r1
L
L t
t
L 46

.
References

Dl T.W. Anderson, An Introduction to Multivari,ate Statistical Analysis,
John Wiley, New York, 1958. -
0
El A. Bj'drck, "Solving linear least squares problems by Gram-Schmidt
orthogonalization,' BIT, 7 (1967), 1-21. 0
[31 A. Bj'drck, 'Iterative refinement of linear least squares solution Ii
BIT, 7 (1967); 257-278. 'Iterative refinement of linear least squares solutions II", BIT, 8 (1968), 8-30.
0
PI A. Bj'drck and G.H. Golub, 'Iterative refinement of linear least square
solutions by Householder transformation", BIT, 7 (1967), 322-337.
[51 P. Businger and G.H. Golub, !'Linear least squares solutions by Householder transformations,"- Num. M-ath., 7 (1965), 269-276.
161 C. Eckart and G. Young, 'The-approximation of one matrix by another
of lower rank", Psychometrika, 1 (1936), 211-218.

[71 D.K. Fadeev, V.N. Kublanovskaya, and V.N. Fadeeva, "Sur les systemes
lingarires alggbriques de matrices rectangulaires et mal-conditionnees," "Programmation en Math&natriques Num&iques,'Editions du Centre National de la Eherche Scientifique, Paris VII, 1968.

PI K. Fan and A. Hoffman, 'Some metric inequalities in the space of
matrices", Proc. Amer. Math. Sot., 6 (1955) 111-116.

r91 G.E. Forsythe and C. Moler, Computer Solution of Linear Algebraic Systems, Prentice-Hall, Englewood Cliffs, New Jersey, 1967.
ml G.E. Forsythe and E.G. Straus, "On best conditional matrices,"
Proc. Amer. Math. Sot., 6 (1955), 340-345.
Cl11 J. Francis, "The QR transformation. A unitary analogue to the LR
transformation', Comput. &, 4 (1961; 1962) 265-271.
[121 G.H. Golub, 'Comparison of the variance of minimum variance and
weighted least squares regression coefficients,' -Ann. Mat-h. Statist., 34 (1% 1, 984-991.

r131 - 1141
r151

G.H. Golub, 'Numerical methods for solving linear least squares
problems",- Num. Ma-th., 7 (1965), 206.216.
G.H. Golub and W. Kahan, 'Calculating the singular values and pseudoinverse of a matrix," 2. SIAM, PNumer.WAnal. Ser. I& 2 (1965), 205-224.
G.H. Golub and J. Wilkinson, 'Iterative refinement of least squares solution," Num. Math., 9 (1966), 139-148.

WI G.H. Golub, "Least squares, singular values and matrix approximations,' Aplikace Matematiky, 13 (1968), 44-51.
47

[17] B. Green, "The orthogonal approximation of an oblique structure in factor analysis", Psychometrika, 17 (1952),429-&&O.

[183 R. Hanson and C. Lawson, "Extensions and applications of the Hoyse-

holder algorithm for solving linear least squares problems," Jet

Propulsion Laboratory, 1968.

b

[lg] H. Hotelling, 'Some new methods in matrix calculation, A-nn. Math-. Statist., 14 .(1g43), l-34.

1201 T. Jordan, 'Experiments on error growth associated with some linear
least-squares procedures, "Math. Comp., 22 (lg68), 579-588.

[21] W. Kahan, "Numerical linear algebra,"pCanad-. Math7. ~u,ll., 9 (1966), 757-801.

[22] C. Lanczos , Linear Differential Operators , Van Nostrand, London, 1951,
Chap. 3.

1231 J. Longley, 'An appraisal of least squares problems for the electronic computer from the point of view of the user,' JASA, 62 (196'7'), 819-841.

[24] C.B. Moler, "Iterative refinement in floating point,' 2. Assoc.
Comput. Mach., 14 (1967), 316-321.

[25] I. Olkin, On Distribution Problems in Multivariate Analysis, Ph.D. thesis, Uns. of North Car-93).

[26] R. Penrose, 'A generalized inverse for matrices', Proc. Cambridge Philos. Sot., 51 (1955) 406-413.

[27] J. Rice, "Experiments on Gram-Schmidt Orthogonalization," Math.
Comp., 20 (lg66), 325-328.

[28] P. Schvnemann, "A generalized solution of the orthogonal procrustes problem', Psychometrika, 31 (1966), l-10.

c291 A- van der Sluis, 'Condition numbers and equilibration of matrices',
(unpublished report from Utrecht University, ERCU 37-4)*

[30] J.H. Wilkinson, RoundingE-rrors in Al ebraigc6 Processes, Prentice-
Hall, Englewood Cliffs, New Jersey, 19 3.

- [31] J.H. Wilkinson,"A priori error analysis of algebraic processes',
Proceedings of the International Congress of Mathematicians, Moscow,
1966, 629-640.

[32] J.H. Wilkinson, 'The solution of ill-conditional linear equations', Mathematical Methods for Digital Computers, Vol. II, A. Ralston,
Ph.D. and H. Wilf, Phx, ed., John Wiley, New York, 1967, 65-93.

48

