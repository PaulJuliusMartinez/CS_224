SU 326 P30 32
ON THE SOLUTION OF LARGE, STRUCTURED LINEAR COMPLEMENTAR ITY PROBLEMS: I I I
bY Richard W. Cottle
Gene l-L Golub Richard S. Sacher
STAN-CS-74-439 AUGUST 1974
COMPUTER SCIENCE DEPARTMENT School of Humanities and Sciences STANFORD UNIVERS ITY

ON THE SOLUTION OF LARGE, STRUCTURED LINEAR COMPLEMENTARITY PROBLEMS: III
bY Richard W. Cattle+
Gene H. Golub* and
Richard S. Sacher
TECHNICAL REPORT 74-7 June 1974
DEPARTMENT OF OPERATIONS RESEARCH Stanford University Stanford, California
t Research and reproduction of this report was partially supported by the Office of Naval Research under contract N-OOOl&-67--A-0112-0011; U.S. Atomic Energy Commission Contract AT(Ok-3)-326 PA #18.
* Research and reproduction of this report was partially supported by U.S. Atomic Energy Commission Contract AT(Ok-3)-326 PA #30; and gational Science Foundation Grant GJ 35135X, Department of Computer Science, Stanford University. This report was also issued as Report STAN-CS-714-439 of the Computer Science Department, Stanford University. Reproduction in whole or in part is permitted for any purposes of the United States Government. This document has been approved for public release and sale; its distribution is unlimited.

SECTION 1 BACKGROUND

1.1. Introduction to the Problem. In previous papers, 1/,2/ R. W. Cattle and R. S. Sacher have
discussed three algorithms for the solution of large-scale linear complementarity problems. For a given matrix M F Rnxn and a given vector qc Rn, the linear complementarity problem is that of finding a solution z to the system

q+Mz>= O
L- 220
zT(q + Mz) = 0 .

The methods required that M be a tridiagonal, Minkowski matrix. This

means M = (mij) satisfies the following conditions: I L W mij $0 if ifj

~ (ii) mij = 0 if Ii-jl>l L (iii) M has positive principal minors.

The three algorithms may be briefly described. Algorithm I is

a modification of the principal pivoting method [13]. Algorithm II is

a specialization of a method proposed by Chandrasekaran [9] and employs

I'R. W. Cottle and R. S. Sacher, "On the Solution of Large, Structured Linear Complementarity Problems: I, lr Technical Report 73-4, Department
of Operations Research, Stanford University, 1973.

2'R S Sacher "On the Solution of Large, Structured Linear Complementarity

Problems: II:" Technical Report 73-5, Department of Operations Research,

Stanford University, 1973.

L

1

,
LU factorizations. The algorithm is "adaptive" in the sense that each iteration exploits the factorization associated with its predecessor. Algorithm III is a modification of the point successive overrelaxation
technique. In this paper, we consider the more general linear complementarity
problem in which the matrix `is no longer necessarily tridiagonal but may be block tridiagonal. We still assume it to be Minkowski, however. This means we may partition M into submatrices Mij (i, j = 1,2,...,m) such that
(i> Mii is a Minkowski matrix of order ni = 1, 2, . . . , m,
L (ii) M.1.J <= 0 (elementwise) if i # j, (iii) M has positive principal minors, (iv) M.. = 0 if Ii-jl > 1.
I, iJ (Matrices satisfying condition (iv) alone are known as block tridiagonal
L matrices.) We also require that M be positive definite and the diagonal blocks, Mii, be symmetric and tridiagonal. (With this last assump-
I tion, we may vastly increase the efficiency of the algorithm we propose in Section 2.4 by incorporating the techniques described in the previously ii cited paper by R. S. Sacher.) Such matrices include block tridiagonal
Stieltjes matrices (see [58;p. 851) whose diagonal blocks are tridiagonal. L These occur frequently in the discretization of elliptic partial differential
equations. In fact, it is in this connection that an important application lb
of the linear complementarity problem is discussed in Section 3. The convergence of the algorithm we propose in Section 2.4
requires only that M be positive definite and that the diagonal blocks,

Mii' be symmetric. (That is, in proving convergence, we drop the assumptions of block tridiagonal structure and nonpositive off-diagonal entries.) The method is consequently stated in full generality.
1.2. Drawbacks of the Generalization of Algorithm I. The success of Algorithms I and II for the tridiagonal case
suggests that they may be profitably applied to the block tridiagonal case. The purpose of this section is to show why the benefits of those techniques are lost in their extensions.
Consider first the modified principal pivoting algorithm. Certain structural properties of the tableaux under principal pivoting when M is block tridiagonal are analogous to those when M is a tridiagonal matrix. Consequently, an immediate extension of Algorithm I _ may create a prohibitive number of nonzero entries to be stored as the algorithm progresses. The following two examples illustrate this remark.
Example 1. In Figure 1, the lighter lines indicate the partition of M. We assume that the matrix is block tridiagonal and Minkowski, Mii is tridiagonal of order ni = n = 3 and the off-diagonal blocks are diagonal matrices. The innermost block is M the pivot block. The
33'
locations of possibly nonzero entries in the pivotal transform z are indicated by the asterisk symbol. The main significance of this example is that with the given pattern of zeroes, the principal block pivot on M may create complete fill-in within the dark border.
33
(For notational convenience, we refer to the entries outside the
3

* ** * ***
*** *** *** * *** *** *** * *** *** *** * *** *** *** *** *** *.** *** *** *** *** *** *** *** *** ***
-&y/y--

cz I I+
I I+I I +I

I I+ I I I+I I I +I I I I+ I I 1+1 I I +I I

I+ I+I +I

I I I

4

-
pIi\.-ot block but inside the dark lines as the frame of the pivot, n
this case, the frame is of width n.)
Lo.

`.
- Example 2. Figure 2 portrays the effect of pivoting on the blocks

j?~~, Mb4' . . . , Mm-l,m-1 where m is odd. If m equals n, one can

easily show that even if (as in Algorithm I) we discard the transformed
,.

tableau entries in columns where a pivot has occurred, the number of non-
-

zeroes which must be stored in the transformed tableau is (g (n-l) + l)n2

=

5 2

n3

_

2 2

n2 .

Compare this with the number of initial nonzero entries

in M, i.e., n(3n-2) + 2(n-1)n = 5n2 - 4n. (It is not uncommon [193

for n to equal 100 and thus to have an approximate increase in the

number of nonzeroes which must be stored from 50,000 to 2,500,000!)
(t-

i

4 L

6.3. Drawbacks of the Generalization of Algorithm II.

L Recall that Algorithm II, the modification of Chandrasekaran's method using factorization, requires the solution of a sequence of

1 systems of linear equations by LU decomposition. The order of the final system solved is equal to the cardinality of the set of positive
LL z-variables in the solution to the linear complementarity problem. From [9], we know that if M.E Rpxp is a Minkowski matrix and if
L q< 0, then the solution is the positive vector z = -M-'q Z Rp and I- we are required to solve a linear system of order p. If we use the
example corresponding to Figure 1, we have p =mn. We would like

to factor M in a wa.y that exploits its structure and sparsity as
i
much as possible. If we were to view it as a band matrix of width n,

wr) would use LU or Cholesky (?tT) factorization since they both

5

*** *** ***

*** *** *SC*

*** **Jjcc* ***

*** * *.* *** *** SC** ***
t - *** *** 3c** `c *** *** *** *** ***
*** *** SC** *** *** *** **++ 3c** *** ***

*** *** -* * * -)c** *** *** ILa- *SC* *** *** *** *** *** *** *** *** *** *** *** 3c** +i** * * - I t - * * * *** ***

*** **-x *** *** *** ***
ILt *** *** **SC

*** *** *+*

1

1

1

1

I:=I

I:::

I::r;l

t

Figure 2. Matrix z of Transformed Tableau After
Principal Pivots on M22, Mb4,...,Mm 1 m 1 9-
L (m = 7). Original Matrix is Block Trii
Diagonal and Minkowski.

6

preserve the bandwidth [36]. Unfortunately, neither method of decom-

position will preserve the sparseness of the original data. This is

illustrated by the example in Figure 3 where the matrix M corres-

ponds to the finite difference equations representing the Laplace

equation: M.1.1 is tridiagonal with diagonal entries equal to 4 and

off-diagonal entries equal to -1; furthermore,

M i,i-1

and Mi,i+l

are negative identity matrices. Since the bandwidth of such a matrix

cannot be reduced any further, the Cholesky (ttT) factorization requires storage for almost 271p"=(m-2)n i=2mn 2 +mn-2n 2 -n nonzero matrix entries. For instance, if m =-n = 100, then the initial number

of nonzeroes in the matrix is approximately 5n2 = 50,000 while

2mn2 + mn - 2n2 - n is approximately 2,000,OOO. The LU factorization

needs nearly twice as much storage as the Cholesky factorization since

L, UT

and 2

have

idential

patterns

of

nonzeroes,

i.e.,

k?ij'

U
ji

and

2ij are simultaneously nonzero or zero [36].

A third alternative for factorization is a special case of

methods known as group- or block-elimination [36, p. 591. Isaacson and

Keller [$I discuss one technique which is a highly efficient direct

method but which requires slightly more storage than the Cholesky

decomposition. Following their discussion, we seek a factorization of the form

M=LU=

Al
B2 A2
B3 A3 ... ...
Bm *m

7

I1 cl
I2 C2 ... ...
.
.l 'm-1
'rn

I,
4 -1 -1 4 -1
-1 44 --11 -1 44--11 --11 4

-1 1 -1
-1 -1
-1

-1
-1 -1 -1 -1

4 --1l -1 4 --11
--11 4 --11 --11 44 --11 --11 44

-1 -1 -1 -1 -1

-1

4 --11

-1

t -1 --11 4 --11 -1

-1 --11 4 --11 -1

i.-

-1 -1

_ --11 4 --11 --11 4

-1 -1

i C-
Lt

-1 -1 -1 -1 -1

4 -1 -1 4 -1
-1 4 -1 -1 4 -1 -1 4

L i
i Figure 3 . Example of a 'LAP" Matrix with m = 4, n = 5.
L

8

where the identity matrices I. and the matrices A., B., J 3J
are all of order n (j = 1, 2, . . . , m). Consequently, j

and C j

Al = Ml1
Bi = Mi J i - 1 Ai = Mii - BiCi 1 and 'i= A-i34i

i = 2, 3, . . . , m, i = 2, 3, . . . , m, i = 2, 3, . . . , m-l.

There are

m-l matrices

A Ai

and m-l matrices

C -f which may I

2 each contain n

nonzero entries.

The Bi

matrices need no additional

storage. Hence, the block-LU decomposition requires 2mn2 - n2 storage

locations

versus

the

2mn2

+

mn

-

2 2n

-

n

required

by

Cholesky

factorization.

In summary, extensions to both principal pivoting methods and

various factorization techniques are stymied by storage problems.

Similar difficulties in solving large systems of linear equations were

recognized by numerical analysts. These difficulties rekindled their

interest in iterative (versus direct) methods of solution--that is, in

determining techniques to accelerate the convergence of existing methods

and in developing new approaches. -In Section 2, some results in the

former category will provide motivation for the iterative technique

we propose for solving the linear complementarity problem (q,M) when

M -is a block tridiagonal, positive definite Minkowski matrix whose

diagonal blocks have symmetric tridiagonal structure.

9

SECTION 2 ALGORITHMS FOR THE BLOCK TRIDIAGONAL LINEAR COMPLEMENTARITY PROBLEM
2.1. Introduction. It is ironic that the algorithm we develop in this section
arises as a generalization of the least computationally attractive method of Algorithm I, II and ILII. Yet the computational experience reported in Section 4 demonstrates that this generalization is at least competitive with, if not superior to, techniques currently available [71, [191, [291, [451, [461, [471, [591 f o r solving the engineering application described in Section 3.
Because of the analogy of the proposed algorithm with relaxation techniques for systems of linear equations, we open this section with some remarks about these methods. For the remainder of the paper, we will observe the following notational conventions. All vectors are column vectors. By a slight abuse of notation, we let z =n (z1 ,Z2"**' zn > denote the column vector z in Rn. Similarly, if zi 5 R i and c'=1 ni = N, then we may let z = (zl,z2, . . . , z m ) denote the column vector z in RN . Finally, the algorithms to be described will generate a sequence of iterates z-, kk = 1, 2, . . . , converging to a
solution. The value of Zk is determined by a specified transformation
on zk-l. Therefore the sequence z ,k k = 1, 2, . . . , is totally
determined by an initial vector Z0 . We denote the sequence z k, k = 1, 2, . . . , by {zk) and suppress its implicit dependence on Z0 .
10

2.2. Point Successive Overrelaxation (SOR) Algorithm for Linear Systems. The point successive overrelaxation algorithm for solving the
linear system Mz + q = 0, where M is an m X m matrix, is an accelerated version of the earlier Gauss-Seidelmethod [58]. This latter method generates a sequence of iterates Zk ERm according to the formula:

Zk+l i =-

( J<z*iim.J.zJ.k+l + j>ci

k m'. J. zJ. + qj)/mii

-i = 1, 2, . . . , m.

t- Each component of zk+l is recursively determined in terms of the
L current values of the others. The recursion formula may be rewritten
in the following format in which LO = 1: Determine Zik+l
(i = l, 2, . . . , m) by

Zk+l
i

Zk
=i

+

(U(;ilk+l

-

zk) i

(1)

where

mii'Oik+'

+ ( z m..Zk+' j<i 'JJ

I- 2 m..zk j>i 'JJ +

qj'

=0l

1c
ii

We interpret the term-k(+zli - z!1) as a direction in which to proteed from the current value of z i = z.k1. The parameter cu is thus

.

thought of as a weighting factor to indicate how far to move in this

direction.

We have noted that in the Gauss-Seidel algorithm o =l. In 1950, young [613 and Frankel [25] simultaneously, but independently, recognized the efficacy of using values of cu different from unity to

-
11

-
gain faster convergence rates. The scalar cu is called the relaxati)n

parameter, and cu > 1 (U < 1) corresponds to overrelaxation (under-

relaxation). The method of Young and Frankel (using cu > 1) is

called the point successive overrelaxation algorithm.

The word "pointU in thr3 name of the method has an interesting L- geometric origin. Suppose we are solving T,aplace's equation, 77 u = f,
over a rectangular region by `1 finite difference method. This first

requires forming a grid over the region. We then seek an approximation to the unknown function u at the grid points only. This is achieved

by assigning a variable zi to each grid point (see Figure 4) and

i - obtaining, by well-known technique:s (see [24, p. l92]), a linear

system Mz+q=O.

We

defint

the

error

at

the

th i

grid

point

to

be

the absolute value of the difference between zi and the function u

evaluated at that grid point. If the grid is square and has n

points on a side, then the maximum of these errors is O(l/n2). When

the point SOR method is applied to the linear system, the algorithm

changes the value of only one variable zi at a time, i.e., only one

grid point is examined at a timct. Hence the WOY*d VpoinOw in the algo-

rith name.

2 -3. Block Successive Overrelaxation (SOR) Algorithm 'for Linear Systems. In certain situations, it is natural to consider simultaneously
changing the values of all variables associated with a coordinate line of the grid points. Such methods are known as line- or block-iterative techniques. The word 'block" refers to the fact that the variables
12

z15 `16
F i g u r e 4 . Grid for Finite Difference Equations
13

which are simultaneously changed correspond to a diagonal block (or

principal submatrix) of the matrix M. Varga [58, p. 961 indicates

that block methods are not new developments but may be traced back to

the work of Gerling [28] in 1843.

We again use the example of the finite difference discretization

of a differential equation over a rectangle. In the corresponding linear system, Mz + q = 0, we conformably partition the vectors z

and q and the matrix M. We will view z as a direct sum of

vectors z = (z 1' z2, . . . ) 'm)

where

z iLr

11
R

1..

Thus

z = (z 11'

z12, . . . , zln ;z~~, 1

z22, ..i

,

z2n ; . . . 2

zmln ). m

A

corres-

ponding relabeling of the grid in Figure 4 is shown in Figure 5.

Finally, q = (91, 92, . . . 1 &) is similarly relabled and partitioned.

The corresponding changes in the recursion equation (1) may now be stated. Recursively determine the subvectors zki +l

(i = 1, 2, . . . , m) by

where

Zk+l
i

=

zk.1

+

-k+l co(Zi - z;,

M ik+l.( 2 M..Z:+l. c Mijz;+'Li)=O*

ii i

j<i '33

j>i

Varga [58, p- 911 remarks that in the numerical solution of many physical problems, the matrix M is endowed with properties which guarantee that block SOR will converge to a solution faster than will point SOR. In these applications, the matrix M is irreducible and Stieltjes (i.e., symmetric Minkowski). Arms, Gates and Zondek [1] state that if M is merely a Minkowski matrix, then block SOR still has the advantage. For a more extensive treatment of successive

14

z41 '42 z43 24-4

z31 '32 z33 z34

z21 z22 --- z2s `24

zll . z12

z13 --az1--4-

Figure 5. The Grid Variables Relabeled

15

overrelaxation techniques and their many variants, see [36], [58],
[591, L-601 l
2.4. Algorithm IV: Modified Block Successive Overrelaxation Algorithm for Linear Complementarity Problems. Tne philosophy of generalizing point SOR to block SOR in order
to solve linear systems finds an analogue in generalizing Algorithm 11.1, the modified point SOR technique for linear complementarity problems. In this section, we show that by restating Algorithm III, a certain generalization suggests itself. In the following sections, an analysis of the latter algorithm will illustrate three points. First, under reasonable hypotheses, the sequence of iterates generated by the algorithm will form a monotonically increasing or decreasing sequence of vectors converging to the solution of the problem. (Under these hypotheses, the results in [52] guarantee that a unique solution exists.) Second, the method may be interpreted as a manifold suboptimization technique applied to a related quadratic programming problem. Third, values of u) greater than unity may be used to accelerate the convergence of the algorithm.
Recall Cryer's [219] description of the modified point SOR algorithm for the linear complementarity problem (q,M) where ME Rmxm is positive definite. The parameter e > 0 is chosen small enough to-insure that the errors in the values of the z-variables are sufficiently small. We shall make a slight modification in notation.
16

Algorithm III (Modified Point SOR) Step 0. Let z" = (zy, zg, . . . , zi) be an arbitrary nonnegative m-vector and cut' (0,2). Set k - 0.

1Step .

Let

*i+' = -

( j<2i

k+l

m..z. 'JJ

+ j>ci

m . . z.+ 'JJ

g. l> mii

Let i Zk+' = maxlo, zki + LU(i$+' - zki)]

i = 1, 2, . . . , m.

Step 2. Define J = {i:zr+'> 0) U (i:z2fl = 0, (Mzk+' + q)i < 0).
If max I( Mzk+' + q)il < E stop. An 'approximateIt solution is iEJ
at hand. Otherwise , go to Step 1 with k replaced by k+l.

The algorithm is essentially the point SOR algorithm for linear systems with the precaution that if a z-variable ever becomes negative, it is . immediately set equal to zero. Cryer [19] gives a convergence proof for Algorithm III under the assumption that M is symmetric and positive definite. Historically, an identical modification of the Gauss-Seidel method has appeared in several varied contexts, see
[53, PC [341.
Algorithm III may be viewed in a slightly different but equivalent way for values of 0 >ZZ l_.

Proposition 1. If cu >= 1, then Step 1 is equivalent to the following:

step 1': (a) Let zki +' solve the linear complementarity problem

(q',M') G ( c mijzJ+l + c m..zk f gi7 m ) .

( c
L

j<i

j>i 'JJ

ii

17

b)

Let

mk+i' A

=

max{G:G

<- w 7

Z: + G( i-k: cl -

zIF)>=ol

.

(c)

Let

zki+'

= ~2 + m(ik+l i

-

z:);

i

= 1, 2, . . . , m.

Proof. The analysis is divided into two cases:

Case 1.

E, J< i

k+l
m. .z
13 j

+ z7
j>i

k m..z
'Jj

+

P.

<

0.

Consequently,

;k+l = Sk+1 i i'

From (b)9 c.iuk+'

equals

co

if and only if

zki + w(z-.k+l-zki) > 0,

In this case Steps 1 and 1' give the same value for i zk+l. SuppoIe, =

on the other

hand,

that

zki + w(z-ik+l -

-k 'i)

<

'7

and therefore Step

1

L--

sets

Zk+l i

equal to zero.

But

max{O, z: f w(gk+' i

- ~k$1 = 0 if

and only if there is an W'<=CU forwhich

z

k i

f

c-o(z-.k1 +l

-

zki)

>= 0

t (resp. < 0) when z <= CU' (resp. G > cur). In this situation, wik+l

I

is chosen to be cu'

in

(b)

and

thus

zk+l i

=

0

in

(c)

of

Step

1'.

L

Case 2.

L

c

z
j<i

k+l
m. .z
1J j

+

z
j>i

k m z.+
ijJ

?L.>= O.

(3)

L

Therefore, in Step 1 we have

$+l i

-z

k i

=<-z

k i'

Zik + w$i+l-

z;,

<= 0

for all co => 1 and zki+l is set equal to zero. In Step l',

equation (3) implies that

-z k+l i=

0

and

that -(kz+il - zk) 1

=

-zk. 1

Consequently, cu => 1 implies that the value of wF+' chosen in (b)

of Step 1' is unity and thus

Zk+l i

is set equal to zero in (c).

H

In Section 2.5, we show that Algorithm III with the Step 1' substitution will converge for all cu 2 (0,2). Under this new interpretation regarding the choice of the relaxation parameter at

18

each iteration, a generalization of the preceding algorithm may be

proposed. By a slight change in notation, we shall pass from a point-

iterative to a block-iterative technique. We will use the notation

described in Section 2.3 on block SOR for linear systems. In particular,

M is partitioned into submatrices

M ij

(i, j = 1, 2, . . . , m) where

Mii

is

of

order

n., 1

z

=

(z p2P**,

'm)

and

q=(z l'cl.p l l .,$)

where

1Z . and qi are II i -vectors. Algorithm III, with the substitution of

Step l', then forms the basis for the following generalization. We

i refer to the new algorithm as the Modified Block SOR Algorithm for --

t the linear complementarity problem (q,M) where M is positive L-
definite and Mii (i = 1, 2, . . . 7 m) is symmetric.

L
r Algorithm IV (Modified Block SOR) L
Step 0. Let z" = (z,O, 5O' l -* ? zm0) be an arbitrary nonnegative
vector and w C (0,2). Set k = 0 and i = 1 .

Step 1. Let Zik+l solve the linear complementarity problem

( z M..zk+' j<i 1J j

+ c' Mijz; f gi' Mii, j>i

zSktep

2.

Let

Uki+l

=

max{G:G

< ;

w3

i +co-(z-ik+l - z;, >= 0).

Let

zik+l

=

1zk

+ ok+'(ik+' zk) i i -i'

3S.tep

If i=m , 4 .`$-O to %?rJ Otherwise , return to Step 1 with

i replaced by i+l.

19

dSte-p 24

Define

J

=

{(i,j):(zF'l)j

>

0

OX-

(i I-=1

MirZrk+l

+

gi>j

<03.

m

If

max (i,j) i-z

J

I( c. r=l

Mirzkr+l

+

qi) 1 < j

E,

stop.

An

approximate

solution

is at hand. Otherwise, return to Step 1 with k replaced by k+l

and i = 1.

!I!he differencesbetween this algorithm and block SOR for

linear systems are evident. In Step 1, we solve a linear complemen6 tarity problem (&M) instead of solving a linear system E f < = 0.

i Also, the nonnegativity constraint of the complementarity problem is

tI-

handled in Step 2 by requiring that movement in the direction-k(z+il -zki) be constrained to remain in the nonnegative orthant. In solving linear

L systems, the nonnegativity restriction is absent and thus wki +l always

I equals cu. The computational bottleneck to the modified block SOR algorithm,

L if one exists, will occur in Step 1 where linear complementarity problems must be repeatedly solved. In general, if Mii is an

! arbitrary positive definite matrix, then the standard methods for
solving (4i,~ii >, ( e-g-9 the principal pivoting technique of Cattle

[13] or Lemke's method [38] may be used--possibly at the expense of

large core storage requiremehts and perhaps not particularly rapid

convergence.) However, if we apply the modified block SOR algorithm

to matrices whose diagonal blocks Mii are tridiagonal Stieltjes

matrices, then Algorithms I and II may be profitably applied to yield

an algorithm of high overall efficiency. An example of this type will be discussed in Section 3.

20

L3. 5* Convergence of Algorithm Iv .
It is natural to look at the corresponding problem of the
minimization of a quadratic function over the nonnegative orthant to
help demonstrate the convergence of the algorithm. We shall use a
method of proof similar to that of Cryer [20] and Schechter [53], [`j&l.
If the matrix M is positive definite, then the Kuhn-Tucker conditions
for the problem
i
/ minimize f(z) = $ zTMz + qz
i
cI subject to z>= o
are the necessary and sufficient conditions which a global minumii
L satisfies. If we further assume that M is symmetric, then the ! Kuhn-Tucker conditions are equivalent to the linear complementarity
- problem (q,M).
The first result will show that the successive iterates {zk}
i
cause the sequence {f(zk )] to be strictly monotonically decreasing.

Theorem 1. Let

f(x,y)

=

$

(;I

T

(;

;)(;)

+

(pS)T

(") Y

where

p,

x \E

Rn,

N-n s,YE' 9

and

x

and

y

are arbitrary vectors.

Assume-that A is symmetric and positive definite. Let G solve

the linear complementarity problem (Hy + p,A) where H = (B+CT)/2.

Then f(x + cu(G-x),y) 5 f(x,y) for VU E (0,2). Furthermore, quality holds if and only if G = x.

21

Proof. We examine the minimization of g(u) E f(u,y) over the non-

negative orthant. Rearranging terms, g(u) = $ uTAu + (Hy + P)~U + c where c is a constant. Since A is symmetric and positive definite,

the minimizing vector G is the solution to the related linear complementarity problem (Hy + p, A). For notational convenience, let

r=H;;+p.

We will assume that G # x. By a principal rearrangement of i / A, we may further assume that G = (G,O), where K is the index set

t

(1, 2, . . . , k) and $ = (2 z2

- - ) > 0 Letting 3 be the

1'

`""~

l

I index set (k+l, k+2, . . . , n), the corresponding partitionings are

I

A

L r = irpr > and A = K

KK .

A

ii? 1

7 ( 7s 9

-x >. E

We want to show that g(x) > g(v) for all v in the

open line segment V = (x, x + 2d). Noting that V may be rewritten

as {v:v = G + Ad, vh c (-l,l)I, we consider two cases h < 0 and =
A > 0.

Case 1. A 5 (-l,O]. Since g is strictly convex, then for all A E (-l,O],
id; + hd) < (1 - IAl) g(G) + IAl g(h)
= (1 - bl) g(G) + IAl g(x) 5 (1 - IN > idx ) + IhI g(x > .
Tellus,
22

g(G + Ad) < g(x) .

(4)

Case 2. A E (0,l). Consider the Taylor series expansion

g(; + Ad) = g(x) + hdT(A; + r) + $ h2dTAd. By showing that dT(& f r) 2 0,

we may conclude that g(G + Ad)= < g(G - hd) for all A E (0,l). But equation (4) shows g(G - Ad) < g(x) for all A E (0,l); thus

g(ii + hd) < g(x) for all A E (0,l) also.

Using the index set K and the corresponding partitionings

described above, we have

dx T %x

( 8dT(Ax + r) = -X

A-

K KK

1) (") +(P)

= -xT(r

- AE<r,,

l

Now recall that 2 = (G,O) satisfies the system

(h 2)(F) + (_)~O, GT(Ax+r)=O.

23

Therefore AKKG + rK =I 0 and AZ< + rE 1 0. Substituting

in the latter of these two systems gives

r-A A zi;jK

-1 K$K=>

0.

Finally, we conclude the argument by noting that x > 0 implies =

dT(Ax + r) <= 0. u

Theorem 1 means that we can use the function f to monitor

the progress of the algorithm. If we can guarantee that f is bounded from below on the nonnegative orthant, then we will be assured that

the sequence of successive iterates -{zk) contain a convergent subsequence. Positive definiteness of M is one sufficient condition

for the boundedness of f. A necessary condition is that M be copositive [313. For f is unbounded on the nonnegative orthant if there is a nonnegative vector x for which xTMx is negative. Hence f is bounded below on the nonnegative orthant only if x TMx is nonnegative for every nonnegative vector x.

Each iteration of the algorithm updates the m subvectors of

the vector

zk

=

kk ( Z1' zt-p

l

l

l

J

For future notational convenience,

let f:(v) = f(zF, z:, . . . , 55: 1, v, zF;f, Z2;,', .., ) Zk-'). m

Theorem 1 thus shows that ft(zF) < fF(zF-', =

with equality if and

only if

z ki

-=

Zk-l i'

Consequently- f(zk) 2 f(zk'l)

with equality if

and only if zk = Zk-l . In the case of equality, we can prove that

Zk solves the linear complementarity problem.

. Proposition 2. If the algorithm generates iterates zJ, j = 1,2,...,k and z-I k-=1 z k, then zk solves (%M).

Proof. From Step 2 of the algorithm, we see that zik = zki-l if and

-k k-l only if zi = zi .

Suppose we are solving for

zki in Step 1.

We require

wi

-=

j<c i'M.JJ.zk

+

Miiz;

+ c M. .zk. -l j>i'JJ

+

gi

>=

0

and

;ziF

=

0.

;k k-l Since i = zi

and zk-l = Zk , then

wk.' ,1

c' j<i

Mijz~

+

Miiz~

f C Mijz; + gi j>i

= wi 2 0

and (wi)kTz.k1 = 0 .

This holds for all i = 1, 2, . . . , m; so zk solves

Finally, we prove that the sequence of iterates have a unique limit z and that the limit solves WC

Proposition 3. If M is positive definite, then the sequence of

iterates

k {z 1

are contained in a compact set and hence contain a

convergent subsequence with limit z.

Proof. (Similar to irryer). From Theorem 1, {f(zk)j is a monotonically

decreasing sequence. Since M is-positive definite and f is a

quadratic function, f is bounded from below and thus there is some value to which f(zk ) converges as k -> ~0.

The set S = {xlf(x) <= f(z'), x >ZZ 0) is compact. It is closed because f is continuous. Furthermore, S is bounded. Suppose

v. '7 S, i = 1, 2, . . . , and
I

IIv.1II -> 00

where II l II

is the Euclidean

1T norm. In the quadratic function f(v) = 2 v Mv + qv, we may assume

that M is symmetric and has real eigenvalues. Let A, > 0 be the

smallest eigenvalue. Then, by the Fisher Minimax Theorem [4, p. 721

vTMv > ?i vTv i- i= iii=

A1 IIvi/!2;

thus

vpvi ->w

as i->m.

Since

1T the quadratic term 2 viMvi

dominates the linear term qTvi as

I/v.Jl becomes large, we conclude that f(vi) -> 00 as i -> 03. But

this contradicts the assumption that f(vi) 5 f(z") < ~0 for all i.

Therefore the iterates (zk] are elements of a compact set

and have a convergent subsequence with some limit point z E s. I

Proposition 4. Using the notation develcped in the algorithm and

assuming that M is positive definite, then
and lim (zk-zk-l) = 0. k+w

lim (-zki-zki-l) = 0 k+m

Proof. From the proof of Theorem 1, for any k and each i,

d;) = f1k(?1) - d;)T (Mii'-ik + q.) -t! $ (d;)T Mii$ and

-fk(~k ii

+

~dk) i

=

-fki('Lkli)

-

h(d~)T

(Mii"2

+

si)

-t

~

'2(d~)T

Miid~

1where

d.k

=

;k i

_

Zk-l .
i

II

Adding these equations, we have

k k-l
fi( `i

- $(iF + Ad!) = -(h+l)(dF)T (MiizF f gi) f i (l-'2)(dk)T Miidt

2 (l-h2)(d:)T M.1.1dk1
>= o since h E (-1,l) .

26

Recall that C$ is the scalar chosen in Step 2 of iteration k and

thus A has the value hk E ~~-1. ii

These values satisfy the following

bounds:

-l<

min(O,

w-1)

< =

A:

<

us

-l<l,

for all k and all i. Therefore there is a scalar cx independent
c-
c of k and i for which 1 - (AF)2 >= a>o.

i

Since the {f(zk>] converge,

also converge.

Therefore
L

L - fr(z: + $dr)) = lim (fk(zk) - fk(zk"))

k+m i i

ii

=o

and also L

lim (dl)TMiid: = 0 . k-m

Since Mii is positive definite, then lim d; = o , i.e., k+a

lim k+w

(i:

-

zik-')

=

0.

Finally, lim k-+w

(zk i

zk-') i

= lim (1-hk)dk k+w ii

=

0.

Proposition 5. Let I be the index set of a convergent subsequence of the iterates {zk] generated by the algorithm. Assume the subsequence converges to the vector z. Then Mz+q>=O.
Proof. If the inequality does not hold, then there are integers i, j and N and some 6 > 0 for which k > N and k E I implies
27

However

( r: MitZ~
t<i

+

MiiZ2

+ C Mitz;
t>i

+

%.L)

< j

-6

(t

;iMit~;+l

+

Miigki +'

+

&Mit$+

q.) > 0 j=

l

Subtracting the second inequality from the first gives

(z
tii

Z;+l)

+

Mii(zki

-

iki+'))

< 3

-

6

.

Since the terms in parentheses become arbitrarily close to zero, we
I
c- have a contradiction. 1
L Proposition 6. Let I be the index set for the convergent subsequence
above. Then zT(Mz + q) = 0.
i
Proof. Suppose the contrary. Then there are integers i, j and N
and some 6 > 0 for which k > N and k E I implies that (z:)~ > 6
and

( z`
t<i

'it':

+

Mg,-.1 zik

+

However

Mitz; + Mii$ + z Mitz;-') = o . (6)

t>i

3

Suppose (Qj > 0. Then dividing equation (6) by (gk) ij
tracting the result from equation (5) gives

and sub-

28

(,

-

zk -k

\

cMl.l- c

i-

'i) + z Mi& t>i

-

z;-'))~

>

6.

As before, Proposition 5 leads to a contradiction. Alternatively,

-k suppose (z.)

= 0.

Pick N' > N sufficiently large to guarantee

13

.
that I(

when k>N'. If (?)j = 0 for all k > N'

when k c I, then I(;: - zF)j[ = (zF)j > g, a contradiction. If

there is some k E I which is greater than N' for which @Jj > 0,

then the analysis in the preceding paragraph applies and a contradiction

follows. 1

In summary, these results show that the algorithm generates a
sequence of vectors {zk} belonging to a compact set S. Given any
convergent subsequence of {zk}, its limit point z solves the linear
*\
. complementarity problem h,M)*
-_
Theorem 2. The entire sequence {zk] has a unique limit point z,
\
and z solves (q,M).
L..
Proof. Since M has positive principal minors, the linear complemen-
I.
tarity problem (q,M) has a unique solution (see [&I, [52]). Propo-
..
sitions 5 and 6 show that the limit of any convergent subsequence of
Tzk3 solves (q,M). Consequently, every convergent subsequence has
a-common (and hence unique) limit point z. Finally, the entire sequence {zk] converges to z since every convergent subsequence
does [49, P. 371. m

29

2 .6. On the Monotonicity of the Iterates zk, k = 0, 1, 2, . . . . If M is merely positive definite, one cannot conclude much
more about the sequence (zk ) than that it converges to a solution of the linear complementarity problem (q,M). However, if we further assume that M is Minkowski and require that 0 < cu <= 1, then a very interesting result obtains. We shall use the following characterization of Minkowski matrices from [ 171.
Lemma 1. (Cottle and Veinott [17]). M is a Minkowski matrix if and
*
and only if the solution z to the linear complementarity problem
(q,M) is the unique vector minimum* of the polyhedral set

-Lemma 2-. If M is Minkowski, ql-< q2 and z: solves z*l=> z2*.

), then

Proof. By Lemma 1, z: is the vector minimum of

Zi = {z:Mz + gi >= O, z>Z.ZO),i=1,2. But qlzq2

z1 c z2, so

z; E z2.

Therefore

z1 = 2* > z*

l

I

impliesthat

Theorem 3. Let M be a knkowski-matrix having diagonal blocks Mii,
i = 1, 2, . . . , m. If cu E (O,l] and z" = 0, then zk+l >= zk for
all k = 0, 1, 2, . . . .

*
I.e., z* E Z and z*= <z forall zEZ.

30

Proof. The proof is by induction. Assume M is partitioned into

submatrices M. ., i, j = 1, 2, . . . 1J

, m, and that q and

Zk

are

conformably partitioned (e.g., zk = 1('zkz*kJ l ** 9 z;,,. Since

z1 > 0 = z" , we may assume that zk > zk-l >

0

l ** > z

and

zk+; J 2 z; for j<i.

Let

;k+l = = = i solve the linear complementarity

problem

( j<zIi

M..z:+' 'JJ

+c j>i

M1. J.zJk.

+

gi'

Mii)

l

c Since

i

j<ci

M'.J.zJk+`+

z j>i

M . '. Jz.Jk+q,i<=j<)ij

M1..Jzjk

+c j>i

M..zk-' 'JJ

+

q1- J

c-

L Lemma 2 implies tha;tk+.l >- ik.

Reca11

that

k:l - k' 'i = zi +

W(-Zk.-i-l- zk) 1i

where

UJE (0,ll.

r

We next show thatk+zli > zr for all i and all k. =

Clearly,

Li

z

0 i

=

0,

so

may

assume

for r <= k. Therefore,

Zk+l i

=

&k+l i

+ (l-m)z;

>=

cuz-ki

+

(l-uI)z;

>

UX

k i

+

(1-U)ZF

= zki .

I

i

Theorem 3 may be made more intuitive by examining a simple

case in which m = 2 and n 1=n2 =l* In Figure 6, we illustrate the sequence {zk) generated by the algorithm when cu is equal to

one. The zigzagging which occurs causes slow convergence as we approach

the solution z*. This problem is mitigated when values of cu greater

than-one are used. However, in those cases, we lose the monotonicity of the vectors {zk].

The next theorem shows that we can also approach z* from above.

31

c
L
cI -

L

Figure 6.

Path of' Algorithm IT/ with CU=l,m=2,n =1
i'

32

Theorem 4. Let M be a Minkowski matrix having diagonal blocks Mii, i = 1, 2, . . . , m. Furthermore, let z* solve (q,M). If cu < (O,l], Mz" + q => 0 and z" => z*, then zk+l <= zk for all k = 0, 1, 2, . . .

Proof. The proof .
and MzJ +q>= O

is by induction. We shall assume that z" => zl> . .

for j<=k.

We

may

further

assume

that

zkj=>

z

k+l j

.> =

zk

for j < i. Let ,:+l solve the linear complementarity problem

( c Mijz;+l + G' Mijz; + giZ Mii>' Then, by assumption

j<i

jSi

o =<

Mi&

+ t" j<i

Mijz;

+- z' M..zk j>i'JJ

+

cfi

<M..zk+ y M..zk+' + ? M..zk + gi l

=

11

i

j<"i iJJ

j3i 'JJ

Thus, by Lemma 2,-k+zli <= zk

and so

Zk+l i

=<

zki'

Furthermore,

M ik+' ii i

+ x M. .zcl j<i '33

+

7 M. .zk
j>'i

+'J. JCJ.

> 0. =

(8)

Since zik+l

is a convex combination of

;k+l i

and

zik, equations (7

and (8) imply that

Mii zik+'

+ 7 M..zk+' j ~ i 'J J-

+ -' M . .zk. + qi > 0 . j$i iJJ =

These arguments hold for all i = 1, 2, . . . , m. Since M.ij =< 0 (componentwise) for i # j, Mzk+l +q>ZZo. Consequently, zk 2 z k+l and Mzk+l + q >= O forall k. I
Intuitively, one might guess that a tldualH version of Theorem 4 exists. For instance, if M and u) are as above, z0<= z *, and z 0
33

is an element of the same cone but lies on the other side of the apex, i.e., Mz 0 + q <= 0, then Zk <= zk+l for all k = 0, 1, 2, . . . . However, Mz" i- q <= 0 may imply that Z0 2- 0, an undesirable situation. A slight modification of these hypotheses will correct this problem.

Theorem 5. Let M be a Minkowski matrix having diagonal blocks Mii,

i = 1, 2, . . . , m. Furthermore, let z* solve (MO and assume that m

implies that ( jZ,F1 Mijz; + %t> =co. If w E (O,l] and

* z , then

Zk <= ,k+l

for all k = 0, 1, 2, . . . .

Proof. The proof is by induction. We first establish that (~1)~ = o

implies that (z'i)t = 0 for all r = 0, 1, 2, . . . . Since 0 =< z"=< z*,

we may assume that 0 4 zr <= z* for r = 0, 1, 2, . . . , k and

0

<=

zk+' J

< z* =j

for

j

<

i.

Therefore

F M.-z:+' j~i'JJ

+ v M..zt + gi

j~i

1JJ

> 7 M. z*

=j~i

1J

j

+

5; M Z*. . . j:iiJJ

+

gi l

By Lemma 2, 0 =< it+15 zr and consequently 0 =< zik+l =< z*i' Thus,
0 =< zr =< z* for all r = 0, 1, 2, . . . . and we have resolved our first
problem. Next, suppose 2 = (zkl+l ,-zk2+l,..., zik+ll,zki,zki+l'...'z~)
is known and 2 satisfies the hypotheses of the theorem. We may determine zik+l by applying Algorithm III (modified point SOR) to the
linear complementarity problem

(P,A) 3 ( ;i: MijzJ+' + hl MijzS + gi, M..) .

j7i

j?i

11

34

We let x0 = z: be the initial guess, xt = (xt, t

t ) be i;!lt>

successive iterates, and let x*= lim xt. t -jw

*x2' (Note x

l
=

**

' "r,,

;k+l '

i

)
l

w

shall demonstrate that if xt satisfies the hypotheses, then x'+'

will also.

Assume

y

s

(xt+1 1

,

xz" Y

...

t+1 t t

,

xj-l'

xj,xj+ly

l'l'

txn

>

has been generated by the algorithm and satisfies the hypotheses. i

There are two cases.

Case

1.

(z*) ij

=

0.

c- *

Since

p= > C r<i

Mirzz

+

q,

then

x* < z

z* i

and

, x. = 0. L rl

Since

X0
j

must also equal zero , we may assume that xi = 0

for all t = 0, 1, 2, . . . .

iI--

t Case 2.

Then (Ay + P)~ < 0. But xt+l = =

1

maxI0, xt

t xj+l,

xj+t2Jy

.

a(& + pJj? > xt. =J
. . , xnt ) + p 5 0
i

Furthermore, A(x1t;+i ,x2t+1,,... since aij <z O for ifj.

,

x+1 j'

We

may

conclude

that k6-i,-1 =

* x

<= *z,zki=(

;k+l i

and thus

Zki=<

Zk+l i.

Since Mij <= 0 (elementwise) for i # j, then

k+l

( z1 , zE+l, . . . , zk+l, Zk i i+l'

Zki+2' l **

'

satisfies the

hypotheses. The rest follows by induction. 1

2.7. The Algorithm Interpreted as a Manifold Suboptimization Technique.

In this section , we shall transfer our attention from the linear complementarity problem to its related quadratic program. In

order to facilitate the following discussion, we create a more general _I
setting for the problem. We may view the function to be minimized as

one defined on the product space v= : v Consequently we have i=] "

z = (z 1, z2, . . . , zm) E V where zi C V i*

Each zi will be

35

m
restricted to the subset Ei c Vi; thus zEE= n: Ei. Let i=l

{*,*); be
I

the

scalar

product

corresponding

to

V.1,

and

let

M, lj

a linear transformation from V. to vi. Then the function

be

f (z) = f(z1, z,,,L . . . , zm) mayjbe defined as

mn

f(z)

=

+2 i=l

jT1fl

(zi,

Mijzjji+

where gi Z V.1.

In the case of Algorithm IV (Modified Block SOR), niXIll.

Vi

= Rni,

Ei

=

(x:x

C

Vi, x > =O?,

and

M.. E 1J

R

J. Recall that we

assumed that the matrix M (having partitions Mij) is positive

definite and that the Mii are, furthermore, symmetric. With this

notation, we may state an algorithm for the minimization of f over E.

Algorithm V. 0
Step. Let z = (zy, ~20, . . . , 0) E E and let u,C (0,2) be given.
Set k=O and i=l.

Step 1.

Determine

;k+l i S Ei

for which

k+l k+l

Zk+l ;k+l k

f(Zl 45

Y l **, i-1'

i Y.Zi+ly**`y zkm)

< f(zk+l k+l k+l k
=I 1 'z2 Y l **, Z i-l' VYZ i+lY " 'Y mZk) for every v C E..
1

36

Let zk+l T zk + ,ok+l(ik+l Zk) i i i i -i'

Step 3. If i = n, go to Step 4. Otherwise return to Step 1 with i replaced by i+l.

4S.tep

rs Zk+l Itreasonably" close to the solution?

If so, stop. Otherwise, return to Step 1 with k replaced by k+l

and i = 1. c

/ I

Notice that Steps 0, 1, 3 and 4 of Algorithm V are essentially

L

identical to the corresponding steps of Algorithm IV. For the problem

i11- described above, the algorithms are, in fact, identical. In Step 1

L of Algorithm V, we perform a constrained minimization of f on the manifold of V de termined by using fixed values in El, E2, l l * Y Ei,1'

Ei+l' l ** ' Em and letting the minimization take place in Ei, the

constraint set in the space V.. 1

This is equivalent to solving

i

minimize

f;(u) = $ uTMiiu f ( 7 M. .zrcl j<"i 'JJ

+7 j>"i

Mijzjk

+

s)' *

u

subject to UC E.1.

(9)

But M ii

is a symmetric positive definite matrix by assumption.

Hence

;k+l i solves (9) if and only if

;k-+l i solves the linear complementarity

problem

(j7i

However, this is Step 1 of Al.gorithm IV.

37

If

we

let

cu

=

1,

then

oik+l

=

1

and

zki+l

=

ik+l i

for

all

k and all i. In this case, Algorithm V is a typical example of a

manifold suboptimization algorithm [63]. When cu is greater than 1,

we have an accelerated version of a manifold suboptimization technique.

The results of Section 2.5 apply and we have convergence for any value

of u) strictly between 0 and 2.

i 2.6. Related Manifold Suboptimization Techniques.

L Methods similar to Algorithm V have appeared in the literature

on the minimization of functionals on Hilbert spaces or reflexive

Banach spaces. J. Cea [8] treats the case in which the bilinear form

(corresponding to our quadratic form uT Mv) is continuous, symmetric
t

I

and coercive.

The sets Ei,

are closed convex subsets of V.. 1

Under

t these hypotheses, Cea proves that if cu = 1, then the zk, k = 1,2,...,

converge weakly to the solution.

i

A. Auslender [2] treats the case in which Vi

and E i

are

defined as above but where the gradient of f satisfies a uniform

Lipschitz condition on the closed, bounded, convex sets of V. If

Ei $ Vi, he requires wt' (O,l] for convergence of his algorithm.

In the unconstrained ease, i.e., Ei =-Vi, cu is permitted to assume

any value strictly between 0 and 2. If V is finite dimensional,

the Lipschitz condition on f is relaxed and replaced by a much

weaker condition.

R. Glowinski [30] uses the same hypotheses as Cea. However,

Glowinski's algorithm modifies Steps 1 and 2 as follows. He minimizes

f' over Vi

instead of

E i

in Step 1.

In Step 2, he uses a fixed

38

value of wi for i = 1, 2, . . . , m and

by lettingZk-ti.1= P.(zk + w (Zk+' - zk))

1 i ii

i

guarantees where Pi

that zki+l is the

S

Ei

Uorthogonal projection operator from Vi -to E. corresponding to
1

the norm* induced by Miio" Glowinski states , without proof, that

if the oi 5 (0,2), i = 1, 2, . . . , m, then the iterates {zk] converpe
strongly to the solution.

t The research of J.-C. Miell ou 1431 and of B. Martinet [42] i is also of related interest.
i.

* lb.4 = ( Miiv'v' where Co,*) is a scalar product .
39

SECTION 3
APPLICATION--THE JOURNAL BEARING PROBLEM

3.1. Statement of the Problem.

A journal bearing consists of a rotating cylindrical shaft

(the journal) which is separated from a bearing surface by a film of

lubricating fluid. The journal and bearing are of length L and have

parallel longitudinal axes (of rotation). A typical journal bearing

is shown in Figure 7 as is an unfolding of the bearing surface into
tI the plane. A cross- section perpendicular to the axis of rotation is

L depicted in Figure 8. The mathematical description of the system will

be stated using various coordinate systems as need dictates. A

I: L

description of the cross-section is most easily couched in polar

coordinates whereas a description of the entire journal bearing has a

L more natural setting in rectangular coordinates.

We wish to know the distribution of pressure on the lubricating

film. An important underlying assumption of the model is that the

lubricating film is so thin that there is no variation in pressure

in the axial direction. Therefore (in Figure8), the pressure is

constant on the Hline" from the journal to the bearing for each value

of 8. Consequently, one may view the problem as the determination

of the pressure distribution on the lubricant of the bearing surface.

An initial understanding of the journal bearing model may be

obtained by first examining the cross-section of Figure 8. We shall

review Cryer's [19] description. The thickness* of the film

sI.e., depth, not viscosity.

40

Free Boundary

Direction of Shaft Rotation
Surface Velocity = u Rotation Velocity V = u/rD
Figure 7 . Side View and Exposed (i.e., Developed) View of a Finite Length Jolnrnal Bearing

Bearing

e=e min

- Journal

Figure 8. Cross Section of a Journal Bearing ( "*" marks the center of the bearing)

is minimum at 0 min

where the angle 8

measures rotation about the

z-axis, the axis of rotation. In the case of a prtial bearing (one

which does not completely encase the journal) the lubricant flows out

at 8 T

and is replenished at 8 0.

In the case of a full bearing,

where 8T = so I- 27~~ the liquid which may have vaporized is assumed

to condense at 8 T into its previous liquid state. (In the full
model of Figure 6, the lubricant can also flow out of both ends of
4L
the journal bearing.) The thickness of the film is denoted by h(8,z);

it satisfies

LI -

h(@,z) > 0

' ' [eO,eT]

L ah as<O

' ~ (Bmin"T) .

L The pressure on the film can be expected to increase between 8 = e.

and e = 8min

and to decrease between

8 = emin and

8 TO

It is

assumed that when 0 = 8 f, the pressure becomes so low that the

lubricant vaporizes. The interface between the two boundaries of the

L

[39]).lubricant is called the free boundary (see

In the finite

length bearing of Figure 7, the location of the free boundary depends on the axial coordinate z and is denoted by e,(*). The pressure is zero (i.e., atmosphere) along and beyond the free boundary 8. This is discussed in more detail in Section 3.4. In Figure 9, we illustrate the profile of the pressure distribution on the lubricant at the

43

Figure 9. Isobars for Developed Bearing

0.8,(I-E. xcerpted

With c = with Permission

of

D/L -ti

=

1

-

[41.Ins-Lit. Mech. Eng l y from

e P-I.&1sher )

44

tL
Li
ctI I i
i i-
1
1. /
I C
! i

(developed) besAng surfxe. TCzC-c=_wtztions and graph we,-e Cone IJ Cameron and 7ood [L]. This journal CetZririg has an eccentricity ratic E = e/r equal to 0.8 and a bearlog diam&er-to-length ratio of D/L equal to 1. The isobars (constant pressure contours) are given in nondimensional units (103R3/$.Jr2)p. 'Die variable p is the pressure, R is the bearing radius, p is the tiscosity of the lubricant, LJ is the surface velocity of the journal, r is the minimum clearance between the bearing and the journal and e is the distance bet-geen the two axes (see Figure 8).
3.2. The Re~jnolds Eq-iation
In 1886, Osborne Reynolds [&8] developed the now-classic
equation governing the mechanism of hydrodynamic lubrication by .ir:compressible flui&. The equation, a special case of the more gene&ral Navier-Stokes equation [45, p. $3, is deduced from seven essential assumptions 0 n the physical properties of the system (see [45, p. 5;).
0) The dimensions are sufficiently large to justify ignoring the
curvature of the journal bearing when studying a small section of it.
(ii> The pressure across the film (from the journal to the bearing)
is constant; i.e., ?3p,/3y = 0. (iii) The fiow is laminar, i.e., there is no turbulence in the film.
0 V 1 There are no external forces acting on the film. ( V-1 The fluid inertia is small ::ompared ta the viscous shear. This means that the rotationa 1 forces of the journal acting on the lubricant are much larger than the natural tendency (e.g., fluid gravity) of the fLuid to remain at rest.
45

(vi) There is no slippage of the fluid at the bearing surfaces. (vii) If u and w are the velocities of the fluid in the x- and
z-directions, respectively (see Figure 13),then all velocity gradients are negligible compared to au/ay and aw/ay.
Pinkus and Sternlicht [45] note that in most practical cases, the bearing is stationary and only the shaft is moving. In these cases, the most general form of the Reynolds equation is
The variable p represents the density of the lubricant, p is the absolute viscosity, and V0 represents a velocity resulting from the motion of the journal center. In the ensuing discussion of equation
( 10) ? we will postulate that V. - 0 and that p and ~1 are constants.
In order to gain a better understanding of the model of a journal bearing of finite length, we first examine a simpler model. By means of this special case, we may motivate the boundary conditions for the problem of more general interest.
3 -3. A Limiting Case: The Infinite Length Full Journal Bearing. If we suppose that the length L of the journal bearing is
infinite, certain further simplifications may be made. We may disregard the effect of fluid flow from the ends of the bearing and therefore ap/az, the pressure gradients in the axial direction, will be zero.
46

Obviously, an infinitely long journal bearing is a physical impossibility and does not closely approximate the dimensions of those used in practice. However, it does provide some understanding of' the behavior of more realistic bearings. Some notable similarities between the finite and infinite length models are the following. The infinite case provides upper limits on both the pressure exerted on the fluid film and on the loads which the film will support. Moreover, Pinkus and Sternlicht [45, pp. 69-711 show that the solution to equation (10) (which describes the finite length journal bearing) is a perturbation to the solution of the infinite length journal bearing problem. The perturbation involves adding the product of the solutions of two differential equations of a single variable. (To the authors knowledge, this realization has not borne fruit due to the difficulty of solving the latter two differential equations.)
As Pinkus and Sternlicht indicate [45, p. 681, the difficulty in obtaining satisfactory solutions for journal bearing problems lies
t
not only in solving a given formulation but in adequately defining the boundary conditions for the formulation. For the remainder of the paper, we shall assume e. = 0. In order to determine these boundary conditions for the simpler model, we first recall that there is no pressure variation in the axial direction. Consequently, it is sufficient to examine an arbitrary cross-section perpendicular to the axial direction (see Figure 8j. Generalization to the finite length case (where, for a given 8, there is pressure variation in the axial direction z) may then be thought of as the examination of a collection of cross-sections along the z-axis, say at z = z 1,~2,~3,...,zN, where
47

-- the boundary conditions for the i th cross-section are analogous to those for the infinite-length journal bearing model. Replacing the variable x by R8, where 8 is in radians and R is the bearing radius, and recalling that ap/az = 0, then the Reynolds equation for an infinite journal bearing is
We may use full instead of partial derivatives because both p and h are now functions of 8 alone. Furthermore, since 6pR is a constant, a change of units is sometimes made to allow setting it equal to unity.
In 1904, Sommerfeld obtained* the first solution to equation L-
(11); he addressed the full journal bearing case in which the boundary I s. values were P(0) = P(N = P,. He also assumed that both journal and
bearing were cylindrical and hence (h being a function of 8 only),
h(Q) = r(l + E cos 0) .
The parameter p, is the ambient (or atmospheric) pressure and is usually set equal to zero. Sommerfeld's expression for the pressure distribution was
~~URC (2 + E: cos 0) sin 0 P(0) = P, + r (2 + E2)(l + E cos e)2
Setting p, equal to zero, the graph of p(B) becomes
*
by a clever transformation of variables

The important thing to notice is that equation (12) yields regions of high negative pressure. This model was unacceptable since a lubricating fluid cannot support such high negative pressures and still remain an incompressible fluid. The underlying problem was that as 8 increased beyond emin = T, the width of the film increased and consequently the pressure exerted on the film decreased. Eventually,
at 8 = ef, the pressure became so low that the tensile strength of
the fluid was overcome and the fluid vaporized. Since the Reynolds equation only holds for incompressible lubricating fluids and the region of the journal bearing beyond the free boundary, i.e., 8 > ef, contained a compressible gaseous lubricant, it was no longer valid to apply equation (11) over the region
(y%$ l
Thus, a different set of boundary conditions was needed to provide a more realistic solution to the problem of determining both the region (BO,Bf) in which the lubricant exists as a liquid and the pressure P(Q) in that region. From the literature, one infers that the boundary conditions commonly used today are due to Swift [55]. They state that when the pressure falls to zero, the
49

circumferential pressure gradient dp/de also falls to zero. In other
words,
( i > P(0) = 0
.___
( >ii P(Qf) = 0
\
(iii) $$ (e,) = 0 .
L-

Clearly, the pressure function p may be continuously extended on the

interval [Bf,eT] by setting it equal to zero on that interval. From

the results of Cryer [19], the free-boundary Bf occurs at the largest

value of 0 = 8 for which p is nonnegative on P,A ' Sommerfeld's technique for solving the differential equation

L. with these boundary conditions is still applicable and yields the

following complex expression for p(8) in terms of another angle, $ \

L

6pUR

PW =

232

r2(l-e ) /

t

2+4q q-esinq-(

- 4E sin e +
2(1 + e COS(jJf

2 e sin - 7-r)

\Ir

cos

9

1 f

\

03)
\

where

E + cos 8 cos gf =
i + E cos 8

and JIf

corresponds to

8 f*

The location of the free boundary 8, is not immediately

L... apparent from the original problem. However, the boundary condition ~(0~) = 0 yields an implicit formula for \Ir,.

&i&ff-d COSbf-7d - .gIf) + 2(4ff COS(~~-~T) - sin(q,+)= 0

The solution under these new boundary conditions has the following graph.
50

Notice that the regions of negative pressure found in the graph of equation (12) do not appear in this graph. This is the principal reason for using the boundary conditions of Swift.
3.4. The Use of Finite Difference Techniques. Before leaving the case of the infinite length journal bearing,
we wish to discuss a situation where Somrnerfeld's technique does not apply and where no other means of obtaining an exact solution is currently known. An example of this might be one where the bearing is not cylindrical and hence the width function h does not have the common form h(B) = r(1 + e cos 63). In 1941, Christopherson [lo] proposed a technique forsolving free boundary problems for journal bearings by means of approximating the differential equation by finite differences. Later, improvements on Christopherson's method were made
II by Raimondi and Boyd [46] and by Gnanadoss and Osborne [29]. The
former solved the difference equations by modifying the Liebman
L
(or Gauss-Seidel) method, the latter by modifying successive over-
relaxation (SOR). In 1971, Cryer [19] analyzed the numerical aspects
51

of Christopherson's algorithm with the SOR modification when applied \ to the infinite journal bearing case. He proved that if
L
L (i) n is the interval length on the approximation grid, (ii> Pj = PLU> is the true value of the pressure at jA, i.e., at the j th grid point, j = 1, 2, . . . , N, (iii) Pj is the discrete approximation value at the j th grid point,
L j = 1, 2, . . . , N, and L (iv) n is sufficiently small, then there is a K < 00 for which
maxlp. - PjI < Kn2
L "-
j'
Furthermore, he showed that the boundary conditions (in particular, the tlfree boundaryI') cause this problem to be equivalent to a linear complementarity problem (q,M). The matrix M corresponds to the
finite difference equations which are fully discussed in [19].
3 l 5* The Finite Length Journal Bearing Model and an Approximation.
A realistic mathematical model of a finite length journal bearing has great potential for becoming very complicated. For instance, the lubricant can be admitted through oil grooves to the bearing at any angle and the larger the angle, the more pronounced is its effect on the resulting pressure distribution. Further, the lubricant is i not always admitted at atmospheric (i.e., zero) pressure. These and other factors contribute a significant complexity to the formulation.
52

In our discussion of the finite length case, we shall treat

a fairly simple model, one in which the bearing is a full (as opposed

to partial) cylindrical bearing. Ambient pressure is taken to be

zero. As in the infinite length case, the lubricant that vaporizes
at the free boundary is assumed to condense along the line where 8 = 0.

The boundary conditions are a natural generalization of (i)-(iii) for

the infinite length case (see [29]). As indicated before, it is

easier to present the finite length case in rectangular coordinates.

Referring to the bearing surface of Figure 7, we shall let p(x,z)

represent the pressure on the lubricant along the bearing surface.

The boundary conditions are

( i>'
( ii> '

P(O,Z) = 0 p&D, z) = 0

for all z, for all z,

(iii)'

( iv> '

P(ef(z)'z) = 0

(V> 7

?JZ (ef(z),
an

Z)

=

0

for all x, for all z, and for all z,

where 8 f is the free boundary, and 26n2 (e,(z), d is the normal

derivative of p at (e,(z), z), i.e., the derivative of p in the 4,

I L direction normal to the tangent of the free boundary Gf at (ef(Z),z).

(In the case of the infinite length journal bearing, the normal derivative at ef becomes 2 (Of> = 0 as in (iii) of Section

.

3.3. >
L Since even this relatively simple model of the finite length

journal bearing has eluded attempts to obtain a closed form solution

by analytic means, other avenues have been explored and have met with
-
53

more success. These alternate methods have included electrolytic tank
models, d-c analogues and finite difference models (see [45]). It
is the last category to which Christopherson's method belongs. To develop the discrete model, we shall first follow Pinkus
and Sternlicht [45, pp. 79-811 in deriving a five-point finite
difference approximation to the Reynolds equation. By a change of variables, we first obtain a dimensionless version of equation (1). Let G = x/D, 2 = z/L, G = h/2r, and 5 = (r2/pVR2)p where V is the speed of the journal measured in revolutions per unit time. This yields
04)

Dropping the bars above the variables and referring to Figure 16, we

I1 L- have the following finite difference representations. j
I i

I - (h3 2) = h3i,j+l 2

L-
II! .

h3i+1/2,j

P*l+l,J.-p.l?J.
AZ

- hi-l/2,j

AZ

ah hi,J'+l /2 - hi,j-l/2 ax= Ax

.

After rearranging terms, the evaluation of the equation at grid point

54

n\

X LZ

i,-,-+
3- - l - + - t I 11
2 - -1-t-t
l 1 I I*
12 3 i

m

i c
L P.3,jf1
I t-
L :t h i,j+l/2

i YP nu

P.l-1,j

h i-1/2,j

P.lj

h i+1/2,j

P.
l+l,j

TL

I?*i,j-1 Figure 10. Finite Difference Approximation
55

(i,j) takes the form

W ij

=

q ij

+ a.l,j,l'i,j-1 + a.l,j,2'i-l,j + ai,j,3Pi,j

+a i, j,4'i+l,j + 'i,j,5'i,j+l

05)

where

a - -h3i,i,j,l -

j-l/2/(k)2J

a.i,j,2 = - (E)' h

3a.1,

j,3

D = (C >

2

(h:+l,2, 3. +hi-1/2 2
bd

j)

+

(h' *+1/z ljJ

+

hi3,Ja-1/2)

2

b3

J

a.l,j,4 = - (f)' h

aiA5

--

-h3
i,j+l/2/(h)2

'

c1. J

=

67-

(h i

-+1/ 2ihi ,j-l/2 >

and

w ij

--

0

if

the

pressure

at

(i,j)

satisfies

the

Reynolds

equation.

If i - 1, 2, . . . , m and j = 1, 2, . . . , n, then the discretized

version of equation (1) is an (mn) X (mn) linear system. For each i, we

define the entries of the matrix M as

56

r-nl. ,j-n - %JW
mi,j-1 = ak,R,2
m.bj = ak,j,3

m.1, j+l = "k&4

m.l,j+n = ak,R,5

i

and

mi,r = o

for all other r

blhere k-l is the largest integer not exceeding i/n and where
i
tI - R = i-kn. In addition we let the subvector pi = (pil, p i2' l ** f Pin > and the vector p = (~1, p2, . . . , pm); we define the vector q

I similarly. The matrix M and corresponding vector q form the basis

for an approximation to the model of a finite length journal bearing
i
having a free boundary 8. As in the infinite length case, there is

an associated linear complementarity problem (q,M) whose equivalence

is illustrated by a synthesis of Christopherson's original application

[lo] of his method to the finite length journal bearing problem and

Cryer's later discussion [19] of the method and its application.

Intuitively, the complementarity problem arises as follows. Denote

the region where the lubricant exists in its liquid (vaporized) state

as the positive (zero) region. These appellations refer, of course,

to the pressure on the lubricant in those regions. In the positive region, the Reynolds equation is required to be satisfied. Hence, if the grid point (i,j) belongs to the positive region, then

W. = 0 and the discretized version, equation (15), becomes 1J

57

wij =

!L.J .

+ m.1, j-nPj-n

+ mi,j-l'j-1

+

m. 3. 193 J

+

m.1,

j-l-lpj+l

+

mi, j+n'n=

O*

On the other hand, if the grid point (i,j) belongs to the 'interior"

of the zero region, then the pressure variables associated with the

adjacent points (p.l,j-n 9 P.l,j-1 9Pi,j+l' 'i,j+n ) have zero value.

Consequently, equation (15) becomes

w ij

=

q, lj

=

6T(hi,j+l/20hi,j-l/2 )/A l

However, the location of the free boundary and the zero region requires

c hi,j+1+,2 - hi,j-1/2, and hence wij, to be nonnegative. Summarizing,

t:i

we have a variable p.. 1J

and an algebraic expression

w ij

associated

with the point (i,j) and related by w = Mp + q. If p.. is 1-J

positive, then w.. 1J

equals zero and if

p ij

is zero, then w ij

is

L nonnegative, i.e., p and w satisfy the conditions of the linear complementarity problem (q,M).

If the bearing is cylindrical in the example discussed above,
then h(x,z) is independent of z and consequently hi-1/2,j and hi+l/2,j is independent of i. From this observation, we may draw several conclusions about the matrix M.
(i> M is a symmetric block tridiagonal Minkowski matrix where
Mij ERnXh and i, j = 1, 2, . . . , m. (ii) Mi+l i = Mi i+l = aiI~ where .ai < 0 and i = 1, 2, l .= , m.
9t (iii) Mii is a tridiagonal matrix whose subdiagonal and superdiagonal

entries are identical and whose diagonal entries are identical.

With this structure, the Modified Block SORAlgorithm may be brought to bear on the journal bearing problem. The computational experience reported in the next section demonstrate s the efficacy of this approach.

58

SECTION 4
COMPUTATIONAL ASPECTS OF ALGORIl-!HM IV

L.1. Storage Requirements.

We first address the question of storage requirements for the

most general form of Algorithm IV. In this case, M is merely assumed

to be positive definite with symmetric diagonal blocks, Mii- The

matrix M is partitioned so that Mii is a square matrix of order n.1
for i = 1, 2, . . . , m. Then, for each i, there are, say, Ni non-

zero double precision matrix entries and ni double precision entries

for each of the subvectors gi and zi. If one uses sparse matrix

techniques to store the entries of M, additional storage demands are

made in the form of row and column index vectors. In the algorithm

itself, the updating of the solution vector iterate z.k1 requires

sufficient space to solve the complementarity problem

(y,M) = (gi + c M. .zlf+' + 2 MijzJ, Mii). This means allocating

j<i `JJ

j>i

space for a copy of 4 and z as well as any additional space re-

quired by the complementarity subroutine. Notice that it is not

necessary to have all the initial data constantly available in core.

For instance, it is sufficient to have the vector z, the subvector

97..,

the

submatrices

M., for 13

j

=

1,

2,

.

.

.

,

m

and

appropriate

storage for solving G,M) l

By restricting our attention to the block tridiagonal case

where Mii

is symmetric and tridiagonal and both Mi i+l and 9

Mi+l i 7

are diagonal matrices, we find certain economies in storage. Suppose

the diagonal blocks Mii are of order ni = n for i = 1, 2, . . . , m.
59

Then, one can easily show that the storage required for M, z and

q is 6m.n + 4n - 2m - 2 (8-byte) words.

What further requirements does Step 1 impose? If Mii is

solved by Lemke's algorithm or the principal pivoting method, we need approximately n2 more 8-byte words. If we further assume, as

above, that Mii is Minkowski, then Algorithms I-III are applicable.

Recall that Algorithms I-III preserve the sparsity of the data. Their additional requirements are approximately 40n, 60n and 40n bytes

of storage, respectively. Of course,-savings (of 8mn-8n bytes) are

LI -

achieved when M is symmetric and more dramatic savings occur when M corresponds to the finite length journal bearing problem described

L in Section 3.5. In the latter case, M has attributes (i)-(iii)

found on page 58.

These storage estimates represent the minimum necessary for

the algorithm. Computational refinements (e.g., reduction of multi-

plications by zero) make further storage demands in the manner of

sparse matrix techniques (i.e., in the form of index sets incorporated into the computer program).

4.2. The Computer Codes. . Three computer codes have been written for Algorithm IV, the
Block Modified SOR Algorithm. They differ from each other in the way that each solves the subproblems found in Step 1. The programs are written in IBM 360/370 Fortran Iv and use double precision (8-byte) floating-point arithmetic.

60

The matrix M is required to be symmetric, block tridiagonal, and positive definite. Furthermore, the diagonal blocks Mii are required to be tridiagonal Minkowski matrices and the off-diagonal blocks Mi i+l and Mi+l i must be diagonal matrices. The "tri-
J9 diagonalV linear complementarity problems occurring in Step 1 are solved by Algorithms I, II and III, respectively.

4.3. Computational Experience.
A computational study of the problem (q,M) was performed in which we used two types of matrices M. The 'tJBV matrix corresponds
to equations (i-5)-(16), the 5-point finite difference approximation
to the Reynolds equation arising in the free boundary problem for the journal bearing problem. (The eccentricity E equals 0.8 and the ratio D/L equals 1.) The %P' matrix corresponds to the five-point difference approximation to Laplace's equation. (See Figure 3 for an example.) In both cases, the diagonal blocks Mii are of order n and m is set equal to n. Thus the matrix M is of order N = n' When the JB matrix is used, the q-vector comes in two varieties. One type corresponds to the finite difference equations for the journal bearing. The other is a random vector in which the absolute values of the components are chosen from a uniform distribution on [O,2] and their sign is determined by the formula

w-dqj > =

+l if - 1 if j(mod a) > @

61

where a and p are given constants. For instance, if 01 = 2C and

f3 = 10, then th e q-vector has a repeating pattern of 11 positive and
9 negative entries. The LAP matrix is used only with the random q-

vectors described above.

Algorithm IV uses two parameters, a stopping criterion tolerance E and a relaxation parameter u). We have set E equal to 10- 7 and for
each experiment, have determined (to within 0.02) the value cu of exp
the parameter cu which minimizes the number of iterations to achieve

the desired level of error in the solution. (In one of the three codes, we solve Step 1 by Algorithm III, the modified point SOR algorithm.

Algorithm III uses its own relaxation parameter cu' and for each

experiment, we have determined (to within 0.1) the value cur of exp
the parameter cu' which minimizes the total solution time when

cU=U! exp'

>

Finally, we shall use the following nomenclature for the

algorithms tested. Let BSORF, BSORP and BSORS denote the three versions of the Modified Block SORAlgorithm with the first solving

Step 1 by Algorithm I--the factorization method, the second by

Algorithm II--the modified principal pivoting method, and the third

by Algorithm III--the modified point SOR algorithm. Also, let PSOR

denote the Modified Point SOR Algorithm as coded for symmetric block

tridiagonal matrices for which Mii is a tridiagonal matrix and both Mi,i-1 and Mi,i+l are diagonal matrices.
The first experiment is a general comparison of the four

methods applied to a sample of each type of problem. The results are summarized in Tables 1, 2 and 3. (The number of iterations of BSORF, BSORP and BSORS is the same for each w.)

- 62

Table 1. Data: JB matrix, JB q-vecto~

I n CD
f=P

iter

BSORF see

BSORP set

15 1.30 18 0.133 -0.183

31 1.54 37 0.881 2.529 63 1.74 78 7.388 33.862

BSORS

1 (Oexp

set

1.3 1.797

1.3 20.517

1.3 182.291

PSOR
"exp iter
1.58 T 1.76 87 1.88 179

set
0.282 2.296 20.616

Table 2. Data: JB matrix, random q-vector, n = 16

BSORF BSORP

G

a

B ("exp

iter

set

see

-T -;; CL2
8 16 1.24

15 4 0.183
18 0.249

0.216 0.266

16 32 1.22 18 0.216 0.299

BSORS set
2.013 , 2.995 2.961

wexp
1.36 1.52 1.56

PsOR iter
26 36 39

set
0.183 0.24P 0.266

ukxp
1.34 1.50 1.32

Table 3: Data : LAP matrix, random q-vector, n = 16

iter

BSORF set

BSORP set

22 o.j16 0.316
33 0.332 0.482
21 0.282 0.332

BSORS
cu' exP
1.1
1.1 1'*I

set
2.329
4.143 2.579

PSOR uexp iter 1.46 --51
1.62 43 1.46 33

set
0.216 O-299 0.232

One notices that BSORJ? is almost always uniformly faster and
BSORS uniformly slower than the others. Further comparison seems to
be very dependent on the sign configuration of the q-vector. From
the results of Section 3.5, we may deduce that the sign configuration of the q-vector used in Table 1 is that the first n(n-1)/2 entries
are negative, the next n are zero (or negative if n is even) and the remainder are positive. Here, we see a pronounced ordering of L convergence speed (as measured in seconds), especially as n increases. From fastest to slowest, it is BSORF, PSOR, BSORP, and BSORS. In
contrast, the q-vectors used in Tables 2 and 3 have a large number of reversals in their sign configurations. Furthermore, a significantly
larger fraction of the z-variables are positive in the experiments of Tables 2 and 3 than in Table 1. These two characteristics tend to be f\ L- levelling effects, i.e., the running times of BSORF, BSORP and PSOR
are nearly equal (as well as we can tell in light of the systematic L-
error involved in measuring execution time in the multi-programming I\
environment of the IBM 360/91). The second experiment dealt specifically with the hypothesis
that when the number of positive components of the solution vector was
small, then BSORF was considerably faster than PSOR and that as the
number of positive components increased, the running times became
equal. A LAP matrix was used with m and n equal to 30. A sequence of constant vectors qt were used in which the first 30t components were -3 and the remaining POO-30t components were + 1. The results, summarized in Table 4, support the hypothesis. Since the number of
positive components of the solution vector is at least as large as the
64

number of negative entries in the q-vector (see [g]), this experiment may serve as a guideline in the choice of an algorithm for a specific problem.

30Table 4. I%ta: LAP matrix, random q-vector, n =

Ratio

l3SORF

PSOR

PSOR/EEORF No. pos.

wexp iter set

LoeXP iter set

iter set z-compon.

1.08 7 0.099 1.20 19 0.449 2.714 4.535 60 1.26 14 0.216 1.40 32 0.732 2.286 3.389 118 1.40 20 0.349 1.50 4-2 0.998 2.100 2.860 174 1.58 36 0.765 1.68 60 1.431 1.667 1.871 346 1.66 50 1.14-8 1.76 79 1.880 1.580 1.638 480 1.72 60 1.580 1.78 89 2.113 1.483 1.337 610 1.74 97 2.995 1.82 124 2 .g61 1.278 0.989 900

(In this Table, the relaxation parameter w was determined to within exp
0.02 for both BSORF and PSOR.)

!The third experiment attempts to relate the solution time to n. From Table 1, we find that a growth rate of order 3/2 holds between the order of the matrix (i.e., n2) and the solution time for BSORF (i.e., t a (n2)3'2 ). Doubling n -*increases the running time of BSORF, BSORP,
BSORS and SOR by a factor of about 8, 13.5, 9 and 7.5, respectively.
The results of further testing with random q-vectors are summarized in Tables 5 and 6. !These approximately support the factors determined from Table 5.

Table .5. Data: JB matrix, random q-vector

naB
16 4 8 32 4 8
16 8 16 32 8 16 16 16 32 32 16 32

EiSORF

%Q

iter -

set -

1.12 15 0.183 1.14 18 1.181

1.24 18 0.249
1.36 32 1.896
1.22 18 0.216 1.50 39 1.747

Table 6. Data: LAP matrix, random q-vector

EORF

naB
16 4 8 32 4 8 16 8 16 32 8 16 16 16 32 32 16. 32

oexp

iter -

set -

1.34 22 0.316 1.36 33 1.880

1.50 33 0.332 1.62 48 2.046

1.32 21 0.282 1.72 67 2.346

66

The fourth experiment demonstrates the sensitivity of the Modified Block SOR Algorithm to the relaxation parameter CU. The test problems used LAP matrices of order 1024 and random q-vectors. Since the number of iterations is identical for ESORF, BSORP and BSORS, we present the results only for BSORF. Summarized in Tables `;, 3 and 9, this experiment indicates that the convergence is fairly robust, e.g., if U is the optimal value, then we still achieve good convergence
exp rates for CUE [CD - .2, cu + .2].
exp exp

8Table 7 . I&ta: LAP matrix, random q-vector, n = 32, a = 4, p =

BSOIU?

cu iter set

cu iter

1.10 59 3.011 1.20 47 2.396 1.30 37 1.836 1.32 35 1.880 1.34 33 1.730 1.36 33 1.880 1.38 33 1.697

1.40 34 1.50 39 1.60 47 1.70 59 1.80 85 1.90 153

set
1.713 1.980 2.396 3.011 4.309 7.870

.
67
:t

Table 8. Data: LAP matrix, random q-vector, n = 32, a = 8, p = 16

BSORF

cu iter set

-cu Piter

set

1.10 >200 mm

1.62 48 2.046

1.20 175 7.288

1.64 51 2.063

1.30 140 5.807

1.70 61 2.529

1.40 109 4.459

1.80 87 3.577

5

1.50 81 3.377

1.90 163 6.739

1.60 53 2.163

i

f b-
L Table 9. Data: LAP matrix, random q-vector, n = 32, a = 16, p = 32 BSOF

r L

co iter set
1.40 >200 - -

cu iter

set

1.70 -F-- 2.612

! 1.50 174 5 0973 1.60 124 4.176

1.72 67 2.346 1.74 71 2.612

1.64 105 3.560

1.76 77 2.728

1.66 95 3.178

1.80 95 3.394

1.68 85 2.928

1.90 175 5 -923

The fifth experiment measures how much of the total solution

time is used by Step 1 alone. The results, reported in Table 10,

indicate that the subproblems use nearly one-third to one-half of the

total time. The times reported are somewhat inaccurate due to the

(16resolution of the timer

milliseconds). Despite this, the results

emphasize the importance of having a very efficient linear complementarity

algorithm for use in Step 1. Further investigation along these lines

68

might study the dependence of the solution time on the partitioning of the matrix, i.e., on the values of n1' 9' l - ., nm-

Table 10. Subproblem Solution Time vs. Total Solution Time

Matrix n a ii!

Subproblem Time (4

Total Time bet>

JB 31 nonrandom

0.688

J-B

63 nonrandom -

4.304

J-B 16 4 8

0.208

JB 16 8 16

0.176

J-B 16 16 32

0.304

LAP 32 4 8

1.409

LAP 32 8 16

1.664

LAP 32 16 3 2

1.792

l-999 13 l 3% 0.448 0.416 0.644
3.178 3.807 4.808

The sixth experiment studies the possibility of accelerating the convergence by varying the value of the relaxation parameter during the progress of the algorithm. It is sometimes profitable when solving systems of linear equations by overrelaxation methods to let z 0 = 0, cl?- = 1 and cok = & for some fixed & and all k > 2. The intended
= effect of this procedure is to reduce the variation in the components
of z 1 which would result if CU' were given a value greater than unity.
The overrelaxation technique then proceeds with some appropriately chosen value* of the relaxation parameter. In applying this scheme to Algorithm IV, we repeat the experiments reported in Tables l-3 and 5-9 and set * theoretically or empirically based
69

G equal to the LU determined in those experiments. As a point exp
of interest, we also determine the value & = 6 that minimizes exp
the number of iterations necessary to satisfy the convergence criterion when using w1 = 1. The results are summarized in Table 11. They indicate that the scheme has a minor effect, if any. When there is a change, it is usually a variation of one more iteration than in the preceding experiments. (However, one test showed a decrease of one iteration.)
Li Tfne eighth, and last, experiment studies another approach to
2.6,f solvingthelinear complementarity problem (q,M). In Section
I-
we indicated that when M is a Minkowski matrix, then the solution t to kbM) is the unique vector minimum of the polyhedral set
{z:Mz+q>=O, z>=O]. It is thus a simple exercise to show that the
- problem (q,M) is equivalent to the linear programming problem
Minimize cTz subject to Mz >= -q
z>= o.
I for any strictly positive vector c.- Letting c be a vector of ones, we solved the linear program with a production code LpMl [41] written at the Systems Optimization Laboratory at Stanford University. The data was a JR matrix of order 225 and the q-vector corresponded to the journal bearing problem. The LPMl code took 4.93 seconds with most of the time spent in the Phase I procedure. (Recall that ESORF took ,133 seconds to solve (q,M).)
70

Table iL Varying the Relaxation Parameter Using BSORF

Matrix CI B n
JB * * 15 .* * 31 * ++ 63 4 8 16 8 16 16 16 32 16 4 8 32 8 16 32 16 32 32

UJ = cuf", uj
exp iter uexp
1.30 18 1.54 37 1.74 78 1.12 15 1.24 18 1.22 18 1.14 18 1.36 32 1.50 39

cu =l
wk =Wexp' k>= 2
iter
19 38 79 15 19 21 18 33 40

cu =l

ok

&I exp'

k>ZZ 2

'exp iter

1.30 19

1.54 38

1.74 79

1.12 15

1.24 1-9

1.26 17

1.14 18

1.38 32

1.50 40

LAP 4 8 16 1.34 22 8 16 16 1.50 33 16 32 16 1.32 21
4 8 32 1.36 33 8 16 32 1.62 48 16 32 32 1.72 67

23 1.34 23 34 1.50 34 21 1.32 21 32 1.36 32 48 1.62 48
67 1.72 67

`I* " indicates that the q-vector corresponds to journal bearing data.
-71

Next, we solved the dual problem using the same data. In this

case, the zero vector was a initially feasible point, thus no Phase I

was necessary. The solution time for LIT!41 solving the dual problem . was 4.09 seconds.

Since the matrix is block tridiagonal, it can be partitioned

so that the nonzero entries exhibit a l(staircaseV structure. Under this partitioning, the corresponding linear programming problem was solved by the Ho-Manne nested decomposition algorithm [35], an algorithm

especially developed for problems with this structure. The running

time was 11.46 seconds. In all cases, the numerical accuracy was L comparable.
L Further experimentation might investigate whether a reordering

fLt

of variables might reduce solution time. One possibility is the so-called 'checkerboard" ordering.* Forsythe and Wasow [24, p-259]

have reported, however, that the (unpublished) work of M. R. Powers t has indicated the convergence of the SOR method for linear equations

may not be very sensitive to various orderings.

c
9.4. Choice of the Relaxation Parameter 0. The problem of determining a- 'theoretically optimal' value of
w for the PSOR algorithm applied to tridiagonal Minkowski matrices is discussed in Section 1.3. The setting was the application of overrelaxation to systems of linear equations. We now review and extend the key notation and results.

*
Also known as the flblack-white" or "odd-even parity" ordering. See [24, p. 2451.

Most generally, an algorithm may be expressed in the operator

form zk+' = .CZ?zk where zk", zk E Rnm. If we let z* represent

the solution and ek = z* - zk be the error vector, then z* = 9z* and ek+' = ZZek. We will let II-[/ b e any vector norm or its induced matrix norm with usage dictating its meaning.

Since lim (l/ekl//l/eol[)l'k < ~(-91, [58] w h e r e ~(2)

is

the

k+00 spectral radius

of

55'

and -e0

=

Z*

-

z0

is the initial error

i vector, we want to minimize p(g)- In the specific case where we are

L applying successive overrelation to the linear system Mz + q = 0,

tI - the operator 9 is formed as follows. We write M = D - E - F where D and (E + F) is a regular splitting of M (see [58, p. 881) and

L let L = D-?E, U = D-?I?. (The splitting used depends on whether we are doing point or block SOR. Since the operator 5? is dependent

r
L

on w and M, we express it as

9'(M) = (I - wL>-l [UN + (1-cu)L] .

When M is a tridiagonal or block tridiagonal Minkowski matrix, it belongs to the class of consistently ordered 2-cyclic matrices [58, pp. gg-lol]. Consequently, the relaxation parameter c+, that minimizes p(qJ can be uniquely specified in terms of B = L f U, the Jacobi matrix associated with M. From a formula of Young [62 9 p. 1691, the optimal parameter value is
cub(M) = 2/o +m ).

73

As discussed in Section 1.3, one approach to theoretically determining an "optima? value of w for Algorithm IV is to imitate the procedure for systems of linear equations outlined above. This
*
is a plausible idea since if z solves (q,M), then for large enough
*
valyes of k, (Mzk + q) i will equal zero when z i is positive. In other words, after a number of iterations, Algorithm IV will appear to act as a block SOR algorithm solving a subsystem of linear equations extracted from the original problem. More specifically, let N= E niJ s = Cl, 2, . . . , N), -T be an index set from $ and
i=l T' be its complement. Also let MTT be the principal submatrix of M corresponding to rows and columns j E T and let S(z) = (i Es :zi > 0`). The results of Cryer [20] are easily generalized to form the basis of the conjecture that the optimal U) for Algorithm IVis cuopt = %(MTT) where z* solves (q,M), w* = Mz?+ q and T = s- s(w*).
In the case where M is a tridiagonal Minkowski matrix, the theoretical estimate of u) is not supported very well by experi-
opt mental evidence [20]. However, when M is a block tridiagonal matrix, the correlation between theory and practice improves considerably. In order to demonstrate this, we first need to develop some technical machinery.
Recall that the expression for % requires the evaluation of -p(B) l In general, this is difficult to do theoretically. If the matrix is symmetric, an approximation may be obtained by setting
74

or by a variety of iterative methods (e.g., the power method [36,

p= 1471). In the special case of the LAP matrix, we can state o(B)

explicitly. We deal first with the Jacobi matrix B arising in the

PSOR algorithm, Algorithm III. We decompose M = D - E - F into a

diagonal matrix D and strictly lower and upper triangular matrices

E and F. The matrix M is partitioned the usual way into sub-

matrices M. lj

for i, j = 1, 2, ..; , m so that

Mii

isnXn. We

will next determine p(k) where K = (1, 2, . . . , kn] for any

k = 1, 2, . . . , m. (Note that BKK is the Jacobi matrix associated

with the LAP matrix %K).
.
Theorem 6. Let ~ be the matrix described above. Then

p(k) = $ (cos r/(k+l) + cos T/(n+l)>.

Proof. Define the s X s matrix TS = (tij) by tl2 = 1, ti i+l = 9

t i+l,i

=

1

for

i

=

2,

3,

.

.

.

,

s-l,

ts Y

s1 -

=

1,

and

t.. = 13

0

otherwise. Let Is be an s X s identity matrix. Recall that if

G and H are s1Xs2and s -Xs matrices, then their tensor
34 product (or Kronecker product [32, pp. 97-981) P = G@ H is an

sls3 ' ??4 matrix of the form:

75

gllH g12H . . . glS2H

P= . .

63s 1 lH gs 1gH -0.

QH "lS2

It is easy to show that h

= TK @ IN + IK @ TN .

BK ,= 4%

Let Q S

be the orthogonal s X s matrix whose column vectors are the eigen-

c vectors of TS and let LS be the diagonal matrix of eigenvalues;

1 thus

TQ=QL ss ss*

The matrix Q = ?K @%N

is orthogonal since

?K

and % are, hence QT$Q has the same eigenvalues as %A . (Note

that we have suppressed the explicit dependence of Q on K and N.)

L Using the fact that (Gl @ G2)(G3 @ G4) = (GlG3) 8 (G2G4) for any

matrices,

G
1'

l ** '

G4 one can show that QT^%Q = 4( 8 $,J -+ $@LN'

P

But this is a KN X KN diagonal matrix with entries

\ +h i4

where

t1l Y i = 1, 2, . . . , k and Anj' j = 1, 2, .*. , n are the diagonal

entries of 4c and LN, respectively. From [33, p. 1541, we know

that h = 2 COS rj/(r+l) for j = 1, 2, . . . , r. Thus the spectral rj
radius of h is ?K

cos

-7r k+l

+

cos

n-+7Tl

and since ;k = 4%~ p(Ek) = $ (COS r/(k+l)+cos T/(n+l)). m We now study the block Jacobi matrix associated with Algorithm
r-v. Let M=D-E- F where, again, M is a I&P matrix and
76

-L =

0

% lo M32

0 ... ...
Mm,m-1

0

Y D=

0 52 1
and -U 0 M32 -
0. . . . Mm-1,m 0
1
Let B = -D-L(L + U) and BKK be the leading principal kn X kn submatrix. The application of Young's formula for '"lb to block SOR algorithms refer to 4m , the Jacobi matrix associated with % . The next result gives the spectral radius of these submatrices of B.
Theorem 7. Let SK be the matrix described above. Then P(h) = COs(T/(k+l))/(2 - cos(7r/(n+l)).

Proof. Define the s X s matrix US = (u..) by uii = 4 for iJ

i = 1, 2, . . . , s,

u12

=

-1,

ui i+l Y

=

u.i,i-1

=

-1,

us s 1 ?-

=

-1

and

U. = 0 otherwise. Let Vs = Ui' and Is be an s X s identity
lj

matrix. Also let TS = 41, - Us. Finally, let PS (resp., $)

be the orthogonal s X s matrix whose column vectors are the eigenvectors

77

of TS (resp., Vs) and let Ls (rasp., C,) be the diagonal matrix

of eigenvalues. Thus, the matrix &=&1(sPN is orthogonal. 0%ain,

we have suppressed the dependence of Q on K and N.)

Notice that ?K = TK% VN. Since BK and QTIfiQ have the same eigenvalues, we can instead determine those of the latter matrix.

But QT~Q = I$@ CN, a diagonal matrix. Hence, the eigenvalues of

3s are all possible products of the diagonal entries of ?K and CNY say ($icnj] where i = 1, 2, . . . , k and j = 1, 2, . . . , n. AS

in Theorem 6, \. = 2 cos rj/(k+l) for j = 1, 2, . . . , k. Further-
J

more, Icnj?

are the reciprocals of the eigenvalues of

thereuN'

fore [33, p. 1541, cnj = l/(4 - 2 cos Tj/(n+l)) for j = 1, 2, . . . , n.

It then follows that

POQ = max cos(ti/(k+l))/(2 - cos(rrj/(n+l)))
l<i<=k ll= j<=n
= cos(T/(k+l))/(2 - cos(r/(n+l))).

There are two problems in applying Theorem 6 or 7 to determine

wopt' The theorems both presuppose that one knows, a priori, the index set T = S(z*) since T determines the linear subsystem k %..TZT + % = 0 which is eventually solved. Furthermore, they both assume that T = {l, 2, . . . , kn) for some 15 k <= m. (The theorems

remain true if K = {t+l, t+2, . . . , t+kn) for t = 0, n, '&,...,(m-1)n

I;I -

and k = 1, 2, . . . , m.) From the Perron-Frobenius theory of nonnegative matrices, if T = (1, 2, . . . , kn, kn+l, . . . , kn+c] where

0 <Z7 k <XI n, then we can bound %(%T) between %(%T1 1) and Um(%2T2)

78

where Tl = cl, 2, . . . , kn) and T2 = {l, 2, . . . , (k+l)n). This

fact mitigates the second problem somewhat although it still leaves

the problem of determining Tl and T2' One might be able to determine Tl and T2 during the progress of Algorithm III or IV by monitoring s(zk) until it appears to stabilize at some index set. From [20],
we know that gross bounds for LU can be obtained by setting opt
Tl = 9(-q) and T2 = 9 . Our computational experience has shown

i these latter bounds are not very useful unless Tl is a fairly large

Li

index set in which case

) and cu (

are fairly close

b !P2T2)

together.
CI - In order to illustrate the use of Theorems 6 and 7, we use

L the data of the second experiment reported in Section 9.3. We let

Tl = cl, 2, l -. , 30t) and T2 = cl, 2, . . . , 30(k+l)] where

r

L Cl, 2, T2.l

*a

,

3Ok) $@ (z*)

5

The results are summarized in

Table 12.

For the FSOR algorithm, u+ = cu (

>

b %lTl

and

u2

= (M > % T2T2

is determined via Theorem 6. For the BSORF algorithm, these quantities

are calculated using the results of Theorem 7. In both cases, LU ; exp
was determined (within 0.02) to be the empirically optimal value and

can be seen to be remarkably close to w2 in most cases. (We would expect ml 5 ("exp 2 "2. ) These results suggest that an adaptive

mechanism which sets ml= cu 1

and changes cu

during the operation

of the algorithms could prove very worthwhile.

79

Table 12. Theoretical Rounds for the Relaxation Parameters

t k+l
12
24 36
6 12
9 17
12 22
30 30

FSOR
*1 "exp
1.07 1.20 1.20 1.20 1.40 1.40 1.31 1.52 1.50 1.52 1.69 1.68 1.63 1.75 1.76 1.69 1.79 1.78 1.82 1.82 1.82

BSORF
Y c"exp 1.00 1.07 1.08 1.07 1.26 1.26 1.17 1*39 1.40 1.39 1*59 1.58 1.51 1.67 1.66 1*59 1.71 1.72 1.75 1.75 1.74

An alternate approach for estimating the optimal relaxation parameter is suggested by some research of Garabedian [27]. In a study of the point SOR method applied to linear systems derived from finite difference approximations to partial differential equations, he proposed an asymptotically good estimate for % ( i.e., the estimate became better as the mesh size on the region R of interest tended to zero). He assumed that the mesh size was uniform and of width h and the area of the closure of R was a. Garabedian then suggested using a relaxation parameter cu = 2/(1 + 3.015(h2/a)1'2). For many shapes of regions, he noted that in several numerical tests carried out by Young, this choice of cu resulted in approximately a 20 percent decrease in convergence rate from the optimal convergence rate. The remarkable -success of this estimate lies in the simplicity of its application in comparison with the application of Young's formula. This suggests that a generalization to the block SOR method (and thence to
80

Algorithm IV) could be worthwhile as future research. The authors have not yet derived similar results for either the block SOR or Modified Block SOR techniques.
i
Li
i L-
81

REFERENCES
[l] R. J. Arms, L. D. Gates and B. Zondek, "A Method of Block
Iteration," J. SLAM 4 (1956), 220-229.
[2] A. Auslender, 'Une M&hode G&&rale Pour la Decomposition et la
Minimisation de Fonctions Non Differentiables," C.R. Acad.
Sci. Paris 271 (197O), A1078-~081.
(I [3] A. Auslender and B. Martinet, t(M&hodes de Decomposition pour
i la Minimisation d'une Fonctionelle sur un Espace Produit," C.R. Acad. Sci. Paris 274-(1972), A6pA635.
[4] E. F. Beckenbach and R. Bellman, Inequalities, Springer-Verlag,
I L- New York, 1965.
L [53 G. V. Berg, Private Communication (Letter to R. W. Cottle, February 5, 1968.)
I [6] A. Cameron and W. L. Wood, ?t!he Full Journal Bearingf 11 Inst. L Mech. Engrs. J. Proc. 161 (1949), 59-64.
! [7] V. Castelli and V. Shapiro," Improved Method for Numerical Solutions of the General Incompressible Fluid Film Problem," J. of Lubr. Technology, Trans. of the ASME 5 (1967), 21~218. I31 J. Cea, uRecherche Numerique d'un Optimum dans un Espace Produit," Colloquium on Methods of Optimization, Lecture Notes in Mathematics, vol. 112, Springer-Verlag, New York, 1970. 193 R. Chandrasekaran, uA Special Case of the Complementary Pivot Problem," Opsearch 1 (1970), 263-268. [lo] D. G. Christopherson, 'A I\lew Mathematical Method for the Solution 4 of Film Lubrication Problems," Inst. Mech. Engrs. J. Proc. G-6 (1941), 126-135. [ll] R. W.xttle, ItNonlinear Programs with Positively Bounded Jacobians," Doctoral Dissertation, University of California, Berkeley, 1964.
82

[12] R. W. Cattle, 'Nonlinear Programs with Positively Bounded
Jacobians," J. srAMApp1. Math. 14 (1966>, 147-158. [13] R. W. Cattle, "Principal Pivot Method of Quadratic Programming,"
Mathematics of the Decision Sciences, Part I (G. B. Dantzig
and A. F. Veinott, eds.), American Mathematical Society,
Providence, R.I., 1968.
[lb] R. W. Cattle, "Monotone Solutions of the Linear Complementarity
Problem," Math. Prog. 2 (1972), 210-224.
[15] R. W. Cattle, "Complementarity and Variational Problems," to
appear in Proceedings of the Conference on Mathematical
Programming and its Applications, Rome, 1974. [16] R. W. Cottle and G. B. Dantzig, "Complementary Pivot Theory of
Mathematical Programming," Linear Algebra and Appl. 1
(1968), 103-125.
[l'i'] R. W. Cattle and A. F. Veinott, Jr., "Polyhedral Sets Having a
Least Element," Mathematical Programming 2 (1972), 238-249. [18] C. W. Cryer, "On the Approximate Solution of Free Boundary
Problems Using Finite Differences," J. Assoc. Comput. Mech.
g (1970), 397-410. [19] C. W. Cryer, "The Method of Christopherson for Solving Free
Boundary Problems for Infinite Journal Bearings by Means
of Finite Differences," Math. of Computation 2 (1971),
435-443.
[20] C. W. Cryer, "The Solution of a Quadratic Programming Problem
Using Systematic Overrelaxation," J. SIAM Control 2
(1971), 385-392.
[21] G. B. Dantzig, Linear Programming and Extensions, Princeton
University Press, Princeton, 1963.
[22] G. B. Dantzig and R. W. Cottle, "Positive Semi-Definite Programming,"
Nonlinear Programming (J. Abadie, ea.), North Holland Publish-
ing Company, Amsterdam, 1967.
83

[23] M. Fiedler and V. Ptak, "On Matrices with Nonpositive OffDiagonal Elements and Positive Principal Minors," Czech.
J. Math. 12 (1962), 382-400.
[24] G. E. Forsythe and W. R. Wasow, Finite-Difference Methods for Partial Differential Equations, J. Wiley and Sons, New York,
1960.
[25] S. P. Frankel, "Convergence Rates of Iterative Treatments of Partial Differential Equations," Math. Tables Aids Comput.
k (1950), 65-75* [26] V. M. Friedman and V. S. Chernina, "An Iteration Process for the
Solution of the Finite-Dimensional Contact Problem," U.S.S.R.
Comp. Math. and Math. Phys. 8 (1967), 210-214.
[27] P. R. Garabedian, "Estimation of the Relaxation Factor for Small
Mesh Size," Math. Tables Aids Comput. 10 (1956), 183-185. [28] C. L. Gerling, Die Ansgleichs-Rechnungen der Practischen
Geometrie oder die Methode der Kleinsten Quadrate mit ihren Anwendungen ftir Geodgtiche Aufgaben, Gothe. Hamburg,
1843.
[29] A. A. Gnanadoss and M. R. Osborne, "The Nwnerical Solution of Reynolds' Equation for a Journal Bearing," Quart. J. Mech.
Appl. Math. 17 (1964), 241-246.
[30] R. Glowinski, "Sur la Minimisation , par Surrelaxation avec
_ Projection, `de Fonctionnelles Quadratiquesdans les Espaces
de Hilbert," C.R. Acad. Sci. Paris 276 (1973), A1421-A1423. [31] M. Hall and M. Newman, "Copositive and Completely Copositive
Quadratic Forms," Proc. Cambridge Phil. Sot. 59 (1963), 329-339. [32] P. Halmos, Finite-Dimensional Vector Spaces, D. Van Nostrand CO., Inc., Princeton, 1958.
84

1331 S. J. Hammarling, Latent Roots and Latent Vectors, Univ. Toronto
Press, Toronto, 1970. 1341 C. Hildreth, "A Quadratic Programming Procedure," Nav. Res.
Logis. Quart. 2 (1957), 79-85. [35] J. K. Ho and A. S. Manne, "Nested Decomposition for Dynamic
1974.Models," to appear in Mathematical Programming, [36] E. Isaacson and H. B. Keller, Analysis of Numerical Methods, L J. Wiley and Sons, New York, 1966. [37]i E. L. Keller, " The General Quadratic Optimization Problem,"
L Mathematical Programming 2 (1973), 331-337.
l-381LE - C. E. Lemke, "On Complementary Pivot Theory," Mathematics of the Decision Sciences, Part I (G. B. Dantzig and A. F. Veinott, Jr.,
L eds.), American Mathematical Society, Providence, R.I., 1968. [39] C. E. Lemke, "Recent Results on Complementarity Problems," Non-
L linear Programming (J. B. Rosen, 0. L. Mangasarian, and
1970.K. Ritter, eds.), Academic Press, New York,
[hOI C. E. Lemke and J. T. Howson, "Equilibrium Points of Bimatrix
Games," J. Sot. Indust. Appl. Math. 12 (1964), 413-442.
[411 LPMl User's Manual, System Optimization Laboratory, Department
1973.of Operations Research, Stanford University, Stanford,
[42] 13. Martinet, "Convergence de Certaines M&hodesde Relaxation en
Programmation Convexe," C.R. Acad. Sci. Paris 265 (1967),
A210-A212.
[43] J.-C. Miellou, "M&hodes de Jacobi Y Gauss-Seidel 9 Sur (Sous)-
relaxation par Blocs, Appliquges a une Classe de Problkmes Non Lineaires," C.R. Acad. Sci. Paris 273 (1971), A1257-A126o.
85

[44] K. G. Murty, " On the Number of Solutions to the Complementarity Problems and Spanning Properties of Complementary Cones,"
Lin. Alg. and Appl. 2 (1972), 65-108.
[45] 0. Pinkus and B. Sternlicht, Theory of Hydrodynamic Lubrication,
McGraw-Hill, New York, 1961.
[46] A. A. mimondi and J. Boyd, "A Solution for the Finite Journal
Bearing and Its Application to Analysis and Design: III,"
Trans. Amer. Sot. Lubrication Engrs. 1 (1958), 194-209. i [ 473 A. A. Raimondi, Private Communication,August 13, 1973.
f
i [48] 0. Reynolds, " On the Theory of-lubrication and its Application
to Mr. Beauchamp Tower's Experiments: Phil. Trans. Roy.
PLI-- Sot., London 177 (1886).
L [49] He Lo Royden, Real Analysis, 2nd edition J Macmillan Co., London ? 1968 . [TO] R. Saigal, "A Note on a Special Linear Complementarity Problem," @search 1 (1970), 175-183. i [51] R. Saigal, 'I Lemke's Algc'rithm and A Special Linear Complementarity
Problem," Opsearch 8 (1971), 201-208.
[52] H. Samelson, R. M. Thrall, and 0. Wesler, "A Partition Theorem
for Euclidean n-Space," Proc. Amer. Math. Sot. 2 (1958), 805-807.
[53] S. Schechter, "Relaxation Methods for Convex Problems," SIAM J. Numer. Anal. 1 (1968), 601-612.
[54] S. Schechter, "Iteration Methods for Nonlinear Problems," Trans. Amer. Math. SOC. 104 (1962), 179-189.
[55] H. W. Swift, "The Stability of Lubricating Films in Journal
Bearings," Proc. Inst. of Civ. Eng. 233 (1932), 267-323.
[56) A. W. Tucker, "A Combinatorial Equivalence of Matrices," Proceedings of Symposia in Applied Mathematics, Vol. 10 (R. Bellman and M. Hall, eds.), American Mathematical Society,
Providence, R.I., 1960.
86

F57] A. W. Tucker, "Principal Pivotal Transforms of Square Matrices," SIAM Review 5 (1963), p. 305.
[58] R. S. Varga, Matrix Iterative Analysis, Prentice Hall Y Englewood Cliffs, N.J., 1962.
[59] J. R. Westlake, A Handbook of Numerical Matrix Inversion and
Solution of Linear Equations, J. Wiley and Sons, New York Y
1968.
t I [60] D. F. Wilcock and E. R. %oser, Hearing Design and Application Y
LI McGraw-Hill, New York, 1957.
[61] J. H. Wilkinson, "Error Analysis of Direct Methods of Matrix L- Inversion," J. Assoc . Comput. Mach. E (1961), 281-330.
L [62] D. M. Young, Iterative Solution of Large Linear Systems, Academic Press, New York, 1971.
I [63] W. I. Zangwill, Nonlinear Programming, A Unified Approach, L Prentice-Hall, Inc. Englewood Cliffs, N.J., 1969.

