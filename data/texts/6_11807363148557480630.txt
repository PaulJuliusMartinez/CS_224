ON STEWART'S SINGULAR VALUE DECOMPOSITION FOR PARTITIONED ORTHOGONAL MATRICES
bY
Charles Van Loan
STAN-(X-79-767 September 1979
DEPARTMENT OF COMPUTER SCIENCE School of Humanities and Sciences STANFORD UNIVERSITY

ON STEWART'S SINGULAR VALUE DECOMPOSITION FOR PARTITIONED ORTHOGONAL MATRICES
Charles Van Loan Department of Computer Science Upson Hall, Cornell University
Ithaca, New York 14853
This research was supported in part by the National Science Foundation Grant No. MCS-78-11985.

ABSTRACT.
A variant of the singular value decomposition for orthogonal matrices due to G. W. Stewart is discussed. It is shown to be usef'ul in the analysis of (a> the total least squares problem, (b) the Golub-Klema-Stewart subset selection algorithm, and (c) the algebraic Riccati equation.

1. Introduction.
In a recent survey article G. W. Stewart [8] presented the following
variant of the singular value decomposition (SVD):
Theorem 1.
If QeIRmXm is orthogonal and partitioned as follows,

I 1Q = $I1 'I2 k % Q22 p kP

k + p = m , k -> p

then there exist orthogonal Ul and Vl in IRkxk and orthogonal U2 and V2 in BPXP such that

iy 0u1 ; 0 T . . ,. . . ., . . . . . . .
i I L L0 :. v2I0 iu,

-. 9. .1. %1

.::...l .$Q2.2IL.2. I

I"I
k-P

0 :. O

k-p

=

0 .

.

.

.

.

.

.C.

.

.:

.

s,

.

P

-0

-

s

:c

I
I

P

k-p p p

where

C = diag(cl,...,cp) S = diag(sl,...,sp)

C.1 = cos (e,) S i = sir&$)

1

For notational convenience we will sometimes express the above decomposition in the form

kP

k P

where

G = diag(Ul,U2),

v = diag(Vl,V2),

Co = diag(1 k-p'

C>, and

so = [ 0 up
k-P P

.

Note that UT Q.. V. displays the singular values of Q.. . The

=J J

13

quantities c19...,cp will be referred to as the p-singular values of Q

and the entire decomposition as the p-SVD . The p-singular values of Q

are thus the singular values of Q's trailing pxp principle submatrix,

'The assumption p -< k is not restrictive. The aim of this paper is to demonstratethat the p-SVD can play a useful

role in the analysis of certain matrix computation problems. This is not

a new endeavor; Davis and Kahan [21 made use of the p-SVD in their detailed

paper about invariant subspace perturbation. Although this paper precedes
Stewart [81, it was in the latter article that Theorem 1 was first made

explicit.

We briefly indicate how Theorem 1 can be proved. For clarity, assume

p=krm3

.

Let

T ul &11

V1 =

diag(c 1'c2' c3 > be the SVD of Qll .

Since 11Ql11i2 5 (I&/I2 = 1 > it follows that cl -< 1 .

2

Let U2 be an orthogonal matrix such that the first column of u@21vl) is a non-positive multiple of el ' the first column of the 3x3 identity. Similarly, let V2 be orthogonal so that the first row of $Q,2)V2 is a non-negative multiple of eT1 . It then follows that

diag(UF,Uz) Q diag(Vl,V2) =

m
I) .
cl0 0:a 0 0 .
0 c2 O .:b x X

00 0 ......

c3':ddxx .c.3 .* . . . . . .

.

X .I

r u v:f g h .

0 x x-:.. k x X 0 x x:j x X .4

where a>-O, rl0 and "x" denotes an arbitrary scalar. Since this transformed matrix is orthogonal, both row 1 and column 1
have unit 2-norm and thus, if s1 = m1 then a = s1 and r = - s 1 ' This implies that f = cl because columns 1 and 4 must have a zero inner product. It then follows from the unit length of row 4 and column 4 that wwbWbdyky and j are all zero. This leaves us with 2 X 2 blocks -Q.E. D. by induction.
2. The p-SVD, Direct Rotations, and Angles between Subspaces. In this section we relate the p-SVD to certain well known relationships
that exist between subspaces. As we mentioned, Davis and Kahan [2] used p-SVD ideas in their study of the invariant subspace perturbations.

In their analysis of this problem, it is necessary to be able to rotate a given p-dimensional subspace A into another p-dimensional subspace B in the most "economical" fashion. More precisely, if

Z = [ZJ z21 w = [WJ w21

n-p P

n-p P

are nXn orthogonal matrices with A = Range (Z,) and B = Range (W2) , then we wish to determine an orthogonal T,;, E R nXn that minimizes
I T - 'nflF subject to the constraint T Z2 = W2 . (Here, IIcII2F = trace (CTC),
a unitarily invaniant norm. )
It is clear that any orthogonal T E IRnxn satisfying T Z2 = W2
must have the form

T = [WIVll W2] [ZIUll Z2]l - W Z'
where ul and V1 are orthogonal matrices in IR (n-pMn-p> . From the
identity
llq w,ll; + /I$ yll'F = llz,zi2' - w2$ll;

it follows that
I T - InIl; = lliTT i - InIl; = IliT; - Inil; = l#z; wlhl - I.-,ll; + ll~p~ w2) 11; + II q wl)vllI; + I@ 5 - IpIg = ppz; wlhl - I n-pll; + I/z2.g - w21"12T1/2F + llz3J2- Ipll;

4

This expression is minimized by choosing Ul and Vl so uT1(zT1 w1>v1 is diagonal. (See [7]). Moreover, if

ul O

0
[

u2

S0T CI

is the p-SVD of Q = ZT W , then
Tmin = -t; ;T = ;(zT;)iT = Z(ZT W> diag(VIU~,Ip) ZT
UTZTST0 11
1 [ IC VT2Z2T
and
I Tmin- `nll~ = 11' - Ip/j2F + 211sll~ + 11' `~~~`2 ' - Ipll~ '

The p-singular values {c0&~)?, of ZT w provide a measure of how different the subspaces A and B are. The 8i are referred to as the "principle angles" between A and B and a stable, efficient algorithm for their computation is given in a paper by Bjork and Golub [l]. Wedin [p] has developed a perturbation theory for the principle angles. Tmin is referred to in [2] as a "direct rotation" from A to B .

3. A Wielandt-Hoffman Theorem for p-Singular Values. If an orthogonal matrix Q is perturbed, how are its p-singular values
effected? The following theorem answers this question.
Theorem 2. If Q and 6 are rnxm orthogonal matrices having p-singular values
ccos (6, ))~=1 and [cos(~^~)]~_, respectively, then

P P 8.- ii.
4x 11 - cos (ei- ii) 1 = 81 sin2 ( I2 '-) < 11~ - ill', .
i=l i=l

Proof. If the p-SVD's of Q and i are given by

1 ul 0 and
respectively, then

'Qll !I2 ?21 %2I

A.
vl O 0 G2 i

cO ST0 TSO C 3

kP

"c
=0
[ i.

1

kP

k P
k P

6

Now the Wielandt-Hoffman for singular values states that if the nxn matrices R and i have SVD's U diag(oi)VT and 6 diag(ei)GT respectively, then

P
1 (0 i- ;iI2-< I R-
i=l

I 1This result follows by applying the "original" Wielandt-Hoffman Theorem [5]

for eigenvalues to the symmetric matrices rqO RoT

and

0 RT

ci

i Iii0

l

(These matrices have eigenvalues -+ CT1. and -+ 6.1 respectively. > Thus, if

ci= COS(~~), si= sin(Bi), si= co&$ and gi= sir&$ , then

P IIQ - ill', >, 2~ (ci- ~i,2 + (si - si)2
i=l
P = 4x El - COs(ei- e^i)'
i=l

=8

In the next section it will be necessary to know how far a given m x n

orthogonal matrix Q

is to the set

sf" defined by P

z12q Izll m-p
i z21 Z22 i P

ZTZ = Im , det(Z22) = 0 # }

m-p 'P

7

i.e., the set of all rnxm orthogonal matrices whose trailing px'p principle submatrix is singular.
Theorem 3.
If Q is an mxm orthogonal matrix with p-SVD given by Theorem 1, and if 6 is defined by

with

? = diag$...,c p-l ,O> i = diag(sl, "'YSp-l" 1)

then
IIQ - illF = min I/Q - z[& = 2qxp) 5 2 cos(ep)
z E a;
Proof.
Any Z E S$ has p-singular values of the form rcos($,. .,co& ,-,)Y 01
and so from Theorem 2,
II z - QII > 8 Esin2 (9) + 8 sin2 ( ep-eTI/2 ) _> 4(1 - sin(ep)) .

By setting 2, = i , the lower bound is attained. The rest of the Theorem follows from elementary trigonometry. g

4. Some Applications. We now apply the p-SVD to several computational problems.
(a) Total Least Squares [4]. Given A e Wmxn , B E lRmxp Cm _> n+p) and nonsingular "weighting
matrices" D = diag(dl,...,$) and T = diag(t1 ,***, tn+p), the total least squares problem (TLS) involves minimizing
llD [E tR]T (IF E E Rmxn, R c RmxP
subject to the constraint Range (B + R) c Range (A + E) .
If a minimizing i and i can be found, then any X E 8" satisfying
(A + ;,X = B f R is a TLS solution. Note that this last equation implies

[D[AlB]T + D[i$]T]T-

=0

and thus, the TLS .problem involves finding the nearest matrix to D[AIBlT that has a null space of dimension p . If

[9b21' D[Ab]T NJ%] = dia&,. ·.,~n+p)

m-p P

nP

9

is the SVD of D[AIB]T , then

D[~I~]T = - U2 diag(an+l,~~"On+p)~ l

The minimizing [GIGI is unique if a, > an+l l Moreover, if

L[QltG$ = &11 &12 n Q2 1 $2 2I P nP

then

[ 1T-l `- = -1 P

T-1 &12 [ $2 2 I

-1
s2

diag

(t

-1 n+l

Y**'Y

tn-+l'p

>

provided Q22 is nonsingular. In this case

-1 -1 yrLS = diag(tl,...,tn) Q&2,$ diag(tn+ly"~,tn+p) l

Numerical difficulties arise in the TLS problem if

2 %

is close

to singularity. Consequently we are interested in how close the TLS problem A
{A,B,D,T} is to a corresponding problem {i,B,D,TJ with no solution.

Theorem 4.
Let A,B,D, and T be as above and suppose F = D[AIB]T has SVD UTFQ = diag(oly...,on+ p> with 0, > an+l l If (co&$)]!& are the p-singular values of Q , then there exists a TLS problem &~,D,T} with no solution satisfying
lW~If31~ - WbI~ll~ < 2 cos(e > ll~MbI~/l~ - P l

10

Proof. Let 6 be the matrix in hzn+P closest to Q as in Theorem 3 . Now P UTFQ = UT(FQQT& = diag(aly~~~,on+P)
and so by defining [Xl from D[A I ii ]T = ?Q;T = D[AIB IT QGT
Ah \ we see that the TLS problem [A,B,D,T) has no solution and
IID[AI~]T - D[AIBITII~ -< IINAIBITII~ IIQ-~I& . The Theorem follows since IlQ-~ll F = 5 2 cOs(ep) . q
(b) Golub-Klema-Stewart Subset Selection [3].
Consider the problem
minIlAx - b(12 A E flxn , b E Wm
where A has SVD
[uJu~I~ A [Ql,gI = diag(al,...,on) r n-r r m-r
and or >> o r+l m 0 . This implies that A is close to a rank r matrix. One way of "coping" with the ill-conditioning is to minimize IJA,x - bll 2

where Ar =

Ul

diag

(oly

l

l

l

T

r`0

`%

.

This least squares problem has the solution

Xr =

'i

where q1. and ui denote the i-th columns of Q and U respectively. A shortcoming of this approach is that the predictor Axr of b involves all n columns of A . Since rank degeneracy implies redundancy in the underlying linear model, it may be desirable to approximate b with r suitably chosen columns of A .
A method for doing this is suggested in [3]. Suppose P E lRnxn is a permutation matrix and that y E R' minimizes FLy - b/l2 where
AP = [B11B21 l
r n-r
If PTQ =

and

A
x

=

P

Y

r

i0I n-r

then it can be shown that
I r^X - r

Here, the notation rz means rz = b - AZ , the residual of z .

Since

I rX"

-

rI
xr 2

=

I Ai - up; bl12

I

it can be argued that G , vis-a-vis P , should be chosen to make this
quantity as small as possible because UIUlTb represents the "stable"
component of b . In [3] this task is approximately accomplished by Cy
chasing P so that the resulting Qll is well-conditioned. This is done
by applying the Q-R with column pivoting algorithm to $1 :

Q;P' = Z[RlIR2] r n-r

ZTZ = Ir , Rl = 7

.

In many applications, however, n - r << r . Since the p-SVD
implies II i$12 = 11"%-21II2 ' we can essentially determine P by triangularizing
the "skinny" matrix Q2 thus saving work . Note, that if r = n-l , then P
should merely interchange rows k and n of Q where Is,1 =maxqI.in I .
l<- i<-n

(c) The Algebraic Riccati Equation.

Suppose A,B,C e Wnxn

are such that AT= A >- 0 ,

-CT=

c>

0

l

Well-

known conditions of stabilizability and detectability [lo] guarantee that if

[ 1B A
M= C -BT

then there exists T,Y,Z E Rnxn such that

13

where Z is nonsingular and T's eigenvalues are in the right-half-plane. Furthermore, it can be shown that X = YZ-1 is the unique, non-negative definite, symmetric solution to the algebraic Riccati equation
A + BX+ XBT - xcx = 0 .
The matrix M is said to have Hamiltonian structure and in [6] the following decomposition is proved:
TR (1)
0 - TT3
where T is upper quasi-triangular, R is symmetric and Q orthogonal. If T has its eigenvalues in the right-half-plane, then X = Q..&-1& solves the Riccati equation.
The transformation Q is said to have symplectic form. Orthogonal symplectic matrices 'preserve Hamiltonian structure and moreover, their p-SVD is of very special form: Theorem 5.
If
nn
14

[ c 1is orthogonal, then there existnxn orthogonal matrices U axid V such that -A AC

where

C = diag$, ""on 1
A = diag(61,.oo,6 n >

1 > a1 -> l ** >- on >- 0 Q2+ $= I
nl

The proof is given in [5]. Note that A may have negative diagonal entries and that if 4+ 0 then
X = Qus-l1 = U diag(&) UT
(In the Riccati application, the 6 i are positive.) In practice it is important to understand the significance of small 6i
since the accuracy of a computed X depends on the size of 6 1 ' In [63 this topic is pursued. Roughly speaking, it can be shown that perturbations in A,B, and C of order hl can result in a Riccati equation i + 6X + XT?.- x6x = 0 that has no symmetric positive definite solution.

15

AcEanow1edgemen-t. The author wishes to thank Professor Gene H. Golub for encouraging
him to prepare this report.
REFERENCES.
Cl1 Bjork, A. and G. H. Golub, 'Numerical methods for computing Angles between Linear Subspaces", Math. Comp. 27 (19731, 'ppe 579-594.
[PI Davis, C. and W. M. Kahan, "The Rotation of Eigenvectors by a Perturbation. III", SIAM J. Numer. Anal. 7 (1970), pp. l-46.
II31 Golub, G. H., V. Klema and G. W. Stewart, "Rank Degeneracy and Least Squares
Problems", Dept. of Computer Science Technical Report 559, Stanford University, Stanford, CA. (1976).
[41 Golub, G. H. and C. Van Loan, "The Total Least Squares Problem", unpublished manuscript.
[51 Hoffman, A. J. and H. W. Wielandt, "The Variation of the Spectrum of a Normal
Matrix", Duke Math. J. 20, (1953),pp. 37-39. hl Paige, C. and C. Van Loan, "A Hamiltonian-Schur Decomposition", Dept. of
Computer Science Technical Report TR 79-377, Cornell University, Ithaca,
New York, 197% [71 Schanemann, P., "A Generalized Solution of the Orthogonal Procrustes Problem",
Psychometrika 31 (19661, l-10. WI Stewart, G. W., "On the Perturbation of Pseudo-Inverses, Projections and Linear
Least Squares Problems", SIAM Review 19 (19771, pp. 634-662. [PI Wedin, Per Ake, 'A Geometrical Approach to Angles in Finite Dimensional Inner
Product Spaces; the Triangular Inequality for Angles", Institute of
Information Processing Rept. UMINF:66.78, University of Umear, Sweden (1979). Dl Wonham, W. M., 'On a Matrix Riccati Equation of Stochastic Control", SIAM J.
Control, 5 (19681, 681-697.
16

