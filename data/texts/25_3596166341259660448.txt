I

The design and implementation of compilers for programming languages is an essential part of systems software. In the last decade many new general-purpose pro-
gramming languages, such as Ada, Bliss, C, Fortran 77, and Pascal, have emerged along with a much larger number of more specialized languages. Although the construction of compilers is becoming easier, thanks in part to new theoretical insights and to new language development tools, the implementation of a compiler for a major language is still a nontrivial task.
Since the early 1960's there has been considerable interest in tools that reduce the effort needed to construct a good compiler. These tools are often called compilercompilers or translator writing systems. The purpose of this special issue of Comnputer is to look at some of the newly created language development tools that are nlow in use and to look at some of the current research in experimental compiler-compilers. Toward that end, it contains three articles on compiler-compilers by active researchers in the field.
In the first article, Stephen Johnson explains some of
the language development tools that he and others have

developed for the Unix* operating system. Of particular interest are a lexical analyzer generator, an LALR parser generator, the systems-oriented language C for encodinig semantic routines, and the Unix conitimiand interpreter. These tools have been used to implemilenit several dozen languages, including a portable compiler for Fortrati 77,1 a preprocessor for a language for typesetting mathemilatics,2 a portable compiler for the programminig language C,) and an interpreter for a pattern-action languagc lor processing files.4
Table-driven code generation is the topic of the second article, in which Susan Graham presents the research on automatic code generation that she and her colleagues are conducting at the University of California at Berkeley. The thrust of this work is on simplicity and efficiency in the code generation process. The author compares the Berkeley approach to several other receitly devised techniques for the design of automatic codc-generator-
generators.
'Unix is a tradeniark of Bell Laboratories.

August 1980

001ts9162/80)/018(1811X)009(51).75 t980 t11

In the third article, Bruce Leverett and his colleagues describe an ambitious compiler-compiler project they have undertaken at%Carnegie-Mellon University. The goal of this project is to develop a compiler-compiler that is capable of producing high-quality optimizing compilers from formal machine descriptions. This article describes some of the decisions, strategies, and assumptions that have gone into the design of this compiler-compiler.
It is impossible to cover all of the ongoing research into compiler-compilers in the space of a few articles. The ones in this issue-in conjunction with the background material in the remainder of this introduction-are intended to indicate some of the major trends in current compiler-compiler research, particularly in the area of automatic code generation. There are several other major compiler-compiler projects currently underway, such as the HLP project at the University of Helsinki,5 the MUG2 project at the University of Munich,6 and the ECS project at IBM Yorktown Heights.7 Another active area of research which can potentially have a significant impact on the automation of compiler construction is the formal specification of the semantics of programming languages.8 More information on the historical development of compilers and compiler-compilers can be found in the
excellent survey articles by Bauer,9 Knuth, 10 and Feldman and Gries,"I and in many of the modern textbooks on compilers. 12-15

defined by the phase. These tools often exploited specialpurpose knowledge derived from extensive theoretical analysis. Some of these tools could be used independently of compiler generation. Therefore, to appreciate recent advances in compiler-compilers it is necessary to understand what happens during each of the phases of compilation.
Lexical analysis. The first phase of a compiler, the lexical analyzer, sees the source program as a sequence of characters. The lexical analyzer groups these characters into tokens, which are substrings of the input stream that logically belong together, such as identifiers and constants. The lexical analyzer is also usually responsible for identifying comments, blanks, and quoted strings in the source program. Information collected about the tokens appearing in the source program is entered into a symbol table by the lexical analyzer. The output of the lexical analysis phase is a stream of tokens.
The notation of regular expressions is useful for describing the structure of tokens in a programming language.12"'6 Efficient algorithms have been developed to construct finite automaton recognizers for tokens. Special-purpose languages for specifying lexical analyzers from regular expression descriptions began to appear in the late 60's. 17,18 These languages were often used for constructing pattern-matching programs for other than compiling applications.

A simple compiler model
A compiler takes as input a program written in a source language and produces as output an equivalent program in a target language. It has proven useful to subdivide the process of translating a source program to a target program into a sequence of simpler subprocesses called phases. The exact function defined by a phase is somewhat arbitrary, but it is convenient to think of a phase as a logically coherent operation that takes as input one representation of the source program and produces as output another representation. Here we shall consider a simple compiler model consisting of the following five phases:
* lexical analysis (or scanning), * syntax analysis (or parsing), * semantic analysis, * code optimization, and * code generation. Early work on compiler-compilers tended to treat the compilation process as a single phase, specified as a syntax-directed translation. A typical compiler-compiler of the 1960's would provide an automatic parsing technique or a language for specifying parsers. Associated with each parsing action would be a semantic routine written in some procedural language, which Would be invoked to perform some semantic action immediately after that parsing action was executed.1 In the 1970's the phases of compilation tended to be studied as separate processes. Tools emerged for implementing some of these phases automatically from higher-level specifications of the input-output mapping

Syntactic analysis. The second phase of the compiler, the syntax analyzer or parser as it is often called, groups the tokens emitted by the lexical analyzer into syntactic structures such as expressions and statements. It is the syntax analysis phase that determines whether the source program satisfies the formation rules of the programming language. If the source program is not syntactically correct, the syntax analyzer must determine where errors occur, give appropriate error diagnostics, and recover from each error to catch any subsequent errors that might be present.
The syntax analyzer usually invokes some semantic action as each syntactic construct is recognized. The semantic action may be to enter some information into a table, emit some error diagnostics, or generate some output. The output of the syntax analyzer is some intermediate language representation of the source program such as postfix Polish notation, a tree structure, or quadruples.
Much of the published compiler research in the 1960's and early 1970's dealt with the development of parsing techniques. The parsing techniques used in the early compiler-compilers were of two types. One type handled very general classes of grammars, but often in a very timeconsuming way; the other type would handle smaller classes of grammars, but much more efficiently. The latter techniques were further classified into "top-down" or "bottom-up" methods. Aho and Ullman describe these early parsing methods in detail.'9
By the early 1970's parsing methods had become well understood from a theoretical point of view. The early work on recursive descent top-down parsing was formalized into the theory of LL grammars.20'2' Knuth22 generalized the work on bottom-up parsing into the

10 COMPUTER

theory of LR grammars, the largest natural class of context-free grammars that could be parsed with-a deterministic pushdown automaton. Subsequent work by Korenjak,23 DeRemer,24 Aho, Johnson, and Ullman,25 and many others succeeded in making LR parsing practical for use in compiler-compilers. Because of its generality and its good error detection capabilities, LR parsing was a natural method to incorporate in automatic parser generators.
The successful automation of syntax analysis was perhaps the most striking advance in compiler-compilers in the 1970's. With a parser generator such as Yacc,26 a programmer can automartically construct a reliable parser for any programming language directly from a grammatical description. A decade before, the construction of the parser had been considered a significant task in the implementation of a compiler.
Work on methods of automatic error recovery proceeded with the development of formal parsing techniques. Although there have been considerable im-
provements,27,28 automatic generation of good diagnostics and error recovery methods is still wanting.
Semantic analysis. Semantic analysis usually refers to the verification of various semantic integrity constraints that must take place sometime during the compilation process. Type checking, determining that functions are called with the appropriate number of arguments, and verifying that identifiers have been declared are typical of what takes place during semantic analysis.

Attribute grammars29 and affix grammars30 are two formalisms that have been proposed for describing the semantics of programming languages. A few compilercompilers have used these formalisms for automating semantic analysis. However, the automatic implementation of attribute and affix grammars can be slow and may require the entire parse tree to be stored in memory.
Nevertheless, there is considerable research interest in attribute grammars at present. (A recent bibliography cites over 150 references on attribute grammars.31) The extent to which attribute grammars can be used in practical compiler-compilers is still to be determined.
Code optimization. An "optimizing" compiler has a code optimization phase that attempts to transform an intermediate language representation of the source program into one from which a faster or smaller (but not necessarily optimal) target program can be generated. The first Fortran compiler set remarkably high standards for code optimization that most subsequent compilers have not met.32
Code optimization was an active research area during the 1970's. Optimizing transformations were codified, and efficient algorithms to implement various optimiza-
tions were found.33'34 It was discovered that "structured"
programs were easier to optimize than unstructured ones. 3536
A number of optimizations can be safely performed only by knowing information gathered from the entire program. Global data-flow analysis techniques were

(

RESEARCH SCIENTISTS. m I

TihfimHononfaoahrtnstesaeeiorCeyrefooCxwsetnmcoteswSeprsleeaflpuolalriloretr,enerccenVSaathtrlLoentoSfdpeidcSrItmascohw.mctiCjaaieeeeoerrndcdnmetdtitipwiTiasiunsaettnttrscevsesehowwrurnldiiebovletsulilSshenorcibaggbigebryaenatcknnhchoemcerkeyeoMgtDdsppirheepanoosvorCnduireteeontgilnaludcnotponispeogAipmlriutaaeietinnsnsoeto,d.st-sf
SOFTWARE ENGINEERING CMEepgbtapdpldiubareaSaeegletonmct,rceesecdskaf:t-re,iPbrgosaopdiatrrshfncarSesmoiacdDtccooenaulouehfggnnrmnsomr,tcErdroreawmenafelsianguoedmnoenriraniqtgamaensouinygtriertina.hcdceenrevleehea-mirgyassvbtoiesleeucefminienaatplsegntnrtpeo,ot,cthagdshphebsiipomistyfinllcdeyissoniuoonostlyCtsmlnstetlsiopsoomoeetemeufgsswmxesxptty.ipmhpwui,naesoeaangtrr,rurorgeeiilacpe,eerhrdernnciemrepccStoaemaqoeeecnstout,scichidiiisterukosneneunenrdgldrscdcooreiese.ssowlenyiamlongoosrAn---,tr--r

DESIGN AUTOMATION
ntIvcdreerenaeqcedlesuahieriitnvnigrioitnnendlgdTguoai.hagnuoealyrtps.soCpeMmlsoaiahkmctotepaiuyhutoltipndoeponlpsrosuiorstsoSidsfcoeeeisnsxneisspgneencrwar-eenfi,qoteurMeniE-ccSrtlhee,eensceittPix.qrnhpiueAceDcrbasoiilomleiripnsEtceunayqetglueiiisi-r-nno cgodiporrpmoeopwcrrtttehuyhHFHMn1oaeriooo0utn.nCrn7nris0oedeei1TymrysirvwopeweLmeefusemcymoltulorneblg(mleddeCnberain)iooCeltaftwoSfWfiethircieyoetpitrlnApehos.ievnrpercseacaasoaentcnolcseukfnaoeaiarCmdglypeSeeenanoh,tttniuieisdttarathinlopvdrreyoafestteaxtoslc:esaenirltoylin,oeannal,t
Bloomnington, MN 55420.
Honeywell

An Equal Opportunity Employer M/F

JN%-l. -- .010,

studied in great depth by Kennedy,37 Kildall,38 and others.35 Some of the data-flow analysis techniques were also used in detecting programming errors39 and in restructuring programs.40 Ullman gives a general survey of data-flow analysis techniques.4'
However, these developments are only beginning to be harnessed into effective code optimizer generators. Interesting experimentation with automatic code optimization is underway at IBM Yorktown Heights42 and at Carnegie-Mellon University.43 The paper by Leverett et al. in this issue describes some of the code optimization strategies incorporated into the Production-Quality Compiler-Compiler project at Carnegie-Mellon.
Code generation. Code generation, the final phase of our compiler model, translates the intermediate language program into the target language program. General code generation algorithms have been studied since the late
1950's.44 In 1970, Sethi and Ullman wrote an influential paper describing how to generate optimal code for expressions on two-address machines.45 In 1976 Aho and Johnson published a template-based code generation algorithm that produces optimal code for expressions on much more general machines.46
However, it was discovered that optimal code generation in the presence of common subexpressions is inherently hard, no matter how simple the machine.4748 Also, optimal code sequences for many existing machine architectures can be very unintuitive.49'50 Therefore, in most cases of practical interest, a compiler must rely on effective heuristics to generate good code. The thrust of much of the recent work in code generation stresses portability and/or retargetability of the resulting compiler.
There are two popular approaches to code generation. One is the use of systems-oriented procedural languages to facilitate implementation of code generators.51,52 The other is the design of general code algorithms that operate using tables to represent the specific target machine instructions.53-58
The latter style of code generation is often implemented as a template-matching process. The intermediate language program is a tree. Code generation consists' of covering the tree with templates representing the target machine instructions. This tree template matching approach allows many machine dependencies to be isolated into the tables containing the templates. This approach appears attractive for use in compiler-compilers in that it may be possible to derive the template tables automatically from a formal specification ofthe target machine and in that the same compiler can be used to generate code for a new machine by changing only the contents of the template tables. The papers by Graham and Leverett et al. in this issue describe this approach to code generation in more detail.
Formal specification of languages
The formal specification of a programming language consists of two parts: a syntactic specification that defines the set of well-formed programs and a semantic specifica-

tion that gives a meaning for each well-forrned program. Some researchers have set the goal of constructing compiler-compilers that will automatically produce production compilers from formal specifications of the source and target languages. Unless the source and target languages are suitably restricted, this goal is unreasonably optimistic at present.
Context-free grammars (BNF) have been the de rigueur syntactic specification of programming languages since
the publication of the Algol 60 report,59 and the associated theory of context-free languages has become well understood.16 As we have noted, an added advantage in
using context-free grammars to specify the syntax of languages is that we can automatically construct an efficient syntax analyzer directly from a defining grammar.
The semantic specification of a language is more difficult. Various methods of specifying the semantics of a programming language have been proposed,60 and considerable progress has been made in developing a sound mathematical theory of programming language semantics.6' Nevertheless, at present there is no formal semantic specification from which production quality compilers can be generated automatically.
Recently, denotational semantic specifications of programming languages have become popular,62-64 and it is possible to automatically generate an interpreter from a denotational semantic specification of a language.65 More research is necessary in order to translate the denotational semantic specification into an efficient code generator for a given target machine.
Our understanding of compilation has increased
substantially during the 1970's. 'We are now able to automatically construct reasonably good lexical and syntactic analyzers for most programming languages, although some problems still remain with the automatic design of'good error-handling routines. The remaining phases of compilation are not yet as thoroughly automated as lexical or syntactic analysis, but templatedriven code generation techniques appear to be attractive
for use in automatic code-generator-generators.
In the last decade, the mathematical foundations of programming language sentantics have become much better understood, but not yet as well as those of syntax. The automatic translation of programming language semantics into machine language semantics is still an active research area.
On the whole, the programming language area is becoming more complex. Programming languages are becoming more diverse. With the arrival of large-scale integration, new machine architectures are potentially easier to fabricate. What machine should we build to implement a given language is a question that will be asked more and more often. Compiler-compilers can help answer this question by enabling us to construct a compiler for a hypothetical new machine quickly and cheaply, and thereby allowing us to measure the quality of code that would be produced for that new machine. The 1980's
promise to be a lively period in the history of programming languages and their compilers.-

12 COMPUTER

Acknowledgments
I would like to express my appreciation to all of the authors for their efforts, and to Ted Lewis for his help in putting together this special issue.
I would also like to thank Steve Johnson, Ravi Sethi, and Tom Szymanski for their helpful comments on this

19. A. V. Aho and J. D. Ullman, The Theory of Parsing,
Translation, and Compiling, Prentice-Hall, Englewood Cliffs, N.J., Vol. 1 1972, Vol. II, 1973. 20. D. E. Knuth, "Top-Down Syntax Analysis," Acta Infor-
matica, Vol. 1, No. 2, 1971, pp. 79-1 10.
21. D. J. Rosenkrantz and R. E. Stearns, "Properties of Deterministic Top-Down Parsing," Information and Control, Vol. 17, No. 3, Oct. 1970, pp. 226-256.

overview.

22. D. E. Knuth, "On the Translation of Languages from Left to Right," Information and Control, Vol. 8, No. 6, Dec.

1965, pp. 607-635.

References
1. S. I. Feldman, "Implementation of a Portable Fortran 77 Compiler Using Modern Tools," Proc. ACM SIGPLAN Symp. Compiler Construction, Aug. 1979, pp. 98-106.
2. B. W. Kernighan and L. L. Cherry, "A System for Typesetting Mathematics," Comm. ACM, Vol. 18, No. 3, Mar. 1977, pp. 151-156.
3. S. C. Johnson, "A Portable Compiler: Theory and Practice," Proc. 5th ACM Symp. Principles of Programming Languages, Jan. 1978, pp. 97-104.
4. A. V. Aho, B. W. Kernighan, and P. J. Weinberger, "AWK-A Pattern Scanning and Processing Language," Software-Practice andExperience, Vol. 9, Apr. 1979, pp. 267-279.
5. K.-J.Raiha, M. Saarinen, E. Soisanlon-Soininen, and M. Tienari, "The Compiler Writing System HLP," Report A-1978-2,, Department of Computer Science, University of Helsinki, Finland, 1978.

23. A. J. Korenjak, "A Practical Method for Constructing LR(k) Processors," Comm. ACM, Vol. 12, No. 11, Nov. 1969, pp. 613-623.
24. F. L. DeRemer, "Simple LR(k) Grammars," Comm. ACM, Vol. 14, No. 7, July 1971, pp. 453-460.
25. A. V. Aho, S. C. Johnson, and J. D. Ullman, "Deterministic Parsing of Ambiguous Grammars," Comm. ACM, Vol. 18, No. 8, Aug. 1975, pp. 441-452.
26. S. C. Johnson, "YACC-Yet Another CompilerCompiler," Computing Science Technical Report 32, Bell Laboratories, Murray Hill, N.J., 1975.
27. S. L. Graham, C. B. Haley, and W. N. Joy, "Practical LR Error Recovery, " Proc. A CM SIGPLANSymp. Compiler Construction, Aug. 1979, pp. 168-175.
28. S. L. Graham and S. P. Rhodes, "Practical Syntactic Error Recovery in Compilers," Comm. ACM, Vol. 18, No. i 1, Nov. 1975, pp. 639-650.
29. D. E. Knuth, "Semantics of Context-free Languages," Math. Systems Theory, Vol. 2, No. 2, June 1968, pp. 127-145.

6. H. Ganzinger, K. Ripken, and R. Wilhelm, "Automatic Generation of Optimizing Multipass Compilers," Information Processing 77, North Holland, Amsterdam, 1977,

pp. 535-540.

7. F. E. Allen et al., "The Experimental Compiling Systems Project," Report RC 6718, IBM T. J. Watson Research Center, Yorktown Heights, N.Y., Sept. 1977.

8. D. Bjorner, "Programming Languages: Formal Development of Interpreters and Compilers," in International Computing Symp. 1977, E. Morlet and D. Ribbens, eds., North Holland, Amsterdam, 1977, pp. 1-21.

9. F. L. Bauer, "Historical Remarks on Compiler Construction," in Compiler Construction: An Advanced Course, F. L. Bauer and J. Eickel, eds., Springer-Verlag, Berlin, 1974, pp. 603-621.

10. D. E. Knuth, "A History of Writing Compilers," Computers and Automation, Vol. 11, No. 12, Dec. 1962, pp. 8-14.
11. J.A. Feldrtan and D. Gries, "Translator Writing Systems," Comm. ACM, Vol. 11, No. 2, Feb. 1968, pp. 77-113.

14N0E1WPLRIOFGE RFAOMRS

12. A. V. Aho and J. D. Ullman, Principles of Compiler Design, Addison-Wesley, Reading, Mass., 1977.
13. F. L. Bauer and J. Eickel, eds., Compiler Construction: An Advanced Course, Springer-Verlag, Berlin, 1974.
14. D. Gries, Compiler Construction for Digital Computers, Wiley, New York, 1971.
15. P. M. Lewis, II, D. J. Rosenkrantz, and R. E. Stearns, Compiler Design Theory, Addison-Wesley, Reading,
Mass., 1976.

CS-TRAN converts your 1401 object programs to COBOL for the mainframe or mini of your choice. CS-TRAN is the only translator that accepts your object programs, patches and all, yet allows you to include actual COBOL paragraph names and record definitions. If you'd like more details about new life for your 1401
programs just call or write Russ Sandberg.

16. J. E. Hopcroft and J. D. Ullman, Introduction to A utomata Theory, Languages, and Computation, Addison-Wesley, Reading, Mass., 1979.

17. W. L. Johnson et al., "Automatic Generation of Efficient Lexical Processors Using Finite State Techniques," Comm. ACM, Vol. 11, No. 12, Dec. 1968, pp. 805-813.

18. M. E. Lesk, "LEX-a Lexical Analyzer Generator,"
Computing Science Technical Report 39, Bell Laboratories, Murray Hill, N.J., 1975.

C S Computer Systems Inc.
90 John Street, New York, NY 10038 * 212-349-3535

August 1980

Reader Service Number 4 0

30. C. H. A. Koster, "Using the CDL Compiler-Compiler," in Compiler-Construction: An Advanced Course, F. L. Bauer and J. Eickel, eds., Springer-Verlag, Berlin, 1974, pp. 366-426.
31. K.-J. Raiha, "Bibliography on Attribute Grammars," ACM SIGPLAN Notices, Vol. 15, No. 3, Mar. 1980, pp. 35-44.
32. J. W. Backus et al., "The FORTRAN Automatic Coding
System," Proc. Western Joint Computer Conf., Vol. 11,
1957, pp. 188-198. 33. F. E. Allen and J. Cocke, "A Catalogue of Optimizing
Transformations," in Design and Automation of Compilers, R. Rustin, ed.; Prentice-Hall, Englewood Cliffs, N. J., 1972, pp. 1-30. 34. D. E. Knuth, "An Empirical Study of FORTRAN Programs," Software-Practice and Experience, Vol.1, No. 2, 1971, pp. 105-133. 35. M. S. Hecht, Data FlowAnalysis of Computer Programs, American Elsevier, New York, 1977. 36. M. V. Zelkowitz and W. G. Bail, "Optimization of Structured Programs," Software-Practice and Experience, Vol. 4, No. 1, 1974, pp. 51-57. 37. K. Kennedy, "A Global Flow Analysis Algorithm," Intl. J. Computer Math., Vol. 3, 1971, pp. 5-15. 38. G. A. Kildall, "A Unified Approach to Global Program Optimization," Proc. ACMMSymp. Principles ofProgrammning Languages, 1973, pp. 194-206. 39. L. D. Fosdick and L. J. Osterweil, "Data Flow Analysis in Software Reliability," Compuling Surveys, Vol. 8, No. 3, 1976, pp. 305-330. 40. B. S. Baker, "An Algorithm for Structuring Programs," J. ACM, Vol. 24, No. 1, Jan. 1977, pp. 98-120.
41. J. D. Ullman, "Data Flow Analysis," Proc. 2nd USA-
Japan Comnputer Con.f., AFIPS Press, Montvale, N.J.,
1975, pp. 335-342. 42. W. Harrison, "A New Strategy for Code Generation-the
General Pupose Optimizing Compiler," Fourth ACM Symp. Principles of Programming Languages, Jan. 1977, pp. 29-37. 43. W. A. Wulf et al., The Design ofan Optimizing Comnpiler, American Elsevier, New York, 1975. 44. A. P. Ershov, "On Programming of Arithmetic Operations," Dokl. A. N. USSR, Vol. 118, No. 3, 1958, pp. 427-430; also in Cotnm. ACM, Vol. 1,No.8,1958, pp.3-9.
45. R. Sethi and S. C. Johnson, "The Generation of Optimal Code for Arithmetic Expressions," J. ACM, Vol. 17, No. 4, 1970, pp. 715-728.
46. A. V. Aho and S. C. Johnson, "Optimal Code Generation for Expression Trees," J. ACM, Vol. 23, No. 3, 1976, pp. 488-501.
47. J. L. Bruno, and R. Sethi, "Code Generation for a One Register Machine," J. ACM, Vol. 23, No. 3, 1976, pp. 502-510.
48. A. V. Aho, S. C. Johnson, and J. D. Ullman,."Code Generation for Expressions with Common Subexpressions," J. ACM, Vol. 24, No. 1, Jan. 1977, pp. 146-160.
49. A. V. Aho, S. C. Johnson, and J. D. Ullman, "Code Generation for Machines with Multiregister Operations," Proc. Fourth ACM, Svmp. Principles of Programnming Languages, 1977, pp. 21-28.
50. T. G. Szymanski, "Assembling Code for Machines with Span-Dependent Instructions," Comnmn. ACM, Vol 21, No. 4, April 1978, pp. 300-308.
51 W. M. McKeeman, J. J. Horning, and D. B. Wortman, A Cotmpiler Generator, Prentice-Hall, Englewood Cliffs, N. J., 1970.
52. M. Elson and S. T. Rake, "Code-Generation Technique for Large-Language Compilers," IBM System J., Vol. 9, No. 3, 1970, pp. 166-188.

53. R. G. G. Cattell, "Automatic Derivation of Code Gener-

ators from Machine Descriptions," ACM Trans. Pro-

gramming Languages 1980, pp. 173-190.

and

Systetns,

Vol.

2,

No.

2,

Apr.

54. C. W. Fraser, "Automatic Generation of Code GeneraUtnoivresr,s"itPyh, DNediwssHeartvaetni,on,CoCnonm.,pu1t9e7r7.Science Dept., Yale

55. K. Ripken, "Formale Beschreibun von Maschinen, Implementierungen und Optimierender MaschinenTceocdheneirszcehueguUnngivearu.sMuAntctrhiebnu,tieMrutneinchP,rGoegrrmaamnmyg,ra1p9h7e7.,"

56. P.L. Miller, "Automatic Creation of a Code Generator from a Machine Description," Tech. Rep. TR 85, Project MAC, MIT, Cambridge, Mass., 1971.

57. J. M. Newcomer, "Machine Independent Generation of

Optimal Local Science Dept., Pa., 1975.

Code," PhD Carnegie-Mellon

dissertation, University,

Computer Pittsburgh,

58. St.imiGz.atWiaonsilCaepwa,bi"liAtieCsomfpoirleCromWprlietxingOrSdyesrteSmtruwcittuhreOsp,-" PhD dissertation, Northwestern University, Evanston,
111., 1971.

59.

P. Naur ALGOL 299-314.

et al., "Report on 60," Comn. ACM,

the Vol.

Algorithmic Language 3, No. 5, May 1960, pp.

60. D. Bjorner, "Programming Languages: Linguistics and Semantics," International Computing Symp. 1977, E. Morlet and D. Ribbens, eds., North Holland, Amsterdam, 1977, pp. 511-536.

61. J. E. Stoy, Denotational Semantics: The Scolt-Strachey

Approach to Progra,nmning
Cambridge, Mass., 1977.

Language

Theory,

MIT

Press,

62. M. J. C. Gordon, The Denotalional Description of Programnming Languages, Springer-Verlag, New York, 1979.

63. R. Sethi, "A Case Study in Specifying the Semantics of a Programming Language," Proc. Seventh Annual ACM Symnp. Principles of Programming Languages, Jan. 1980,
pp. 117-130.

64.

R. D. ming

Tennent, "The Denotational Languages," Cotnmn. ACM,

Semantics Vol. 19,

of ProgramNo. 8, Aug.

1976, pp. 437-453.

65. P. D. Mosses, "SIS: A Compiler-Generator System Using
Denotational Semantics," Report 78-4-3, Dept. of Com-
puter Science, University of Aarhus, Denmark, June 1978.

Alfred V. Aho is head of the Computing

Principles Research Department at Bell

Laboratories, Murfay Hill, New Jersey.

His current research interests include

algorithms, compilers, data bases, and

theoretical computer science. He is an

author of numerous papers and books in

the computer science field. He is an af-

filiate professor at Stevens Institute of

ACM

Special

Technology
Interest Group on

and is past chairman of the Automata and Computability

Theory.

Aho received the BASc degree in engineering physics from the

University of Toronto in 1963 and the MA and PhD degrees in

EECS from Princeton University in 1965 and 1967.

COMPUTER

