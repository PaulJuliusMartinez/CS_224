From: AAAI-86 Proceedings. Copyright ©1986, AAAI (www.aaai.org). All rights reserved.

Quantifying

the inductive bias in concept learning (extended abstract)

David Haussler

Department of Mathematics and Computer Science, University of Denver, Denver, Colorado 80208.

Abstract

We show that the notion of bias in inductive concept

learning can be quantified in a way that directly relates to

learning performance,

and that this quantitative

theory of

bias can provide guidance in the design of effective learning

algorithms.

We apply this idea by measuring some common

language biases, including restriction to conjunctive concepts

and conjunctive

concepts with internal disjunction,

and,

uided by these measurements,

develop learning algorithms

Por these classes of concepts that have provably good conver-

gence properties.

Introduction

The theme inductive concept way that enables ties for learning

of this pa er is that the notion of bias in learning IpU86] [R86/ can be quantified in a us to rove meaningful convergence proper-
algorit i ms. We measure bias with a com-

binatorial parameter defined on classes of concepts known as

the Vapnik-Chervonenkis

dimension (or simply d+ensiorr )

[VC71/, [P78j', JBEHW86/. The lower the dlmenslon of the

class of concepts considered by the learning algorithm, the

stronger the bias. In /BEHW86,`? this parameter

has been

shown to be strongly correlated with learning performance, as

defined in the learning performance model introduced by Valiant jV84j, [VSS]. Th is model can be outlined as follows.

A concept is defined by its set of instances in some

instance s ace. A sample of a concept is sequence of observa-

tions, eat rl of which is an instance of the concept ( positive

observation ) or a non-instance

of the concept

negatrve obser-

vation ). Samples are assumed to be create 6 from inde en-

dent, random observations,

chosen according to some Kxed

probability distribution on the instance space. Given a samle of a target concept to be learned, a learning algorithm
f!orms a hypothesis, which is itself a concept. The algorithm is consistent if its hypothesis is always consistent with the given

sample, i.e. includes all observed positive instances and no

observed negative instances. A consistent hypothesis may still

disagree with the target concept by failing to include unob-

served instances of the target concept or including unobserved

non-instances

of the target concept.

The error of a

hypothesis is the combined probability of such instances, i.e. the probability that the hypothesis will disa ree with a random observation of the target concept, se`iected from the

instance space according to the fixed probability distribution.

Two performance measures are applied to learning algo-

rithms in this setting. 1. The convergence rate of the learning algorithm is measured

in terms of the sample size that is required for the algorithm

to produce, with high probability,

a hypothesis that has a

small error. The qualification

"with high probability"

is

required because the creation of the sample is a probabilistic

event. Even the best learning algorithm cannot succeed in the

unlikely event that the sample is not indicative of typical

observations.

However, while the model is probabilistic,

no

specific assumptions are made about the probability distribu-

tion that governs the observations.

This distinguishes

this

approach from usual statistical methods employed in pattern

recognition, where the object of learning is usually reduced to

the estimation of certain parameters of a classical distribu-

tion. The distribution-free

formulation of convergence rate is

obtained by upper bounding the worst case convergence rate

of the learning algorithm over all probability distributions on

the instance space. This provides an extremely robust perfor-

mance guarantee. 2. The >omputational efficiency of the learning algorithm

is

measured in terms of the (worst case) computation

time

required to pass from a sample of a given size to a hypothesis.

Our results for conjunctive concepts indicate the possibility of

a trade-off between convergence

rate and computational

efficiency., in which the fastest converging learning methods

require significantly more computation time than their slower

converging counterparts.

In order to optimize this trade-off,

applying the general method developed in JBEHW86/, we

em loy heuristic techniques based on the greedy method for fin x ine: a small set cover iN69l lJ741 that trade off a small

decrezse in the convergence rate' for's very large increase in

computational

efficiency. This general idea forms a secondary

theme of the paper.

1. Quantifying inductive bias

In the simplest type of inductive concept learnin , each

instance of a concept is defined by the values of a fixe li set of

attributes, not all of which are necessarily

relevant.

For

example, an instance of the concept "red triangle" might be

characterized

by the fact that its color is red, its shape is tri-

angular and its size is 5. Following [MCLBS], we consider

three types of attributes.

A nominal attribute is one that

takes on a finite, unordered set of mutually exclusive values,

e.g. the attribute

color, restricted to the six primary and

secondary colors. A linear attribute is one with a linearly

ordered set of mutually exclusive values, e.g. a real-valued or

integer-valued

attribute.

A tree-structured

attribute is one

with a finite set of hierarchically ordered values, e.g. the attribute shape with values triangle, square, hezagonll circle, polygon and any-shape, arranged in the usual "is-a hierarchy. Only the leaf values triangle, square, hexagon and circle

are directly observed. Since a nominal attribute can be con-

verted to a tree-structured

attribute by addition of the spe-

cial value any-value, we will restrict our discussion to tree-

structured and linear attributes.

Equations relating attributes

to values will be called

terms, which are either elementary or compound. The possible

forms of elementary terms are as follows.

For tree-structured

attributes:

color = red, shape = polygon.

attribute = value, e.g.

For linear attributes:

value1 2 attribute < value2 e.g.

5 < sire < 12. Strict inequalities are also permzted, as well

as-intervds

open on one side. Terms such as 5 6 size 5 5 are

abbreviated as size = 5.

Compound terms /MC83/ can take the following forms.

For tree-structured

attributes:

attribute = value. or value, or * * . or value,

e.g. shape = square 0: circle, aid for linear at&ibutes: any disjunction of intervals e.g. 0 5 age 2 21 or age 2 65. Disjunctive operators within compound terms are called internal disjunctions.

We consider the following types of concepts:

1. pure conjunctive:

term1 and term2 and * * . and termk,

where each term. is an elementary

term,

e.g.

color = red and 5 1. .&e 5 12,

LEARNING / 485

2. pure disjunctive: same as pure conjunctive connected by "or" 3. internal disjunctive: same as pure conjunctive, compound terms, e.g.

but terms are but allowing

(cofor = red or blue or yellow) and (5 2 size 2 12)

These concept types have the following interpretations

in

the context of rule based knowledge representations. Pure conjunctive: antecedent of a single, variable-free clause rule (PROLOG rule), e.g.

Horn

type = pos t color = red and 5 < size 5 12

Pure disjunctive: antecedents of severafrules,

each with a sin-

le term and all with a common consequent.

f nternal disjunctive: antecedent of a single rule with pure dis-

junctive "helper rules" for the compound terms, e.g. for the

internal disjunctive concept given above, create a new value

"primary" for color and form the rules
color = primary t color = red

color = primary t color = blue

color = primary t color = yellow

type = pos t color = primary and 5 5 site 2 12

In Section 2 we will see how collections nal disjunctive concepts can be generated samples. But first, we describe how these algorithms can be evaluated,

of rules for intermechanically from and other learning

To quantify the inductive bias of a learning al orithm,

we use the following notion from `VC71]. Let 2 be an instance s ace and let H be a class o i concepts defined on X,

e.g. the cfass of pure conjunctive concepts over an instance

space determined by a fixed set of attributes.

For any finite

set S C X of instances, El,(S) = {S n h : h E H}, i.e. the

set of 21 subsets of S that can be obtained by intersecting S

with a concept in H, or equivalently,

the set of all ways the

instances of S can be divided into positive and negative instances so as to be consistent with some concept in H. If n,(S) is the set of all subsets of S then we say that S is shat-

tered by H. The Vapnik-Chervonenkis simply the dimension of H) is the smallest no S C X of cardinality d + 1 is shattered existsrthe dimension of H is infinite.

dimension of H (or integer d such that by H. If no such d

As an example, suppose X is the instance space defined by one linearly ordered attribute size and H is the set of pure conjunctive concepts over X. Thus H is just the set of ele-

mentary terms involving site, i.e. size intervals.

For any

three distinct instances, i.e. instances where size = z, size = y and sire = z, with x < y < Z, there is no concept in H for

which the first and third instances are positive but the second

instance is negative because there is no interval that contains

z and z without containin y. Hence no set of three instances in X can be shattered by fI implying that the dimension of H

is at most 2. Since any two `out of three distinct instances can be shattered by H, this upper bound is tight, at least when

size has three or more distinct values.

cpper bounds on the dimensions of the more general con-

cept classes introduced above are as follows: k term pure conjunctive concepts on n attributes,

each tree-

structured or linear:

1) F(or

k of

d 5 4klog(4k"/n). size roughly n/2

or

larger,

i!`i) bette: u',,`e"r bound .

k term pure*disjunctive

concepts on n attributes:

k(2)term

d < 4klog(l6n)(log(2k)

+ loglog( 16n)).

internal disJunctlve concepts on n attributes,

total of j internal disjunctions:

(3)

d < 5(k+j)log(5(k+j

d;).

Justifications

for these boun d s are omitted

due to

using a lack of

space'.

Let C be a class of target concepts of some type and

level of complexity, e.g. p-term pure conjunctive concepts over

an instance space defined by n-attributes.

Given a target

concept in C and some number m of observations of this con-

`/VC71), [WSl],

[AA831 and [BEHWM] g'lve a variety of other examples of

concept classes of finite dimeoslon.

When H is of finite dimension, Wenocur and

Dudley call H a Vapalk-Chervonenkis

Class (VCC) iwD8li.

The Vapntk-

Chervonenkis nttmber of this class, denoted V(H), corresponds to the dimension of H

plus one.

cept! a learning algorithm will explore some space of possible hypotheses. This will be called the eflective hypothesis space of

the learning algorithm for target concepts in C and sample size m. The numerical bias of the learning algorithm is

defined as the Vapnik-Chervonenkis

dimension of its effective

hypothesis space. A lower bias is a stronger bias. For exam-

ple, in the next section we will present an algorithm (Algo-

rithm 2) for learning pure conjunctive concepts that has the following property: presented with m observations of an unk-

nown p-term pure conjunctive target concept over an instance

space of it attributes, conjunctive hypothesis

it always produces a consistent pure with at most Dlnm + 1 terms. Hence

the-effective hypothesis space of the a'lgorithm for target con-

cepts of this type with samde size m is the class of at most

plnm + l-term pure con'unct*ive

concepts over an instance

space of n attrlbutes.

+ hese limitations on the hypothesis

space are due to the fact that the algorithm only considers

pure conjunctive hypotheses and prefers concepts with fewer

terms, two of the informal types of bias identified in jV86/. Using formula (1) with k = plnm, we can approximately

upper bound the numerical bias of the algorithm for p-term pure conjunctive target concepts over an instance space of n

~~ibute~~~d~~~~~~~~~,y

e can now use the following theorem to relate this numerical bias with the convergence rate of the algorithm for these

target concepts. Theorem 1. [BEHWSS,p Given any consistent

learning

algorithm with numerical bias for targetaconcepts

in a class C

and sample size m of at most rm , where r 2 2 and

0 5 N < 1, then for any probability

distribution

on the

instance space, any target concept in C and any E and 6

between 0 and 1, given a random sample of the target concept

of size at least

I I 8r 8r laol

(6) max %og2, -log-

E 6 e(r-cr)

c( l-(Y) 1 I

the algorithm produces, with probability

at least 1 - 6, a

hypothesis with errpr at most e. If the numerical bias is bounded by r(log m) , it suffices to have sample size
l-cl

(7) I 2 21+4r

8(21+2)`+lr

-q

max %og-,E6

1% f[

6 1!

Plu ging in formula (5 above into (7) but ignoring the

hdln(m ,`, term (i.e. letting 1 = 1 and r = 4plog(4pVn)),

this

theorem shows that given a p-term pure conjunctive target

concept over n attributes and approximately

2

(8) max

128plog(4p t

vi)

log

random observations, Algorithm 2 produces, with probability

at least 1 - 6, a hypothesis with error at most e, independent of the target concept and independent of the underlying distri-

bution governing the generation of observations. By a different

argument, using bound (1') and (6) with (Y = 0, we can also

obtain

the upper bound (9) max

2 16n Qlog- -logt 6' CT

116n t

on the required number of observations, better for small n.

which is considerably

cepts over n attributes,

%This is derived from Theorem 11 of [BEHW86]. We are suppressing some ad-

ditional measurability

assumptions required in the general form of the theorem since

they wll not be relevant in our intended applications (see appendix of [BEHW86/3.

486 / SCIENCE

formulas (8 and (9) is that the convergence rate does not depend at a 11 on the size or complexity of the trees that define

the values of the tree-structured

attributes,

nor on the

number of values of the linearly ordered attributes.

It also

shows that the convergence rate depends only logarithmically

on the number of attributes and the confidence facltor S. The

strongest dependence is on the inverse error - and the

number p of terms in the target concept, yet neitger of these is much worse than linear.

In fact, the argument used in proving Theorem 1 shows

the following stronger result: given any p-term pure conjunc-

tive target concept over n attributes and a sample of approx-

imately the size given in 8) or (9), with probability at least

1 - 6 any consistent hypot 6 esis within the effective hypothesis

space of Algorithm 2, i.e. any consistent conjunct with at most plnm + 1 terms, will have error at most e, independent

of the underlying probability

distribution

that governs the

observations.

Thus no matter what our method is, if we hap-

pen to find a conjunct with at most plnm + 1 terms con-

sistent with a sample of this junct as our hypothesis and that its error is at most

size, then we can use this conhave confidence at least 1 - 6 E. This kind of a posteriori
is what lead Pearl to call the

sample grows. 2. Application: tion.

learning concepts with internal disjunc-

We now illustrate the application

of the analytical

method outlined above in the stepwise development

and

analysis of learning algorithms for pure conjunctive, pure dis-

junctive and finally internal disjunctive concepts.

We will use the single representation

trick, as described

in [CSZ]: each observation is encoded as a rule, e.g. a positive

observation

of a red triangle

of size 5 becomes:

type = pos c color = red and site = 5 and shape = triangle.

Let S be a sample encoded in this form and A be an attri-

bute. If A is a tree-structured

attribute, for each term A = v

that occurs in the sample S, mark the leaf of the tree for A

that represents the value u with the number of positive obser-

vations and the number of negative observations that include

the term A = w. If A is a linear attribute, build a list of such

pairs of numbers, ordered by the values 11. This data structure

will be called the projection of the sample onto the attribute

A.

Given the projection of S onto A, we can find the most s ecific term of the form A = v that implies all of the positive oiiservations, which we call the minimal dominating term for

A'. If A is a tree-structured

attribute, the minimal dominat-

ing term is A = V, where v is the value of the node that is the

least common ancestor of all the leaves of the tree of A WV;:

values occur in at least one positive observation.

minimal dominating term is found using the climbing tree heuristic of [MCL~Z?]. It corres onds to the "lower mark" in the attribute trees of IBSPSS]. pf A is a linear attribute, the minimal dominating term is the term ur 5 A 5 u2, where y1 and v2 are the smallest and largest values of A that occur m

at least one positive observation,

i.e. the result of applying

the "closing interval rule" of /MCL83/. We can use the

minimal dominating terms to find the most specific pure con-

junctive concept consistent with a given sample.

Algorithm 1. (naive algorithm for learning conjunctive concepts)

`For simplicity, we will assume that every sample contains at leaat one poaitive and one negative observation. This implies (among other things) that a minimal dominating term always exists, and will make our algorithms simpler.

1. For each attribute, calculate the projection of the sample onto this attribute and find the minimal dominating term. Let the conjunction of these minimal dominating terms be the expression E. 2. If no negative examples are implied by E then return E, else report that the sample is not consistent with any pure conjunctive concept.

The effective hypothesis space of this algorithm is the

class of all pure conjunctive concepts over some fixed set of

attributes

and doesn't depend on the sample size or the

number of terms in the target concept. Since the dimension of pure con'unctive concepts on n attributes is at most 2n by

formula (1'3 above, the convergence rate of this algorithm is iven by formula (9) above, i.e. given a random sample of size 9), Algorithm 1 produces, with probability at least 1 - 6, a

Kypothesis with error at most e for any pure conjunctive tar-

get concept and any distribution on the instance space.

While significant in its generality,

this upper bound

suffers from the fact that the number of observations required

grows at least linearly in the number of attributes.

In many

AI learning situations where conjunctive concepts are used,

the task is to learn relatively simple conjuncts from samples

over instance spaces with many attributes.

In this case a

better algorithm would be to find the simplest conjunct (i.e.

the conjunct with the least number of terms) that is con-

sistent with the data, rather than the most specific conjunct. With this strategy, given a sample of any p-term pure con-

junctive concept on n attributes, we always find a consistent ure conjunctive hypothesis that has at most p terms. Thus

Ky the same analysis (i.e. using (6) with a = 0) and using for-

mula (1) instead of (1 ) (with k = p)! the upper bound on the

sample size required for convergence 1s reduced to

(10)

32p log( 4p u/n ) 32p log(4p <)

max %og2,

1%

[c 6

E

c

1

which is logarithmic in the number of attributes.

Call this

the optimal algorithm.

Can it be efficiently implemented?

The following shows that it probably cannot.

Theorem ,!?. Given a sample on n attributes,

it is NP-

hard to find a consistent pure conjunctive concept for this

sample with the minimum number of terms.

In proving this theorem, we show that this problem is equivalent to following NP-hard problem [GJ79]:

Minimum Set Cover: given a collection of sets with union

T find a subcollection

whose union is T that has the

mmimum number of sets.

There is, however, an obvious heuristic for approximating the minimum cover of T: First choose a largest set. Then remove the elements of this set from T and choose another set that includes the maximum number of the remaining elements, continuing in this manner until T is exhausted. This is called the greedy method. Applying it to the problem of finding pure conjunctive concepts, we get the following.

Algorithm 2. (greedy algorithm for learning pure conjunctive concepts)

1. For each attribute, calculate the projection of the sample

onto this attribute and find the minimal dominating term. 2. Starting with the empty expression E, while there are nega-

tive observations in the sample do:

a. Among all attributes,

find the minimal dominating

term that eliminates the most negative observations and add it to E, breaking out of the loop if no minimal dom-

inating term eliminates any negative examples.

b. Remove from the sample the negative observations

that are eliminated and update the projections onto the

attributes accordingly. 3. If there are no negative observations

left return E,

else report that the sample is not consistent with any pure

conjunctive concept.

It can be shown that if the set T to be covered has m

elements and p is the size of the minimum cover, then the

greedy method is uaranteed to find a cover of size at most

p logm + 1 [N69] fJ7.41. H ence given a sample of an p-term pure conjunctive concept with m negative observations, Algo-

rithm 2 is guaranteed

to find a consistent pure conjunctive

hypothesis with at most approximately

plogm terms. Using

LEARNING / 43'

Theorem 1, this ives the approximate

upper bound on the

convergence rate 7or Algorithm 2 given by formula (8) in the

previous section. Since Algorithm 2 is, like Algorithm 1, a

consistent algorithm for arbitrary pure .conjunctTve concepts,

the bound on the convergence rate given in formula (9) holds

as well. Note that the b&nd on theconvergence

rate for the

gir;z:~, method is not much worse than the bound (10) for the

algorithm! yet the greedy method 1s slgmficantly

cheaper computationally.

The compliments of pure conjunctive concepts can be

represented as pure disjunctive concepts. Hence this is the dual form of pure conjunctive concepts. A variant of Algo-

rithm 2 can be used to learn oure disiunctive concerts. In the

dual form, each term must eliminate811 negative observat,ions

and need only imply some subset of positive observations, and

all terms together must imply all positive observations. The

dual greedy method is to repeatedly choose the term that

implies the most positive observations and add it to the dis-

junct, removing the positive observations

that are implied,

until all ositive observations

are accounted for. This is a

variant o P the "star" method in MCL891. Since k term nure

disjunctive concepts have a Vannik-Chervonenkis

dimension

similar to that of k term pure cbnjunctive concepts formula

(2)), the analysis of the convergence rate of this a fgorithm

goes through as above.

We now tackle internal disjunctive concepts. The calcu-

lation of the Vannik-Chervonenkis

dimension of these concerts

given in the pretious section indicates that the strongest bias

yn learning them is to minimize the total number `;f terms plus internal disiunctions. i.e. to minimize the total size of all

the terms, where the size `of a compound term is defined as the number of internal disjunctions it contains plus one. Let E be

an internal disjunctive concept that is consistent with a given sample. As with pure conjunctive concepts, each term in E

implies all positive observations

and eliminates some set of

negative observations.

A compound term with this pro erty

will be called a dominating compound term. We would liKe to

eliminate all the negative observations

using a set of terms

with the smallest total size. This leads to the following.

Minimum Set Cover problem with positive integer costs: given a collection of sets with union T, where each set has

associated with it a positive integer cost, find a subcollection whose union is T that has the minimum total cost.

Since it generalizes Minimum Set Cover, this problem is

clearly NP-hard. However, approximate

solutions can found

by a generalized greedy method. Let T' be a set of elements

remainine to be covered. For each set in the collection. define the gain,&ost ratio of this set as the number of elements of T'

it contains divided by its cost. The generalized greedy

method is to always choose the set with the highest gain/cost

ratio. As with the basic Minimum Set Cover problem, it can

be shown that if the original set T to be covered has m elements and p is the minimum cost of any cover, then the gen-

eralized greedy method is guaranteed most plogm L 1.

to find a cover of size at

To apply this method in learning internal disjunctions, let the gain/cost ratio of a dominating compound term be the number of negative observations it eliminates divided by its size.

Algorithm 3. (greedy algorithm for learning internal dis-

junctive concepts)

1. For each attribute, calculate the projection of the sample

onto this attribute. 2. Starting with the empty expression E, while there are nega-

tive observations in the samnle do:

a. Among all attributei,

find the dominating compound

term t with the highest gain/cost ratio, breaking out of

the loop if none have ositive gains. If there is no term

for the attribute of t aPready in E, add t to E. Otherwise

replace the old term in E for the attribute of t with t.

b. Remove from the sample the negative observations

t

eliminates and update the projections onto all attributes

accordingly. 3. If'there are no negative observations

left return E,

else report that the sample is not consistent with any internal

disjunctive concept.

-

To implement this algorithm, we need a procedure to find a dominating compound term with the highest gain/cost

ratio for a given attribute from the projection of the sample

onto that attribute.

Since there are in general exponentially

many distinct dominating compound terms with respect to the

number of leaves of a tree-structured

attribute or the number

of values of a linear attribute, this cannot be done by exhaustive search. However, there is a reasonably efficient recursive

procedure that does this for tree-structured

attributes, and a

simple iterative procedure fsr linear attributes.

Each of these

procedures takes time O(q ), where q is the number of dis-

tinct values of the attribute that ap ear in the observations.

Space limitations preclude a detaile cr discussion of these pro-

cedures.

By formula (3) and the above result on the performance

of the generalized greedy method. the numerical bias of Algo-

rithm g for k-terminteinal

disju'nctive target concepts using

a total of j internal disjunctions (i.e. of size k + j) and sam-

ple size S(k+j)fn(m)log

S(k?j ln(rr$`&)

most

atIgnoring

the

l$l"n~~j]

term, formula (i ) of 4 heorem 1 gives an upper bound on the

convergence rate similar to that of Algorithm 2 given in

equation (8), with k+j substituted for p.

3. Extensions
There are several possible extensions to these algorithms that would increase their domain of application. We outline two of them here.

1. The ability to handle "don't care" values for some attributes in the sample (see e.g. [QSS], [V84/).

A "don't care" value for attribute A corresponds to an observation in rule form having the term A = any-value. In

fact, we can go one step further and let observations be arbi-

trary pure conjunctive expressions, where, for example, the

positive observation

shape = polygon and color = blue means

that the concept contains all blue polygons, and the

corresponding

negative observation

means that no blue

olygons are contained in the concept. In this form, the prob-
Pem of learning from examples is seen to be a special. case of the more general problem of knowledge refinement /MICS,Y,I,

wherein we start with a collection of rules that are already

known and try to derive from them a simpler, more general

(and hopefully more comprehensible) set of rules. This exten-

sion can be accomplished by modifying the notion of the pro-

jection of the samples onto the attributes to allow terms of

the observations

to project to internal nodes of the tree-

structured attributes or intervals in the linear attributes. Other parts of the algorithm are changed accordingly.

2. Promoting synergy while learning a set of concepts.

So far we have only considered the problem of learning a

single concept in isolation. In fact, we would like to build sys-

tems that learn many concepts, with higher level concepts

bein built upon intermediate and low level concepts (see e.g. P4 / SB86/). The first step is to extend our notion of concept to inc ude many-valued observations, rather than just positive

and negative. In this way we can learn rules that define the

values of one attribute in terms of the values of the other

attributes. This is essentially knowledge refinement on rela-

tional databases JMIC86/. Ignoring attributes

with many

values for the time being, this can be accomplished in a reasonable way by finding a separate concept for each value of

the attribute that discriminates this value from all the others.

Once we have learned to recognize the values of the new

attribute.in

terms of the primitive attributes, it can be added

to the set of primitive attributes and used later in learning to

reco nize the values of other attributes. In this scheme new

attri f utes are always nominal. However, they could acquire a

tree structure as they are used to define later concepts in the

following manner (see also [US61 /BSPSS/): whenever an inter-

nal disjunctive concept is formed using a compound term

A =vloru20t

a*- otvk, check to see if this same com-

pound term is required by other concepts. If it is required often enough, check the tree for the attribute A. If a node

v can be added without destroying the

~&tlZZ~~~~

"d:, `.Z. ff a new node is added, the compound

terms it reprdsents can be replaced by an elementary term

using the value of the new node. Thus the collection of rules

given in Section 1 for the internal disjunctive concept involv-

488 / SCIENCE

ing the primary colors might be created by the "discovery" of

the higher level value of primary for the attribute color. In

this way a useful vocabulary

of more abstract

values for

attributes evolves under the pressure to find simple forms for

higher level concepts, creating a synergy between learned con-

cepts.

Another type of synergy is achieved by using the algo-

rithm for pure conjunctive concepts along with the dual algo-

rithm for pure disjunctive concepts. If new Boolean attributes

are defined for often-used pure conjuncts or disjuncts, then

these can allow the recognition of higher level concepts in

DNF and CNF respectively

by effectively reducing these

expressions to pure disjunctive or conjunctive form. Often

used internal disiunctive concents could be used as well. The

creation of thesk new attribites

can greatly increase the

number of attributes that are considered in- later learning

tasks, which argues strongly for learning methods whose per-

formance does not degrade badly as the number of attributes

grows, such as those we have presented.

Conclusion.

We have presented a methodology for the quantitative

analysis of learning performance based on a relatively simple

combinatorial property of the space of hypotheses explored by

the learning algorithm.

Applications

of this methodology

have been presented in the development and analysis of learn-

ing algorithms for pure conjunctive,

pure disjunctive and

internal disjunctive concepts. Several open problems remain,

in addition to those mentioned above. Some are:

1. Can we develop the proper analytic tools to deal with algo-

rithms that

a. attempt to handle the problem of noisy data jQSS/ or

b. attemnt to learn "fuzzv" concepts that are defin'ed proba-

bilisticaliy with respect to-the instance space?

2. What power is gained by allowing the learnin algorithm to

form queries during the learning process [SASS] b'vG86/?

3. Can we find provably efficient incremental learning algorithms (i.e. ones that modify an evolving hypothesis after each

observation) to replace the "batch processing" learning algo-

rithms we have given here?

4. To what extent can we extend these results to concepts

that involve internal structure, expressed with the use of vari-

ables, quantifiers and binary relations (e.g. the c-expressions [MCL 83/)?

of

Acknowledgements.

I would like to thank Larry Rendell

for suggestin

the relationship

between the Vapnik-

Chervonenkis i imension and Utgoff's notion of inductive bias

and Ryszard Michalski for suggesting I look at the problem of

learning internal disjunctive concepts. I also thank Les Vali-

ant, Leonard Pitt, Phil Laird, Ivan Bratko and Stephan Mug-

gleton and Andrzej Ehrenfeucht these ideas, and an anonymous

for helpful discussions of referee for suggestions on

improving the presentation.

References:

[ANG88] .bgluin, A., "Learning regular sets from queries and

counter-examples," Tech. rep. sity, 1986.
[A881 Assouad, P., "Densite Grenoble 33 (3) (1983) 233-282.

YALEU/DCS/TR-464, et Dimension," Ann.

Yale UniverInst. Fourier,

[B85] Banerji, R., "The logic of learning: a basis for pattern recognition and improvement of performance," in Advances in Computers,

24, (1985) 177-216. [BEHW86] Bl umer, A., A. Ehrenfeucht,

D. Haussler and MLI.War-

muth, "Classifying learnable geometric concepts with the VapnikChervonenkis dimension," 18th ACM Symp. Theor. Comp., Berkeley,

CA, 1986, to appear. [BSPSS] Bundy, A., B. Silver
parison of some rule-learning 137-181.

and D. Plummer, "An analytical com-

programs,"

Attif. Intel. 27 (1985)

[CSZ] Cohen, P. and E. Feigenbaum,

Handbook oj AI, Vol. 3, Wil-

liam Kaufmann, 1982, 323-494.

(GJ79] Garey, M. and D. Johnson, Computers and Intractability:

Guide to the Theory of NP-Completeness, W.H.Freeman,

1979.

A

[J74] Johnson, D.S.. "Approximation

algorithms

problems," J. Comp. Sys. Sci., 9, 1974.

for combinatorial

[MCL83] ,Michalski, R.S., "A theory and methodology of inductive learning, " in Machine learning: an artificial intelligence approach,

Tioga Press, 1983, 83-134.

[MIC83] Michie, D., "Inductive rule generation in the context of the fifth generation," Proc. Int. Mach. Learning Workshop, Monticello,

Il., (1983) 65-70. [MIT821 Mitchell, (1982) 203-226.

T.&l., "Generalization

as search;" Art. Intefl. 18

[N69] Nigmatullin, R.G., "The Fastest Descent Method for Covering Problems (in Russian)," Proceedings of a Symposium on Questions of Precision and Eficiency of Computer Algorithms, Book 5, Kiev, 1969, pp. 116-126.

[P78] Pearl, J., "On the connection between the complexity and
credibility of inferred models," Id. J. Gen. Sys., 4, 1978, 255-64.
[QSS] Quinlan, J.R., "Induction of decision trees," Machine Learning, 1 (1) (1986), to appear. [R86] Rendell, L., "A general framework for induction and a study
of selective induction," Machine Learning 1 (2) (1986), to appear.
[SBSB] Sammut, C., and R. Banerji, "Learning concepts by asking questions," in Machine Learning II, R. Michalski, J. Carbonell and
T. Mitchell, eds., Morgan Kaufmann, Los Altos, CA, 1986.

[U86] Utgoff, P., "Shift of Bias for inductive Concept Learning,"

ibid.
[V84] Valiant, L.G., "A theory 27(11), 1984, pp. 1134-42.

of the learnable,"

Comm.

[V85] Valiant, L.G., "Learning disjunctions 9th IJCAI, Los Angeles, CA, 1985, 560-6.

of conjunctions,"

ACM, Proc.

[VC71] Vapnik, V.N. and A.Ya.Chervonenkis,

"On the uniform con-

vergence of relative frequencies of events to their probabilities,"

Th.

Prob. and its iippl., 16(2), 1971, 264-80.

[WDSl] Wenocur, R.S. and R.M.Dudley,

"Some special Vapnik-

Chervonenkis classes," Discrete Math., 33, 1981, 313-8.

LEARNING

/ 4%)

