Analyzing the Population Based Incremental
Learning Algorithm by Means of Discrete
Dynamical Systems

Cristina Gonz ´alez!
Jose A. Lozano
Pedro Larra ˜naga
Intelligent Systems Group,
Department of Computer Science and Artiﬁcial Intelligence,
University of the Basque Country, Spain

In this paper the convergence behavior of the population based incremen-
tal learning algorithm (PBIL) is analyzed using discrete dynamical systems.
A discrete dynamical system is associated with the PBIL algorithm. We
demonstrate that the behavior of the PBIL algorithm follows the iterates
of the discrete dynamical system for a long time when the parameter Α
is near zero. We show that all the points of the search space are ﬁxed
points of the dynamical system, and that the local optimum points for the
function to optimize coincide with the stable ﬁxed points. Hence it can
be deduced that the PBIL algorithm converges to the global optimum in
unimodal functions.

1. Introduction

During the 1990s many real combinatorial optimization problems were
solved successfully by means of genetic algorithms (GAs). But the exis-
tence of deceptive problems, where the performance of GAs is very poor,
has motivated the search for new optimization algorithms. To overcome
these difﬁculties a number of researches have recently suggested a family
of new algorithms called estimation of distribution algorithms (EDAs)
[1, 2].

Introduced by M ¨uhlenbein and PaaΒ [2], EDAs constitute an exam-
ple of stochastic heuristics based on populations of individuals, each of
which encodes a possible solution of the optimization problem. These
populations evolve in successive generations as the search progresses,
organized in the same way as most evolutionary computation heuris-
tics.
In contrast to GAs, which consider the crossover and mutation
operators as essential tools to generate new populations, EDAs replace

!Electronic mail address: ccpgomoc@si.ehu.es.

Complex Systems, 12 (2000) 465–479; $ 2000 Complex Systems Publications, Inc.

466

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

those operators by the estimation and sampling of the joint probability
distribution of the selected individuals.

However, the bottleneck of this new heuristic lies in estimating the
joint probability distribution associated with the database containing
the selected individuals. To avoid this problem, several authors have
proposed different algorithms where simpliﬁed assumptions concern-
ing the conditional (in)dependencies between the variables of the joint
probability distribution are made. A review of the different approaches
in the combinatorial and numerical ﬁelds can be found in [3–5].

The population based incremental learning algorithm (PBIL) can be
considered as an EDA, as proposed in [6]. PBIL supposes that all the
variables are independent. At each step of the algorithm a probabil-
ity vector is maintained. This vector is sampled Λ times to obtain Λ
new solutions. The Μ ’ Λ best solutions are selected and these are
used to modify the probability vector with a neural networks-inspired
rule.

Recently, there has been increasing interest in PBIL and many papers
have appeared in the literature. Some of these papers are concerned
with applications of the algorithm [7–11]. Others are concerned with
extensions of the method to general cardinality spaces [12], with contin-
uous spaces [13–16], with parallel versions [17], and with comparisons
[18, 19]. Finally there are papers concerned with modiﬁcations of the
method: adding genetics information [20] and extending the algorithm
to nonstatic multiobjective problems [21]. However, despite the effort
devoted to applications or to creating new variants, little attention has
been given to the theoretical aspects of PBIL.

In this paper we introduce a new framework for studying the PBIL
algorithm theoretically. We assign a discrete dynamical system to PBIL.
It will be shown that the behavior of PBIL follows the iterations of the
discrete dynamical system when an algorithm’s parameter Α is near zero.
This fact enables us to study the discrete dynamical system instead of
the iterations of PBIL. We discover that all the points of the search space
are ﬁxed points for the discrete dynamical system. Moreover, the local
optima are stable ﬁxed points and the other points of the search space
are unstable ﬁxed points. This result has various outcomes, the most
important of which is that PBIL converges to the global optimum in
unimodal functions.

The remainder of this work is organized as follows. Section 2 de-
scribes the PBIL algorithm. Section 3 reviews the work carried out in the
theoretical analysis of PBIL. Section 4 introduces the discrete dynam-
ical system associated with PBIL. The relation between PBIL and the
dynamical system is analyzed in section 5, while the dynamical system
is studied in section 6. Finally we draw conclusions in section 7.

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

467

2. An introduction to the population based incremental learning

algorithm

Before presenting the algorithm, let us introduce some notation. We
denote a vector or a matrix by a bold-face letter and a component of
a vector by a normal letter. The random variables will be written in
capital letters. We use the letters m and r as component indexes, and
the letters i and k as vector indexes, hence yi,m will represent the mth
component of the yi individual.

PBIL was introduced by Baluja [6] in 1994 and further improved by
Baluja and Caruana [22] in 1995. This algorithm is based on the idea
of substituting the individuals of a population by a set of their statistics.
In our case we suppose that the function to optimize is deﬁned in the
binary space ( ) *0, 1+l with ,(, ) 2l ) n. Given a population, the set of
statistics is expressed by a vector of probabilities p ) (p1, . . . , pm, . . . , pl),
where pm represents the probability of obtaining a value of 1 in the mth
component.

The algorithm works as follows. At each step, drawing the probabil-
ity vector p, Λ individuals are obtained and the Μ best of them (Μ ’ Λ),
y1-Λ, y2-Λ, . . . , yΜ-Λ, are selected. These selected individuals will be used
to modify the probability vector. A Hebbian-inspired rule is used to
update the probability vector:

p(t.1) ) (1 / Α)p(t) . Α

1
Μ

y(t)
k-Λ

(1)

Μk)1

where p(t) is the probability vector at step t, and Α 0 (0, 1] is an algo-
rithm’s parameter. Figure 1 shows a pseudocode for the PBIL algorithm.
Once we have seen Figure 1, it is useful to note that the behavior of

the algorithm is the same in two functions f1 and f2 if:
f1(y) > f1(y2) 3 f2(y) > f2(y2).

1 y, y2 0 (,

(2)

Obtain an initial probability vector p(0)
while no convergence do

1 , y(t)

begin
Using p(t) obtain Λ individuals y(t)
1 , y(t)
Evaluate and rank y(t)
2 , . . . , y(t)
Λ
Select the Μ ’ Λ best individuals y(t)
1-Λ, y(t)
k)1 y(t)
k-Λ

p(t.1) ) (1 / Α)p(t) . Α1/Μ"Μ

end

Figure 1. Pseudocode for the PBIL algorithm.

2 , . . . , y(t)
Λ

2-Λ, . . . , y(t)
Μ-Λ

Complex Systems, 12 (2000) 465–479

468

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

Moreover the important thing is not the particular value that a function
f has in an individual y but the ranking implied by this function on (.
Below we assume that we have a minimization problem. Keeping in
mind the previous argument we can consider a function

f - ( 4 —

as a permutation of the elements of (. In this case the last individual of
(, yn has the smallest function value, the penultimate yn/1 the second
smallest, and so on, with the ﬁrst individual y1 being the one with the
biggest function value:

f (y1) 5 ! 5 f (yn/1) 5 f (yn).

(3)

Therefore the number of different injective functions on ( is given
by n!. In order to simplify notation, we assume in the rest of the paper
that we have to optimize an injective function. However, the results can
easily be extended to noninjective functions.

3. Previous approaches to modeling population based incremental

learning mathematically

In the literature we have found different ways to mathematically model
PBIL.

It can be modeled by means of Markov chains given that the proba-
bility vector p(t.1) at step t . 1 only depends on the probability vector
p(t) at step t. However, this chain has inﬁnite numerable states once an
initial probability vector p(0), a value for the parameter Α, and values
for Λ and Μ have been established. In that case the chain is neither ir-
reducible nor aperiodic, hence not much information can be extracted.
However, using this model it is shown in [23] that for PBIL with Λ ) 2
and Μ=1 applied to the OneMax function in two dimensions:

(4)

t46

Plim

p(t) ) (a, b) 4 1

when Α 4 1, p(0) 4 (a, b), and (a, b) 0 *0, 1+2. This shows a strong
dependence of PBIL on the initial parameters.

In [24] it is shown that if we denote by E[p(t)] the expectation of the

probability vector in time t, then for a linear function:

lim
t46

E[p(t)] ) y!

(5)

where y! is the optimum point of (.

In [25] a statistical framework for combinatorial optimization over
ﬁxed-length binary strings is presented and it is shown that the PBIL
algorithm can be derived from a gradient dynamical system acting on
Bernoulli probability vectors.

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

469

When Α ) 1, a particular case of PBIL, the univariate marginal dis-
tribution algorithm (UMDA) [26] is found.
In [27] it is shown that
UMDA with inﬁnite population and proportional selection stops at lo-
cal optima.

4. Assigning a discrete dynamical system to population based

incremental learning

A new approach can be opened if we model PBIL by means of discrete
dynamical systems. This idea has been used previously for the simple
GA [28, 29] obtaining important results. Our approach and notation
are strongly inspired by these previous works. The key question is to
associate PBIL with a discrete dynamical system, such that, the trajecto-
ries followed by the probability vectors *p(t)+t)0,1,2,... in PBIL are related
to the iterations of the discrete dynamical system.

PBIL can be considered as a sequence of probability vectors, each one

given by a transition stochastic rule Τ:

p(0) Τ4 p(1) Τ4 p(2) Τ4 ! Τ4 p(t) Τ4 p(t.1) Τ4 !

that is, p(t.1) ) Τ(p(t)) ) Τt.1(p(0)). We are interested in the trajectories
followed by the iterations of Τ, and in particular, in its limit behavior:

Τt(p(0)).

lim
t46

We deﬁne a new operator G:

G - [0, 1]l 4 [0, 1]l

(6)

such that G(p) ) (G1(p), . . . , Gl(p)) ) E[Τ(p)]. The operator G is a de-
terministic function that gives the expected value of random operator Τ.
The iterations of G are deﬁned as Gt(p) ) G(Gt/1(p)). We are interested
in the relation between the iterations of Τ and the iterations of G, and in
particular we want to answer the question: Is there any relation between
Τt(p) and Gt(p)? However, before looking for this relation we calculate
the expression of G(p).

The operator G can be expressed as follows:

G(p) ) E

89999999999:

(1 / Α)p . Α

1
Μ

Yk-Λ

;<<<<<<<<<<=

Μk)1
89999999999:
Μk)1

;<<<<<<<<<<=

) (1 / Α)p . Α

1
Μ

E

Yk-Λ,p

.

(7)

Hence, we have to calculate the expected sum of the Μ best individuals
given that Λ have been sampled from the probability vector p. Our
analysis is restricted to the case Μ ) 1 (the most frequent in the literature).

Complex Systems, 12 (2000) 465–479

470

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

In this case the G operator can be calculated explicitly, which we do in
several steps.

k)1 Yk-Λ,p] is reduced for the case Μ ) 1

to:

First, the expected value E["Μ

E[Y1-Λ,p] )

yiP(Y1-Λ ) yi , p)

(8)

ni)1

i/1j)1
ij)1

where P(Y1-Λ ) yi , p) denotes the probability of obtaining yi as the best
individual after an iteration of the algorithm. Let P(S ) yi , p) denote
the probability of sampling vector yi. The probability that yi is the best
individual, given that we have sampled Λ individuals and the probability
vector is p, can be expressed as follows [24]:

P(Y1-Λ ) yi , p) ) P(S ) yi , p)

where:

i ) *yj 0 ( , f (yj) > f (yi)+
(>
(5
i ) *yj 0 ( , f (yj) 5 f (yi)+.

Λk)1

P((>
i

, p)k/1P((5
i

, p)Λ/k,

(9)

(10)
(11)

Finally, it is important to note that, given a probability vector p, the
probability of sampling a particular individual y ) (y1, . . . , ym, . . . , yl)
is:

qy(p) ) P(S ) y , p) )

ymm (1 / pm)1/ym.
p

(12)

According to the previous results it can be said that the operator G
can be expressed as a polynomial function of the probability vector p.
In fact, if we take into account that each function can be considered
as a permutation of ( (only the individuals preceding yi have a bigger
function value than yi), then:

lm)1

P((>
i

, p) )

P((5
i

, p) )

qyj

(p)

qyj

(p).

(13)

(14)

Finally G(p) can be expressed in our study case (Μ ) 1) as follows:

G(p) ) (1 / Α)p

.Α

yiqyi

(p)

ni)1

>???????

@

Λk)1

qyj

(p)

>??????

@

i/1j)1

qyj

(p)

Λ/kABBBBBBB
ABBBBBB

C

C

.

(15)

k/1 >??????
ABBBBBB

C

@

ij)1

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

471

5. Relationship between Τt(p) and Gt(p)

In this section we demonstrate that when the parameter Α is near 0, then
the stochastic operator Τ follows the deterministic operator G for a long
time. First we set up the relation between G and Τ and then we study
the relation between their iterates.
Lemma 1. Given Ε > 0 and Γ < 1, there exists Α0 > 0 independent of
the probability vector p such that with probability at least Γ:

Α < Α0 F ,,Τ(p) / G(p),, < Ε.

(16)
Proof. Suppose that Τ(p) ) (1 / Α)p . Αy. The discrepancy between G
and Τ can be bounded by:

,,Τ(p) / G(p),, ) ,,(1 / Α)p . Αy / (1 / Α)p / ΑE[Y1-Λ,p],,

) Α,,y / E[Y1-Λ,p],, ’ Αl.

(17)

Since the right-hand side goes to zero as Α 4 0 the proof is completed.
Theorem 1. Given k > 0, Ε > 0, and Γ < 1, there exists Α0 such that
with probability at least Γ and for all 0 ’ t ’ k:

Α < Α0 F ,,Τt(p) / Gt(p),, < Ε.

(18)
Proof. We make the proof by induction on k. The base case k ) 1
coincides with Lemma 1. Given that G is uniformly continuous (it is
continuous in the compact [0, 1]l), choose ∆ such that

,,Τk/1(p) / Gk/1(p),, < ∆ F ,,G(Τk/1(p)) / G(Gk/1(p)),, < Ε
2

(19)
By the inductive hypothesis, if Α < Α1 then with probability at least

.

1 / (1 / Γ)/2 we have

,,Τk/1(p) / Gk/1(p),, < ∆.

(20)
By Lemma 1 (applied to Τk/1(p) instead of p) let Α2 be such that with

probability at least 1 / (1 / Γ)/2

Α < Α2 F ,,Τk(p) / G(Τk/1(p)),, < Ε
2

(21)
It follows that if Α < Α0 ) min*Α1, Α2+, then with probability at least Γ

.

,,Τk(p) / Gk(p),, ’ ,,G(Τk/1(p)) / Gk(p),,

.,,Τk(p) / G(Τk/1(p)),, < Ε
2

.

Ε
2

.

(22)

Theorem 1 means that when Α is near 0, the stochastic operator Τ
follows, with high probability and for a long time, the iterations of the
deterministic operator G.

Complex Systems, 12 (2000) 465–479

472

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

The operator G can be thought of as a discrete dynamical system:

p, G(p), . . . , Gt(p), . . . .

6. The discrete dynamical system G

In this section we try to ﬁnd properties of the discrete dynamical sys-
tem G that will give some information concerning the behavior of the
PBIL algorithm. Before studying the discrete dynamical system G, it is
important to note that the following discrete dynamical system G:

qyj

(p)

Λ/kABBBBBBB
ABBBBBB

C

C

(23)

G(p) )

yiqyi

(p)

qyj

(p)

ni)1

>???????

@

Λk)1

>??????

@

i/1j)1

k/1 >??????
ABBBBBB

@

C

ij)1

has the same behavior as G in the points of the search space. All the
results obtained here for G are valid for G. In particular they have the
same singular points, so we study the dynamical system of equation (23)
instead of the operator G.

Theorem 2. All the points of ( are ﬁxed points of G.
Proof. Given y 0 (, clearly E[Y1-Λ,p ) y] ) y, because the probability
of sampling an individual different from y given p ) y is zero. Hence:

G(y) ) E[Y1-Λ,p ) y] ) y.

(24)

Before introducing Theorem 3, we deﬁne what we mean by a “local
optimum” for the Hamming distance. We also give a result borrowed
from page 126 in [30], that will be useful in the proof of the theorem.

Deﬁnition 1. Given a real function f deﬁned in (, a point y is a local
minimum for the Hamming distance dH if:

lm)1

for all y2 such that dH(y2, y) )

,y2
m / ym, ) 1 F f (y2) 5 f (y).

(25)

Lemma 2. Let x be a ﬁxed point of a discrete dynamical system G.

If the eigenvalues of DG(x) all have absolute value less than 1 then x is a
stable ﬁxed point of G.

If some eigenvalue of DG(x) has absolute value greater than 1 then x is
an unstable ﬁxed point of G.

Theorem 3. Given a real function f deﬁned on (, we have the follow-
ing.

All the local optima of f with respect to the Hamming distance are stable
ﬁxed points of G.

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

473

All the nonlocal optima of f with respect to the Hamming distance are
unstable ﬁxed points of G.

Proof. We use Lemma 2 in order to prove these afﬁrmations.

We ﬁrst show that all the local optima are stable points; and, in a

second step, the last result.

Let y 0 ( be a local optimum of a function f (i.e., all the individuals
in ( whose Hamming distance from y is one, precede y in the order
imposed by f in (). We will see that in this case DG,y ) 0. In fact, we
will see that

) 0 for all r, m ) 1, 2, . . . , l,

(26)

where Gr is the rth component of equation (23):

Gr(p) )

yi,rqyi

(p)

ni)1

>???????

@

Λk)1

>??????

@

i/1j)1

qyj

(p)

k/1 >??????
ABBBBBB

@

C

qyj

(p)

Λ/kABBBBBBB
ABBBBBB

C

C

.

(27)

ij)1

To show equation (26), we must ﬁrst take into account the follow-
ing results which can be checked easily using the deﬁnition of qy(p)
(equation (12)):

HGr

Hpm&&&&&&&&y

(28)
(29)

(30)

(31)

(32)

(33)

) 0 for all y such that dH(y, yk) 5 2

if dH(y, yk) ) 1 and yk,m ) 1, ym ) 0
/1 if dH(y, yk) ) 1 and yk,m ) 0, ym ) 1

) 0 if dH(y, yk) ) 1 and yk,m ) ym.

(y) ) 0 for all y I yk
(yk) ) 1 for all yk 0 (
if yk,m ) 1
/1 if yk,m ) 0

Hqyk

Hqyk

qyk
qyk
Hqyk

Hpm&&&&&&&&yk
Hpm&&&&&&&&y
Hpm&&&&&&&&y
Hpm&&&&&&&&y
Hpm&&&&&&&&y

Hqyk

HGr

)

) 1
) 1
ni)1

The partial derivative of Gr with respect to pm can be expressed as:

H
Hpm

8999999999999:

yi,rqyi

>???????

@

Λk)1

>??????

@

i/1j)1

k/1 >??????
ABBBBBB

@

qyj

C

qyj

Λ/kABBBBBBB
ABBBBBB

C

C

ij)1

.

(34)

We analyze each adding term of HGr/Hpm,y separately and split it into
three different cases. The partial derivative of a term of equation (34)
must be written ﬁrst (yi,r has been eliminated, because it is a constant

;<<<<<<<<<<<<=&&&&&&&&&&&y

Complex Systems, 12 (2000) 465–479

474

term):

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

j)1 qyj

Hqyi

j)1 qyj
Hpm

Hqyi"Λ
k)1("i/1
)k/1("i
Hpm&&&&&&&&y
Λk)1
i/1j)1
H"Λ
k)1("i/1

.qyi

qyj

(y)

(y)

>???????

>??????

@

@

k/1 >??????
ABBBBBB
C
@
j)1 qyj

)Λ/k
ij)1
)k/1("i

qyj

Hpm

)

&&&&&&&&&&&y

(y)

Λ/kABBBBBBB
ABBBBBB
C
j)1 qyj

C

)Λ/k

&&&&&&&&&&&y

.

(35)

We split the problem into the following three different cases.

1. Let yi 0 ( be an individual such that dH(y, yi) 5 2. In this case, using
(y) ) 0 (equation (28)), equa-

/Hpm,y ) 0 (equation (31)) and qyi

Hqyi
tion (35) has a value of 0.

ABBBBBB

C

>??????

@

>???????

@

2. Let yi 0 ( be an individual such that dH(y, yi) ) 1. In this case, in the
(y) ) 0 (equation (28)) but, in the ﬁrst,
/Hpm,y could be different from zero. However in this case (because
(y) ) 0 for all j ) 1, ... , i)

second term we again have qyi
Hqyi
yi is before y in the order in ( and then qyj
j)1 qyj

j)1 qyj

of zero in y. Hence equation (35) has a value of zero for this kind of
individual.

)Λ/k has a value

3. Finally we take into account the term corresponding to individual y. If
ym ) 0 this term does not appear in the sum. In the other case (ym ) 1), if
i represents the place that individual y takes in the ordering of ( (y ) yi),
equation (35) can be expressed by:

(y)

qyj

qyj

Hqy

.qy(y)

k/1 >??????

Λ/kABBBBBBB
ABBBBBB

)k/1("i

the second multiplicative term"Λ
k)1("i/1
Hpm&&&&&&&&y
i/1j)1
Λk)1
ij)1
&&&&&&&&&&&y
H"Λ
)Λ/k
k)1("i/1
)k/1("i
Hpm&&&&&&&&y
H"Λ
Λk)1
(y) and Bi(y) )"i
where Ai(y) )"i/1
Hpm&&&&&&&yBi(y)Λ/1 . Ai(y)Bi(y)Λ/2 . ! . Ai(y)Λ/1 ) 1.

Hpm
Ai(y)k/1Bi(y)Λ/kABBBBB

k)1 Ai
Hpm

C
j)1 qyj

. qy(y)

j)1 qyj

j)1 qyj

j)1 qyj

k/1Bi

Hqy

Hqy

(y).

In the next reasoning, note that Ai(y) ) 0 and Bi(y) ) 1.
The ﬁrst term of equation (36) is:

,

(36)

&&&&&&&&&&&y

(37)

Λ/k

(y)

C

>?????

@

)

C

@

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

475

Bi(y)Λ/2.

(38)

Substituting again the values of Ai(y) and Bi(y) in equation (38):

The second term of equation (36) can be expressed as:

i

) (Λ / 1)Bi(y)Λ/2 HBi

Bi(y)Λ/2 . Ai(y)(Λ / 2)Bi(y)Λ/3 HBi

Hpm&&&&&&&&&y

Hpm&&&&&&&&&y

qy(y)

!"#$)1

(Λ / 1)

(Λ / 1)

Hqyj

)

Hqyj

.

HAi

.

HAi

.

ij)1

) (Λ / 1)Bi(y)Λ/2 HBi

.! . (Λ / 1)Ai(y)Λ/2 HAi

i . AiBΛ/2
i . ! . AΛ/1
Hpm

&&&&&&&&&y
HBΛ/1

Hpm&&&&&&&&&y
Hpm&&&&&&&&&y
Hpm&&&&&&&&&y
Hpm&&&&&&&&&y
Hpm&&&&&&&y
Hpm&&&&&&&y
i/1j)1
>??????????????????????
Hpm&&&&&&&y
Hpm&&&&&&&y
. dH(yj,y))1
@ dH (yj,y)52
"#
!
$
>??????????????????????
ABBBBBBBBBBBBBBBBBBBBBB
Hpm&&&&&&&y
Hpm&&&&&&&y
@ dH (yj,y)52
. dH(yj,y))1
"#
!
ABBBBBBBBBBBBBBBBBBBBBB
>??????????????????????
Hpm&&&&&&&y
Hpm&&&&&&&y
Hpm&&&&&&&y

>??????????????????????
! "# $

. dH (yj,y))1
"#
!

yj,m)ym

)/1

! "# $

Hqyk

Hqyk

.1

.

Hqyj

Hqyj

.

C

)/1

@

Hqyj

j<i

)0

j<i

)0

)0

C

@

$

Hqyj

Hqyj

$

(Λ / 1)

.

.

Hqy

Hpm&&&&&&&y

! "# $)1

ABBBBBBBBBBBBBBBBBBBBBB

C

(39)

) /1.(40)

ABBBBBBBBBBBBBBBBBBBBBB

C

. dH (yj,y))1
"#
!

yj,m)ym

)0

Hqyj

Hpm&&&&&&&y

$

Complex Systems, 12 (2000) 465–479

Taking into account that yk is such that dH(y, yk) ) 1, yk,m ) 0, and

ym ) 1 we ﬁnd that the second term of equation (36) is:

Hence DG(y) ) 0 for all local optimum points y and all the eigen-
values have a value of zero. Moreover, we have shown that the local

476

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

optimum points for a function f are stable ﬁxed points of the discrete
dynamical system G.

In the case of a point y of ( that is not a local optimum, following

arguments similar to the previous case, it can be shown that:

) 0 for all r I m.

(41)

In all the previous cases the adding terms have a value of zero, except
for the case that there exists y2 such that d(y, y2) ) 1, y2
m I ym and in
addition f (y2) > f (y) (y is before y2 in the order imposed by f in (),
the adding terms corresponding to y and y2 are different from zero but
their sum is zero.

In the case r ) m, there exists m 0 *1, . . . , l+ such that

HGr

Hpm&&&&&&&&y
Hpm&&&&&&y

HGm

> 1.

From Theorem 3 it can be deduced that when the value of Α is near
0, then the PBIL algorithm can only converge to the local optima of f .
So, the PBIL algorithm converges to the global optimum in unimodal
functions.

7. Conclusions and future work

We have opened a new approach to the theoretical study of PBIL: us-
ing discrete dynamical systems. Associating an appropiate dynamical
system we have shown that the PBIL algorithm follows the iterates of
the dynamical system.
In addition, we have seen that PBIL can only
converge to local optima; meaning, in the case of unimodal functions,
that PBIL converges to the global optimum.
This is a preliminary analysis and much work remains to be done.
Our ﬁrst objective is to generalize the results to the case in which Μ > 1
individuals are selected. Furthermore, we plan to study the size of the
basin of attraction of each local optimum to calculate the probability of
convergence to each local optimum.

Acknowledgments

This work was supported by the University of the Basque Country
under grant 9/UPV/EHU 00140.226-12084/2000. Also C. Gonz´alez is
supported by UPV-EHU.

References

[1] P. Larra ˜naga and J. A. Lozano, Estimation of Distribution Algorithms: A
New Tool for Evolutionary Computation (Kluwer Academic Publishers,
2001).

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

477

[2] H. M ¨uhlenbein and G. PaaΒ, “From Recombination of Genes to the Esti-
mation of Distributions: I. Binary Parameters,” in Parallel Problem Solv-
ing from Nature, PPSN-IV, edited by H.-M. Voigt, W. Ebeling, I. Rechen-
berg, and H.-P. Schwefel (Springer-Verlag, Berlin Heidelberg, 1996).

[3] P. Larra˜naga, R. Etxeberria, J. A. Lozano, and J. M. Pe˜na, “Combinato-
rial Optimization by Learning and Simulation of Bayesian Networks,” in
Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial In-
telligence, UAI-2000, edited by C. Boutilier and M. Goldszmidt (Morgan
Kaufmann, San Francisco, 2000).

[4] P. Larra ˜naga, R. Etxeberria, J. A. Lozano, and J. M. Pe˜na, “Optimization
in Continuous Domains by Learning and Simulation of Gaussian Net-
works,” in Proceedings of the Genetic and Evolutionary Computation
Conference, Workshop Program, edited by A. S. Wu (Las Vegas, Nevada,
2000).

[5] M. Pelikan, D. E. Goldberg, and F. Lobo “A Survey of Optimization
by Building and Using Probabilistic Models,” Technical Report IlliGAL,
99018 (University of Illinois at Urbana-Champaign, Illinois Genetic Al-
gorithms Laboratory, 1999).

[6] S. Baluja, “Population-Based Incremental Learning: A Method for In-
tegrating Genetic Search Based Function Optimization and Competitive
Learning,” Technical Report CMU-CS-94-163 (Computer Science Depart-
ment, Carnegie Mellon University, Pittsburgh, 1994).

[7] E. Gali´c and M. H ¨ohfeld, “Improving the Generalization Performance
of Multi-Layer Perceptrons with Population-Based Incremental Learn-
ing,” in Parallel Problem Solving from Nature, PPSN-IV, edited by H.-M.
Voight, W. Ebeling, I. Rechenberg, and H.-P. Schwefel (Springer-Verlag,
Berlin Heidelberg, 1996).

[8] B. Maxwell and S. Anderson, “Training Hidden Markov Models Us-
ing Population-Based Learning,” Genetic and Evolutionary Computation
Conference, GECCO-99 (1999).

[9] I. Inza, M. Merino, P. Larra˜naga, J. Quiroga, B. Sierra, and M. Girala,
“Feature Subset Selection by Genetic Algorithms and Estimation of Dis-
tribution Algorithms. A Case Study in the Survival of Cirrhotic Patients
Treated with TIPS,” Artiﬁcial Intelligence in Medicine (in press).

[10] R. Sukthankar, S. Baluja, and J. Hancock, “Evolving an Intelligent Vehicle
for Tactical Reasoning in Trafﬁc,” International Conference on Robotics
and Automation, ICRA’97 (1997).

[11] M. Gallagher, “Multi-layer Perceptron Error Surfaces: Visualization,
Structure and Modelling,” PhD thesis, (Department of Computer Science
and Electrical Engineering, University of Queensland, 2000).

[12] M. P. Servais, G. de Jaer, and J. R. Greene, “Function Optimization Using
Multiple-Base Population Based Incremental Learning,” in Proceedings of
the Eighth South African Workshop on Pattern Recognition (1997).

Complex Systems, 12 (2000) 465–479

478

C. Gonz´alez, J. A. Lozano, and P. Larra˜naga

[13] V. Kvasnicka, M. Pelikan, and J. Pospichal, “Hill Climbing with Learn-
ing (an Abstraction of Genetic Algorithms),” Neural Networks World, 6
(1996) 773–796.

[14] S. Rudlof and M. K ¨oppen, “Stochastic Hill Climbing by Vectors of Nor-
mal Distributions,” in Proceedings of the First Online Workshop on Soft
Computing, WSC1 (University of Nagoya, Nagoya, Japan, 1996).

[15] M. Sebag and A. Ducoulombier, “Extending Population-Based Incremen-
tal Learning to Continuous Search Spaces,” in Parallel Problem Solving
from Nature, PPSN-V (Springer, 1998).

[16] A. Berny, “An Adaptative Scheme for Real Function Optimization Acting
as a Selection Operator,” in First IEEE Symposium of Combinations of
Evolutionary Computation and Neural Networks (2000).

[17] S. Baluja, “Genetic Algorithms and Explicit Search Statistics,” Advances in
Neural Information Processing Systems, edited by M. Mozer, M. Jordan,
and T. Petsche (The MIT Press, Cambridge, MA, 1997).

[18] N. Monmarch´e, E. Ramat, L. Desbarats, and G. Venturini, “Probabilistic
Search with Genetic Algorithms and Ant Colonies,” in Proceedings of the
Genetic and Evolutionary Computation Conference, Workshop Program,
edited by A. S. Wu (Las Vegas, Nevada, 2000).

[19] N. Monmarch´e, E. Ramat, G. Dromel, M. Slimane, and G. Venturini, “On
the Similarities between AS, BSC, and PBIL: Toward the Birth of a New
Meta-Heuristic,” Technical Report, 215, E3i (Laboratoire d’Informatique
pour l’Industrie (E3i), Universit´e de Tours, 1999).

[20] M. Schmidt, K. Kristensen, and T. R. Jensen, “Adding Genetics to the
Standard PBIL Algorithm,” in Congress on Evolutionary Computation,
CEC’99 (1999).

[21] C. Fyfe, “Structured Population-Based Incremental Learning,” Soft Com-

puting, 2 (1999) 191–198.

[22] S. Baluja and R. Caruana, “Removing the Genetics from the Standard
Genetic Algorithm,” in Proceedings of the International Conference on
Machine Learning, edited by A. Prieditis and S. Russell (Morgan Kauf-
mann, 1995).

[23] C. Gonz´alez, J. A. Lozano, and P. Larra˜naga, “The Convergence Behav-
ior of the PBIL Algorithm: A Preliminary Approach,” in Fifth Interna-
tional Conference on Artiﬁcial Neural Networks and Genetic Algorithms,
ICANNGA’2001 (in press).

[24] M. H ¨ohfeld and G. Rudolph, “Towards a Theory of Population-Based
Incremental Learning,” in Proceedings of the Fourth IEEE Conference on
Evolutionary Computation (IEEE Press, 1997).

Complex Systems, 12 (2000) 465–479

Analyzing the PBIL Algorithm by Means of Discrete Dynamical Systems

479

[25] A. Berny, “Selection and Reinforcement Learning for Combinatorial Op-
timization,” in Parallel Problem Solving from Nature, PPSN-VI, edited
by M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lutton, J. J. Merelo,
and H.-P. Schwefel (Springer-Verlag, Berlin Heidelberg, 2000).

[26] H. M ¨uhlenbein, “The Equation for Response to Selection and its Use for

Prediction,” Evolutionary Computation, 5 (1998) 303–346.

[27] H. M ¨uhlenbein and T. Mahnig, “Evolutionary Computation and Wright’s

Equation,” Theoretical Computer Science (in press).

[28] M. D. Vose, The Simple Genetic Algorithm: Foundations and Theory

(MIT Press, 1999).

[29] M. D. Vose, “Random Heuristic Search,” Theoretical Computer Science,

229(1-2) (1999) 103–142.

[30] E. R. Sheinerman, Invitation to Dynamical Systems (Prentice-Hall, 1996).

Complex Systems, 12 (2000) 465–479

