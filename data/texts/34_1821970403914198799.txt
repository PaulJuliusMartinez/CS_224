Randomized PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension

Manfred K. Warmuth Computer Science Department University of California - Santa Cruz manfred@cse.ucsc.edu

Dima Kuzmin Computer Science Department University of California - Santa Cruz
dima@cse.ucsc.edu

Abstract
We design an on-line algorithm for Principal Component Analysis. In each trial the current instance is projected onto a probabilistically chosen low dimensional subspace. The total expected quadratic approximation error equals the total quadratic approximation error of the best subspace chosen in hindsight plus some additional term that grows linearly in dimension of the subspace but logarithmically in the dimension of the instances.

1 Introduction

In Principal Component Analysis the n-dimensional data instances are projected into a kdimensional subspace (k < n) so that the total quadratic approximation error is minimized. After centering the data, the problem is equivalent to finding the eigenvectors of the k largest eigenvalues of the data covariance matrix.

We develop a probabilistic on-line version of PCA: in each trial the algorithm chooses a kdimensional projection matrix P t based on some internal parameter; then an instance xt is received and the algorithm incurs loss xt - P txt 22; finally the internal parameter is updated. The goal is to obtain algorithms whose total loss in all trials is close to the smallest total loss of any k-dimensional
subspace P chosen in hindsight.

We first develop our algorithms in the expert setting of on-line learning. The algorithm maintains a mixture vector over the n experts. At the beginning of trial t the algorithm chooses a subset P t of k experts based on the current mixture vector wt. It then receives a loss vector t  [0..1]n and incurs loss equal to the remaining n - k components of the loss vector, i.e. i{1,...,n}-P t it. Finally it
updates its mixture vector to wt+1. Note that now the subset P t corresponds to the subspace onto which we "project", i.e. we incur no loss on the k components of P t and are charged only for the
remaining n - k components.

The trick is to maintain a mixture vector wt as a parameter with the additional constraint that wti 

1 n-k

.

We

will

show

that

these

constrained

mixture

vectors

represent

an

implicit

mixture

over

subsets

of experts of size n - k, and given wt we can efficiently sample from the implicit mixture and use it

to predict. This gives an on-line algorithm whose total loss is close to the smallest n - k components

of t t and this algorithm generalizes to an on-line PCA algorithm when the mixture vectors are

replaced

by

density

matrices

whose

eigenvalues

are

bounded

by

1 n-k

.

Now

the

constrained

density

matrices represent implicit mixtures of the (n - k)-dimensional subspaces. The complementary

k-dimensional space is used to project the current instance.

2 Standard PCA and On-line PCA

Given a sequence of data vectors x1, . . . , xT , the goal is to find a low-dimensional approximation
of this data that minimizes the 2-norm approximation error. Specifically, we want to find a rank k projection matrix P and a bias vector b  Rn such that the following cost function is minimized:

T
loss(P , b) = xt - (P xt + b) 22.
t=1

Differentiating and solving for b gives us b = (I - P ) x¯, where x¯ is the data mean. Substituting this bias b into the loss we obtain

TT

loss(P ) =

(I - P )(xt - x¯)

2 2

=

(xt - x¯) (I - P )2(xt - x¯).

t=1 t=1

Since I - P is a projection matrix, (I - P )2 = I - P , and we get:

T

loss(P ) = tr((I - P ) (xi - x¯)(xi - x¯) ) = tr((I - P ) C) = tr(C) - tr( P C),

i=1

rank n-k

rank k

where C is the data covariance matrix. Therefore the loss is minimized over (n - k)-dimensional subspaces and this is equivalent to maximizing over k-dimensional subspaces.

In the on-line setting, learning proceeds in trials. (For the sake of simplicity we are not using a bias

term at this point.) At trial t, the algorithm chooses a rank k projection matrix P t. It then receives

an instance xt and incurs loss

xt - P txt

2 2

=

tr((I

- P t) xt(xt)

). Our goal is to obtain an

algorithm whose total loss over a sequence of trials

T t=1

tr((I

-

P

t)

xt(xt)

) is close to the total

loss of the best rank k projection matrix P , i.e. infP tr((I - P )

T t=1

xt(xt)

).

Note that the

latter loss is equal to the loss of standard PCA on data sequence x1, . . . , xT (assuming the data is

centered).

3 Choosing a Subset of Experts

Recall that projection matrices are symmetric positive definite matrices with eigenvalues in {0, 1}.

Thus a rank k projection matrix can be written as P =

k i=1

pipi

,

where

the

pi

are

the

k

ortho-

normal vectors forming the basis of the subspace. Assume for the moment that the eigenvectors are

restricted to be standard basis vectors. Now projection matrices become diagonal matrices with en-

tries in {0, 1}, where the number of ones is the rank. Also, the trace of a product of such a diagonal

projection matrix and any symmetric matrix becomes a dot product between the diagonals of both

matrices and the whole problem reduces to working with vectors: the rank k projection matrices

reduce to vectors with k ones and n - k zeros and the diagonal of the symmetric matrix may be seen as a loss vector t. Our goal now is to develop on-line algorithms for finding the lowest n - k components of the loss vectors t so that the total loss is close the to the lowest n - k components

of

T t=1

t.

Equivalently,

we

want

to

find

the

highest

k

components

in

t.

We begin by developing some methods for dealing with subsets of components. For convenience we

encode such subsets as probability vectors: we call r  [0, 1]n an m-corner if it has m components

set to

1 m

and the remaining n - m components set to zero.

At trial t the algorithm chooses an

(n - k)-corner rt. It then receives a loss vector t and incurs loss (n - k) rt · t.

Let Amn of the

mnconmsi-sct oorfnearlls.coCnlveeaxrlycoamnybincaotmiopnosnoefntmw-icoorfnearsv.ecItnorotwherinwAormndsi,sAamnt misostthem1cobnevceaxusheuiltl

is

a

convex

combination

of

numbers

in

[0..

1 m

].

Therefore

Amn



Bmn ,

where

Bmn

is

the

set

of

n-

dimensional vectors w for which |w| = theorem implies that Anm = Bmn :

i wi

=

1 and 0



wi



1 m

,

for

all

i.

The following

Theorem 1. Algorithm 1 produces a convex combination1 of at most n m-corners for any vector in

Bmn .

Algorithm 1 Mixture Construction

input 1  m < n and w  Bmn repeat

Let r be a corner whose m components correspond to nonzero components of w

and

contain

all

the

components

of

w

that

are

equal

to

|w| m

Let s be the smallest of the m chosen components in w

and l be the largest value of the remaining n - m components

p

w := w - min(m s, |w| - m l) r and output p r until w = 0

Proof.

Let b(w) be the number of boundary components in w, i.e.

b(w)

:=

|{i

:

wi is 0 or

|w| m

}|.

Let

Bmn

be

all

vectors

w

such

that

0



wi



|w| m

,

for

all

i.

If

b(w)

=

n,

then

w

is

either

a

corner

or

0. The loop stops when w = 0. If w is a corner then it takes one iteration to arrive at 0. We show

if w  Bmn and w is neither a corner nor 0, then the successor w  Bmn and b(w) > b(w). Clearly,

w  0, because the amount that is subtracted in the m components of the corner is at most as large

as wi

the =

corresponding components

wi -

p m



|w|-p m

=

|wb |
m

.

of w. We next Otherwise wi

show = wi

that wi   l, and

|wb |
m
l

.

If i |wb |
m

belongs follows

to the corner from the fact

then that

p  |w| - m l. This proves that w  Bmn .

For showing that b(w) > b(w) first observe that all boundary components in w remain boundary

components in

|w|-p m

=

|wb |
m

.

w: zeros stay zeros However, the number

and if wi = of boundary

|w| m

then

i

components

is is

included in the corner and wi = increased at least by one because

the components corresponding to s and l are both non-boundary components in w and at least one

of them becomes a boundary point in w: if p = m s then the component corresponding to s in

w

is

s

-

p m

l

=

|w|-p m

=

=0

|wb |
m

.

in It

w and follows

if p that

= |w| - m l then it may take up to n

the component corresponding iterations to arrive at a corner

to l in w which has

is n

boundary components and one more iteration to arrive at 0. Finally note that there is no weight

vector w  Bmn s.t. b(w) = n - 1 and therefore the size of the produced linear combination is at most n. More precisely, the size is at most n - b(w) if n - b(w)  n - 2 and one if w is a corner.

The algorithm produces a linear combinations of corners, i.e. w = j pjrj. Since pj  0 and all |rj| = 1, j pj = 1 and we actually have a convex combination.

Fact 1. For any loss vector , the following corner has the smallest loss of any convex combination of corners in Anm = Bmn : Greedily pick the component of minimum loss (m times).

How can we use the above construction and fact? It seems too hard to maintain information about all

n n-k

corners of size n-k. However, the best corner is also the best convex combination of corners,

i.e. the best from the set Ann-k where each member of this set is given by

n n-k

coefficients. Luckily,

this set.

sTeht eorfecfoornevewxeccoamnbsienaartciohnfsoerqtuhaelsbeBsntn-hkypaontdheist itsakinesthnecsoeetffiBcnni-enktsatnodsfpoercaifnyyasmucehmhbyeproitnhtehsaist

we can always construct a convex combination (of size  n) of (n - k)-corners which has the same

expected loss for each loss vector. This means that any algorithm predicting with a hypothesis vector

in Bnn-k can be converted to an algorithm that probabilistically chooses an (n - k)-corner. Finally, the set P t of the k components missed by the chosen (n - k)-corner corresponds to the subspace

we project onto.

Algorithm 2 spells out the details for this approach. The algorithm chooses a corner probabilistically

and (n - k) wt · t is the expected loss in one trial. The projection wt onto Bnn-k can be achieved as

follows:

find

the

smallest

l

s.t.

capping

the

largest

l

components

to

1 n-k

and

rescaling

the

remaining

n-l

weights

to

total

weight

1-

l n-k

makes

none

of

the

rescaled

weights

go

above

1 n-k

.

The

simplest

1The existence of a convex combination of at most n corners is implied by Carathe´odory's theorem [Roc70], but the algorithm gives an effective construction.

algorithm starts with sorting the weights and then searches for l with a binary search. However, a linear algorithm that recursively uses the median is given in [HW01].

Algorithm 2 Capped Weighted Majority Algorithm

input: 1  k < n and an initial probability vector w1  Bnn-k

for t = 1 to T do

Decompose wt as j pjrj with Algorithm 1, where m = n - k

Draw a corner r = rj with probability pj Let P t be the k components outside the drawn corner

Receive loss vector t

Incur loss (n - k) r · t =

i{1,...,n}-P t

t i

.

wit

:=

wit

exp(-

t i

)

/

Z

,

where

Z

normalizes

the

weights

to one

wt+1 := argmin d(w, wt)

wBnn-k

end for

When k = n - 1, n - k = 1 and B1n is the entire probability simplex. In this case the call to Algorithm 1 and the projection onto B1n are vacuous and we get the standard Randomized Weighted Majority algorithm [LW94]2 with loss vector t.

Let d(u, w) denote the relative entropy between two probability vectors: d(u, w) =

i

ui

log

ui wi

.

Theorem 2. On an arbitrary sequence of loss vectors 1, . . . , T  [0, 1]n, the total expected loss

of Algorithm 2 is bounded as follows:

(n - k) T wt · t  (n - k) 

T t=1

u

·

t

+

d(u,

w1)

-

d(u,

wT

+1)

,

1 - exp(-)

t=1

for any learning rate  > 0 and comparison vector u  Bnn-k.

Proof. The update for wt in Algorithm 2 is the update of the Continuous Weighted Majority for which the following basic inequality is known (essentially [LW94], Lemma 5.3):

d(u, wt) - d(u, wt)  - u · t + wt · t(1 - exp(-)).

(1)

The weight vector wt+1 is a Bregman projection of vector wt onto the convex set Bnn-k. For such projections the Generalized Pythagorean Theorem holds (see e.g [HW01] for details):

d(u, wt)  d(u, wt+1) + d(wt+1, wt)

Since Bregman divergences are non-negative, we can drop the d(wt+1, wt) term and get the follow-

ing inequality:

d(u, wt) - d(u, wt+1)  0, for u  Bnn-k.

Adding this to the previous inequality we get:

d(u, wt) - d(u, wt+1)  - u · t + wt · t(1 - exp(-))

By summing over t, multiplying by n - k, and dividing by 1 - exp(-), the bound follows.

4 On-line PCA

In 0.

this context Also the set

(matrix) corners Anm consists of

are density matrices with m eigenvalues all convex combinations of such corners.

equal

to

1 m

and

The maximum

the rest are eigenvalue

of a convex combination of symmetric matrices is at most as large as the maximum eigenvalue of

any of the matrices ([Bha97], Corollary III.2.2). Therefore each convex combination of corners is

2The original Weighted Majority algorithms were described for the absolute loss. The idea of using loss vectors instead was introduced in [FS97].

a

density

matrix

whose

eigenvalues

are

bounded

by

1 m

and

Anm



Bmn ,

where

Bnm

consists

of

all

density

matrices

whose

maximum

eigenvalue

is

at

most

1 m

.

Assume

we

have

some

density

matrix

W  Bmn with eigendecomposition W diag()W . Algorithm 1 can be applied to the vector of

eigenvalues  of this density matrix. The output convex combination of up to n diagonal corners

 = j pjrj can be turned into a convex combination of matrix corners that expresses the density matrix: W = j pj W diag(rj)W . It follows that Anm = Bmn as in the diagonal case.

Theorem 3. For any symmetric matrix S, minW Bnm tr(W S) attains its minimum at the following matrix corner: greedily choose orthogonal eigenvectors of S of minimum eigenvalue (m times).

Proof. Let (W ) denote the vector of eigenvalues of W in descending order and let (S) be the same vector of S but in ascending order. Since both matrices are symmetric, tr(W S)  (W ) · (S) ([MO79], Fact H.1.h of Chapter 9). Since (W )  Bmn , the dot product is minimized and the inequality is tight when W is an m-corner corresponding to the m smallest eigenvalues of S.
Also the greedy algorithm finds the solution (see Fact 1 of this paper).

Algorithm 2 generalizes to the matrix setting. The Weighted Majority update is replaced by the corresponding matrix version which employs the matrix exponential and matrix logarithm [WK06] (The update can be seen as a special case of the Matrix Exponentiated Gradient update [TRW05]).

The following theorem shows that for the projection we can keep the eigensystem fixed. Here (U , W ) denotes the quantum relative entropy tr(U (log U - log W )).
Theorem 4. Projecting a density matrix onto Bnm w.r.t. the quantum relative entropy is equivalent to projecting the vector of eigenvalues w.r.t. the "normal" relative entropy: If W has the eigende-

composition W diag()W , then

argmin (U , W ) = WuW , where u = argmin d(u, ).

U Bmn

uBmn

Proof. If (S) denotes the vector of eigenvalues of a symmetric matrix S arranged in de-

scending order, then tr(ST )  (S) · (T ) ([MO79], Fact H.1.g of Chapter 9). This im-

plies that tr(U log W )  (U ) · log (W ) and (U , W )  d((U ), (W )). Therefore

min (U , W )  min d(u, ) and if u minimizes the r.h.s. then W diag(u)W mini-

U Bnm

uBmn

mizes the l.h.s. because (W diag(u)W, W ) = d(u, ).

Algorithm 3 On-line PCA algorithm

input: 1  k < n and an initial density matrix W 1  Bnn-k for t = 1 to T do
Perform eigendecomposition W t = WW

Decompose  as j pjrj with Algorithm 1, where m = n - k Draw a corner r = rj with probability pj

Form a matrix corner R = W diag(r)W

Form a rank k projection matrix P t = I - (n - k)R

Receive data instance vector xt

Incur loss

xt - P txt

2 2

=

tr((I

-

P t) xt(xt)

)

W t = exp(log W t -  xt(xt) ) / Z, where Z normalizes the trace to 1

W t+1 :=

argmin

t
(W , W )

W Bnn-k

end for

The expected loss in trial t of this algorithm is given by (n - k)tr(W txt(xt) )

Theorem 5. For an arbitrary sequence of data instances x1, . . . , xT of 2-norm at most one, the total expected loss of the algorithm is bounded as follows:

T

(n - k)tr(W txt(xt)

)



(n

-

 k)

T t=1

tr(U

xt(xt) 1

) + (U , W - exp(-)

1

)

-

(U

,

W

T

)

,

t=1

for any learning rate  > 0 and comparator density matrix U  Bnn-k.3

t
Proof. The update for W is a density matrix version of the standard Weighted Majority update which was used for variance minimization along a single direction (i.e. k = n - 1) in [WK06]. The
basic inequality (1) for that update becomes:

(U

,

W

t)

-

(U

,

W

t
)



-

tr(U

xt(xt)

) + tr(W txt(xt)

)(1 - exp(-))

As in the proof of Theorem 2 of this paper, the Generalized Pythagorean theorem applies and dropping one term we get the following inequality:

t
(U , W )

- (U , W t+1)



0,

for

U



Bnn-k .

Adding this to the previous inequality we get:

(U , W t) - (U , W t+1)  - tr(U xt(xt) ) + tr(W txt(xt) )(1 - exp(-))

By summing over t, multiplying by n - k, and dividing by 1 - exp(-), the bound follows.

It

is

easy

to

see

that

(U , W 1)



(n -

k) log

n n-k

.

If

k



n/2,

then

this

is

further

bounded

by

k

log

n k

.

Thus,

the

r.h.s.

is

essentially

linear

in

k,

but

logarithmic

in

the

dimension

n.

By tuning  [CBFH+97, FS97], we can get regret bounds of the form:

(expected total loss of alg.) - (total loss best k-space) nn
= O (total loss of best k-subspace) k log k + k log k .

(2)

Using standard but significantly simplified conversion techniques from [CBFH+97] based on the leave-one-out loss we also obtain algorithms with good regret bounds in the following model: the algorithm is given T - 1 instances drawn from a fixed but unknown distribution and produces a k-space based on those instances; it then receives a new instance from the same distribution. We can bound the expected loss on the last instance:

(expected loss of alg.) - (expected loss best k-space)

=O

(expected loss

of best k-subspace) k log

n k

+

k log

n k

.

TT

(3)

5 Lower Bound

The simplest competitor to our on-line PCA algorithm is the algorithm that does standard (uncen-
tered) PCA on all the data points seen so far. In the expert setting this algorithm corresponds to "projecting" to the n - k experts that have minimum loss so far (where ties are broken arbitrarily). When k = n - 1, this becomes the follow the leader algorithm. It is easy to construct an adversary strategy for this type of deterministic algorithm (any k) that forces the on-line algorithm to incur n times as much loss as the off-line algorithm. In contrast our algorithm is guaranteed to have expected additional loss (regret) of the order of square root of k ln n times the total loss of the best
off-line algorithm. When the instances are diagonal matrices then our algorithm specializes to the
standard expert setting and in that setting there are probabilistic lower bounds that show that our tuned bounds (2,3) are tight [CBFH+97].

6 Simple Experiments
The above lower bounds do not justify our complicated algorithms for on-line PCA because natural data might be more benign. However natural data often shifts and we constructed a simple dataset of this type in Figure 1. The first 333 20-dimensional points were drawn from a Gaussian distribution with a rank 2 covariance matrix. This is repeated twice for different covariance matrices of rank
3The xt(xt) can replaced by symmetric matrices St whose eigenvalues have range at most one.

Figure 1: The data set used for the experiments. Different colors/symbols denote the data points that came from three different Gaussians with rank 2 covariance matrices. The data vectors are 20-dimensional but we plot only the first 3 dimensions.

Figure 2: The blue curve plots the total loss of on-line algorithm up to trial t for 50 different runs (with k = 2 and  fixed to one). Note that the variance of the losses is small. The red single curve plots the total loss of the best subspace of dimension 2 for the first t points.

Figure 3: Behavior of the algorithm around a transition point between two distributions. Each ellipse depicts the projection matrix with the largest coefficient in the decomposition of W t. The transition sequence starts with the algorithm focused on the projection matrix for the first subset of data and ends with essentially the optimal matrix for the second subset. The depicted transition takes about 60 trials.
2. We compare the total loss of our on-line algorithm with the total loss of the best subspace for the first t data points. During the first 333 datapoints the latter loss is zero since the first dataset is 2-dimensional, but after the third dataset is completed, the loss of any fixed off-line comparator is large. Figure 3 depicts how our algorithm transitions between datasets and exploits the on-lineness of the data. Randomly permuting the dataset removes the on-lineness and results in a plot where the total loss of the algorithm is somewhat above that of the off-line comparator (not shown).
Any simple "windowing algorithm" would also be able to detect the switches. Such algorithms are often unwieldy and we don't know any strong regret bounds for them. In the expert setting there is however a long line of research on shifting (see e.g. [BW02, HW98]). An algorithm that mixes a little bit of the uniform distribution into the current mixture vector is able to restart when the data switches. More importantly, an algorithm that mixes in a little bit of the past average density matrix is able to switch quickly to previously seen subspaces and to our knowledge windowing techniques cannot exploit this type of switching. Preliminary experiments on face image data indicate that the algorithms that accommodate switching work as expected, but more comprehensive experiments still need to be done.

7 Conclusions

We developed a new set of techniques for low dimensional approximation with provable bounds. Following [TRW05, WK06], we essentially lifted the algorithms and bounds developed for diagonal case to the matrix case. Are there general reductions?

The on-line PCA problem was also addressed in [Cra06]. However, that paper does not fully capture the PCA problem because their algorithm predicts with a full-rank matrix in each trial, whereas we predict with a probabilistically chosen projection matrix of the desired rank k. Furthermore, that paper proves bounds on the filtering loss, which are typically easier to prove, and it is not clear how this loss relates to the more standard regret bounds proven in this paper.

For the expert setting there are alternate techniques for designing on-line algorithms that do as

well as the best subset of n - k experts: set {i1, . . . , in-k} receives weight proportional to

exp(-

j

<t ij

)

=

j

exp(-

<t ij

).

In this case we can get away with keeping only one weight

per expert (the ith expert gets weight exp(- <i t)) and then use dynamic programming to sum over

sets (see e.g. [TW03] for this type of methods). With some more work, dynamic programming can

also be applied for PCA. However, our new trick of using additional constraints on the eigenvalues

is an alternative that avoids dynamic programming.

Many technical problems remain. For example we would like to enhance our algorithms to learn a bias as well and apply our low-dimensional approximation techniques to regression problems.

Acknowledgment: Thanks to Allen Van Gelder for valuable discussions re. Algorithm 1.

References

[Bha97] R. Bhatia. Matrix Analysis. Springer, Berlin, 1997.

[BW02]

Olivier Bousquet and Manfred K. Warmuth. Tracking a small set of experts by mixing past posteriors. Journal of Machine Learning Research, 3:363­396, 2002.

[CBFH+97] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How to use expert advice. 44(3):427­485, 1997.

[Cra06]

Koby Crammer. Online tracking of linear subspaces. In Proceedings of the 19th Annual Conference on Learning Theory (COLT 06), Pittsburg, June 2006. Springer.

[FS97]

Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119­139, August 1997.

[HW98]

Mark Herbster and Manfred Warmuth. Tracking the best expert. Machine Learning, 32(2):151­178, 1998. Earlier version in 12th ICML, 1995.

[HW01]

Mark Herbster and Manfred K. Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research, 1:281­309, 2001.

[LW94]

N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Inform. Comput., 108(2):212­261, 1994.

[MO79]

A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and its Applications. Academic Press, 1979.

[Roc70] R. Rockafellar. Convex Analysis. Princeton University Press, 1970.

[TRW05]

K. Tsuda, G. Ra¨tsch, and M. K. Warmuth. Matrix exponentiated gradient updates for on-line learning and Bregman projections. Journal of Machine Learning Research, 6:995­1018, June 2005.

[TW03]

Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine Learning Research, 4:773­818, 2003.

[WK06]

Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. In Proceedings of the 19th Annual Conference on Learning Theory (COLT 06), Pittsburg, June 2006. Springer.

