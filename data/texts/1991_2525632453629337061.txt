ROC Graphs: Notes and Practical Considerations for
Researchers

Tom Fawcett (tom.fawcett@hp.com)
HP Laboratories, MS 1143, 1501 Page Mill Road, Palo Alto, CA 94304

March 16, 2004

Abstract. Receiver Operating Characteristics (ROC) graphs are a useful technique
for organizing classi(cid:12)ers and visualizing their performance. ROC graphs are com-
monly used in medical decision making, and in recent years have been increasingly
adopted in the machine learning and data mining research communities. Although
ROC graphs are apparently simple, there are some common misconceptions and pit-
falls when using them in practice. This article serves both as a tutorial introduction
to ROC graphs and as a practical guide for using them in research.

Keywords: classi(cid:12)cation, classi(cid:12)er evaluation, ROC, visualization

Introduction

An ROC graph is a technique for visualizing, organizing and selecting
classi(cid:12)ers based on their performance. ROC graphs have long been
used in signal detection theory to depict the tradeo(cid:11) between hit rates
and false alarm rates of classi(cid:12)ers (Egan, 1975; Swets et al., 2000).
ROC analysis has been extended for use in visualizing and analyzing
the behavior of diagnostic systems (Swets, 1988). The medical deci-
sion making community has an extensive literature on the use of ROC
graphs for diagnostic testing (Zou, 2002). Swets, Dawes and Monahan
(2000) recently brought ROC curves to the attention of the wider public
with their Scienti(cid:12)c American article.

One of the earliest adopters of ROC graphs in machine learning
was Spackman (1989), who demonstrated the value of ROC curves
in evaluating and comparing algorithms. Recent years have seen an
increase in the use of ROC graphs in the machine learning community.
In addition to being a generally useful performance graphing method,
they have properties that make them especially useful for domains
with skewed class distribution and unequal classi(cid:12)cation error costs.
These characteristics have become increasingly important as research
continues into the areas of cost-sensitive learning and learning in the
presence of unbalanced classes.

Most books on data mining and machine learning, if they mention
ROC graphs at all, have only a brief description of the technique.
ROC graphs are conceptually simple, but there are some non-obvious

c(cid:13) 2004 Kluwer Academic Publishers. Printed in the Netherlands.

ROC101.tex; 16/03/2004; 12:56; p.1

2

Tom Fawcett

complexities that arise when they are used in research. There are also
common misconceptions and pitfalls when using them in practice.

This article attempts to serve as a tutorial introduction to ROC
graphs and as a practical guide for using them in research. It collects
some important observations that are perhaps not obvious to many in
the community. Some of these points have been made in previously
published articles, but they were often buried in text and were sub-
sidiary to the main points. Other notes are the result of information
passed around in email between researchers, but left unpublished. The
goal of this article is to advance general knowledge about ROC graphs
so as to promote better evaluation practices in the (cid:12)eld.

This article is divided into two parts. The (cid:12)rst part, comprising
sections 1 through 6, covers basic issues that will be encountered in
most research uses of ROC graphs. Each topic has a separate section
and is treated in detail, usually including algorithms. Researchers in-
tending to use ROC curves seriously in their work should be familiar
with this material. The second part, in section 7, covers some related
but ancillary topics. They are more esoteric and are discussed in less
detail, but pointers to further reading are included.

Note: Implementations of the algorithms in this article, in the Perl
language, are collected in an archive available from: http://www.purl.
org/NET/tfawcett/software/ROC_algs.tar.gz

1. Classi(cid:12)er Performance

We begin by considering classi(cid:12)cation problems using only two classes.
Formally, each instance I is mapped to one element of the set fp; ng of
positive and negative class labels. A classi(cid:12)cation model (or classi(cid:12)er )
is a mapping from instances to predicted classes. Some classi(cid:12)cation
models produce a continuous output (e.g., an estimate of an instance’s
class membership probability) to which di(cid:11)erent thresholds may be
applied to predict class membership. Other models produce a discrete
class label indicating only the predicted class of the instance. To dis-
tinguish between the actual class and the predicted class we use the
labels fY; Ng for the class predictions produced by a model.

Given a classi(cid:12)er and an instance, there are four possible outcomes.
If the instance is positive and it is classi(cid:12)ed as positive, it is counted
as a true positive; if it is classi(cid:12)ed as negative, it is counted as a false
negative. If the instance is negative and it is classi(cid:12)ed as negative, it is
counted as a true negative; if it is classi(cid:12)ed as positive, it is counted as
a false positive. Given a classi(cid:12)er and a set of instances (the test set),
a two-by-two confusion matrix (also called a contingency table) can be

ROC101.tex; 16/03/2004; 12:56; p.2

ROC graphs

3

True class
n

p

Hypothesized
class

Y

True

Positives

False

Positives

N

False

Negatives

True

Negatives

Column totals:

P

N

fp rate = F P
N

precision = T P

T P +F P

tp rate = T P
P

recall = T P
P

accuracy = T P +T N
P +N

F-measure =

2

1=precision+1=recall

Figure 1. Confusion matrix and common performance metrics calculated from it

constructed representing the dispositions of the set of instances. This
matrix forms the basis for many common metrics.

Figure 1 shows a confusion matrix and equations of several common
metrics that can be calculated from it. The numbers along the major
diagonal represent the correct decisions made, and the numbers o(cid:11)
this diagonal represent the errors|the confusion|between the various
classes. The true positive rate1 (also called hit rate and recall) of a
classi(cid:12)er is estimated as:

tp rate (cid:25)

Positives correctly classi(cid:12)ed

Total positives

1 For clarity, counts such as TP and FP will be denoted with upper-case letters

and rates such as tp rate will be denoted with lower-case.

ROC101.tex; 16/03/2004; 12:56; p.3

4

Tom Fawcett

D

A

B

C

E

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.8
False Positive rate

0.6

1.0

Figure 2. A basic ROC graph showing (cid:12)ve discrete classi(cid:12)ers.

The false positive rate (also called false alarm rate) of the classi-

(cid:12)er is:

fp rate (cid:25)

negatives incorrectly classi(cid:12)ed

total negatives

Additional terms associated with ROC curves are:

sensitivity = recall

speci(cid:12)city =

True negatives

False positives + True negatives

= 1 (cid:0) fp rate

positive predictive value = precision

2. ROC Space

ROC graphs are two-dimensional graphs in which TP rate is plotted
on the Y axis and FP rate is plotted on the X axis. An ROC graph
depicts relative trade-o(cid:11)s between bene(cid:12)ts (true positives) and costs
(false positives). Figure 2 shows an ROC graph with (cid:12)ve classi(cid:12)ers
labeled A through E.

A discrete classi(cid:12)er is one that outputs only a class label. Each
discrete classi(cid:12)er produces an (fp rate, tp rate) pair corresponding to
a single point in ROC space. The classi(cid:12)ers in (cid:12)gure 2 are all discrete
classi(cid:12)ers.

ROC101.tex; 16/03/2004; 12:56; p.4

ROC graphs

5

Several points in ROC space are important to note. The lower left
point (0; 0) represents the strategy of never issuing a positive classi(cid:12)ca-
tion; such a classi(cid:12)er commits no false positive errors but also gains no
true positives. The opposite strategy, of unconditionally issuing positive
classi(cid:12)cations, is represented by the upper right point (1; 1).

The point (0; 1) represents perfect classi(cid:12)cation. D’s performance is

perfect as shown.

Informally, one point in ROC space is better than another if it is
to the northwest (tp rate is higher, fp rate is lower, or both) of the
(cid:12)rst. Classi(cid:12)ers appearing on the left hand-side of an ROC graph, near
the X axis, may be thought of as \conservative": they make positive
classi(cid:12)cations only with strong evidence so they make few false positive
errors, but they often have low true positive rates as well. Classi(cid:12)ers
on the upper right-hand side of an ROC graph may be thought of as
\liberal": they make positive classi(cid:12)cations with weak evidence so they
classify nearly all positives correctly, but they often have high false
positive rates. In (cid:12)gure 2, A is more conservative than B. Many real
world domains are dominated by large numbers of negative instances,
so performance in the far left-hand side of the ROC graph becomes
more interesting.

2.1. Random Performance

The diagonal line y = x represents the strategy of randomly guessing
a class. For example, if a classi(cid:12)er randomly guesses the positive class
half the time, it can be expected to get half the positives and half the
negatives correct; this yields the point (0:5; 0:5) in ROC space. If it
guesses the positive class 90% of the time, it can be expected to get
90% of the positives correct but its false positive rate will increase to
90% as well, yielding (0:9; 0:9) in ROC space. Thus a random classi(cid:12)er
will produce a ROC point that \slides" back and forth on the diagonal
based on the frequency with which it guesses the positive class. In
order to get away from this diagonal into the upper triangular region,
the classi(cid:12)er must exploit some information in the data. In (cid:12)gure 2,
C’s performance is virtually random. At (0:7; 0:7), C may be said to be
guessing the positive class 70% of the time,

Any classi(cid:12)er that appears in the lower right triangle performs worse
than random guessing. This triangle is therefore usually empty in ROC
graphs. However, note that the decision space is symmetrical about the
diagonal separating the two triangles. If we negate a classi(cid:12)er|that is,
reverse its classi(cid:12)cation decisions on every instance|its true positive
classi(cid:12)cations become false negative mistakes, and its false positives
become true negatives. Therefore, any classi(cid:12)er that produces a point

ROC101.tex; 16/03/2004; 12:56; p.5

6

Tom Fawcett

in the lower right triangle can be negated to produce a point in the
upper left triangle. In (cid:12)gure 2, E performs much worse than random,
and is in fact the negation of B. Any classi(cid:12)er on the diagonal may
be said to have no information about the class. A classi(cid:12)er below the
diagonal may be said to have useful information, but it is applying the
information incorrectly (Flach and Wu, 2003).

Given an ROC graph in which a classi(cid:12)er’s performance appears
to be slightly better than random, it is natural to ask: \is this classi-
(cid:12)er’s performance truly signi(cid:12)cant or is it only better than random by
chance?" There is no conclusive test for this, but Forman (2002) has
shown a methodology that addresses this question with ROC curves.

3. Curves in ROC space

Many classi(cid:12)ers, such as decision trees or rule sets, are designed to pro-
duce only a class decision, i.e., a Y or N on each instance. When such
a discrete classi(cid:12)er is applied to a test set, it yields a single confusion
matrix, which in turn corresponds to one ROC point. Thus, a discrete
classi(cid:12)er produces only a single point in ROC space.

Some classi(cid:12)ers, such as a Naive Bayes classi(cid:12)er or a neural network,
naturally yield an instance probability or score, a numeric value that
represents the degree to which an instance is a member of a class. These
values can be strict probabilities, in which case they adhere to standard
theorems of probability; or they can be general, uncalibrated scores, in
which case the only property that holds is that a higher score indicates
a higher probability. We shall call both a probabilistic classi(cid:12)er, in spite
of the fact that the output may not be a proper probability2.

Such a ranking or scoring classi(cid:12)er can be used with a threshold to
produce a discrete (binary) classi(cid:12)er: if the classi(cid:12)er output is above the
threshold, the classi(cid:12)er produces a Y, else a N. Each threshold value
produces a di(cid:11)erent point in ROC space. Conceptually, we may imagine
varying a threshold from (cid:0)1 to +1 and tracing a curve through ROC
space. Algorithm 1 describes this basic idea. Computationally, this is a
poor way of generating an ROC curve, and the next section describes
a more e(cid:14)cient and careful method.

Figure 3 shows an example of an ROC \curve" on a test set of twenty
instances. The instances, ten positive and ten negative, are shown in
the table beside the graph. Any ROC curve generated from a (cid:12)nite set
of instances is actually a step function, which approaches a true curve
as the number of instances approaches in(cid:12)nity. The step function in

2 Techniques exist for converting an uncalibrated score into a proper probability

but this conversion is unnecesary for ROC curves.

ROC101.tex; 16/03/2004; 12:56; p.6

ROC graphs

7

.1

.30

.33

.34

.35

.37

.36

.38

.39

.4

.51

.505

.53

.52

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

.54

.55

.6

.7

.8

.9

0.1
Infinity
0

0

0.1

0.2

0.4

0.3
0.7
False positive rate

0.5

0.6

0.8

0.9

1

Inst# Class Score

Inst# Class Score

1
2
3
4
5
6
7
8
9
10

p
p
n
p
p
p
n
n
p
n

.9
.8
.7
.6
.55
.54
.53
.52
.51
.505

11
12
13
14
15
16
17
18
19
20

p
n
p
n
n
n
p
n
p
n

.4
.39
.38
.37
.36
.35
.34
.33
.30
.1

Figure 3. The ROC \curve" created by thresholding a test set. The table at right
shows twenty data and the score assigned to each by a scoring classi(cid:12)er. The graph
at left shows the corresponding ROC curve with each point labeled by the threshold
that produces it.

(cid:12)gure 3 is taken from a very small instance set so that each point’s
derivation can be understood. In the table of (cid:12)gure 3, the instances are
sorted by their scores, and each point in the ROC graph is labeled by
the score threshold that produces it. A threshold of +1 produces the
point (0; 0). As we lower the threshold to 0:9 the (cid:12)rst positive instance is
classi(cid:12)ed positive, yielding (0; 0:1). As the threshold is further reduced,
the curve climbs up and to the right, ending up at (1; 1) with a threshold

ROC101.tex; 16/03/2004; 12:56; p.7

8

Tom Fawcett

F P   0
T P   0
for i 2 L do

Algorithm 1 Conceptual method for calculating an ROC curve. See
algorithm 2 for a practical method.
Inputs: L, the set of test instances; f (i), the probabilistic classi(cid:12)er’s es-
timate that instance i is positive; min and max, the smallest and largest
values returned by f ; increment, the smallest di(cid:11)erence between any two f
values.
1: for t = min to max by increment do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: end

/* i is a negative example, so this is a false positive */

end if
end for
Add point ( F P

N ; T P

P ) to ROC curve

if f (i) (cid:21) t then

/* This example is over threshold */

if i is a positive example then

T P   T P + 1

F P   F P + 1

else

end if

of 0:1. Note that lowering this threshold corresponds to moving from
the \conservative" to the \liberal" areas of the graph.

Although the test set is very small, we can make some tentative
observations about the classi(cid:12)er. It appears to perform better in the
more conservative region of the graph; the ROC point at (0:1; 0:5)
produces its highest accuracy (70%). This is equivalent to saying that
the classi(cid:12)er is better at identifying likely positives than at identifying
likely negatives. Note also that the classi(cid:12)er’s best accuracy occurs at
a threshold of (cid:21) :54, rather than at (cid:21) :5 as we might expect with a
balanced distribution. The next section discusses this phenomenon.

3.1. Relative versus absolute scores

An important point about ROC graphs is that they measure the ability
of a classi(cid:12)er to produce good relative instance scores. A classi(cid:12)er need
not produce accurate, calibrated probability estimates; it need only
produce relative accurate scores that serve to discriminate positive and
negative instances.

Consider the simple instance scores shown in (cid:12)gure 4, which came
from a Naive Bayes classi(cid:12)er. Comparing the hypothesized class (which
is Y if score> 0:5, else N) against the true classes, we can see that
the classi(cid:12)er gets instances 7 and 8 wrong, yielding 80% accuracy.

ROC101.tex; 16/03/2004; 12:56; p.8

ROC graphs

9

1

0.8

0.6

0.4

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

Accuracy point (threshold = 0.5)

Accuracy point (threshold = 0.6)

0

0

0.2

0.4

0.6

False positive rate

0.8

1

Inst
no. True Hyp

Class

Score

1
2
3
4
5
6
7
8
9
10

p
p
p
p
p
p
n
n
n
n

Y
Y
Y
Y
Y
Y
Y
Y
N
N

0.99999
0.99999
0.99993
0.99986
0.99964
0.99955
0.68139
0.50961
0.48880
0.44951

Figure 4. Scores and classi(cid:12)cations of ten instances, and the resulting ROC curve.

However, consider the ROC curve on the left side of the (cid:12)gure. The
curve rises vertically from (0; 0) to (0; 1), then horizontally to (1; 1).
This indicates perfect classi(cid:12)cation performance on this test set. Why
is there a discrepancy?

The explanation lies in what each is measuring. The ROC curve
shows the ability of the classi(cid:12)er to rank the positive instances relative
to the negative instances, and it is indeed perfect in this ability. The
accuracy metric imposes a threshold (score> 0:5) and measures the re-
sulting classi(cid:12)cations with respect to the scores. The accuracy measure
would be appropriate if the scores were proper probabilities, but they
are not. Another way of saying this is that the scores are not properly
calibrated, as true probabilities are. In ROC space, the imposition of
a 0:5 threshold results in the performance designated by the circled
\accuracy point" in (cid:12)gure 4. This operating point is suboptimal. We
could use the training set to estimate a prior for p(p) = 6=10 = 0:6
and use this as a threshold, but it would still produce suboptimal
performance (90% accuracy).

One way to eliminate this phenomenon is to calibrate the classi(cid:12)er
scores. There are some methods for doing this (Zadrozny and Elkan,
2001). Another approach is to use an ROC method that chooses operat-
ing points based on their relative performance, and there are methods
for doing this as well (Provost and Fawcett, 1998; Provost and Fawcett,
2001). These latter methods are discussed brie(cid:13)y in section 7.1.

A consequence of relative scoring is that classi(cid:12)er scores should not
be compared across model classes. One model class may be designed

ROC101.tex; 16/03/2004; 12:56; p.9

10

Tom Fawcett

to produce scores in the range [0; 1] while another produces scores
in [(cid:0)1; +1] or [1; 100]. Comparing model performance at a common
threshold will be meaningless.

3.2. Class skew

ROC curves have an attractive property: they are insensitive to changes
in class distribution. If the proportion of positive to negative instances
changes in a test set, the ROC curves will not change. To see why this
is so, consider the confusion matrix in (cid:12)gure 1. Note that the class
distribution|the proportion of positive to negative instances|is the
relationship of the left (+) column to the right (-) column. Any per-
formance metric that uses values from both columns will be inherently
sensitive to class skews. Metrics such as accuracy, precision, lift and
F score use values from both columns of the confusion matrix. As a
class distribution changes these measures will change as well, even if
the fundamental classi(cid:12)er performance does not. ROC graphs are based
upon tp rate and fp rate, in which each dimension is a strict columnar
ratio, so do not depend on class distributions.

To some researchers, large class skews and large changes in class
distributions may seem contrived and unrealistic. However, class skews
of 101 and 102 are very common in real world domains, and skews up
to 106 have been observed in some domains (Clearwater and Stern,
1991; Fawcett and Provost, 1996; Kubat et al., 1998; Saitta and Neri,
1998). Substantial changes in class distributions are not unrealistic
either. For example, in medical decision making epidemics may cause
the incidence of a disease to increase over time. In fraud detection,
proportions of fraud varied signi(cid:12)cantly from month to month and
place to place (Fawcett and Provost, 1997). Changes in a manufacturing
practice may cause the proportion of defective units produced by a
manufacturing line to increase or decrease. In each of these examples
the prevalance of a class may change drastically without altering the
fundamental characteristic of the class, i.e., the target concept.

Precision and recall are common in information retrieval for evalu-
ating retrieval (classi(cid:12)cation) performance (Lewis, 1990; Lewis, 1991).
Precision-recall graphs are commonly used where static document sets
can sometimes be assumed; however, they are also used in dynamic
environments such as web page retrieval, where the number of pages
irrelevant to a query (N ) is many orders of magnitude greater than P
and probably increases steadily over time as web pages are created.

To see the e(cid:11)ect of class skew, consider the curves in (cid:12)gure 5, which
show two classi(cid:12)ers evaluated using ROC curves and precision-recall
curves. In 5a and b, the test set has a balanced 1:1 class distribution.

ROC101.tex; 16/03/2004; 12:56; p.10

ROC graphs

’insts.roc.+’
’insts2.roc.+’

 0.2

 0.4

 0.6

 0.8

 1

(a) ROC curves, 1:1

’instsx10.roc.+’
’insts2x10.roc.+’

 0.2

 0.4

 0.6

 0.8

 1

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

11

’insts.precall.+’
’insts2.precall.+’

 0.2

 0.4

 0.6

 0.8

 1

(b) Precision-recall curves, 1:1

’instsx10.precall.+’
’insts2x10.precall.+’

 0.2

 0.4

 0.6

 0.8

 1

(c) ROC curves, 1:10

(d) Precision-recall curves, 1:10

Figure 5. ROC and precision-recall curves under class skew.

Graphs 5c and d show the same two classi(cid:12)ers on the same domain,
but the number of negative instances has been increased ten-fold. Note
that the classi(cid:12)ers and the underlying concept has not changed; only
the class distribution is di(cid:11)erent. Observe that the ROC graphs in 5a
and 5c are identical, while the precision-recall graphs in 5b and 5d
di(cid:11)er dramatically. In some cases, the conclusion of which classi(cid:12)er has
superior performance can change with a shifted distribution.

3.3. Creating scoring classifiers

Many classi(cid:12)er models are discrete: they are designed to produce only a
class label from each test instance. However, we often want to generate

ROC101.tex; 16/03/2004; 12:56; p.11

12

Tom Fawcett

a full ROC curve from a classi(cid:12)er instead of just a single point. To this
end we want to generate scores from a classi(cid:12)er rather than just a class
label. There are several ways of producing such scores.

Many discrete classi(cid:12)er models may easily be converted to scoring
classi(cid:12)ers by \looking inside" them at the instance statistics they keep.
For example, a decision tree determines a class label of a leaf node from
the proportion of instances at the node; the class decision is simply
the most prevalent class. These class proportions may serve as a score
(Provost and Domingos, 2001). Appendix A gives a basic algorithm for
generating an ROC curve directly from a decision tree. A rule learner
keeps similar statistics on rule con(cid:12)dence, and the con(cid:12)dence of a rule
matching an instance can be used as a score (Fawcett, 2001).

Even if a classi(cid:12)er only produces a class label, an aggregation of
them may be used to generate a score. MetaCost (Domingos, 1999)
employs bagging to generate an ensemble of discrete classi(cid:12)ers, each of
which produces a vote. The set of votes could be used to generate a
score3.

Finally, some combination of scoring and voting can be employed.
For example, rules can provide basic probability estimates, which may
then be used in weighted voting (Fawcett, 2001).

4. E(cid:14)cient generation of ROC curves

Given a test set, we often want to generate an ROC curve e(cid:14)ciently
from it. Although some researchers have employed methods like al-
gorithm 1, this method is neither e(cid:14)cient nor practical: it requires
knowing max, min and increment, which must be estimated from the
test set and f values. It involves two nested loops; because the outer
loop must increment t at least n times, the complexity is O(n2) in the
number of test set instances.

A much better algorithm can be created by exploiting the mono-
tonicity of thresholded classi(cid:12)cations: any instance that is classi(cid:12)ed
positive with respect to a given threshold will be classi(cid:12)ed positive
for all lower thresholds as well. Therefore, we can simply sort the test
instances decreasing by f scores and move down the list, processing
one instance at a time and updating T P and F P as we go. In this way
an ROC graph can be created from a linear scan.

3 MetaCost actually works in the opposite direction because its goal is to generate
a discrete classi(cid:12)er. It (cid:12)rst creates a probabilistic classi(cid:12)er, then applies knowledge
of the error costs and class skews to relabel the instances so as to \optimize" their
classi(cid:12)cations. Finally, it learns a speci(cid:12)c discrete classi(cid:12)er from this new instance
set. Thus, MetaCost is not a good method for creating a good scoring classi(cid:12)er,
though its bagging method may be.

ROC101.tex; 16/03/2004; 12:56; p.12

ROC graphs

13

Algorithm 2 E(cid:14)cient method for generating ROC points
Inputs: L, the set of test examples; f (i), the probabilistic classi(cid:12)er’s estimate
that example i is positive; P and N , the number of positive and negative
examples.
Outputs: R, a list of ROC points increasing by fp rate.
Require: P > 0 and N > 0
1: Lsorted   L sorted decreasing by f scores
2: F P   T P   0
3: R   hi
4: fprev   (cid:0)1
5: i   1
6: while i (cid:20) jLsortedj do
if f (i) 6= fprev then
7:
8:
9:
10:
11:
12:
13:
14:
end if
15:
i   i + 1
16:
17: end while
18: push (cid:0) F P
19: end

end if
if Lsorted[i] is a positive example then

N ; T P
push (cid:0) F P
fprev   f (i)

N ; T P

P (cid:1) onto R

/* i is a negative example */

/* This is (1,1) */

P (cid:1) onto R

T P   T P + 1

else

F P   F P + 1

The new algorithm is shown in algorithm 2. T P and F P both start
at zero. For each positive instance we increment T P and for every
negative instance we increment F P . We maintain a stack R of ROC
points, pushing a new point onto R after each instance is processed.
The (cid:12)nal output is the stack R, which will contain points on the ROC
curve.

Let n be the number of points in the test set. This algorithm requires
an O(n log n) sort followed by an O(n) scan down the list, resulting in
O(n log n) total complexity.

4.1. Equally scored instances

Statements 7{10 need some explanation. These are necessary in order
to correctly handle sequences of equally scored instances. Consider the
ROC curve shown in (cid:12)gure 6. Assume we have a test set in which
there is a sequence of instances, four negatives and six positives, all
scored equally by f . The sort in line 1 of algorithm 2 does not impose
any speci(cid:12)c ordering on these instances since their f scores are equal.

ROC101.tex; 16/03/2004; 12:56; p.13

14

Tom Fawcett

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

Optimistic

0

0.2

Expected

Pessimistic

0.4

0.8
False Positive rate

0.6

1.0

Figure 6. The optimistic, pessimistic and expected ROC segments resulting from a
sequence of ten equally scored instances.

What happens when we create an ROC curve? In one extreme case, all
the positives end up at the beginning of the sequence and we generate
the \optimistic" upper L segment shown in (cid:12)gure 6. In the opposite
extreme, all the negatives end up at the beginning of the sequence
and we get the \pessimistic" lower L shown in (cid:12)gure 6. Any mixed
ordering of the instances will give a di(cid:11)erent set of step segments
within the rectangle formed by these two extremes. However, the ROC
curve should represent the expected performance of the classi(cid:12)er, which,
lacking any other information, is the average of the pessimistic and
optimistic segments. This average is the diagonal of the rectangle, and
can be created in the ROC curve algorithm by not emitting an ROC
point until all instances of equal f values have been processed. This is
what the fprev variable and the if statement of line 7 accomplish.

Instances that are scored equally may seem unusual but with some
classi(cid:12)er models they are common. For example, if we use instance
counts at nodes in a decision tree to score instances, a large, high-
entropy leaf node may produce many equally scored instances of both
classes. If such instances are not averaged, the resulting ROC curves will
be sensitive to the test set ordering, and di(cid:11)erent orderings can yield
very misleading curves. This can be especially critical in calculating the
area under an ROC curve, discussed in section 5. Consider a decision
tree containing a leaf node accounting for n positives and m negatives.

ROC101.tex; 16/03/2004; 12:56; p.14

ROC graphs

15

B

A

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.8
False Positive rate

0.6

B

A

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

1.0

0

0.2

0.4

0.8
False Positive rate

0.6

1.0

Figure 7. Two ROC graphs. The graph on the left shows the area under two ROC
curves. The graph on the right shows the area under the curves of a discrete classi(cid:12)er
(A) and a probabilistic classi(cid:12)er (B).

Every instance that is classi(cid:12)ed to this leaf node will be assigned the
same score. The rectangle of (cid:12)gure 6 will be of size nm
P N , and if these
instances are not averaged this one leaf may account for errors in ROC
curve area as high as nm

2P N .

5. Area under an ROC Curve (AUC)

An ROC curve is a two-dimensional depiction of classi(cid:12)er perfor-
mance. To compare classi(cid:12)ers we may want to reduce ROC performance
to a single scalar value representing expected performance. A common
method is to calculate the area under the ROC curve, abbreviated
AUC (Bradley, 1997; Hanley and McNeil, 1982). Since the AUC is a
portion of the area of the unit square, its value will always be between 0
and 1.0. However, because random guessing produces the diagonal line
between (0; 0) and (1; 1), which has an area of 0.5, no realistic classi(cid:12)er
should have an AUC less than 0.5.

The AUC has an important statistical property: the AUC of a
classi(cid:12)er is equivalent to the probability that the classi(cid:12)er will rank
a randomly chosen positive instance higher than a randomly chosen
negative instance. This is equivalent to the Wilcoxon test of ranks
(Hanley and McNeil, 1982). The AUC is also closely related to the
Gini coe(cid:14)cient (Breiman et al., 1984), which is twice the area between
the diagonal and the ROC curve. Hand and Till (2001) point out that
Gini + 1 = 2 (cid:2) AUC.

Figure 7a shows the areas under two ROC curves, A and B. Classi(cid:12)er
B has greater area and therefore better average performance. Figure 7b

ROC101.tex; 16/03/2004; 12:56; p.15

16

Tom Fawcett

Algorithm 3 Calculating the area under an ROC curve
Inputs: L, the set of test examples; f (i), the probabilistic classi(cid:12)er’s estimate
that example i is positive; P and N , the number of positive and negative
examples.
Outputs: A, the area under the ROC curve.
Require: P > 0 and N > 0
1: Lsorted   L sorted decreasing by f scores
2: F P   T P   0
3: F Pprev   T Pprev   0
4: A   0
5: fprev   (cid:0)1
6: i   1
7: while i (cid:20) jLsortedj do
if f (i) 6= fprev then
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end while
20: A   A + trap area(1; F Pprev; 1; T Pprev)
21: A   A=(P (cid:2) N )
22: end
1: function trapezoid area(X1; X2; Y 1; Y 2)
2: Base   jX1 (cid:0) X2j
3: Heightavg   (Y 1 + Y 2)=2
4: return Base (cid:2) Heightavg
5: end function

A   A + trapezoid area(F P; F Pprev; T P; T Pprev)
fprev   f (i)
F Pprev   F P
T Pprev   T P

end if
if i is a positive example then

/* scale from P (cid:2) N onto the unit square */

/* i is a negative example */

T P   T P + 1

F P   F P + 1

else

end if

shows the area under the curve of a binary classi(cid:12)er A and a scoring
classi(cid:12)er B. Classi(cid:12)er A represents the performance of B when B is
used with a single, (cid:12)xed threshold. Though the performance of the two
is equal at the (cid:12)xed point (A’s threshold), A’s performance becomes
inferior to B further from this point.

It is possible for a high-AUC classi(cid:12)er to perform worse in a spe-
ci(cid:12)c region of ROC space than a low-AUC classi(cid:12)er. Figure 7a shows
an example of this: classi(cid:12)er B is generally better than A except at
FPrate > 0:6 where A has a slight advantage. But in practice the

ROC101.tex; 16/03/2004; 12:56; p.16

ROC graphs

17

AUC performs very well and is often used when a general measure of
predictiveness is desired.

The AUC may be computed easily using a small modi(cid:12)cation of
algorithm 2, shown in algorithm 3. Instead of collecting ROC points, the
algorithm adds successive areas of trapezoids to A. Finally, it divides
A by the total possible area to scale the value to the unit square.

6. Averaging ROC curves

Although ROC curves may be used to evaluate classi(cid:12)ers, care should
be taken when using them to make conclusions about classi(cid:12)er superi-
ority. Some researchers have assumed that an ROC graph may be used
to select the best classi(cid:12)ers simply by graphing them in ROC space
and seeing which ones dominate. This is misleading; it is analogous to
taking the maximum of a set of accuracy (cid:12)gures from a single test set.
Without a measure of variance we cannot compare the classi(cid:12)ers.

Averaging ROC curves is easy if the original instances are available.
Given test sets T1; T2; (cid:1) (cid:1) (cid:1) ; Tn, generated from cross-validation or the
bootstrap method, we can simply merge sort the instances together
by their assigned scores4 into one large test set TM . We then run an
ROC curve generation algorithm such as algorithm 2 on TM and plot
the result. However, the primary reason for using multiple test sets is
to derive a measure of variance, which this simple merging does not
provide. We need a more sophisticated method that samples individual
curves at di(cid:11)erent points and averages the samples.

ROC space is two-dimensional, and any average is necessarily one-
dimensional. ROC curves can be projected onto a single dimension
and averaged conventionally, but this leads to the question of whether
the projection is appropriate, or more precisely, whether it preserves
characteristics of interest. The answer depends upon the reason for
averaging the curves. This section presents two methods for averaging
ROC curves: vertical and threshold averaging.

Figure 8a shows (cid:12)ve ROC curves to be averaged. Each contains a
thousand points and has some concavities. Figure 8b shows the curve
formed by merging the (cid:12)ve test sets and computing their combined
ROC curve. Figures 8c and 8d show average curves formed by sampling
the (cid:12)ve individual ROC curves. The error bars are 95% con(cid:12)dence
intervals.

4 This assumes that the scores generated by the models are comparable. If
the same learning algorithm is being used, and the training and testing sets are
representative samples of the population, the scores should be comparable.

ROC101.tex; 16/03/2004; 12:56; p.17

18

1

e
t
a
r
 
e
v
i
t
i
s
o
p

 
e
u
r
T

0.8

0.6

0.4

0.2

0

0

0.2

Tom Fawcett

1

0.8

0.6

0.4

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p

 
e
u
r
T

0.8

1

0

0

0.2

0.4

0.6

False positive rate

0.8

1

0.4

0.6

False positive rate

(a) ROC curves of (cid:12)ve instance sam-
ples

(b) ROC curve formed by merging the
(cid:12)ve samples

1

0.8

0.6

0.4

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

0

0

0.2

1

0.8

0.6

0.4

0.2

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
T

0.8

1

0

0

0.2

0.4

0.6

False positive rate

0.8

1

0.4

0.6

False positive rate

(c) The curves of a averaged vertically

(d) The curves of a averaged by
threshold

Figure 8. ROC curve averaging

6.1. Vertical averaging

Vertical averaging takes vertical samples of the ROC curves for (cid:12)xed
FP rates and averages the corresponding TP rates. Such averaging is
appropriate when the FP rate can indeed be (cid:12)xed by the researcher,
or when a single-dimensional measure of variation is desired. Provost,

ROC101.tex; 16/03/2004; 12:56; p.18

ROC graphs

19

tprsum   0
for i = 1 to nrocs do

tprsum   tprsum + tpr for fpr(f prsample; ROCS[i]; npts[i])

end for
tpravg[s]   tprsum=i
s   s + 1

Algorithm 4 Vertical averaging of ROC curves.
Inputs: samples, the number of FP samples; nrocs, the number of ROC
curves to be sampled, ROCS[nrocs], an array of nrocs ROC curves; npts[m],
the number of points in ROC curve m. Each ROC point is a structure of two
members, fpr and tpr.
Output: Array tpravg, containing the vertical averages.
1: s   1
2: for f prsample = 0 to 1 by 1=samples do
3:
4:
5:
6:
7:
8:
9: end for
10: end
1: function tpr for fpr(f prsample; ROC; npts)
2: i   1
3: while i < npts and ROC[i + 1]:f pr (cid:20) f prsample do
i   i + 1
4:
5: end while
6: if ROC[i]:f pr = f prsample then
7:
8: else
9:
10: end if
11: end function
1: function interpolate(ROCP 1; ROCP 2; X)
2: slope = (ROCP 2:tpr (cid:0) ROCP 1:tpr)=(ROCP 2:f pr (cid:0) ROCP 1:f pr)
3: return ROCP 1:tpr + slope (cid:1) (X (cid:0) ROCP 1:f pr)
4: end function

return ROC[i]:tpr

return interpolate(ROC[i]; ROC[i + 1]; f prsample)

Fawcett and Kohavi (1998) used this method in their work of averaging
ROC curves of a classi(cid:12)er for k-fold cross-validation.

In this method each ROC curve is treated as a function, Ri, such that
tp rate = Ri(fp rate). This is done by choosing the maximum tp rate
for each fp rate and interpolating between points when necessary. The
averaged ROC curve is the function ^R(fp rate) = mean[Ri(fp rate)].
To plot an average ROC curve we can sample from ^R at points reg-
ularly spaced along the fp rate-axis. Con(cid:12)dence intervals of the mean
of tp rateare computed using the common assumption of a binomial
distribution.

Algorithm 4 computes this vertical average of a set of ROC points.

It leaves the means in the array T P avg.

ROC101.tex; 16/03/2004; 12:56; p.19

20

Tom Fawcett

Several extensions have been left out of this algorithm for clarity.
The algorithm may easily be extended to compute standard deviations
of the samples in order to draw con(cid:12)dence bars. Also, the function
tp for fp may be optimized somwhat. Because it is only called on
monotonically increasing values of F P , it need not scan each ROC
array from the beginning every time; it could keep a record of the last
point seen and initialize i from this array.

Figure 8c shows the vertical average of the (cid:12)ve curves in (cid:12)gure 8a.
The vertical bars on the curve show the 95% con(cid:12)dence region of the
ROC mean. For this average curve, the curves were sampled at FP
rates from 0 through 1 by 0.1. It is possible to sample curves much
more (cid:12)nely but the con(cid:12)dence bars may become di(cid:14)cult to read.

6.2. Threshold averaging

Vertical averaging has the advantage that averages are made of
a single dependent variable, the true positive rate, which simpli(cid:12)es
computing con(cid:12)dence intervals. However, Holte (2002) has pointed out
that the independent variable, false positive rate, is often not under the
direct control of the researcher. It may be preferable to average ROC
points using an independent variable whose value can be controlled
directly, such as the threshold on the classi(cid:12)er scores.

Threshold averaging accomplishes this. Instead of sampling points
based on their positions in ROC space, as vertical averaging does,
it samples based on the thresholds that produced these points. The
method must generate a set of thresholds to sample, then for each
threshold it (cid:12)nds the corresponding point of each ROC curve and
averages them.

Algorithm 5 shows the basic method for doing this. It generates an
array T of classi(cid:12)er scores which are sorted from largest to smallest
and used as the set of thresholds. These thresholds are sampled at
(cid:12)xed intervals determined by samples, the number of samples desired.
For a given threshold, the algorithm selects from each ROC curve the
the point of greatest score less than or equal to the threshold.5 These
points are then averaged separately along their X and Y axes, with the
center point returned in the Avg array.

Figure 8d shows the result of averaging the (cid:12)ve curves of 8a by
thresholds. The resulting curve has average points and con(cid:12)dence bars
in the X and Y directions. The bars shown are at the 95% con(cid:12)dence
level.

5 We assume the ROC points have been generated by an algorithm like 2 that

deals correctly with equally scored instances.

ROC101.tex; 16/03/2004; 12:56; p.20

ROC graphs

21

Algorithm 5 Threshold averaging of ROC curves.
Inputs: samples, the number of threshold samples; nrocs, the number of
ROC curves to be sampled; ROCS[nrocs], an array of nrocs ROC curves
sorted by score; npts[m], the number of points in ROC curve m. Each ROC
point is a structure of three members, fpr, tpr and score.
Output: Avg, an array of (X,Y) points constituting the average ROC
curve.
Require: samples > 1
1: initialize array T to contain all scores of all ROC points
2: sort T in descending order
3: s   1
4: for tidx = 1 to length(T ) by int(length(T )=samples) do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: end
1: function roc point at threshold(ROC; npts; thresh)
2: i   1
3: while i (cid:20) npts and ROC[i]:score > thresh do
i   i + 1
4:
5: end while
6: return ROC[i]
7: end function

p   roc point at threshold(ROCS[i]; npts[i]; T [tidx])
f prsum   f prsum + p:f pr
tprsum   tprsum + p:tpr

f prsum   0
tprsum   0
for i = 1 to nrocs do

end for
Avg[s]   (f prsum=i ; tprsum=i)
s   s + 1

There are some minor limitations of threshold averaging with re-
spect to vertical averaging. To perform threshold averaging we need
the classi(cid:12)er score assigned to each point. Also, section 3.1 pointed
out that classi(cid:12)er scores should not be compared across model classes.
Because of this, ROC curves averaged from di(cid:11)erent model classes may
be misleading because the scores may be incommensurate.

Finally, Macskassy and Provost (2004) have investigated di(cid:11)erent
techniques for generating con(cid:12)dence bands for ROC curves. They inves-
tigate con(cid:12)dence intervals from vertical and threshold averaging, as well
as three methods from the medical (cid:12)eld for generating bands (simul-
taneous join con(cid:12)dence regions, Working-Hotelling based bands, and
(cid:12)xed-width con(cid:12)dence bands). The reader is referred to their paper for

ROC101.tex; 16/03/2004; 12:56; p.21

22

Tom Fawcett

a much more detailed discussion of the techniques, their assumptions,
and empirical studies.

7. Additional Topics

The previous sections are intended to be self-contained and to cover
the basic issues that arise in using ROC curves in machine learning
research. This section discusses additional, slightly more esoteric topics.

7.1. The ROC convex hull

One advantage of ROC graphs is that they enable visualizing and
organizing classi(cid:12)er performance without regard to class distributions
or error costs. This ability becomes very important when investigat-
ing learning with skewed distributions or cost-sensitive learning. A
researcher can graph the performance of a set of classi(cid:12)ers, and that
graph will remain invariant with respect to the operating conditions
(class skew and error costs). As these conditions change, the region of
interest may change, but the graph itself will not.

Provost and Fawcett (1998; 2001) show that a set of operating con-
ditions may be transformed easily into a so-called iso-performance line
in ROC space. Two points in ROC space, (F P1,T P1) and (F P2,T P2),
have the same performance if

T P2 (cid:0) T P1
F P2 (cid:0) F P1

=

c(Y; n)p(n)
c(N; p)p(p)

= m

This equation de(cid:12)nes the slope of an iso-performance line. All clas-
si(cid:12)ers corresponding to points on a line of slope m have the same
expected cost. Each set of class and cost distributions de(cid:12)nes a family
of iso-performance lines. Lines \more northwest" (having a larger T P -
intercept) are better because they correspond to classi(cid:12)ers with lower
expected cost.

The details are beyond the scope of this article, but more generally
a classi(cid:12)er is potentially optimal if and only if it lies on the convex
hull (Barber et al., 1993) of the set of points in ROC space. We call
the convex hull of the set of points in ROC space the ROC convex hull
(ROCCH) of the corresponding set of classi(cid:12)ers.

This ROCCH formulation has a number of useful implications. Since
only the classi(cid:12)ers on the convex hull are potentially optimal, no others
need be retained. The operating conditions of the classi(cid:12)er may be
translated into an iso-performance line, which in turn may be used to
identify a portion of the ROCCH. As conditions change, the hull itself
does not change; only the portion of interest will.

ROC101.tex; 16/03/2004; 12:56; p.22

ROC graphs

23

fraudulent

legitimate

fraudulent

legitimate

refuse
approve

$20
(cid:0)x

(a)

(cid:0)$20
0:02x

refuse
approve

0

0

$20 + x

0:02x + $20

(b)

Figure 9. Matrices for the credit approval domain. (a) original bene(cid:12)t matrix,
(b) transformed cost-bene(cid:12)t matrix

7.2. Example-specific costs

In some domains the cost of a particular kind of error is not constant
throughout the population, but varies by example. Consider a simple
credit card transaction domain used by Elkan (2001) in which the task
is to decide whether to approve or refuse a given transaction. Elkan
describes a bene(cid:12)t matrix for the task, shown in (cid:12)gure 9a. This cost
matrix is justi(cid:12)ed with the following explanation. A refused fraudulent
transaction has a bene(cid:12)t of $20 because it may prevent future fraud. Re-
fusing a legitimate transaction has a negative bene(cid:12)t because it annoys
a customer. Approving a fraudulent transaction has a negative bene(cid:12)t
proporational to the transaction amount (x). Approving a legitimate
transaction generates a small amount of income proportional to the
transaction amount (0:02x).

ROC graphs have been criticized because of their inability to handle
example-speci(cid:12)c costs. In the traditional formulation this is correct
because the axes graph rates that are based on simple counts of TP
and FP examples, which is in turn based on the assumption that all
true positives are equivalent and all false positives are equivalent.

However, with a straightforward transformation we can show how
ROC graphs may be used with example-speci(cid:12)c costs. For this domain
we assume that a Y decision corresponds to approving a transaction,
and N means denying it. To use the matrix for an ROC graph we
transform it into a cost-bene(cid:12)t matrix where the costs are relative only
to Y (approve) decisions. First we subtract the (cid:12)rst row from both rows
in the matrix. Conceptually this matrix now corresponds to a baseline
situation where all transactions are refused, so all fraud is denied and
all legitimate customers are annoyed. We then negate the approve-
fraudulent cell to turn it into a cost. This yields the cost-bene(cid:12)t matrix
of (cid:12)gure 9b which forms the de(cid:12)nition of the cost function c(Y; p; x)
and c(Y; n; x).

In standard ROC graphs the x axis represents the fraction of total
FP mistakes possible. In the example-speci(cid:12)c cost formulation it will
represent the fraction of total FP cost possible, so the denominator will

ROC101.tex; 16/03/2004; 12:56; p.23

24

Tom Fawcett

P total   P total + c(Y; p; x)

if x is a positive example then

Algorithm 6 Generating ROC points from an dataset with example-
speci(cid:12)c costs
Inputs: L, the set of test examples; f (i), the probabilistic classi(cid:12)er’s estimate
that example i is positive; P and N , the number of positive and negative
examples; c(Y; class; i), the cost of judging instance i of class class to be Y.
Outputs: R, a list of ROC points increasing by fp rate.
Require: P > 0 and N > 0
1: for x 2 L do
2:
3:
4:
5:
end if
6:
7: end for
8: Lsorted   L sorted decreasing by f scores
9: F P cost   0
10: T P benef it   0
11: R   hi
12: fprev   (cid:0)1
13: i   1
14: while i (cid:20) jLsortedj do
if f (i) 6= fprev then
15:

N total   N total + c(Y; n; x)

else

N total ; T P benef it

P total (cid:17) onto R

push (cid:16) F P cost
fprev   f (i)

end if
if Lsorted[i] is a positive example then

T P benef it   T P benef it + c(Y; p; Lsorted[i])

F P cost   F P cost + c(Y; n; L sorted[i])

/* i is a negative example */

16:

else

17:
18:
19:
20:
21:
22:
end if
23:
i   i + 1
24:
25: end while
26: push (cid:16) F P cost
27: end

N

; T P benef it

P

(cid:17) onto R

/* This is (1,1) */

now be

X

x2f raudulent

$20 + x

Similarly the y axis will be the fraction of total TP bene(cid:12)ts so its
denominator will be

X

x2legitimate

0:02x + $20

ROC101.tex; 16/03/2004; 12:56; p.24

ROC graphs

25

Instead of incrementing T P and F P instance counts, as in algorithm 2,
we increment T P benef it and F P cost by the cost (bene(cid:12)t) of each
negative (positive) instance as it is processed. The ROC points are
the fractions of total bene(cid:12)ts and costs, respectively. Conceptually this
transformation corresponds to replicating instances in the instance set
in proportion to their cost, though this transformation has the advan-
tage that no actual replication is performed and non-integer costs are
easily accommodated. The (cid:12)nal algorithm is shown in algorithm 6.

It is important to mention two caveats in adopting this transforma-
tion. First, while example costs may vary, ROC analysis requires that
costs always be negative and bene(cid:12)ts always be positive. For example,
if we de(cid:12)ned c(Y; p; x) = x (cid:0) $20, with example x values ranging in
[0; 40], this could be violated. Second, incorporating error costs into
the ROC graph in this way introduces an additional assumption. Tra-
ditional ROC graphs assume that the fp rate and tp rate metrics of
the test population will be similar to those of the training population;
in particular that a classi(cid:12)er’s performance on random samples will be
similar. This new formulation adds the assumption that the example
costs will be similar as well; in other words, not only will the classi(cid:12)er
continue to score instances similarly between the training and testing
sets, but the costs and bene(cid:12)ts of those instances will be similar between
the sets too.

7.3. Decision problems with more than two classes

Discussions up to this point have dealt with only two classes, and
much of the ROC literature maintains this assumption. ROC analysis
is commonly employed in medical decision making in which two-class
diagnostic problems|presence or absence of an abnormal condition|
are common. The two axes represent tradeo(cid:11)s between errors (false
positives) and bene(cid:12)ts (true positives) that a classi(cid:12)er makes between
two classes. Much of the analysis is straightforward because of the sym-
metry that exists in the two-class problem. The resulting performance
can be graphed in two dimensions, which is easy to visualize.

7.3.1. Multi-class ROC graphs
With more than two classes the situation becomes much more complex
if the entire space is to be managed. With n classes the confusion
matrix becomes an n (cid:2) n matrix containing the n correct classi(cid:12)cations
(the major diagonal entries) and n2 (cid:0)n possible errors (the o(cid:11)-diagonal
entries). Instead of managing trade-o(cid:11)s between TP and FP, we have n
bene(cid:12)ts and n2 (cid:0) n errors. With only three classes, the surface becomes
a 32 (cid:0) 3 = 6-dimensional polytope. Lane (2000) has written a short

ROC101.tex; 16/03/2004; 12:56; p.25

26

Tom Fawcett

paper outlining the issues involved and the prospects for addressing
them. Srinivasan (1999) has shown that the analysis behind the ROC
convex hull extends to multiple classes and multi-dimensional convex
hulls.

One method for handling n classes is to produce n di(cid:11)erent ROC
graphs, one for each class. Called this the class reference formulation.
Speci(cid:12)cally, if C is the set of all classes, ROC graph i plots the clas-
si(cid:12)cation performance using class ci as the positive class and all other
classes as the negative class, i.e.,

Pi = ci
Ni = [

j6=i

cj 2 C

(1)

(2)

While this is a convenient formulation, it compromises one of the
attractions of ROC graphs, namely that they are insensitive to class
skew (see section 3.2). Because each Ni comprises the union of n (cid:0) 1
classes, changes in prevalence within these classes may alter the ci’s
ROC graph. For example, assume that some class ck 2 N is particularly
easy to identify. A classi(cid:12)er for class ci; i 6= k may exploit some charac-
teristic of ck in order to produce low scores for ck instances. Increasing
the prevalence of ck might alter the performance of the classi(cid:12)er, and
would be tantamount to changing the target concept by increasing the
prevalence of one of its disjuncts. This in turn would alter the ROC
curve. However, with this caveat, this method can work well in practice
and provide reasonable (cid:13)exibility in evaluation.

7.3.2. Multi-class AUC
The AUC is a measure of the discriminability of a pair of classes. In a
two-class problem, the AUC is a single scalar value, but a multi-class
problem introduces the issue of combining multiple pairwise discrim-
inability values. The reader is referred to Hand and Till’s (2001) article
for an excellent discussion of these issues.

One approach to calculating multi-class AUCs was taken by Provost
and Domingos (2001) in their work on probability estimation trees.
They calculated AUCs for multi-class problems by generating each class
reference ROC curve in turn, measuring the area under the curve, then
summing the AUCs weighted by the reference class’s prevalence in the
data. More precisely, they de(cid:12)ne:

AU Ctotal = X

ci2C

AU C(ci) (cid:1) p(ci)

ROC101.tex; 16/03/2004; 12:56; p.26

ROC graphs

27

where AU C(ci) is the area under the class reference ROC curve for ci,
as in equations 2. This de(cid:12)nition requires only jCj AUC calculations,
so its overall complexity is O(jCjn log n).

The advantage of Provost and Domingos’s AUC formulation is that
AU Ctotal is generated directly from class reference ROC curves, and
these curves can be generated and visualized easily. The disadvantage
is that the class reference ROC is sensitive to class distributions and
error costs, so this formulation of AU Ctotal is as well.

Hand and Till (2001) take a di(cid:11)erent approach in their derivation
of a multi-class generalization of the AUC. They desired a measure
that is insensitive to class distribution and error costs. The derivation
is too detailed to summarize here, but it is based upon the fact that
the AUC is equivalent to the probability that the classi(cid:12)er will rank
a randomly chosen positive instance higher than a randomly chosen
negative instance. From this probabilistic form, they derive a formula-
tion that measures the unweighted pairwise discriminability of classes.
Their measure, which they call M, is equivalent to:

AU Ctotal =

2

jCj(jCj (cid:0) 1) X

fci;cj g2C

AU C(ci; cj)

where n is the number of classes and AU C(ci; cj) is the area under
the two-class ROC curve involving classes ci and cj. The summation is
calculated over all pairs of distinct classes, irrespective of order. There
are jCj(jCj (cid:0) 1)=2 such pairs, so the time complexity of their measure
is O(jCj2 n log n). While Hand and Till’s formulation is well justi(cid:12)ed
and is insensitive to changes in class distribution, there is no easy way
to visualize the surface whose area is being calculated.

7.4. Combining classifiers

While ROC curves are commonly used for visualizing and evaluating
individual classi(cid:12)ers, ROC space can also be used to estimate the
performance of combinations of classi(cid:12)ers.

7.4.1. Interpolating classi(cid:12)ers
Sometimes the performance desired of a classi(cid:12)er is not exactly pro-
duced by any available classi(cid:12)er, but lies between two available clas-
si(cid:12)ers. The desired performance can be obtained by sampling the de-
cisions of each classi(cid:12)er. The sampling ratio will determine where the
resulting classi(cid:12)cation performance lies.

For a concrete example, consider the decision problem of the CoIL
Challenge 2000 (van der Putten and van Someren, 2000). In this chal-
lenge there is a set of 4000 clients to whom we wish to market a new

ROC101.tex; 16/03/2004; 12:56; p.27

28

Tom Fawcett

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

constraint line: 
TPr * 240 + FPr * 3760 = 800

B

C

}k

A

0

0.05

0.1

0.2
False Positive rate

0.15

0.25

0.3

Figure 10. Interpolating classi(cid:12)ers

insurance policy. Our budget dictates that we can a(cid:11)ord to market to
only 800 of them, so we want to select the 800 who are most likely
to respond to the o(cid:11)er. The expected class prior of responders is 6%,
so within the population of 4000 we expect to have 240 responders
(positives) and 3760 non-responders (negatives).

Assume we have generated two classi(cid:12)ers, A and B, which score
clients by the probability they will buy the policy. In ROC space A
lies at (:1; :2) and B lies at (:25; :6), as shown in (cid:12)gure 10. We want
to market to exactly 800 people so our solution constraint is fp rate (cid:2)
3760 + tp rate (cid:2) 240 = 800. If we use A we expect :1 (cid:2) 3760 + :2 (cid:2) 240 =
424 candidates, which is too few. If we use B we expect :25 (cid:2) 3760 +
:6 (cid:2) 240 = 1084 candidates, which is too many. We want a classi(cid:12)er
between A and B.

The solution constraint is shown as a dashed line in (cid:12)gure 10. It
intersects the line between A and B at C, approximately (:18; :42). A
classi(cid:12)er at point C would give the performance we desire and we can
achieve it using linear interpolation. Calculate k as the proportional
distance that C lies on the line between A and B:

k =

0:18 (cid:0) 0:1
0:25 (cid:0) 0:1

(cid:25) 0:53

Therefore, if we sample B’s decisions at a rate of .53 and A’s decisions
at a rate of 1 (cid:0) :53 = :47 we should attain C’s performance. In practice
this fractional sampling can be done by randomly sampling decisions

ROC101.tex; 16/03/2004; 12:56; p.28

ROC graphs

29

C

B

A

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

C

B’

B

A

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.8
False Positive rate

0.6

1.0

0

0.2

0.4

0.8
False Positive rate

0.6

1.0

(a)

(b)

Figure 11. Removing concavities

from each: for each instance, generate a random number between zero
and one. If the random number is greater than k, apply classi(cid:12)er A to
the instance and report its decision, else pass the instance to B.

7.4.2. Conditional combinations of classi(cid:12)ers to remove concavities
A concavity in an ROC curve represents a sub-optimality in the clas-
si(cid:12)er. Speci(cid:12)cally, a concavity occurs whenever a segment of slope r is
joined at the right to a segment of slope s where s > r. The slope of an
ROC curve represents the class likelihood ratio. A concavity indicates
that the group of instances producing s have a higher posterior class
ratio than those accounting for r. Because s occurs to the right of r,
r’s instances should have been ranked more highly than s’s, but were
not. This is a sub-optimality of the classi(cid:12)er. In practice, concavities
in ROC curves produced by learned classi(cid:12)ers may be due either to
idiosyncracies in learning or to small test set e(cid:11)ects.6

Section 2.1 mentioned that the diagonal y = x on an ROC graph
represents a zone of \no information", where a classi(cid:12)er is randomly
guessing at classi(cid:12)cations. Any classi(cid:12)er below the diagonal can have
its classi(cid:12)cations reversed to bring it above the diagonal. Flach and
Wu (2003) show that in some cases this can be done locally to remove
concavities in an ROC graph.

Figure 11a shows three classi(cid:12)ers, A, B and C. B introduces a con-
cavity in the ROC graph. The segment BC has higher slope than AB,
so ideally we would want to \swap" the position of segment BC for that

6 Bradley’s (1997) ROC curves exhibit noticeable concavities, as do the Breast

cancer and RoadGrass domains of Provost et al. (1998).

ROC101.tex; 16/03/2004; 12:56; p.29

30

Tom Fawcett

of AB in the ROC graph. If A, B and C are related|for example, if
they represent di(cid:11)erent thresholds applied to the same scoring model|
then this can be done. Let A(x) represent the classi(cid:12)cation assigned to
instance x by classi(cid:12)er A. Flach and Wu’s method involves creating a
new classi(cid:12)er (cid:19)B de(cid:12)ned as:

(cid:19)B(x) = 8<
:

N if A(x) = N ^ C(x) = N
Y if A(x) = Y ^ C(x) = Y
if A(x) = N ^ C(x) = Y

:B(x)

Figure 11b shows the new classi(cid:12)er (cid:19)B. Its position is equivalent to
re(cid:13)ecting B’s about the line AC or, equivalently, transposing the deci-
sions in AB with those in BC. Flach and Wu (2003) demonstrate this
construction in greater detail and prove its performance formally.

An important caveat is that A, B and C must be dependent. Speci(cid:12)-
cally, it must be the case that T PA (cid:18) T PB (cid:18) T PC and F PA (cid:18) F PB (cid:18)
F PC. This is commonly achieved when A, B and C are the results
of imposing a threshold T on a single model and TA < TB < TC.
Because of these relationships, there need be no fourth clause covering
A(x) = N ^ C(x) = Y in the de(cid:12)nition of (cid:19)B since these conditions are
contradictory.

7.4.3. Logically combining classi(cid:12)ers
As we have seen, with two classes a classi(cid:12)er c can be viewed as a
predicate on an instance x where c(x) is true i(cid:11) c(x) = Y. We can
then speak of boolean combinations of classi(cid:12)ers, and an ROC graph
can provide a way of visualizing the performance of such combinations.
It can help to illustrate both the bounding region of the new classi(cid:12)er
and its expected position.

If two classi(cid:12)ers c1 and c2 are conjoined to create c3 = c1 ^ c2,
where will c3 lie in ROC space? Let TPrate3 and FPrate3 be the ROC
positions of c3. The minimum number of instances c3 can match is zero.
The maximum is limited by the intersection of their positive sets. Since
a new instance must satisfy both c1 and c2, we can bound c3’s position:

0 (cid:20) TPrate3 (cid:20) min(TPrate1; TPrate2)
0 (cid:20) TPrate3 (cid:20) min(FPrate1; FPrate2)

Figure 12 shows this bounding rectangle for two classi(cid:12)ers c1^c2, the
shaded rectangle in the lower left corner. Where within this rectangle
do we expect c3 to lie? Let x be an instance in the true positive set
T P3 of c3. Then:

T P rate3 (cid:25) p(x 2 T P3)

(cid:25) p(x 2 T P1 ^ x 2 T P2)

ROC101.tex; 16/03/2004; 12:56; p.30

ROC graphs

31

e
t
a
r
 
e
v
i
t
i
s
o
P
 
e
u
r
T

1.0

0.8

0.6

0.4

0.2

0

c1

c1 ^ c2

0

0.2

+
c1Vc2

c2

0.4

0.8
False Positive rate

0.6

1.0

Figure 12. The expected positions of boolean combinations of c1 and c2.

By assuming independence of c1 and c2, we can continue:

T P rate3 (cid:25) p(x 2 T P1) (cid:1) p(x 2 T P2)

(cid:25)

j T P1 j
j P j

(cid:1)

j T P2 j
j P j

(cid:25) T P rate1 (cid:1) T P rate2

A similar derivation can be done for FPrate3, showing that F P rate3 (cid:25)
F P rate1 (cid:1) F P rate2. Thus, the conjunction of two classi(cid:12)ers c1 and c2
can be expected to lie at the point

(FPrate1 (cid:1) FPrate2 ; TPrate1 (cid:1) TPrate2)

in ROC space. This point is shown as the triangle in (cid:12)gure 12 at
(0:08; 0:42). This estimate assumes independence of classi(cid:12)ers; inter-
actions between c1 and c2 may cause the position of c3 in ROC space
to vary from this estimate.

We can derive similar expressions for the disjunction c4 = c1 _ c2.

In this case the rates are bounded by:

max(TPrate1; TPrate2) (cid:20) TPrate4 (cid:20) min(1; TPrate1 + TPrate2)
max(FPrate1; FPrate2) (cid:20) FPrate4 (cid:20) min(1; FPrate1 + FPrate2)

ROC101.tex; 16/03/2004; 12:56; p.31

32

Tom Fawcett

This bounding region is indicated in (cid:12)gure 12 by the shaded rectangle
in the upper right portion of the ROC graph. The expected position,
assuming independence, is:

TPrate4 = 1 (cid:0) [1 (cid:0) TPrate1 (cid:0) TPrate2 + TPrate1 (cid:1) TPrate2]
FPrate4 = 1 (cid:0) [1 (cid:0) FPrate1 (cid:0) FPrate2 + FPrate1 (cid:1) FPrate2]

This point is indicted by the marked + symbol within the bounding
rectangle.

It is worth noting that the expected location of both c1^c2 and c1_c2
are outside of the ROC convex hull formed by c1 and c2. In other words,
logical combinations of classi(cid:12)ers can produce performance outside of
the convex hull and better than what could be achieved with linear
interpolation.

7.4.4. Chaining classi(cid:12)ers
Section 2 mentioned that classi(cid:12)ers on the left side of an ROC graph
near X = 0 may be thought of as \conservative"; and classi(cid:12)ers on
the upper side of an ROC graph near Y = 1 may be thought of as
\liberal". With this interpretation it might be tempting to devise a
composite scheme that applies classi(cid:12)ers sequentially like a rule list.
Such a technique might work as follows: Given the classi(cid:12)ers on the
ROC convex hull, an instance is (cid:12)rst given to the most conservative
(left-most) classi(cid:12)er. If that classi(cid:12)er returns Y, the composite classi(cid:12)er
returns Y; otherwise, the second most conservative classi(cid:12)er is tested,
and so on. The sequence terminates when some classi(cid:12)er issues a Y
classi(cid:12)cation, or when the classi(cid:12)ers reach a maximum expected cost,
such as may be speci(cid:12)ed by an iso-performance line. The resulting
classi(cid:12)er is c1 _ c2 _ (cid:1) (cid:1) (cid:1) _ ck, where ck has the highest expected cost
tolerable.

Unfortunately, this chaining of classi(cid:12)ers may not work as desired.
Classi(cid:12)ers’ positions in ROC space are based upon their independent
performance. When classi(cid:12)ers are applied in sequence this way, they
are not being used independently but are instead being applied to
instances which more conservative classi(cid:12)ers have already classi(cid:12)ed as
negative. Due to classi(cid:12)er interactions (intersections among classi(cid:12)ers’
T P and F P sets), the resulting classi(cid:12)er may have very di(cid:11)erent perfor-
mance characteristics than any of the component classi(cid:12)ers. Although
section 7.4.3 introduced an independence assumption that may be rea-
sonable for combining two classi(cid:12)ers, this assumption becomes much
less tenable as longer chains of classi(cid:12)ers are constructed.

ROC101.tex; 16/03/2004; 12:56; p.32

ROC graphs

33

7.4.5. The importance of (cid:12)nal validation
To close this section on classi(cid:12)er combination, we emphasize a basic
point that is easy to forget. ROC graphs are commonly used in evalua-
tion, and are generated from a (cid:12)nal test set. If an ROC graph is instead
used to select or to combine classi(cid:12)ers, this use must be considered to
be part of the training phase. A separate held-out validation set must
be used to estimate the expected performance of the classi(cid:12)er(s). This
is true even if the ROC curves are being used to form a convex hull.

7.5. Alternatives to ROC graphs

Recently, various alternatives to ROC graphs have been proposed. We
brie(cid:13)y summarize them here.

7.5.1. DET curves
DET graphs (Martin et al., 1997) are not so much an alternative to
ROC curves as an alternative way of presenting them. There are two
di(cid:11)erences. First, DET graphs plot false negatives on the Y axis instead
of true positives, so they plot one kind of error against another. Second,
DET graphs are log scaled on both axes so that the area of the lower
left part of the curve (which corresponds to the upper left portion
of an ROC graph) is expanded. Martin et al. (1997) argue that well-
performing classi(cid:12)ers, with low false positive rates and/or low false
negative rates, tend to be \bunched up" together in the lower left
portion of a ROC graph. The log scaling of a DET graph gives this
region greater surface area and allows these classi(cid:12)ers to be compared
more easily.

7.5.2. Cost curves
Section 7.1 showed how information about class proportions and er-
ror costs could be combined to de(cid:12)ne the slope of a so-called iso-
performance line. Such a line can be placed on an ROC curve and
used to identify which classi(cid:12)er(s) perform best under the conditions
of interest. In many cost minimization scenarios, this requires inspect-
ing the curves and judging the tangent angles for which one classi(cid:12)er
dominates.

Drummond and Holte (2000; 2002) point out that reading slope
angles from an ROC curve may be di(cid:14)cult to do. Determining the
regions of superiority, and the amount by which one classi(cid:12)er is superior
to another, is challenging when the comparison lines are curve tangents
rather than simple vertical lines. Drummond and Holte reason that if
the primary use of a curve is to compare relative costs, the graphs
should represent these costs explicitly. They propose cost curves as an
alternative to ROC curves.

ROC101.tex; 16/03/2004; 12:56; p.33

34

Tom Fawcett

On a cost curve, the X axis ranges from 0 to 1 and measures the
proportion of positives in the distribution. The Y axis, also from 0
to 1, is the relative expected misclassi(cid:12)cation cost. A perfect classi(cid:12)er
is a horizontal line from (0; 0) to (0; 1). Cost curves are a point-line
dual of ROC curves: a point (i.e., a discrete classi(cid:12)er) in ROC space is
represented by a line in cost space, with the line designating the relative
expected cost of the classi(cid:12)er. For any X point, the corresponding Y
points represent the expected costs of the classi(cid:12)ers. Thus, while in
ROC space the convex hull contains the set of lowest-cost classi(cid:12)ers, in
cost space the lower envelope represents this set.

7.5.3. Relative superiority graphs and the LC index
Like cost curves, the LC index (Adams and Hand, 1999) is a trans-
formation of ROC curves that facilitates comparing classi(cid:12)ers by cost.
Adams and Hand argue that precise cost information is rare, but some
information about costs is always available, and so the AUC is too
coarse of a measure of classi(cid:12)er performance. An expert may not be
able to specify exactly what the costs of a false positive and false
negative should be, but an expert usually has some idea how much
more expensive one error is than another. This can be expressed as a
range of values in which the error cost ratio will lie.

Adams and Hand’s method maps the ratio of error costs onto the
interval (0,1). It then transforms a set of ROC curves into a set of
parallel lines showing which classi(cid:12)er dominates at which region in
the interval. An expert provides a sub-range of (0,1) within which the
ratio is expected to fall, as well as a most likely value for the ratio.
This serves to focus attention on the interval of interest. Upon these
\relative superiority" graphs a measure of con(cid:12)dence|the LC index|
can be de(cid:12)ned indicating how likely it is that one classi(cid:12)er is superior
to another within this interval.

The relative superiority graphs may be seen as a binary version
of cost curves, in which we are only interested in which classi(cid:12)er is
superior. The LC index (for loss comparison) is thus a measure of
con(cid:12)dence of superiority rather than of cost di(cid:11)erence.

8. Conclusion

ROC graphs are a very useful tool for visualizing and evaluating clas-
si(cid:12)ers. They are able to provide a richer measure of classi(cid:12)cation per-
formance than accuracy or error rate can, and they have advantages
over other evaluation measures such as precision-recall graphs and lift
curves. However, as with any evaluation metric, using them wisely

ROC101.tex; 16/03/2004; 12:56; p.34

ROC graphs

35

requires knowing their characteristics and limitations. It is hoped that
this article advances the general knowledge about ROC graphs and
helps to promote better evaluation practices in the data mining com-
munity.

Appendix

A. Generating an ROC curve from a decision tree

As a basic example of how scores can be derived from some model
classes, and how a ROC curve can be generated directly from them,
we present a procedure for generating a ROC curve from a decision
tree. Algorithm 7 shows the basic idea. For simplicity the algorithm is
written in terms of descending the tree structure, but it could just as
easily extract the same information from the printed tree representa-
tion. Following C4.5 usage, each leaf node keeps a record of the number
of examples matched by the condition, the number of errors (local false
positives), and the class concluded by the node.

Acknowledgements

While at Bell Atlantic, Foster Provost and I investigated ROC graphs
and ROC analysis for use in real-world domains. My understanding of
ROC analysis has bene(cid:12)ted greatly from discussions with him.

I am indebted to Rob Holte and Chris Drummond for many en-
lightening email exchanges on ROC graphs, especially on the topics of
cost curves and averaging ROC curves. These discussions increased my
understanding of the complexity of the issues involved. I wish to thank
Terran Lane, David Hand and Jos(cid:19)e Hernandez-Orallo for discussions
clarifying their work. I wish to thank Kelly Zou and Holly Jimison for
pointers to relevant articles in the medical decision making literature.
I am grateful to the following people for their comments and correc-
tions on previous drafts of this article: Chris Drummond, Peter Flach,
George Forman, Rob Holte, Sofus Macskassy, Sean Mooney, Joshua
O’Madadhain and Foster Provost. Of course, any misunderstandings
or errors remaining are my own responsibility.

Much open source software was used in this work. I wish to thank
the authors and maintainers of XEmacs, TEXand LATEX, Perl and its
many user-contributed packages, and the Free Software Foundation’s
GNU Project. The (cid:12)gures in this paper were created with Tgif, Grace
and Gnuplot.

ROC101.tex; 16/03/2004; 12:56; p.35

36

Tom Fawcett

References

Adams, N. M. and D. J. Hand: 1999, ‘Comparing classi(cid:12)ers when the misallocations

costs are uncertain’. Pattern Recognition 32, 1139{1147.

Barber, C., D. Dobkin, and H. Huhdanpaa: 1993, ‘The quickhull algorithm for convex
hull’. Technical Report GCG53, University of Minnesota. Available: ftp://
geom.umn.edu/pub/software/qhull.tar.Z.

Bradley, A. P.: 1997, ‘The use of the area under the ROC curve in the evaluation of

machine learning algorithms’. Pattern Recognition 30(7), 1145{1159.

Breiman, L., J. Friedman, R. Olshen, and C. Stone: 1984, Classi(cid:12)cation and

regression trees. Belmont, CA: Wadsworth International Group.

Clearwater, S. and E. Stern: 1991, ‘A rule-learning program in high energy physics

event classi(cid:12)cation’. Comp Physics Comm 67, 159{182.

Domingos, P.: 1999,

‘MetaCost: A general method for making classi(cid:12)ers cost-
sensitive’. In: Proceedings of the Fifth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. pp. 155{164.

Drummond, C. and R. C. Holte: 2000, ‘Explicitly Representing Expected Cost: An
alternative to ROC representation’. In: R. Ramakrishnan and S. Stolfo (eds.):
Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. pp. 198{207, ACM Press.

Drummond, C. and R. C. Holte: 2002, ‘Classi(cid:12)er cost curves: Making performance
evaluation easier and more informative’. Unpublished manuscript available from
the authors.

Egan, J. P.: 1975, Signal Detection Theory and ROC Analysis, Series in Cognitition

and Perception. New York: Academic Press.

Elkan, C.: 2001, ‘The Foundations of Cost-Sensitive Learning’. In: Proceedings of

the IJCAI-01. pp. 973{978.

Fawcett, T.: 2001, ‘Using Rule Sets to Maximize ROC Performance’. In: Proceed-
ings of the IEEE International Conference on Data Mining (ICDM-2001). Los
Alamitos, CA, pp. 131{138, IEEE Computer Society.

Fawcett, T. and F. Provost: 1996, ‘Combining Data Mining and Machine Learning
for E(cid:11)ective User Pro(cid:12)ling’. In: Simoudis, Han, and Fayyad (eds.): Proceedings on
the Second International Conference on Knowledge Discovery and Data Mining.
Menlo Park, CA, pp. 8{13, AAAI Press.

Fawcett, T. and F. Provost: 1997, ‘Adaptive Fraud Detection’. Data Mining and

Knowledge Discovery 1(3), 291{316.

Flach, P. and S. Wu: 2003, ‘Repairing concavities in ROC curves’. In: Proc. 2003

UK Workshop on Computational Intelligence. pp. 38{44.

Forman, G.: 2002, ‘A method for discovering the insigni(cid:12)cance of one’s best clas-
si(cid:12)er and the unlearnability of a classi(cid:12)cation task’. In: Lavrac, Motoda, and
Fawcett (eds.): Proceedings of th First International Workshop on Data Mining
Lessons Learned (DMLL-2002). Available: http://www.hpl.hp.com/personal/
Tom_Fawcett/DMLL-2002/Forman.pdf.

Hand, D. J. and R. J. Till: 2001, ‘A simple generalization of the area under the
ROC curve to multiple class classi(cid:12)cation problems’. Machine Learning 45(2),
171{186.

Hanley, J. A. and B. J. McNeil: 1982, ‘The Meaning and Use of the Area under a

Receiver Operating Characteristic (ROC) Curve’. Radiology 143, 29{36.

Holte, R.: 2002, ‘Personal communication’.
Kubat, M., R. C. Holte, and S. Matwin: 1998, ‘Machine Learning for the Detection

of Oil Spills in Satellite Radar Images’. Machine Learning 30, 195{215.

ROC101.tex; 16/03/2004; 12:56; p.36

ROC graphs

37

Lane, T.: 2000, ‘Extensions of ROC Analysis to multi-class domains’. In: T. Diet-
terich, D. Margineantu, F. Provost, and P. Turney (eds.): ICML-2000 Workshop
on Cost-Sensitive Learning.

Lewis, D.: 1990, ‘Representation quality in text classi(cid:12)cation: An introduction and
In: Proceedings of Workshop on Speech and Natural Language.

experiment’.
Hidden Valley, PA, pp. 288{295, Morgan Kaufmann.

Lewis, D.: 1991, ‘Evaluating Text Categorization’.

In: Proceedings of Speech and

Natural Language Workshop. pp. 312{318, Morgan Kaufmann.

Macskassy, S. A. and F. Provost: 2004, ‘Con(cid:12)dence Bands for ROC curves’.

In:

Submitted to ICML-2004.

Martin, A., G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki: 1997, ‘The
DET Curve in Assessment of Detection Task Performance’. In: Proc. Eurospeech
’97. Rhodes, Greece, pp. 1895{1898.

Provost, F. and P. Domingos: 2001,

‘Well-trained PETs: Improving Probability
Estimation Trees’. CeDER Working Paper #IS-00-04, Stern School of Business,
New York University, NY, NY 10012.

Provost, F. and T. Fawcett: 1998,

‘Robust classi(cid:12)cation systems for imprecise
environments’.
In: Proceedings of the Fifteenth National Conference on Arti-
(cid:12)cial Intelligence (AAAI-98). Menlo Park, CA, pp. 706{713. Available: http:
//www.purl.org/NET/tfawcett/papers/aaai98-dist.ps.gz.

Provost, F. and T. Fawcett: 2001, ‘Robust Classi(cid:12)cation for Imprecise Environ-

ments’. Machine Learning 42(3), 203{231.

Provost, F., T. Fawcett, and R. Kohavi: 1998, ‘The Case Against Accuracy Esti-
mation for Comparing Induction Algorithms’. In: J. Shavlik (ed.): Proceedings
of the Fifteenth International Conference on Machine Learning. San Francisco,
CA, pp. 445{453.

Saitta, L. and F. Neri: 1998, ‘Learning in the \Real World"’. Machine Learning 30,

133{163.

Spackman, K. A.: 1989, ‘Signal detection theory: Valuable tools for evaluating induc-
tive learning’. In: Proceedings of the Sixth International Workshop on Machine
Learning. San Mateo, CA, pp. 160{163, Morgan Kaufman.

Srinivasan, A.: 1999, ‘Note on the location of optimal classi(cid:12)ers in n-dimensional
ROC space’. Technical Report PRG-TR-2-99, Oxford University Computing
Laboratory, Oxford, England.

Swets, J.: 1988, ‘Measuring the accuracy of diagnostic systems’. Science 240, 1285{

1293.

Swets, J. A., R. M. Dawes, and J. Monahan: 2000,

‘Better Decisions
Available: http:

through Science’.
//www.psychologicalscience.org/newsresearch/publications/journals/
%siam.pdf.

Scienti(cid:12)c American 283, 82{87.

van der Putten, P. and M. van Someren: 2000, ‘CoIL Challenge 2000: The Insurance
Company Case’. Tech report 2000-09, Leiden Institute of Advanced Computer
Science.

Zadrozny, B. and C. Elkan: 2001, ‘Obtaining calibrated probability estimates from
In: Proceedings of the Eighteenth

decision trees and naive bayesian classiers’.
International Conference on Machine Learning. pp. 609{616.

Zou, K. H.: 2002, ‘Receiver operating characteristic (ROC) literature research’. On-
line bibliography available from http://splweb.bwh.harvard.edu:8000/pages/
ppl/zou/roc.html.

ROC101.tex; 16/03/2004; 12:56; p.37

38

Tom Fawcett

pt:x   pt:x=count[neg class]
pt:y   pt:y=count[pos class]

pt:x   (count[neg class] (cid:0) pt:x)=count[neg class]
pt:y   (count[pos class] (cid:0) pt:y)=count[pos class]

Algorithm 7 Generating an ROC curve from a decision tree
Inputs: pos class and neg class, the positive and negative classes; and T , the
decision tree root node. Each tree node has (cid:12)elds class, the class concluded
by the node; matched, the number of instances matched by the condition; and
errors, the number of non-class instances matched by the condition. If a node
is not a leaf node it also has children, an array of pointers to its children, and
n children, the number of children.
Outputs: R, a list of ROC points.
1: pos points   (); neg points   ()
2: count[pos class]   0; count[neg class]   0
3: descend(T, pos class);
4: for pt 2 pos points do
5:
6:
7: end for
8: for pt 2 neg points do
9:
10:
11: end for
12: R   pos points [ neg points [ (0; 0) [ (1; 1)
13: sort R increasing by x values
14: end
1: function descend(node; pos class)
2: if node is a leaf node then
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: else
15:
16:
17:
18: end if
19: end function

T P   node:matched (cid:0) node:errors
F P   node:errors
count[node:class]   count[node:class] + T P
pt   new point
if node:class = pos class then

for i = 1 to node:n children do

descend(node:children[i]; pos class)

/* node is an internal node */

pt:x   F P ; pt:y   T P
push pt onto pos points

pt:x   T P ; pt:y   F P
push pt onto neg points

else

end if

end for

ROC101.tex; 16/03/2004; 12:56; p.38

