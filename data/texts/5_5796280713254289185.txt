Speeding up stochastic gradient descent
Yoshua Bengio
December 6th, 2007 NIPS WORKSHOP ON EFFICIENT MACHINE
LEARNING
Thanks to: Olivier Delalleau, Frederic Bastien, Nicolas Le Roux, Pierre-Antoine Manzagol
Yoshua Bengio

Summary
Motivations: large-scale AI learning tasks Insufficient Depth  poor generalization Stochastic gradient or online second order methods asymptotically preferable
Challenge: efficient training of deep architectures Greedy layerwise learning of multiple levels of abstractions Parallelizing stochastic gradient in deep networks Online natural gradient approximations Continuation methods for optimizing deep architectures
Yoshua Bengio

Machine Vision Example
Yoshua Bengio

· MAN abstraction corresponds to convoluted set of images (some very far in pixel distance) · multiple levels of representation · multiple levels of computation · not clear which low & intermediate-level abstractions are good · want to learn representations at all levels · low & intermediate representations can be shared across many tasks

Computation Graph and Depth

element set

output
*
sin

element set

output neuron

* +
sin
+*
- xa b
inputs

neuron
neuron
...
neuron

neuron neuron neuron neuron neuron

inputs

Each node  computations element set. Left: compute x  sin(a  x + b), depth 4. Right: elements = artificial neurons f (x) = tanh(b + w x). Computes a multi-layer neural network of depth 3.

Yoshua Bengio

Current Learning Algorithms: Depth
Depth = number of levels of composition of adaptable elements:

kernel machines: shallow
multi-layer neural networks: usually shallow, can be deep?
decision trees: shallow
boosting: adds one level, but generally shallow

... ...

Yoshua Bengio

Gist of Results on Depth of Architecture
When a function can be compactly represented by a deep architecture, it may need a very large architecture to be represented by an insufficiently deep one
Yoshua Bengio

Insufficient Depth
FFT time O(n2) with depth 1 , O(n log n) with depth log n Two-layer logic gates circuits:
can represent any function. most functions require exponential nb gates (e.g. parity).  functions computable with a polynomial-size depth k circuit, requiring exponential size with depth k - 1. Similar result holds for threshold neuron. (Hastad, 1986; Yao, 1985; Wegener, 1987; Hastad and Goldmann, 1991; Bengio, 2003)
Yoshua Bengio

Optimizing Deep Architectures
Theory suggests that DEPTH is necessary for statistical efficiency Until 2006, we knew no way to train a deep neural net to obtain better results than a shallow one (1 or 2 hidden layer) except for convolutional neural nets (Bengio and Le Cun, 2007). Not convex Traditional approach to train neural nets: random initial weights + stochastic gradient descent. Training seemed to get stuck in sub-optimal solutions, local minima or plateaus or just a too convoluted error surface. Still not clear why it is so difficult to optimize deep architectures by gradient-based techniques, and why it is easier with convolutional nets.
Yoshua Bengio

What happened in 2006?
Geoff Hinton, Simon Osindero and Yee-Wye Teh published a Neural Computation paper on "A fast learning algorithm for Deep Belief Nets" (2006), that introduces these ideas:
A deep unsupervised network could be trained greedily, layer by layer. Each layer an RBM modeling its inputs. Each layer outputs a representation of its input. This unsupervised net is a good initialization for a supervised net. Presumably easier to learn "locally" (within each layer) than having to coordinate all the layers in a deep network.
Yoshua Bengio

Why Online?
Bottou & Le Cun (NIPS'2003) clearly demonstrate that with abundant training data and limited computing resources, online learning asymptotically outperforms any batch algorithm. Bottou & Bousquet (NIPS'2007) study the trade-off between approximation error (too small class), estimation error (variance), and optimization error (not enough CPU time): to reach the best generalization error, large-scale learning makes it necessary to allow some optimization error.
Latter paper compares convergence rates of 1st and 2nd order batch methods with 1st and 2nd order online methods. Both are O(.) similar, up to condition number vs input dimension.
Yoshua Bengio

Underfitting, not Overfitting
The last 15 years of NIPS have been dominated by the underfitting problem. Semi-supervised learning for AI: TONS OF DATA (mostly unlabeled), e.g. R. Collobert with 17 BILLION examples, and Google? Local non-parametric models (a la SVM, nearest-neighbor, etc.) won't scale because of statistical AND computational reasons. Main challenge: training large models with non-convex optimization = underfitting
Yoshua Bengio

Natural Gradient

Let L be a cost function defined as

L~() = L(, x)p~(x) dx
x
Its gradient with respect to the parameters  is

g~ = L(, x) p~(x) dx = g (·, x)p~(x) dx

x 

x

The direction of the natural gradient (Amari, 1998) is

C~ -1g~

with C~ the covariance of the gradients

C~ = [g (·, x) - g~()] [g (·, x) - g~()]T p~(x) dx
x

Yoshua Bengio

Natural gradient minimizes overfitting

No p~,only iid sample {x1, . . . , xn}: empirical cost

L() =

n i =1

L(, xi )

whose

gradient

wrt



is

nn
g = g (·, xi ) = gi
i =1 i =1

Central Limit Theorem 

g N

g~,

C~ n

Assuming Gaussian(0; 2) prior on g~, direction u minimizing

E [uT g~] is

u

n2I + C~

-1
g~

and direction v minimizing E [1vT g~>0] is v  C~ -1g

 go in directions where gradients not only are strong, but also agree.
Yoshua Bengio

Blue: samples gradient Red: mean gradient Green: natural gradient Axis: gradient covariance (e-vector × e-value)
Yoshua Bengio

Exact Natural Gradient is Impractical
1 Computing C is O(np2) with p parameters 2 Computing C -1g is O(p3) 3 Memory for C is O(p2) 4 For 784 - 300 - 10 neural network (small network MNIST):
212 Gb Other problems arise in the online setting. Indeed, this algorithm needs a good estimate of C . Although C varies smoothly, it must be evaluated over a large number of samples.
Yoshua Bengio

Low Rank Approximations of C and C -1g

Low rank + moving covariance C with forgetting factor .

1 Gram matrix of gradients, computing k eigenvectors is O(pk2) (instead of O(p2k))

2

Do it every b steps: O

pk 2 b

.

i

Ci = i C + i-k gk gkT + I

i = 1, . . . , b

k=1

vi = Ci-1gi

Xi =

i
2U



i

-1 2

g1

...

1
 2 gi-1 gi

Ci = Xi XiT + I

vi = Xi i

i = (XiT Xi + I )-1yi

vi = Xi (XiT Xi + I )-1yi

gi = Xi yi

(1)

Yoshua Bengio

Block Diagonal Modelling
Neural nets & mixture models: almost independent groups of parameters  block diagonal approximation (one block per group)
Yoshua Bengio

Experimental Results: MNIST

Classification error on the test set

Classification error on the training set

50,000 training examples, 784-800-10 neural net

0.06 0.06
Block diagonal TONGA

Stochastic batchsize=1

0.055

0.05 Stochastic batchsize=400

Stochastic batchsize=1000 Stochastic batchsize=2000

0.05

0.04 0.045

Block diagonal TONGA Stochastic batchsize=1 Stochastic batchsize=400 Stochastic batchsize=1000 Stochastic batchsize=2000

0.04 0.03
0.035

0.02 0.03

0.025 0.01
0.02

0

0

500

1000

1500

2000

2500

3000

3500

4000

4500

CPU time (in seconds)

0.015 0

500

1000

1500

2000

2500

3000

3500

4000

4500

CPU time (in seconds)

Negative log-likelihood on the training set

0.2
Block diagonal TONGA
0.18 Stochastic batchsize=1
Stochastic batchsize=400 0.16 Stochastic batchsize=1000
Stochastic batchsize=2000
0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

500

1000

1500

2000

2500

3000

3500

4000

4500

CPU time (in seconds)Yoshua Bengio

Negative log-likelihood on the test set

0.2
Block diagonal TONGA Stochastic batchsize=1 Stochastic batchsize=400 Stochastic batchsize=1000 Stochastic batchsize=2000
0.15

0.1

0.05 0

500

1000

1500

2000

2500

3000

3500

4000

4500

CPU time (in seconds)

Experimental Results: Rectangles Data

900,000 training examples, 784-200-200-100-2 neural net

0.5 0.2

Stochastic gradient

Stochastic gradient

0.45

Block diagonal TONGA

0.18

Block diagonal TONGA

Classification error on the validation set

Classification error on the training set

0.4 0.16

0.35

0.14

0.3 0.12

0.25

0.1

0.2 0.08

Negative log-likelihood on the training set

0 0.5 1 1.5 2 2.5 3 3.5

CPU time (in seconds)

x 104

0.55
Stochastic gradient 0.5 Block diagonal TONGA

0.45

0.4

0.35

0.3

0.25

0.2

0 0.5 1 1.5 2 2.5 3 3.5
CPU time (in seconds)Yoshua Bengxi1o04

Negative log-likelihood on the validation set

0.06 0 0.5 1 1.5 2 2.5 3 3.5

CPU time (in seconds)

x 104

0.2
Stochastic gradient 0.18 Block diagonal TONGA

0.16

0.14

0.12

0.1

0.08

0.06 0 0.5 1 1.5 2 2.5 3 3.5

CPU time (in seconds)

x 104

Mini-batch stochastic gradient

"Trade-off" between batch and stochastic:

k+1  k -

sk +b C (t , zt )

k t =sk



Typical size of mini-batches: on the order of 10's or 100's.

Yoshua Bengio

Computing Faster Products
Bottleneck computation in neural nets: matrix-vector product. Use minibatches: matrix-matrix product  use BLAS BLAS implementations:
ACML (AMD) ATLAS: Automatically Tuned Linear Algebra Software NVIDIA: GPU, float only currently Goto (Kazushige Goto from University of Texas at Austin) MKL (Intel) BLAS: reference version from netlib.org
Yoshua Bengio

Comparing Different BLAS

1000 times 1024 × 1024 * 1024 × 1024 float on Intel Core2 Quad Uniprocessor

Time(s) Speedup
vs C

C 84.7
1

Multiprocessor

BLAS ACML ATLAS Goto 47.6 8.3 8.1 6.0
1.8 10.2 10.5 14.1

MKL 6.7
12.6

NVIDIA 2
42.35

C Goto1 Goto2 Goto4 MKL1 MKL2 MKL4

Time(s) 84.7 6.0 5.6 3.2 6.7 3.7 1.5

Speedup vs C

1

14.1 15.1 26.5 12.6 22.9 56.5

Yoshua Bengio

On Actual Neural Net Code

512 hidden, 512 inputs, 32 examples mini-batch: 512 × 512 * 512 × 32

Time(s) Speedup
vs BLAS

BLAS 403
1

ACML ATLAS Goto 318 158 131
1.3 2.6 3.1

MKL 135
3.0

BLAS Goto1 Time(s) 403.1 131.1 Speedup
vs 1 3.1 BLAS Goto is GOOD and FREE.

Goto2 120.6
3.4

Goto4 104.8
3.8

MKL1 134.9
3.0

MKL2 96.6
4.2

MKL4 117.0
3.4

Yoshua Bengio

Parallelization on SMPs
Using 8-core and 16-core machines, NOT 100's on a cluster. Even on SMPs, communication is a bottleneck! Use existing parallel implementations of BLAS to speed-up matrix-matrix multiplications: low speed-up unless using very large mini-batches ( equivalent to batch) Multiple simultaneous read access in same memory area: OK Multiple simultaneous write access in same memory area: NOT OK!
Yoshua Bengio

Data-Parallel Stochastic Gradient
Split data into c chunks Each of the c cores sees one chunk of the data Perform mini-batch gradient descent with parameters in shared memory:
if all cores are updating parameters simultaneously: poor performance (time wasted waiting for memory write locks) proposed solution: at any time, only one core is allowed to update parameters. The index of the next core to update is stored in shared memory, and incremented after each update.
Yoshua Bengio

The Big Picture
Index of CPU allowed to update: 1

Data part 1

CPU 1

Internal update 1
Yoshua Bengio

Parameters

CPU 2

Data part 2

Internal update 2

Experiments
UCI Letter dataset 16000 training + 4000 test, 16-dimensional input, 750 hidden, 26 classes Target test classification error: 3% Minibatch sizes optimized in {4,8,16,32,64,128} Learning rate optimized in [.001,.002,.005,. . . ,.05,.1] RESULTS ARE HIGHLY VARIABLE (5 initial conditions)
Yoshua Bengio

Straw Man
Our algorithm is compared with a similar form of parallelization in which we do not try to avoid write collisions:
Each core works on a different mini-batch. After the mini-batch they share (sum) their gradients. All update their parameters with total gradient from all cores.
Yoshua Bengio

Results per Update
Proposed method gives good speed-up in terms of raw "samples/sec" speed. Preliminary experiment measuring TIME PER EXAMPLE, not taking convergence time into account.
N = 16 cores Speedup = 13 fold = 80% of linear speed-up
Yoshua Bengio

Computational Speed-up vs Convergence Slow-down
Data-parallelization = bigger mini-batches BUT decrease frequency of updates  lower convergence speed Suggested experimental setting:
Identify target training error e from simple experiments For each number of cores c, measure time needed to reach error e Hyper-parameters (learning rate and mini-batch size in particular) need to be re-optimized for each value of c.
Yoshua Bengio

Results with Training Convergence

§

¦

¥

" &
$% # #

¤

!"

£

¢

¡
   

'

( )0

12

3

) 45

6

7

80

49

@

¡ ¢£

¤¥

¨ ©













¦§

Yoshua Bengio

Global Optimization through Continuation Methods

Continuation Methods: optimize a sequence of gradually less smooth cost functions leading to target cost function.

final solution

target cost fn
slightly smoothed
heavily smoothed

track minima easy to find initial solution
Yoshua Bengio

Global Optimization though Continuation Methods
Can prove that the greedy layer-wise approach to training RBMs is a discrete continuation method. Adding each layer removes a constraint. Stochastic gradient descent from small parameters is nearly a continuation method (with parameter norm as the smoothness control). A curriculum can be framed as a continuation method (changing a sampling probability on examples according to how difficult they currently are to learn).
Yoshua Bengio

Conclusions
Large-scale AI learning tasks Insufficient Depth  poor generalization Need stochastic/minibatch gradient or online second order methods Optimizing deep architectures?
Greedy layerwise learning of multiple levels of abstractions Parallelizing stochastic gradient in deep networks: BLAS + architecture-aware solutions Natural gradient can be efficiently approximated online: BEATS STOCHASTIC GRADIENT Continuation methods for global optimization of deep architectures
Yoshua Bengio

To Know More
Available on my web page: these slides review paper on deep architectures and Deep Belief Nets
Learning Deep Architectures for AI
TONGA (NIPS 2007 paper) Top-Momoute Online Natural Gradient Algorithm
Yoshua Bengio

Amari, S. (1998). Natural gradient works efficiently in learning. Neural Computation, 10(2):251­276.
Bengio, Y. (2003). Learning deep architectures for AI. Technical Report 1312, Dept. IRO, Universit´e de Montr´eal.
Bengio, Y. and Le Cun, Y. (2007). Scaling learning algorithms towards AI. In Bottou, L., Chapelle, O., DeCoste, D., and Weston, J., editors, Large Scale Kernel Machines. MIT Press.
Hastad, J. (1986). Almost optimal lower bounds for small depth circuits. In Proceedings of the 18th annual ACM Symposium on Theory of Computing, pages 6­20, Berkeley, California. ACM Press.
Hastad, J. and Goldmann, M. (1991). On the power of small-depth threshold circuits. Computational Complexity, 1:113­129.
Hinton, G. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771­1800.
Hinton, G. and Ghahramani, Z. (1997). Generative models for discovering sparse distributed representations. Philosophical Transactions of the Royal Society of London, B(352):1177­1190.
Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18:1527­1554.
Hinton, G. E., Osindero, S., Welling, M., and Teh, Y.-W. (2006). Unsupervised discovery of non-linear structure using contrastive backpropagation. Cognitive Science, 30(4).
Yoshua Bengio

Paccanaro, A. and Hinton, G. (2000). Extracting distributed representations of concepts and relations from positive and negative propositions. In Proceedings of the International Joint Conference on Neural Network, IJCNN'2000, Como, Italy. IEEE, New York.
Wegener, I. (1987). The Complexity of Boolean Functions. John Wiley & Sons.
Yao, A. (1985). Separating the polynomial-time hierarchy by oracles. In Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science, pages 1­10.
Yoshua Bengio

