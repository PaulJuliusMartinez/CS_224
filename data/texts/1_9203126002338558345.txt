2010 IEEE International Conference on Computational Intelligence and Computing Research

Evaluating the Performance of Filtering Techniques for Feature Selection in High Dimensional Imbalanced Dataset

T.Deepa1, Dr.M.Punithavalli2
1 Research Scholar, Computer Science Department, Karpagam University,Coimbatore,TamilNadu,India 2 Director and Head,Sri Ramakrishna College of Arts & Science for Women, Coimbatore ,TamilNadu, India.

(DeepaRaman12@gmail.com ,mpunitha_srcw@yahoo.co

Abstract - In Mining High-Dimensional Imbalanced dataset(where one class outnumbers the other) feature selection is an important and crucial task. Filtering Technique is the simplest feature selection methodology used to extract features in High-Dimensional database. This paper involves evaluating the performance of commonly used filtering techniques and proposes the accurate technique for feature selection. The Techniques are evaluated using micro array dataset.
Keywords ­ Feature selection, Imbalanced dataset, Sampling, Under sampling, Oversampling.

The [T.S.Furey,N.Duffy,2000] work analyzes the filters and [I.Inza,R.Blanco2004] compare the filters, wrappers, principal component analysis. Genetic algorithm and novel feature selection technique is also used for micro array data analysis.
The contribution of this paper is evaluating the performance of various filtering techniques, sampling techniques for analyzing micro array data on highdimensional Imbalanced dataset.
III. FILTERING TECHNIQUES

I. INTRODUCTION
A feature selection technique is a vigorous research area in many fields of data mining applications, bioinformatics, statistics etc. Feature selection task involves extracting a subset of features from a given dataset by eliminating features with less or no predictive information. Eliminating the number of features results in several benefits such as i) Quicker training model, ii) Over fitting can be reduced, iii)storage, memory and processing requirement can be minimized while analyzing the data. The drawback of the technique is that critical attribute will be omitted which effects the classifier performance.
Filtering is one of the simplest feature selection technique to select a subset of features from the Imbalanced (Where one class outnumbers the other) micro array dataset. This work evaluates the performance of commonly used filters and the results are compared to propose an optimized technique for selecting subset of features.
The Sampling techniques such as Under Sampling, Over Sampling, SMOTE are used for balancing the dataset. The accuracy of these techniques is also evaluated.

Feature selection Techniques can be classified into three types: 1) Filtering Technique 2) Wrapper Method 3) Embedded methods.

Feature selection Technique

Filter

Wrapper

Embedded

Fig 1. Feature selection Methods
This work is focused on Filtering techniques which assess the subset of features based on intrinsic properties of data and it is more suitable for high-dimensional datasets. The unique characteristic of filtering is that it is simple, fast and independent of the classification algorithm. Some of the commonly used filters such as chisquare statistic, Information gain, Gain Ratio, F-Measure, Symmetric uncertainty, Geometric Mean.
A. Chi-square statistic:

II. RELATED WORK
The research on micro array data analysis results in two dimensions:
1) Known Features: Leads in improving classification in the existing research.
2) Unknown Features: Leads to new research with the unknown samples.

Chi-Square statistic compares the expected and

observed values to determine how well an

experimenter's predictions fit the data It performs an

hypothesis test on the data. The null hypothesis mean

there is no correlation.

( )2

X2 =

y 

c 

Oij - Ei, j

.

i=1 j=1 Ei, j

(1)

ISBN: 97881 8371 362 7

2010 IEEE International Conference on Computational Intelligence and Computing Research

In the above equation r is the number of values of the feature expected, c is the number of classes expected
(c=2 default), Oi,j is the number of instances observed with value i and class j.

B. Information Gain (IG):

This technique is used for evaluating how important a given feature is. The information gain of a given attribute X with respect to the class attribute Y is the reduction in uncertainty about the value of Y when we know the value of X, I(Y ;X). The uncertainty about the value of Y is measured by its entropy, H(Y). The uncertainty about the value of Y when we know the value of X is given by the conditional entropy of Y given X, H(Y |X).

I (Y ; X ) = H (Y ) - H (Y | X )

(2)

When Y and X are discrete variables that take values in {y1...yk} and {x1...xl} then the entropy of Y is given by

i= k

( ( ))H (Y ) = -  P(Y = i=1

yi ) log 2

P

Y=

yi

(3)

The conditional entropy of Y given X is:

j= l

H (Y

|

X)=

-

 P(X j=1

=

x j )H (Y

|

X)=

xj)

(4)

The Information gain of an attribute is measured by the reduction in entropy

IG ( X ) = H (Y )- H (Y / X )

(5)

C. Gain Ratio(GR):

The gain ratio is used for calculating important flaws

in Information gain. The Intrinsic value of attribute Xis

given by

P

( ) ( )IV (X ) = -  i=1

| Xi | /n

log

| Xi | /n)

(6)

where |Xi| is the number of instances where attribute X takes value Xi, the number of distinct values of X is p, and n is the total number of instances in the dataset. IV adds a separate term for each value of the feature times the log of the relative number of instances in each value.
The gain ratio of X is defined as the entropy of X divided by the intrinsic value of X:

D. F-Measure(FM):

The F-measure (F) is derived from recall and precision. F-measure uses a tunable parameter  to indicate the relative importance of recall and precision. By default,  = 1 is used (as is the case in our study).

( )1 +  2 × TPR(t) × PRE(t)

F - measure= max t0,1

 2 × TPR(t) + PRE(t)

.

(8)

Recall and precision are calculated at each point along the normalized attribute range of 0 to 1. The maximum Fmeasure obtained by each attribute represents how strongly that particular attribute relates to the class, according to the F-measure.

E. Symmetric Uncertainty(SU) It is used for evaluating Feature-class correlation

H(D)- H (D | X ) U (D, X ) = 2.
H (D)+ H(X )

(9)

U refers to Uncertainty, D is set of all instances, X is the set of feature instances, H(D|X) is the entropy of D Which is calculated similarly as equation(4).
F. Geometric Mean(GM):
It is the square root of the product of the true positive rate and true Negative rate. It ranges from 0 to1. It is used for finding the error rate and measuring the performance.

GM = max TPR(t) × TNR(t) t0,1

(10)

IV. SAMPLING TECHNIQUES
The class Imbalance problem occurs when the data collection process is limited due to privacy reasons, Complexity, etc. A number of solutions to the classimbalance problem were proposed by many researchers to overcome the imbalance problem.

IG( X ) H (D) - H (D | X )

GR( X ) =

=

.

IV ( X )

H(X)

(7)

Where D=Y.

2010 IEEE International Conference on Computational Intelligence and Computing Research

Figure 2: (a) Many negative cases against some spare positive cases. (b)
balanced data set with well-defined clusters.
Sampling is one of the technique to solve the Imbalance problem. It is used for balancing the class distribution. Under Sampling, Over Sampling and SMOTE are some of the sampling techniques.
A. Under sampling
Random under-sampling is a non-heuristic method that aims to balance class distribution through the random elimination of majority class examples. The logic behind this is to try to balance out the dataset in an attempt to overcome the idiosyncrasies of the machine learning algorithm. The major drawback of random under sampling is that this method can discard potentially useful data that could be important for the induction process.
B. Over sampling
Random over-sampling is a non-heuristic method that aims to balance class distribution through the random replication of minority class examples. Several authors ([18],[4])agree that random over-sampling can increase the likelihood of occurring over fitting, since it makes exact copies of the minority class examples.
C. SMOTE
SMOTE generates synthetic minority examples to over-sample the minority class .Its main idea is to form new minority class examples by interpolating between several minority class examples that lie together. For every minority example, its k (which is set to 5 in SMOTE) nearest neighbors of the same class are calculated, then some examples are randomly selected from them according to the over-sampling rate. A new synthetic examples are generated along the line between the minority example and its selected nearest neighbors. Thus, the over fitting problem is avoided and causes the decision boundaries for the minority class to spread further into the majority class space.
V. EXPERIMENTAL RESULT
This work mainly focus on evaluation of various filtering techniques used for feature selection in highdimensional imbalanced dataset. The techniques such as

chi-square statistic, Information gain, Gain Ratio, FMeasure, Symmetric uncertainty, Geometric Mean are evaluated by applying to three types of datasetLymphoma dataset, Lung cancer dataset and Colon Dataset which are high dimensional and naturally imbalanced. The evaluation is focused on comparing three dimensions (filtering techniques, Sampling , dataset) as in fig 3.

Dataset Lym Lungcancer Colon

Filtering CS IG GR SU FM GM

Sampling RUS ROS SM

Fig 3:Three Dimensions of Comparison
All the above three datasets are naturally imbalanced dataset(differs in positive and negative rate) and it contains attributes of about 2000,2001,12534 attributes. These datasets are balanced using sampling techniques and the features are selected using the filtering technique.

TABLE I
THREE DIMENSIONS OF EVALUATION

Comparison
cs-ros-lungcancer cs-ros-Lymphoma cs-ros-colon cs-rus-lungcan cs-rus-lymp cs-rus-colon cs-sm-lym cs-sm-colon ig-ros-lym ig-ros-lungcan ig-ros-colon ig-rus-lymp ig-rus-lungcan ig-rus-colon ig-sm-lym su-ros-lungcan su-ros-colon gm-rus-lym fm-ros-lungcan fm-rus-lungcan fm-sm-colon

Acc
382.3 346.16 511.5 346.11 317.01 511.5 286.44 501.88 361.05 217.83 519.17 358.11 396.25 491.66 310.64 348.68 498.03 353.74 238.41 281.44 490.33

Err
11.14 14.18 4.86 14.18 13.16 4.86 15.3 4.94 12.71 17.38 4.79 11.18 11.21 5.08 14.25 10.77 4.97 13.92 16.42 14.49 5.03

Time
0: 0:19 0:00:11 0:00:12 0:00:11 0: 0:16 0: 0:12 0:00:37 0: 0:11 0: 0:17 0: 0:12 0: 0:11 0:00:15 0:00:12 0:00:11 0:00:36 0:00:16 0:00:11 0:00:14 0:00:11 0:00:11 0:00:11

This work runs into an evaluation that same dataset when applied different sampling and filtering produces different result. The accuracy and time varies according to the technique. Table 1 shows the comparison and also the accuracy, Error rate and time taken for selecting the

2010 IEEE International Conference on Computational Intelligence and Computing Research

subset of features. From Table 1 it is understood that SMOTE is the best sampling technique when compared to others and colon dataset is the suitable and best dataset for feature selection. It is also understood that Chi-Square and F-Measure gains the best accuracy rate.
VI. CONCLUSION AND FUTURE WORK
The proposed work presents a detailed three dimensional comparison-Filtering, Sampling and dataset. It also analyzes that same dataset when applied different sampling and filtering technique the result varies. Finally it is concluded that SMOTE is the best sampling technique and F-Measure is the best technique for selecting subset of features. Future work can be concentrated on other feature selection techniques and the speed of the data can be improved. It is also recommended that other feature selection techniques such as wrapper and Embedded methods can be considered.
REFERENCES
[1] E. A. P. A. Batista, R. C. Prati, and M. C. Monard. "A study of the behavior of several methods for balancing gmachine learning training data."SIGKDD Explorations, 6(1):20-29,2004.
[2] In N. V. Chawla, N. Japkowicz, and A. Ko lcz, "Special Issue on learning from Imbalanced dataset",Sigkdd Explorations Vol 6,issue 1,Pages1-6, 2006.
[3] N. V. Chawla, N. Japkowicz, and A. Ko lcz, editors, Proceedings of the ICML'2003 "Workshop on Learning from Imbalanced Data Sets". 2003.
[4] N. V. Chawla, L. O. Hall, K. W. Bowyer, and W. P. Kegelmeyer"SMOTE: Synthetic Minority Oversampling Technique". Journal of Artificial Intelligence Research, 16:321- 357, 2002.
[5] Drummond, C., and Holte, R. C. "C4.5, Class Imbalance, and Cost Sensitivity: Why Undersampling beats Over-sampling". In Workshop on Learning from Imbalanced Data Sets II (2003).
[6] Guyon and A. Elissee_. An "Introduction to variable and feature selection". Journal of Machine Learning Research, 3:1157{1182, 2003.
[7] N. Japkowicz, editor, Proceedings of the AAAI'2000 "Workshop on Learning from Imbalanced Data Sets" ,AAAI Tech Report WS00-05. AAAI, 2000.
[8] D. Mladenic and M. Grobelnik, "Feature selection for unbalanced class distribution and naive bayes". In Proceedings of the 16th International Conference on Machine Learning, pages 258{267, 1999.)
[9] Z. Zheng, X. Wu, and R. Srihari. "Feature selection for text categorization on imbalanced data". SIGKDD Explorations, 6(1):80{89, 2004.

[10] T. Jirapech-Umpai and S. Aitken. "Feature

selection and classification for microarray data

analysis: Evolutionary methods for identifying

predictive genes". BMC bioinformatics, 6(1):148,

2005.

[11] . Kira and L. A. Rendell. "The feature selection

problem:Traditional methods and a new

solution. In AAAI '92:" Proc.10th Nat'l Conf.

on Artificial Intelligence, number 10, pages

129­134. John Wiley & Sons, Ltd., July 1992.

[12] B. J. Lee, H. G. Lee, J. Y. Lee, and K. H. Ryu.

"Classification of enzyme function from protein

sequence based on feature representation". pages

741­747, Oct. 2007.

[13] L. Li, H. Tang, Z. Wu, J. Gong, M. Gruidl, J.

Zou, M. Tockman,and R. A. Clark. "Data mining

techniques for cancer detection using serum

proteomic profiling". Artificial Intelligence in

Medicine, 32(2):71 ­ 83, 2004.

[14] F. Model. "Feature selection for dna methylation

based cancer classification". Bioinformatics,

17:157­164(8), June 2001.

[15] S. D. Peddada, E. K. Lobenhofer, L. Li, C. A.

Afshari, C. R.Weinberg, and D.M. Umbach.

"Gene selection and clustering for time-course

and dose-response micro array experiments using

order-restricted inference". Bioinformatics,

19(7):834­841, 2003.

[16] H. Peng, F. Long, and C. Ding. "Feature selection

based on mutual information: Criteria of max-

dependency, maxrelevance, and min-edundancy".

IEEE Transactions on Pattern Analysis and

Machine Intelligence, 27(8):1226­1238,2005.

[17] Y. Sun, M. Robinson, R. Adams, R. te

Boekhorst, A. Rust, and N. Davey. "Using feature

selection filtering methods forbinding site

prredictions". volume 1, pages 566­571, July

2006.

[18] Jason

VanHulse,Amri

NapoLitano,

T.M,Khoshgoftaar "Feature Selection with High-

Dimensional Imbalanced data" IEEE Conf on

Data mining Tech.2009.

[19] T. S. Furey, N. Cristianini, N. Duffy, D. W.

Bednarski, M. Schummer, and D. Haussler.

"Support vector machine classification and

validation of cancer tissue samples

usingmicroarray expression data". Bioinformatics,

16(10):906­914, 2000

[20] I. Inza, P. Larraaga, R. Blanco, and A. J.

Cerrolaza. "Filter versus wrapper gene selection

approaches in dna microarray domains". Artificial

Intelligence in Medicine, 31(2):91 ­ 103,2004.

Data Mining in Genomics and Proteomics.

