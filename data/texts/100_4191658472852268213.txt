Appeared in Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp. 161{166, 1992.

Gain Adaptation Beats Least Squares?
Richard S. Sutton
GTE Laboratories Incorporated Waltham, MA 02254 sutton@gte.com

Abstract
I present computational results suggesting that gainadaptation algorithms based in part on connectionist learning methods may improve over least squares and other classical parameter-estimation methods for stochastic time-varying linear systems. The new algorithms are evaluated with respect to classical methods along three dimensions: asymptotic error, computational complexity, and required prior knowledge about the system. The new algorithms are all of the same order of complexity as LMS methods,
O(n), where n is the dimensionality of the system,
whereas least-squares methods and the Kalman l-
ter are O(n2). The new methods also improve over
the Kalman lter in that they do not require a complete statistical model of how the system varies over time. In a simple computational experiment, the new methods are shown to produce asymptotic error levels near that of the optimal Kalman lter and significantly below those of least-squares and LMS methods. The new methods may perform better even than the Kalman lter if there is any error in the lter's model of how the system varies over time.
Introduction
Approximation of linear and nonlinear functions from examples is a central problem in many elds, including adaptive control and estimation, statistics, neural networks and machine learning. It is natural then to expect cross-fertilization, in which ideas developed in one eld may also contribute to methods used in another. One topic of recent interest in connectionist learning is the automatic selection of learning-rate or gain parameters during learning (Jacobs, 1988 Lippmann & Lee, 1990 Sutton, 1992 Gluck, Glauthier & Sutton, 1992 cf. Kesten, 1958). Such dynamic-learning-rate (DLR) methods have been shown to speed convergence on a variety of learning tasks, particularly those with many irrelevant input features and in which the task is non-stationary (the correct solution changes over time). In this paper I present new linear parameter estimation algorithms that combine ideas from DLR methods with ideas from Kalman ltering and leastsquares methods.

We will estimating

tchoenspidaerramtheetecrlavsesicctaolrpr(otb)leomf

of incrementally the linear scalar

system

y(t) = (t)T (t) + (t)

(1)

forb(otsm)erisvoawbtshioeirtnveanntooiioissnees a(ontfd) yis((tmt))eaiasnnsd-lozwer(loty)g,vaaturys=sinia0gn,o1vwe2irt:ht:imv:,aerw.iahTnehcreee R. The parameter vector (t) drifts over time according
to a simple random walk

(t + 1) = (t) + d(t)

(2)

where d(t), the drift, is a mean-zero, gaussian random vector with covariance matrix Q = E d(t)d(t)T . We assume that (t), d(t), and (t) are all independently cho-
sen random variables. Popular estimation methods for this problem include the Kalman lter, least squares, and LMS (projection) methods.

We will consider three ways of evaluating algorithms: asymptotic error, computational complexity, and required prior knowledge of the system. The Kalman lter is indeed optimal in terms of asymptotic error, but requires
accurate knowledge of Q, which is usually not available in
practice. The closely related least-squares methods generally do not assume such knowledge, but neither do they perform as well as the Kalman lter. In addition, both
of these methods are O(n2) in computational complexity, where n is the number of input signals (the dimensionality of (t) and (t)). Their complexity alone rules out their use on large learning problems, where n may be thousands or tens of thousands. The LMS methods are only O(n) in
computational complexity, but perform signi cantly worse than both the Kalman lter and least-squares methods.

O(nT) hien

three DLR methods I present here are computational complexity, but in our

also only computa-

tional experiments they performed much better than both

the LMS methods and least squares, as summarized in

Figure 3. The best of the DLR methods (K1 and K2) in

fact performed very nearly as well as the Kalman lter,

despite their reduced complexity, and despite their lack of

the special knowledge required by the Kalman lter.

The next section is a detailed speci cation of the new algorithms and the classical algorithms used in this study. The third section describes the speci cs of the computational experiment and presents its results.

The Algorithms

This study considered seven algorithms: the Kalman lter, a least-squares method with covariance modi cation, the least-mean-square (LMS) algorithm, normalized LMS (NLMS), and three DLR algorithms called K1, K2, and IDBD. K1 and K2 are new to this paper whereas IDBD was introduced in (Sutton, 1992). This section completely describes the algorithms. They are all of the form

^(t

+

1)

=

^(t)

+

h
K(t) y(t)

;

^(t)T

i
(t)

(3)

where^(0) = (0), and K(t) is a gain vector that di ers
from algorithm to algorithm.

The Classical Algorithms
The LMS (projection) algorithm uses

K(t) = R^ + E^ f

(t) (t)T

(t)g

(4)

where > 0 is a scalar constant, R^ is an estimate of R = E 2(t) , and E^ (t)T (t) is an estimate of the expected value of (t)T (t) (a scalar constant). In this study, the LMS algorithm used E^ (t)T (t) = n, where n is the dimensionality of (t), and all the algorithms used R^ = 1. These estimates are exactly correct for the system
used in the computational experiment. The normalized LMS (NLMS) algorithm uses

K(t) = R^ +

(t) (t)T

(t)

(5)

where 2 > > 0. This algorithm is normally slightly more
e cient than LMS (see Goodwin & Sin, 1984) We turn now to the least-squares and Kalman- lter
algorithms. Given the form of the system time variation, (2), the appropriate least-squares method is least squares with covariance modi cation (e.g., see Goodwin & Sin, 1984). This method uses

K(t) =

R^ +

P(t) (t) (t)T P(t)

(t)

(6)

where P(t) is an n n matrix computed recursively by

P(t + 1) = P(t) ;

P (t) R+

(t) (t)TP(t) (t)T P(t) (t)

+ Q^

(7)

where P(0) = 0 and Q^ is an estimate of the drift covariance matrix Q = E d(t)d(t)T . The algorithm I refer to as LS uses Q^ = I, for > 0. This choice treats all input
signals symmetrically, and re ects no prior knowledge of correlations among the drift variables.
If Q^ = Q and R^ = R, then the algorithm given above
is the Kalman lter (see, e.g., Goodwin & Sin, 1984). In
this case the estimates ^(t) can be shown to be optimal in
the sense that

^(t) = E f (t) j Y(t ; 1)g

2w)he: r: e:

Y

(t ;
(0)

1) = (y(t;1) y(t;
(0)) is the history

2) of

::
all

: y(0) (t;1) (t;
observations preced-

sinhgowtinmtehtapt lPus(ti)niitsiatlhecoensdtiimtioantiso.nA-elrsroorinctohviasrciaansceeitmcaatnribxe:

P(t) = E (t) ; ^(t)] (t) ; ^(t)]T j Y(t ; 1) (8)

However, in practice Q is never known exactly, and an

approximation must be used. In this study, we will con-

basinydderQ>^a=0fa,ImwaihltyicthohferaKnag=lems1afner-xomtlrteeQm^r e=a:lgQoraitthtmhse,

parameterized = 0 extreme,

Q^ = (1 ; )Q + I

(9)

I refer to this in what follows as the Kalman algorithm.

The DLR Algorithms
If n is the dimension of the vectors (t) and (t), then
any algorithm based on (3) requires memory and compu-
tation that is at least of O(n). The LMS and NLMS algorithms are O(n), whereas the Kalman and LS algorithms are O(n2) in both computation and memory. Clearly, no algorithm can be O(n) that stores a full n n matrix such as P(t). However, we need not go as far as the LMS and NLMS algorithms, which have no memory other than ^(t). The idea in algorithms K1 and K2 is to approximate P(t) as a diagonal matrix, using memory only for the n diagonal
entries. K1 and K2 use

K(t) =

R^ +

P^(t) (t) (t)T P^(t)

(t)

(10)

where P^(t) is a diagonal matrix with diagonal entries p^ii(t). pc^oiir(rtAe)slpgisoorrniedtlhainmtgedKset1toaodfia(ppta)trsbatymheetp^eiris(t)ib(yt)g, ria=die1nt: :d:esnce, nwthinerae

p^ii(t) = e i(t+1)

(11)

gibgKnoee1orcp^imaptiuhiee(smrttefr)oip^cKrbimie1s(ctstaea)gppuirspssaerdtionhitxeehinp^nmetiniaa(dsttes)esuxsc(reeetedn.hd-gtes.oi,izgnfeuraaplsdwit(oieaetrpy)nsstdraob(dtwieenhisnnecrgebinty(phttoa)1s)n0it%pdivri)eor,.edcauAtncllyde-

i(t + 1) =

i

(t)

;

1 2

@ 2(t) @i

(12)

where (t) is the error, (t) = y(t);^(t)T (t), and > 0 is
a constant, the meta-step-size parameter. Algorithm K1's update equations were derived from (12) as shown in the appendix. They are

i(t + 1) = i(t) + (t) i(t)hi(t)

(13)

where

i(0) = log R^

(14)

and where parameters

hcio(mt)p, uit=ed1by: :

:

n

is

another

set

of

modi

able

h
hi(t + 1) = hi(t) + ki(t)

ih
(t) 1 ; ki(t)

i(t)i+

(15)

where
x]+ is

xkii(ft)x

i>s

the ith component
0, else 0. Algorithm

of K(t)
K1 thus

(see (10)) and requires mem-

ory for 2n additional
beyond the memory

numbers (n for
for the original

ni(tn)uamnbdenrsfoforrhi^((tt))).

This may seem excessive, but of course it is far less than

required by the LS and Kalman methods for storing the

n n matrix P(t). I doubt that there is any way to e ec-

tively approximate the gradient descent (12) while storing

fewer than 2n additional numbers, but the next algorithm,

K2, shows that there are other ways of approximating P(t)

that require storing only n additional numbers.

KNo1tTdeohteehs,aKtb2uPta(litgt)oarcdiatahnpmtbseathprepelraotix(eitdm) aitntoeasthtpoeitia(etlxl)ypeudcsitineedgresnqit(utwa)raeayds.
error by

E 2(t)

=

E

h
y(t) ;

(t)T ^(t)i2

= E h (t)T (t) + (t) ; (t)T ^(t)i2

=

E

h
(t)T

(t) ; ^(t)] + (t)i2

= E h (t)T (t) + (t)i2

(where (t) = (t) ; ^(t)) = E h (t)T (t)i2 + (t)2 + 2 (t) (t)T (t)

E (t)T (t) (t)T (t) + R + 0

= (t)T E (t) (t)T (t) + R

= R + (t)T P(t) (t)

and for a diagonal P(t) with diagonal entries pii(t), = R + X pii(t) 2i (t)
i

In other words, the linear prediction of

p2ii((tt))

are the coe
from 2(t).

cients of the best An approximation

of these coe cients can be formed by using any incremen-

tal regression method. The K2 algorithm uses an NLMS

method to do this. That is, it uses (10), (11), and (14),

and updates the i(t) by

i(t+1) =

i(t)+ 1

+ P2ij(t)4j(t)h

2(t);R^;X p^ii(t)
i

i
2i (t)
(16)

The K2 algorithm is strikingly similar in concept to that developed by Sanger, Matheus, and Sutton for a different purpose (Sanger, 1991 Sutton & Matheus, 1991 Sanger, Sutton & Matheus, 1992). Like K2, their method used the coe cients of a regression of the squared error onto the squared inputs to identify relevant inputs. In their method, however, the inputs identi ed as relevant in this way were not given higher learning rates, but rather were favored for being combined multiplicatively to produce higher order terms in a polynomial approximation of a nonlinear system.

Finally, the IDBD algorithm (Sutton, 1992) is the im-

mediate ancester of the K1 algorithm. In short, K1 is to

NLMS as IDBD is to LMS. The IDBD algorithm is de ned

by

ki(t) = e i(t+1) i(t)

(17)

with i(t) de ned by (13), with

i(0)

=

log

1
n

(18)

and

h
hi(t + 1) = hi(t) 1 ; ki(t)

i(t)i+ + ki(t)

(t)

(20)

The derivation of (13) and (20) from the gradient descent equation (12) is given in (Sutton, 1992). It is similar to that for K1 given in the appendix.

The Computational Experiment

A simple computational experiment was performed

to assess the asymptotic performance of the seven al-

gdToohrmeithivnmaprsui.atbslTeigshneawldsitihmie(mnts)eiaownnerzoeefrionthdaeenpdseynsudtneenmitt

was n = 20.
gaussian ranvariance (i.e.,

E (t) (t)T = I). The observation noise was also mean-

zero gaussian with unit variance (R = 1). The rst ve

components of the drift vector d(t) were mean-zero gaus-

sian with unit variance (uncorrelated), while the remain-

ing 15 components were always zero (the drift covariance

matrix Q = E d(t)d(t)T was all 0 except for the rst

5 diagonal entries, which were 1). This means that only

the rst 5 components of changed over time, and thus

asymptotically only the rst 5 input signals were relevant.

The other 15 inputs signals could and should be ignored

for optimal tracking. All random variables ( (t), (t), and

d(t)) were chosen independently. The initial parameter

vector was (0) = (0 : : : 0)T .

On this problem it su ces to perform a single long run for each algorithm and measure its asymptotic tracking performance. In this experiment, each algorithm was run for 20,000 time steps to get past any transients, and then for another 10,000 time steps. The square root of the
mean-squared error (RMSE) between y(t) and ^(t)T (t)
over the 10,000 time steps was used as the measure of the performance of the algorithm. The seed of the random number generator was reset at the beginning of each run, so that each algorithm experienced the exact same
sequence of y(t) and (t).

The algorithms were run at each of a range of values of their free parameter ( , , or ). The results are plotted for each algorithm individually in separate panels of gure 1. Figure 2 combines the results from all seven algorithms by appropriately rescaling their free parameters into one range. For algorithms LMS, NLMS, K1, K2, and IDBD, results are shown for most of the feasible range of . For values slightly higher than those shown, these algorithms became unstable. The standard errors in all cases are smaller than the symbols marking the data points, and thus almost all perceptible di erences are signi cant.

Asymptotic RMSE

LMS NLMS
30
25 LMS 20 NLMS
15
10
5
0 .0 .5 1.0 1.5
µ

2.0

Asymptotic RMSE

12 10
8 6 4 2 0 .000

IDBD

.0005

µ

.001

.0015

12 K2
10 8 6 4 2 0 .00 .005 .01 .015 .02 .025 .03
µ

Asymptotic RMSE

12 10
8 6 4 2 0 .000

K1
.002 .004 .006
µ

.008

Asymptotic RMSE

Asymptotic RMSE

10
5
0 0.01

LS
1


100

Asymptotic RMSE

Kalman
10
5
0 012


Figure 1: Detailed performance data on all algorithms as a function of their free parameter. Plotted on the vertical axis of each graph is the square root of the mean squared error averaged over the last 10,000 time steps of a long run. Each data point represents a di erent run using a di erent value for the algorithm's free parameter ( , , or ).

Asymptotic RMSE Best RMSE

30 LMS NLMS
25 Kalman LS
20 IDBD K2
15 K1
10

12 10
8 6 4 2

5
0 0 0.2 0.4 0.6 0.8 1
µ//
Figure 2: Performance comparison of all algorithms, combining all the data from gure 1 in one graph. Each algorithm's free parameter ( , , or ) was rescaled such that the values for which data was obtained lay between 0 and 1.

0 LMS NLMS LS Kalman K1

K2 IDBD

Figure 3: Summary performance comparison of all algorithms. The height of each column is the performance of the corresponding algorithm at the best value of its free parameter.

All the algorithms (except Kalman) are only weakly dependent upon the value of their free parameter when it is near its best value. It is thus reasonable to summarize the performance of the algorithms by taking their performance at their optimal parameter values, as is done in gure 3. The relative performance of the classical methods is consistent with expectations. The Kalman lter performed best, much better than the least-squares method (LS), which was much better that NLMS, which in turn was slightly better than LMS. The three DLR methods, however, all performed signi cantly better than least squares, and approached the performance of the Kalman lter. Moreover, inspection of gures 1 and 2 reveals that the Kalman algorithm performed better than the DLR algorithms only within a very narrow range of its free parameter . If the
Kalman algorithm's estimate Q^ was even slightly inaccu-
rate, then K1, K2, and even IDBD performed better than the Kalman algorithm. This is only one experiment, and further work is needed, but these results do suggest that in practice the DLR methods may perform better than the LS and Kalman algorithms.

How could it be that the DLR algorithms outperform

least-squares and Kalman lter methods? The DLR algo-

kirtinythowomflsetddhgoeennooeftwQrem.queIntirheaododdrsittiasioknaet,atldehvaeasntctoaamngepouortfdaaetnrioyonsfaplneccloieamslspp(lresioxeer-

table 1 for a summary of the computational complexity of

the algorithms). These limitations should put the DLR

methods at a signi Kalman methods. they can use only

aBcaecncratuuddseei,satOdhve(annD)tLamRgoedmreeleltaohtfoivdtehsetaorseyLsSOte(amnn)'ds,

time variation, but unlike LS and Kalman, they adapt

that model to the actual system. The LS and Kalman

algorithms on the other hand use a xed model of the

system's time variation. If that model is exactly correct,

then the Kalman lter is optimal, but if the model is even

slightly special

sitnateursroarn,dthceanKbaelmbaenataenvdenLSbyalOgo(nri)thmmesthhoadvse.

no

This perspective suggests that it may be possible to
design an O(n2) method that is adaptive, that would ap-
proach the performance of the optimal Kalman lter and outperform all other methods. This is an interesting possibility for future research. We note, however, that for many problems such a method would be ruled out simply
because of its O(n2) complexity.
Acknowledgements
The author wishes to thank Chris Matheus, Ron Williams, Oliver Selfridge, Chris Atkeson, and Judy Franklin for helpful discussions of these ideas, and additionally Hamid Benbrahim, Chris Matheus, Judy Franklin, Oliver Selfridge, and Marty Hiller for reading and commenting on an earlier draft of the paper.

Computational Complexity

Algorithm Memory Adds & Mults

LMS n

4n

NLMS

n

6n

K2 2n

16n

IDBD 3n

13n

K1 3n

17n

LS Kalman

1 2

n2

+

n

1 2

n2

+

n

2:5n2 + 8:5n 2:5n2 + 8:5n

TABLE 1 Approximate computational complexity of the algorithms

References
oanGfnulduacuclpkeCs,-ysoMpcnhef.ocAeilro.e,gcniGclceealaalorupnfteihtnrhisgeperre,CacPttoei.gvTsne.isi,nt.i&vaPedraSoScpucettieitevdoneinnc,negesRStw.ooScfo.irtekh(t1sey:9.F9Co2ou)mrAtpedeuantpatthtaiotAinonan-l Goodwin, G.C. & Sin, K.S. (1984) Adaptive Filtering Prediction and Control. Englewood Cli s, NJ: Prentice-Hall, 1984. Jraatceobasd,aRp.tAat.i(o1n9.8N8)euInrcarleNaseetdworarktess1o,f2c9o5n{v3e0rg7e. nce through learning MKeasttheenm, Hat.i(c1a9l 5S8t)aAtisctcieclser2a9t,ed41s{to5c9h.astic approximation. Annals of Lnp2e,reoteDwb, .loYSer.m.kT&sao.nLuIdnirpecAtpozmdnkvvyaae,nnnEntc,ideoR.sn, .ai1Pnl6. p8N(a1{et91ut97er07ra),nlMPIcnlroaafrsocgsrtaiimcneaaKrltsciaoohunnafmraParacrtontiecnrec.isisastilincasgnodSfysnspeteueemrcahsl Sanger T.D. (1991) Basis-function trees as a generalization of local vRmvaa.arPnin.ca,neb.sMleionsoedlNeycetuJir.oEanl.,mITneoftouhrromedtaszktifyoornDfP.uSrn.o,cctEeiosdsnsi.n,agp7p0Sr0yo{sx7tie0mm6a,stMio3on,r.gLaiIpnnp:KmaAaundfn-StIinaofnnogroemrf,asTptia.oDrns.e,PSpruootcltyeonsnsoimRngi.aSSl.,yaMsptpearmtohxseium4s,aCMti.ooJnr.gs(.a1n9In9K2Aa)udIftvmearnaancteinvs.eicnonNsetururac-l Smtiuoetnnttaoalnl,CvRoenr.Ssfie.or(ne1n9oc9fe2d)oenAltdAaa-rbptaitrin-cdgiealbltiaIan.stebPlylrioggcerenaedcdeiien.ngts doefsctehnet:TAennthinNcrae-LSfeueaatttruonrnienRcgo.,Sn2.s,0t8Mr{ua2ct1thi2oe,nuM.s oCPr.grJoa.cn(.1K9E9ai1ug)fhmLthaeanIrnnn.tinl.g pWoolyrnkosmhoipalofunncMtiaocnhsibnye Williams, R.J. & Zipser, D. (1989) Experimental analysis of the real-time recurrent learning algorithm. Connection Science 1, 87{ 111.

Appendix: Derivation of K1

This appendix presents a derivation of the K1 algorithm, (13) and (15), from (12). First we de ne some useful intermediate terms and derivatives:

@ 2(t) @ ^i (t)

=

2

(t)

@ (t) @ ^i (t)

=

2

(t)

@

@ ^i(t)

h
y(t)

;

^(t)T

i
(t)

=

;2

(t)

X
j

@ @

^j(t) ^i(t)

j(t) = ;2 (t)

i(t)

(A1)

@ (t) @i

=

@ @

i

h
y(t)

;

^(t)T

i
(t)

=

;

X
j

@ ^j @

(t)
i

j(t)

;

@ ^i(t) @i

i(t)

(A2)

The approximation above

omfa^r.y

e ect That

of is,

changing we assume

i

is reasonable should be on

itnhesoitfharcaosmtphoenpernit-

@ ^j (t) @i

0

for i 6= j

(A3)

In these equations, the partial derivitive with respect to

dei riwvaitthivoeutwaithtimreespiencdtexto

should an in

be interpretted as the nitesimal change in i

at all time steps. A similar technique is used in gradientdescent analyses of recurrent connectionist networks (c.f., e.g., Williams & Zipser, 1989). Some further intermediate computations will be needed:
D(t) d=ef R^ + (t)T P^(t) (t) = R^ + X e j(t+1) j2(t) (A4)
j

ki(t) d=ef e i(t+1) i(t)D;1(t)

(A5)

@D(t) @i

=

X
j

@e

j (t+1)
@i

2j (t)

e t( )i(t+1) i2

(A6)

@D;1(t) @i

=

;D;2(t)

@D(t) @i

;D;1(t)ki(t) i(t) (A7)

@ki(t) @i

= = =

hh@k@@iei(ht@)ie(+ti+i(1et)+i1()it(+ti1)()Dt);iD(1t;()tD1)(;t+)1ie(t)ik(ti+(t1))

i(t) @
i

D;1(t) @i

i

i(t)

hi

= ki(t) 1 ; ki(t) i(t)

(A8)

Now we are ready to expand (12):

i(t + 1)
=
=

i(t)

;

1 2

@ 2(t) @i

i(t)

;

1 2

X @ 2(t) @^j(t) j @ ^j(t) @ i

i(t)

;

1 2

@ 2(t) @^i(t) @^i(t) @ i

i(t) + (t) i(t)hi(t)

(A9)

using (A1) and (A3), and where hi(t) is an approximation

to

,@ ^i(t)
@i

derived

as

follows

hi(t + 1)

=

@^i(t + 1)

@ @

@h
i

i
^i(t)

+

ki

(t)

i
(t)

hi(t)

+

ki(t)

@ @

(t)
i

+

(t)

@ki(t) @i

h

i

hi(t) ; ki(t)hi(t) i(t) + (t)ki(t) 1 ; ki(t) i(t)

h ih i

= hi(t) 1 ; ki(t) i(t) + (t)ki(t) 1 ; ki(t) i(t)

h ih i

= hi(t) + (t)ki(t) 1 ; ki(t) i(t) (A10)

using (3), (A2), and (A8). After adding a positive-

bounding operation, this is the original update rule for

shaim(t)e,

(15), while as (13).

the

derived

update

(A9)

for

i(t) is the

Q.E.D.

Tramenhafdeidsrepeinastgcoaeeewbrqerua-esataytckpioosernswrseeetc(rt8eve)edc,rh.s(ai1on5ng),eod(f1st7lh)igeahontrldyig,(iaAnna1dl0)pm.uibTnlohicreacttoiitorlrnee.cintSi8oot/mnh1see5w/lei9rnrs5eet]

