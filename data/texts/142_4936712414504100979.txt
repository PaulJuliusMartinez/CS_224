From: AAAI-02 Proceedings. Copyright © 2002, AAAI (www.aaai.org). All rights reserved. 

Reinforcement  Learning  of  Coordination  in  Cooperative  Multi-agent  Systems

Spiros Kapetanakis and Daniel Kudenko

{spiros, kudenko}@cs.york.ac.uk

Department of Computer Science

University of York, Heslington, York

YO10 5DD, U.K.

Abstract

We report on an investigation of reinforcement learning tech-
niques for the learning of coordination in cooperative multi-
agent systems. Speciﬁcally, we focus on a novel action selec-
tion strategy for Q-learning (Watkins 1989). The new tech-
nique is applicable to scenarios where mutual observation of
actions is not possible.
To date, reinforcement learning approaches for such indepen-
dent agents did not guarantee convergence to the optimal joint
action in scenarios with high miscoordination costs. We im-
prove on previous results (Claus & Boutilier 1998) by demon-
strating empirically that our extension causes the agents to
converge almost always to the optimal joint action even in
these difﬁcult cases.

Introduction

(Boutilier 1999),

Learning to coordinate in cooperative multi-agent systems
is a central and widely studied problem, see, for example
(Lauer & Riedmiller 2000),
(Claus &
Boutilier 1998), (Sen & Sekaran 1998), (Sen, Sekaran, &
Hale 1994), (Weiss 1993). In this context, coordination is
deﬁned as the ability of two or more agents to jointly reach a
consensus over which actions to perform in an environment.
We investigate the case of independent agents that cannot
observe one another’s actions, which often is a more realis-
tic assumption.

In this investigation, we focus on reinforcement learn-
ing, where the agents must learn to coordinate their actions
through environmental feedback. To date, reinforcement
learning methods for independent agents (Tan 1993), (Sen,
Sekaran, & Hale 1994) did not guarantee convergence to
the optimal joint action in scenarios where miscoordination
is associated with high penalties. Even approaches using
agents that are able to build predictive models of each other
(so-called joint-action learners) have failed to show con-
vergence to the optimal joint action in such difﬁcult cases
(Claus & Boutilier 1998). We investigate variants of Q-
learning (Watkins 1989) in search of improved convergence
to the optimal joint action in the case of independent agents.
More speciﬁcally, we investigate the effect of the estimated
value function in the Boltzmann action selection strategy for

Copyright c 2002, American Association for Artiﬁcial Intelli-
gence (www.aaai.org). All rights reserved.

326    AAAI-02 

Q-learning. We introduce a novel estimated value function
and evaluate it experimentally on two especially difﬁcult co-
ordination problems that were ﬁrst introduced by Claus &
Boutilier in 1998: the climbing game and the penalty game.
The empirical results show that the convergence probabil-
ity to the optimal joint action is greatly improved over other
approaches, in fact reaching almost 100%.

Our paper is structured as follows: we ﬁrst introduce the
aforementioned common testbed for the study of learning
coordination in cooperative multi-agent systems. We then
introduce a novel action selection strategy and discuss the
experimental results. We ﬁnish with an outlook on future
work.

Single-stage coordination games

agents

A common testbed for studying the problem of multi-agent
coordination is that of repeated cooperative single-stage
games (Fudenberg & Levine 1998).
In these games, the
agents have common interests i.e. they are rewarded based
on their joint action and all agents receive the same reward.
In each round of the game, every agent chooses an action.
These actions are executed simultaneously and the reward
that corresponds to the joint action is broadcast to all agents.
A more formal account of this type of problem was given
by Claus & Boutilier in 1998. In brief, we assume a group
of
each of which have a ﬁnite set of
which is known as the agent’s action
individual actions
space. In this game, each agent
chooses an individual
action from its action space to perform. The action choices
make up a joint action. Upon execution of their actions all
agents receive the reward that corresponds to the joint ac-
tion. For example, Table 1 describes the reward function for
a simple cooperative single-stage game. If agent 1 executes
and agent 2 executes action , the reward they re-
action
ceive is 5. Obviously, the optimal joint-action in this simple
game is
as it is associated with the highest reward of
10.

Our goal is to enable the agents to learn optimal coordina-
tion from repeated trials. To achieve this goal, one can use
either independent or joint-action learners. The difference
between the two types lies in the amount of information they
can perceive in the game. Although both types of learners
can perceive the reward that is associated with each joint ac-
tion, the former are unaware of the existence of other agents

Agent 2

Agent 1

3
0

5
10

Table 1: A simple cooperative game reward function.

whereas the latter can also perceive the actions of others. In
this way, joint-action learners can maintain a model of the
strategy of other agents and choose their actions based on
the other participants’ perceived strategy. In contrast, inde-
pendent learners must estimate the value of their individual
actions based solely on the rewards that they receive for their
actions. In this paper, we focus on individual learners, these
being more universally applicable.

In our study, we focus on two particularly difﬁcult coor-
dination problems, the climbing game and the penalty game.
These games were introduced by Claus & Boutilier in 1998.
This focus is without loss of generality since the climbing
game is representative of problems with high miscoordina-
tion penalty and a single optimal joint action whereas the
penalty game is representative of problems with high misco-
ordination penalty and multiple optimal joint actions. Both
games are played between two agents. The reward functions
for the two games are included in Tables 2 and 3:

Agent 1

11
-30
0

-30
7
0

0
6
5

Agent 2

Table 2: The climbing game table.

In the climbing game, it is difﬁcult for the agents to con-
because of the nega-
verge to the optimal joint action
tive reward in the case of miscoordination. For example, if
agent 1 plays
and agent 2 plays , then both will receive
a negative reward of -30.
Incorporating this reward into
the learning process can be so detrimental that both agents
tend to avoid playing the same action again.
In contrast,
when choosing action , miscoordination is not punished so
severely. Therefore, in most cases, both agents are easily
tempted by action . The reason is as follows: if agent 1
plays
to get a posi-
,
tive reward (6 and 5 respectively). Even if agent 2 plays
the result is not catastrophic since the reward is 0. Simi-
larly, if agent 2 plays , whatever agent 1 plays, the result-
ing reward will be at least 0. From this analysis, we can
see that the climbing game is a challenging problem for the
study of learning coordination. It includes heavy miscoor-
dination penalties and “safe” actions that are likely to tempt
the agents away from the optimal joint action.

, then agent 2 can play either

or

Another way to make coordination more elusive is by in-
cluding multiple optimal joint actions. This is precisely what
happens in the penalty game of Table 3.

In the penalty game, it is not only important to avoid the
and

miscoordination penalties associated with actions

Agent 1

10
0

0
2
0

0
10

Agent 2

Table 3: The penalty game table.

. It is equally important to agree on which optimal joint

and

. If agent 1 plays

action to choose out of
expecting agent 2 to also play so they can receive the max-
(perhaps expecting
imum reward of 10 but agent 2 plays
agent 1 to play
so that, again, they receive the maximum
reward of 10) then the resulting penalty can be very detri-
mental to both agents’ learning process. In this game,
is
the “safe” action for both agents since playing
is guaran-
teed to result in a reward of 0 or 2, regardless of what the
other agent plays. Similarly with the climbing game, it is
clear that the penalty game is a challenging testbed for the
study of learning coordination in multi-agent systems.

Reinforcement learning

A popular technique for learning coordination in coopera-
tive single-stage games is one-step Q-learning, a reinforce-
ment learning technique. Since the agents in a single-stage
game are stateless, we need a simple reformulation of the
general Q-learning algorithm such as the one used by Claus
& Boutilier. Each agent maintains a Q value for each of its
provides an estimate of the
actions. The value
usefulness of performing this action in the next iteration of
the game and these values are updated after each step of the
game according to the reward received for the action. We
apply Q-learning with the following update function:

where
that corresponds to choosing this action.

is the learning rate

and is the reward

In a single-agent learning scenario, Q-learning is guaran-
teed to converge to the optimal action independent of the
action selection strategy. In other words, given the assump-
tion of a stationary reward function, single-agent Q-learning
will converge to the optimal policy for the problem. How-
ever, in a multi-agent setting, the action selection strategy
becomes crucial for convergence to any joint action. A ma-
jor challenge in deﬁning a suitable strategy for the selection
of actions is to strike a balance between exploring the use-
fulness of moves that have been attempted only a few times
and exploiting those in which the agent’s conﬁdence in get-
ting a high reward is relatively strong. This is known as the
exploration/exploitation problem.

The action selection strategy that we have chosen for our
research is the Boltzmann strategy (Kaelbling, Littman, &
chooses an action to
Moore 1996) which states that agent
perform in the next iteration of the game with a probability
that is based on its current estimate of the usefulness of that

AAAI-02    327

action, denoted by

1 :

In the case of Q-learning, the agent’s estimate of the useful-
ness of an action may be given by the Q values themselves,
an approach that has been usually taken to date.

We have concentrated on a proper choice for the two pa-
rameters of the Boltzmann function: the estimated value and
the temperature. The importance of the temperature lies in
that it provides an element of controlled randomness in the
action selection: high values in temperature encourage ex-
ploration since variations in Q values become less important.
In contrast, low temperature values encourage exploitation.
The value of the temperature is typically decreased over time
from an initial value as exploitation takes over from explo-
ration until it reaches some designated lower limit. The three
important settings for the temperature are the initial value,
the rate of decrease and the number of steps until it reaches
its lowest limit. The lower limit of the temperature needs to
be set to a value that is close enough to 0 to allow the learn-
ers to converge by stopping their exploration. Variations in
these three parameters can provide signiﬁcant difference in
the performance of the learners. For example, starting with a
very high value for the temperature forces the agents to make
random moves until the temperature reaches a low enough
value to play a part in the learning. This may be beneﬁcial
if the agents are gathering statistical information about the
environment or the other agents. However, this may also
dramatically slow down the learning process.

It has been shown (Singh et al. 2000) that convergence
to a joint action can be ensured if the temperature function
adheres to certain properties. However, we have found that
there is more that can be done to ensure not just convergence
to some joint action but convergence to the optimal joint ac-
tion, even in the case of independent learners. This is not just
in terms of the temperature function but, more importantly,
in terms of the action selection strategy. More speciﬁcally, it
turns out that a proper choice for the estimated value func-
tion in the Boltzmann strategy can signiﬁcantly increase the
likelihood of convergence to the optimal joint action.

FMQ heuristic

In difﬁcult coordination problems, such as the climbing
game and the penalty game, the way to achieve convergence
to the optimal joint action is by inﬂuencing the learners to-
wards their individual components of the optimal joint ac-
tion(s). To this effect, there exist two strategies: altering the
Q-update function and altering the action selection strategy.
Lauer & Riedmiller (2000) describe an algorithm for
multi-agent reinforcement learning which is based on the
optimistic assumption. In the context of reinforcement learn-
ing, this assumption implies that an agent chooses any ac-
tion it ﬁnds suitable expecting the other agent to choose the

1In (Kaelbling, Littman, & Moore 1996), the estimated value is

introduced as expected reward (ER).

328    AAAI-02 

best match accordingly. More speciﬁcally, the optimistic as-
sumption affects the way Q values are updated. Under this
assumption, the update rule for playing action
deﬁnes that
is only updated if the new value is greater than the

current one.

Incorporating the optimistic assumption into Q-learning
solves both the climbing game and penalty game every time.
This fact is not surprising since the penalties for miscoor-
dination, which make learning optimal actions difﬁcult, are
neglected as their incorporation into the learning tends to
lower the Q values of the corresponding actions. Such low-
ering of Q values is not allowed under the optimistic as-
sumption so that all the Q values eventually converge to
the maximum reward corresponding to that action for each
agent. However, the optimistic assumption fails to converge
to the optimal joint action in cases where the maximum re-
ward is misleading, e.g., in stochastic games (see experi-
ments below). We therefore consider an alternative: the Fre-
quency Maximum Q Value (FMQ) heuristic.

Unlike the optimistic assumption, that applies to the Q
update function, the FMQ heuristic applies to the action se-
lection strategy, speciﬁcally the choice of
the
function that computes the estimated value of action . As
mentioned before, the standard approach is to set

, i.e.

. Instead, we propose the following modiﬁcation:

where:

denotes the maximum reward encoun-

tered so far for choosing action .

➀

➁

is the fraction of

times that
has been received as a reward for action

over the times that action

has been executed.

➂ is a weight that controls the importance of the

FMQ heuristic in the action selection.

Informally, the FMQ heuristic carries the information of
how frequently an action produces its maximum correspond-
ing reward. Note that, for an agent to receive the maxi-
mum reward corresponding to one of its actions, the other
agent must be playing the game accordingly. For example,
in the climbing game, if agent 1 plays action which is agent
1’s component of the optimal joint-action
but agent 2
doesn’t, then they both receive a reward that is less than the
then the two agents receive 0
maximum. If agent 2 plays
and, provided they have already encountered the maximum
rewards for their actions, both agents’ FMQ estimates for
their actions are lowered. This is due to the fact that the fre-
quency of occurrence of maximum reward is lowered. Note
that setting the FMQ weight
to zero reduces the estimated
value function to:

.

In the case of independent learners, there is nothing other
than action choices and rewards that an agent can use to learn
coordination. By ensuring that enough exploration is per-
mitted in the beginning of the experiment, the agents have a
good chance of visiting the optimal joint action so that the
FMQ heuristic can inﬂuence them towards their appropriate
individual action components. In a sense, the FMQ heuristic

deﬁnes a model of the environment that the agent operates
in, the other agent being part of that environment.

Experimental results

This section contains our experimental results. We com-
pare the performance of Q-learning using the FMQ heuris-
tic against the baseline experiments i.e. experiments where
the Q values are used as the estimated value of an action in
the Boltzmann action selection strategy. In both cases, we
use only independent learners. The comparison is done by
keeping all other parameters of the experiment the same, i.e.
using the same temperature function and experiment length.
The evaluation of the two approaches is performed on both
the climbing game and the penalty game.

Temperature settings
Exponential decay in the value of the temperature is a pop-
ular choice in reinforcement learning. This way, the agents
perform all their learning until the temperature reaches some
lower limit. The experiment then ﬁnishes and results are col-
lected. The temperature limit is normally set to zero which
may cause complications when calculating the action selec-
tion probabilities with the Boltzmann function. To avoid
such problems, we have set the temperature limit to 1 in our
experiments2.

In our analysis, we use the following temperature func-

tion:

and initial temperature

is the number of iterations of the game so far,

where
is the parameter that controls the rate of exponential decay
is the value of the temperature at the begin-
and
ning of the experiment. For a given length of the experiment
the ap-
is automatically derived. Varying
propriate rate of decay
the parameters of the temperature function allows a detailed
speciﬁcation of the temperature. For a given
,
combina-
we experimented with a variety of
tions and found that they didn’t have a signiﬁcant impact
on the learning in the baseline experiments. Their impact
is more signiﬁcant when using the FMQ heuristic. This is
at a very high value means that
because setting
the agent makes random moves in the initial part of the ex-
periment. It then starts making more knowledgeable moves
(i.e. moves based on the estimated value of its actions) when
the temperature has become low enough to allow variations
in the estimated value of an action to have an impact on the
probability of selecting that action.

Evaluation on the climbing game
and
The climbing game has one optimal joint action
. We use the
two heavily penalised actions
settings
from 500
. Figure 1 depicts
to 2000. The learning rate
the likelihood of convergence to the optimal joint action in
the baseline experiments and using the FMQ heuristic with
. The FMQ heuristic outperforms the

and vary
is set to

and

and

2This is done without loss of generality.

baseline experiments for both settings of
, the
FMQ heuristic converges to the optimal joint action almost
always even for short experiments.

. For

1

0.8

0.6

0.4

0.2

l

a
m

i
t
p
o
 
o
t
 
e
c
n
e
g
r
e
v
n
o
c
 
f
o
 
d
o
o
h

i
l

e
k

i
l

0

500

FMQ (c=10)
FMQ (c=5)
FMQ (c=1)
baseline

750

1000

1250

1500

1750

2000

number of iterations

Figure 1: Likelihood of convergence to the optimal joint ac-
tion in the climbing game (averaged over 1000 trials).

for all values of

Evaluation on the penalty game
The penalty game is harder to analyse than the climbing
game. This is because it has two optimal joint actions
and
. The extent to which the
optimal joint actions are reached by the agents is affected
severely by the size of the penalty. However, the perfor-
mance of the agents depends not only on the size of the
penalty
but also on whether the agents manage to agree
on which optimal joint action to choose. Table 2 depicts the
performance of the learners for
for the baseline ex-
periments and with the FMQ heuristic for

.

1

0.8

0.6

0.4

0.2

l

a
m

i
t

p
o

 

o

t
 

e
c
n
e
g
r
e
v
n
o
c
 
f

o

 

d
o
o
h

i
l

e
k

i
l

0

500

FMQ (c=1)
baseline

750

1000

1250

1500

1750

2000

number of iterations

Figure 2: Likelihood of convergence to the optimal joint ac-
tion in the penalty game
(averaged over 1000 trials).

As shown in Figure 2,

the performance of the FMQ
heuristic is much better than the baseline experiment. When
, the reason for the baseline experiment’s failure is
not the existence of a miscoordination penalty. Instead, it
is the existence of multiple optimal joint actions that causes
the agents to converge to the optimal joint action so infre-
quently. Of course, the penalty game becomes much harder

AAAI-02    329

for greater penalty. To analyse the impact of the penalty
on the convergence to optimal, Figure 3 depicts the likeli-
hood that convergence to optimal occurs as a function of
the penalty. The four plots correspond to the baseline ex-
periments and using Q-learning with the FMQ heuristic for

,

and

.

1

0.8

0.6

0.4

0.2

l

a
m

i
t
p
o
 
o
t
 
e
c
n
e
g
r
e
v
n
o
c
 
f
o
 
d
o
o
h

i
l

e
k

i
l

0

-100

FMQ (c=1)
FMQ (c=5)
FMQ (c=10)
baseline

-80

-60

-40

-20

0

penalty k

Figure 3: Likelihood of convergence to the optimal joint ac-
tion as a function of the penalty (averaged over 1000 trials).

From Figure 3, it is obvious that higher values of the FMQ
weight
perform better for higher penalty. This is because
there is a greater need to inﬂuence the learners towards the
optimal joint action when the penalty is more severe.

Further experiments

We have described two approaches that perform very well
on the climbing game and the penalty game: FMQ and the
optimistic assumption. However, the two approaches are
different and this difference can be highlighted by looking at
alternative versions of the climbing game. In order to com-
pare the FMQ heuristic to the optimistic assumption (Lauer
& Riedmiller 2000), we introduce a variant of the climbing
game which we term the partially stochastic climbing game.
This version of the climbing game differs from the origi-
nal in that one of the joint actions is now associated with
a stochastic reward. The reward function for the partially
stochastic climbing game is included in Table 4.

Agent 1

11
-30
0

-30
14/0

0

0
6
5

Agent 2

Table 4: The partially stochastic climbing game table.

Joint action

yields a reward of 14 or 0 with proba-
bility 50%. The partially stochastic climbing game is func-
tionally equivalent to the original version. This is because,
action, they re-
if the two agents consistently choose their
ceive the same overall value of 7 over time as in the original
game.

330    AAAI-02 

Using the optimistic assumption on the partially stochas-
tic climbing game consistently converges to the suboptimal
. This because the frequency of occur-
joint action
rence of a high reward is not taken into consideration at all.
In contrast, the FMQ heuristic shows much more promise
in convergence to the optimal joint action.
It also com-
pares favourably with the baseline experimental results. Ta-
bles 5, 6 and 7 contain the results obtained with the base-
line experiments, the optimistic assumption and the FMQ
In all cases,
heuristic for 1000 experiments respectively.
,
the parameters are:

,

and, in the case of FMQ,

.

212
0
0

0
12
0

3
289
381

Table 5: Baseline experimental results.

0
0
0

1000

0

0

0
0
0

Table 6: Results with optimistic assumption.

988
0
0

0
4
7

0
0
1

Table 7: Results with the FMQ heuristic.

The ﬁnal topic for evaluation of the FMQ heuristic is to
analyse the inﬂuence of the weight
on the learning. In-
formally, the more difﬁcult the problem, the greater the need
for a high FMQ weight. However, setting the FMQ weight at
too high a value can be detrimental to the learning. Figure 4
contains a plot of the likelihood of convergence to optimal
in the climbing game as a function of the FMQ weight.

From Figure 4, we can see that setting the value of the
FMQ weight above 15 lowers the probability that the agents
will converge to the optimal joint action. This is because,
by setting the FMQ weight too high, the probabilities for
action selection are inﬂuenced too much towards the action
with the highest FMQ value which may not be the optimal
joint action early in the experiment.
In other words, the
agents become too narrow-minded and follow the heuristic
blindly since the FMQ part of the estimated value function
overwhelms the Q values. This property is also reﬂected in
the experimental results on the penalty game (see Figure 3)
where setting the FMQ weight to 10 performs very well in
but there is
difﬁcult experiments with
a drop in performance for easier experiments. In contrast,
the likelihood of convergence to the optimal joint
for
action in easier experiments is signiﬁcantly higher than in
more difﬁcult ones.

l

a
m

i
t
p
o
 
o
t
 
e
c
n
e
g
r
e
v
n
o
c
 
f
o
 
d
o
o
h

i
l

e
k

i
l

1

0.8

0.6

0.4

0.2

0

10

20

30

40

50

60

70

80

90

100

FMQ weight

Figure 4: Likelihood of convergence to optimal in the climb-
ing game as a function of the FMQ weight (averaged over
1000 trials).

Limitations

The FMQ heuristic performs equally well in the partially
stochastic climbing game and the original deterministic
climbing game. In contrast, the optimistic assumption only
succeeds in solving the deterministic climbing game. How-
ever, we have found a variant of the climbing game in which
both heuristics perform poorly: the fully stochastic climbing
game. This game has the characteristic that all joint actions
are probabilistically linked with two rewards. The average
of the two rewards for each action is the same as the original
reward from the deterministic version of the climbing game
so the two games are functionally equivalent. For the rest
of this discussion, we assume a 50% probability. The re-
ward function for the stochastic climbing game is included
in Table 8.

Agent 1

10/12
5/-65
5/-5

5/-65
14/0
5/-5

8/-8
12/0
10/0

Agent 2

Table 8: The stochastic climbing game table (50%).

It is obvious why the optimistic assumption fails to solve
the fully stochastic climbing game. It is for the same rea-
son that it fails with the partially stochastic climbing game.
The maximum reward is associated with joint action
which is a suboptimal action. The FMQ heuristic, although
it performs marginally better than normal Q-learning still
doesn’t provide any substantial success ratios. However, we
are working on an extension that may overcome this limita-
tion.

Outlook

We have presented an investigation of techniques that al-
lows two independent agents that are unable to sense each
other’s actions to learn coordination in cooperative single-
stage games, even in difﬁcult cases with high miscoordina-

tion penalties. However, there is still much to be done to-
wards understanding exactly how the action selection strat-
egy can influence the learning of optimal joint actions in this
type of repeated games. In the future, we plan to investigate
this issue in more detail.

Furthermore, since agents typically have a state compo-
nent associated with them, we plan to investigate how to in-
corporate such coordination learning mechanisms in multi-
stage games. We intend to further analyse the applicability
of various reinforcement learning techniques to agents with
a substantially greater action space. Finally, we intend to
perform a similar systematic examination of the applicabil-
ity of such techniques to partially observable environments
where the rewards are perceived stochastically.

References

Boutilier, C. 1999. Sequential optimality and coordina-
tion in multiagent systems.
In Proceedings of the Six-
teenth International Joint Conference on Articial Intelli-
gence (IJCAI-99), 478–485.
Claus, C., and Boutilier, C. 1998. The dynamics of rein-
forcement learning in cooperative multiagent systems. In
Proceedings of the Fifteenth National Conference on Arti-
cial Intelligence, 746–752.
Fudenberg, D., and Levine, D. K. 1998. The Theory of
Learning in Games. Cambridge, MA: MIT Press.
Kaelbling, L. P.; Littman, M.; and Moore, A. W. 1996.
Reinforcement learning: A survey. Journal of Artiﬁcial
Intelligence Research 4.
Lauer, M., and Riedmiller, M. 2000. An algorithm for dis-
tributed reinforcement learning in cooperative multi-agent
systems. In Proceedings of the Seventeenth International
Conference in Machine Learning.
Sen, S., and Sekaran, M. 1998.
coordination knowledge. JETAI 10(3):333–356.
Sen, S.; Sekaran, M.; and Hale, J. 1994. Learning to co-
ordinate without sharing information.
In Proceedings of
the Twelfth National Conference on Artiﬁcial Intelligence,
426–431.
Singh, S.; Jaakkola, T.; Littman, M. L.; and Szpes-
vari, C. 2000. Convergence results for single-step on-
policy reinforcement-learning algorithms. Machine Learn-
ing Journal 38(3):287–308.
Tan, M. 1993. Multi-agent reinforcement learning: Inde-
pendent vs. cooperative agents. In Proceedings of the Tenth
International Conference on Machine Learning, 330–337.
Watkins, C. J. C. H. 1989. Learning from Delayed Re-
wards. Ph.D. Dissertation, Cambridge University, Cam-
bridge, England.
Weiss, G. 1993. Learning to coordinate actions in multi-
agent systems.
In Proceedings of the Thirteenth Inter-
national Joint Conference on Artiﬁcial Intelligence, vol-
ume 1, 311–316. Morgan Kaufmann Publ.

Individual learning of

AAAI-02    331

