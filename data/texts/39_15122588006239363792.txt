Approaches to concurrency control in distributed data base systems*

by PHILIP A. BERNSTEIN and NATHAN GOODMAN
Harvard University Cambridge, Massachusetts

INTRODUCTION
Whenever multiple users or programs access a data base concurrently, the problem of concurrency control arises. The problem is to synchronize concurrent interactions so that each reads consistent data from the data base, writes consistent data, and is ultimately processed to completion. In a distributed data base this problem is exacerbated because a concurrency control mechanism at one site cannot instantaneously know about interactions at other sites. No fewer than 30 papers on this topic have appeared to date. Our purpose is to survey this literature, concentrating on three approaches-locking, majority consensus, and SDD-I protocols-which together subsume the bulk of the litera-
ture. **
Distributed concurrency control is complex and our treatment is, of necessity, sketchy. We urge the interested reader to consult the source materials listed in the bibiiography.
BACKGROUND
Preliminary definitions
A distributed data base management system (abbr. DDBMS) is a collection of sites interconnected by a network. Each site is a computer running a local (i.e. nondistributed) DBMS, and the network is any computer-tocomputer communication system. We assume that sites are widely dispersed geographically, so the network must employ long-distance communication media. Consequently, inter-site communication is quaiitativeiy siower and more costly than intra-site computation.
We define a data base to be a collection of data items. In practice, a data item may be a field. a record, a file, etc. This "level of granularity" is important, but does not impact concurrency control and so we leave it unspecified.
Each data item may be stored at any site in the sysiem,

and moreover each may be stored redundantly at several sites. Redundant data improves the robustness and performance of a DDBMS and must be supported by general purpose systems. Unfortunately, it is also a major source of complexity. A stored copy of a data item is called a stored ddta item. Though it is impossible for all stored copies of a data item to be identical at every instant of time, it is essential that all "converge" to the same final value. We use the term logical data item when the distinction between "data item" and "stored data item" requires emphasis.
Users interact with the DDBMS by entering transactions, by which we mean a program or on-line query that accesses the data base. Transactions have two important properties in our model-( 1) We assume they represent complete and correct computations: i.e. each transaction, if executed alone on an initially consistent data base, would terminate, output correct results, and leave the data base consistent. (2) We assume transactions obtain data from the data base by issuing Read commands to the DDBMS, and modify data by issuing Write commands. The arguments to these commands are logical data items and it is the responsibility of the DDBMS (a) to choose one stored copy of each data item for Reads, and (b) to update all stored copies of each data item for Writes. We model a transaction as a sequence of Read and Write operations paying no attention to its internal computations.
The read-set of a transaction is the set of logical data items it reads, and its write-set is the set of logical data items it writes. Two transactions conflict if the write-set of one intersects the read-set or write-set of the other. Similarly, two operations conflict if one is a Write and they operate on the same daia. ii is a fundamental theorem of concurrency control that two transactions require synchronization only if they conflict. (The converse need not be true, as we shall see in the fifth section).
Serializability

* This work was supported by the National Science Foundation under Grant
MCS-77-05314 and by the Advanced Research Projects Agency of the De-
partmenr of Defense. contract number NOOO39-78-G-0020.
*" References on these approaches are listed in the bibliography by topic.
We will limit our use of in-text references in the interest of readability.

A log is a sequence of Reads and Writes. A log is serial if the Reads and Writes for each transaction are contiguous (see Figure I). Such a log represents an execution in which no transactions execute concurrently. Since we assume each

813

From the collection of the Computer History Museum (www.computerhistory.org)

814 National Computer Conference, 1979

Serial log

Serializab1e log

Rl (X)} Trans. Tl W1(x)

R1(x)
W2(Z)}These operations can W(x) be interchanged with-
1 out affecting the R (x) computation.
2
W2(x)

Figure I-Serial and seriaIizable logs.

transaction preserves consistency if executed alone, a serial sequence of transactions also preserves consistency. A log is serializable (abbr. SR) if it is "equivalent" to a serial log, meaning that for all initial data base states it produces the same output and the same final data base state as some serial log. Since serial logs preserve consistency, and SR logs are equivalent to serial logs. SR logs preserve consistency as well.
In a DDBMS, each site processes a different log. We define a distributed log (abbr. dlog) to be a set of logs, one per site. A serial dlog is a dlog in which each component log is serial and reflects the same total ordering of transactions (i.e., all transactions are in the same relative order in all logs in which they appear). A dlog is serializable if it is equivalent to a serial dlog.
Serializability has been adopted almost universally as the correctness criterion for DBMS concurrency control; all the approaches we describe follow this convention. Alternate correctness criteria are discussed in References 20 and 22.

DISTRIBUTED LOCKING ALGORITHMS
Locking is the most widely used concurrency control technique. We describe locking first in the centralized DBMS context and then present several extensions for distributed systems.
Centralized locking
Locking synchronizes transactions by explicitly detecting and preventing conflicts. When a transaction issues a Read or Write command, the DBMS attempts to "set a lock" on the desired data item; the lock is "granted" only if no other transaction holds a conflicting lock. If the lock is not granted, the requesting transaction waits until the lock is available and can be granted.
Since the DBMS processes all Read and Write commands from every transaction, it can automatically generate lock requests for each command. This is important because it allows programmers to ignore concurrency control issues when writing their transactions.
Eswaran et aUG prove that locking is sufficient to ensure serializability provided no transaction requests new locks after releasing a lock. This usually amounts to having transactions hold all locks until they finish execution.
Since transactions are made to wait for unavailable locks, the possibility of deadlock exists (see Figure 2). Deadlocks can be detected by maintaining a deadlock graph in the DBMS. The nodes of the graph represent transactions and the arcs represent the "waiting-for" relationship; an arc is drawn from transaction T; to transaction TJ if T; is waiting

Other aspects of concurrency control
In addition to ensuring serializability, a concurrency controller must guarantee termination; it must operate robustly: and it must operate efficiently.
A transaction may fail to terminate for one of three reasons--(l) Deadlock may occur, i.e. two or more operations might be forced to wait for each other. (2) Some operation may be indefinitely postponed by an unexpected conspiracy of events. Or (3) C.vdic restart might be experienced, meaning that the transaction repeatedly reaches a blocked state and is aborted and restarted. Every concurrency control approach is susceptible to some combination of these problems.
With respect to robustness, all approaches face essentially identical problems. We discuss this issue in the sixth section.
The efficiency of a distributed concurrency controller is determined principally by how much inter-site communication it requires. Typically, message delays in long distance networks are tenths of seconds, and network capacity is the scarcest system resource. In analyzing the performance of a controller, then, it is reasonable to study its communication behavior, and ignore other aspects. We compare the performance of various approaches in the Conclusion section.

Transactions

Order in which transactions issue Reads &Writes

T1: Read(X);~

Write(y);

R1(x)

end R2(y)

Order is which DBMS executes Reads &Writes

W1(y) cannot be scheduled because it conflicts with R2(y)
W2(z) cannot be scheduled because it conflicts with R3(Z)
W3(x) cannot be scheduled because it conflicts with R1(x).
. '. OF;>iHIl ock !
Figure 2-DeadJock

From the collection of the Computer History Museum (www.computerhistory.org)

Approaches to Concurrency Control in Distributed Data Base Systems

815

W1(y) must wait for T2 to release lock on y.

primary site tends to be a bottleneck-the capacity of the primary site to process locks bounds the capacity of the entire' distributed system.

JW2(Z) must wait for T3 to
release lock on z.
W3(x) must wait for T1 to release lock on x.
Figure 3-Deadlock graph for Figure 2.
for a lock held by Tj (see Figure 3). There is a deadlock in the system if and only if the deadlock graph has a cycle.5 If a deadlock exists, some transaction in the cycle is backed out and restarted. This deadlock elimination technique can potentially lead to cyclic restart. A simple way of avoiding this problem is to always abort the "youngest" transaction involved in the deadlock. Other solutions to cyclic restart are described in the section on Conflict-Driven Restarts.
Indefinite postponement can be prevented in a locking system by processing lock requests on a first-come-firstserved basis. Other solutions are discussed in References 14 and 19.
Primary site locking
Primary site locking is a simple extension of centralized locking. One site of a DDBMS is designated to be the "primary site" and it manages all synchronization. When a transaction wishes to access data at any site, a lock is requested from the primary site. The primary site processes lock requests exactly as described in the previous section, the only difference being that lock requests come in over the network. Similarly, issues of termination are handled by the primary site exactly as in centralized locking.
Although locks are centralized at the primary site, the data base is, of course, distributed. Once a transaction is granted a lock, it may access data at whatever site has a copy. It is important that if a transaction updates a data item that has many stored copies all copies are actually updated before the lock is released; otherwise another transaction can read a copy of the data item before the first update propagated there. It is also important that read-only transactions follow the locking discipline, or else they could read inconsistent data (see Figure 4). This point is often overlooked in discussions of distributed locking, yet is important because most applications predominantly consist of readonly transactions.
The principal drawback of primary site locking is that the

Primary copy locking
Primary copy locking is an extension of primary site locking that eliminates the primary site bottleneck. For each logical data item, one copy is designated the "primary copy"; when a transaction wishes to access a data item, it locks the primary copy. Since the primary copies of different data items may be stored at different sites, no single site is primary in any sense. This eliminates the bottleneck, but introduces a new problem-deadlock detection.
To test for deadlock, all sites with some primary copy must participate. For example, Figure 5 illustrates a deadlock involving two sites which cannot be recognized locally by either site. The solution is to designate one site of the DDBMS as the "deadlock detector"; periodically each other site sends it a list of newly granted or released locks, and newly pending requests. The deadlock detector then operates as in the centralized case.
As with primary site locking, if a transaction writes into a data item, all copies must be updated before the lock is

Transactions

T1: Write(x);

T2: Read(x);

Write(y);

Read(y) ;

end end

Order in which operations executed
lock x for
WI (x) at site 1 R2(x) from site R2(y) from site lock y for
WI (y) at site 1

Database
y,Z

T2 reads inconsistent data because it sees only part of T11 s updates.
Figure 4-Read-only transactions must lock, too.

From the collection of the Computer History Museum (www.computerhistory.org)

816 National Computer Conference, 1979

T1: Read(x); Wri te (y) ;
end

Transactions T2: Read (y) ;
Write(z) ; end

T3: Read(z); Write(x) end

Order in which locks are requested at each site
site 1
lock x for Rl - lock x for W3
site 2
lock y for R2 - lock y for WI
site 3
lock z for R3 -lock z for W2

Database
x*,y y*,z
z * *denotes primary copy

is equivalent to asking whether a data item is locked, this approach is essentially a locking mechanism. 12
The analysis of conflict-driven restart yields interesting observations about termination problems. If the system responds to conflict by making the requesting transaction wait, deadlock is possible. To avoid deadlock Rosenkrantz et al. propose two mechanisms that substitute restarts for waiting. Both mechanisms require that transactions be assigned unique "timestamps" when they are submitted. Intuitively, timestamps correspond to the time a transaction was submitted, and have two important properties-timestamps assigned at any particular site must strictly increase with time, and timestamps assigned at different sites must be different. Timestamps are used to resolve conflicts as follows. In one mechanism, called the Wait-Die System, the requesting transaction waits if it has a smaller timestamp (i.e. is older); else it is restarted. In the second mechanism, called the Wound- Wait System, the requesting transaction waits if it has a larger timestamp (i.e., is younger); else the transaction it conflicts with is restarted.
Rosenkrantz et al. prove that both mechanisms avoid cyclic restart, but the details of their behavior is quite different. In a Wound-Wait system, an o~d transaction may be restarted many times, whereas in a Wait-Die system old transactions are never restarted. It is suggested that WoundWait produces fewer overall restarts, but the justification is more intuitive than analytic.

None of the  led locks can be granted, hence the system is in deadlock. But deadlock graphs at each site are acyclic:

site 1

site 2

site 3

Figure 5-Multi-site deadlock.
released: and read-only transactions must follow the locking rules. too.
Conj1ict-driven restarts
An interesting variant of primary copy locking has been described by Rosenkrantz, Lewis and Stearns.33 The mechanism, which we call conj1ict-drh'en restart, uses a model of transaction execution in which each transaction is active at only one site at a time and moves from site to site during its execution. When a transaction wishes to access a data item at a site, it tests whether it conflicts with a previous access made by an in-progress transaction. If it does conflict. one of three actions is possihle-it waits. it is restarted. or the other transaction is restarted. Since testing for conflict

MAJORITY CONSENSUS ALGORITHM
The majority consensus algorithm of R. Thomas37 was one of the first distributed concurrency control mechanisms proposed. Many of Thomas's ideas have found their way into more recent designs.
The majority consensus algorithm as presented by Thomas assumes a fully redundant data base, meaning that every site has a stored copy of every logical data item in the data base. A transaction executes at one site. Its Read commands access stored data at its site, and do so without locking or any other synchronization. Whenever the transaction issues a Write command, the name of the data item being updated and its new value are recorded in an update list: the data base itself is not modified at this time. When the transaction completes, the update list is sent to all sites and each site votes on it. If a majority of the sites vote "Yes," the transaction is accepted, and the updates are installed at all sites. Otherwise the transaction is restarted. The heart of the algorithm is the rules that determine how each site votes.
A site votes .. Yes" on transaction T if
1. The data items read by T have not been modified since T read them (the algorithm requires that a data item must be read before it can be written).
2. T does not conflict with any transaction T' that is pending at the site (T' is pending if the site has voted "Yes" hut T' h::ts nM yet heen <lccepted or rejected system-wide).

From the collection of the Computer History Museum (www.computerhistory.org)

Approaches to Concurrency Control in Distributed Data Base Systems

817

One way to meet Condition 1 is to use locking; but the majority consensus algorithm uses a timestamping technique instead.
Transactions are assigned timestamps as in "conflict driven restart," and each stored data item is tagged with the timestamp of the most recent transaction that has updated it. Also, update lists are augmented to include the name of each data item read by the transaction and its timestamp. Now, when a site receives an update list it can compare timestamps to determine whether Condition 1 holds. Since augmented updated lists specify transactions' read-sets and write-sets, Condition 2 is easily checked fl,S well.
If Condition 1 is not satisfied, the site "vetoes" the transaction and it is restarted. If (1) is satisfied but (2) is not, the site cannot vote on this transaction until the pending one is resolved. Since different sites receive update lists in different orders, they vote in--diff-er-ent--erdei's and deadlock c-oukl. result. To avoid deadlock, the site votes "No" if (1) holds, (2) does not hold, and the transaction has a larger timestamp (i.e. is younger) than the pending one. If a majority of sites vote "No," the transaction is restarted.
The voting rules ensure that. two conflicting transactions are both accepted only if one has read the other's output. Since both transactions received a majority of "Yes" votes, some site, say S, must have voted "Yes" on both transactions. Since they conflict, S must have installed one before voting on the other; this guarantees that the second read the first one's output, for otherwise S would not have voted " Yes." This is sufficient to guarantee serializability.
THE SDD-l APPROACH
The SDD-l DDBMS39-42 employs a qualitatively different approach to concurrency control. Each of the preceding methods synchronizes all conflicting Reads and Writes. However, not all conflicts can violate serializability (see Figure 6). SDD-l exploits this fact by means of two mechanisms---{;'onflict graph analysis. and timestamp-based protocols.

Transact; ons

T1: Read(x,y);

T2: Read (x) ;

Write(z);

Write(y);

end end

Order of execution (assume a non-dist.
database)

Equivalent serial log

note confl i ct

R1(X,y) W1(z)
R2(X)
W2(y)

Figure 6-A conflict that does not violate serializability.

flict graph whose nodes represent class read-sets and writesets, and whose edges represent conflicts. (There is also an edge between the read-set and write-set of each individual class. See Figure 7.) The important property of a conflict graph is that transaction: that do not lie on a cycle are always serializable and do not need synchronization. Only transactions that lie on cycles require synchronization.
In a conflict graph system, the conflict graph is constructed and analyzed statically when the data base and classes are defined. Classes that do not lie on cycles are noted; the TMs corresponding to these classes are 'toid' not

Class definitions:
C1: read-set {x,y}, write-set = {z}
C2: read-set {x}, write-set = {y}

Conflict graph analysis
Conflict graph analysis is a technique for determining which conflicts require synchronization. The method begins with the definition oftransaction classes. A transaction class is defined by a read-set and write-set. A transaction is a member of a class if the transaction's read-set and write-set are contained in the class's read-set and write-set (respectively). Asso('iated with each class is a transaction module (abbr. TM), a software DBMS component that serially processes transactions from that class. Since transactions in a single class run serially, only transactions in different classes can .'interfere. " Hence, only inter-class conflicts need be considered.
Due to the way classes are defined, transactions in different classes can conflict only if their corresponding classes conflict. Class conflicts are modelled by an undirected COI1-

Confl ict graph C11 s read-set

{x,y}

C2 's read-set {x}

C1 IS write-set {z}

C2 IS write-set {y}

Note: Transactions T1 and T2 in figure 6 are in classes C1 and C2 resp. Since the conflict graph is acyclic, their conflict cannot violate serializability (see text).
Figure 7-Conflict graph.

From the collection of the Computer History Museum (www.computerhistory.org)

818 National Computer Conference, 1979

to use synchronization when executing transactions. The remaining TMs must synchronize their transactions.
Locking is one correct way to synchronize transactions that lie on cycles. If all transactions on cycles use locking, all executions are serializable.12 However, other synchronization mechanisms are possible.
Timestamp-based protocols
SDD-l uses timestamp-based synchronization protocols in place of locking. While the details of these protocols are too involved for this paper, their basic structure can be sketched.
Each edge from one class's read-set to another's write-set represents a conflict that must be synchronized if the edge lies in a cycle. This synchronization occurs during the processing of Read commands. Suppose the read-set of class i conflicts with the write-set of class j, and suppose Ti is a transaction in class i. To process a Read for Ti at site S, the concurrency controller waits until S has processed all Writes for transactions in classj that are "older" than Tj,*** but no Writes for transactions in classj that are "younger" than Ti  This has the same effect as locking the data shared by i's read-set andj's write-set.
The advantage of timestamp-based protocols lies in the wide range of protocols that can be used. There is a special protocol for read-only transactions which is more efficient than locking. There is a special protocol for infrequently run transactions that places a heavier synchronization burden on these transactions while reducing the synchronization required for common transactions. In addition, all protocols use timestamps to resolve conflicts, so deadlock is prevented without the overhead of a detection algorithm.
The correctness of conflict graph analysis and the SDD-l protocols is proved in Reference 41.
ROBUSTNESS
Component failures are inevitable in a DDBMS, and any practical concurrency controller must operate correctly despite them. Problems of three types arise-(1) A failed site may hold information needed to synchronize in-progress transactions. (2) A failed site may hold stored copies of data items being updated by a transaction. (3) A transaction that is updating data at several sites may fail after performing some updates but not all of them. No mechanism yet developed attains 100 percent robustness and it is believed that no such mechanism is possible. 4 Given this apparent limitation, one cannot prove that a concurrency controller is, or is not, robust: all one can do is express the level of robustness it attains.
*** SSD-J assigns unique timestamps to transactions in the same manner as
majority consensus.

Loss of synchronization information
When a site holding synchronization information fails, there are two options. One is to abort all in-progress transactions that depend on the information. If transactions are short and failures occur infrequently, this simple approach is satisfactory. The alternative is to maintain redundant copies of synchronization information. Techniques for managing these redundant copies have been proposed by Afsberg and Day26 and Menasce et al.31 The techniques are presented in the context of primary site or primary copy locking, but could be adapted for other approaches.
The Alsberg and Day technique employs a back-up for each primary copy. When transaction T wishes to access data item x, it requests a lock against the primary copy as described in the section on Primary Copy Locking. If the concurrency controller decides to grant the lock, it forwards this information to the back-up, which records the lock in memory. Only when the lock is safely recorded at the backup is transaction T permitted to access x. If the site containing the primary copy fails, the back-up can immediately take over and a new back-up selected. This scheme offers loo-percent protection against single-site failures, but of course is susceptible to multi-site failures. Protection against multiple failures can be improved by using mUltiple backups, although Alsberg et alY argue that one back-up is sufficient for most applications.
Menasce et al. 31 propose a similar mechanism designed for multiple back-ups. The heart of their approach is a communication procedure for ensuring that all locks are received by all back-ups, and a procedure for reconstructing consistent lock tables following site failures.
Non-availability of stored data items
Suppose a transaction issues a Write command against a logical data itemx, and some stored copy ofx is unavailable. Since the DDBMS must update all stored copies of x, we have a problem. The DDBMS could delay the Write until all stored copies were simultaneously available, but this might never happen. Or it could abort the transaction, but then the availability of a data item would decrease as more copies of it were maintained. The solution is to buffer Write operations against non-available sites and to perform them when the failed site recovers. By buffering the Writes at mUltiple sites, increased protection against multiple failures can be achieved. This technique is sometimes called spooling. 47
Transaction failures
If a transaction fails before completion, a serious concurrency control problem is created-every Write performed by the transaction must be backed out to avoid leaving partial results in the database. The usual technique for doing this is called two phnsp rnmmit. 4

From the collection of the Computer History Museum (www.computerhistory.org)

Approaches to Concurrency Control in Distributed Data Base Systems

819

While a transaction executes, all Writes it performs are placed in temporary files and not the permanent data base. When the transaction completes, it issues Commit messages to each site holding temporary files, whereupon the temporary file is merged into the permanent data base. If the transaction fails before sending the first Commit, no updates are installed. If it fails after sending some but not all Commits, the sites holding temporary files can recognize the situation and can consult the other sites. If any site did receive a Commit, all sites will perform the update.
This technique achieves loo-percent protection against failures of the transaction alone, but is not fully robust with respect to multi-site failures. Hammer and Shipman47 describe mechanisms for improving the multi-site robustness of two phase commit.
CONCLUSION
We have presented several approaches to distributed concurrency control, and the obvious question is, "Which one is best?" We have no clear answer to this question, but a comparison of the methods may be helpful.
First, all methods presented here are correct-they all guarantee serializable executions. Second, the methods offer slightly different degrees of concurrency-conflict-driven restart and majority consensus offer slightly less concurrency than conventional locking; conflict graph analysis combined with locking offers slightly more concurrency than conventional locking; and conflict graph analysis coupled with SDD-l timestamp protocols offers an .'incomparable" degree of concurrency, meaning it allows some executions the other techniques prohibit, while prohibiting some executions the others allow. Termination issues are best understood in the context of locking, and locking is the only technique for which termination can be proved. Majority consensus is susceptible to cyclic restart, and conflict graph analysis coupled with SDD-l protocols can lead to indefinite postponement; in practice, however, the probability of nontermination can be made acceptably small. With respect to robustness, all approaches share the same problems, and the same techniques. So the approaches compare almost identically on these four issues, at least.
The remaining area of comparison is performance. As explained in the section on Other Aspects of Concurrency Control, the key determinant of performance is communications behavior. Unfortunately, few quantitative performance results are available and we shall limit ourselves to basic observations.
Primary site locking requires inter-site communication whenever a lock is requested or released. In principle, primary copy locking could require the same amount of communication, but in a well configured system we would expect to do better. The reason is locality of reference-in many distributed applications, the majority of transactions access data local to the site at which they run. If that data is the primary copy, all lock requests can be processed locally. Of

course, this advantage cannot be realized for data items that are heavily accessed from multiple sites.
The performance of the SDD-l technique similarly depends on application-specific factors. If many transactions run in classes that do not require synchronization, the system will require few synchronization messages. Since the class definitions are tunable, classes can (in principle) be designed so that frequently-executed classes do not lie on cycles. However, if most transactions run in classes that do require synchronization, the communication overhead involved will be comparable to locking.
The performance of majority consensus is comparable to primary site locking with these differences-all locks are in effect requested in a single message; in return for this savings, though, multiple restarts may have to be endured.
The material we have presented on each approach is a bare outline. We have left out many -important details and variations, and we urge the interested reader to consult source materials directly.
ACKNOWLEDGMENTS
We would like to thank Clarice Louis for editorial assistance in preparing this paper. We also thank S. Kimbleton and R. Muntz for prodding us to produce this tutorial survey.
BIBLIOGRAPHY
Note: References are listed by topic, and many do not appear in the text.
General
1. Aho, A., J. Hopcroft and J. Ullman, The Design and Analysis of Computer Algorithms, Addison-Wesley, Reading, Mass., 1975.
2. Date, C. J., An Introduction to Database Systems, 2nd ed., AddisonWesley, Reading, Mass., 1977.
3. Deppe, M. E. and J. P. Fry, "Distributed Databases: A Summary of Research," Computer Networks, Vol. I. No.2, North-Holland, Amsterdam, Sept. 1976.
4. Gray, J. N., Notes on Database Operating Systems, unpublished lecture notes, IBM San Jose Research Laboratory, San Jose, Calif., 1977.
5. King, P. F., and A. J. Collmeyer, "Database Sharing-An Efficient Mechanism for Supporting Concurrent Processes," Proc. 1974 NeC, AFIPS Press, Montvale, New Jersey, 1974.
6. Martin, J., Computer Database Organization, Prentice-Hall, Englewood Cliffs, New Jersey, 1975.
7. Martin, J., Principles of Database Management, Prentice-Hall, Englewood Cliffs, New Jersey, 1976.
8. Peebles, R. et aI., "System Architecture for Distributed Data Management," Computer, Vol. II, No.1, Jan. 1978.
9. Rothnie, J. B.. Jr .. and N. Goodman, "A Survey of Research and Development in Distributed Database Management," Proc. Third Int. Conf. on Very Large Databases. IEEE, 1977.
10. Rothnie. J. B.. Jr., N. Goodman and T. Marill, "Database Management in Distributed Networks" in F. F. Kuo (ed.), Protocols and Techniques for Data Communication Networks, Prentice-Haii, Engiewood Cliffs, N.J., 1978.
II. Stonebraker, M., and E. Neuhold, "A Distributed Database Version of INGRES," Proc. 2nd Berkeley Workshop on Distributed Databases and Computer Networks. May, 1977.

From the collection of the Computer History Museum (www.computerhistory.org)

820 National Computer Conference, 1979

Concurrency control
12. Bernstein, P. A., D. W. Shipman and W. S. Wong, "A Formal Model of Concurrency Control Mechanisms for Database Systems," IEEE Trans. on Software Engineering. to appear.
13. Chamberlin, D. D., R. F. Boyce and I. L. Traiger, "A Deadlock-Free Scheme for Resource Allocation in a Database Environment," Info. Proc. 74, North-Holland, Amsterdam, 1974.
14. Courtois, P. J., F. Heymans and D. L. Parnas, "Concurrent Control with Readers and Writers," Comm. ACM. Vol. 14, No. 10, Oct. 1971.
15. Everest, G. C., "Concurrent Update Control and Database Integrity," in Database Management. J. W. Klimbie and K. L. Koffeman (eds.). North-Holland, Amsterdam, 1974.
16. Eswaran, K. P., J. N. Gray, R. A. Lorie and I. L. Traiger, "The Notions of Consistency and Predicate Locks in a Database System," Comm. ACM. Vol. 19, No. II, Nov. 1976.
17. Garcia-Molina, H . "Performance Comparisons of Two Update Algorithms for Distributed Databases," Proc. 3rd Berkeley Workshop on Distributed Databases and Computer Networks. August 1978.
18. Gelenbe, E., and K. Sevcik, "Analysis of Update Synchronization for Multiple Copy Databases," Proc. 3rd Berkeley Workshop on Distributed Databases and Computer Networks, August 1978.
19. Greif, I., "Formal Problem Specifications for Readers and Writers Scheduling," Proc. MRI Symp. on Computer S(~ftware Engineering. Polytechnic Inst. of N.Y., 1976.
20. Gray, J. N., R. A. Lorie, G. R. Putzulo and I. L. Traiger, "Granularity of Locks and Degrees of Consistency in a Shared Database," IBM Research Report RJ 1654, September 1975.
21. Lamport. L., Time. Clocks. and the Ordering of Events in a Distributed System. Massachusetts Computer Associates. CA-7603-2911, Wakefield, Mass., March 1976.
22. Lamport, L.. Towards a Theory of Correctness (~f Multi-User Database Systems. Massachusetts Computer Associates, CA-7610-0712, October 1976.
23. Papadimitriou. C. H .. P. A. Bernstein and J. B. Rothnie, Jr., "Some Computational Problems Related to Database Concurrency Control," Proc. Conf. on Theoretical Computer Science, Waterloo, August 1977.
24. Papadimitriou, C. H., Serializability of Concurrent Updates. TR-I4-78, Center for Research in Computing Technology, Harvard University. Cambridge, Mass. 1978.
25. Steams, R. C., P. M. Lewis and D. J. Rosenkrantz, "Concurrency Control for Database Systems," ProC'. Conf. on Foundations of Computer Scien('e'. 1976.
Distributed locking
26. Alsberg, P. A., and J. D. Day. "A Principle of Resilient Sharing of Distributed Resources," Proc. 2nd Int. Con!. on Software Engineering. October 1976.
27. Alsberg, P. A., G. G. Belford, J. D. Day and E. Grapa, "Multi-Copy Resiliency Techniques." Center for Advanced Computation, AC Document No. 202. University of Illinois at Urbana-Champaign, May 1976.
28. Ellis, C. A., "A Robust Algorithm for Updating Duplicate Databases," Proc. 2nd Berkeley Workshop on Distributed Databases and Computer Networks. May 1977.
29. Gouda, M. G., "A Hierarchical Controller for Concurrent Accessing of Distributed Databases," Proc. 4th Workshop on Computer Architecture for Non-Numeric Processing. August 1978.
30. Menasce, D. A., and R. R. Muntz, "Locking and Deadlock Detection in Distributed Databases," Proc. 3rd Berkeley Workshop on Distributed Databases and Computer Networks. August 1978.
31. Menasce, D. A., G. J. Popek, and R. R. Muntz, "A Locking Protocol for Resource Coordination in Distributed Databases." 1978 ACM Int. Conf. on Management of Data. May 1978.
32. Minoura. T .. "Maximally Concurrent Transaction Processing," Proc. 3rd Berkeley Workshop Oil Distributed Database5 and Computer Xet1I'0rks. August 1978.

33. Rosenkrantz, D. J., R. E. Steams and P. M. Lewis II, "System Level

Concurrency Control for Distributed Database Systems," ACM Trans.

on Database SYstems. Vol. 3, No.2, June 1978.

M.,34. Stonebraker,

"Concurrency Control and Consistency of Multiple

Copies of Data in Distributed INGRES," Proc. 3rd Int. Workshop on

Distributed Databases and Computer Networks. August 1978.

35. Stucki, M. J., J. R. Cox, Jr., G. C. Roman and P. N. Turcu, "Coordi-

nating Concurrent Access in a Distributed Database Architecture," Proc.

4th Workshop on Computer Architecture for Non-Numeric Processing.

August 1978.

See also Reference 4.

Majority consensus
36. Johnson, P. R., and R. H. Thomas, 'The Maintenance of Duplicate Databases," Network Working Group RFC#677 NIC #31507, January 27, 1975.
37. Thomas, R. H., A Solution to the Update Problem for Multiple Cop)' Databases which Uses Distributed Control. BBN Report No. 3340, Bolt, Beranek & Newman, Cambridge, Mass. July 1975.
38. Thomas, R. H., "A Solution to the Concurrency Control Problem for Multiple Copy Databases," Proc. COMPCON 1978. IEEE, N.Y.

SDD-l
39. Bernstein, P. A., J. B. Rothnie, N. Goodman and C. H. Papadimitriou, "The Concurrency Control Mechanism of SDD-l: A System for Distributed Databases (the Fully Redundant Case)," IEEE Trans. on Software Engineering .. Vol. SE-4, No.3 (May 1978).
40. Bernstein, P. A., D. W. Shipman, J. B. Rothnie and N. Goodman, The Concurrenc\, Control Mechanism of SDD-I: A System for Distributed Databases.' Tech. Rep. CCA-77-09, Computer Corp. of America, Cambridge, Mass., December 1977.
41. Bernstein, P. A . and D. W. Shipman, New Analysis of Concurrency Control in SDD-I: A Systemfor Distributed Databases. Tech. Rep. CCA78-08, Computer Corp. of America, Cambridge, Mass., June 1978.
42. Rothnie, J. B., and N. Goodman, "An Overview of the Preliminary Design of SDD-I: A System for Distributed Databases," Proc. 1977 Berkeley Workshop on Distributed Data Management and Computer Networks, May 1977, pp. 39-57.
Other
43. Badal, D. Z., and G. J. Popek, "A Proposal for Distributed Concurrency Control for Partially Redundant Distributed Data Base System," Proc. 3rd Berkeley Workshop on Distributed Data Managemellt and Computer Networks. 1978, pp. 273-288.
44. LeLann, G., "Algorithms for Distributed Data-Sharing Systems Which Use Tickets," Proc. 3rd Berkeley Workshop on Distributed Data Management and Computer Networks. 1978, pp. 259-272.
Robustness
45. Belford, G. C. P. M. Schwartz and S. Sluizer, The Effect of Back-up Strategy on Database Availability, CAC Document No. 181, CCTC WAD Document No. 5515, Center for Advanced Computation, University of Illinois as Urbana-Champaign, February 1976.
46. Lampson, B. and H. Sturgis, Crash Recovery in a Distributed Data Storage System. Tech. Report, Computer Science Laboratory, Xerox Palo Alto Research Center, Palo Ako, Calif., 1976.
47. Hammer, M. M., and D. W. Shipman, "An Overview of Reliability Mechanisms for a Distributed Data Base System," Pmc. 1977 COJ1PCON. IEEE, N.Y. See also References 4. 26. 27 and 31.

From the collection of the Computer History Museum (www.computerhistory.org)

