Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

A Review of Artificial Intelligence

E. S. Brunette, R. C. Flemmer and C. L. Flemmer
School of Engineering and Advanced Technology Massey University
Palmerston North, New Zealand
Emma.Brunette.1@uni.massey.ac.nz

Abstract-- This paper reviews the field of artificial intelligence focusing on embodied artificial intelligence. It also considers models of artificial consciousness, agent-based artificial intelligence and the philosophical commentary on artificial intelligence. It concludes that there is almost no consensus nor formalism in the field and that the achievements of the field are meager.
Keywords-consciouness; artificial intelligence; embodied intelligence; machine intelligence
I. INTRODUCTION Over the fifty years during which artificial intelligence (AI) has been a defined and active field, there have been several literature surveys [1-4]. However the field is extraordinarily difficult to encapsulate either chronologically or thematically. We suggest that the reason for this is that there has never been a groundswell of effort leading to a recognized achievement. Never-the-less, there is a considerable body of literature which the neophyte must master before attempting to grapple with what has proved thus far to be a hydra-headed monster. This review attempts to order the literature in a way which can be comprehended.
We present a chronological narrative followed by a review of several perceived themes.
II. HISTORICAL PERSPECTIVE There have been speculations as to the nature of intelligence going back to the Greeks and other philosophers of the Mediterranean littoral. More recently, Thorndike, 1932 [5] and Hebb, 1949 [6] proposed that intelligence is fundamentally related to neuronal and synaptic activity.
With the nascence of computing in the nineteen fifties, it was natural that these concepts should be extended to artificial intelligence and we see the advent of the Turing Test [7] in 1950 and the first "Checkers" program of Strachey, 1952 [8] which was later updated by Samuel, 1959 [9] to the point where it was able to beat the best players of the time. This research led to the concept of an evolutionary program as old versions of the program were pitted against more modern versions.
The field of AI is generally held to have started at a conference in July 1956 at Dartmouth College when the phrase "Artifical Intelligence" was first used. It was attended by many of those who became leaders in the field including John McCarthy, Marvin Minsky, Oliver Selfridge, Ray Solomonoff, Trenchard More, Claude Shannon, Nathan Rochester, Arthur

Samuel, Allen Newell, and Herbert Simon [10]. Some of these researchers went on to open centers of AI research around the world, such as at MIT1, Stanford, Edinburgh and Carnegie Mellon University.
Two main approaches were developed for general AI; the "top down" approach which started with the higher level functions and implemented those, and the "bottom up" approach which looked at the neuron level and worked up to create higher level functions. By 1956, Allen Newell [11] had developed the "Logic Theorist", a theorem-proving program.
In the following years several programs and methodologies were developed; "General Problem Solver" 1959 [12] , "Geometry Theorem Prover" 1958 [13], "STRIPS" 1971 [14], Oettinger's "Virtual Mall" 1952 [15], natural language processing implemented in the "Eliza" program in 1966 [16], SHRDLU 1973 [17], expert systems leading to Deep Blue 1997 [18], and some of the earlier versions of embodied intelligence such as "Herbert", "Toto", and "Genghis" by Brooks, 1987 [19, 20] which roamed the laboratories at MIT.
By the 1980's AI researchers were beginning to understand that creating artificial intelligence was a lot more complicated than first thought. Given this, Brooks came to believe that the way forward in consciousness was for researchers to focus on creating individual modules based on different aspects of the human brain, such as a planning module, a memory module etc., which could later be combined together to create intelligence.
In the recent past, with the improvement of the technologies associated with computing and robots, there has been a broadbased attempt to build embodied intelligences. But the peculiar nature of this field has resulted in the many attempts being almost entirely unconnected. Because of the difficulty and lack of success in building physical robots, there has been a tendency towards computer simulation, termed "Artificial General Intelligence" where virtual agents in a virtual reality world attempt to achieve intelligent behaviour.
After this brief historical mise en scene, we discuss the field thematically.
III. MODELS OF CONSCIOUNESS To review this theme and abstract a narrative thread is not possible because there have been very many proposals for a structure of consciousness/control but almost without exception they have not been implemented and further, they are totally
1 Massachussetts Institute of Technology

978-1-4244-2713-0/09/$25.00 ©2009 IEEE 385

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

unrelated. There is consequently no organizational theme and we are left with reporting individual ideas. We do this chronologically and blandly even though many of them stretch the envelope of plausibility and even credibility. We will note the few cases where a simulation has been programmed. There is no instance of an embodied intelligence resulting from the proposals.
Dennett, 1984 [21] discusses the frame problem and how it relates to the difficulties arising from attempting to give robots common sense. The problem is to cause a robot to consider the important results of actions without having to make the robot look at all non-relevant results.
Minsky, 1988 [22] (with a hind glance at Brookes op. cit.) in his seminal book "Society of Mind" believes that consciousness is the result of many small modules, which he called agents. Individually there is no great intelligence in each agent, but when they work together at different levels they produce a cognitive system.
Baar's Global Workspace Theory was first proposed in 1988 [23, 24], and is often described using a theatre metaphor. In this metaphor, a spotlight only shines on one area of the stage while many actions are occurring in the background outside the area shown by the spotlight. This corresponds to consciousness only paying attention to one thing, while many other tasks are being done in parallel in the background. Many other researchers have based their work on this theory. This is one of the few developments which has found general currency rather than instant obscurity.
Block, 1994 [25] attempted to classify different types of consciousness. The main two being the difference between Phenomenal Consciousness; relating to what we feel and experience, and Access Consciousness; relating to processing information and behavioral control.
Chalmers, 1995 [26] described the "hard problem", that of raw feeling, and how difficult it is to implement this; and the "easy problem", which covers the functional areas of consciousness such as planning, memory etc.
Kitamura, Otsuka, and Nakao, 1995 [27] suggested an eight-level hierarchical model. Consciousness appears at a level when action on an immediately lower level is inhibited and as a result the higher level task is carried out. Simulations of this model are claimed to show animal-like behaviour.
Nilsson, 1996 [28] and Holland along with other researchers at the University of Essex, 2006 [29] propose the intellectually interesting notion that consciousness is merely a simulated model of oneself acting in the current environment. A little reflection shows that this does not really advance our understanding.
Kitamura, 1999 [30] developed CBA (Conscious Based Architecture) to determine at what level an autonomous robot can operate without requiring the ability to learn. CBD consists of five levels which correspond to the different levels of consciousness found in living creatures, from single-cell organisms to monkeys. He came to the conclusion that learning ability becomes a requirement around level three.

Gallagher, 2000 [31] looked at the difference between the minimal self and narrative self. The minimal self is only concerned with what is happening at present, whereas, the narrative self requires memories of the past and can plan for the future.
Finland and Jarvilehto, 2001 [32] believe it to be impossible to build consciousness. Their theory is that consciousness is a function of shared goals and social interaction. Therefore, at most, robots can only be extensions of humans and cannot act independently.
Kitamura, Otsubo and Abe, 2002 [33] propose a model with six levels stacked on top of each other, where each level has a different set of behavioral functions. Two emotion-value criteria are used to create vertical and horizontal behaviour selection in an attempt to maximise pleasure. A working computer simulation was developed which worked as expected within its basic environment.
Mikawa, 2004 [34] proposed a system based on Freud's three levels of the human mind; consciousness, preconsciousness and unconsciousness. In this model most data processing is done in the non-conscious states. He therefore proposed a system where the level of information processing changed, based on visual information being received. In his model, external information processing is conducted when the robot is awake. However, when the robot is in sleep mode, external information processing is reduced and more internal information possessing is conducted.
Jie and Jian-gang, 2004 [35] proposed a three-level distributed consciousness network using parallel processing. The first layer was a "physical mnemonic" (memory) layer with global workspace and associated recognition. The second layer offered abstract thinking. Both layers are combined together through the third, a recognition layer.
Kuipers, 2005 [36] observes that the mass of information available to an animal with human-like senses can be likened to "drinking from a firehose of experience". He believes that a "tracker" is required to monitor and evaluate the information and pick out useful information to send to higher level functions.
Kawamura et al., 2005 [37] suggest a multi-agent model with a "central executive" which controls two working memory systems; the phonological loop (hearing) and the visio-spatial sketch pad (sight). They suggest three forms of memory; spatio-temporal short term memory, procedural/declarative/episodic long term memory and taskoriented adaptive working memory. They have not reported a working system.
MacLennan, 2005 [38] believes consciousness-like functions are not used for everything, i.e., conscious is not needed for things such as walking, eating, and breathing; a large amount of human functioning is considered unconscious. He is an advocate of the reductionist approach where masses of information are reduced to manageable level by entities known as protophenomena, and believes that, if you had enough of these mini entities working together, you would eventually reach a point where they could be called self aware.

386

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

Shanahan, 2005 [39] proposes two systems working together. The first-order system deals with the environment and sensors. The second-order system receives input from the firstorder system and from itself. These two loops together make up consciousness. The model was implemented using NRM (Neural Representation Modeller) and Web Bots and proved capable of generating motor responses to cope with a changing environment.
Maeno, 2005 [40] believes that the unconscious mind is a distributed system covering intellect, feeling, and willpower. The conscious mind merely processes and memorizes information from the unconscious mind and therefore, what we consciously believe, is really an illusion.
Perlovsky, 2007 [41] uses Modeling Field Theory (MFT). He believes that the mind is not entirely hierarchical but has multiple feedback loops for both top-down and bottom-up signals. In this theory, learning is implemented by estimating and comparing parameters from these feedback loops.
Doeben-Henisch, 2007 [42] believes shared knowledge, i.e., language, is the key requirement for intelligence. He also believes in a state-based approach where states are changed by using transfer functions.
Mehler, 2007 [43] believes language is critical to survival and its evolution requires distribution over multiple agents. He believes that it is the social interactions between agents that produce language evolution.
Menant, 2007 [44] studied the theory of evolution in relation to the development of self-awareness with the belief that his theories could be carried through to AI research. He believed that the first emotion to evolve was anxiety. This anxiety needed to be limited and so empathy, imitation, and language developed and created a feedback loop to further selfconsciousness evolution. He suggested a process to repeat this evolution in AI involving creating a robot with a representation of the environment, giving this representation meaning and giving the robot evolutionary engines, but not necessarily anxiety.
Pezzulo, 2007 [45] describes a model of the mind which only consists of anticipatory drives. Implicit or behavioural drives are generally the result of anticipation of something and even actions that appear reasoned are often still anticipatory of what is expected to happen.
Lipson, 2007 [46] considers the nature of intelligence and believes it relates strongly to a creature's ability to be creative because people often consider creative children to be intelligent!
Kuipers, 2007 [47] believes that by looking at why some experiences are more vivid then others, we gain a better idea of how to solve the "hard problem". Others such as O'Regan, 2007 [48] disagree with this view and believe that Access Consciousness is the harder problem and Phenomenal Consciousness is simply a matter of being engaged with sensory motor skill.
Friedlander and Franklin, 2008 [49] believe that we attribute mental states to others only by evidence from our own mental states. We build models of other agents in hypothetical

environments and use these models to decide our own behaviour.
Cowley, 2008 [50] believes that the more human a robot looks and the extent to which it is able to move similarly to humans determines how close it can get to "human intelligence". Therefore, to create "human intelligence" in a robot requires a machine heavily modeled on ourselves. However, he also points out the "uncanny valley problem", where if you keep making a robot more human-like you eventually get to the point where it is so close to looking and acting like a human but will still get classified as not being human yet but just "creepy".
Koch and Tononi, 2008 [51] do not believe sight and memory are requirements for consciousness. Instead they believe this depends entirely on the amount of information being processed.
IV. COMPUTATIONAL LANGUAGES The first computational language was LISP, developed by John McCarthy, 1960 [52]. This is a combination of information processing language (IPL) and lambda calculus. In the early 1970's another language was developed for AI use, that of PROLOG. The language's formal logic background made it suitable for many AI applications [1]. ConAg, 2003 [53] is a reusable Java framework developed to produce intelligent software agents by the Conscious Software Research Group (CSRG). It was developed with the intent of reducing AI implementation costs and development time. The intelligence model used is based on Baar's Global Workspace theory [23, 24]. More recently, this trend has continued with the introduction of freely distributed computer simulations of robots or agents for other researchers to work with. This includes tools such as Web Bots, NRM [39], and the SIMNOS program [54] used to simulate the CRONUS robot. Moreno and de Miguel, 2005 [55] created the CERA (Consciousness and Emotion Reasoning Architecture) for autonomous agents. This is a software architecture based on Baar's global workspace theory. The purpose of the system was to allow different conscious components to be integrated together. Their model has currently only been implemented on computer simulations.
V. AGENT BASED MODELS Cmattie is a meeting planning and reminder management software program developed by McCauley, and Franklin, 1988 [56]. The agent's behavior changes based on its overall emotional state and this is calculated as a combination of values from its four individual emotions; anger, happiness, sadness, and fear. This results in different "moods" being portrayed in the agent's interactions with people. For example, if Cmattie is angry with someone for not attending a meeting, the next meeting reminder email would carry an angry tone. Franklin, 2000 [57] developed the IDA software agent for the United States Navy. This system is designed to communicate with Navy Personal, via emails written in the

387

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

form of natural language, to negotiate their next deployment. The system is based on Baar's global workspace theory, op. cit., combined with fuzzy logic and behavioral nets.
Restivo, 2001 [58] believes sociological theories should be applied to consciousness research. He believes research should look at creating the SOCIO agent/computer. This is an agent/computer which learns new behaviors and gains knowledge through social interaction with humans or other computers.
McDermott, 2003 [59] described a regression-planning program which searches situation space to develop a recommended course of actions. This work is based on earlier planning algorithms but has been expanded to cover several autonomous processes working together, which take into account consequence of actions, and is able to deal with outside sources of change. The program also includes functions which work out how good the suggested action actually is.
Negatu et al., 2006 [60] created LIDA, an architecture which included a learning mechanism for autonomous agents. LIDA was an extension of earlier work on the IDA model which allows the new system to learn through connections to the internet and databases. The actual learning mechanism is based on functional consciousness. The entire system includes an anticipatory payoff mechanism, state and reliability mechanisms, selective attention, procedural memory, perceptual and associative memory, anticipatory learning and procedural learning.
Vogt, 2007 [61] implemented a language game over the internet using lego robots to study the different methods of categorizing input. This work was based on his belief that language evolution plays a major part in intelligence evolution.
Grim and Kokalis, 2007 [62] created an entity survival simulation via a 64X64 array in which each square is a different colour and each colour represents a type of entity, i.e., food, prey or predator. Squares can communicate with neighboring squares of the same type. This means prey can communicate the presence of predators or food. Simulations produced appropriate results, i.e., prey left areas when predators approached and prey and predators converged on areas with high levels of food. It seems that this interaction has lost some of the fine texture of an actual predator/prey/forage situation such as plays out on the Serengeti.
VI. PROPOSALS FOR AN EMBODIED INTELLIGENCE Brooks and Stein, 1994 [63] proposed a robot with object manipulation ability and visual processing controlled by a large amount of parallel processing. The aim was to use the robot for research, however, currently the robot is only in the planning stage. Dennett, 1994 [64] proposed the creation of a human torso robot known as COG, at MIT. The aim was to create successive generations of the COG robot and use each to implement research done at MIT, much of which related to artificial consciousness and intelligence. Manzotti et al., 1998 [65] created the Babybot project. This consisted of a robot arm which had to learn to pick up blocks

using colour discrimination. However it was only simulated in a computer model and only had four degrees of freedom, two of which related to the robotic head and camera.
Bamba and Nakazato, 2000 [66] use fuzzy logic to calculate emotion values for implementation in an all-terrain vehicle control system which has to reach a defined location while avoiding obstacles. Stimuli enter "conscious space" and are processed based on probabilities into emotions using fuzzy logic.
Aramaki et al., 2002 [67] describe a multi-operating system and multi-task control structure for a humanoid robot containing 3 levels of consciousness; the control task level associated with motor control and sensor input, the unconscious levels associated with conditioned reactions and the conscious level which controls action sequences and strategy. They propose a parent-child task structure to pick up a block.
Gonzalez et al., 2004 [68] believe that all consciousness is based on feedback loops and its actions are closely related to its environment. It follows that the reason consciousness has not been developed in robots is that they are built out of separate components. To see consciousness appear in a robot requires an "embedded embodiedness" approach as in the development and evolution of living organisms.
Singh et al., 2004 [69] designed and simulated two people building a tower of blocks. Control was based on a distributed system in which "critics" and "selectors" are used to evaluate performance during problem solving and attempt to find a better path.
Kawamura et al., 2005 [37] propose a central executive to control two working memory phonological loops and a visual spatial sketch pad in order to create consciousness.
Kelemen, 2006 [70] believes that consciousness can be considered to be either wholly or partly made up from the randomness associated with sensors, actuators, and hardwiring associated with embodied robots.
Chella and Gaglio, 2007 [71] attempted to create a selfaware robot using 2D and 3D image processing. The main problem they found was a lack of good image processing capabilities and an information storage problem (all images were stored from when the robot began life).
Parisi and Mirolli, 2007 [72] believe that robots need to know the difference between inputs relating to objects in the external environment and those relating directly to themselves. By knowing the difference, a robot can predict whether actions will affect it directly. They extend this concept to telling the difference between public and private knowledge in social situations
Bittencourt and Marchi, 2007 [73] believe that the environment provides "experience flux". They use mathematical logic to change this experience flux to binary information. Emotional values are decided by whether the information is good or bad for motivation. The code was written in LISP and tested in SATLIB with the expectation that it would be implemented in robot soccer. This has not yet been reported.

388

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

VII. ACTUAL EMBODIED INTELLIGENCE Taylor, 1994 [74] created robotic bugs with drives. The value of the drives was modeled by mathematical equations and the highest value determined the action sequence of the bug. A sketchpad approach using a visual net and a drive net was proposed for planning, although this was not implemented.
Dennett's, 1984, 1994 [21, 64] work has led the COG project at MIT. There are actually several versions of this robot, each of which has been built making successive improvements on the previous versions. They are used to test theories about consciousness, human-computer interaction, image processing, speech processing, and object manipulation and embodiment. Some areas currently being implemented in COG include; detecting people and objects in the environment by looking for patterns; the ability to learn how to reach for a visual target; reflective arm withdrawal when COG comes into contact with an object; the ability to play drums in synchronization with a tune the robot is hearing; and the ability to play with a slinky, saw wood, turn a crank, and swing a pendulum [75].
Nilsson, 1996 [28] created a snake robot which used a virtual model of itself to create hypothetical situations and experiment before moving. This allows the robot to determine the consequences of actions before making them. This was done as the author believes programming the movement normally with so many degrees of freedom is difficult. The author believes their experiments showed snake-like movement was learned at a rate faster than it would have been without the virtual model.
Brooks has made significant contributions in the area and is considered to be a pioneer. Brooks began his work in 1987 mostly based at the MIT artificial intelligence laboratory. He follows the theory that individual modules can be combined together to form something like a human brain. He called his architecture the subsumption architecture and based most of his work around it. He has created a range of robots based on this theory which exist in the environment of the laboratories and corridors at MIT. These include the six legged Attila, whose control is based on "hormone" levels which decay with time, Allen - a sonar range finding robot, Tom and Jerry - two identical race car robots used to test computational power required for the subsumption architecture, Herbert - a robot with an arm that moves around looking for empty soda cans on people's desks, Genghis - a six legged insect-like robot, Squirt - a tiny robot, weighing only 50 grams, which hides in corner and ventures out to investigate noises, and Toto - a robot implementing a layered architecture with navigation capabilities. All of these are based to some extent on Brooks' layered approach where each layer represents a different function and each layer can act on a lower level by reading or suppressing its outputs [19, 20, 76-82].
Ogiso et al., 2005 [83] have designed and built a robotic head which looks and moves as a human head does and associates emotions with words. The word-to-emotion association is based on an associative word network and the researchers have classified consciousness as occurring when part of the network is activated. The internet was used to build

this network and associate pleasure and displeasure with words.
The DARPA challenge is a competition between universities and other research institutions to build a vehicle capable of driving a desert course, specified by GPS coordinates, without a driver. This involves a large amount of embodied AI although not necessarily machine consciousness. One such vehicle was Sandstorm from Carnegie Mellon University, 2004 [84].
Zoe, 2007 [85] is a robot which collects remote samples for scientists, does some analysis, and makes decisions on whether further exploration by human scientists is needed. She was built at the Carnegie Mellon University with the intention that she may one day be used for exploring other planets.
Groundhog, 2006 [86] is an autonomous robot designed to traverse underground mines in place of humans. Its aim is to reduce danger in exploring mine shafts by reducing the need for humans to enter them. It was built at Carnegie Mellon University.
Grace, 2007 [87] is a robot designed to attend conferences and act as a human would. She was built at Carnegie Mellon University as part of a competition.
The CRONUS project is run by Holland, 2006 [29] and aims to build a humanoid robot using virtual reality. Control is implemented through the SIMNOS program which models the system in terms of spike streams and neural modeling. Planning is done in terms of virtual reality.
Emaru and Tsuchiya, 2007 [88] created a sonar sensor robot which implemented a basic neural network structure. It is based on two levels of consciousness, the first (unconscious) level for inputs, and the second (conscious) level for higher functions like navigation and route planning.
Ponticorvo and Walker, 2007 [89] implemented theories on evolutionary robots using the miniature Khepera robot. This is a 55mm diameter differential-drive robot that is used in many universities for research. In particular they are often used to study evolutionary robotics. In this case, the work involved the evolution of orientation, navigation, and spatial cognition over several generations as the fittest in these areas carried on to the next generation of Khepera.
MacLennan, 2007 [90] worked on the development of simple machines using a control structure of finite state machines and artificial neural networks called symbol states. He used this control structure to conduct experiments on evolutionary robotics.
Neural network models have also been implemented in Khepera robots, where each sensory input corresponds to a neural excitation by Hulse et al., 2007 [91]. Zahedi and Pasemann, 2007 [92] used Khepera robots for obstacle avoidance based on self regulating neurons where each sensor input corresponded to one neuron.
Kaplan and Oudeyer, 2007 [93] believe that robots need to be motivated like children who play with everything because it makes them happy. They implemented this theory in a toy robot playing games on a mat.

389

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

The ASIMO and Auroro robots are humanoid robots, but they are not intelligent. The Auroro robot is used to attract the attention of children with autism [94].
Itohl et al., 2007 [95] are developing the WE-4RII humanoid robot. It has a torso with human-like arms and a head capable of emotional expression. These emotion changes show in the robot's face by changes to cheek tones, the shape of lips and eyebrow position. It associates memory models with moods which are then implemented in the robot.
Boblan et al., 2007 [96] built the ZAR5 robot, This is a human-like torso with a five-fingered hand which uses fluidic muscle, from Festo2, for actuation. It is currently controlled from a data suit and two five-fingered gloves and does not implement any form of consciousness.
Sandini et al., 2007 [97] designed the iCub, a 53 degreeof-freedom cognitive humanoid robot the size of a 3 year-old human. The iCub will start life with basic skills and then learn more advanced movements such as crawling, sitting up and how to manipulate objects. This is currently in the design phase and has not yet been built and implemented [98].
VIII. CONCLUSION This review has not attempted to detail all the literature in the area but to report mainly the most recent work, particularly in the area of embodied AI. There is a major field of agentbased programs, many of them commercial, exemplified by The World of Warcraft. This has barely been touched. The disparate nature of the reported work makes it very difficult to grasp or perhaps makes it unnecessary to grasp. Perhaps the only two concepts which have been shared between researchers are Baar's Global Workspace Theory and the agent-based model, advanced independently by Brooks and Minsky. A curious aspect of the literature is the very large preponderance of proposed schemes over schemes actually implemented. Practitioners in the field shy away from actually building robots, whether from considerations of cost or from a lack of expertise in the area. Having digested all of these reported efforts, two basic conclusions must be drawn; firstly, the researcher is free to go forward unfettered because there is no existing formalism in the field. Secondly, the achievements of the field, attended as they are by a 33 million-fold (Moore's law) improvement in computing, are disappointing - the field is a long way from producing a robot which approaches the intelligence and functionality of a cockroach.
REFERENCES
[1] M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer, "50 years of artificial intelligence", essays dedicated to the 50th anniversary of artificial intelligence, Springer, New York, 2007.
2 Festo Corporation, Hauppauge, NY http://www.festo.com/INetDomino/files_01/501_mailer.pdf

[2] H. Knight, "Early artificial intellligence projects", T. Greene (Editor), Computing Science and Artificial Intelligence Laboratory (CSAIL), Cambridge, available at http://projects.csail.mit.edu/films/AIFilms.html, 2006.

[3] P. Husbands, O. Holland, and M. Wheeler, "The machanical mind in history", Bradford Books, Cambridge, London, 2008.

[4] B. G. Buchanan, "A very brief history of artificial intelligence", Artificial Intelligence Magazine, 25th annversary issue pp. 53-60, 2005.

[5] E. L. Thorndike, "Fundamentals of learning", Columbia University Teacher College, New York, 1932.

[6] O. L. Hebb, "The organisation of behaviour", Wiley, New York, 1949.

[7] A. M. Turing, "Computing machinery and intelligence", Mind, vol. 59, pp. 433-460, 1950.

[8] C. Strachey, "Logical or non-mathematical programmes", in Proceedings of the Association for Computing Machinery Meeting, Association of Computing Machinery, New York, pp. 46-49, 1952.

[9] A. Samuel, "Some studies in machine learning using the game of checkers," IBM Journal, vol. 3, pp. 210-299, 1959.

[10] J. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon, "A proposal for the Darthmouth summer research project on artificial intelligence", available at http://wwwformal.stanford.edu/jmc/history/dartmouth/dartmouth.html, August 31 1955.

[11] A. Newell and H. A. Simon, "The logic theory machine a complex

information processing system", The Rand Corperation, Santa Monica,

available

at

http://shelf1.library.cmu.edu/IMLS/MindModels/logictheorymachine.pd

f ,1956.

[12] A. Newell, J. C. Shaw, and H. A. Simon, "Report on general problem-solving program", Proceeding of the International Conferenceon Information Processing, Paris, pp. 256-264, 1959.

[13] H. L. Gelernter and N. Rochester, "Intelligent Behaviour in ProblemSolving Machines", IBM Journal of Reasearch and Development, vol. 2, p. 336, 1958.

[14] R. Fikes and N. Nilsson, "STRIPS: a new appraoch to the application of theorem proving to problem solving", Artificial Intelligence, pp. 274279, 1971.

[15] A. G. Oettinger, "Simple learning by a digital computer", Proceeding of Association of Computing Machinery, pp. 55-60, September 1952.

[16] J. Weizenbaum, "ELIZA - A computer program for the study of natural language communication between man and machine", Communications of the Association for Computing Machinary (ACM), vol. 9, pp. 36-45, 1966.

[17] T. Winograd, "Procedures as a representation for data in a computer program for understanding natural language", Cognitive Psychology, vol. 3, issue 1, 1972.

[18] H. Feng-hsiung, "Behind Deep Blue: Building the computer that defeated the world chess champion", Princeton University Press, Princeton, 2002.

[19] R. A. Brooks, "Elephants don't play chess," in "Cambrian Intelligence: the early history of the new AI", Bradford Books, Cambridge, pp. 111-131, 2003.

[20] R. A. Brooks, "A robot that walks: emergent behaviours from a carefully evolved network" in "Cambrian Intelligence: the early history of the new AI", Bradford Books, Cambridge," pp. 27-36, 2003.

[21] D. C. Dennett, "Cognitive wheels: the frame problem of AI" in "Mind, Machines, and Evolution", Cambridge University Press, Cambridge, pp. 129-151, 1984.

[22] M. Minsky, "The Society of Mind", Simon and Schuster, New York, 1988.

[23] B. J. Baars, "In the theater of consciousness: global workspace theory, a rigorous scientific theory of consciousness", Journal of Consciousness Studies, vol. 4, pp. 292-309, 1997.

[24] B. J. Baars, "A cognitive theory of consciouness", Cambridge University Press, Cambridge, 1988.

390

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

[25] N. Block, "The mind as the software of the brain" in "An invitation to cognitive science", MIT Press L. G. D. Osherson, S. Kosslyn, E. Smith and S. Sternberg (Editors), New York, 1994.
[26] D. J. Chalmers, "Facing up to the problem of consciouness", Journal of Consciousness Studies, vol. 2, pp. 200-219, 1995.
[27] T. Kitamura, Y. Otsuka, and T. Nakao, "Imitation of animal behaviour with use of a model of consciouness behaviour relation for a small robot", IEEE International Workshop on Robot and Human Communication, Tokyo, Japan, pp. 313-317, 1995.
[28] M. Nilsson, "Towards conscious robots", Institute of Electrical Engineers Colloquium Digest on Self Learning Robots, vol. 26, pp. 12, London 1996.
[29] O. E. Holland, "CRONUS robot", available at http://cswww.essex.ac.uk/staff/owen/machine/cronos.html, 2006.
[30] T. Kitamura, "Can a robot's adaptive behavior be animal-like without a learning algorithm?", IEEE Systems, Man, and Cybernetics Conference, Tokyo, 1999.
[31] S. Gallagher, "Philosophical conceptions of the self: implications for cognitive science", Trends in Science, vol. 4, pp. 14-21, 2000.
[32] J. Finland and T. Jarvilehto, "Machines as part of human consciousness and culture", International Symposium on Machine Consciouness, Jyvaskyla, Finland, 2001.
[33] T. Kitamura, J. Otsubo, and M. Abe, "Emotional intelligence for linking symbolic behaviors", IEEE International Conference on Robotics and Automation, Washington DC, 2002.
[34] M. Mikawa, "Robot vision system based on sleep and wake functions of human being", Society of Instrument and Control Engineers (SICE) Annual Conference, Sapporo, Japan,2004.
[35] L. Jie and Y. Jian-gang, "Neural computing consciousness model for intelligent robot", International Conference on intelligent Mechatronics and Automation, Chengdu, China, 2004.
[36] B. Kuipers, "Consciousness: drinking from the firehose of experience", National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania, 2005.
[37] K. Kawamura, W. Dodd, P. Ratanaswasd, and R. A. Gutierrez, "Development of a robot with a sense of self", 2005 IEEE International Symposium on International Symposium on Computational Intelligence in Robotics and Automation, Espoo, Finland 2005.
[38] B. J. MacLennan, "Consciousness in robots: the hard problem and some less hard problems", 2005 IEEE International Workshop on Robots and Human Interactive Communication, Rome, 2005.
[39] M. Shanahan, "Consciousness, emotion, and imagination a braininspired architecture for cognitive robotics", American Studies in Britian (AISB) 2005 Workshop: "Next Generation Approaches to Machine Consciousness", pp. 26-35, 2005.
[40] T. Maeno, "How to make a conscious robot-fundamental idea based on passive consciousness model", Journal of the Robotics Society of Japan, vol. 23, pp. 51-62, 2005.
[41] L. Perlovsky, "Modeling feild theory of higher cognitive functions", Artificial Cognition Systems, A. Loula, R. Gudwin, and J. Queiroz, (Editors), Hershey Printing and Publishing, Hershey, pp. 64-105, 2007.
[42] G. Doeben-Henisch, "Reconsctructing human intelligence within computational sciences: an introductory essay", Artificial Cognition Systems, A. Loula, R. Gudwin, and J. Queiroz (Editors), Hershey Printing and Publishing, Hershey, pp. 106-139, 2007.
[43] A. Mehler, "Stratified constraint satisfaction networks in synergetic multi-agent simulations of language evolution", in Artificial Cognition Systems, A. Loula, R. Gudwin, and J. Queiroz, (Editors), Hershey Printing and Publishing, Hershey, pp. 140-175, 2007.
[44] C. Menant, "Proposal for an approach to artificial consciousness based on self-consciousness", American Association for Artificial Intelligence (AAAI) Fall Symposium, Menlo Park, California, 2007.
[45] G. Pezzulo, "Anticipation and future oriented capabilities in natural and artifical cognition" in "50 years of artificial intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 257-270, 2007.

[46] H. Lipson, "Curious and creative machines" in "50 years of artificial intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 215-319, 2007.
[47] B. Kuipers, "Sneaking up on the hard problem of consciousness", Proceedings of American Association for Artificial Intelligence Symposium on Machine Consciuness and AI , Washington, 2007.
[48] J. K. O'Regan, "How to build consciousness into a robot: the sensor motor approach," in "50 years of artificial intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 332-346, 2007.
[49] D. Friedlander and S. Franklin, "LIDA and a theory of mind", Proceeding of Artificial General Intelligence (AGI), B. Goertzel and P. Wang (Editors), IOS Press, Amsterdam, 2008.
[50] M. Cowley, "The relevance of intent to human-android strategic interaction and artificial consciousness", 15th IEEE International Symposium on Robot and Human Interactive Communication, Hatfield UK, 2008.
[51] C. Koch and G. Tononi, "Can machines be conscious?", IEEE Spectrum Online, vol. 45, June 2008.
[52] J. McCarthy, "Recursive functions of symbolic expressions and their computation by machine", Communications of the Association for Computing Machinary (ACM), vol. 3, pp. 184-195, 1960.
[53] M. Bogner, J. Maletic, and S. Franklin, "ConAg: a reusable framework for developing "conscious" software agents" in "The International Journal on Artificial Intelligence Tools", World Scientific Publising Company, River Edge, 2003.
[54] O. Holland, "SIMNOS simulator", R. Newcombe (Editor), Available at http://cswww.essex.ac.uk/staff/owen/machine/simnos.html, 2006.
[55] R. A. Moreno and A. S. de Miguel, "A machine consciousness approach to autonomous mobile robotics", American Association for Artificial Intelligence,vol.29, pp. 175-184, 2006.
[56] L. McCauley and S. Franklin, "An architecture for emotion", American Association for Artificial Inteligence (AAAI) Fall Symposium on Emotion and Intelligence, Menlo Park, California, pp. 122-127, 1988.
[57] S. Franklin, "Deliberation and voluntary action in "conscious" software agents," Neural Networks World, vol. 10, pp. 505-521, 2000.
[58] S. Restivo, "Bringing up and booting up: social theory and the emergence of socially intelligent robots", IEEE, vol.4, pp.2110-2117 2001.
[59] D. V. McDermott, "Reasoning about autonomous processes in an estimated-regression planner", Proceedings of International Conference on AI Planning and Scheduling, Menlo Park, California, 2003.
[60] A. Negatu, S. D'Mello, and S. Franklin, "Cognitively inspired anticipation and anticipatory learning mechanisms for autonomous agents", Springer, vol. 4520, Berlin, pp. 108-127, 2006.
[61] P. Vogt, "Language evolution and robotics: issues on symbol grounding and language acquisition", in "Artificial Cognition Systems", A. Loula, R. Gudwin, and J. Queiroz (Editors), Hershey Printing and Publishing, Hershey, pp. 176-209, 2007.
[62] P. Grim and T. Kokalis, "Envromental variability and the emergence of meaning: simulated studies across imitation, genetic algorithms, and neural networks," in "Artificial Cognition Systems", A. Loula, R. Gudwin, and J. Queiroz (Editors), Hershey Printing and Publishing, Hershey, pp. 284-326, 2007.
[63] R. A. Brooks and L. A. Stein, "Building brains for bodies", Autonomous Robots, vol. 1, pp. 7-25, 1994.
[64] D. C. Dennett, "Consciousness in human and robot minds", in IIAS Symposium on Cognition,Computation and Consciouness, Kyoto, September 1994.
[65] R. Manzotti, G. Metta, and G. Sandini, "Emotions and learning in a developing robot", Proceeding of Emotions, Qualia, and Consciousness, Naples, 19-24 October 1998.
[66] E. Bamba and K. Nakazato, "Fuzzy theoretical interactions between consciouness and emotions", IEEE International Workshop of Robot and Human Interactive Communication, Osaka, Japan, pp 218-223, 2000.

391

Proceedings of the 4th International Conference on Autonomous Robots and Agents, Feb 10-12, 2009, Wellington, New Zealand

[67] S. Aramaki, H. Shirouzu, K. Kurashige, and T. Kinoshita, "Control program structure of humanoid robot", IEEE, vol. 3, pp.1796-1800, 2002.

[68] E. Gonzalez, M. Broens, and P. Haselager, "Consciousness and agency: the importance of self-organized action", Networks, vol. 3-4, pp. 103-113, 2004.

[69] P. Singh, M. Minsky, and I. Eslick, "Computing commonscence", BTexact Technolgies Technology Journal, vol. 22, pp. 201-210, October 2004.

[70] J. Kelemen, "On a possible future of computationalism", 7th International symposium of Hungarian Researchers on Computational Intelligence, Budapest, November, 2006.

[71] A. Chella and S. Gaglio, "A cognitive approach to robot selfconsciouness", Cognitive Modeling, vol. 4, August 2007.

[72] D. Parisi and M. Mirolli, "Steps towards artificial consciousness: a robot's knowledge of its own body", American Association for Artificial Intelligence; Fall Symposium on Theoretical Fundations and current Approachs, Washington DC, November 2007.

[73] G. Bittencourt and J. Marchi, "An embodied logical model for cognition in artificial cognition systems", in "Artificial Cognition Systems", A. Loula, R. Gudwin, and J. Queiroz (Editors), Hershey Printing and Publishing, Hershey, 2007.

[74] J. G. Taylor, "Goals, drives, and consciouness", Neural Networks, vol. 7, pp. 1181-1190, 1994.

[75] "Cog

Project

Homepage."

available

at

http://www.ai.mit.edu/projects/humanoid-robotics-group/cog/cog.html

2008.

[76] R. A. Brooks, "Intelligence without reason", in "Cambrian Intelligence: The Early History of the New AI" Bradford Books, Cambridge, pp. 133-186, 2003.

[77] R. A. Brooks, "Planning is just a way of avoiding figuring out what to do next", in "Cambrian Intelligence: The Early History of the New AI" Bradford Books, Cambridge, pp. 103-110, 2003.

[78] R. A. Brooks, "Intelligence without representation", in "Cambrian Intelligence: The Early History of the New AI" Bradford Books, Cambridge, pp. 79-101, 2003.

[79] R. A. Brooks, "New approachs to robotics", in "Cambrian Intelligence: The Early History of the New AI" Bradford Books, Cambridge, pp. 59-75, 2003.

[80] R. A. Brooks, "A robust layered control system for a mobile obot", in "Cambrian Intelligence: The Early History of the New AI" Bradford Books, Cambridge, pp. 3-26, 2003.

[81] R. A. Brooks, "Integrated systems based on behaviours", Stanford University Press, pp. 46-50, 1991.

[82] R. A. Brooks, "Intelligence without representation", Artificial Intelligence, pp. 139-159, 1987.

[83] A. Ogiso, S. Kurokawa, M. Yamanaka, Y. Imai, and J. Takeno, "Expression of emotion in robots using a flow of artifical consciouness", IEEE International Symposium on Computational Intelligence in Robotics and Automation, Espoo, Finland, pp. 421-426, 2005.

[84] C. Urmson, J. Anhalt, M. Clark, T. Galatali, J. P. Gonzalez, J. Gowdy, A. Gutierrez, S. Harbaugh, M. Johnson-Roberson, H. Kato, P. Koon, K. Peterson, B. K. Smith, S. Spiker, E. Tryzelaar, and W. L. Whittaker, "High speed navigation of unrehearsed terrain: red team technology for Grand Challenge 2004", technical report CMU-RI-TR04-37, Robotics Institute, Carnegie Mellon University, June 2004.

[85] F. Calderon, "Autonomous reflectance spectroscopy by a mobile robot for mineralogical characterization", technical report CMU-RI-TR07-46, Robotics Institute, Carnegie Mellon University, 2007.

[86] D. Silver, D. Ferguson, A. C. Morris, and S. Thayer, "Topological exploration of subterranean environments", Journal of Field Robotics, vol. 23, pp. 395-415, July 2006.

[87] M. P. Michalowski, S. Sabanovic, C. F. DiSalvo, D. Busquet-Font, L. M. Hiatt, N. Melchior, and R. Simmons, "Socially distributed perception: GRACE plays social tag at AAAI 2005", Autonomous Robots, vol. 22, pp. 385-397, May 2007.

[88] T. Emaru and T. Tsuchiya, "Impelmenatation of unconsciousness movements for mobile robot by using sonar sensor", International

Conference on Control, Automation, and Systems, Seoul, Korea, pp. 96101, 2007.
[89] M. Ponticorvo and R. Walker, "Evolutionary robotics as a tool to investigate spatial cognition in artificial and natural systems", in "Artificial Cognition Systems", A. Loula, R. Gudwin, and J. Queiroz, (Editors), Hershey Printing and Publishing, Hershey, pp. 210-237, 2007.
[90] B. MacLennan, "Making meaning in computers: synthetic ethology revisited", in "Artificial Cognition Systems", A. Loula, R. Gudwin, and J. Queiroz, (Editors), Hershey Printing and Publishing, Hershey, pp. 252-283, 2007.
[91] M. Hulse, S. Wishmann, P. Manoonpong, A. Twickel, and F. Pasemann, "Dynamic systems in the sensor motor loop: on the interrelation between internal and external mechanisms of evolved robot behavior", in "50 Years of Artificial Intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 186-195, 2007.
[92] K. Zahedi and F. Pasemann, "Adaptive behavior control with selfregulating neurons", in "50 Years of Artificial Intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 196-205, 2007.
[93] F. Kaplan and P. Oudeyer, "Intrinsically motivated machines", in "50 Years of Artificial Intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 303-314, 2007.
[94] C. Evas-Pughe, "Masters of their fate? ", Engineering & Technology, vol 2, pp. 26-30, April 2007.
[95] K. Itohl, H. Miwa, Y. Nukariya, M. Zecca, H. Takanobu, P. Dario, and A. Takanishi", "New memory model for humanoid robots introduction of co-Associative memory using coupled chaotic neural networks", Proceeding of the International Joint Conference on Neural Networks, vol. 5, pp. 2790-2795, 2005.
[96] I. Boblan, R. Bannasch, A. Schulz, and H. Schwenk, "A human-like robot Torso ZAR5 with fluid muscles: towards a common platform for embodied AI", in "50 Years of Artificial Intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 347-357, 2007.
[97] G. Sandini, G. Metta, and D. Vernon, "The iCub cognitive humanoid robot: an open-system research platform for enactive cognition", in "50 Years of Artificial Intelligence: essays dedicated to the 50th anniversary of artificial inteligence", M. Lungarella, F. Lida, J. Bongard, and R. Pfeifer (Editors), Springer, New York, pp. 358-369, 2007.
[98] "RobotCub Homepage." available at http://www.robotcub.org/, 2008.

392

