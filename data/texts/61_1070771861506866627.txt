HYBRID NEURAL NETWORK/HIDDEN MARKOV MODEL CONTINUOUS-SPEECH RECOGNITION

Michael Cohen*, Horacio Franco*, Nelson Morgan**,

David Rumelhart***, and Victor Abrash*

* Speech Research Program, SRI International, Menlo Park, CA 9402
5

*

* Intl. Computer Science Inst., 1947 Center Street, Suite 600, Berkeley, CA 94704

*** Stanford University, Dept. of Psychology, Stanford, CA 94305

ABSTRACT

In this paper we present a hybrid multilayer perceptron (MLP)/hidde
n
M
arkov model (HMM) speaker-independent continuous-speech recogni-
tion system, in which the advantages of both approaches are combined
b
y using MLPs to estimate the state-dependent observation probabilities
of an HMM. New MLP architectures and training procedures are
p
resented which allow the modeling of multiple distributions for phonetic
classes and context-dependent phonetic classes. Comparisons with
a
p
ure HMM system illustrate advantages of the hybrid approach both in
terms of recognition accuracy and number of parameters required.

1. INTRODUCTION

Hidden Markov models (HMMs) are used in most current state
-
o
f-the-art continuous-speech recognition systems. This approach is
limited by the need for strong statistical assumptions that are
u
nlikely to be valid for speech. Techniques using multilayer per-
ceptrons (MLPs) for probability estimation have recently been
i
ntroduced [1] which reduce the assumption of independence for
multi-feature probability computation. Additional advantages o
f
M
LP probability estimation include the inherently discriminant
nature of the training algorithm and the distributed representation
,
w
hich leads to efﬁcient use of the available parameters. When
applied to speech, this results in a reduction of the number o
f
p
arameters needed for detailed phonetic modeling as a result of
increased sharing of model parameters between phonetic classes.

Pure MLP-based approaches have not previously been
demonstrated to function well for continuous-speech recognitio
n
b
ecause of the need for accurate segmentation of the speech sig-
nal. HMMs, on the other hand, provide a framework for simul
-
t
aneous segmentation and classiﬁcation of speech, which has been
demonstrated to be useful for continuous recognition. Previou
s
w
ork by Morgan and Bourlard [1] has shown theoretically and
practically that MLPs and HMMs can be combined by usin
g
M
LPs for the estimation of the HMM state-dependent observation
probabilities,
bot
h
a
pproaches.

advantages

exploiting

thereby

the

of

We have incorporated MLP-based probability estimation tech-
niques into the HMM-based SRI-DECIPHER system, which is a
tate-of-the-art, speaker-independent, continuous-speech recogni-
s
tion system.
In this paper we describe the initial baseline DECI
-
P
HER system and the approach for integrating MLP-based estima-
tion techniques; present a number of new techniques that have
b
een developed to allow the modeling of multiple distributions for
phonetic classes and context-dependent phonetic classes; and show
t
he results of recognition experiments for the systems described.

- 2 -

Abrash et al. [2] and Konig et al. [3] describe extensions of th
e
s
ystem presented here to the modeling of long-term speech con-
sistencies.

2. HYBRID MLP/HMM

is

the SRI-DECIPHER system,

2.1 The DECIPHER System
he baseline system into which we incorporated MLP probability
T
estimation
phone-based
,
s
peaker-independent, continuous-speech recognition system, based
on semicontinuous (tied Gaussian mixture) HMMs [4]. The system
e
xtracts four features from the input speech waveform, including
12th-order mel cepstrum, log energy, and their smoothed deriva
-
t
ives. The front end produces the 26 coefﬁcients for these four
features for each 10-ms frame of speech.

a

Training of

the phonetic models is based on maximum-
likelihood estimation using the forward-backward algorithm [5]
.
ost of the phonetic models in DECIPHER have three states, each
M
state having a self transition and a transition to the following state
.
A
small number of phone models have two states, to allow for
short realizations.

The

context-dependent

phonetic models.

High recognition performance with HMM systems generally
requires
-
context
d
ependent version of the DECIPHER system uses phone models
trained at a variety of levels of context dependence (in addition to
c
ontext-independent models),
tri-
phone, generalized triphone, cross-word triphone (constrained to
c
onnect to appropriate contexts in preceding or following words),
left and right biphone, and generalized biphone. Models condi-
t
ioned by more speciﬁc contexts are linearly smoothed with more
general models using the deleted interpolation algorithm [6] in
o
rder to maintain robustness even in highly speciﬁc contexts that
have little training data.

including word-speciﬁc phone,

In DECIPHER words are represented as probabilistic net-
works of phone models, specifying multiple pronunciations. These
n
etworks are generated by the application of phonological rules to
baseform pronunciations for each word. To limit the number o
f
arameters that must be estimated, phonological rules are chosen
p
based on measures of coverage and overcoverage of a database o
f
p
ronunciations. This process results in networks that maximize
the coverage of observed pronunciations while minimizing net
-
w
ork size. Probabilities of pronunciations are estimated by the
forward-backward algorithm at the same time the phonetic model
s
a
re trained, after tying together instances of the same phonological
process in different words. Phonological rules can be speciﬁed to
a
pply across words, adding initial or ﬁnal arcs that are con-
strained to connect only to arcs fulﬁlling the context of the rules
.
P
honological modeling in the DECIPHER system is described in
detail by Cohen [7].

Recognition uses the Viterbi algorithm [5] to ﬁnd the HMM
state sequence (corresponding to a sentence) that has the highes
t
robability of generating the observed acoustic sequence.
p

- 3 -

2.2 Incorporating MLPs into DECIPHER
T
he hybrid MLP/HMM DECIPHER system substitutes (scaled)
probability estimates computed with MLPs for the tied-mixture
H
MM state-dependent observation probability densities. No
changes are made in the topology of the HMM system.

The initial hybrid system used an MLP to compute context-
independent phonetic probabilities for the 69 phone classes in the
D
ECIPHER system. Separate probabilities were not computed for
the different states of phone models. During the Viterbi recogni
-
t
given the phone
ion search, the probability of acoustic vector
class
, is required for each HMM state. Since MLPs
can compute Bayesian posterior probabilities, we compute the
r
equired HMM probabilities using

P (Y e q )
j

qj

Yt

,

t

(Y e q ) =
P t

j

P (q e Y )P (Y )
t
hhhhhhhhhhhhh

t

j
P (q )
j

.

(1)

j

t

Y

P (q e Y )
t

is the posterior probability of phone class

The factor
q
j
g
iven the input vector
at time . This is computed by a back-
propagation-trained [8] three-layer feed-forward MLP.
is th
e
and is estimated by counting
p
rior probability of phone class
class occurrences in the examples used to train the MLP.
s
i
c
ommon to all states for any given time frame, and can therefore
be discarded in the Viterbi computation, since it will not chang
e
t
he optimal state sequence used to get the recognized string.

P (q )j

P (Y

qj

t )

The MLP has an input layer of 234 units, spanning 9 frame
s
(
with 26 coefﬁcients for each) of cepstra, delta-cepstra, log-energy,
and delta-log-energy that are normalized to have zero mean and
u
nit variance. The hidden layer has 1000 units, and the output
layer has 69 units, one for each context-independent phonetic class
i
n the DECIPHER system. Both the hidden and output layers con-
sist of sigmoidal units.

j

qj

P (q e Y )
t

, where

The MLP is trained to estimate

is the class
associated with the middle frame of the input window. Stochasti
c
g
radient descent is used. The training signal is provided by the
HMM DECIPHER system previously trained by the forward
-
b
ackward algorithm. Forced Viterbi alignments (alignments to the
known word string) for every training sentence provide phon
e
l
abels, among 69 classes, for every frame of speech. The target
distribution is deﬁned as 1 for the index corresponding to the
p
hone class label and 0 for the other classes. A minimum relative
entropy between posterior target distribution and posterior outpu
t
d
istribution is used as a training criterion. With this training cri-
terion and target distribution, assuming enough parameters in the
M
the training does not get
stuck in a local minimum, the MLP outputs will approximate the
p
osterior class probabilities
[1]. Frame classiﬁcation on
an independent cross-validation set is used to control the learnin
g
r
ate and to decide when to stop training as in [9]. The initial
learning rate is kept constant until cross-validation performance
i
ncreases less than 0.5%, after which it is reduced as
until
performance increases no further.

LP, enough training data, and that

P (q e Y )
t

1/2n

j

3. MULTIPLE PHONETIC DISTRIBUTIONS

- 4 -

qj

q .
j

P j

for all states of the phone model for phone class

Experience with HMM-based systems has shown the importanc
e
o
f modeling phonetic units with a sequence of distributions rather
than a single one. This allows the model to capture some of the
d
ynamics of phonetic segments. SRI’s DECIPHER system models
most phones with a sequence of three HMM states. Our initia
l
h
ybrid system, described in Section 2.2, used an MLP with a sin-
gle output unit for each phone class. For any particular HMM
hone model considered during the Viterbi search, the activation
p
of the output unit corresponding to phone class
would be use
d
as

(q e Y )
t
Our initial attempt to extend the hybrid system to the model-
ing of a sequence of distributions for each phone involved increas
-
i
ng the number of output units from 69 (corresponding to phone
classes) to 200 (corresponding to the states of the HMM phon
e
m
odels). This resulted in an increase in word-recognition error
rate by almost 30%. Experiments at ICSI had a similar result [9]
.
T
he higher error rate seemed to be due to the discriminative nature
of the MLP training algorithm. The new MLP, with 200 outpu
t
u
classes,
nits, was
corresponding to HMM states. As a result, the MLP was attempt-
i
ng to discriminate into separate classes acoustic vectors that
corresponded to the same phone and, in many cases, were very
s
imilar but were aligned with different HMM states. There were
likely to have been many cases in which almost identical acousti
c
t
raining vectors were labeled as a positive example in one instance
and a negative example in another for the same output class. Th
e
a
ppropriate level at which to train discrimination is likely to be the
level of the phone (or higher) rather than the subphonetic HMM
-
s
tate level (to which these outputs units correspond).

attempting

to discriminate

subphonetic

The reason multiple-state models did not cause a simila
r
p
roblem for HMMs is that the standard HMM training procedure
is based on maximum-likelihood estimation, which uses each dat
a
p
oint to update the density model of the class it has been assigned
to. Discriminative training, such as the MLP training procedur
e
d
escribed above, focuses on modeling the boundaries between
classes.
In discriminative training, for each data point, the param
-
e
ters are adjusted to increase the probability of the correct class
and decrease the probability of the incorrect class.

We have developed a new MLP architecture that uses three
separate output layers, corresponding to the three states of HMM
p
hone models. Each output layer consists of 69 units, one for
each phonetic class. During training, only frames aligned with
ﬁ
rst states of HMM phones are presented to the ﬁrst output layer,
frames aligned with last states of HMM phones are presented to
t
he third output layer, and frames aligned with second states of
three-state HMM phones are presented to the second output layer
.
T
his MLP can be viewed as a set of three MLPs, corresponding to
the three HMM state-positions, which have the same input-to
-
h
idden weights. Since the training proceeds as if each output
layer were part of an independent net, the system learns discrimi-
n
ation between phonetic classes (as represented within each output
learn discrimination between the differen
layer), but does not
t
s
tates of the same phonetic class (because they are represented in
different output layers). During the Viterbi recognition search, th
e

- 5 -

appropriate output layer is referenced depending on which HM
M
s
tate-position is being visited. This technique has been combined
with the approach to context-dependent modeling, described next.

4. CONTEXT-DEPENDENCE

improves

recognition

phonetic models

Experience with HMM technology has shown that using context
-
d
accuracy
ependent
signiﬁcantly [10]. This is because acoustic correlates of coarticula
-
t
ory effects are modeled explicitly, producing sharper and less
overlapping probability density functions for the different phone
c
lasses. Context-dependent HMMs use different probability distri-
butions for every phone in every different relevant context. This
p
ractice reduces the amount of data available to train phones in
highly speciﬁc contexts, resulting in models that are not robust
.
T
he solution to this problem used by many HMM systems (includ-
ing DECIPHER) is to use the deleted interpolation algorithm to
l
inearly smooth models trained at different
levels of context
speciﬁcity. This approach cannot be directly extended to MLP-
ased systems because the smoothing of MLP weights makes no
b
sense.
It would be possible to use this approach to average th
e
p
robabilities from different MLPs; however, since the MLP train-
ing algorithm is a discriminant procedure, it would be desirable to
se a discriminant procedure to smooth the MLP probabilities.
u

ith MLPs was proposed by Bourlard et al. [11].

An earlier approach to context-dependent phonetic modelin
g
w
It is based on
factoring the context-dependent likelihood and uses a set of binar
y
i
nputs to the network to specify context classes. The number of
parameters and the computational load using this approach are no
t
m

uch greater than those for the original context-independent net.
We have developed a context-dependent modeling approac
h
t
hat uses a different factoring of the desired context-dependent
likelihoods, a network architecture that shares the input-to-hidden
l
ayer among the context-dependent classes to reduce the number
of parameters, and a training procedure that smooths networks
w
ith different degrees of context-dependence in order to achieve
robustness in probability estimates [12, 13].

Our

initial

implementation of

context-dependent MLPs
models generalized biphone phonetic categories. We chose a se
t
o
f eight left and eight right generalized biphone phonetic-context
classes, based principally on place of articulation and acoustic
c
haracteristics. The context-dependent architecture is shown in
Figure 1. A separate output layer (consisting of 69 output unit
s
c
orresponding to 69 context-dependent phonetic classes) is trained
for each context. The context-dependent MLP can be viewed as
a
s
et of MLPs, one for each context, which have the same input-to-
hidden weights. Separate sets of context-dependent output layers
a
re used to model context effects in different states of HMM
phone models,
thereby combining the modeling of multipl
e
p
honetic distributions described in Section 3 with context-
dependent modeling. During training and recognition, speech
f
rames aligned with ﬁrst states of HMM phones are associated
with the appropriate left context output layer, those aligned wit
h
l
ast states of HMM phones are associated with the appropriate

- 6 -

right context output layer, and middle states of three state model
s
a
re associated with the context-independent output layer. As a
result, since the training proceeds (as before) as if each outpu
t
l
ayer were part of an independent net, the system learns discrimi-
nation between the different phonetic classes within an outpu
t
l
ayer (which now corresponds to a speciﬁc context and HMM-
state position), but does not
learn discrimination between
o
ccurrences of the same phone in different contexts or between the
different states of the same HMM phone.
4
.1 Context-Dependent Factoring
In a context-dependent HMM, every state is associated with
a
peciﬁc phone class and context. During the Viterbi recognition
s
search,
e
given th
t
) is required for each state.
p
hone class
We compute the required HMM probabilities using

P (Y e q ,c )
k
in the context class

(the probability of acoustic vector

j
q

Yt

c

k

j

(Y e q ,c ) =
P t

k

j

P (q e Y ,c )P (Y e c )
k
hhhhhhhhhhhhhhhhhh

j

t

k

t
P (q e c )
k

j

where

P (Y e c )
k

t

can be factored again as

(Y e c ) =
P t

k

P (c e Y )P (Y )
t
hhhhhhhhhhhhh

t

k
P (c )
k

.

(2)

(3)

j

Y
t

P (q e Y c )
t , k

and the context class

is the posterior probability of phone class

The factor
q
j
g
. To compute
iven the input vector
this factor, we use a direct interpretation of the deﬁnition of condi
-
t
in (2) as res-
ional probability, considering the conditioning on
tricting the set of input vectors only to those produced in the con
-
t
is the number of context classes, this implementation
ext
uses a set of MLPs (all sharing the same input-to-hidden layer)
M
similar to those used in the context-independent case except tha
t
e
ach MLP is trained using only input-output examples obtained
from the corresponding context,

. If

ck

M

c

c

k

k

.ck

Every context-speciﬁc net performs a simpler classiﬁcation
than in the context-independent case because within a context th
e
coustics corresponding to different phones have less overlap.
a

k

and

P (c e Y )
t

is computed using a three-layer feed-forward ML
P
with an output unit corresponding to each context class, the same
234 inputs as the MLP described above, and 512 hidden units
.
are estimated by counting over the training
(q e c )
P (c )
P j
k
k
e
is common to all states for any given
xamples. Finally,
P (Y )
t
time frame, and can therefore be discarded in the Viterbi computa
-
t
ion, since it will not change the optimal state sequence used to
get the recognized string.
4
.2 Context-Dependent Training and Smoothing
In order to achieve robust training of context-speciﬁc nets, we us
t
he following method:

e

An initial context-independent MLP is trained, as described
in Section 2, to estimate the context-independent posterior proba
-
b
ilities over the N phone classes. After the context-independent
training converges, the resulting weights are used to initialize the
w
layers. Context-
dependent training proceeds by backpropagating error only from

eights going to the context-speciﬁc output

- 7 -

the appropriate output layer for each training example. Otherwise
,
t
he training procedure is similar to that for the context-independent
net, using stochastic gradient descent and a relative-entropy train-
i
ng criterion. Overall classiﬁcation performance evaluated on an
independent cross-validation set is used to determine the learnin
g
r
ate and stopping point. Only hidden-to-output weights are
training. We can view th
adjusted during context-dependent
e
s
eparate output layers as belonging to independent nets, each one
trained on a non-overlapping subset of the original training data.

j

t

j

P (q e Y )
t

P (q e Y ,c )
k

Every context-speciﬁc net would asymptotically converge to
the context conditioned posteriors
-
given enough train
ing data and training iterations. As a result of the initialization
,
t
, and from that point it follows a
he net starts estimating
trajectory in weight space, incrementally moving away from the
context-independent parameters as long as classiﬁcation perfor
-
m
ance on the cross-validation set improves. As a result, the net
retains useful information from the context-independent initial con-
d
itions.
In this way, we perform a type of nonlinear smoothing
between the pure context-independent parameters and the pur
e
c
ontext-dependent parameters.
4.3 Recognition
T
he smoothed context-dependent posterior probabilities supplied
by the MLP have to be converted during recognition to (scaled
)
s
tate-conditioned observation probabilities using the normalization
factors provided by Eqs. (2) and (3). However, because thes
e
v
alues are a result of smoothing context-dependent and indepen-
dent networks, the normalization factors should be a combination
o
f
those corresponding to the context-dependent and context-
independent cases. We use the following heuristics for converting
t
he smoothed posteriors
to smoothed (scaled) observa-
tion probabilities

s

j

t

s
P (q e Y c )
t , k
P (Y e q c :
j , k )
I
J
L

k
j

k

j

t

h 1hhhhh + (1 -
P (q )
j

P s

(Y e q ,c ) = P (q e Y ,c )

s

k

t

j

k
j

)

k

P (c e Y )
t

hhhhhhhhhhhhh
P (q e c )P (c )
k

k

j

M
J
O

(4)

(5)

where

=

k
j

N (j )

ci

hhhhhhhhhhhhhhhhh
N (j )+b [N (j ,k )]
ci

d

c

.

is the number of training examples for phone class

N (j )
ci
context-independent net.
ples for the context-dependent net for phone class
c
ontext class
inimum word-recognition error.
m

for the
is the number of training exam
-
e
and for th
is optimized on a development set for

. Constant

N (j ,k )

b

cd

k

j

j

5. EVALUATION

5.1 Methods
T
raining and recognition experiments comparing the MLP/HMM
hybrid to the pure HMM DECIPHER system were conducte
d
u
continuous-speech, DARPA
sing
Resource Management database. The vocabulary size is 99
8
w
ords. Tests were run both with a word-pair (perplexity 60) gram-
mar and with no grammar. The training set for the HMM system

speaker-independent,

the

a
a
a
- 8 -

consisted of the 3990 sentences that make up the standard DARP
A
s
peaker-independent
training set for the Resource Management
task. To train the MLP, the training set was divided into a set of
3
510 sentences for adjusting weights during back-propagation
training, and 480 sentences for cross-validation. None of the test
s
d
escribed here use the gender-dependent DECIPHER system,
which is described elsewhere [2].
T
pure HMM and hybrid MLP/HMM with no grammar.

able 1: Number of system parameters and percent word error for

iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
# Parameters
i
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
i CI-HMM
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
i CI-MLP
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
i CD-MLP
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
iCD-HMM
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
i MIXED
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii

Feb91
44.8
25.0
20.5
19.7
16.9

Feb89
44.1
24.9
19.4
22.1
17.2

125K
311K
1,409K
5,541K
5,853K

Oct89
45.3
27.0
20.8
21.7
18.9

Table 2: Percent word error
M

LP/HMM with word-pair grammar.

for pure HMM and hybri

d

iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
i
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
Feb91i
10.8 i
CI-HMM
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
6.2 i
CI-MLP
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
5.0 i
CD-MLP
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
CD-HMM
4.0 i
iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
MIXED
3.3 i
ciiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii

Feb89
14.1
5.4
4.7
5.0
3.9

Oct89
14.0
7.6
5.7
4.9
4.1

5.2 Results
T
able 1 presents word recognition error and number of system
parameters for ﬁve different versions of the system, for thre
e
d
ifferent Resource Management test sets (February 89, October 89,
and February 91) using the word-pair grammar. Table 2 presents
w
ord recognition error for the corresponding tests with no gram-
mar (the number of parameters are the same as those shown in
T

able 1).

Comparing

context-independent HMM (CI-HMM)

to
context-independent MLP (CI-MLP) consistently shows very sub
-
s
tantial reductions in error rates using MLPs to estimate context-
independent observation probabilities. One should keep in mind
hat although the CI-MLP computes context-independent phonetic
t
probabilities of the form
, it can make use of some acousti
c
context, since the input window scans nine frames. However, this
acoustic context is typically on a different timescale than that fo
r
p
conceptually from computing
honetic
context-dependent probabilities. The CI-MLP system, which use
s
a
bout 300,000 parameters, showed performance close to the range
shown by other systems listed with many more parameters.

and differs

context,

P (q e Y )
t

j

The context-dependent HMM system (CD-HMM) performs
consistently better than the CI-MLP system. This is not surpris
-
ng, since the CD-HMM system is far more complex, with almost
i
a factor of 20 more parameters, context-dependent phone models
,
a
nd a sequence of (two or three) states with different observation
distributions for each phone model. The CI-MLP system has
a

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
- 9 -

I
single context-independent output unit for each phonetic class.
t
s, in fact, interesting that such a relatively simple system performs
i
as well as it does compared with far more complex systems.

The MIXED system uses a weighted mixture of the logs of
state observation likelihoods provided by the CI-MLP and the
C
D-HMM [9]. This system shows the best recognition perfor-
mance so far achieved with the DECIPHER system on the
R
esource Management database.
In all six tests, it performs better
than the pure CD-HMM system.

3 with

the

context-dependent modeling

MM does better and in three cases CD-MLP does better.

Comparing CI-MLP to context-dependent MLP (CD-MLP)
shows improvements with CD-MLP in all six tests, with an aver
-
a
ge reduction in word error of 20%. The CD-MLP system com-
bines the multiple-distribution modeling technique described in
S
ection
technique
described in Section 4. The CD-MLP system has
roughly
e
quivalent recognition performance to the CD-HMM system (with
roughly one fourth the number of parameters): in three cases CD
-
H
5.3 Discussion
T
he results shown in Tables 1 and 2 suggest that MLP estimation
of HMM observation likelihoods can improve the performance o
f
s
tandard HMMs. These results also suggest that systems that use
MLP-based probability estimation make more efﬁcient use of thei
r
p
arameters than standard HMM systems. In standard HMMs, most
of the parameters in the system are in the distributions associated
w
ith the individual states. MLPs use representations that are more
distributed in nature, allowing more sharing of representationa
l
esources and better allocation of representational resources based
r
on training.
In addition, since MLPs are trained to discriminat
e
b
they focus on modeling boundaries between
etween classes,
classes rather than class internals. HMMs which model gender
-
b
ased speech consistencies typically have twice the number of
parameters of gender-independent HMMs. For example, a version
o
f the DECIPHER system that models male and female speech
separately has more than 11 million parameters. Abrash et al. [2
]
a
nd Konig et al. [3] discuss methods for exploiting the distributed
nature of MLPs to model gender-based speech consistencies wit
h
o
nly a relatively small increase in the number of parameters.

The distributed representation used by MLPs is exploited i
n
t
he context-dependent modeling approach described in Section 4
by sharing the input-to-hidden layer weights between all contex
t
c
lasses. This sharing substantially reduces the number of parame-
ters to train and the amount of computation required during both
t
raining and recognition. The context-dependent MLP has 17
times as many output units (classes) as the context-independen
t
M
LP, but has only a factor of 4.6 times as many parameters. In
addition, we do not adjust the input-to-hidden weights during th
e
c
the features
ontext-dependent phase of training, assuming that
provided by the hidden layer activations are relatively low leve
l
a
nd are appropriate for context-dependent as well as context-
independent modeling. This procedure
substantially reduces
training time. The large decrease in cross-
c
ontext-dependent
validation error observed going from context-independent
to

- 10 -

learned by the hidden layer during the

e
th
context-dependent MLPs (30.6% to 21.4%) suggests that
context-
f
eatures
independent
training phase, combined with the extra modeling
p
ower of the context-speciﬁc hidden-to-output layers, were ade-
quate to capture the more detailed context-speciﬁc phone classes.

One should keep in mind that the reduction in memory needs
that may be attained by replacing HMM distributions with MLP
-
b
ased estimates must be traded off against increased computational
load during both training and recognition. The MLP computations
d
uring training and recognition are much larger
than the
corresponding Gaussian mixture computations for HMM systems.

The performance of

the CD-MLP system was

roughly
equivalent to that of the CD-HMM, although CD-MLP is a fa
r
s
impler system, with approximately a factor of four fewer parame-
ters and modeling of only generalized biphone phonetic contexts
.
W
e are currently extending the CD-MLP system to the modeling
of more speciﬁc context classes, with the hope of exceeding th
e
r
ecognition performance of the CD-HMM.

The best performance shown in Tables 1 and 2 is that of th
e
M
IXED system, which combines CI-MLP and CD-HMM probabil-
ities. The CD-MLP probabilities can also be combined with CD
-
H
MM probabilities; however, we expect that the planned extension
of our CD-MLP system to ﬁner contexts will lead to a system tha
t
p
erforms better than our current best system without the need for
such mixing, therefore resulting in a simpler system.

6. CONCLUSIONS

MLP-based probability estimation can be useful for both improv
-
ng recognition accuracy and reducing memory needs for HMM-
i
based speech recognition systems. These beneﬁts, however, mus
t
b
e weighed against the increased computational requirements using
MLPs, especially during training.

We have demonstrated a number of systems of different sizes
and complexities, and shown some of the tradeoffs between sys
-
t
In the near future we
em size, complexity, and performance.
intend to extend our system to the modeling of ﬁner phonetic con
-
t
exts and to combine our context-dependent MLPs with MLPs that
model gender-based speech consistencies.

ACKNOWLEDGEMENTS

The work reported here was partially supported by DARPA Contrac
M

DA904-90-C-5253. Talks with Herve Bourlard were very helpful.

t

REFERENCES

lburquerque, New Mexico, 1990.

[
1] N. Morgan and H. Bourlard, ‘‘Continuous Speech Recognition Using Mul-
tilayer Perceptrons with Hidden Markov Models,’’ ICASSP 90 , pp. 413-416
,
A
[2] V. Abrash, H. Franco, M. Cohen, N. Morgan, and Y. Konig, ‘‘Connectionist
Gender Adaptation in a Hybrid Neural Network/Hidden Markov Model Speech
R
[3] Y. Konig, N. Morgan, and C. Chandra, ‘‘GDNN: A Gender-Dependent
Neural Network for Continuous Speech Recognition,’’ International Computer
cience Institute Technical Report TR-91-071, December 1991.
S

ecognition System,’’ this volume.

- 11

-

[4] H. Murveit, M. Cohen, P. Price, G. Baldwin, M. Weintraub, and J. Bern
-
s
tein, ‘‘SRI’s DECIPHER System,’’ DARPA Speech and Natural Language
Workshop, February 1989.
[
5] S. E. Levinson, L. R. Rabiner, and M.M. Sondhi, ‘‘An introduction to the
application of the theory of probabilistic functions of a Markov process to
a
utomatic speech recognition,’’ Bell Syst. Tech. Journal 62, 1035-1074, 1983.
[6] F. Jelinek and R. L. Mercer, ‘‘Interpolated estimation of markov sourc
e
p
in Pattern Recognition in Practice, E. S.
arameters from sparse data,’’
Gelsema and L. N Kanal, Eds. Amsterdam: North-Holland, 1980, pp. 381-397.
[
7] M. Cohen, ‘‘Phonological Structures for Speech Recognition,’’ PhD thesis,
Computer Science Department, UC Berkeley, April 1989.
[
8] D. Rumelhart, G. Hinton, and R. Williams, ‘‘Learning Internal Representa-
tions by Error Propagation,’’ in Parallel Distributed Processing: Explorations
f the Microstructure of Cognition, vol 1: Foundations, Ed D Rumelhart & J.
o
McClelland. MIT Press, 1986.
[
9] S. Renals, N. Morgan, M. Cohen, H. Franco, ‘‘Connectionist Probability
Estimation in the DECIPHER Speech Recognition System,’’ ICASSP 92, Vol
.
1
, pp. 601-604, San Francisco, 1992.
[10] R. M. Schwartz, Y. L. Chow, O. A. Kimball, S. Roucos, M. Krasner, and
J. Makhoul, ‘‘Context-dependent modeling for acoustic-phonetic recognition o
f
c
ontinuous speech,’’ ICASSP 85, 1205-1208, 1985.
[11] H. Bourlard, N. Morgan, C. Wooters, S Renals, ‘‘CDNN: A Contex
t
D
ependent Neural Network for Continuous Speech Recognition,’’ ICASSP 92,
Vol. 2, pp. 349-352, San Francisco, 1992.
[
12] H. Franco, M. Cohen, N. Morgan, D. Rumelhart, V. Abrash, ‘‘Context-
Dependent Connectionist Probability Estimation in a Hybrid HNN-Neural Ne
t
S
peech Recognition System,’’ in press, IJCNN, Beijing, 1992.
[13] M. Cohen, H. Franco, N. Morgan, D. Rumelhart, V. Abrash, ‘‘Multiple
-
S
tate Context-Dependent Phonetic Modeling with MLPs,’’ Proceedings Speech
Research Symposium XII, Rutgers, 1992.

F
igure 1: Context-dependent MLP.

