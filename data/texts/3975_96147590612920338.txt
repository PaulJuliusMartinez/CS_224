Computer Vision
A MODERN APPROACH

second edition

David A. Forsyth

University of Illinois at Urbana-Champaign

Jean Ponce

Ecole Normale Supérieure

Boston Columbus Indianapolis New York San Francisco Upper Saddle River

Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto

Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo

013608592X_Forsythe_title.indd 1

9/30/11 2:18 PM

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedVice President and Editorial Director, ECS:

Marcia Horton

Editor in Chief: Michael Hirsch
Executive Editor: Tracy Dunkelberger
Senior Project Manager: Carole Snyder
Vice President Marketing: Patrice Jones
Marketing Manager: Yez Alayan
Marketing Coordinator: Kathryn Ferranti
Marketing Assistant: Emma Snider
Vice President and Director of Production:

Vince O’Brien

Managing Editor: Jeff Holcomb

Senior Production Project Manager: Marilyn Lloyd
Senior Operations Supervisor: Alan Fischer
Operations Specialist: Lisa McDowell
Art Director, Cover: Jayne Conte
Text Permissions: Dana Weightman/RightsHouse,

Inc. and Jen Roach/PreMediaGlobal

Cover Image: © Maxppp/ZUMAPRESS.com
Media Editor: Dan Sandin
Composition: David Forsyth
Printer/Binder: Edwards Brothers
Cover Printer: Lehigh-Phoenix Color

Credits and acknowledgments borrowed from other sources and reproduced, with permission, in this textbook
appear on the appropriate page within text.

Copyright © 2012, 2003 by Pearson Education, Inc., publishing as Prentice Hall. All rights reserved.
Manufactured in the United States of America. This publication is protected by Copyright, and permission
should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or
transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To
obtain permission(s) to use material from this work, please submit a written request to Pearson Education,
Inc., Permissions Department, One Lake Street, Upper Saddle River, New Jersey 07458, or you may fax
your request to 201-236-3290.

Many of the designations by manufacturers and sellers to distinguish their products are claimed as trade-
marks. Where those designations appear in this book, and the publisher was aware of a trademark claim,
the designations have been printed in initial caps or all caps.

Library of Congress Cataloging-in-Publication Data available upon request

10 9 8 7 6 5 4 3 2 1

ISBN-13: 978-0-13-608592-8
ISBN-10: 0-13-608592-X

013608592X_Forsythe_title.indd 2

9/30/11 2:18 PM

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedTo my family—DAF

To my father, Jean-Jacques Ponce —JP

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedContents

I

IMAGE FORMATION

1 Geometric Camera Models

1.1

1.2

Image Formation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Pinhole Perspective
. . . . . . . . . . . . . . . . . . . . . . .
1.1.2 Weak Perspective . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.3 Cameras with Lenses . . . . . . . . . . . . . . . . . . . . . . .
1.1.4 The Human Eye . . . . . . . . . . . . . . . . . . . . . . . . .
Intrinsic and Extrinsic Parameters . . . . . . . . . . . . . . . . . . .
1.2.1 Rigid Transformations and Homogeneous Coordinates . . . .
1.2.2
Intrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . .
1.2.3 Extrinsic Parameters . . . . . . . . . . . . . . . . . . . . . . .
1.2.4 Perspective Projection Matrices . . . . . . . . . . . . . . . . .
1.2.5 Weak-Perspective Projection Matrices . . . . . . . . . . . . .
1.3 Geometric Camera Calibration . . . . . . . . . . . . . . . . . . . . .
1.3.1 A Linear Approach to Camera Calibration . . . . . . . . . . .
1.3.2 A Nonlinear Approach to Camera Calibration . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Notes

2 Light and Shading

2.2

2.1 Modelling Pixel Brightness

. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
2.1.1 Reﬂection at Surfaces
2.1.2
Sources and Their Eﬀects . . . . . . . . . . . . . . . . . . . .
2.1.3 The Lambertian+Specular Model . . . . . . . . . . . . . . . .
2.1.4 Area Sources . . . . . . . . . . . . . . . . . . . . . . . . . . .
Inference from Shading . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Radiometric Calibration and High Dynamic Range Images . .
2.2.2 The Shape of Specularities
. . . . . . . . . . . . . . . . . . .
Inferring Lightness and Illumination . . . . . . . . . . . . . .
2.2.3
. .
2.2.4 Photometric Stereo: Shape from Multiple Shaded Images
2.3 Modelling Interreﬂection . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 The Illumination at a Patch Due to an Area Source
. . . . .
2.3.2 Radiosity and Exitance
. . . . . . . . . . . . . . . . . . . . .
2.3.3 An Interreﬂection Model . . . . . . . . . . . . . . . . . . . . .
2.3.4 Qualitative Properties of Interreﬂections . . . . . . . . . . . .
2.4 Shape from One Shaded Image . . . . . . . . . . . . . . . . . . . . .

1

3
4
4
6
8
12
14
14
16
18
19
20
22
23
27
29

32
32
33
34
36
36
37
38
40
43
46
52
52
54
55
56
59

v

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved2.5 Notes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

61

vi

3 Color

3.2.1 The Color of Light Sources
3.2.2 The Color of Surfaces

3.1 Human Color Perception . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 Color Matching . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.1.2 Color Receptors
3.2 The Physics of Color . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
3.3 Representing Color . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 Linear Color Spaces
. . . . . . . . . . . . . . . . . . . . . . .
3.3.2 Non-linear Color Spaces . . . . . . . . . . . . . . . . . . . . .
3.4 A Model of Image Color . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 The Diﬀuse Term . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.2 The Specular Term . . . . . . . . . . . . . . . . . . . . . . . .
Inference from Color . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.1 Finding Specularities Using Color
. . . . . . . . . . . . . . .
3.5.2
Shadow Removal Using Color . . . . . . . . . . . . . . . . . .
3.5.3 Color Constancy: Surface Color from Image Color . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6 Notes

3.5

68
68
68
71
73
73
76
77
77
83
86
88
90
90
90
92
95
99

II EARLY VISION: JUST ONE IMAGE

105

4 Linear Filters

4.2 Shift Invariant Linear Systems

107
4.1 Linear Filters and Convolution . . . . . . . . . . . . . . . . . . . . . 107
4.1.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
. . . . . . . . . . . . . . . . . . . . . 112
4.2.1 Discrete Convolution . . . . . . . . . . . . . . . . . . . . . . . 113
4.2.2 Continuous Convolution . . . . . . . . . . . . . . . . . . . . . 115
4.2.3 Edge Eﬀects in Discrete Convolutions
. . . . . . . . . . . . . 118
4.3 Spatial Frequency and Fourier Transforms . . . . . . . . . . . . . . . 118
4.3.1 Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . 119
4.4 Sampling and Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.4.1
Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4.2 Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
Smoothing and Resampling . . . . . . . . . . . . . . . . . . . 126
4.4.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.5.1 Convolution as a Dot Product
. . . . . . . . . . . . . . . . . 131
4.5.2 Changing Basis . . . . . . . . . . . . . . . . . . . . . . . . . . 132
. . . . . . 132

4.6 Technique: Normalized Correlation and Finding Patterns

4.5 Filters as Templates

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedvii

4.6.1 Controlling the Television by Finding Hands by Normalized

Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.7 Technique: Scale and Image Pyramids . . . . . . . . . . . . . . . . . 134
4.7.1 The Gaussian Pyramid . . . . . . . . . . . . . . . . . . . . . 135
4.7.2 Applications of Scaled Representations . . . . . . . . . . . . . 136
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

4.8 Notes

5 Local Image Features

141
5.1 Computing the Image Gradient . . . . . . . . . . . . . . . . . . . . . 141
5.1.1 Derivative of Gaussian Filters . . . . . . . . . . . . . . . . . . 142
5.2 Representing the Image Gradient . . . . . . . . . . . . . . . . . . . . 144
5.2.1 Gradient-Based Edge Detectors . . . . . . . . . . . . . . . . . 145
5.2.2 Orientations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.3 Finding Corners and Building Neighborhoods . . . . . . . . . . . . . 148
5.3.1 Finding Corners
. . . . . . . . . . . . . . . . . . . . . . . . . 149
5.3.2 Using Scale and Orientation to Build a Neighborhood . . . . 151
. . . . . . 155
. . . . . . . . . . . . . . . . . . . . . . . . . . 157
. . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.5 Computing Local Features in Practice . . . . . . . . . . . . . . . . . 160
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.6 Notes

5.4 Describing Neighborhoods with SIFT and HOG Features

5.4.1
SIFT Features
5.4.2 HOG Features

6 Texture

6.3 Synthesizing Textures and Filling Holes in Images

164
6.1 Local Texture Representations Using Filters . . . . . . . . . . . . . . 166
6.1.1
Spots and Bars . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.1.2 From Filter Outputs to Texture Representation . . . . . . . . 168
6.1.3 Local Texture Representations in Practice . . . . . . . . . . . 170
6.2 Pooled Texture Representations by Discovering Textons . . . . . . . 171
6.2.1 Vector Quantization and Textons . . . . . . . . . . . . . . . . 172
6.2.2 K-means Clustering for Vector Quantization . . . . . . . . . . 172
. . . . . . . . . . 176
6.3.1
Synthesis by Sampling Local Models . . . . . . . . . . . . . . 176
6.3.2 Filling in Holes in Images . . . . . . . . . . . . . . . . . . . . 179
Image Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.4.1 Non-local Means . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.4.2 Block Matching 3D (BM3D)
. . . . . . . . . . . . . . . . . . 183
6.4.3 Learned Sparse Coding . . . . . . . . . . . . . . . . . . . . . 184
6.4.4 Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
. . . . . . . . . . . . . . . . . . . . . . . . . . . 187
Shape from Texture for Planes
. . . . . . . . . . . . . . . . . 187
Shape from Texture for Curved Surfaces . . . . . . . . . . . . 190

6.5 Shape from Texture

6.4

6.5.1
6.5.2

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved6.6 Notes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

viii

III EARLY VISION: MULTIPLE IMAGES

195

7 Stereopsis

7.2.1

197
7.1 Binocular Camera Geometry and the Epipolar Constraint . . . . . . 198
7.1.1 Epipolar Geometry . . . . . . . . . . . . . . . . . . . . . . . . 198
7.1.2 The Essential Matrix . . . . . . . . . . . . . . . . . . . . . . . 200
7.1.3 The Fundamental Matrix . . . . . . . . . . . . . . . . . . . . 201
7.2 Binocular Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 201
Image Rectiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 202
7.3 Human Stereopsis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
7.4 Local Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 205
7.4.1 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
7.4.2 Multi-Scale Edge Matching . . . . . . . . . . . . . . . . . . . 207
7.5 Global Methods for Binocular Fusion . . . . . . . . . . . . . . . . . . 210
7.5.1 Ordering Constraints and Dynamic Programming . . . . . . . 210
7.5.2
Smoothness and Graphs . . . . . . . . . . . . . . . . . . . . . 211
7.6 Using More Cameras . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
7.7 Application: Robot Navigation . . . . . . . . . . . . . . . . . . . . . 215
7.8 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

8 Structure from Motion

8.1

221
Internally Calibrated Perspective Cameras . . . . . . . . . . . . . . . 221
8.1.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 223
8.1.2 Euclidean Structure and Motion from Two Images . . . . . . 224
8.1.3 Euclidean Structure and Motion from Multiple Images . . . . 228
8.2 Uncalibrated Weak-Perspective Cameras . . . . . . . . . . . . . . . . 230
8.2.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 231
8.2.2 Aﬃne Structure and Motion from Two Images
. . . . . . . . 233
8.2.3 Aﬃne Structure and Motion from Multiple Images . . . . . . 237
8.2.4 From Aﬃne to Euclidean Shape
. . . . . . . . . . . . . . . . 238
. . . . . . . . . . . . . . . . . . . 240
8.3.1 Natural Ambiguity of the Problem . . . . . . . . . . . . . . . 241
8.3.2 Projective Structure and Motion from Two Images . . . . . . 242
8.3.3 Projective Structure and Motion from Multiple Images . . . . 244
8.3.4 From Projective to Euclidean Shape . . . . . . . . . . . . . . 246
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248

8.3 Uncalibrated Perspective Cameras

8.4 Notes

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedIV MID-LEVEL VISION

ix

253

9 Segmentation by Clustering

9.3

9.1 Human Vision: Grouping and Gestalt
9.2

255
. . . . . . . . . . . . . . . . . 256
. . . . . . . . . . . . . . . . . . . . . . . . . 261
Important Applications
9.2.1 Background Subtraction . . . . . . . . . . . . . . . . . . . . . 261
Shot Boundary Detection . . . . . . . . . . . . . . . . . . . . 264
9.2.2
9.2.3
Interactive Segmentation . . . . . . . . . . . . . . . . . . . . 265
9.2.4 Forming Image Regions . . . . . . . . . . . . . . . . . . . . . 266
Image Segmentation by Clustering Pixels
. . . . . . . . . . . . . . . 268
9.3.1 Basic Clustering Methods . . . . . . . . . . . . . . . . . . . . 269
9.3.2 The Watershed Algorithm . . . . . . . . . . . . . . . . . . . . 271
9.3.3
Segmentation Using K-means . . . . . . . . . . . . . . . . . . 272
9.3.4 Mean Shift: Finding Local Modes in Data . . . . . . . . . . . 273
9.3.5 Clustering and Segmentation with Mean Shift . . . . . . . . . 275
9.4 Segmentation, Clustering, and Graphs . . . . . . . . . . . . . . . . . 277
9.4.1 Terminology and Facts for Graphs . . . . . . . . . . . . . . . 277
9.4.2 Agglomerative Clustering with a Graph . . . . . . . . . . . . 279
9.4.3 Divisive Clustering with a Graph . . . . . . . . . . . . . . . . 281
9.4.4 Normalized Cuts . . . . . . . . . . . . . . . . . . . . . . . . . 284
Image Segmentation in Practice . . . . . . . . . . . . . . . . . . . . . 285
9.5.1 Evaluating Segmenters . . . . . . . . . . . . . . . . . . . . . . 286
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

9.5

9.6 Notes

10 Grouping and Model Fitting

290
10.1 The Hough Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 290
10.1.1 Fitting Lines with the Hough Transform . . . . . . . . . . . . 290
10.1.2 Using the Hough Transform . . . . . . . . . . . . . . . . . . . 292
10.2 Fitting Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . 293
10.2.1 Fitting a Single Line . . . . . . . . . . . . . . . . . . . . . . . 294
10.2.2 Fitting Planes
. . . . . . . . . . . . . . . . . . . . . . . . . . 295
10.2.3 Fitting Multiple Lines . . . . . . . . . . . . . . . . . . . . . . 296
10.3 Fitting Curved Structures . . . . . . . . . . . . . . . . . . . . . . . . 297
10.4 Robustness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
10.4.1 M-Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
10.4.2 RANSAC: Searching for Good Points
. . . . . . . . . . . . . 302
10.5 Fitting Using Probabilistic Models . . . . . . . . . . . . . . . . . . . 306
10.5.1 Missing Data Problems
. . . . . . . . . . . . . . . . . . . . . 307
10.5.2 Mixture Models and Hidden Variables . . . . . . . . . . . . . 309
10.5.3 The EM Algorithm for Mixture Models
. . . . . . . . . . . . 310
10.5.4 Diﬃculties with the EM Algorithm . . . . . . . . . . . . . . . 312

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedx

10.6 Motion Segmentation by Parameter Estimation . . . . . . . . . . . . 313
10.6.1 Optical Flow and Motion . . . . . . . . . . . . . . . . . . . . 315
. . . . . . . . . . . . . . . . . . . . . . . . . . . 316
10.6.2 Flow Models
10.6.3 Motion Segmentation with Layers
. . . . . . . . . . . . . . . 317
10.7 Model Selection: Which Model Is the Best Fit? . . . . . . . . . . . . 319
10.7.1 Model Selection Using Cross-Validation . . . . . . . . . . . . 322
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322

10.8 Notes

11 Tracking

326
11.1 Simple Tracking Strategies . . . . . . . . . . . . . . . . . . . . . . . . 327
11.1.1 Tracking by Detection . . . . . . . . . . . . . . . . . . . . . . 327
11.1.2 Tracking Translations by Matching . . . . . . . . . . . . . . . 330
11.1.3 Using Aﬃne Transformations to Conﬁrm a Match . . . . . . 332
11.2 Tracking Using Matching . . . . . . . . . . . . . . . . . . . . . . . . 334
11.2.1 Matching Summary Representations . . . . . . . . . . . . . . 335
11.2.2 Tracking Using Flow . . . . . . . . . . . . . . . . . . . . . . . 337
11.3 Tracking Linear Dynamical Models with Kalman Filters . . . . . . . 339
11.3.1 Linear Measurements and Linear Dynamics . . . . . . . . . . 340
11.3.2 The Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . 344
11.3.3 Forward-backward Smoothing . . . . . . . . . . . . . . . . . . 345
11.4 Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
11.4.1 Linking Kalman Filters with Detection Methods
. . . . . . . 349
11.4.2 Key Methods of Data Association . . . . . . . . . . . . . . . 350
11.5 Particle Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
11.5.1 Sampled Representations of Probability Distributions
. . . . 351
11.5.2 The Simplest Particle Filter . . . . . . . . . . . . . . . . . . . 355
11.5.3 The Tracking Algorithm . . . . . . . . . . . . . . . . . . . . . 356
11.5.4 A Workable Particle Filter . . . . . . . . . . . . . . . . . . . . 358
11.5.5 Practical Issues in Particle Filters
. . . . . . . . . . . . . . . 360
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362

11.6 Notes

V HIGH-LEVEL VISION

365

12 Registration

367
12.1 Registering Rigid Objects . . . . . . . . . . . . . . . . . . . . . . . . 368
12.1.1 Iterated Closest Points . . . . . . . . . . . . . . . . . . . . . . 368
12.1.2 Searching for Transformations via Correspondences . . . . . . 369
12.1.3 Application: Building Image Mosaics . . . . . . . . . . . . . . 370
12.2 Model-based Vision: Registering Rigid Objects with Projection . . . 375

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved12.2.1 Veriﬁcation: Comparing Transformed and Rendered Source

xi

to Target

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
12.3 Registering Deformable Objects . . . . . . . . . . . . . . . . . . . . . 378
12.3.1 Deforming Texture with Active Appearance Models
. . . . . 378
12.3.2 Active Appearance Models in Practice . . . . . . . . . . . . . 381
12.3.3 Application: Registration in Medical Imaging Systems . . . . 383
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

12.4 Notes

13 Smooth Surfaces and Their Outlines

391
13.1 Elements of Diﬀerential Geometry . . . . . . . . . . . . . . . . . . . 393
13.1.1 Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
13.1.2 Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
13.2 Contour Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
13.2.1 The Occluding Contour and the Image Contour . . . . . . . . 402
13.2.2 The Cusps and Inﬂections of the Image Contour
. . . . . . . 403
13.2.3 Koenderink’s Theorem . . . . . . . . . . . . . . . . . . . . . . 404
13.3 Visual Events: More Diﬀerential Geometry . . . . . . . . . . . . . . 407
13.3.1 The Geometry of the Gauss Map . . . . . . . . . . . . . . . . 407
13.3.2 Asymptotic Curves . . . . . . . . . . . . . . . . . . . . . . . . 409
13.3.3 The Asymptotic Spherical Map . . . . . . . . . . . . . . . . . 410
13.3.4 Local Visual Events
. . . . . . . . . . . . . . . . . . . . . . . 412
13.3.5 The Bitangent Ray Manifold . . . . . . . . . . . . . . . . . . 413
13.3.6 Multilocal Visual Events . . . . . . . . . . . . . . . . . . . . . 414
13.3.7 The Aspect Graph . . . . . . . . . . . . . . . . . . . . . . . . 416
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417

13.4 Notes

14 Range Data

422
14.1 Active Range Sensors
. . . . . . . . . . . . . . . . . . . . . . . . . . 422
14.2 Range Data Segmentation . . . . . . . . . . . . . . . . . . . . . . . . 424
14.2.1 Elements of Analytical Diﬀerential Geometry . . . . . . . . . 424
14.2.2 Finding Step and Roof Edges in Range Images . . . . . . . . 426
14.2.3 Segmenting Range Images into Planar Regions . . . . . . . . 431
14.3 Range Image Registration and Model Acquisition . . . . . . . . . . . 432
14.3.1 Quaternions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
14.3.2 Registering Range Images . . . . . . . . . . . . . . . . . . . . 434
14.3.3 Fusing Multiple Range Images
. . . . . . . . . . . . . . . . . 436
14.4 Object Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
14.4.1 Matching Using Interpretation Trees . . . . . . . . . . . . . . 438
14.4.2 Matching Free-Form Surfaces Using Spin Images . . . . . . . 441
14.5 Kinect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
14.5.1 Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved14.5.2 Technique: Decision Trees and Random Forests . . . . . . . . 448
14.5.3 Labeling Pixels . . . . . . . . . . . . . . . . . . . . . . . . . . 450
14.5.4 Computing Joint Positions
. . . . . . . . . . . . . . . . . . . 453
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453

14.6 Notes

xii

15 Learning to Classify

15.2 Major Classiﬁcation Strategies

457
15.1 Classiﬁcation, Error, and Loss . . . . . . . . . . . . . . . . . . . . . . 457
15.1.1 Using Loss to Determine Decisions . . . . . . . . . . . . . . . 457
15.1.2 Training Error, Test Error, and Overﬁtting . . . . . . . . . . 459
15.1.3 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 460
15.1.4 Error Rate and Cross-Validation . . . . . . . . . . . . . . . . 463
15.1.5 Receiver Operating Curves
. . . . . . . . . . . . . . . . . . . 465
. . . . . . . . . . . . . . . . . . . . . 467
15.2.1 Example: Mahalanobis Distance . . . . . . . . . . . . . . . . 467
15.2.2 Example: Class-Conditional Histograms and Naive Bayes . . 468
15.2.3 Example: Classiﬁcation Using Nearest Neighbors . . . . . . . 469
15.2.4 Example: The Linear Support Vector Machine . . . . . . . . 470
. . . . . . . . . . . . . . . . . . . 473
15.2.5 Example: Kernel Machines
15.2.6 Example: Boosting and Adaboost
. . . . . . . . . . . . . . . 475
15.3 Practical Methods for Building Classiﬁers . . . . . . . . . . . . . . . 475
15.3.1 Manipulating Training Data to Improve Performance . . . . . 477
15.3.2 Building Multi-Class Classiﬁers Out of Binary Classiﬁers
. . 479
15.3.3 Solving for SVMS and Kernel Machines . . . . . . . . . . . . 480
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

15.4 Notes

16 Classifying Images

16.1 Building Good Image Features

482
. . . . . . . . . . . . . . . . . . . . . 482
16.1.1 Example Applications . . . . . . . . . . . . . . . . . . . . . . 482
16.1.2 Encoding Layout with GIST Features
. . . . . . . . . . . . . 485
16.1.3 Summarizing Images with Visual Words . . . . . . . . . . . . 487
16.1.4 The Spatial Pyramid Kernel . . . . . . . . . . . . . . . . . . . 489
16.1.5 Dimension Reduction with Principal Components . . . . . . . 493
. . . . . . . . 494
16.1.6 Dimension Reduction with Canonical Variates
16.1.7 Example Application: Identifying Explicit Images
. . . . . . 498
16.1.8 Example Application: Classifying Materials . . . . . . . . . . 502
16.1.9 Example Application: Classifying Scenes . . . . . . . . . . . . 502
16.2 Classifying Images of Single Objects . . . . . . . . . . . . . . . . . . 504
16.2.1 Image Classiﬁcation Strategies
. . . . . . . . . . . . . . . . . 505
16.2.2 Evaluating Image Classiﬁcation Systems . . . . . . . . . . . . 505
16.2.3 Fixed Sets of Classes . . . . . . . . . . . . . . . . . . . . . . . 508
16.2.4 Large Numbers of Classes . . . . . . . . . . . . . . . . . . . . 509

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedxiii

16.2.5 Flowers, Leaves, and Birds: Some Specialized Problems

. . . 511
16.3 Image Classiﬁcation in Practice . . . . . . . . . . . . . . . . . . . . . 512
16.3.1 Codes for Image Features . . . . . . . . . . . . . . . . . . . . 513
16.3.2 Image Classiﬁcation Datasets . . . . . . . . . . . . . . . . . . 513
16.3.3 Dataset Bias
. . . . . . . . . . . . . . . . . . . . . . . . . . . 515
16.3.4 Crowdsourcing Dataset Collection . . . . . . . . . . . . . . . 515
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

16.4 Notes

17 Detecting Objects in Images

519
17.1 The Sliding Window Method . . . . . . . . . . . . . . . . . . . . . . 519
17.1.1 Face Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 520
17.1.2 Detecting Humans . . . . . . . . . . . . . . . . . . . . . . . . 525
. . . . . . . . . . . . . . . . . . . . . . 527
17.1.3 Detecting Boundaries
17.2 Detecting Deformable Objects . . . . . . . . . . . . . . . . . . . . . . 530
17.3 The State of the Art of Object Detection . . . . . . . . . . . . . . . 535
17.3.1 Datasets and Resources . . . . . . . . . . . . . . . . . . . . . 538
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539

17.4 Notes

18 Topics in Object Recognition

18.2 Feature Questions

540
18.1 What Should Object Recognition Do? . . . . . . . . . . . . . . . . . 540
18.1.1 What Should an Object Recognition System Do? . . . . . . . 540
18.1.2 Current Strategies for Object Recognition . . . . . . . . . . . 542
18.1.3 What Is Categorization? . . . . . . . . . . . . . . . . . . . . . 542
18.1.4 Selection: What Should Be Described? . . . . . . . . . . . . . 544
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
18.2.1 Improving Current Image Features . . . . . . . . . . . . . . . 544
18.2.2 Other Kinds of Image Feature . . . . . . . . . . . . . . . . . . 546
18.3 Geometric Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
18.4 Semantic Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
18.4.1 Attributes and the Unfamiliar . . . . . . . . . . . . . . . . . . 550
18.4.2 Parts, Poselets and Consistency . . . . . . . . . . . . . . . . . 551
18.4.3 Chunks of Meaning . . . . . . . . . . . . . . . . . . . . . . . . 554

VI APPLICATIONS AND TOPICS

557

19 Image-Based Modeling and Rendering

559
19.1 Visual Hulls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
19.1.1 Main Elements of the Visual Hull Model . . . . . . . . . . . . 561
19.1.2 Tracing Intersection Curves . . . . . . . . . . . . . . . . . . . 563
19.1.3 Clipping Intersection Curves
. . . . . . . . . . . . . . . . . . 566

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedxiv

19.1.4 Triangulating Cone Strips . . . . . . . . . . . . . . . . . . . . 567
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
19.1.5 Results
19.1.6 Going Further: Carved Visual Hulls
. . . . . . . . . . . . . . 572
19.2 Patch-Based Multi-View Stereopsis . . . . . . . . . . . . . . . . . . . 573
19.2.1 Main Elements of the PMVS Model
. . . . . . . . . . . . . . 575
19.2.2 Initial Feature Matching . . . . . . . . . . . . . . . . . . . . . 578
19.2.3 Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
19.2.4 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
19.2.5 Results
19.3 The Light Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
19.4 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587

20 Looking at People

20.2 Parsing People in Images

590
20.1 HMM’s, Dynamic Programming, and Tree-Structured Models . . . . 590
20.1.1 Hidden Markov Models
. . . . . . . . . . . . . . . . . . . . . 590
20.1.2 Inference for an HMM . . . . . . . . . . . . . . . . . . . . . . 592
20.1.3 Fitting an HMM with EM . . . . . . . . . . . . . . . . . . . . 597
20.1.4 Tree-Structured Energy Models . . . . . . . . . . . . . . . . . 600
. . . . . . . . . . . . . . . . . . . . . . . . 602
20.2.1 Parsing with Pictorial Structure Models . . . . . . . . . . . . 602
20.2.2 Estimating the Appearance of Clothing . . . . . . . . . . . . 604
20.3 Tracking People . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
20.3.1 Why Human Tracking Is Hard . . . . . . . . . . . . . . . . . 606
20.3.2 Kinematic Tracking by Appearance . . . . . . . . . . . . . . . 608
20.3.3 Kinematic Human Tracking Using Templates . . . . . . . . . 609
20.4 3D from 2D: Lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
20.4.1 Reconstruction in an Orthographic View . . . . . . . . . . . . 611
20.4.2 Exploiting Appearance for Unambiguous Reconstructions . . 613
20.4.3 Exploiting Motion for Unambiguous Reconstructions . . . . . 615
20.5 Activity Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
20.5.1 Background: Human Motion Data . . . . . . . . . . . . . . . 617
20.5.2 Body Conﬁguration and Activity Recognition . . . . . . . . . 621
20.5.3 Recognizing Human Activities with Appearance Features
. . 622
20.5.4 Recognizing Human Activities with Compositional Models . . 624
20.6 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
20.7 Notes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626

21 Image Search and Retrieval

627
21.1 The Application Context . . . . . . . . . . . . . . . . . . . . . . . . . 627
. . . . . . . . . . . . . . . . . . . . . . . . . . . 628
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 629

21.1.1 Applications
21.1.2 User Needs

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedxv

21.3 Images as Documents

21.1.3 Types of Image Query . . . . . . . . . . . . . . . . . . . . . . 630
. . . . . . . . . . . . 631
21.1.4 What Users Do with Image Collections
21.2 Basic Technologies from Information Retrieval . . . . . . . . . . . . . 632
21.2.1 Word Counts . . . . . . . . . . . . . . . . . . . . . . . . . . . 632
21.2.2 Smoothing Word Counts . . . . . . . . . . . . . . . . . . . . . 633
21.2.3 Approximate Nearest Neighbors and Hashing . . . . . . . . . 634
21.2.4 Ranking Documents . . . . . . . . . . . . . . . . . . . . . . . 638
. . . . . . . . . . . . . . . . . . . . . . . . . . 639
21.3.1 Matching Without Quantization . . . . . . . . . . . . . . . . 640
. . . . . . . . . . . . . . . . . 641
21.3.2 Ranking Image Search Results
21.3.3 Browsing and Layout
. . . . . . . . . . . . . . . . . . . . . . 643
21.3.4 Laying Out Images for Browsing . . . . . . . . . . . . . . . . 644
. . . . . . . . . . . . . . . . . . 645
21.4.1 Annotations from Nearby Words . . . . . . . . . . . . . . . . 646
21.4.2 Annotations from the Whole Image
. . . . . . . . . . . . . . 646
21.4.3 Predicting Correlated Words with Classiﬁers
. . . . . . . . . 648
21.4.4 Names and Faces
. . . . . . . . . . . . . . . . . . . . . . . . 649
21.4.5 Generating Tags with Segments . . . . . . . . . . . . . . . . . 651
21.5 The State of the Art of Word Prediction . . . . . . . . . . . . . . . . 654
21.5.1 Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.2 Comparing Methods . . . . . . . . . . . . . . . . . . . . . . . 655
21.5.3 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 656
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659

21.4 Predicting Annotations for Pictures

21.6 Notes

VII BACKGROUND MATERIAL

661

22 Optimization Techniques

663
22.1 Linear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . . . 663
22.1.1 Normal Equations and the Pseudoinverse . . . . . . . . . . . 664
22.1.2 Homogeneous Systems and Eigenvalue Problems
. . . . . . . 665
22.1.3 Generalized Eigenvalues Problems
. . . . . . . . . . . . . . . 666
22.1.4 An Example: Fitting a Line to Points in a Plane . . . . . . . 666
22.1.5 Singular Value Decomposition . . . . . . . . . . . . . . . . . . 667
22.2 Nonlinear Least-Squares Methods . . . . . . . . . . . . . . . . . . . . 669
22.2.1 Newton’s Method: Square Systems of Nonlinear Equations.
. 670
22.2.2 Newton’s Method for Overconstrained Systems . . . . . . . . 670
22.2.3 The Gauss–Newton and Levenberg–Marquardt Algorithms
. 671
22.3 Sparse Coding and Dictionary Learning . . . . . . . . . . . . . . . . 672
22.3.1 Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . 672
22.3.2 Dictionary Learning . . . . . . . . . . . . . . . . . . . . . . . 673

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reservedxvi

22.3.3 Supervised Dictionary Learning . . . . . . . . . . . . . . . . . 675
22.4 Min-Cut/Max-Flow Problems and Combinatorial Optimization . . . 675
22.4.1 Min-Cut Problems . . . . . . . . . . . . . . . . . . . . . . . . 676
22.4.2 Quadratic Pseudo-Boolean Functions . . . . . . . . . . . . . . 677
22.4.3 Generalization to Integer Variables . . . . . . . . . . . . . . . 679
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 682

22.5 Notes

Bibliography

Index

List of Algorithms

684

737

760

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

Computer vision as a ﬁeld is an intellectual frontier. Like any frontier, it is
exciting and disorganized, and there is often no reliable authority to appeal to.
Many useful ideas have no theoretical grounding, and some theories are useless
in practice; developed areas are widely scattered, and often one looks completely
inaccessible from the other. Nevertheless, we have attempted in this book to present
a fairly orderly picture of the ﬁeld.

We see computer vision—or just “vision”; apologies to those who study human
or animal vision—as an enterprise that uses statistical methods to disentangle data
using models constructed with the aid of geometry, physics, and learning theory.
Thus, in our view, vision relies on a solid understanding of cameras and of the
physical process of image formation (Part I of this book) to obtain simple inferences
from individual pixel values (Part II), combine the information available in multiple
images into a coherent whole (Part III), impose some order on groups of pixels to
separate them from each other or infer shape information (Part IV), and recognize
objects using geometric information or probabilistic techniques (Part V). Computer
vision has a wide variety of applications, both old (e.g., mobile robot navigation,
industrial inspection, and military intelligence) and new (e.g., human computer
interaction, image retrieval in digital libraries, medical image analysis, and the
realistic rendering of synthetic scenes in computer graphics). We discuss some of
these applications in part VII.

IN THE SECOND EDITION

We have made a variety of changes since the ﬁrst edition, which we hope have
improved the usefulness of this book. Perhaps the most important change follows
a big change in the discipline since the last edition. Code and data are now widely
published over the Internet. It is now quite usual to build systems out of other
people’s published code, at least in the ﬁrst instance, and to evaluate them on
other people’s datasets. In the chapters, we have provided guides to experimental
resources available online. As is the nature of the Internet, not all of these URL’s
will work all the time; we have tried to give enough information so that searching
Google with the authors’ names or the name of the dataset or codes will get the
right result.

Other changes include:

• We have simpliﬁed. We give a simpler, clearer treatment of mathematical
topics. We have particularly simpliﬁed our treatment of cameras (Chapter
1), shading (Chapter 2), and reconstruction from two views (Chapter 7) and
from multiple views (Chapter 8)

• We describe a broad range of applications, including image-based mod-
elling and rendering (Chapter 19), image search (Chapter 22), building image
mosaics (Section 12.1), medical image registration (Section 12.3), interpreting
range data (Chapter 14), and understanding human activity (Chapter 21).

xvii

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xviii

• We have written a comprehensive treatment of the modern features, par-
ticularly HOG and SIFT (both in Chapter 5), that drive applications ranging
from building image mosaics to object recognition.

• We give a detailed treatment of modern image editing techniques, in-
cluding removing shadows (Section 3.5), ﬁlling holes in images (Section 6.3),
noise removal (Section 6.4), and interactive image segmentation (Section 9.2).
• We give a comprehensive treatment of modern object recognition tech-
niques. We start with a practical discussion of classiﬁers (Chapter 15); we
then describe standard methods for image classiﬁcation techniques (Chapter
16), and object detection (Chapter 17). Finally, Chapter 18 reviews a wide
range of recent topics in object recognition.

• Finally, this book has a very detailed index, and a bibliography that is as

comprehensive and up-to-date as we could make it.

WHY STUDY VISION?

Computer vision’s great trick is extracting descriptions of the world from pictures
or sequences of pictures. This is unequivocally useful. Taking pictures is usually
nondestructive and sometimes discreet. It is also easy and (now) cheap. The de-
scriptions that users seek can diﬀer widely between applications. For example, a
technique known as structure from motion makes it possible to extract a representa-
tion of what is depicted and how the camera moved from a series of pictures. People
in the entertainment industry use these techniques to build three-dimensional (3D)
computer models of buildings, typically keeping the structure and throwing away
the motion. These models are used where real buildings cannot be; they are set ﬁre
to, blown up, etc. Good, simple, accurate, and convincing models can be built from
quite small sets of photographs. People who wish to control mobile robots usually
keep the motion and throw away the structure. This is because they generally know
something about the area where the robot is working, but usually don’t know the
precise robot location in that area. They can determine it from information about
how a camera bolted to the robot is moving.

There are a number of other, important applications of computer vision. One
is in medical imaging: one builds software systems that can enhance imagery, or
identify important phenomena or events, or visualize information obtained by imag-
ing. Another is in inspection: one takes pictures of objects to determine whether
they are within speciﬁcation. A third is in interpreting satellite images, both for
military purposes (a program might be required to determine what militarily inter-
esting phenomena have occurred in a given region recently; or what damage was
caused by a bombing) and for civilian purposes (what will this year’s maize crop
be? How much rainforest is left?) A fourth is in organizing and structuring collec-
tions of pictures. We know how to search and browse text libraries (though this is
a subject that still has diﬃcult open questions) but don’t really know what to do
with image or video libraries.

Computer vision is at an extraordinary point in its development. The subject
itself has been around since the 1960s, but only recently has it been possible to
build useful computer systems using ideas from computer vision. This ﬂourishing

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xix

has been driven by several trends: Computers and imaging systems have become
very cheap. Not all that long ago, it took tens of thousands of dollars to get good
digital color images; now it takes a few hundred at most. Not all that long ago, a
color printer was something one found in few, if any, research labs; now they are
in many homes. This means it is easier to do research. It also means that there
are many people with problems to which the methods of computer vision apply.
For example, people would like to organize their collections of photographs, make
3D models of the world around them, and manage and edit collections of videos.
Our understanding of the basic geometry and physics underlying vision and, more
important, what to do about it, has improved signiﬁcantly. We are beginning to be
able to solve problems that lots of people care about, but none of the hard problems
have been solved, and there are plenty of easy ones that have not been solved either
(to keep one intellectually ﬁt while trying to solve hard problems). It is a great
time to be studying this subject.

What Is in this Book

This book covers what we feel a computer vision professional ought to know. How-
ever, it is addressed to a wider audience. We hope that those engaged in compu-
tational geometry, computer graphics, image processing, imaging in general, and
robotics will ﬁnd it an informative reference. We have tried to make the book
accessible to senior undergraduates or graduate students with a passing interest
in vision. Each chapter covers a diﬀerent part of the subject, and, as a glance at
Table 1 will conﬁrm, chapters are relatively independent. This means that one can
dip into the book as well as read it from cover to cover. Generally, we have tried to
make chapters run from easy material at the start to more arcane matters at the
end. Each chapter has brief notes at the end, containing historical material and
assorted opinions. We have tried to produce a book that describes ideas that are
useful, or likely to be so in the future. We have put emphasis on understanding the
basic geometry and physics of imaging, but have tried to link this with actual ap-
plications. In general, this book reﬂects the enormous recent inﬂuence of geometry
and various forms of applied statistics on computer vision.

Reading this Book

A reader who goes from cover to cover will hopefully be well informed, if exhausted;
there is too much in this book to cover in a one-semester class. Of course, prospec-
tive (or active) computer vision professionals should read every word, do all the
exercises, and report any bugs found for the third edition (of which it is probably a
good idea to plan on buying a copy!). Although the study of computer vision does
not require deep mathematics, it does require facility with a lot of diﬀerent math-
ematical ideas. We have tried to make the book self-contained, in the sense that
readers with the level of mathematical sophistication of an engineering senior should
be comfortable with the material of the book and should not need to refer to other
texts. We have also tried to keep the mathematics to the necessary minimum—after
all, this book is about computer vision, not applied mathematics—and have chosen
to insert what mathematics we have kept in the main chapter bodies instead of a
separate appendix.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xx

TABLE 1: Dependencies between chapters: It will be diﬃcult to read a chapter if you
don’t have a good grasp of the material in the chapters it “requires.” If you have not read
the chapters labeled “helpful,” you might need to look up one or two things.
Part
I

Requires

Helpful

2

4

5, 4

1

1, 7

1
1

15, 5

16, 15, 5

17, 16, 15, 5

1, 2, 7, 8

2
22
22
2, 3, 4, 5, 6, 22
9
2, 5, 22
14

12
22

17, 16, 15, 11, 5
17, 16, 15, 11, 5

Chapter

Geometric Camera Models
Light and Shading
Color
Linear Filters
Local Image Features
Texture
Stereopsis
Structure from Motion
Segmentation by Clustering
Grouping and Model Fitting
Tracking
Registration
Smooth Surfaces and Their Outlines
Range Data
Learning to Classify
Classifying Images
Detecting Objects in Images
Topics in Object Recognition
Image-Based Modeling and Rendering
Looking at People
Image Search and Retrieval
Optimization Techniques

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

II

III

IV

V

VI

VII

Generally, we have tried to reduce the interdependence between chapters, so
that readers interested in particular topics can avoid wading through the whole
book. It is not possible to make each chapter entirely self-contained, however, and
Table 1 indicates the dependencies between chapters.

We have tried to make the index comprehensive, so that if you encounter a new
term, you are likely to ﬁnd it in the book by looking it up in the index. Computer
vision is now fortunate in having a rich range of intellectual resources. Software
and datasets are widely shared, and we have given pointers to useful datasets and
software in relevant chapters; you can also look in the index, under “software” and
under “datasets,” or under the general topic.

We have tried to make the bibliography comprehensive, without being over-
whelming. However, we have not been able to give complete bibliographic references
for any topic, because the literature is so large.

What Is Not in this Book

The computer vision literature is vast, and it was not easy to produce a book about
computer vision that could be lifted by ordinary mortals. To do so, we had to cut
material, ignore topics, and so on.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxi

We left out some topics because of personal taste, or because we became
exhausted and stopped writing about a particular area, or because we learned
about them too late to put them in, or because we had to shorten some chapter, or
because we didn’t understand them, or any of hundreds of other reasons. We have
tended to omit detailed discussions of material that is mainly of historical interest,
and oﬀer instead some historical remarks at the end of each chapter.

We have tried to be both generous and careful in attributing ideas, but neither
of us claims to be a ﬂuent intellectual archaeologist, and computer vision is a very
big topic indeed. This means that some ideas may have deeper histories than we
have indicated, and that we may have omitted citations.

There are several recent textbooks on computer vision. Szeliski (2010) deals
with the whole of vision. Parker (2010) deals speciﬁcally with algorithms. Davies
(2005) and Steger et al. (2008) deal with practical applications, particularly regis-
tration. Bradski and Kaehler (2008) is an introduction to OpenCV, an important
open-source package of computer vision routines.

There are numerous more specialized references. Hartley and Zisserman
(2000a) is a comprehensive account of what is known about multiple view ge-
ometry and estimation of multiple view parameters. Ma et al. (2003b) deals with
3D reconstruction methods. Cyganek and Siebert (2009) covers 3D reconstruction
and matching. Paragios et al. (2010) deals with mathematical models in computer
vision. Blake et al. (2011) is a recent summary of what is known about Markov
random ﬁeld models in computer vision. Li and Jain (2005) is a comprehensive
account of face recognition. Moeslund et al. (2011), which is in press at time of
writing, promises to be a comprehensive account of computer vision methods for
watching people. Dickinson et al. (2009) is a collection of recent summaries of the
state of the art in object recognition. Radke (2012) is a forthcoming account of
computer vision methods applied to special eﬀects.

Much of computer vision literature appears in the proceedings of various con-
ferences. The three main conferences are: the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR); the IEEE International Conference on
Computer Vision (ICCV); and the European Conference on Computer Vision. A
signiﬁcant fraction of the literature appears in regional conferences, particularly
the Asian Conference on Computer Vision (ACCV) and the British Machine Vi-
sion Conference (BMVC). A high percentage of published papers are available on
the web, and can be found with search engines; while some papers are conﬁned to
pay-libraries, to which many universities provide access, most can be found without
cost.

ACKNOWLEDGMENTS

In preparing this book, we have accumulated a signiﬁcant set of debts. A number
of anonymous reviewers read several drafts of the book for both ﬁrst and second
edition and made extremely helpful contributions. We are grateful to them for their
time and eﬀorts.

Our editor for the ﬁrst edition, Alan Apt, organized these reviews with the

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxii

help of Jake Warde. We thank them both. Leslie Galen, Joe Albrecht, and Dianne
Parish, of Integre Technical Publishing, helped us overcome numerous issues with
proofreading and illustrations in the ﬁrst edition.

Our editor for the second edition, Tracy Dunkelberger, organized reviews
with the help of Carole Snyder. We thank them both. We thank Marilyn Lloyd for
helping us get over various production problems.

Both the overall coverage of topics and several chapters were reviewed by
various colleagues, who made valuable and detailed suggestions for their revision.
We thank Narendra Ahuja, Francis Bach, Kobus Barnard, Margaret Fleck, Martial
Hebert, Julia Hockenmaier, Derek Hoiem, David Kriegman, Jitendra Malik, and
Andrew Zisserman.

A number of people contributed suggestions, ideas for ﬁgures, proofreading
comments, and other valuable material, while they were our students. We thank
Okan Arikan, Louise Benoˆıt, Tamara Berg, S´ebastien Blind, Y-Lan Boureau, Liang-
Liang Cao, Martha Cepeda, Stephen Chenney, Frank Cho, Florent Couzinie-Devy,
Olivier Duchenne, Pinar Duygulu, Ian Endres, Ali Farhadi, Yasutaka Furukawa,
Yakup Genc, John Haddon, Varsha Hedau, Nazli Ikizler-Cinbis, Leslie Ikemoto,
Sergey Ioﬀe, Armand Joulin, Kevin Karsch, Svetlana Lazebnik, Cathy Lee, Binbin
Liao, Nicolas Loeﬀ, Julien Mairal, Sung-il Pae, David Parks, Deva Ramanan, Fred
Rothganger, Amin Sadeghi, Alex Sorokin, Attawith Sudsang, Du Tran, Duan Tran,
Gang Wang, Yang Wang, Ryan White, and the students in several oﬀerings of our
vision classes at UIUC, U.C. Berkeley and ENS.

We have been very lucky to have colleagues at various universities use (of-
ten rough) drafts of our book in their vision classes. Institutions whose students
suﬀered through these drafts include, in addition to ours, Carnegie-Mellon Univer-
sity, Stanford University, the University of Wisconsin at Madison, the University of
California at Santa Barbara and the University of Southern California; there may
be others we are not aware of. We are grateful for all the helpful comments from
adopters, in particular Chris Bregler, Chuck Dyer, Martial Hebert, David Krieg-
man, B.S. Manjunath, and Ram Nevatia, who sent us many detailed and helpful
comments and corrections.

The book has also beneﬁtted from comments and corrections from Karteek
Alahari, Aydin Alaylioglu, Srinivas Akella, Francis Bach, Marie Banich, Serge Be-
longie, Tamara Berg, Ajit M. Chaudhari, Navneet Dalal, Jennifer Evans, Yasutaka
Furukawa, Richard Hartley, Glenn Healey, Mike Heath, Martial Hebert, Janne
Heikkil¨a, Hayley Iben, St´ephanie Jonqui`eres, Ivan Laptev, Christine Laubenberger,
Svetlana Lazebnik, Yann LeCun, Tony Lewis, Benson Limketkai, Julien Mairal, Si-
mon Maskell, Brian Milch, Roger Mohr, Deva Ramanan, Guillermo Sapiro, Cordelia
Schmid, Brigitte Serlin, Gerry Serlin, Ilan Shimshoni, Jamie Shotton, Josef Sivic,
Eric de Sturler, Camillo J. Taylor, Jeﬀ Thompson, Claire Vallat, Daniel S. Wilker-
son, Jinghan Yu, Hao Zhang, Zhengyou Zhang, and Andrew Zisserman.

In the ﬁrst edition, we said

If you ﬁnd an apparent typographic error, please email DAF... with
the details, using the phrase “book typo” in your email; we will try to
credit the ﬁrst ﬁnder of each typo in the second edition.

which turns out to have been a mistake. DAF’s ability to manage and preserve

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxiii

email logs was just not up to this challenge. We thank all ﬁnders of typographic
errors; we have tried to ﬁx the errors and have made eﬀorts to credit all the people
who have helped us.

We also thank P. Besl, B. Boufama, J. Costeira, P. Debevec, O. Faugeras, Y.
Genc, M. Hebert, D. Huber, K. Ikeuchi, A.E. Johnson, T. Kanade, K. Kutulakos,
M. Levoy, Y. LeCun, S. Mahamud, R. Mohr, H. Moravec, H. Murase, Y. Ohta, M.
Okutami, M. Pollefeys, H. Saito, C. Schmid, J. Shotton, S. Sullivan, C. Tomasi,
and M. Turk for providing the originals of some of the ﬁgures shown in this book.
DAF acknowledges ongoing research support from the National Science Foun-
dation. Awards that have directly contributed to the writing of this book are
IIS-0803603, IIS-1029035, and IIS-0916014; other awards have shaped the view de-
scribed here. DAF acknowledges ongoing research support from the Oﬃce of Naval
Research, under awards N00014-01-1-0890 and N00014-10-1-0934, which are part
of the MURI program. Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect
those of NSF or ONR.

DAF acknowledges a wide range of intellectual debts, starting at kindergarten.
Important ﬁgures in the very long list of his creditors include Gerald Alanthwaite,
Mike Brady, Tom Fair, Margaret Fleck, Jitendra Malik, Joe Mundy, Mike Rodd,
Charlie Rothwell, and Andrew Zisserman. JP cannot even remember kindergarten,
but acknowledges his debts to Olivier Faugeras, Mike Brady, and Tom Binford. He
also wishes to thank Sharon Collins for her help. Without her, this book, like most
of his work, probably would have never been ﬁnished. Both authors would also like
to acknowledge the profound inﬂuence of Jan Koenderink’s writings on their work
at large and on this book in particular.

Figures: Some images used herein were obtained from IMSI’s Master Photos
Collection, 1895 Francisco Blvd. East, San Rafael, CA 94901-5506, USA. We have
made extensive use of ﬁgures from the published literature; these ﬁgures are credited
in their captions. We thank the copyright holders for extending permission to use
these ﬁgures.

Bibliography: In preparing the bibliography, we have made extensive use
of Keith Price’s excellent computer vision bibliography, which can be found at
http://iris.usc.edu/Vision-Notes/bibliography/contents.html.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedTABLE 2: A one-semester introductory class in computer vision for seniors or ﬁrst-year
graduate students in computer science, electrical engineering, or other engineering or
science disciplines.
Week Chapter

Sections

Preface

xxiv

1

2

3
4
5

6
7
8

9
10

11
12
13
14
15

1, 2

1.1, 2.1, 2.2.x

Key topics
pinhole cameras, pixel shading models,
one inference from shading example

3

4
5
6

7
8
9

10
11

12
15
16
17

choice

3.1–3.5

human color perception, color physics, color spaces,

all
all

6.1, 6.2

7.1, 7.2

8.1

9.1–9.3

image color model

linear ﬁlters
building local features
texture representations from ﬁlters,

from vector quantization

binocular geometry, stereopsis
structure from motion with perspective cameras
segmentation ideas, applications,

segmentation by clustering pixels

10.1–10.4
11.1-11.3

Hough transform, ﬁtting lines, robustness, RANSAC,
simple tracking strategies, tracking by matching,

all
all
all
all
all

Kalman ﬁlters, data association

registration
classiﬁcation
classifying images
detection
one of chapters 14, 19, 20, 21 (application topics)

SAMPLE SYLLABUSES
The whole book can be covered in two (rather intense) semesters, by starting at
the ﬁrst page and plunging on. Ideally, one would cover one application chapter—
probably the chapter on image-based rendering—in the ﬁrst semester, and the other
one in the second. Few departments will experience heavy demand for such a de-
tailed sequence of courses. We have tried to structure this book so that instructors
can choose areas according to taste. Sample syllabuses for busy 15-week semesters
appear in Tables 2 to 6, structured according to needs that can reasonably be ex-
pected. We would encourage (and expect!) instructors to rearrange these according
to taste.

Table 2 contains a suggested syllabus for a one-semester introductory class
in computer vision for seniors or ﬁrst-year graduate students in computer science,
electrical engineering, or other engineering or science disciplines. The students
receive a broad presentation of the ﬁeld, including application areas such as digital
libraries and image-based rendering. Although the hardest theoretical material is
omitted, there is a thorough treatment of the basic geometry and physics of image
formation. We assume that students will have a wide range of backgrounds, and
can be assigned background readings in probability. We have put oﬀ the application
chapters to the end, but many may prefer to cover them earlier.

Table 3 contains a syllabus for students of computer graphics who want to
know the elements of vision that are relevant to their topic. We have emphasized
methods that make it possible to recover object models from image information;

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxv

TABLE 3: A syllabus for students of computer graphics who want to know the elements
of vision that are relevant to their topic.
Week Chapter

Sections

1, 2

1.1, 2.1, 2.2.4

Key topics
pinhole cameras, pixel shading models,

1

2

3
4
5
6
7
8
9
10

11

12
13
14
15

3

4
5
6
7
7
8
10
9

11

12
14
19
13

photometric stereo

3.1–3.5

human color perception, color physics, color spaces,

all
all

6.3, 6.4
7.1, 7.2
7.4, 7.5

8.1

10.1–10.4

9.1–9.3

image color model

linear ﬁlters
building local features
texture synthesis, image denoising
binocular geometry, stereopsis
advanced stereo methods
structure from motion with perspective cameras
Hough transform, ﬁtting lines, robustness, RANSAC,
segmentation ideas, applications,

segmentation by clustering pixels

11.1-11.3

simple tracking strategies, tracking by matching,

all
all
all
all

Kalman ﬁlters, data association

registration
range data
image-based modeling and rendering
surfaces and outlines

understanding these topics needs a working knowledge of cameras and ﬁlters. Track-
ing is becoming useful in the graphics world, where it is particularly important for
motion capture. We assume that students will have a wide range of backgrounds,
and have some exposure to probability.

Table 4 shows a syllabus for students who are primarily interested in the
applications of computer vision. We cover material of most immediate practical
interest. We assume that students will have a wide range of backgrounds, and can
be assigned background reading.

Table 5 is a suggested syllabus for students of cognitive science or artiﬁcial
intelligence who want a basic outline of the important notions of computer vision.
This syllabus is less aggressively paced, and assumes less mathematical experience.
Our experience of teaching computer vision is that no single idea presents any
particular conceptual diﬃculties, though some are harder than others. Diﬃculties
are caused by the tremendous number of new ideas required by the subject. Each
subproblem seems to require its own way of thinking, and new tools to cope with it.
This makes learning the subject rather daunting. Table 6 shows a sample syllabus
for students who are really not bothered by these diﬃculties. They would need
to have quite a strong interest in applied mathematics, electrical engineering or
physics, and be very good at picking things up as they go along. This syllabus sets
a furious pace, and assumes that students can cope with a lot of new material.

NOTATION

We use the following notation throughout the book: Points, lines, and planes are
denoted by Roman or Greek letters in italic font (e.g., P , Δ, or Π). Vectors are

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedTABLE 4: A syllabus for students who are primarily interested in the applications of
computer vision.
Week Chapter

Sections

Key topics
pinhole cameras, pixel shading models,

1, 2

1.1, 2.1, 2.2.4

Preface

xxvi

1

2

3
4
5
6
7
8

9
10
11
12
13
14
15

3

4
5
6
7
7

8, 9

8.1, 9.1–9.2

photometric stereo

3.1–3.5

human color perception, color physics, color spaces,

all
all

6.3, 6.4
7.1, 7.2
7.4, 7.5

image color model

linear ﬁlters
building local features
texture synthesis, image denoising
binocular geometry, stereopsis
advanced stereo methods
structure from motion with perspective cameras,

segmentation ideas, applications

10
12
14
16
19
20
21

10.1–10.4

all
all
all
all
all
all

Hough transform, ﬁtting lines, robustness, RANSAC,
registration
range data
classifying images
image based modeling and rendering
looking at people
image search and retrieval

−−→
usually denoted by Roman or Greek bold-italic letters (e.g., v, P , or ξ), but the
vector joining two points P and Q is often denoted by
P Q. Lower-case letters are
normally used to denote geometric ﬁgures in the image plane (e.g., p, p, δ), and
upper-case letters are used for scene objects (e.g., P , Π). Matrices are denoted by
Roman letters in calligraphic font (e.g., U).

The familiar three-dimensional Euclidean space is denoted by E

3, and the
vector space formed by n-tuples of real numbers with the usual laws of addition
n, with 0 being used to denote the
and multiplication by a scalar is denoted by R
zero vector. Likewise, the vector space formed by m × n matrices with real entries
m×n. When m = n, Id is used to denote the identity matrix—
is denoted by R
that is, the n × n matrix whose diagonal entries are equal to 1 and nondiagonal
entries are equal to 0. The transpose of the m × n matrix U with coeﬃcients uij
is the n × m matrix denoted by U T with coeﬃcients uji. Elements of R
n are often
identiﬁed with column vectors or n × 1 matrices, for example, a = (a1, a2, a3)T is
the transpose of a 1 × 3 matrix (or row vector), i.e., an 3 × 1 matrix (or column
vector), or equivalently an element of R

3.

The dot product (or inner product) of two vectors a = (a1, . . . , an)T and

b = (b1, . . . , bn)T in R

n is deﬁned by

a · b = a1b1 + ··· + anbn,

and it can also be written as a matrix product, i.e., a · b = aT b = bT a. We denote
by |a|2 = a · a the square of the Euclidean norm of the vector a and denote by d
n, i.e., d(P, Q) = |−−→
P Q|.
the distance function induced by the Euclidean norm in E
Given a matrix U in R
m×n, we generally use |U| to denote its Frobenius norm, i.e.,
the square root of the sum of its squared entries.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxvii

TABLE 5: For students of cognitive science or artiﬁcial intelligence who want a basic
outline of the important notions of computer vision.
Week Chapter

Sections

1, 2

1.1, 2.1, 2.2.x

Key topics
pinhole cameras, pixel shading models,
one inference from shading example

3

4
5
6

7
9

11

15
16
20
21
17
18

3.1–3.5

human color perception, color physics, color spaces,

all
all

6.1, 6.2

7.1, 7.2
9.1–9.3

image color model

linear ﬁlters
building local features
texture representations from ﬁlters,

from vector quantization

binocular geometry, stereopsis
segmentation ideas, applications,

segmentation by clustering pixels

11.1, 11.2

simple tracking strategies, tracking using matching,

all
all
all
all
all
all

optical ﬂow

classiﬁcation
classifying images
looking at people
image search and retrieval
detection
topics in object recognition

1

2

3
4
5

6
8

9

10
11
12
13
14
15

When the vector a has unit norm, the dot product a·b is equal to the (signed)

length of the projection of b onto a. More generally,
a · b = |a||b| cos θ,

where θ is the angle between the two vectors, which shows that a necessary and
suﬃcient condition for two vectors to be orthogonal is that their dot product be
zero.

The cross product (or outer product) of two vectors a = (a1, a2, a3)T and

b = (b1, b2, b3)T in R

3 is the vector


a2b3 − a3b2
a3b1 − a1b3
a1b2 − a2b1

a × b

def=


 0
a3
−a2

−a3
0
a1

a2
−a1
0


.

.

Note that a × b = [a×]b, where

[a×] def=

The cross product of two vectors a and b in R

3 is orthogonal to these two
vectors, and a necessary and suﬃcient condition for a and b to have the same
direction is that a × b = 0. If θ denotes as before the angle between the vectors a
and b, it can be shown that

|a × b| = |a||b||sin θ|.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedTABLE 6: A syllabus for students who have a strong interest in applied mathematics,
electrical engineering, or physics.
Week
Sections

Chapter

Preface

xxviii

1
2
3
4
5
6
7
8
9
10

11
12
13
14
15

1, 2

all; 2.1–2.4

3
4
5
6
7
8
9
10
11

12
15
16
17

choice

all
all
all
all
all
all
all
all

11.1–11.3

all
all
all
all
all

Key topics
cameras, shading
color
linear ﬁlters
building local features
texture
stereopsis
structure from motion with perspective cameras
segmentation by clustering pixels
ﬁtting models
simple tracking strategies, tracking by matching,

Kalman ﬁlters, data association

registration
classiﬁcation
classifying images
detection
one of chapters 14, 19, 20, 21

PROGRAMMING ASSIGNMENTS AND RESOURCES

The programming assignments given throughout this book sometimes require rou-
tines for numerical linear algebra, singular value decomposition, and linear and
nonlinear least squares. An extensive set of such routines is available in MATLAB
as well as in public-domain libraries such as LINPACK, LAPACK, and MINPACK,
which can be downloaded from the Netlib repository (http://www.netlib.org/).
In the text, we oﬀer extensive pointers to software published on the Web and to
datasets published on the Web. OpenCV is an important open-source package of
computer vision routines (see Bradski and Kaehler (2008)).

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights ReservedPreface

xxix

ABOUT THE AUTHORS

David Forsyth received a B.Sc. (Elec. Eng.) from the University of the Witwa-
tersrand, Johannesburg in 1984, an M.Sc. (Elec. Eng.) from that university in
1986, and a D.Phil.
from Balliol College, Oxford in 1989. He spent three years
on the faculty at the University of Iowa, ten years on the faculty at the University
of California at Berkeley, and then moved to the University of Illinois. He served
as program co-chair for IEEE Computer Vision and Pattern Recognition in 2000
and in 2011, general co-chair for CVPR 2006, and program co-chair for the Euro-
pean Conference on Computer Vision 2008, and is a regular member of the program
committee of all major international conferences on computer vision. He has served
ﬁve terms on the SIGGRAPH program committee. In 2006, he received an IEEE
technical achievement award, and in 2009 he was named an IEEE Fellow.

Jean Ponce received the Doctorat de Troisieme Cycle and Doctorat d’ ´Etat
degrees in Computer Science from the University of Paris Orsay in 1983 and 1988.
He has held Research Scientist positions at the Institut National de la Recherche en
Informatique et Automatique, the MIT Artiﬁcial Intelligence Laboratory, and the
Stanford University Robotics Laboratory, and served on the faculty of the Dept. of
Computer Science at the University of Illinois at Urbana-Champaign from 1990 to
2005. Since 2005, he has been a Professor at Ecole Normale Superieure in Paris,
France. Dr. Ponce has served on the editorial boards of Computer Vision and
Image Understanding, Foundations and Trends in Computer Graphics and Vision,
the IEEE Transactions on Robotics and Automation, the International Journal of
Computer Vision (for which he served as Editor-in-Chief from 2003 to 2008), and
the SIAM Journal on Imaging Sciences. He was Program Chair of the 1997 IEEE
Conference on Computer Vision and Pattern Recognition and served as General
Chair of the year 2000 edition of this conference. He also served as General Chair
of the 2008 European Conference on Computer Vision. In 2003, he was named an
IEEE Fellow for his contributions to Computer Vision, and he received a US patent
for the development of a robotic parts feeder.

©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved©2012 Pearson Education, Inc., Upper Saddle River, NJ 07458. All Rights Reserved