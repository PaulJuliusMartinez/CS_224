Identification of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models 
Ilya Shpitser, Judea Pearl
Cognitive Systems Laboratory Department of Computer Science University of California, Los Angeles
Los Angeles, CA. 90095 {ilyas, judea}@cs.ucla.edu

Abstract This paper is concerned with estimating the effects of actions from causal assumptions, represented concisely as a directed graph, and statistical knowledge, given as a probability distribution. We provide a necessary and sufficient graphical condition for the cases when the causal effect of an arbitrary set of variables on another arbitrary set can be determined uniquely from the available information, as well as an algorithm which computes the effect whenever this condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995], and a version of an identification algorithm in [Tian, 2002] for the same identification problem. Finally, we derive a complete characterization of semiMarkovian models in which all causal effects are identifiable.
Introduction
This paper deals with computing effects of actions in domains specified as causal diagrams, or graphs with directed and bidirected edges. Vertices in such graphs correspond to variables of interest, directed edges correspond to potential direct causal relationships between variables, and bidirected edges correspond to 'hidden common causes,' or spurious dependencies between variables [Pearl, 1995], [Pearl, 2000]. Aside from causal knowledge encoded by these graphs, we also have statistical knowledge in the form of a joint probability distribution over observable variables, which we will denote by P .
An action on a variable set X in a causal domain consists of forcing X to particular values x, regardless of the values X would have taken prior to the intervention. This action, denoted do(x) in [Pearl, 2000], changes the original joint distribution P over observables into a new interventional distribution denoted Px. The marginal distribution Px(Y) of a variable set Y obtained from Px will be our notion of effect of action do(x) on Y.
Our task is to characterize cases when Px(Y) can be determined uniquely from P , or identif ied in a given graph G. It is well known that in Markovian models, those causal domains whose graphs do not contain bidirected edges, all
This work was supported in part by AFOSR grant #F4962001-1-0055, NSF grant #IIS-0535223, and MURI grant #N0001400-1-0617. Copyright c 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.

effects are identifiable [Pearl, 2000]. If our model con-

tains 'hidden common causes,' that is if the model is semi-

Markovian, the situation is less clear.

Consider the causal diagrams in Fig. 1 (a) and (b) which

might represent a situation in diagnostic medicine. For in-

stance, nodes W1, W2 are afflictions of a pregnant mother and her unborn child, respectively. X is a toxin produced

in the mother's body as a result of the illness, which could

be artificially lowered by a treatment. Y1, Y2 stand for the survival of mother and child. Bidirected arcs represent con-

founding factors for this situation not explicitly named in

the model, but affecting the outcome. We are interested in

computing the effect of lowering X on Y1, Y2 without actually performing the potentially dangerous treatment. In our

framework this corresponds P (X, W1, W2, Y1, Y2). The

to computing Px(Y1, Y2) from subtlety of this problem can be

illustrated by noting that in Fig. 1 (a), the effect is identifi-

able, while in Fig. 1 (b), it is not.

Multiple sufficient conditions for identifiability in the

semi-Markovian case are known [Spirtes, Glymour, &

Scheines, 1993], [Pearl & Robins, 1995], [Pearl, 1995],

[Kuroki & Miyakawa, 1999]. A summary of these results

can be found in [Pearl, 2000]. Most work in this area has

generally taken advantage of the fact that certain properties

of the causal diagram reflect properties of P , and is phrased

in the language of graph theory. For example, the back-door

criterion [Pearl, 2000], states that if there exists a set Z of

non-descendants of X that 'blocks' certain paths in the graph

from X to Results

Y, then Px(Y) = in [Pearl, 1995],

[HzaPlp(eYrn|z,,

x)P (z). 2000] take

a

differ-

ent approach, and provide sound rules which are used to

manipulate the expression corresponding to the effect alge-

braically. These rules are then applied until the resulting ex-

pression can be computed from P .

Though the axioms in [Halpern, 2000] were shown to be

complete, the practical applicability of the result to identi-

fiability is limited, since it does not provide a closed form

criterion for the cases when effects are not identifiable, nor

a closed form algorithm for expressing effects in terms of P

when they are identifiable. Instead, one must rely on find-

ing a good proof strategy and hope the effect expression is

reduced to something derivable from P .

Recently, a number of necessity results for identifiabil-

ity have been proven. One such result [Tian & Pearl, 2002]

1219

W1 X Y1

W1 X Y1

W2 Y2

W2 Y2

(a) (b)
Figure 1: (a) Graph G. (b) Graph G . W1, W2 - illnesses of pregnant mother and unborn child, X - toxin-lowering treatment, Y1, Y2 - survival of the patients

states that Px is identifiable if and only if there is no path consisting entirely of bidirected arcs from X to a child of X. The authors have also been made aware of a paper currently in review [Huang & Valtorta, 2006] which shows a modified version of an algorithm found in [Tian, 2002] is complete for identifying Px(y), where X, Y are sets. One of the contributions of this paper is a simpler proof of the same result, using non-positive distributions. The results in this paper were independently derived.
In this paper, we offer a complete solution to the problem of identifying Px(y) in semi-Markovian models. Using a graphical structure called a hedge, we construct a sound and complete algorithm for identifying Px(y) from P . The algorithm returns either an expression derivable from P or a hedge which witnesses the non-identifiability of the effect. We also show that steps of our algorithm correspond to sequences of applications of rules of do-calculus [Pearl, 1995], thus proving completeness of do-calculus for the same identification problem. Furthermore, we show a version of Tian's algorithm [Tian, 2002] is also complete and thus equivalent to ours. Finally, we derive a complete characterization of models in which all effects are identifiable.
Notation and Definitions
In this section we reproduce the technical definitions needed for the rest of the paper, and introduce common nonidentifying graph structures. We will denote variables by capital letters, and their values by small letters. Similarly, sets of variables will be denoted by bold capital letters, and sets of values by bold small letters. We will use some graphtheoretic abbreviations: P a(Y)G, An(Y)G, and De(Y)G will denote the set of (observable) parents, ancestors, and descendants of the node set Y in G, respectively. The lowercase versions of the above kinship sets will denote corresponding sets of values. We will omit the graph subscript if the graph in question is assumed or obvious. We will denote the set {X  G|De(X)G = } as the root set of G. For a given node Vi in a graph G and a topological ordering  of nodes in G, we denote V(i-1) to be the set of observable nodes preceding Vi in . A topological ordering  of G is a total order where no node can be greater than its descendant in G.
Having fixed our notation, we can proceed to formalize the notions discussed in the previous section. A probabilistic causal model is a tuple M = U, V, F, P (U) , where V

is a set of observable variables, U is a set of unobservable

variables distributed according to P (U), and F is a set of

functions. Each variable V  V has a corresponding func-

tion fV  F that determines the value of V in terms of other variables in V and U.

The induced graph G of a causal model M contains a

node for every element in V, a directed edge between nodes

X and Y if fY possibly uses the values of X directly to determine the value of Y , and a bidirected edge between nodes

X and Y if fX and fY both possibly use the value of some variable in U to determine their respective values. In this

paper we consider recursive causal models, those models

which induce acyclic graphs.

For the purposes of this paper, we assume all variable do-

mains are finite, V induced by P

and (U)

P (U) and F

= will

biePd(eUnio)t.eTdhPe(dVis)t.ribution

on

Sometimes it is assumed P (V) is a positive distribution.

In this paper we do not make this assumption. Thus, we must

make sure that for every distribution P (W|Z) that we consider, P (Z) must be positive. This can be achieved by mak-

ing sure to sum over events with positive probability only.

Furthermore, for any action do(x) that we consider, it must

be the bution

case that Px(V) is

Pno(txw|Peall(dXe)fiGne\dX[)Pe>ar0l,

otherwise 2000].

the

distri-

In any causal model there is a relationship between its

induced graph G and i P (vi|pa(Vi)G)

P, jP

w(uhje)r,eaPnd(vP1,a...(,.v)Gn,

u1, ..., uk) = also includes

unobservable parents [Pearl, 2000]. Whenever this relation-

ship holds, we say that G is an I-map (independence map)

of P . The I-map relationship allows us to link independence

properties of P to G by using the following well known notion of path separation [Pearl, 1988].

Definition 1 (d-separation) A path p in G is said to be dseparated by a set Z if and only if either

1 p contains a chain I  M  J or fork I  M  J, such that M  Z, or
2 p contains an inverted fork I  M  J such that De(M )G  Z = .
Two sets X, Y are said to be d-separated given Z in G if all paths from X to Y in G are d-separated by Z. The following well known theorem links d-separation of vertex sets in an I-map G with the independence of corresponding variable sets in P .

Theorem 1 If sets X and Y are d-separated by Z in G, then X is independent of Y given Z in every P for which G is an Imap. We will abbreviate this statement of d-separation, and corresponding independence by (X  Y|Z)G, following the notation in [Dawid, 1979].

A path that is not d-separated is said to be d - connected. A path starting from a node X with an arrow pointing to X is called a back - door path from X. A path consisting entirely of bidirected arcs is called a bidirected path.
In the framework of causal models, actions are modifications of functional relationships. Each action do(x) on a causal model M produces a new model Mx = U, V, Fx, P (U) , where Fx, is obtained by replacing fX 

1220

F for every X  X with a new function that outputs a constant value x given by do(x).
Since subscripts are used to denote submodels, we will use numeric superscripts to enumerate models (e.g. M 1). For a model M i, we will often denote its associated probability distributions as P i rather than P .
We can now define formally the notion of identifiability of interventions from observational distributions.

Definition 2 (Causal Effect Identifiability) The causal effect of an action do(x) on a set of variables Y such that Y  X =  is said to be identifiable from P in G if Px(Y) is (uniquely) computable from P (V) in any causal model which induces G.

The following lemma establishes the conventional technique used to prove non-identifiability in a given G.

Lemma 1 Let X, Y be two sets of variables. Assume there

exist two causal models M 1 and M 2 with the same induced

graph G such and Px1(Y) =

that P 1(V) = P 2(V), Px2(Y). Then Px(y) is

P 1(x|P a(X)G\X) > not identifiable in G.

0,

Proof: No function from P to Px(y) can exist by assumption,

let alone a computable function.

2

The simplest example of a non-identifiable graph struc-

ture is the so called 'bow arc' graph, see Fig. 2 (a). Although

it is well known that Px(Y ) is not identifiable in this graph, we give a simple proof here which will serve as a seed of a

similar proof for more general graph structures.

Theorem 2 Px(Y ) is not identifiable in the bow arc graph.

Proof: We construct two causal models M 1 and M 2 such

that P 1(X, Y ) = models agree on

tPhe2(fXol,loYw)i,nagn:daPllx13(Yva)ri=abPlex2s(aYre).bTohoeletwano,

U is a fair coin, and fX (u) = u. Let  denote the exclusive or (XOR) function. Then the value of Y is determined

by the function u  x in M 1, while Y is set to 0 in M 2.

Then P 1(Y = 0) = P 2(Y = 0) = 1, P 1(X = 0) =

P 2(X = 0) = 0.5. Therefore, P 1(X, Y ) = P 2(X, Y ),

whNiloetPe xt2h(aYt

= 0) while

= P

1is=noPnx-1p(oYsi=tive0,)i=t is0s.5tr.aightforward

to

modify the proof for the positive case by letting fY functions in both models return 1 half the time, and the values outlined

above half the time.

2

A number of other specific graphs have been shown to

contain unidentifiable effects. For instance, in all graphs in

Fig. 2, taken from [Pearl, 2000], Px(Y ) is not identifiable. Throughout the paper, we will make use of the 3 rules of

do-calculus [Pearl, 1995].

· Rule 1: Px(y|z, w) = Px(y|w) if (Y  Z|X, W)Gx

· Rule 2: Px,z(y|w) = Px(y|z, w) if (Y  Z|X, W)Gx,z

· Rule 3: Px,z(y|w) = Px(y|w) if (Y  Z|X, W)Gx,z(w)

where Z These

(rWule)s=aZllo\wAinn(sWert)ioGnX

. and

deletion

of

interventions

and observational evidence into and from distributions, us-

ing probabilistic independencies implied by the causal graph

due to Theorem 1. Here Gxz is taken to mean the graph obtained from G by removing arrows pointing to X and arrows leaving Z.

C-Trees and Direct Effects

Sets of nodes interconnected by bidirected paths turned out to be an important notion for identifiability and have been studied at length in [Tian, 2002] under the name of Ccomponents.

Definition 3 (C-component) Let G be a semi-Markovian graph such that a subset of its bidirected arcs forms a spanning tree over all vertices in G. Then G is a C-component (confounded component).

If G is not a C-component, it can be uniquely partitioned into a set C(G) of subgraphs, each a maximal Ccomponent. An important result states that for any set C which is a C-component, in a causal model M with graph G, Pv\c(C) is identifiable [Tian, 2002]. The quantity Pv\c(C) will also be denoted as Q[C]. For the purposes of this paper, C-components are important because a distribution P in a semi-Markovian graph G factorizes such that each product term corresponds to a C-component. For instance, the graphs shown in Fig. 2 (b) and (c), both have 2 Ccomponents: {X, Z} and {Y }. Thus, the corresponding distribution factorizes as P (x, z, y) = Q[{x, z}]Q[{y}] = Py(x, z)Px,z(y). It is this factorization which will ultimately allow us to decompose the identification problem into smaller subproblems, and thus construct an identification algorithm.
We now consider a special kind of C-component which generalizes the unidentifiable bow arc graph from the previous section.

Definition 4 (C-tree) Let G be a semi-Markovian graph such that G is a C-component, all observable nodes have at most one child, and there is a node Y such that An(Y )G = G. Then G is a Y -rooted C-tree (confounded tree).

The graphs in Fig. 2 (a) (d) (e) (f) and (h) are Y -rooted C-trees.
There is a relationship between C-trees and interventional distributions of the form Ppa(Y )(Y ). Such distributions are known as direct ef f ects, and correspond to the influence of a variable X on its child Y along some edge, where the variables P a(Y ) \ {X} are fixed to some reference values.
Direct effects are of great importance in the legal domain, where one is often concerned with whether a given party was directly responsible for damages, as well as medicine, where elucidating the direct effect of medication, or disease on the human body in a given context is crucial. See [Pearl, 2000],[Pearl, 2001] for a more complete discussion of direct effects. The absence of Y -rooted C-trees in G means the direct effect on Y is identifiable.

Lemma 2 Let M be a causal model with graph G. Then for any node Y , the direct effect Ppa(Y )(Y ) is identifiable if there is no subgraph of G which forms a Y -rooted C-tree.

Proof: From [Tian, 2002], we know that whenever there is

no subgraph G of G, such that all nodes in G are ancestors

of Y , and G is a C-component, Ppa(Y )(Y ) is identifiable.

This entails the lemma.

2

Theorem 2 suggests that C-trees are troublesome struc-

tures for the purposes of identification of direct effects. In

1221

X
Y (a)

X
Z
Y (b)

Z XX

Y (c)

Z

Y (d)

Z X

X

X

ZX

Y (e)

Z
Y (f)

Z1
Y (g)

Z2

Figure 2: Graphs where Px(Y ) is not identifiable.

W
Y (h)

fact, our investigation revealed that Y -rooted C-trees are troublesome for any effect on Y , as the following theorem shows.

Theorem 3 Let G be a Y -rooted C-tree. Then the effect of any set of nodes X in G on Y is not identifiable if Y  X.

Proof: We generalize the proof for the bow arc graph. We

construct two models with binary nodes. In the first model,

the value of all observable nodes is set to the bit parity (sum

modulo 2) of the parent values. In the second model, the

same is true for all nodes except Y , with the latter being set

to 0 explicitly. All U nodes in both models are fair coins.

Since G is a tree, and since every U  U has exactly two

children in G, every U  U has exactly two distinct down-

ward paths to Y in G. It's then easy to establish that Y counts

the bit parity of every node in U twice in the first model. But

this implies P 1(Y = 1) = 0.

Because bidirected arcs form a spanning tree over observ-

able nodes in G, for any set of nodes X such that Y  X,

there exists U  in G \ An(X)G.

U with one Thus Px1(Y

child = 1)

in >

0A,nb(uXt P)Gx2(aYnd=on1e)

child = 0.

It is straightforward to generalize this proof for the positive

P (V) in the same way as in Theorem 2.

2

While this theorem closes the identification problem for

direct effects, the problem of identifying general effects on

a single variable Y is more subtle, as the following corollary

shows.

Corollary 1 Let G be a semi-Markovian graph, let X and Y be disjoint sets of variables. If there exists W  An(Y)Gx such that there exists a W -rooted C-tree which contains any variables in X, then Px(Y) is not identifiable.

Proof: Fix a W -rooted C-tree T , and a path p from W to Y  Y, where W  An(Y )Gx . Consider the graph p  T . Note that in this graph Px(Y ) = w Px(w)P (Y |w). It is now

easy to construct P (Y |W ) in such a way that the mapping

from Px(W ) to Px(Y ) is one to one, while making sure P is

positive.

2

This corollary implies that the effect of do(x) on a given

singleton Y can be non-identifiable even if Y is nowhere near a C-tree, as long as the effect of do(x) on a set of ances-

tors of Y is non-identifiable. Therefore identifying effects

on a single variable is not really any easier than the gen-

eral problem of identifying effects on multiple variables. We

consider this general problem in the next section.

Finally, we note that the last two results relied on exis-

tence of a C-tree without giving an explicit algorithm for

constructing one. In the remainder of the paper we will give

an algorithm which, among other things, will construct the

necessary C-tree, if it exists.

C-Forests, Hedges, and Non-Identifiability
The previous section established a powerful necessary condition for the identification of effects on a single variable. It is the natural next step to ask whether a similar condition exists for effects on multiple variables. We start by considering a multi-root generalization of a C-tree. Definition 5 (C-forest) Let G be a semi-Markovian graph, where Y is the root set. Then G is a Y-rooted C-f orest (confounded forest) if G is a C-component, and all observable nodes have at most one child.
We will show that just as there is a close relationship between C-trees and direct effects, there is a close connection between C-forests and general effects of the form Px(Y), where X and Y are sets of variables. To explicate this connection, we introduce a special structure formed by a pair of C-forests that will feature prominently in the remainder of the paper.

1222

Definition 6 (hedge) Let X, Y be disjoint sets of variables

in G. Let F, F be R-rooted C-forests such that F  X = ,

F  X = , F form a hedge for

 F , and R Px(y) in G.



An(Y)Gx .

Then

F

and

F

The mental picture for a hedge is as follows. We start with

a C-forest F . Then, F 'grows' new branches, while retain-

ing the same root set, and becomes F . Finally, we 'trim the

hedge,' by performing the action do(x) which has the effect

of removing some incoming arrows in F \ F , the 'newly

grown' portion of the hedge. It's easy to check that every

graph in Fig. 2 contains a pair of C-forests that form a hedge

for Px(Y ). The graph in Fig. 1 (a) does not contain C-forests forming

a hedge for Px(Y1, Y2), while the graph in Fig. 1 (b) does:

if F

e

is =

the edge between W1 F \ {X}. Note that

and X, then F for the special

= G \ {e}, and case of C-trees,

F is the C-tree itself, and F is the singleton root Y . This

last observation suggests the next result as a generalization

of Theorem 3.

Theorem 4 Assume there exist R-rooted C-forests F, F that form a hedge for Px(y) in G. Then Px(y) is not identifiable in G.

Proof: We first show Px(r) is not identifiable in F . As before, we construct two models with binary nodes. In M 1

every variable in F is equal to the bit parity of its parents.

In M 2 the same is true, except all nodes in F disregard the

parent values in F \ F . All U are fair coins in both models.

As was the case with C-trees, for any C-forest F , every

U  U  F has exactly two downward paths to R. It is now

easy to establish that in M 1, R counts the bit parity of every

node in U1 twice, while in M 2, R counts the bit parity of

every node in U2  F twice. Thus, in both models with no

interventions, the bit parity of R is even.

Next, fix two distinct instantiations of U that differ by val-

ues of U. Consider the topmost node W  F with an odd

number of parents in U (which exists because bidirected

edges in F form a spanning tree). Then flipping the values

of U once will flip the value W once. Thus the function

from U to V induced by a C-forest F in M 1 and M 2 is one

to one.

The above results, coupled with the fact that in a C-

forest, |U| + 1 = |V| implies that any assignment where

r (mod 2) = 0 is equally likely, and all other node

assignments are impossible in both F and F . Since the

two models agree on all functions and distributions in F \

F , f P 1 = f P 2. It follows that the observational distributions are the same in both models. Furthermore,

X)r>P

1(V) 0 for

is a positive any x.

distribution,

thus

P 1(x|P a(X)G

\

As before, we can find U  U with one child in An(X)F ,

and one child in F \ An(X)F , which implies Px(1 = r

(mod 2)) > 0 in M 1, but not M 2. Since Px(r) is not iden-

tifiable to be a

in G, and one to one

RmappAinng(Ybe)tGwxe, ewnePcxa(nr)coanndstrPuxc(tyP),(aYs|wRe)

did in Corollary 1.

For instance, let Y be the minimal subset of Y such that

ARn(R)Abne(Yequ)Galx

. Then let all nodes in G = to the bit parity of the parents.

WAnith(You)tGlox s\s

function ID(y, x, P, G) INPUT: x,y value assignments, P a probability distribution, G a causal diagram (an I-map of P). OUTPUT: Expression for Px(y) in terms of P or FAIL(F,F').

1 if x = , return v\y P (v).

2 if V = An(Y)G, return ID(y, x  An(Y)G, P (An(Y)), An(Y)G).

3

let W = (V \ X) if W = , return

I\DA(yn,(xY)Gwx .,

P,

G).

4 if C(G \ X) = {S1, ..., Sk}, return v\(yx) i ID(si, v \ si, P, G).

if C(G \ X) = {S},

5 if C(G) = {G}, throw FAIL(G, S).

6 if S  C(G), return s\y ViS P (vi|v(i-1)). 7 if (S )S  S  C(G), return ID(y, x  S ,
ViS P (Vi|V(i-1)  S , v(i-1) \ S ), S ).

Figure 3: A complete identification algorithm. FAIL propagates through recursive calls like an exception, and returns F, F which form the hedge which witnesses nonidentifiability of Px(y).  is some topological ordering of nodes in G.

of generality, assume every node in G has at most one child.

Then every R  R has a unique downward path to Y , which

means the bit parities of R and Y are the same. This implies

the result.

2

Hedges generalize not only the C-tree condition, but also

t[hTeiacno&mpPleetaerlc,o2n0d0it2io] nwfhoirchidsetnattiefiscathtiaotnifofYPixs

from P a child

in of

X and there a bidirected path from X to Y then (and only

then) Px is not identifiable. Let G consist of X, Y and the nodes W1, ..., Wk on the bidirected path from X to Y . It is not difficult to check that G and G \ {X} form a hedge for

Px(Y, W1, ..., Wk). Since hedges generalize two complete conditions for spe-

cial cases of the identification problem, it might be reason-

able to suppose that a complete characterization of identi-

fiability might involve hedges in some way. To prove this

supposition, we would need to construct an algorithm which

identifies any effect lacking a hedge. This algorithm is the

subject of the next section.

A Complete Identification Algorithm
Given the characterization of unidentifiable effects in the previous section, we can attempt to solve the identification problem in all other cases, and hope for completeness. To do this we construct an algorithm that systematically takes advantage of the properties of C-components to decompose the identification problem into smaller subproblems until either the entire expression is identified, or we run into the problematic hedge structure. This algorithm, called ID, is shown in Fig. 3.
Before showing the soundness and completeness proper-

1223

X W1

Y1

W1

Y1

(a) (b)
Figure 4: Subgraphs of G used for identifying Px(y1, y2).

ties of ID, we give the following example of the operation

of the algorithm. Consider the graph G in Fig. 1 (a), where

weWweanktntoowidethnatitfyGPx=(y1A, yn2(){fYr1o,mY2P}()VG,).C(G \ {X}) =

{{3GXa,n\Wd{}Xa)tt}e}m=, patn{dt{oYW1i}d,e{n=Wtif2y}{,WP{Y1x},2w.}(}yT,1h,usyos2,

we invoke line ). Now C(G \ we invoke line

4. Thus the original problem reduces to identifying

Swo2 lPvixn,wg1f,owr2

,y2 (y1)Pw1,x,y1,y2 (w2)Px,w1,w2,y1 the second expression, we trigger

(y2). line 2,

not-

ing that we can ignore nodes which are not ancestors of

W2. Thus, Pw1,x,y1,y2 (w2) = P (w2). Similarly, we ignore non-ancestors of Y2 in the third expression to obtain Ptaxin,wP1,ww22(,yy12()y=2) P=(yP2|ww22(y).2). We conclude at line 6, to ob-
Solving for the first expression, we first trigger line 2 also,

obtaining Px,w1,w2,y2 (y1) = Px,w1 (y1). The corresponding G is shown in Fig. 4 (a). Next, we trigger line 7, reducing the

problem to computing Pw1 (y1) from P (Y1|X, W1)P (W1). The corresponding G is shown in Fig. 4 (b). Finally, we trig-

ger line 2, obtaining Pw1 (y1) = Putting everything together, we

owb1taPin(:y1P|xx,(yw11,)yP2)(w1=).

w2
As

P (y2|w2)P (w2) we showed before,

w1 P (y1|x, w1 the very same

)P (w1 effect

). Px

(y1

,

y2)

in

a very similar graph G shown in Fig. 1 (b) is not identifiable

due to the presence of C-forests forming a hedge.

We now prove that ID terminates and is sound.

Lemma 3 ID always terminates.

Proof: At any call on line 7, (X  X)X  S , else the

failure condition on line 5 would have been triggered. Thus

any recursive call to ID reduces the size of either the set X

or the set V \ X. Since both of these sets are finite, and their

union forms V, ID must terminate.

2

To show soundness, we need a number of utility lemmas

justifying various lines of the algorithm. Though some of

these results are already known, we will reprove them here

using do-calculus to entail the results in the next section.

When we refer to do-calculus we will just refer to rule num-

bers (e.g. 'by rule 2'). Throughout the proofs we will fix

some topological ordering  of observable nodes in G. First,

we must show that an effect of the form Px(y) decomposes according to the set of C-components of the graph G \ X.

Lemma 4 Let M be a causal model with graph G. Let y, x be value assignments. Let C(G \ X) = {S1, ..., Sk}. Then Px(y) = v\(yx) i Pv\si (si). Proof: Assume X = , and let Ai = An(Si)G \ Si. Then

Pv\si (si) = Pai (si) =

Pai (vj |v(j-1) \ ai)

i i i Vj Si

= P (vj |v(j-1)) = P (vi|v(i-1)) = P (v)

i Vj Si

i

The first identity is by rule 3, the second is by chain rule

of probability. To prove the third identity, we consider two

cases. If A  Ai \ V(j-1), we can eliminate the intervention

on A from the expression Pai (vj |v(j-1)) by rule 3, since (Vj  A|V(j-1))Gai .
If A  Ai  V(j-1), consider any back-door path from

Ai to Vj. Any such path with a node not in V(j-1) will be d-separated because, due to recursiveness, it must contain a

blocked collider. Further, this path must contain bidirected

arcs only, since all nodes on this path are conditioned or

fixed. Because Ai  Si = , all such paths are d-separated. The identity now follows from rule 2.

The last two identities are just grouping of terms, and ap-

plication of chain rule. The same factorization applies to the

submodel Mx which induces the graph G \ X, which implies

the result.

2

The next lemma shows that to identify the effect on Y, it

is sufficient to restrict our attention to the ancestor set of Y,

thereby ensuring the soundness of line 2.

Lemma 5 Let X = XAn(Y)G. Then Px(y) obtained from

P in G is An(Y)G.

equal

to

Px

(y)

obtained

from

P

= P (An(Y)) in

Proof: Let W = induces the graph

V G

\ \

AWn(Y=)GA. nT(hYe)nGt,heansdubitms oddiesltriMbuw-

tion is P = Pw(An(Y)) = P (An(Y)) by rule 3. Now

Px(y) = Px (y) = Px ,w(y) = Px (y) by rule 3.

2

Next, we use do-calculus to show that introducing addi-

tional interventions in line 3 is sound as well.

Lemma Px,w(y),

6 Let where

W= w are

(V \ X) arbitrary

v\alAuens(Yof)GWx..

Then

Px(y)

=

Proof: Note that by assumption, Y  W|X in Gx,w. The

conclusion follows by rule 3.

2

Next, we must ensure the validity of the positive base case

on line 6.

Lemma 7 When the conditions of line 6 are satisfied,

Px(y) = s\y ViS P (vi|v(i-1)).

Proof: If line 6 preconditions are met, then G local to that

recursive call is partitioned into S and X, and there are no

bidirected arcs from X to S. The conclusion now follows

from the proof of Lemma 4.

2

Finally, we show the soundness of the last recursive call.

Lemma 8 Whenever the conditions of the last recursive call

of ID are satisfied, Px obtained from P in the graph G is equal to PxS obtained from P = ViS P (Vi|V(i-1)  S , v(i-1) \ S ) in the graph S .

Proof: It is easy to see that when the last recursive call exe-

cutes, X and S partition that the submodel Mx\S

Gin,dauncdesXthe gAranp(hSG)G\(. XT\hSis

implies )=S.

The distribution Px\S of Mx\S is equal to P by the proof

of Lemma 4. It now follows that Px = PxS ,x\S = PxS .

2
We can now show the soundness of ID.

1224

Theorem 5 (soundness) Whenever ID returns an expression for Px(y), it is correct.

Proof: If x = , the desired effect can be obtained from

P by marginalization, thus this base case is clearly correct.

The soundness of all other lines except the failing line 5 has

already been established.

2

Finally, we can characterize the relationship between

hedges and the inability of ID to identify an effect.

Theorem 6 Assume ID fails to identify Px(y) (executes line 5). Then there exist X  X, Y  Y such that the graph pair G, S returned by the fail condition of ID contain as edge subgraphs C-forests F, F that form a hedge for Px (y ).

Proof: Consider line 5, and G and y local to that recur-

sive call. Let R be the root set of G. Since G is a single

C-component, it is possible to remove a set of directed ar-

rows from G while preserving the root set R such that the

resulting graph F is an R-rooted C-forest.

Moreover, since F = F  S is closed under descendants,

and since only single directed arrows were removed from S

to obtain F , F is also a C-forest. F  X = , and F  X =

 by construction. algorithm. It's also

R clear

tAhant(Yy,)xGxlobcyal

lines 2 to the

and 3 of recursive

the call

in question are subsets of the original input.

2

Corollary 2 (completeness) ID is complete.

Proof: By the previous theorem, if ID fails, then Px (y ) is not identifiable in a subgraph H = An(Y)GDe(F )G of G. Moreover, X  H = X , by construction of H. As such, it is

easy to extend the counterexamples in Theorem 6 with vari-

ables independent of H, with the resulting models inducing

G, and witnessing the unidentifiability of Px(y).

2

The following is now immediate.

Corollary 3 (hedge criterion) Px(y) is identifiable from P in G if and only if there does not exist a hedge for Px (y ) in G, for any X  X and Y  Y.

So far we have not only established completeness, but also fully characterized graphically all situations where distributions of the form Px(y) are identifiable. We can use these results to derive a characterization of identifiable models, that is, causal models where all effects are identifiable.

Corollary 4 (model identification) Let G be a semiMarkovian causal diagram. Then all causal effects are identifiable in G if and only if G does not contain a node X connected to its child Y by a bidirected path.

Proof: If F, F are C-forests which form a hedge for some

effect, there must be a variable X  F , which is an ances-

tor of another variable Y  F . Thus, if no X exists with a

child Y in the same C-component, then no hedge can exist in

G, and ID never reaches the fail condition. Thus all effects

are identifiable. Pearl, 2002].

Otherwise,

Px

is

not

identifiable

by

[Tian

&
2

The complete algorithm presented in this section can be

viewed as a marriage of graphical and algebraic approaches

to identifiability. ID manipulates the first three arguments al-

gebraically, in a manner similar to do-calculus ­ not a coin-

cidental similarity as the following section will show. At the

function c-identify(C, T, Q[T]) INPUT: C  T , both are C-components, Q[T ] a probability distribution OUTPUT: Expression for Q[C] in terms of Q[T ] or FAIL
Let A = An(C)T . 1 If A = C, return T \C P 2 If A = T , return FAIL 3 If C  A  T , there exists a C-component T such that
C  T  A. return c-identify(C, T , Q[T ]) (Q[T ] is known to be computable from T \A Q[T ]) function identify(y, x, P, G) INPUT: x,y value assignments, P a probability distribution, G a causal diagram. OUTPUT: Expression for Px(y) in terms of P or FAIL. 1 Let D = An(Y)Gx . 2 Assume C(D) = {D1, ..., Dk}, C(G) = {C1, ..., Cm}. 3 return D\S i c-identify(Di, CDi , Q[CDi ]), where (i)Di  CDi
Figure 5: An identification algorithm modified from [Tian, 2002]
same time, if we ignore the third argument, ID can be viewed as a purely graphical algorithm which, given an effect suspected of being non-identifiable, constructs the problematic hedge structure witnessing this property.
Connections to Existing Identification Algorithms
In the previous section we established that ID is a sound and complete algorithm for all effects of the form Px(y). It is natural to ask whether this result can be used to show completeness of earlier algorithms conjectured to be complete.
First we consider do-calculus, which can be viewed as a declarative identification algorithm, with its completeness remaining an open question. We show that the steps of the algorithm ID correspond to sequences of standard probabilistic manipulations, and applications of rules of do-calculus, which entails completeness of do-calculus for identifying unconditional effects.
Theorem 7 The rules of do-calculus, together with standard probability manipulations are complete for determining identifiability of all effects of the form Px(y). Proof: We must show that all operations corresponding to lines of ID correspond to sequences of standard probability manipulations and applications of the rules of do-calculus. These manipulations are done either on the effect expression Px(y), or the observational distribution P , until the algorithm either fails, or the two expressions 'meet' by producing a single chain of manipulations.

1225

Line 1 is just standard probability operations. Line 5 is

a fail condition. The proof that lines 2, 3, 4, 6, and 7 cor-

respond to sequences of do-calculus manipulations follows

from Lemmas 5, 6, 4, 7, and 8 respectively.

2

Next, we consider a version of an identification algorithm

due to Tian, shown in Fig. 5. The soundness of this algo-

rithm has already been addressed elsewhere, so we turn to

the matter of completeness.

Theorem 8 Assume identify fails to identify Px(y). Then there exist C-forests F, F forming a hedge for Px (y ), where X  X, Y  Y.

Proof: Assume c-identify fails. Consider C-components

C, T local to the failed recursive call. Let R be the root set

of C. Because T = An(C)T , R is also a root set of T . As in the proof of Theorem 6, we can remove a set of directed

arrows from C and T while preserving R as the root set such

that the resulting edge subgraphs are C-forests. By line 1 of

ideFnitniaflyl,yC, b,eTcausAe nc-(iYde)Gntxi.fy will always succeed if Di =

XCDi

, it C

must = ,

be the XT

c=ase.thTahtuDs,iedgeCsDuib.gBraupthtshoisf

implies C, and

T are C-forests forming a hedge for Px (y ), where X  X,

Y  Y.

2

Corollary 5 identify is complete.

Proof: This is implied by Theorem 8, and Corollary 3. 2

Acknowledgments
We would like to thank Gunez Ercal, Manabu Kuroki, and Jin Tian for helpful discussions on earlier versions of this paper. We also thank anonymous reviewers whose comments helped improve this paper.

Conclusions
We have provided a complete characterization of cases when joint interventional distributions are identifiable in semiMarkovian models. Using a graphical structure called the hedge, we were able to construct a sound and complete algorithm for this identification problem, prove completeness of two existing algorithms, and derive a complete description of semi-Markovian models in which all effects are identifiable.
The natural open question stemming from this work is whether the algorithm presented can lead to the identification of conditional interventional distributions of the form Px(y|z). Another remaining question is whether the results in this paper could prove helpful for identifying general counterfactual expressions such as those invoked in natural direct and indirect effects [Pearl, 2001], and path-specific effects [Avin, Shpitser, & Pearl, 2005].

References
[1] Avin, C.; Shpitser, I.; and Pearl, J. 2005. Identifiability of pathspecific effects. In International Joint Conference on Artificial Intelligence, volume 19, 357­363.
[2] Dawid, A. P. 1979. Conditional independence in statistical theory. Journal of the Royal Statistical Society 41:1­31.

[3] Halpern, J. 2000. Axiomatizing causal reasoning. Journal of A.I. Research 317­337.
[4] Huang, Y., and Valtorta, M. 2006. On the completeness of an identifiability algorithm for semi-markovian models. Technical Report TR-2006-01, Computer Science and Engineering Department, University of South Carolina.
[5] Kuroki, M., and Miyakawa, M. 1999. Identifiability criteria for causal effects of joint interventions. Journal of Japan Statistical Society 29:105­117.
[6] Pearl, J., and Robins, J. M. 1995. Probabilistic evaluation of sequential plans from causal models with hidden variables. In Uncertainty in Artificial Intelligence, volume 11, 444­453.
[7] Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems. Morgan and Kaufmann, San Mateo.
[8] Pearl, J. 1995. Causal diagrams for empirical research. Biometrika 82(4):669­709.
[9] Pearl, J. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press.
[10] Pearl, J. 2001. Direct and indirect effects. In Proceedings of UAI-01, 411­420.
[11] Spirtes, P.; Glymour, C.; and Scheines, R. 1993. Causation, Prediction, and Search. Springer Verlag, New York.
[12] Tian, J., and Pearl, J. 2002. A general identification condition for causal effects. In Eighteenth National Conference on Artificial Intelligence, 567­573.
[13] Tian, J. 2002. Studies in Causal Reasoning and Learning. Ph.D. Dissertation, Department of Computer Science, University of California, Los Angeles.

1226

