Unsupervised learning of distursiibnugtitownos loanyebrinnaertywovrekcstors
Yoav Freund David Haussler
UCSC-CRL-94-25 June 22, 1994
Baskin Center for Computer Engineering & Information Sciences
University of California, Santa Cruz Santa Cruz, CA 95064 USA
abstract
We present a distribution model for binary vectors, called the in uence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model is closely related to the Harmonium model de ned by Smolensky RM86] Ch.6]. In the rst part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The rst algorithm is based on gradient ascent. Here we give a closed form for this gradient that is signi cantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.

1. Introduction

1

Suppose that we are given a large (unordered) set of binary vectors and that we wish to nd the types of correlations and redundancies that exist between the bits in these vectors. We assume that each binary
vector is of the form ~x = fx1; : : :; xng 2 f 1gn, and that each vector is generated independently at random according some unknown distribution on f 1gn. Such an assumption is natural, for instance, when each instance consists of (possibly noisy) measurements of n di erent binary attributes of a randomly selected object. Our interest is in cases where the dimension n of the vectors is large, say n > 50. One example
of this type of scenario is when the instances are binary images of handwritten digits, where each bit corresponds to the black or white color of a single pixel in the image. The correlations that we expect to see in this case correspond to the fact that the values of neighboring pixels or pixels that lie along lines or curves are strongly dependent on each other.
Knowledge of the correlations between di erent bits of the binary vector is useful when we want to use a set of measurements for various classi cation and prediction tasks. The idea that features that are useful for classi cation can be deduced from the distribution of typical inputs is the basis of several existing algorithms for unsupervised learning. One type of algorithm selects projections of the input based on Principle Component analysis San89, Oja89]. Another type of algorithm clusters data based on an assumption that the underlying distribution is a mixture of Gaussians EH81, Now90]. The combination model presented in this paper is related to both of these lines of work and has some advantages over each of them.
If we nd a good model of the distribution, we can tackle other interesting learning problems, such as the problem of estimating the conditional distribution on certain components of the vector x~ when provided with the values for the other components (a kind of regression problem), or predicting the actual values for certain components of ~x based on the values of the other components (a kind of pattern completion task). In the example of the binary images presented above, this would amount to the task of recovering the value of a pixel whose value has been corrupted. We can often also use the distribution model to help us in a supervised learning task. This is because it is often easier to express the mapping of an instance to the correct label by using \features" that are correlation patterns among the bits of the instance. For example, it is easier to describe each of the ten digits in terms of patterns such as lines and circles, rather than in terms of the values of individual pixels, that are more likely to change between di erent instances of the same digit.
The process of learning an unknown distribution from examples is usually called density estimation or parameter estimation in statistics, depending on the nature of the class of distributions used as models. There has been considerable work on density/parameter estimation for distributions on real vector spaces (see e.g. DH73]), and less on binary vector spaces. The most popular mainstream statistics models for
distributions on f 1gn for large n appear to be small mixtures of Bernoulli product distributions1 EH81, Now90], and models in which only k-wise dependencies between the components of the input are allowed,
1A Bernoulli product distribution is a distribution over binary vectors in which each component is chosen independently of the rest.

2 1. Introduction
for some k << n Fre87, CS89]. Newer and more exciting models include Bayes networks Pea88] and Markov random elds Pea88, GG84, Gem86]. In the neural network area, both Hop eld nets Hop82] and Boltzmann machines AHS85] can be used as models of probability distributions on f 1gn for relatively large n. We will look at a class of models de ned by a special type of Boltzmann machine.
Hop eld networks, Boltzmann machines and Markov random elds are all based on the statistical physics concepts of energy and local interaction between units whose state is binary.2 The models de ned by Hop eld networks and Boltzmann Machines are special cases of the more general Markov random eld model. The units in a Hop eld network correspond to the bits of the binary vectors and the interaction between units are restricted to symmetric pairwise interactions. Boltzmann machines also employ only pairwise interactions, but in addition to the units that correspond to bits of the data vectors, commonly called the input units, there are hidden units, which correspond to unobserved binary variables. These hidden units interact with the input units and generate correlations between the vector bits that the input units represent. The distribution of the binary vectors generated by the Boltzmann Machine is de ned as the marginal distribution induced on the state of the input units by the Markov random eld over all units, both observed and hidden.
While the Hop eld network is relatively well understood, it is limited in the types of distributions that it can model. On the other hand, Boltzmann machines are universal in the sense that they are powerful enough to model any distribution (to any degree of approximation), but the mathematical analysis of their capabilities is often intractable. Moreover, the standard learning algorithm for the Boltzmann machine, a gradient ascent heuristic to compute the maximum likelihood estimates for the weights and thresholds, requires repeated stochastic approximation, which results in unacceptably slow learning. Many methods have been proposed to speed up learning in Boltzmann machines. One of these methods is the mean- eld approximation PA87]. In Section 2.2 we shall see some relations between one of our learning rules and the mean eld approximation.
In our research we have attempted to narrow the gap between Hop eld networks and Boltzmann machines by nding a model that will be powerful enough to be universal, yet simple enough to be analyzable and computationally e cient.3 The model that we use in this work is essentially a Boltzmann Machine whose connection graph is bipartite. There are two types of nodes: \input" nodes and \hidden" nodes. Each input node is connected to each of the hidden nodes, and no other connections exist. We call
2Informally, a Markov random eld consists of a set of random variables that are connected as nodes in a graph. The distribution of each random variable is determined by the value of its neighbors. In other words, given the value of all the neighbors of random variables, the value of the random variable is independent of the state of the rest of the random variables. The Markov process is a special case of the Markov eld in which each random variable corresponds to a speci c time step and its neighbors are the random variables that correspond to the previous and the succeeding time steps. In general, Markov- eld distributions have a canonical description that is based on the concept of interaction energy.
3Recent work on modeling correlations by hidden units has also been done by Radford M. Neal Nea90]. In his work he gives a di erent variant of the Boltzmann Machine algorithm that uses distribution models similar to Judea Pearl's Bayes Networks Pea88, GP87]. His model is superior to the Boltzmann Machine in the sense that the connection weights are interpreted as conditional probabilities, which is a more accessible interpretation than local energy interactions. The learning algorithms that Neal used are based on stochastic approximation. The question of whether a two-layer model of this type has universal representation capabilities is open.

3
this model the in uence combination machine, or, for short, the combination machine. We refer to the distribution that is de ned on the binary vectors by the combination machine as the combination model. This type of Boltzmann machine was previously studied by Smolensky in his harmony theory RM86] Ch.6]. In his work he discusses several possible ways of using this type of model for solving problems in Arti cial Intelligence. In our work we concentrate on the mathematical properties of the model and on e cient algorithms for learning the model from random instances.
The combination machine consists of two types of units: input units, each of which holds one component of the input vector, and hidden units, representing hidden variables. There is a weighted connection between each input unit and each hidden unit, and no connections between input units or between hidden units (see Figure 2.1). The presence of the hidden units induces dependencies, or correlations, between the variables modeled by input units. To illustrate the representational power of the combination model, consider the distribution of people that visit a speci c co ee shop on Sunday. Let each of the n input variables represent the presence (+1) or absence ( 1) of a particular person that Sunday. These random variables are clearly not independent. For example, if Fred's wife and daughter are there, it is more likely that Fred is there as well, if you see three members of the golf club, you expect to see other members of the golf club, if Bill is there, you are unlikely to see his ex-wife Brenda there, etc. This situation can be modeled by a combination model in which each hidden variable represents the presence or absence of a social group. The weights connecting a hidden unit and an input unit measure the tendency of the corresponding person to be associated with the corresponding group. In this co ee shop situation, several social groups may be present at the same time, exerting a combined in uence on the distribution of customers. In Sections 2.3 and 2.4 we discuss why the combination model is better for describing this type of distributions than popular models such as the mixture model and principal components methods.4
We show that the combination model is a universal model in the sense that any probability distribution
on f 1gn can be represented by a combination model with n input units to within any desired accuracy.
Then we show that the standard Boltzmann machine learning rule, when applied to a combination model, can be computed in closed form, instead of using random sampling techniques. Thus we get a faster learning algorithm than the standard Boltzmann rule that is also exact. The computational complexity of the learning rule is exponential in the number of hidden units. However, under certain natural conditions we show that there exists a good approximation that requires only polynomial time.
We then explore the relationships between the distributions generated by the combination model and those studied in Projection Pursuit density estimation Hub85, FWS84, Fri87]. We show that the search for hidden variables that have a strong in uence on the input distribution can be interpreted as a search for projections of the input that have a non-Normal marginal distribution. Based on this observation, we propose a learning algorithm based on exploratory projection pursuit for the combination model. This method is a greedy method that adds a single hidden unit at a time to the model. The time complexity of this method is linear in the number of hidden units compared to the exponential complexity of the gradient based method. However, while the gradient based method is guaranteed to converge to a local minimum in the model space, the projection pursuit method does not have this guarantee.
4Noisy-OR gates have been introduced in the framework of Bayes Networks to allow for such combinations Pea88]. However, using this in networks with hidden units has not been studied, to the best of our knowledge.

4 1. Introduction
We conclude this paper with results of some experiments. The rst set of experiments compare the two learning algorithms on synthetically generated data, and demonstrate their advantages and de ciencies. The second set of experiments compare the performance of the combination model to that of the mixture model and demonstrate the di erence in the type of distribution representations that they generate.

2. The in uence combination distribution model

5

2.1 Notation

For the most part, we use standard algebraic notation in our formulas. Elements from the n-dimensional spaces Rn and f 1; +1gn are denoted by vectors ~x; ~y; : : :. We denote by jj~xjj1,jj~xjj2 the l1 and l2 norms of ~x, and by ~x ~y the dot product between two vectors. We use the standard hyperbolic trigonometric

functions

sinh(x) = ex

e 2

x;

cosh(x)

=

ex

+e 2

x;

tanh(x)

=

sinh(x) cosh(x)

:

We denote the natural base logarithm by \ln". Finally, we use the function logistic(x) = 1=(1 + exp( x))

that is commonly used in the de nition of Boltzmann Machines.

2.2 The Model

In this section we present the combination machine and the corresponding distribution model, which is the in uence combination distribution model. The combination machine is a simple Connectionist type model which is a special case of the Boltzmann Machine AHS85]. As we shall see, the simplicity of this special case makes it easier to analyze than the general Boltzmann machine and allows the use of more e cient learning algorithms. At the same time, the model is still powerful enough to approximate any distribution of binary vectors.
To model a distribution on f 1gn we use a machine with n + m units. There are two types of units, n input units, each of which represents a single bit in the random vector, and m hidden units, whose role
is the create correlation between the values of the input units. These units are connected in a bipartite graph as illustrated in Figure (2.1).
h1 h2 h3
Hidden Units
m=3

w (2) 1 n=5
x1Input Units x2 x3 x4 x5 Figure 2.1: The bipartite graph of the combination model

6 2. The in uence combination distribution model

The random variables represented by the input units each take values in f+1; 1g, while the hidden

variables, represented by the hidden units, take values in f0; 1g. The state of the machine is de ned by

the and

values of these random
by ~h = (h1; : : :; hm) 2

variables. We denote by x~ = (x1; : : :; f0; 1gm the state of the hidden units.

xn

)

2

f

1gn the state of the input units,

There are m(n+1) real-valued parameters associated with the machine. Each particular setting of these

parameters de nes the parameter vector of the machine. Each parameter vector de nes a distribution on

the states of the machine. Summing over the state of the hidden units we get a distribution on the

input units, which is the in uence combination distribution de ned by the particular parameter vector.

There are two variants of the combination model, which we call the binary valued and the real valued

combination machines. While we are mostly interested in the binary model, the real valued model is a

useful approximation in some cases.

The parameters are all real-valued and are de ned as follows. There is a weight parameter associated with each edge in the bipartite graph. We denote by !j(i) the weight of the edge connecting the ith hidden unit to the jth input unit. We also use !~ (i) to denote the vector of all n weights associated with the ith

hidden unit.1 There is a bias parameter associated with each hidden unit. We denote the bias of the ith

hidden unit by (i) 2 R. The complete parameter vector of a binary combination model is denoted by

B = f(!~ (1); ;)(1) : : :; (!~ (m); (m))g. For a given B, the energy of a state of the combination machine is

de ned as

E(~x;~hj B) = Xm (~!(i) ~x + (i))hi
i=1

(2:1)

and the probability of a state is de ned to be

P r(~x; ~hj

B) =

1 ZB

e

E(~x;~hj

B)

where

ZB = X e E(x~;~hj B):
~x;~h

We nd it useful to de ne the \combined weight" of a particular state of the hidden units as the sum of the weight vectors corresponding to the hidden units whose state is 1:

!~ (~h) = Xm hi!~ (i)
i=1

Plugging the de nition of the energy into the de nition of ZB, we get that

ZB = X exp

Xm (!~ (i)

~x +

!
(i))hi :

x~;~h i=1

Expanding the sum in the exponent we get that

ZB = X @0exp(Xm hi (i))

X

1 exp(~x !~ (~h))A :

~h2f0;1gm i=1 ~x2f 1;+1gn

1In RM86] Ch.6], binary connection weights are used, here we use real-valued weights.

2.2. The Model

7

Expanding the sum over ~x 2 f 1; +1gn, we get that

ZB =

X

0@exp(Xm hi

(i)) Yn (exp(!~ (~h)j) + exp(

1 !~ (~h)j))A ;

~h2f0;1gm i=1 j=1

where !~ (~h)j denotes the jth component of !~ (~h). Using the de nition of cosh(x), we can rewrite the last

expression as

ZB = 2n

X

42exp(Xm hi

(i)) Yn

3 cosh(!~ (~h)j )5

:

(2:2)

~h2f0;1gm

i=1

j=1

Note that the trivial model, in which there are no hidden units, de nes the uniform distribution over the state vectors ~x. In the general case the probability distribution over possible state vectors on the input

units is given by

P r(~xj

B) = X Pr(~x;~hj
~h2f0;1gm

B) =

1 ZB

X
~h2f0;1gm

exp

Xm (!~ (i)
i=1

~x +

!
(i))hi

(2:3)

By separating the sum over ~h into a sum over all ~h such that hm = 0 and a sum over all ~h such that

hm = 1, we can rewrite Equation (2.3) in the following form:

P r(~xj

B) =

1 ZB

e e+0 ~!(m) x~+ (m)

X exp
fh1;:::;hm 1g2f0;1gm 1

mX1(~!(i)
i=1

~x +

!
(i))hi

Repeating this manipulation for all m components of ~h we get that

P r(~xj

B)

=

1 ZB

Ym
i=1

e1 + ~!(i) x~+ (i)

:

(2:4)

Equation (2.4) is a simple closed form representation of the distribution de ned by the parameter vector B. Notice that the hidden unit variables, hi, are not explicitly present in this formula. Each factor in the product is associated with one hidden unit in the corresponding machine. This product form is particular

to the combination model, and does not hold for general Boltzmann machines. Product form distribution

models have been used for density estimation in Projection Pursuit Hub85, FWS84, Fri87]. We shall look

further into this relationship in Section 3.2.

In some of the following discussion we shall nd it useful to use a variant of the combination model that de nes distributions over the whole real space Rn, i.e. to allow each input to have any real-value instead

of limiting it to only +1 and -1. The structure of the machine is the same, we keep the hidden variables
f0; 1g-valued, and the distribution is de ned in a similar way, but the energy function has an extra term

that is necessary for ensuring that the distribution can be normalized. This term corresponds to each input

unit having a connection of strength 1 to itself. To di erentiate between the binary and the real-valued models we subscript quantities relating to the real-valued model by R. The energy of a particular state of

the real-valued model is given by
E(~x;~hj R) =

Xm (w~i
i=1

~x +

!
(i))hi

+

1 2

jj~xjj22;

(2:5)

8 2. The in uence combination distribution model

which produces the following distribution over the Rn:

Pr(~xj R) = X Pr(~x;~hj R) = e
~h2f0;1gm

Y1
2

jj~xjj22

1 ZR

m i=1

1 + ew~i ~x+ (i)

;

where

=

~h2fX0;=1gm(2ZR)nnZ=e2xRp=XZR12nj~hje~x2xjfXjp022;"1+gXmmXi=meh1x(ipw~(ii)

~h2f0;1gm

i=1

E x~ +

(~x;~hj R) d~x

!
+ (i))hi d~x

1 2

# jj!~ (~h)jj22

;

using the integral of the Gaussian distribution.

We discuss the relation between the real-valued and the binary-valued model in Section 2.6.

(2:6) (2.7) (2.8)

2.3 Discussion of the model

The right hand side of Equation (2.4) has a simple intuitive interpretation. The ith factor in the product

corresponds to the hidden variable hi and is an increasing function of the dot product between ~x and the

weight vector of the ith hidden unit. Hence an input vector ~x will tend to have large probability when it is

in the direction of one of the weight vectors w~i (i.e. when w~i ~x is large), and small probability otherwise.

This is the way that the hidden variables can be seen to exert their "in uence"; each corresponds to a

preferred or "prototypical" direction in space. The bias parameter (i), together with the length jjw~ijj2

of the weight vector, control the strength of the in uence of the ith hidden variable in comparison with

the other hidden variables, as well as its \width", i.e. how close in direction x~ has to be to w~i before it

signi cantly in uences its probability. Increasing either jjw~ijj2 or (i) increases the strength of the in uence of the hidden unit. Decreasing (i) and, at the same time, increasing jjw~ijj2, decreases the \width" of the

in uence, making the in uence of the ith hidden unit more restricted to input vectors whose direction is

very close to the direction of w~i. This is true for both the binary-valued and the real-valued combination

models.

Equation (2.3) shows that the combination model can be written as a mixture of 2m distributions of

the form

1 Z(~h)

exp

Xm (!~ (i)
i=1

x~ +

!
(i))hi ;

where ~h 2 f0; 1gm and Z(~h) is the appropriate normalization factor. Each of these distributions is a product

of n with

Bernoulli distribution, probabilities logistic(

i2.e!~.(~thh)ej)xajnids

drawn independently at random and logistic(+2!~ (~h)j) respectively, which

attains implies

a value of 1 or +1 that the mean of xj

in according to this distribution is tanh(!~ (~h)j). We shall refer to this type of distribution as a \Bernoulli

product distribution". The combination model is a mixture of 2m Bernoulli product distributions, each

corresponding to a setting of ~h and each having a mixture coe cient Z(~h).

2.3. Discussion of the model

9

It is interesting to compare the class of combination models to the standard class of models de ned

by a mixture of Bernoulli product distributions. The same bipartite graph described in Figure (2.1) can

be used to probability

de pi

ne a such

tshtaatndParmi=d1mpiix=tu1r.e

model. Assign To generate an

each of the m hidden units example, choose one of the

a weight vector w~i and a hidden units according to

itshethdeisatpripbruotpiorinatdeenonremd abliyzatthioe npif'asc, taonrdsothtehnatchPo~xo2sfe

the
1gn

vector ~x according Pi(~x) = 1. We thus

to Pi(~x) = Z1i ew~i ~x, where get the distribution

Zi

P

(~x)

=

Xm
i=1

pi Zi

ew~

i

~x

(2:9)

This form for presenting the standard mixture model emphasizes the similarity between this model and the combination model. A vector x~ will have large probability if the dot product w~i ~x is large for some 1 i m (so long as pi is not too small). However, unlike the standard mixture model, the combination model allows more than one hidden variable to be +1 for any generated example. This means that several
hidden in uences can combine in the generation of a single example, because several hidden variables can
be +1 at the same time.

To see why this is useful, consider two examples. First, consider the co ee shop example given in the introduction. At any moment of time it is reasonable to nd several social groups of people sitting in the shop. The combination model will have a natural representation for this situation, while in order for the standard mixture model to describe it accurately, a hidden variable has to be assigned to each combination of social groups that is likely to be found in the shop at the same time. Similarly, when we want to represent the distribution of binary images of digits, it is reasonable to assume that each speci c image contains several patterns, such as lines and curves. Of course, the whole digit can be perceived as a pattern, in which case the mixture model is the relevant distribution model. However, we claim that it is often more appropriate to represent each digit image as a combination of patterns rather than a single pattern. In other words, we claim that, for typical sets of images of digits, the maximal likelihood combination model will have larger likelihood than a mixture model with the same number of parameters. In Section 4 we give experimental evidence to support this claim. In cases where this claim is correct the combination model is exponentially more succinct than the standard mixture model, and naturally captures the underlying product structure of the distribution. Of course, if the space of hidden variables does not have a product structure of this type, then the combination model is no better than the standard mixture model.

In analogy with the binary combination model, the real-valued combination model can be shown to represent a mixture of 2m symmetric Gaussian distributions. From Equation (2.6) we get that for the empty case, m = 0, where there are no hidden variables present, the distribution is a symmetric Gaussian by de nition. When a single hidden variable is present, the distribution becomes

P r(~xj

R) = e

1 2

jj~xjj22

1 Z

e1 + w~1 x~+ (1)

=

1 Z

e + e1 2

jjx~jj22

1 2

jj~xjj22

+w~

1

~x+

(1)

=

Z e e :1 +1 2

jj~xjj22

1 2

jj~x

w~ 1jj22

+

1 2

jjw~1jj22

+

(1)

This is a mixture of two Gaussians, both of which have spherical symmetry. They di er only in the location

of the mean, which is ~0 for the rst component and w~1 for the second component, and in their relative

10 2. The in uence combination distribution model probabilities (mixture weights). Each additional hidden unit has the e ect of transforming the previous distribution into a mixture of two distributions, one is the previous distribution, and the other is the original distribution shifted by w~i (See Figure 2.2).
w3
w2 w1
Figure 2.2: The distribution over R2 generated by a (real valued) combination model with three hidden units. Each pair of concentric circles denotes a single Gaussian distribution. The distribution de ned by the combination model is a mixture of these eight Gaussians.
In the general case the combination model with m hidden units is equivalent to a mixture of 2m Gaussians whose expected values are located at the combined weight, ~!(~h), corresponding to each of the 2m possible states of ~h.2 This interpretation of the real-valued model will be used in Section (3.5) in a Projection Pursuit algorithm for learning the combination model.
2.4 Comparison with principal components analysis
Principal Component Analysis (PCA) is a popular method for the analysis of high order correlations (see e.g. Jol86]). Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks San89, Oja89]. The method is based on the covariance matrix, which measures pairwise correlations among input bits. The main assumption underlying the method is that the low dimension projections of the data that retain the largest amount of information are those projections that have the largest variance. One justi cation of this assumption is that if the data has a simple enough distribution such as a Gaussian distribution then the reconstruction of the original input from its projections is optimal for this choice of projections. The directions with largest variance are equal to the directions of the eigenvectors of the covariance matrix that have the largest eigenvalues.
2Compare this to the mixture of Bernoulli products whose expected values are tanh(!~ (~h)). A more detailed comparison of
the two models will be given in Section 2.6.

2.5. Universality of the model

11

The neural network implementation of PCA is usually a two layer network with the same architecture as the combination model. The learning rule, however, is di erent, and tries to make the weight vectors of the hidden units equal to eigenvectors of the covariance matrix of the input. The outputs of the hidden units are thus projections of the data (or a nonlinear transformation of such projections).
This type of network is capable of representing each input as a combination of correlation patterns. In this sense it is as powerful as the combination model and does not su er from the de ciencies of mixture models described in the previous section. However, as this method of analysis is based only on the second order correlations among pixels it necessarily ignores part of the structure of the distribution. In the combination model, on the other hand, each hidden unit can represent correlations of arbitrary order. We claim that some natural distributions have strong high order correlation and that taking into account only the second order correlations ignores some of the most important information available in the distribution. In Section 4 we shall give some experimental evidence to support this claim.

2.5 Universality of the model

Despite its limited connectivity, it is not hard to show that the class of binary combination models is
universal in the sense that for every n and every distribution on f 1gn there is a combination model with n input units that approximates that distribution to within any desired accuracy. The argument is similar
to an argument for the same claim regarding the class of mixtures of Bernoulli product distributions.

Assume rst that the distribution we want to estimate is Pr(~x) = p for ~x = (1; 1; : : :; 1) and Pr(~x) =

21n

p
1

for

~x

6=

(1; 1; : : :; 1).

Here we need only one hidden unit.

We

de

ne

q

=

p(2n
1

p1)

and

choose

!~ (1) = (a; a; : : :; a) and =(1) na + ln(q values for f(~x) := 1 + e!~(1) ~x+ .(1)

1),

where

a

=

1 2

ln(q

1) +

1 2

ln(1=

).

We

get

the

following

If ~x = (+1; +1; : : :; +1), then f(~x) is equal to q. For a vector where exactly one component is equal to 1 and all the rest are +1, f(~x) is equal to 1 + , and for a vector x~ which has k components that are equal to 1, f(~x) is equal to 1 + (q 1)( =(q 1))k 1 + k. By setting small enough we can make 1 + e!~(1) ~x+ (1) arbitrarily close to 1 for all ~x 6= x~ = (+1; +1; : : :; +1). Normalizing the distribution to sum to 1 we can get a distribution that is arbitrarily close to the desired distribution.

To approximate an arbitrary distribution, we multiply 2n factors, each approximating a distribution that is highly concentrated on a single setting of ~x and almost uniform on all other settings. By appropriate choice of the parameters we can approximate the arbitrary distribution closely for each value of x~. Of course
this requires exponentially many hidden units, but this is unavoidable since it requires an exponential
number of parameters to specify an arbitrary distribution over f 1gn in any reasonable parametric model.

Of course, we are interested in cases where the distribution of the data can be represented well by a small combination model. While a general distribution might require many hidden units to model it, distributions that are encountered in nature are often simple, and can be modeled well by a model that has only a small number of hidden units. In Section 4 we show that the distribution of images of handwritten digits can be approximated well by a combination model with few hidden units.

12 2. The in uence combination distribution model
2.6 Relations between the binary-valued and the real-valued models

Two variants of the combination model were introduced in Section 2.2, the binary-valued model (Equations (2.1) to (2.4)) and the real-valued model (Equations (2.5) to (2.8)). The binary-valued model is the natural model for representing distributions of binary vectors, and thus, ideally, we would like to use only this model. On the other hand, the real-valued model has properties that make it possible to use more e cient learning algorithms to learn it. As we show in this section, the real-valued model is an approximation of the binary model when the weights are all small. Thus we can use the algorithms for the real-valued model to nd an approximate parameter vector of the binary model.

The real-valued model de nes a density function, in contrast with the binary model, which de nes a

point mass distribution. However, the ratio between the densities assigned by a real-valued model to any

pair of points in f 1; +1gn is equal to the ratio of the probabilities assigned to the same points by a binary

model with the same parameters.

This is because the factor of e

1 2

jj~xjj22

in

the

density

function

is

equal

to

e n=2 for all vectors in f 1; +1gn.

This does not mean that the maximum-likelihood parameter vector for a given set of examples is equal for both models. This is because the normalization factors ZB and ZR are di erent for each of the two cases. However, as we shall now see, when the weight vectors !~ are small the normalization factors are
very close to each other.

Recall Equation (2.2):

ZB = 2n

X

2 4exp

Xm hi

!
(i)

Yn cosh

3 ~!(~h)j 5 :

~h2f0;1gm

i=1

j=1

The Taylor expansion of cosh(x) around x = 0 is:

cosh(x)

=

1

+

x2 2!

+

x4 4!

+

x6 6!

+

:

:

:

thus the rst order approximation of ZB for small values of !j(i) is:

ZB

2n

X

2 4exp

~h2f0;1gm

Xm hi

!
(i)

i=1

Yn
j=1

1

+

1 2

!~ (~h)j 2

3 5

On the other hand, note that Equation (2.8) can also be written as:

ZR = (2 )n=2

X

2 4exp

~h2f0;1gm

Xm hi

!
(i)

i=1

Yn exp
j=1

1 2

!~ (~h)j 2

3 5;

The Taylor expansion of exp(x) around x = 0 is

exp(x)

=

1

+

x

+

x2 2!

+

x3 3!

+

:

:

:

2.6. Relations between the binary-valued and the real-valued models

13

thus the rst order approximation of ZR for small values of !j(i) is:

ZR

(2 )n=2

X

2 4exp

~h2f0;1gm

Xm hi

!
(i)

i=1

Yn
j=1

1

+

1 2

!~ (~h)j 2

3 5

2 n=2 ZB :

The fact that the two models di er by a constant factor is of no consequence when looking for the maximallikelihood parameter vector, because this constant factor disappears in the derivative of the log of the likelihood.
The di erence between the two approximations is of the order of jj!~ (~h)jj42. Thus if jj!~(~h)jj2 is much
smaller than 1 the approximation is reasonable.
There is another way in which the two models can be compared. In Section 2.3 we have shown that both the real-valued and the binary-valued combination models are equivalent to mixture models. The real-valued model is equal to a mixture of 2m Gaussian distributions. Each mixture component corresponds to a setting of ~h and has an expected value of !~ (~h). Similarly, the binary-valued combination model is equivalent to a mixture of 2m Bernoulli product distributions, each of which has an expected value of tanh(~!(~h)). When !~ (~h) is small tanh(~!(~h)) !~ (~h).
It is easy to show that every one dimensional projection of a Gaussian distribution generates a Normal marginal distribution. Thus the marginal distribution that is generated by the real-valued combination model is a mixture of normal distributions. Diaconis and Friedman DF84] have shown that, in some sense, most \well-behaved" distributions generate a marginal distribution that is close to normal when projected on a randomly chosen direction. In particular, the uniform distribution on the 2n binary vectors
in f 1; +1gn generates, with very high probability, a marginal distribution that is close to the normal distribution, when the projection direction is chosen uniformly at random from the n dimensional sphere, and n is large. In Appendix A, we show that this is also true for Bernoulli product distributions, if the
distributions of the individual coordinates are not too biased. Thus, under reasonable assumptions, the marginal distribution that is generated by the binary valued combination model is also mixture of normal distributions.
In addition, if the weight vectors, w~i, are short, then the mixture coe cients and the means of the mixture components of the two models are close, which implies that the projections of the distributions de ned by the real-valued model and the binary valued model with the same parameters are very close to each other. We use this correspondence in our analysis of the projection pursuit learning methods, which are based on properties of projections of the data.

14
3. Learning the model from examples

3. Learning the model from examples

3.0.1 Learning by gradient ascent on the log-likelihood

We now suppose that we are given a sample consisting of a set S of vectors in f 1gn drawn independently at random from some unknown distribution. Our goal is to use the sample S to nd a good model for this unknown distribution using a combination model with m hidden units, if possible. The method
we investigate here is the method of maximum likelihood estimation using gradient ascent. The goal of
learning is reduced to nding the set of parameters for the combination model that maximizes the (log of the) probability of the set of examples S. In fact, this gives the standard learning algorithm for general
Boltzmann machines AHS85]. For a general Boltzmann machine this would require stochastic estimation of the parameters. As stochastic estimation is very time-consuming, the result is that learning is very slow.
In this section we show that stochastic estimation need not be used for the combination model.

From Equation (2.4), the log of the likelihood of a sample of input vectors

S = f~x(1); ~x(2); : : :; ~x(N)g, given a particular setting B = f(~! ;(1) ;)(1) : : :; (~!(m); (m))g of the parame-

ters

of

the

model is: log-likelihood(

B) =

X ln Pr(~xj

B

)

=

Xm

0 @

X

ln(1

+

e!~ (i)

x~+

1 A)(i)

N ln ZB :

(3:1)

~x2S i=1 ~x2S

Taking the gradient of the log-likelihood results in the following formulas. For the bias parameters we

get:

@

@
(i)

log-likelihood

(

B

)

=

X
~x2S

1

+

e

1
(~!(i) ~x+ (i))

N X Pr(~xj
x~2f 1gn

B)1+ e

1
(~!(i) x~+ (i))

(3:2)

and for the jth component of !~ (i)

@

@ !j(i)

log-likelihood

(

B)

=

X
~x2S

xj

1

+

e

1
(!~ (i) ~x+ (i))

N X Pr(~xj
x~2f 1gn

B)xj 1 + e

1
(!~ (i) x~+ (i))

(3:3)

The purpose of the clamped and unclamped phases (also called action and sleep phases) in the

Boltzmann machine learning algorithm is to approximate these two expressions. The rst term in each

expression corresponds to the clamped phase, and the second one to the unclamped, or sleep phase. In

general Boltzmann Machines, this estimation is performed using stochastic methods. However, here the

clamped term is easy to calculate, it requires summing a logistic type function over all training examples.

The same term is obtained by making the mean eld approximation for the clamped phase in the general

algorithm PA87], which is exact in this case. It is more di cult to compute the sleep phase term, as it

is an explicit sum over the entire input space, and within each term of this sum there is an implicit sum

over the entire space of states of hidden units in the factor Pr(~xj B). However, again taking advantage

of the special structure of the combination model, we can reduce this sleep phase gradient term to a sum

only

over

the

states

of

the

hZidBd=en2unnitsX. Rec24aellxEp(qXumathioi n(i()2).Y2n):

cosh(!~ (~h)j

3 )5

:

~h2f0;1gm

i=1

j=1

15

A

similar

derivation

gives that
Pr(~hj B)

=

P
~h0

2f0;e1xgpm(hPexim=p1(Phiim=(1i)h)0iQ(jni=))1Qcojns=h1(c!~o(s~hh)(jw~) (~h0)j

i
)

(3:4)

The second term of the derivative w.r.t. (i) is @=@ (i) ln ZB = (@=@ (i)ZB)=ZB. As (i) appears only once

in ZB, we get that:

@

@
(i)

log-likelihood

(

B)

=

X
x~2S

1

+

e

1
(!~ (i) ~x+ (i))

N X Pr(~hj B)hi :
~h2f0;1gm

(3:5)

Similarly, for each component of w~i, we use the fact that d cosh(t)=dt = tanh(t), to get that

@

@ !j(i)

log-likelihood

(

B)

=

X
~x2S

xj

1

+

e

1
(!~ (i) ~x+ (i))

N X Pr(~hj B)hi tanh(~!(~h)j)
~h2f0;1gm

(3:6)

The formulas for the gradients of the log likelihood for the real-valued model are very similar. A

derivation similar to the one used to derive Equation (3.4), gives us that, for the real-valued model

Pr(~hj

R) =

P~h02f0e;1xgpm(Phexim=p1(Phi mi=(i1)

+ hi0

12(ij)j!~+(~h21)jjjj!~22)(~h)jj22)i

:

(3:7)

Using this equation we get that

@

@
(i)

log-likelihood

(

R)

=

X
~x2S

1

+

e

1
(~!(i) x~+

(i) )

N X Pr(~hj R)hi
~h2f0;1gm

(3:8)

and for each j

@

@ !j(i)

log-likelihood

(

R)

=

X
~x2S

xj

1

+

e

1
(!~ (i) ~x+ (i))

N X Pr(~hj R)hi!~ (~h)j
~h2f0;1gm

(3:9)

Equations (3.5-3.4) are very similar to Equations (3.8-3.7). The di erences are in the partial derivative

of the normalization factors, ZB and ZR, with respect to the weight vectors. Note that the equations for the real-valued model are simpler. As was discussed in Section (2.6), the normalization factors for the

real and binary models are very close to each other when the weight vectors w~i have small l2 norm. Thus although the equations for the maximal likelihood solutions di er, the solution of the real-valued model

are approximate solutions for the binary model and vice versa.

The time required to compute Equations (3.5) and (3.6), (or Equations (3.8) and (3.9) is O(jSjn + 2m).

Thus, if m is small compared with the size of the sample S, then the computation time is linear in the

number of training example and in the size of the input vector, which is reasonable. However, for large m it might not be possible to compute all 2m terms. There is a way to avoid this exponential explosion

if we can assume that a small number of terms dominate the sums. If, for instance, we assume that the

probability that more than k hidden units are active (+1) at the same time is negligibly small we can get a good approximation by computing only O(mk) terms. In the extreme case where we assume that only

one hidden unit is active at a time (i.e. k = 1), the combination model essentially reduces to the standard

mixture model as discussed is Section 2.3. For larger k, this type of assumption provides a middle ground

between the generality of the combination model and the simplicity of the mixture model. In the next

section we show how the gradient of the real-valued model can be approximated when m is large.

16 3. Learning the model from examples

3.1 Approximating the gradient

One possible approach to estimating the gradient when m is large is to search for the larger terms in Equations (3.8,3.9) and ignore the smaller ones. We now show that in the case of the real-valued model the problem of locating the large terms is equivalent to a simple geometric problem. Although this problem is NP-hard in the general case it might typically be easy in the cases that we encounter in real life problems.

Recall Equation (3.7)

P r(~hj

R) =

Ph~02f0e;1xgpm(Phexim=p1(Phi mi=(i1)

+ hi0

1 jj
2
(i)

Pmi=1

+

1 jj
2

Phiwim~=i1jjh220i)w~ ijj22)i

We would like to such that g(~h) = denote a column

vPeescimt=tiom1rhaaitne(diw) ~x+hTic21htjjooPdf eimt=nh1oehteviwe~ictitsjoj22trrsias~hnlsacproogrsere.e,sDip.eeo.nnaderloathwrgeevfoetcleltoromwr.isn.Wgi.eme.dawetreinxwenooutladtiloikne.

to We

nd use

all ~x

~h to

0 = BBB@BB

(w~1)T (w~2)T
...

1 CCACCC

(w~ m)T

Using this notation we de ne g(~h) as

~ = ( 1; 2; : : :; m)T

and rewrite Equation (3.7) as Rearranging g(~h) we get

g(~h) = ~h ~ + 12jj~hT jj22 ;

P r(~hj

R) =

P
h~0

exp(g(~h)) 2f0;1gm exp(g(h~0))

:

g(~h)

=

1 2

jj~hT

+ ~T (

T ) 1 jj22

12~T (

T) 1~ ;

assuming that T is not singular.

The second term is a constant and is eliminated by the normalization. We can therefore ignore it. The

rst term corresponds to the distance between a sum of a subset of the weight vectors and the xed vector ~T ( T) 1 . The problem of nding the settings of ~h for which g(~h) is largest translates to the problem

of nding a subset of a given set of vectors which is furthest away (in the regular Euclidean distance) from

a given xed vector.

It is not clear how hard this computation problem is in the general case. If the vectors are orthogonal then the problem is easy. In this case the set of all 2m vector combinations de nes the corners of a
rectangular box. If the dot product, ~x w~i, is equal to jjw~ijj2=2 for all i, then ~x is in the center of the box. Any deviation from equality for a particular index i determines whether the vector corresponding vector,

3.2. Projection Pursuit methods

17

w~i, is in the subset whose sum is furthest from ~x. In general, one of the closest subset-sums is equal to

~hT , where each coordinate of ~h is de ned by:

hi

=

(

1 0

if w~i ~x

1 2

jjw~ijj2

otherwise

A promising direction for further research is to nd methods that can solve this problem e ciently in the general case. Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that de nes it.

3.2 Projection Pursuit methods

A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique Hub85, FWS84, Fri87]. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. This algorithm is a greedy algorithm that generates the hidden units one by one. It avoids the exponential blowup of the standard gradient ascent technique, and also has that advantage that the number m of hidden units is estimated from the sample as well, rather than being speci ed in advance.

3.3 Overview of Projection Pursuit

Many methods for analyzing high dimensional data study the rst and second order statistics of the data, which are the mean vector and the covariance matrix. Principal components analysis is an example of such a method. Such methods necessarily ignore the structure of the distribution that is not re ected in the rst and second order statistics, which may be an important part. Projection Pursuit methods can sometimes nd this important high-order structure.
The distribution model over Rn with the largest entropy for a given average and covariance is a Gaussian distribution. Thus one natural de nition of the information ignored by the second order analysis is the deviation of the empirical distribution from the Gaussian distribution. Low order linear projections have been traditionally used by researchers in their e orts to understand high dimensional distributions. As all projections of a Gaussian distribution produce a Normal marginal distribution. Thus, if a projection of a distribution generates a marginal distribution that is very di erent from the normal distribution, this is an indication that the projection contains information about the distribution that does not exist in its covariance matrix. Such a projection may be called an \interesting" projection. There are various \projection indices" de ned in the PP literature to measure how interesting a particular projection is, and many of these indices relate directly to the deviation of the marginal distribution from a Normal distribution. Projection Pursuit methods locate the low dimensional projections in which the projection index is largest, i.e. those projections that are most interesting.
Originally, PP was used to suggest projection directions as an aid for the manual exploration of high dimensional data via two or three dimensional projections. Later PP became a complete method for

18 3. Learning the model from examples

statistical data analysis, using repeated search for interesting projections to generate n-dimensional density

estimations. The search for a description of the distribution of a sample in terms of its projections can

be formalized in the context of maximal likelihood density estimation in the following way Fri87]. De ne

p0(~x) to be the initial estimate of the density over Rn, i.e. the Gaussian density with appropriate mean and covariance. De ne G to be a family of functions from R to R and A to be the set of vectors of length 1,

i.e. A = f~ 2 Rnj jj~jj2 = 1g. Using these we de ne the nth order projection estimates to be the following

set of densities

P

P

m

=

(

1 Z

p0(~x)

Ym
i=1

gi(~

(i)

~x)

~(i) 2 A;

gi 2 G;

Z

=

Z
Rn

p0(~x)

Ym
i=1

gi(~ (i)

)
~x)d~x

(3:10)

The log-likelihood of a speci c density p 2 PPm with respect to a sample S = f~x(1); ~x(2); : : :; ~x(N)g, where x~(i) 2 Rn is de ned, in the standard way, to be
LL(pjS) = X ln p(~x) :
~x2S

The goal of Projection Pursuit is to nd a series of approximations:
p1 2 PP1, p2 2 PP2; : : :pm 2 PPm that have maximally increasing log-likelihood. The rst approximation, p0, is the Gaussian density itself, and the (i + 1)-st approximation is generated by adding a factor gi(~(i)) ~x) to the ith approximation. The vector ~(i) is called the ith projection of the data.
The projection index is a function of ~(i) that is a heuristic measure of the anticipated contribution of a factor involving the projection ~(i) to the likelihood of the model. Given a choice of ~(i), the optimal choice of the function gi( ) in terms of maximizing the likelihood is the following FWS84]. De ne pi~(i)(t) to be the marginal density on R generated by projecting the density pi on the direction ~(i). Similarly de ne p^~(i)(t) to be an approximation to the marginal density generated by projecting the true density on the direction ~(i), estimated empirically using the sample S.1 Then the optimal choice for gi( ), in terms of maximizing the likelihood of the model, is

gi(t)

=

p^~ (i) (t) pi~(i 1)(t)

(3:11)

As the optimal choice of gi( ), for a given choice of ~(i) is simple to calculate. The main problem of designing a projection pursuit method is nding a good projection index whose calculation can be performed e ciently. Various projection indices have been discussed in the literature Hub85, Fri87]. Selection of a direction that has a high projection index is usually performed using gradient following methods. After a local maximum of the projection index has been found, the index function is altered to prevent the search from nding the same direction again, and a search for a direction with a high projection index is started from a di erent starting point.

1Note that the marginal density is a one dimensional function, thus the number of samples needed for estimating it is relatively small. In this way projection pursuit avoids, to some degree, the infamous \curse of dimensionality" in the estimation of the distribution of high dimensional data.

3.4. Projection Pursuit and the combination model

19

The search for new projection directions can be simpli ed if instead of altering the projection index function, the sample is altered in a way that previously found interesting projections (~ ;(1) ~ ;(2) : : :; ~(i )1)

are made to appear uninteresting, i.e. Normally distributed. So called \structure removal" methods have

been devised towards this goal Hub85, Fri87]. These methods alter the sample in such a way that a speci c

single projection that has been interesting is made uninteresting while all orthogonal projections are left

unchanged. Put in another way, suppose that some density p 2 PPm has high likelihood with respect to a

given sample, and that one of the factors in p is g1(~(1) ~x). Then removing the structure corresponding

to

g ~( (1) 1

~x) means transforming the sample into a sample for which p(~x)=g1(~(1)

~x), which is a model

in PPm 1, has high likelihood.

To summarize, most iterative projection pursuit methods share the following common structure:

Initialization

Set S0 to be the input sample. Set p0 to be the initial density (Gaussian).

Iteration

Repeat the following steps for i = 1; 2 : : : until all projections of Si are almost Normal. 1. Find a direction ~(i) for which the projection index of the projection of Si 1 is maximized.

2. Approximate the actual marginal density in the direction ~(i) by nding a close t to the density

of the projection of the sample Si 1. Set gi( ) to be the ratio between this approximation and the marginal density produced on ~(i) by pi 1, using Equation (3.11).

3. Set Si to be Si 1 with the structure de ned by the factor gi(~(i) ~x) removed. This makes the projection of Si on ~(i) uninteresting, and all of the orthogonal projections remain equal to that of Si 1.

Notice

4. Set pi(~x) to be pi that in this method

1(~x)gi(~(i) ~x). the functions gi

are

chosen

in

such

a

way

that

the

product

Qmi=1

gi(~ (i)

~x)

is normalized for each m and there is no need for an additional normalization term Z, as appears in the

de nition of PPm in Equation (3.10).

Projection Pursuit has proved itself successful in some experiments Fri87]. However, the search for

best density is performed in a greedy manner and might not succeed in nding the optimal density in
PPm. While there is quite a large body of research on the representational power of projection pursuit
models, little is theoretically known about reliability of the associated learning algorithms, such as the one

presented above.

3.4 Projection Pursuit and the combination model

Recall Equation (2.6), which describes the density generated by the real-valued combination model:

p(~x) = e

Y1
2

jjx~jj22

1 ZR

m i=1

1 + e i+w~i x~

:

20 3. Learning the model from examples

Using the following de nitions we see that this class of models is a special case of the class of models

presented in Equation (3.10).

p0(~x) = (2

)

n=2e

1 2

jj~xjj22

=

N

(0;

1)

~ (i)

=

w~ i jjw~ijj2

G=

g

:

R

!

R

j

g(t)

=

1 Z

1 + e i+tjjw~ijj2

;Z 2R

It is clear that, under these de nitions, p(~x) is a function in PPm. In the next section we present a greedy
algorithm for learning the combination model that is based on this relation.

A similar relationship holds for the binary model. However, we have not managed to nd a good

structure removal procedure for the binary-valued model. We thus present an algorithm for learning the

real-valued model and, based on the relations given in Section 2.6, we claim that the solutions that we nd

for the real-valued model are approximate solutions for the binary-valued model.

There are two main di erences between our work and previous work on using exploratory projection

pursuit algorithms for estimating distributions. The rst di erence is that while our model de nes a
distribution on all Rn, our data-points are taken from f 1; +1gn. However, as discussed at the end of

Section 2.6, the projections of the binary vectors generate marginal distributions that are close to Normal,

similarly to the distributions we expect from real-valued data.

The second di erence is that the family of functions G from which the gis are taken is a very restricted set of functions. This is unlike standard PP techniques, in which the functions gi are chosen from some very broad family, such as some family of spline functions. This means that, in our case, any single function g 2 G might be far from adequate for describing the marginal distribution on some direction ~(i) and several factors with the same ~ might be needed. This, in turn, has the e ect that eliminating

the structure generated by a single factor does not amount to transforming the marginal distribution on

the corresponding projection so that it becomes completely uninteresting. As most structure elimination

techniques do exactly that, they are un t in the context of learning the combination model.

3.5 PP algorithm for learning the combination model
In this section we present a variant of PP that is a learning algorithm for the combination model. Our algorithm combines the search for an interesting projection direction, ~, with the search for the corresponding projection function, g( ). The algorithm searches for the optimal factor by maximizing the likelihood of a single factor model with respect to the (possibly altered) sample. After such a factor is found, the algorithm alters the examples in such a way that the structure encoded in the factor is eliminated, and subsequent searches will nd di erent factors.
The algorithm is thus based on two elements. The rst element is a method for nding a maximal likelihood combination model with a single hidden unit. This method serves both for nding a projection direction, and for nding the function gi( ) associated with this direction. The second element is a structure removal procedure. We shall describe the two elements in turn.

3.5. PP algorithm for learning the combination model

21

We have previously described how gradient ascent can be used for nding model with highest loglikelihood. However, for the special case where there is only a single hidden unit in the model, a much

faster method can be used. This method is an Expectation-Maximization (EM) method DLR77]. EM is a general method for estimating the parameters of distribution models that have both observable and unobservable random variables. This method achieves extremely fast convergence when used for estimating a mixture of product distributions.2

The Expectation Maximization method is based on iterative improvement of the estimates of the

maximal likelihood values of the model parameters. It starts with some initial guess of the parameters init, and proceeds by iterating the following two steps. It can be shown DLR77], that each of these iterations improves the likelihood of the parameters.

1. Using the old setting of the parameters, old, as if they were the actual parameters, some statistics of the joint distribution of the hidden and the observable variables are calculated.

2. The old setting of the parameters, old, is replaced with a new setting of the parameters new, which is the most likely setting of the parameters given the values of the statistics calculated in step

1. These new parameters are used as the old parameters in the following iteration.

To see how this method is implemented for the problem of estimating the parameters of a real-

valued combination model with a single hidden unit let us calculate the maximal likelihood setting of

the parameters assuming that we are given a sample S0, of size N, in which each element describes the

value of both the observable random variables, ~x, and the unobservable random variable h. The log

likelihood is

LL(

; ~!jS0) = X ln P(~x; hj
(h;x~)2S0

;

~!)

=

X
(h;~x)2S

0

ln

exp

(h(

+ ZR

~!

~x))

= X h( + !~ ~x) N(2 )n=2 ln 1 + exp
(h;x~)2S0

+

1 2

jj~!jj22

Taking the derivative of the log-likelihood with respect to the parameters and equating to zero to nd the

optimal setting of the parameters, we get the following equations. From the derivative w.r.t. we get that

X h = N logistic
(h;x~)2S

opt

+

1 2

jj!~ optjj22

;

(3:12)

and from the gradient w.r.t. ~! we get that

X h~x = ~!optN logistic
(h;x~)2S

opt

+

1 2

jj!~ optjj22

(3:13)

Notice that if we divide the sums on the left hand side of Equations (3.12) and (3.13) by N, we get the de nition of the empirical estimates of E(h) and of E(h~x), which we shall denote by E^(h) and E^(h~x). Solving Equations (3.12) and (3.13) for the values of the optimal parameters, we get that:

~!opt

=

E^(h~x) E^(h)

;

(3:14)

2It is not easy to implement EM directly on the complete combination model, because although this distribution can be expressed as a mixture of product distributions, the parameters that de ne the mixture components are coupled.

22 3. Learning the model from examples

and

opt =

ln

1

E^(h) E^(h)

1 2

jj!~ optjj22

:

(3:15)

We thus see that the statistics that we need to estimate in the rst step of the EM iteration are E^(h) and E^(h). These statistics can be directly calculated from the sample S0, as this sample includes both x~ and h. However, given a setting of the parameters, we can compute the distribution of h for any setting of ~x,
and thus calculate the desired statistics.

The implementation of the EM method for the combination model with a single hidden unit is thus as follows. We start with an initial setting of the parameters: (~!init; init) and proceeds by iterating the
following two steps on the given sample S = h~x1; ~x2; : : :; ~xNi
1. In the Expectation calculation step the current parameters (~!old; old) are used as if they describe the correct input distribution. Given this description and a particular setting of the input units, x~, we can compute probability that each hidden unit is 0 or 1 given any setting of the observable vector ~x:
Pr(hi = 1j~x; !~old; old) = logistic(~!old ~x + old)~x :

Using this equation and the sample S, it is possible to compute the following estimates:

E^(h~x)

=

1 N

X
x~2S

logistic(!~ old

~x +

old)~x

E^(h

=

1)

=

1 N

X
~x2S

logistic(!~ old

~x +

old)

2. In the Maximization step, new parameters (~!new; new) are calculated using Equations (3.14) and (3.15). The new parameters (!~new; new) are used as the old parameters (~!old; old) in the following iteration.
3. The iteration terminates when the di erence between (!~new; new) and (~!old; old) becomes insigni cant.
We now present the structure removal procedure. In the analysis of the real-valued model in Section (2.3) we have shown that the addition of a hidden variable has the e ect of replacing the previous distribution by a mixture of two distributions, the rst of which is equivalent to the previous, and the second is a shifted copy of the previous distribution, shifted by the weight vector w~i that corresponds to the hidden unit. The shifted copy corresponds to the case in which hi = 1 while the unshifted one correspond to the case where hi = 0. For each data point we compute the probability, p, that hi = 1. We then ip a random coin whose bias is p and, according to the outcome of the coin ip, either keep the example as it is or subtract w~i from it. This has the e ect of shifting the shifted copy, which corresponds to hi = 1 to coincide with the unshifted copy, which corresponds to hi = 0. In this way the structure encoded by the

3.5. PP algorithm for learning the combination model

23

hidden unit is eliminated from the empirical distribution. Details are described below.
Initialization
Set S0 to be the input sample. Set p0 to be the initial distribution (Gaussian).
Iteration
Repeat the following steps for i = 1; 2 : : : until no single-variable combination model has a signi cantly higher likelihood than the Gaussian distribution with respect to Si.
1. Perform an EM procedure to maximize the log-likelihood of a single hidden variable model on the sample Si 1. Denote by i and w~i the parameters found by this procedure, and create a new hidden unit with associated binary random variable hi with these weights and bias.
2. Transform Si 1 into Si using the following structure removal procedure. For each example x~ 2 Si 1 compute the probability that the hidden variable hi found in the last step is 1 on this input:

P (hi = 1) = 1 + e ( i+w~i x~) 1

Flip a coin that has probability of \head" equal to P(hi = 1). If the coin turns out \head" then

3.

add ~x w~i to Si else add x~ to Si. Set pi(~x) to be pi 1(~x)Zi 1 1 + e i+w~i x~

,

where

Zi

=

P
x~

pi

1(~x)

1 + e i+w~i x~

.

24
4. Experimental work

4. Experimental work

We have carried out several experiments to test the performance of unsupervised learning using the

combination model. The goals of these experiments is to show that the combination model is a useful one

and to compare the performance of the di erent learning algorithm that we have developed.

The rst set of experiments compares the two learning methods for the combination model presented

in this paper. The rst is the gradient ascent method, and the second is the projection pursuit method.

The experiments in this set were performed on synthetically generated data. The input consisted of 4,000

binary vectors of 64 bits that represent 8 8 binary images. The binary vectors are synthesized using

a combination model with 10 hidden units whose weights were set as in Figure (4.1,a). Each square in

this image denotes a single real valued parameter,1 the matrix corresponds to the weight vector, and the

rectangle above the matrix corresponds to the bias parameter . We shall refer to each random binary

vector as an instance.

The ultimate goal of the learning algorithms was to retrieve the model that generated the instances,

which we call the \target" model. However, this goal is generally not achievable. The rst reason is that

the optimal model is not unique, i.e. there usually are other combination models that generate the exact

same distribution as the target model, or a distribution that is very close to it. For example, a permutation

of the hidden units does not change the distribution de ned by the model. As we have found out in the

experiments, other simple transformations of the target model produce models that are almost as good as

the target model. Another reason that we cannot retrieve the exact target is that the parameter vector of

the target is real valued, and thus cannot be exactly identi ed by a nite number of instances. The third

reason is that our algorithms are not guaranteed to nd the optimal model for the given data. The gradient

ascent algorithm is only guaranteed to locate a local maximum of the likelihood, and the Projection Pursuit

algorithm is only guaranteed to increase the likelihood of the model with each additional hidden unit.

While the di erence between the parameter vectors of the learned model and of the target model is

usually large, their performance as models of the random instances is similar. We measure this performance

using three di erent error measures. Each error measure de nes a way of computing the error of a

combination model with respect to a set of instances. We have measured these errors for the target

model and for each of the learned models. Each measurement was taken both with respect to the instances

that were used for learning (the \training" instances) and with respect to an independent test set of 4000

instances.

We now describe each of the three measures of error that we have used:

Average log-loss

Each learned distribution model de nes a probability distribution, P, on the space of images. A

ipsodpeulanredmeaassurPe oxf(Qth(ex)dliostgaPnc(ex)b)e.twTeheencProsasndentthroepayctiusaml dinisimtriizbeudtiownheQn

is P

the cross entropy, = Q, and is then

which equal

to the entropy of Q. The cross entropy can be estimated by taking the average value of minus log

1The results are given using Hinton diagrams RM86], i.e. positive values are displayed as full rectangles, negative values as empty rectangles, and the area of the rectangle is proportional to the absolute value.

25

of the probability that the model assigns to each instance in the sample. This measure of error is also called the log-loss error. We scale the error so that the uniform distribution model, that assigns equal probability to all instances, has an expected error of 1. The log-loss error is hard to compute for large combination models, which is why we use it only in the experiments on synthetic data in which we use only 10 hidden units in the models.

Single bit completion We estimate the average number of mistakes made by the model when it is used to predict the value of single bits of the instances. More precisely, the mistakes it makes when used to predict the value of each single bit in each of the instances in the sample, when given the values of all the other bits of that instance. The combination model de nes a probability for any possible instance. The prediction is de ned as the value of the bit that corresponds to the more probable instance. We estimate this average number by choosing at random 5 bit locations for each instance in the sample.

Input reconstruction
We estimate the quality of the combination model as an input representation scheme. For each instance (x1; : : :; xn) we compute the most probable state of the hidden units. This state can be seen as an encoding of the instance. One way of de ning the quality of this encoding scheme is to measure how much additional information is required to reconstruct the instance from the state of the hidden units alone. Each state of the hidden units de ned a Bernoulli product distribution over the images.
The additional information that is required to encode a particular instance is the log of one over the probability assigned to the instance. As the distribution is a Bernoulli product, this can be written as the following sum:

H(~xj~h)

=

1 2

Xn
i=1

(1

+

xi)

log2

pi

+

(1

xi) log2(1

pi)] ;

where pi is the independent probability of the ith input bit to be +1 given the hidden state, which

is equal to

pi

=

logistic

@0Xm

!i(j)

hj

1 A

j=1

This measure of error is scaled so that it measures the additional information that is required per input bit.
All experiments used a test set and a separate training set, each containing 4000 examples. The gradient ascent method is based on the binary distribution model. It typically needed about 1000 epochs to stabilize.2 In the projection pursuit algorithm, 4 iterations of EM per hidden unit proved su cient to nd a stable solution. The results are summarized in the following table and in Figure (4.1).3

2The algorithm used a standard momentum term (see HKP91], page 123) to accelerate the convergence.
3The di erence between the measurements of the quality of the true model on the test set and on the training set are due to the random uctuations between the two sets of examples. These di erences provide an indication of the accuracy of our measurements.

26 4. Experimental work

gradient ascent for 1000 epochs projection pursuit
Projection pursuit followed by gradient ascent for 100 epochs Projection pursuit followed by gradient ascent for 1000 epochs true model

log-loss train test 0.399 0.425 0.893 0.993
0.411 0.430
0.377 0.405 0.401 0.396

single bit prediction train test 0.098 0.100 0.119 0.114
0.091 0.089
0.071 0.082 0.077 0.071

input reconstruction train test 0.311 0.338 0.475 0.480
0.315 0.334
0.261 0.287 0.286 0.283

(a)

(b)

(c)

(d)
Figure 4.1: The weight vectors of the models in the synthetic data experiments. Each matrix represents the 64 weights of one hidden unit. The range of the weights is 6; +6] with the large white squares representing the value 6. The square above the matrix represents the units bias. positive weights are displayed as full squares and negative weights as empty squares, the area of the square is proportional to the absolute value of the weight. (a) The weights in the model used for generating the data. (b) The weights in the model found by gradient ascent alone. (c) The weights in the model found by projection pursuit alone. (d) The weights in the model found by projection pursuit followed by gradient ascent. For this last model we also show the histograms of the projection of the examples on the directions de ned by those weight vectors; the bimodality expected from projection pursuit analysis is evident.
The best learning result was achieved by starting with the projection pursuit algorithm then using the parameter vector that was learned as a starting point for the gradient ascent algorithm. The nal result of this combination is presented in Figure 4.1(d), together with the corresponding projections of the data

27

along the directions de ned by the weight vectors. We can see that there is a close correspondence between

the weight vectors in the learned model and the vectors in the target model described in Figure 4.1(a).

Counting from left to right, the weight vectors of units 1,2,8,9, and 10 in the learned model are almost

identical to the weight vectors of units 1,4,6,7,and 5 in the target model. Units 3 and 7 in the learned model

are close to the negation of units 8 and 3 in the target model, and units 4 and 5 in the learned model are

combinations of units (10,2) and (9,2) of the target model respectively. There is no exact correspondence of

the biases. As we see from the table, the performance of the learned model is almost as good as that of the

target model according to all three measures. We thus conclude that reversing the sign of weight vectors

and combining them can sometimes create a di erent combination model whose corresponding distribution

is very similar.

When the gradient ascent model is used to learn by itself (Figure 4.1(b)),it tends to get stuck in local

minima, as can be seen in the table. It is also a very slow method, both because of the large number of

iterations that is required and because each iteration requires complex calculations. The fact that the local

search process is stuck in a sub-optimal solution can be seen in the weight vectors of the learned model in

that four of the weight vectors (those of units 1,2,6,10, counting from the left) have no clear correspondence

to any of the weight vectors in the target model.

The Projection Pursuit method is very fast, but its results are weaker than those of the gradient ascent

method by itself. It tends to nd a model whose weight vectors correspond to various combinations of

the weight vectors of the target model and their negations. The performance of the results of projection

pursuit are similar to those of the gradient method in the single bit prediction measure and in the input

reconstruction measure. On the other hand, the performance of the Projection pursuit model in terms

of the likelihood of the model that it generates is very poor. The reason is that the data that we use is

generated by a binary valued combination model, while the projection pursuit model is based on a real

valued combination model. The di erence between these two models is large, because the weights that are used in the target model are in the range 6; +6]. As we have shown in Section 2.6, the binary model

and the real valued model are approximately equal when the weights are small. To show that this is

indeed the source of the error, we repeated the previous experiments using a target model with the weight

vectors divided by a factor of 7, so that now all the weights are in the range 6=7; +6=7]. The results are

summarized in the following table

log-loss single bit prediction input reconstruction

train test train test train test

True Model

0.939 0.941 0.36 0.36 0.86 0.87

gradient ascent for 400 epochs

0.937 0.944 0.36 0.37 0.86 0.87

projection pursuit

0.964 0.966 0.38 0.39 0.92 0.92

Projection pursuit followed by

gradient ascent for 400 epochs 0.935 0.943 0.36 0.37 0.86 0.87

We see that in this case, the likelihood of the model found by the projection pursuit algorithm is similar

28 4. Experimental work
to that of the other models. Because in this case the weights are so small, the di erence between the distribution de ned by the model and the uniform distribution is small, as is re ected in the measures of accuracy. However, the di erence from the uniform distribution is statistically signi cant. The combination of the two learning algorithms was able to retrieve the weights of the target model almost as well as in the previous experiment (see Figure 4.2).
(a)
(b)
(c)
Figure 4.2: The weight vectors of the models in the synthetic data experiments. The target target is the same as in the previous experiment but the range of the weights is divided by a factor of 7, so that the largest white squares represent the value of 6=7. (a) The weights in the model found by gradient ascent alone. (b) The weights in the model found by projection pursuit alone. (c) The weights in the model found by projection pursuit followed by gradient ascent.
In the second set of experiments we compare the performance of the combination model to that of the mixture model. The comparison uses real world data extracted from the NIST handwritten data base.4 Examples are 16 16 binary images (see Figure (4.3)). There are 500 examples in the training set and 500 in the test set. We use 45 hidden units to model the distribution in both of the models. Because of the large number of hidden units we cannot use gradient ascent learning and instead use projection pursuit. For the same reason it was not possible to compute the likelihood of the combination model and only the other two measures of error were used. Each test was run several times to estimate the accuracy of our measurements.
For learning a mixture model we use an incremental version of EM. We start with a model with a single Bernoulli product distribution and run EM until the method converges. We then take a mixture of two Bernoulli product distributions, each of which is initialized to be a slight random perturbation of the single Bernoulli product. We then let EM run on this model until it converges, and then we split each component into two in a similar way. Continuing in this fashion we repeatedly double the size of the model.5
The nal errors of many runs of these algorithms, starting from di erent initial weights, are summarized in the table below. The errors of two representative runs are given in Figures 4.6 and 4.7. A sample of the nal weight vectors of the learned combination model and mixture model are given in Figures 4.4 and 4.5
4NIST Special Database 1, HWDB Rel1-1.1, May 1990. 5When 32 units are to be split, only the rst 13 of them are split, to give the nal number of 45 mixture components.

29

respectively. A complete list of all of the 45 weight vectors for each model are given in Figures 4.8 and

4.9.

single bit prediction

input reconstruction

train test train test

Product distribution 0.29 0.01 0.30 0.01 0.78 0.01 0.80 0.01

Mixture model

0.19 0.01 0.26 0.01 0.55 0.01 0.70 0.01

combination model 0.19 0.01 0.20 0.01 0.60 0.01 0.64 0.01

The rst line in this table, named \Product distribution" summarizes the performance of a simple dis-

tribution model that assumes that the pixels are distributed according to a Bernoulli product distribution.

The reconstruction of the input, in this case, is simply the xed reconstruction in which each bit is set to

its more probable value. The performance of this model provides a baseline with respect to which we can

compare the performance of the other distribution models whose goal is to capture dependencies between

the pixels. We see that the performance of the combination model is signi cantly better than that of the

mixture model on the test set. The di erence is especially signi cant when compared to the baseline of

the Product distribution model. Also, we see that the di erence between the performance on the test set

and on the training set, i.e. the over- tting, is much smaller for the combination model.

A qualitative comparison between the weight vectors found by the two models con rms the expected

advantage of the combination model in describing combinations of correlations. While the typical weight

vectors of the mixture model (see Figure (4.5)), which is a sample out of Figure (4.8)) look very much like

an average prototype of a speci c digit, the weight vectors of the combination model relate to more local

features, such as lines and curves (see Figure (4.4)), which is a sample out of Figure (4.9)). This relates to

fact that the mixture model relates each example with the single weight vector that is most similar to it,

while the combination model relates each example with a combination of its weights.

Figure 4.3: A few examples from the handwritten digits sample.
As the experiments on synthetic data have shown that PP does not reach optimal solutions by itself we expect the advantage of the combination model over the mixture model to increase further by using improved learning methods. Of course, the combination model is a very general distribution model and is not speci cally tuned to the domain of handwritten digit images, thus it cannot be compared to models speci cally developed to capture structures in this domain. However, the experimental results support our claim that the combination model is a simple and tractable mathematical model for describing distributions in which several correlation patterns combine to generate each instance.

30 4. Experimental work Figure 4.4: Typical weight vectors found by the combination model
Figure 4.5: Typical weight vectors found by the mixture model

31
Figure 4.6: A comparison of the input reconstruction error on 16 16 pixel digit images. This error measures the average amount of additional information that is required for reconstructing the input from the state of the hidden units. The information is measured in bits per pixel. The higher and lower curves in each graph describe the error on the test set and on the training set respectively. The graph on the left describes the error of the mixture model as a function of the number of training iterations (epochs). The number of mixture components is doubled every 20 iterations. There is a spike in the error immediately following the doubling, as a result of the added randomization. The graph on the right describes the error of the combination model as a function of the number of iterations. (The spike in the graph around iteration 230 is a side e ect of a \back tting" stage that has not proven to be useful.)

32 4. Experimental work
Figure 4.7: A comparison of the single bit completion error on 16 16 pixel digit images. The error measures the probability of a mistake in predicting a random single missing bit in the image, using the distribution model and the values of all the rest of the pixels. The higher and lower curves in each graph describe the error on the test set and on the training set respectively. The graph on the left describes the error of the mixture model as a function of the number of training iterations (epochs). The number of mixture components is doubled every 20 iterations. The graph on the right describes the error of the combination model as a function of the number of iterations. (The peak in the graph around iteration 230 is a side e ect of a \back tting" stage that has not proven to be useful.)

33 Figure 4.8: The weight vector, or image templates, found by the the mixture model

34 4. Experimental work

References
References

35

AHS85] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive Science, 9:147{169, 1985.
CS89] D. R. Cox and E. J. Snell. Analysis of binary data. Chapman and Hall, 1989.
DF84] P. Diaconis and D. Freedman. Asymptotics of graphical projection pursuit. Annals of Statistics, 12:793{815, 1984.
DH73] R. O. Duda and P. E. Hart. Pattern Classi cation and Scene Analysis. Wiley, 1973.
DLR77] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Roy. Statist. Soc. B, 39:1{38, 1977.
EH81] B.S. Everitt and D.J. Hand. Finite mixture distributions. Chapman and Hall, 1981.
Fre87] D. H. Freeman. Applied Catagorical Data Analysis. Marcel Dekker, 1987.
Fri87] J. H. Friedman. Exploratory projection pursuit. J. Amer. Stat.Assoc., 82(397):599{608, March 1987.
FWS84] J. H. Friedman, W.Stuetzle, and A. Schroeder. Projection pursuit density estimation. J. Amer. Stat.Assoc., 79:599{608, 1984.
Gem86] Stuart Geman. Stochastic relaxation methods for image restoration and expert systems. In D.B. Cooper, R.L.Launer, and D.E. McClure, editors, Automated Image Analysis: Theory and Experiments. Academic Press, 1986.
GG84] S Geman and D Geman. Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 6:721{742, 1984.
GP87] Hector Gefner and Judea Pearl. On the probabilistic semantics of connectionist networks. Technical Report CSD-870033, UCLA Computer Science Department, July 1987.
HKP91] John Hertz, Anders Krogh, and Richard G. Palmer. Introduction To The Theory Of Neural Computation. Addison Wesley, 1991.
Hop82] J.J. Hop eld. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad Sci. USA, 79:2554{2558, April 1982.
Hub85] P.J. Huber. Projection pursuit (with discussion). Ann. Stat., 13:435{525, 1985.
Jol86] I.T. Jolli e. Principle Component Analysis. New York: Springer-Verlag, 1986.
Nea90] Radford M. Neal. Learning stochastic feedforward networks. Technical report, Department of Computer Science, University of Toronto, November 1990.
Now90] S. Nowlan. Maximum likelihood competitive learning. In D. Touretsky, editor, Advances in Neural Information Processing Systems, volume 2, pages 574{582. Morgan Kaufmann, 1990.
Oja89] E. Oja. Neural networks, principle components, and subspaces. Int. J. Neural Systems, 1(1):61{68, 1989.
PA87] Carsten Peterson and James R. Anderson. A mean eld theory learning algorithm for neural networks. Complex Systems, 1:995{1019, 1987.

36 References
Pea88] Judea Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988. RM86] D. E. Rumelhart and J. L. McClelland. Parallel Distributed Processing: Explorations in the
Microstructure of Cognition. Volume 1: Foundations. MIT Press, Cambridge, Mass., 1986. San89] T.D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network.
Neural Networks, 2:459{473, 1989. Ser80] R. J. Ser ing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 1980.

37
Appendix A. Projection distributions of the binary combination model.

In this section we use results from DF84] to show that the projections of the binary combination

model are very similar to those of the real-valued combination model model when the weight vectors are

small. As has been discussed in Section (2.3), the binary combination model distribution can be viewed as a mixture of 2m generalized binomial distributions. We call these binomial distributions binoms. Each binom corresponds to a particular setting of the hidden vector ~h and to a single Gaussian component in

the real-valued model. We shall show that although the distribution of the binoms are very di erent from

the corresponding Gaussians, their projections onto almost any direction are very similar. This implies

that the projections of the binary-valued combination model are very similar to those of the real-valued

combination model. Because Projection pursuit methods depend only on properties of the projections

of the distribution, it is a valid approximation to use the real-valued combination model for learning

distributions generated by a binary-valued combination model.

The mixture coe binom corresponding

cients to ~h is

of the (~hi) =

btiannohm(Ps ami=r1e

P r(~hj hi!(i))

to each component of ~x. If the weight vectors !(i) are

) as de ned in Equation (3.4).
awllhesmreabllytthaennht(~axn)hw(ePdmie=n1ohtie!t(hi)e)

The mean of the

appPliimc=a1tihoin!

of tanh (i), and

we get that the means of the binoms are very close to the means of the corresponding Gaussians. Next we

show that under mild assumptions, the projection of each binom is very close to a Gaussian.

Diaconis and Freedman DF84] discuss conditions under which most projections of high-dimensional

data sets are close to Gaussian. Their analysis considers large sets of points taken from high dimensional

spaces. These points are not assumed to be generated by a distribution. Instead, the conditions for

Gaussianity of the projection are given as geometric relations among the points. These relations must hold

in the limit where both the dimension of the space and the size of the sets tends to in nity. We shall

show that if the weight vectors of the combination model are generated by some distribution then, with

high probability, samples generated by each binom have the required geometric properties and thus most

of their projections are close to Gaussians.

We follow most of the notation used in DF84]. Let ~x1; ~x2; : : :; ~xN be vectors in Rn, this is the data set. Suppose that n; N and the data set all depend on some common index , and that as tends to in nity, so do n and N. Let Sn 1 be the unit sphere in Rn and let be chosen uniformly at random from Sn 1. Theorem 1.1 in DF84] states that if the following conditions hold, then the empirical distribution of x~i converges weakly to the normal distribution N (0; 2) in probability, as ! 1. Where \weak convergence" is convergence as a measure on R and \in probability" is w.r.t. the uniform distributions on

Sn 1.

The required conditions follow. There must exist some nite and positive 2 such that for any positive

, the following limits hold as tends to in nity,

f1

j

N

:

jk~xj

k2 2

2nj > ng =N ! 0

j f1 j; k N : j~xj x~kj > ngj =N2 ! 0

(A:1) (A:2)

38 Appendix A. Projection distributions of the binary combination model.

Where # denotes the cardinality of a set. The rst condition intuitively means that vectors are almost all of almost the same length. The second condition means that most pairs of vectors are close to orthogonal.
We are interested in projections of samples generated by the combination model, as these are random samples, we would like to show that the geometric conditions hold with probability one. Suppose we have a sequence of binomial distributions over binary cubes of increasing dimension:
f 1; +1g; f 1; +1g2; : : :; f 1; +1gn; : : :. Each distribution is fully speci ed by its mean vector: ~1 2 1; +1];~2 2 1; +1]2; : : :; ~n 2 1; +1]n; : : :. Suppose that we have a sample from each distribution and that the sample size increases with the dimension n of the space: h~x11i; h~x21; ~x22i; ; h~xn1; : : :; ~xnni; .
We would like to show that random projections of these samples produce empirical marginal distributions
that are very close to Gaussian distributions with a probability that goes to 1 as n ! 1. However,
it is not hard to construct sequences of mean vectors such that this will not happen. For instance,
if ~ = f0; +1; : : :; +1g, then the distribution is concentrated in the two points f 1; +1; : : :; +1g, and f+1; +1; : : :; +1g, and all projections of this distribution will also be concentrated on two points.
We prove that the desired asymptotic conditions hold with probability 1 if the mean vectors ~n are selected in the following way. Assume there is some distribution P on 1; +1] and that each component of each ~n is drawn independently at random from this distribution. For this to hold for the mixture components of the combination model it is enough to assume that the components of the weight vectors in the model underlying the data are chosen independently at random.
Theorem A.0.1: Suppose that a sequence of vectors of increasing dimension:

~1 2 1; +1];~ 2 2 1; +1]2; : : :; ~n 2 1; +1]n; : : :

is randomly drawn by selecting each component of each vector according to some distribution P over 1; +1]. Each vector ~n de nes a distribution over f 1; +1gn in which the components are independent and the
expected value is ~n. Suppose that for each n we draw n vectors from this distribution, and that from each random vector we subtract the mean, ~n.
Suppose that for each n we draw a vector w~ uniformly at random from the n dimensional unit sphere, project the n random vectors on the direction de ned by w~ and assign each of the points in the projection a probability mass of 1=n. In this way we create, for each n, a discrete distribution over the reals.
With probability one, over all the random choices that create the sequence of distributions, there exists
0 such that the sequence of distributions converges weakly to the normal distribution N (0; 2).1
Proof: We prove the theorem by showing that the conditions of Theorem 1.1 in DF84] hold with
probability one.
The proof of the condition A.1 is a simple application of the Markov bound. We wish to show that for some and for any ; > 0:

nl!im1 P(#f1

j

n

:

jk~xj

k2 2

2nj > ng > n) = 0

1Weak convergence means that for any measurable set A, the probability assigned to A by the sequence of distributions
converges to the probability of the limit distribution.

39

The n examples are independent, thus as n increases the fraction of the vectors that obey the condition

becomes very close to the probability of obeying the condition. Thus it su ces to show that for a randomly

chosen example ~x

nl!im1 P ( k~xk22 2nj > n) = 0

the squared length of a vector is a sum of the squares of its components. As the components are chosen

independently at random independently at random The variance of each term

according according is at most

to the mean vector ~n and as the components of

to 1.

P we get that Thus de ning 2

tthoebaev1eragRe+11lexn2gdtPh

of (x)

~x is and

n(1 using

~Rn+11axre2dcPh(oxs)e)n.
Markov bounds

we get that

P ( k~xk22

2nj > n)

(

n n)2

=

1 n2

:

and as n increases the probability decreases to zero as desired.

The proof of condition (A.2) is a bit more involved, because in this case the n2 pairs that are checked

for the condition are not independent. However, using the theory of U-statistics Ser80] Chap. 5] their

behavior can be related to that of independently drawn pairs. We wish to show that for any ; > 0:

nl!im1 P(#f1 j; k N : j~xj x~kj > ng > n2) = 0
rst observe that when j = k the condition will most often not hold, as we have just proved that the squared length of a vector is concentrated around 2n. However we can ignore this set as it is a vanishing fraction of the n2 pairs. It is thus su cient to prove that

nl!im1 P(#f1 j; k n; j =6 k : j~xj ~xkj > ng > n(n 1)) = 0

Using the notation of Ser80] we de ne

h(~x; ~y)

=

(

1 0

if j~x ~yj > n
otherwise

and observe the corresponding U-statistic, that is a random variable de ned over samples of size n:

U

(~x1;

:

:

:;

~xn)

=

2 n(n

1) 1

X
i<j

h(~xi; ~xj)
n

This random variable is exactly the cardinality of the set of pairs that have a dot product larger than n divided by n(n 1). Our goal is thus reduced to proving that the probability of a sample for which U is too large is small. We do that by using Markov inequality. The fact that U is an unbiased statistic means that the average of U is equal to the average of h(~x; ~y) when x~ and ~y are chosen independently at random.
In other words it is equal to the probability that two randomly chosen vectors have a dot product larger than n. We shall denote that probability by t. The variance of U can be related to the variance of h(~x; ~y)
by using Lemma A. from page 183 of Ser80].

V ar(U(~x1; : : :; ~xn))

2 n(n

1)

2(n

2) 1 + 2]

4 n2

40 Appendix A. Projection distributions of the binary combination model.

Where 2 is simply the variance of h(~x; ~y) when x~ and y~ are chosen independently at random. As h(~x; ~y) is either 0 or 1, its variance is t(1 t). Putting the bound on the variance into the Markov bound we get:

P n(n 1)U(~x1; : : :; ~xn) > (n(n 1))] P jU(~x1; : : :; ~xn) tj >

t]

4 t(1 t) n ( t)2

It is easy to see that

t = P(j~x ~yj > n)

4 2n

thus limn!1 t = 0 and we get that the desired probability goes to zero, which completes the proof.

