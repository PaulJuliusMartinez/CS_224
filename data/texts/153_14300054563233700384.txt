Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis

Alexei Vinokourov John Shawe-Taylor Dept. Computer Science Royal Holloway, University of London Egham, Surrey, UK, TW20 0EX alexei@cs.rhul.ac.uk
john@cs.rhul.ac.uk

Nello Cristianini Dept. Statistics UC Davis, Berkeley, US
nello@support-vector.net

Abstract
The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short English document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a standard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the first and in the second space that are maximally correlated. Since we assume the two representations are completely independent apart from the semantic content, any correlation between them should reflect some semantic similarity. Certain patterns of English words that relate to a specific meaning should correlate with certain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we first demonstrate that the correlations detected between the two versions of the corpus are significantly higher than random, and hence that a representation based on such features does capture statistical patterns that should reflect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and significantly superior to LSI on the same data.
1 Introduction
Most text retrieval or categorization methods depend on exact matches between words. Such methods will, however, fail to recognize relevant documents that do not share words with a users' queries. One reason for this is that the standard representation models (e.g. boolean, standard vector, probabilistic) treat words as if they are independent, although it is clear that they are not. A central problem in this field is to automatically model term-

term semantic interrelationships, in a way to improve retrieval, and possibly to do so in an unsupervised way or with a minimal amount of supervision. For example latent semantic indexing (LSI) has been used to extract information about co-occurrence of terms in the same documents, an indicator of semantic relations, and this is achieved by singular value decomposition (SVD) of the term-document matrix. The LSI method has also been adapted to deal with the important problem of cross-language retrieval, where a query in a language is used to retrieve documents in a different language. Using a paired corpus (a set of pairs of documents, each pair being formed by two versions of the same text in two different languages), after merging each pair into a single 'document', we can interpret frequent co-occurrence of two terms in the same document as an indication of cross-linguistic correlation [5]. In this framework, a common vector-space, including words from both languages, is created and then the training set is analysed in this space using SVD. This method, termed CL-LSI, will be briefly discussed in Section 4. More generally, many other statistical and linear algebra methods have been used to obtain an improved semantic representation of text data over LSI [6]. In this study we address the problem of learning a semantic representation of text from a paired bilingual corpus, a problem that is important both for mono-lingual and cross-lingual applications. This problem can be regarded either as an unsupervised problem with paired documents, or as a supervised monolingual problem with very complex labels (i.e. the label of an english document could be its french counterpart). In either way, the data can be readily obtained without an explicit labeling effort, and furthermore there is not the loss of information due to compressing the meaning of a document into a discrete label. We employ kernel Canonical Correlation Analysis (KCCA) [1] to learn a representation of text that captures aspects of its meaning. Given a paired bilingual corpus, this method defines two embedding spaces for the documents of the corpus, one for each language, and an obvious one-to-one correspondence between points in the two spaces. KCCA then finds projections in the two embedding spaces for which the resulting projected values are highly correlated. In other words, it looks for particular combinations of words that appear to have the same co-occurrence patterns in the two languages. Our hypothesis is that finding such correlations across a paired crosslingual corpus will locate the underlying semantics, since we assume that the two languages are 'conditionally independent', or that the only thing they have in common is their meaning. The directions would carry information about the concepts that stood behind the process of generation of the text and, although expressed differently in different languages, are, nevertheless, semantically equivalent. To illustrate such representation we have printed the most probable (most typical) words in each language ¢¡ for some of the first few kernel canonical corrleation components found for bilingual 36 Canadian Parliament corpus (Hansards) (left column is English space and right column is French space):

PENSIONS PLAN?

pension plan cpp canadians benefits retirement fund tax investment income finance young years rate superannuation disability taxes mounted future premiums seniors country rates jobs pay

regime pensions rpc prestations canadiens retraite cotisations fonds discours impo^ t revenu jeunes ans pension argent regimes investissement milliards prestation plan finances pays avenir invalidit resolution

AGRICULTURE?

wheat board farmers newfoundland grain party amendment producers canadian speaker referendum minister directors quebec speech school system marketing provinces constitution throne money section rendum majorit

bl commission agriculteurs producteurs canadienne grain parti conseil commercialisat neuve ministre administration modification qubec terre formistes partis grains op nationale lus bloc nations chambre administration

CANADIAN LANDS?

park land aboriginal yukon marine government valley water boards territories board north parks resource agreements northwest resources development treaty nations territoire work territory atlantic programs

parc autochtones terres ches vall ressources yukon nord gouvernement offices marin eaux territoires parcs nations territoriales revendications ministre cheurs ouest entente rights office atlantique ententes

FISHING INDUSTRY?

fisheries atlantic operatives fishermen newfoundland fishery problem operative fishing industry fish years problems wheat coast oceans west salmon tags minister communities program commission motion stocks

pe^ches atlantique pe^cheurs pe^che probl coop ans industrie poisson neuve terre ouest stocks ratives ministre sant saumon affaiblies facult secteur programme gion scientifiques travailler conduite

This representation is then used for retrieval tasks, providing better performance than existing techniques. Such directions are then used to calculate the coordinates of the

documents in a 'language independent' way. Of course, particular statistical care is needed for excluding 'spurious' correlations. We show that the correlations we find are not the effect of chance, and that the resulting representation significantly improves performance of retrieval systems. We find that the correlation existing between certain sets of words in English and French documents cannot be explained as a random correlation. Hence we need to explain it by means of relations between the generative processes of the two versions of the documents, that we assume to be conditionally independent given the topic or content. Under such assumptions, hence, such correlations detect similarities in content between the two documents, and can be exploited to derive a semantic representation of the text. This representation is then used for retrieval tasks, providing better performance than existing techniques. We first apply the method to crosslingual information retrieval, comparing performance with a related approach based on latent semantic indexing (LSI) described below [5]. Secondly, we treat the second language as a complex label for the first language document and view the projection obtained by CL-KCCA as a semantic map for use in a multilingual classification task with very encouraging results. From the computational point of view, we detect such correlations by solving an eigenproblem, that is avoiding problems like local minima, and we do so by using kernels.
The KCCA machinery will be given in Section 3 and in Section 4 we will show how to apply KCCA to cross-lingual retrieval while Section 4 describes the monolingual applications. Finally, results will be presented in Section 5.

2 Previous work

The use of LSI for cross-language retrieval was proposed by [5]. LSI uses a method from

linear algebra, singular value decomposition, to discover the important associative relation-

ships. create

aAsnetinoiftidaluasal-mlapnlgeuoafgedotrcauimnienngtsdoiscutrmanesnltaste ¢d¤¡ £¦b¥¨y£§©human o ra, npder ha! ps ,

b¢  y#" £$m%¥ §&£a©ch.inAef,tteor

preprocessing documents a common vector-space, including words from both languages,

is created and then the training set is analysed in this space using SVD:

' ) (

  10

32!4685 @7 9

(1)

where the A -th column of  corresponds to document A with its first set of coordinates giv-

ing the first language features and the second set the second language features. To translate

a new document (query) B to a language-independent representation one projects (folds-in)

its expanded tation B C into

(filled up the space

with zero components spanned by the D first

reeiglaetnevdetcotoarnsoF2 thE e:r

Gl%BaHngI ua2 gEe7 )B C

vector . The

represensimilarity

between two documents is measured as the inner product between their projections. The

documents that are the most similar to the query are considered to be relevant.

3 Kernel Canonical Correlation Analysis
In this study our aim is to find an appropriate language-independent representation. Suppose as for cross-lingual LSI (CL-LSI) we¡¤£QarSP e Rgiven aligned texts in, fo"TrQ£ sPVimU plicity, two ldable aaniennmdtgwgebuuf eneaaesiggnnioeegP sinom,rareXila.svegfpi.ece,eaiescentvtuvoietvrefhereryoaslsyalstple.etatxOhhcstepeeuiarknXdcehaoer ytsnnapaeesoslpolstaohYaontiehn`fgsb¡atiutssth£¦a,c eitg[sha2etetnw]h)dpao,trcwmohojearaepvpccuitpanisiiongsnn ¢agtl"Tshest¦£ae,r`d¥¨raifc£§n.n©geos.(r9 lspYaametutomi` asob¡ tXan% r£hniic¡¤co$ctW£eif§£&ac%¥©sst)&£§ e©oxYadfati`trnmh"Tedecd£ ac tpi`hin(ofpwnnpeesdi9 triaY htnf op` ear#" aonq£ PdhcWoWcuitag£§hXc©nhetds-r of input data images from the different languages would be maximally correlated. We

h(X ave thX u¤s i£ ntX ui ti)vwelhyicdhefiisndedefiannedeeads for the notion of a kernel canonical X -correlation ¢  ¡

¥  ¡



 ¨¦   §!#© " ¡

$!%'&(&

i` `df



9

aY `

¡

q£ Wc c

9

d` f

g9

T" d£ Wc ic c aY `



¨¦ § ©  !#" ¡

0

)

) £ `df  9 Ya` ¡ £¦cWc d` f p9 Ya` #" £qcWc

£

h` f



9

¡ aY `

£ cic12)43

`df



9

" aY `

3

cic 1

(2)

We search for ducing kernel

 f

and

Hilbert

sf p aicne,thReKspHaSce[2s]p)a:nf n ed

by6) th85 e7

5Y

Ya-i` m¡ 5 ac g, ef sV of

th)@e B9daAta9

poin" t9 s
Ya`

c(repro. This

rewrites

the

numerator

ofC

(2) as £ d` f  9

aY `

¡

£ cWc d` f



9" Ya`

£ Wc c



7 7 e e

A

(3)

7

  7 5¥ A

 A 9 ¥

where is the vector with components and the vector with components

. The

problem (2) can then be reformulated as

 ¡

6¦¨D § E ©

7 GF F
e

7 

7

e

FHF¥ HF e F
e

A A

HF F

(4)

Once we have moved to a kernel defined feature space the extra flexibility introduced means

that there is a danger of overfitting. By this we mean that we can find spurious correlations

by using large weight vectors to project the data so that the two projections are completely

aligned. For example, if the data are linearly independent in both feature spaces we can

find linear transformations that map the input data to an orthogonal basis in each feature

space. It is now possible to find I perfect correlations between the two representations.

Using kernel functions will frequently result in linear independence of the training set, for

example, a control

when on the

uflseinxgibGiliatyusosfiatnhekeprrnoejelsc.tiIotnismcalepaprinthgesrfefoarnedthf a t.wTeo

will need to introduce do that in the spirit of

Partial Least Squares (PLS) we would add a multiple of 2-norm squared:

P

FHF
f



FGF 1



BP Q C 5

7

5¡
aY `

5c

9C S5 R

7 5 R ¡ 5 R cUT
aY `



P7 7 e 7

(5)

iFHnF
e

th7e

`VXW P

FHdF 1 enominator.

: cYFHF
e

7

HF F a1 `

P

Convexly combining

FGF  HF F 1 
f

` VXW P c 7 7 e 1

PLS 7` P

wA e substitute its square root into denominator of

regularization 7 7 7  7 7
e HF F (4) instead of

e

term (5) and

e

 

7`i`FHVXF W

P

c e



and do

kCCA term ` Pcb c 7 (6) the same for

: '  ¡ @¨¦ D § E ©

D¤g ih ffFGeF pi reFGF ntiatrq iq hhnrqgq

the and

e  e  wA v `eV W P

d

` `eV W

P

fc FGF
e



7

FHF 1 ` P ¦¨§©

77 FGF 
f

eHF F


¨1 c

e

A `i` VeW

P

fc GF F 7e

 A HF F 1 `

P

FHF
f

cfsesFGDxF e p7 r7e7 se sHF iFo1 7n`

uunt de er

P

FGF
f



7 FHF

, 18x

with respect to , taking into

and equating the derivative to zero

W

77e

e

 A W` ` VeW

P

c e

`

cP b c e

 FHF 1¨c (7)
account that we obtain  7 uy (8)

7 We Anote that

can be normalised so that ` VW

P

cfFGF
e

7

FHF 1 `

P

FGF
f



FHF 1



V . Similar operations

for yield analogous equations that together with (8) can be written in a matrix form:

     

(9)

w7 7hee re

 
e

 (

i sA

the average , and

per

point

correlation

between

projections

h` f



9

b¡ cic Ya`

and `hf

p9

e  e 

e

 e




0

9 '

(

i` ` VeW

P

c e

`

cP b c e





i` ` VeW

P

c e



`

cP b c e

g" Wc c Ya` :
0

(10)

 ¢¡
Table 1: Statistics for 'House debates' of the 36 Canadian Parliament proceedings corpus.

TRAINING
TESTING 1

SENTENCE
PAIRS
948K 62K

ENGLISH
WORDS
14,614K 995K

FRENCH
WORDS
15,657K 1067K

¡  £¢
where



77 A7

7 . Equation (9) is known as a generalised eigenvalue problem.The

standard approach to the solution of (9) in the
¥¤ ¤ ¦ §¤plete Cholesky decomposition of the matrix

 ca:se

of

a

sym7 metarnicd

is to define

per form

incomwhich

©¤ ¨ ¤©¨ ¦ ¦allo7ws

us, 

aft er 

simple . We

wtriallndsfisocrumsasthioonws,totochreowosreiteP

it as a standard in Section 5.

eigenvalue

problem

7 It is easy to see that if

A or

changes sign in (9),  

also changes sign. Thus, the spectrum

of the problem (9) has paired positive and negative values between W V and V .

4 Applications of KCCA

Cross-linguistic retrieval with KCCA. The kernel CCA procedure identifies a set of pro-

jections from both languages into a common semantic space. This provides a natural frame-

work for performing cross-language information retrieval. We first select a number of

  semantic dimensions, V

I , with largest correlation values   . To process an in-

   coming query B we expand B into the  it onto the canonical X -correlation vector for that language, where is a

vIceocm£ toprornmeepanrttersis:xenGw%B tHahtoi osen

fo7 r

it7 s B

language B using the

and project appropriate

columns are the first solutions

of (9) for

 "! # $%%$ $ !   &$'$%$that

`qaY ¡ ` `



cth9 aYe¡ g`1 B ivWc cenislasni¡gmu§ palgyc eosr7orB tewdhbe` yre" eigeni" sv1 atlhuee

in descending order. Here we assumed

train" i§ ngc

corpus .

in

the

given

language:

(  Using
guage

the

semantic can

space in text categorisation. be exported and used in some

The semantic vectors in the given lanother application, for example, Support

Vector Machine classification. We first find common features of the training data used to

extract the semantics and the data used to train SVM classifier, cut the features that are not

common and compute the new kernel which is the inner product of the projected data:

0) (1(e

¡ h`

£9¡

3

c



¡ 7£

7 ¡3

(11)

(1(The term-term relationship matrix

7 can be computed only once and stored for further

use in the SVM learning process and classification.

5 Experiments
Experimental setup. Following [5] we conducted a series of experiments with the Hansard collection [3] to measure the ability of CL-LSI and CL-KCCA for any document from a test collection in one language to find its mate in another language. The whole collection consists¢  ¡ of 1.3 million pairs of aligned text chunks (sentences or smaller fragments) from the 36 Canadian Parliament proceedings. In our experiments we used only the 'house debates' part for which statistics is given in Table 1. As a testing collection we used only 'testing 1'. The raw text was split into sentences with Adwait Ratnaparkhi's MXTERMINATOR and the sentences were aligned with I. Dan Melamed's GSA tool (for details on the collection and also for the source see [3]).

$ $ '$ %$ $ $Table 2: Average accuracy of top-rank (first retrieved) English  French retrieval, %
((ly e¢ fV t)(9 y agt n9 d av9(ey ra¡ g),e%p(rreicgihsti)on of English  French retrieval over set of fixed recalls cl-lsi 81400¥£ ¤ 92100§£ ¦ 93300£§¦ 94500£§¦ 9fu7l§£l ¦ 71300£§¦ 72800§£ ¦ 38000§£ ¦ 48020§£ ¦ f8u2l¥£l ¨ cl-kcca 98£§¦ 99§£ ¦ 99§£ ¦ 99£§¦ 99§£ ¦ 91£© 91£© 91£ © 91£ © 87£ 

The text chunks were split into 'paragraphs' based on '***' delimiters and these 'para-

Etgernramgplh-isbsh'y-wpdaeorrctesutamrneedantretad'rEeanswgsoleirpsdhas'r(amit.eea.dtraoipxcpuaemnadreinn" tgs!.lV eAV sfs£ttehrV art ne# mt"horev'eiFntrgiemnstecoshp)'-wwmeoaortrdbistxaiin(nwbeedota hlVsFo r¡ er£enmcV htoavnedd

a few documents that appeared to be problematic when split into paragraphs). As these

matrices were still too large to perform SVD and KCCA on them, we split the whole col-

lection into 14 chunks of about 910 documents each and conducted experiments separately

with them, measuring the performance of the methods each time on a 917-document test

$collection. The results were then averaged. We have also trained the CL-KCCA method

oy nVr aonndotemstlydaretaaswsohciciahteisdfFarrelonwche-rEtnhganlisrhesduoltcsuomnetnhtepnaoirns-raannddoombseorrvigeidnaalccduartaa.cyIt

of is

about worth

noting that CL-KCCA behaves differently from CL-LSI over the full scale of the spectrum.

When CL-LSI only increases its performance with more eigenvectors taken from the lower

part of spectrum (which is, somewhat unexpectedly, quite different from its behaviour in

the monolinguistic setting), CL-KCCA's performance, on the contrary, tends to deteriorate

with the dimensionality of the semantic subspace approaching the dimensionality of the

input data space.

The partial Singular Value Decomposition of the matrices was done using Matlab's 'svds' function and full SVD was performed using the 'kernel trick' discussed in the previous section and 'svd' function which took about 2 minutes to compute on Linux Pentium III 1GHz system for a selection of 1000 documents. The Matlab implementation of KCCA using the same function, 'svd', which solves the generalised eigenvalue problem through Cholesky incomplete decomposition, took about 8 minutes to compute on the same data.

Mate retrieval. The results are presented in Table 2. Only one - mate document in French

$ $ $%$'$ $was considered as relevant to each of the test English documents which were treated as

queries and along with

the relative number of average precision over

cfioxreredctrleycarelltsr:ievy edV ,dy oct u,men,tsy

w¡ a. s

computed (Table 2) Very similar results

(omitted here) were obtained when French documents were treated as queries and English

as test documents. As one can see from the semantics in the first few components

Taacbhlieev2inCgL¡ - K$ CCacAcusreaecmy swtiothcaasptluitrtelemaoss1t 0o0f

components when CL-LSI needs all components for a similar figure.

Selecting the regularization parameter. The regularization parameter P (6) not only

makes the problem the function space

(9) well-posed numerically, where the solution is being

but also sought.

pTrohveidleasrgceornvtraollueosveorfcPapaarceit,ythoef

less sensitive the method to the input data is, therefore, the more stable (less prone to

finding spurious relations) the solution becomes. We should thus observe an increase of

"reliability" of the solution. We measure the ability of the method to catch useful sig-

nal by comparing the solutions on original is constructed by random reassociations of

input and "random" data. The
the data pairs, for example, &` %

"9 &ra§"n')d( o&` m0 "cic

data de-

notes English-French parallel corpus which is obtained from the original English-French
¤ ¤  ¤ ¤ tap31 hle2iergfnpeecadtie rcecodorlrdleealctata2tisio`oennt sb ` yi9anr de19 schhedu1 nefc cnfl.eoinItGFefgF4 stthWthheeee mF(preeotnhscoithdiv2 (eie`sqpoua1 vrivtei9aro filfe)t1nt1istnplcYygFHe,6F ctE5thrneugy mdl9 wiasothhaf)etirhdteeow4 cKuiilsCml tCbehneAetasas.lboll-Sleouunttpioeopnvofiesoncend-,

1.5 1.5 1
11

0.5 0.5 0.5

¤ ¤  ¤ ¤  ¤ ¤ 0
0

1

2

3

4 00

1

2

3

4

0 0

1

2

3

4

FGF 4

FigurFHFe4
and

1: W

Qe uantitie2 s` %

9 & W §#' e (

&` 0

2
ic Yc FHF

&` %

9 & §")' ( &` %

cWcYFHF GF F4
(left),

W

(right) as functions of the

e 2 `&% 9
regularization

0

Yc HF F para(mmeitdedr lPe).

(Graphs were obtained for the regularization schema discussed in [1]).

¤ ¤  ¤ ¤  ¤ ¤  ¤ ¤ itveeHF noFv4arelW.Funiee Wgisnueorr2efath`n1P%edros2e9thhm%f` ooe% lcwry)es9 .p&atu#§ehsTss'ceehot( riqctus&` hiu0maiiatnsciendYcodatiiHFfsctFpiaeaaaatslsielmrssHFft.Fe4uhoaAnWevscesue ttreriPfioesttnitstnisoncisgoraefs2csasilts`&enoh%escsessee9 t&trhhtet§#heoge)' eus(tdphlme`aee%grceirtatzcWerhlacfuelotFG-miFdo,oonfnGFoieFs4 ofpWvastahpbee rerelaeficmrttattreoniuntdmefigor.nm2 (dP T`tlh.% ycheora9 Fer0ssreospecforeglFGcacFrs,ittamairapoutanehnmlddssl $odribagtshaetrbgvearcatoipomhnsecsinafnaFrbifgeruoermexp1allolt-hiotiesndev,faowlurhecilhceootuohlsaditnbogefttdhheeeriopvpaeitdrimeadasdlloyvcianulugmeseoonmftseP rw.emhFearroienmsbecttohwrereemelnaitdeV ddla.enTdahntids.
For the experiments reported in this study we used the value of V  .
Pseudo query test. To perform a more realistic test we generated short queries, which are most likely to occur in search engines, that consisted of the 5 most probable words from each test document. The relevant documents were the test documents themselves in monolinguistic retrieval (English query - English document) and their mates in the crosslinguistic (English query - French document) test. Table 3 shows the relative number of correctly retrieved as top-ranked English documents for English queries (left) and the relative number of correctly retrieved documents in the top ten ranked (right). Table 4 provides analogous results but for cross-linguistic retrieval.

Table 3: English  English top-ranked retrieval accuracy, % (left) and English  English

to¢ p-ten retrieval accuracy, % (right)

cl-lsi cl-kcca

6510300£§£ ©¦

6623000£§§£ ¦¦

7630400§££§¦¦

7641600£§§£ ¦¦

77fu30l§§££l ¦¦

9810200£§§£ ¦¦

9823600§£§£ ¦¦

9384080£§£§¦¦

9485090§§££ ¦¦

9f9u51l§£§£l ¦¦

Table 4: English  French top-ranked retrieval accuracy, % (left) and English-French top-
ten retrieval accuracy, % (right)

¢

cl-lsi cl-kcca

6318000£§£§¦¦

7325800§§££ ¦¦

7438200§££©¦

7449500§£§£ ¦¦

84fu19l§£¥£l ¨¦

9614700§§££ ¦¦

9726500£§£©¦

9377090£§£ ©¦

9488010§££ ©¦

9f8u84l££§§l ¦¦

Text categorisation using semantics learned on a completely different corpus. The semantics (300 vectors) extracted from the Canadian Parliament corpus (Hansard) was used in Support Vector Machine (SVM) text classification [2] of Reuters-21578 corpus (Table 5). In this experimental setting the intersection of vector spaces of the Hansards,

5159 English words from the first 1000-French-English-document training chunk, and

Reuters ModApt split, 9962 wo rds 'y fry om the 9602 training and 3299 test documents

had 1473 words. The extracted

KCCA vectors from English and French parts

(raw 'KCCA' of 5  T£ a¡ b  le 5) and 300 eigenvectors from the same data (raw 'CL-LSI') were

used in the SVM [4] with the kernel (11) to classify the Reuters-21578 data. The

experiments were averaged over 10 runs with 5% each time randomly chosen fraction of

training data as the difference between bag-of-words and semantic methods is more con-

trasting on smaller samples. Both CL-KCCA and CL-LSI perform remarkably well when

one considers that they are based on just 1473 words. In all cases CL-KCCA outperforms

the bag-of-words kernel.

Table 5: 0  value, %, averaged over 10 subsequent runs of SVM classifier with original
Reuters-21578 data ('bag-of-words') and preprocessed using semantics (300 vectors)
extracted from the Canadian Parliament corpus by various methods.

CLASS
BAG-OF-WORDS CL-KCCA CL-LSI

879E170A£££¥RN¤¡©

55A7725C¥¥£££Q ¤¤

36G4 343R¥§££££ A I¨¢¦

N


C143308R£§£¥£UD¤©¦ E©

6 Conclusions
We have presented a novel procedure for extracting semantic information in an unsupervised way from a bilingual corpus, and we have used it in text retrieval applications. Our main findings are that: the correlation existing between certain sets of words in english and french documents cannot be explained as random correlations. Hence we need to explain it by means of relations between the generative processes of the two versions of the documents. The correlations detect similarities in content between the two documents, and can be exploited to derive a semantic representation of the text. The representation is then used for retrieval tasks, providing better performance than existing techniques.

References

[1] F. R. Bach and M. I. Jordan. Kernel indepedendent component analysis. Journal of Machine Learning Research, 3:1­48, 2002.

[2] Nello Cristianini and John Shawe-Taylor. An introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000.

[3] Ulrich Germann. Aligned Hansards of the 36th Parliament of Canada.

[4]

Ththtpo:r/s/twenww.isJio.eadcuh/inmast.ural-languag¤ e/¦5 do¥ wn5 £ l  o¡ a  d/ha-nsardS/u, 2p0p0o1rt.

Release 2001-1a. Vector Machine.

http://svmlight.joachims.org, 2002.

[5] M. L. Littman, S. T. Dumais, and T. K. Landauer. Automatic cross-language information retrieval using latent semantic indexing. In G. Grefenstette, editor, Cross language information retrieval. Kluwer, 1998.

[6] Alexei Vinokourov and Mark Girolami. A probabilistic framework for the hierarchic organisation and classification of document collections. Journal of Intelligent Information Systems, 18(2/3):153­172, 2002. Special Issue on Automated Text Categorization.

