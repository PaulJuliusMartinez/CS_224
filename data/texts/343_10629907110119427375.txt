ARA*: Anytime A* with Provable Bounds on

Sub-Optimality

Maxim Likhachev, Geoff Gordon and Sebastian Thrun

School of Computer Science
Carnegie Mellon University

Pittsburgh, PA 15213

{maxim+, ggordon, thrun}@cs.cmu.edu

Abstract

In real world planning problems, time for deliberation is often limited.
Anytime planners are well suited for these problems: they ﬁnd a feasi-
ble solution quickly and then continually work on improving it until time
runs out. In this paper we propose an anytime heuristic search, ARA*,
which tunes its performance bound based on available search time. It
starts by ﬁnding a suboptimal solution quickly using a loose bound, then
tightens the bound progressively as time allows. Given enough time it
ﬁnds a provably optimal solution. While improving its bound, ARA*
reuses previous search efforts and, as a result, is signiﬁcantly more efﬁ-
cient than other anytime search methods. In addition to our theoretical
analysis, we demonstrate the practical utility of ARA* with experiments
on a simulated robot kinematic arm and a dynamic path planning prob-
lem for an outdoor rover.

1 Introduction
Optimal search is often infeasible for real world problems, as we are given a limited amount
of time for deliberation and want to ﬁnd the best solution given the time provided.
In
these conditions anytime algorithms [9, 2] prove to be useful as they usually ﬁnd a ﬁrst,
possibly highly suboptimal, solution very fast and then continually work on improving
the solution until allocated time expires. Unfortunately, they can rarely provide bounds
on the sub-optimality of their solutions unless the cost of an optimal solution is already
known. Even less often can these algorithms control their sub-optimality. Providing sub-
optimality bounds is valuable, though: it allows one to judge the quality of the current
plan, decide whether to continue or preempt search based on the current sub-optimality, and
evaluate the quality of past planning episodes and allocate time for future planning episodes
accordingly. Control over the sub-optimality bounds helps in adjusting the tradeoff between
computation and plan quality.

A* search with inﬂated heuristics (actual heuristic values are multiplied by an inﬂation
factor  > 1) is sub-optimal but proves to be fast for many domains [1, 5, 8] and also pro-
vides a bound on the sub-optimality, namely, the  by which the heuristic is inﬂated [7].
To construct an anytime algorithm with sub-optimality bounds one could run a succession
of these A* searches with decreasing inﬂation factors. This naive approach results in a se-
ries of solutions, each one with a sub-optimality factor equal to the corresponding inﬂation

factor. This approach has control over the sub-optimality bound, but wastes a lot of com-
putation since each search iteration duplicates most of the efforts of the previous searches.
One could try to employ incremental heuristic searches (e.g., [4]), but the sub-optimality
bounds for each search iteration would no longer be guaranteed.

To this end we propose the ARA* (Anytime Repairing A*) algorithm, which is an
efﬁcient anytime heuristic search that also runs A* with inﬂated heuristics in succession
but reuses search efforts from previous executions in such a way that the sub-optimality
bounds are still satisﬁed. As a result, a substantial speedup is achieved by not re-computing
the state values that have been correctly computed in the previous iterations. We show the
efﬁciency of ARA* on two different domains. An evaluation of ARA* on a simulated robot
kinematic arm with six degrees of freedom shows up to 6-fold speedup over the succession
of A* searches. We also demonstrate ARA* on the problem of planning a path for a mobile
robot that takes into account the robot’s dynamics.

The only other anytime heuristic search known to us is Anytime A*, described in [8]. It
also ﬁrst executes an A* with inﬂated heuristics and then continues to improve a solution.
However, the algorithm does not have control over its sub-optimality bound, except by
selecting the inﬂation factor of the ﬁrst search. Our experiments show that ARA* is able
to decrease its bounds much more gradually and, moreover, does so signiﬁcantly faster.
Another advantage of ARA* is that it guarantees to examine each state at most once during
its ﬁrst search, unlike the algorithm of [8]. This property is important because it provides
a bound on the amount of time before ARA* produces its ﬁrst plan. Nevertheless, as
mentioned later, [8] describes a number of very interesting ideas that are also applicable to
ARA*.

2 The ARA* Algorithm

2.1 A* with Weighted Heuristic
Normally, A* takes as input a heuristic h(s) which must be consistent. That is, h(s) ≤
c(s, s0) + h(s0) for any successor s0 of s if s 6= sgoal and h(s) = 0 if s = sgoal. Here
c(s, s0) denotes the cost of an edge from s to s0 and has to be positive. Consistency, in
its turn, guarantees that the heuristic is admissible: h(s) is never larger than the true cost
Inﬂating the heuristic (that is, using  ∗ h(s) for  > 1)
of reaching the goal from s.
often results in much fewer state expansions and consequently faster searches. However,
inﬂating the heuristic may also violate the admissibility property, and as a result, a solution
is no longer guaranteed to be optimal. The pseudocode of A* with inﬂated heuristic is
given in Figure 1 for easy comparison with our algorithm, ARA*, presented later.
A* maintains two functions from states to real numbers: g(s) is the cost of the current
path from the start node to s (it is assumed to be ∞ if no path to s has been found yet), and
f(s) = g(s)+∗h(s) is an estimate of the total distance from start to goal going through s.
A* also maintains a priority queue, OPEN, of states which it plans to expand. The OPEN
queue is sorted by f(s), so that A* always expands next the state which appears to be on
the shortest path from start to goal. A* initializes the OPEN list with the start state, sstart
(line 02). Each time it expands a state s (lines 04-11), it removes s from OPEN. It then
updates the g-values of all of s’s neighbors; if it decreases g(s0), it inserts s0 into OPEN.
A* terminates as soon as the goal state is expanded.
01 g(sstart) = 0; OPEN = ∅;
02 insert sstart into OPEN with f (sstart) =  ∗ h(sstart);
03 while(sgoal is not expanded)
remove s with the smallest f -value from OPEN;
04
for each successor s0 of s
05
if s0 was not visited before then
06
f (s0) = g(s0) = ∞;
07
if g(s0) > g(s) + c(s, s0)
08
g(s0) = g(s) + c(s, s0);
09
f (s0) = g(s0) +  ∗ h(s0);
10
insert s0 into OPEN with f (s0);
11

Figure 1: A* with heuristic weighted by  ≥ 1

 = 2.5

 = 1.5

 = 1.0

 = 2.5

 = 1.5

 = 1.0

Figure 2: Left three columns: A* searches with decreasing . Right three columns: the corresponding
ARA* search iterations.

Setting  to 1 results in standard A* with an uninﬂated heuristic; the resulting solution
is guaranteed to be optimal. For  > 1 a solution can be sub-optimal, but the sub-optimality
is bounded by a factor of : the length of the found solution is no larger than  times the
length of the optimal solution [7].

The left three columns in Figure 2 show the operation of the A* algorithm with a
heuristic inﬂated by  = 2.5,  = 1.5, and  = 1 (no inﬂation) on a simple grid world. In
this example we use an eight-connected grid with black cells being obstacles. S denotes a
start state, while G denotes a goal state. The cost of moving from one cell to its neighbor
is one. The heuristic is the larger of the x and y distances from the cell to the goal. The
cells which were expanded are shown in grey. (A* can stop search as soon as it is about
to expand a goal state without actually expanding it. Thus, the goal state is not shown in
grey.) The paths found by these searches are shown with grey arrows. The A* searches with
inﬂated heuristics expand substantially fewer cells than A* with  = 1, but their solution is
sub-optimal.

2.2 ARA*: Reuse of Search Results
ARA* works by executing A* multiple times, starting with a large  and decreasing  prior
to each execution until  = 1. As a result, after each search a solution is guaranteed to be
within a factor  of optimal. Running A* search from scratch every time we decrease ,
however, would be very expensive. We will now explain how ARA* reuses the results of
the previous searches to save computation. We ﬁrst explain the ImprovePath function (left
column in Figure 3) that recomputes a path for a given . In the next section we explain the
Main function of ARA* (right column in Figure 3) that repetitively calls the ImprovePath
function with a series of decreasing s.

Let us ﬁrst introduce a notion of local inconsistency (we borrow this term from [4]). A
state is called locally inconsistent every time its g-value is decreased (line 09, Figure 1) and
until the next time the state is expanded. That is, suppose that state s is the best predecessor
for some state s0: that is, g(s0) = mins00∈pred(s0)(g(s00)+c(s00, s0)) = g(s)+c(s, s0). Then,
if g(s) decreases we get g(s0) > mins00∈pred(s0)(g(s00) + c(s00, s0)). In other words, the
decrease in g(s) introduces a local inconsistency between the g-value of s and the g-values
of its successors. Whenever s is expanded, on the other hand, the inconsistency of s is
corrected by re-evaluating the g-values of the successors of s (line 08-09, Figure 1). This
in turn makes the successors of s locally inconsistent. In this way the local inconsistency
is propagated to the children of s via a series of expansions. Eventually the children no
longer rely on s, none of their g-values are lowered, and none of them are inserted into
the OPEN list. Given this deﬁnition of local inconsistency it is clear that the OPEN list
consists of exactly all locally inconsistent states: every time a g-value is lowered the state
is inserted into OPEN, and every time a state is expanded it is removed from OPEN until
the next time its g-value is lowered. Thus, the OPEN list can be viewed as a set of states
from which we need to propagate local inconsistency.

A* with a consistent heuristic is guaranteed not to expand any state more than once.
Setting  > 1, however, may violate consistency, and as a result A* search may re-expand
states multiple times. It turns out that if we restrict each state to be expanded no more
than once, then the sub-optimality bound of  still holds. To implement this restriction we
check any state whose g-value is lowered and insert it into OPEN only if it has not been
previously expanded (line 10, Figure 3). The set of expanded states is maintained in the
CLOSED variable.

procedure fvalue(s)
01 return g(s) +  ∗ h(s);

procedure ImprovePath()
02 while(fvalue(sgoal) > mins∈OPEN(fvalue(s)))
remove s with the smallest fvalue(s) from OPEN;
03
04 CLOSED = CLOSED ∪ {s};
for each successor s0 of s
05
if s0 was not visited before then
06
g(s0) = ∞;
07
if g(s0) > g(s) + c(s, s0)
08
g(s0) = g(s) + c(s, s0);
09
if s0 6∈ CLOSED
10
insert s0 into OPEN with fvalue(s0);
11
else
12
insert s0 into INCONS;
13

procedure Main()
01’ g(sgoal) = ∞; g(sstart) = 0;
02’ OPEN = CLOSED = INCONS = ∅;
03’ insert sstart into OPEN with fvalue(sstart);
04’ ImprovePath();
05’ 0 = min(, g(sgoal)/ mins∈OPEN∪INCONS(g(s)+h(s)));
06’ publish current 0-suboptimal solution;
07’ while 0 > 1
08’
decrease ;
09’ Move states from INCONS into OPEN;
10’ Update the priorities for all s ∈ OPEN according to fvalue(s);
11’ CLOSED = ∅;
ImprovePath();
12’
0 = min(, g(sgoal)/ mins∈OPEN∪INCONS(g(s)+h(s)));
13’
publish current 0-suboptimal solution;
14’

Figure 3: ARA*

With this restriction we will expand each state at most once, but OPEN may no longer
contain all the locally inconsistent states. In fact, it will only contain the locally inconsistent
states that have not yet been expanded. It is important, however, to keep track of all the
locally inconsistent states as they will be the starting points for inconsistency propagation
in the future search iterations. We do this by maintaining the set INCONS of all the locally
inconsistent states that are not in OPEN (lines 12-13, Figure 3). Thus, the union of INCONS
and OPEN is exactly the set of all locally inconsistent states, and can be used as a starting
point for inconsistency propagation before each new search iteration.

The only other difference between the ImprovePath function and A* is the termination
condition. Since the ImprovePath function reuses search efforts from the previous execu-
tions, sgoal may never become locally inconsistent and thus may never be inserted into
OPEN. As a result, the termination condition of A* becomes invalid. A* search, however,
can also stop as soon as f(sgoal) is equal to the minimal f-value among all the states on
OPEN list. This is the condition that we use in the ImprovePath function (line 02, Fig-
ure 3). It also allows us to avoid expanding sgoal as well as possibly some other states
with the same f-value. (Note that ARA* no longer maintains f-values as variables since in
between the calls to the ImprovePath function  is changed, and it would be prohibitively
expensive to update the f-values of all the states. Instead, the fvalue(s) function is called
to compute and return the f-values only for the states in OPEN and sgoal.)

2.3 ARA*: Iterative Execution of Searches
We now introduce the main function of ARA* (right column in Figure 3) which performs a
series of search iterations. It does initialization and then repetitively calls the ImprovePath
function with a series of decreasing s. Before each call to the ImprovePath function a
new OPEN list is constructed by moving into it the contents of the set INCONS. Since
OPEN list has to be sorted by the current f-values of states it is also re-ordered (lines 09’-
10’, Figure 3). Thus, after each call to the ImprovePath function we get a solution that is
sub-optimal by at most a factor of .

As suggested in [8] a sub-optimality bound can also be computed as the ratio between
g(sgoal), which gives an upper bound on the cost of an optimal solution, and the minimum
un-weighted f-value of a locally inconsistent state, which gives a lower bound on the cost
of an optimal solution. (This is a valid sub-optimality bound as long as the ratio is larger
than or equal to one. Otherwise, g(sgoal) is already equal to the cost of an optimal solution.)
Thus, the actual sub-optimality bound for ARA* is computed as the minimum between 
and the ratio (lines 05’ and 13’, Figure 3). At ﬁrst, one may also think of using this actual
sub-optimality bound in deciding how to decrease  between search iterations (e.g., setting
 to 0 minus a small delta). Experiments, however, seem to suggest that decreasing  in
small steps is still more beneﬁcial. The reason is that a small decrease in  often results
in the improvement of the solution, despite the fact that the actual sub-optimality bound of
the previous solution was already substantially less than the value of . A large decrease in
, on the other hand, may often result in the expansion of too many states during the next
search. (Another useful suggestion from [8], which we have not implemented in ARA*, is
to prune OPEN so that it never contains a state whose un-weighted f-value is larger than

or equal to g(sgoal).)

Within each execution of the ImprovePath function we mainly save computation by
not re-expanding the states which were locally consistent and whose g-values were already
correct before the call to ImprovePath (Theorem 2 states this more precisely). For example,
the right three columns in Figure 2 show a series of calls to the ImprovePath function.
States that are locally inconsistent at the end of an iteration are shown with an asterisk.
While the ﬁrst call ( = 2.5) is identical to the A* call with the same , the second call
to the ImprovePath function ( = 1.5) expands only 1 cell. This is in contrast to 15 cells
expanded by A* search with the same . For both searches the sub-optimality factor, ,
decreases from 2.5 to 1.5. Finally, the third call to the ImprovePath function with  set to
1 expands only 9 cells. The solution is now optimal, and the total number of expansions
is 23. Only 2 cells are expanded more than once across all three calls to the ImprovePath
function. Even a single optimal search from scratch expands 20 cells.

2.4 Theoretical Properties of the Algorithm
We now present some of the theoretical properties of ARA*. For the proofs of these and
other properties of the algorithm please refer to [6]. We use g∗(s) to denote the cost of an
optimal path from sstart to s. Let us also deﬁne a greedy path from sstart to s as a path
that is computed by tracing it backward as follows: start at s, and at any state si pick a state
si−1 = arg mins0∈pred(si)(g(s0) + c(s0, si)) until si−1 = sstart.
for any state s with f(s) ≤
Theorem 1 Whenever the ImprovePath function exits,
mins0∈OPEN(f(s0)), we have g∗(s) ≤ g(s) ≤  ∗ g∗(s), and the cost of a greedy path
from sstart to s is no larger than g(s).

The correctness of ARA* follows from this theorem: each execution of the Im-
provePath function terminates when f(sgoal) is no larger than the minimum f-value in
OPEN, which means that the greedy path from start to goal that we have found is within a
factor  of optimal. Since before each iteration  is decreased, and it, in its turn, is an upper
bound on 0, ARA* gradually decreases the sub-optimality bound and ﬁnds new solutions
to satisfy the bound.
Theorem 2 Within each call to ImprovePath() a state is expanded at most once and only
if it was locally inconsistent before the call to ImprovePath() or its g-value was lowered
during the current execution of ImprovePath().

The second theorem formalizes where the computational savings for ARA* search
come from. Unlike A* search with an inﬂated heuristic, each search iteration in ARA*
is guaranteed not to expand states more than once. Moreover, it also does not expand states
whose g-values before a call to the ImprovePath function have already been correctly com-
puted by some previous search iteration, unless they are in the set of locally inconsistent
states already and thus need to update their neighbors (propagate local inconsistency).

3 Experimental Study
3.1 Robotic Arm
We ﬁrst evaluate the performance of ARA* on simulated 6 and 20 degree of freedom (DOF)
robotic arms (Figure 4). The base of the arm is ﬁxed, and the task is to move its end-effector
to the goal while navigating around obstacles (indicated by grey rectangles). An action
is deﬁned as a change of a global angle of any particular joint (i.e., the next joint further
along the arm rotates in the opposite direction to maintain the global angle of the remaining
joints.) We discretitize the workspace into 50 by 50 cells and compute a distance from each
cell to the cell containing the goal while taking into account that some cells are occupied
by obstacles. This distance is our heuristic. In order for the heuristic not to overestimate
true costs, joint angles are discretitized so as to never move the end-effector by more than
one cell in a single action. The resulting state-space is over 3 billion states for a 6 DOF
robot arm and over 1026 states for a 20 DOF robot arm, and memory for states is allocated
on demand.

(a) 6D arm trajectory for  = 3

(b) uniform costs

(c) non-uniform costs

(d) both Anytime A* and A*
after 90 secs, cost=682, 0=15.5
Figure 4: Top row: 6D robot arm experiments. Bottom row: 20D robot arm experiments (the
trajectories shown are downsampled by 6). Anytime A* is the algorithm in [8].

after 90 secs, cost=657, 0=14.9

(e) ARA*

(f) non-uniform costs

Figure 4a shows the planned trajectory of the robot arm after the initial search of ARA*
with  = 3.0. This search takes about 0.05 secs. (By comparison, a search for an optimal
trajectory is infeasible as it runs out of memory very quickly.) The plot in Figure 4b shows
that ARA* improves both the quality of the solution and the bound on its sub-optimality
faster and in a more gradual manner than either a succession of A* searches or Anytime
A* [8]. In this experiment  is initially set to 3.0 for all three algorithms. For all the ex-
periments in this section  is decreased in steps of 0.02 (2% sub-optimality) for ARA* and
a succession of A* searches. Anytime A* does not control , and in this experiment it
apparently performs a lot of computations that result in a large decrease of  at the end. On
the other hand, it does reach the optimal solution ﬁrst this way. To evaluate the expense of
the anytime property of ARA* we also ran ARA* and an optimal A* search in a slightly
simpler environment (for the optimal search to be feasible). Optimal A* search required
about 5.3 mins (2,202,666 state expanded) to ﬁnd an optimal solution, while ARA* re-
quired about 5.5 mins (2,207,178 state expanded) to decrease  in steps of 0.02 from 3.0
until a provably optimal solution was found (about 4% overhead).

While in the experiment for Figure 4b all the actions have the same cost, in the exper-
iment for Figure 4c actions have non-uniform costs: changing a joint angle closer to the
base is more expensive than changing a higher joint angle. As a result of the non-uniform
costs our heuristic becomes less informative, and so search is much more expensive. In
this experiment we start with  = 10, and run all algorithms for 30 minutes. At the end,
ARA* achieves a solution with a substantially smaller cost (200 vs. 220 for the succession
of A* searches and 223 for Anytime A*) and a better sub-optimality bound (3.92 vs. 4.46
for both the succession of A* searches and Anytime A*). Also, since ARA* controls  it
decreases the cost of the solution gradually. Reading the graph differently, ARA* reaches
a sub-optimality bound 0 = 4.5 after about 59 thousand expansions and 11.7 secs, while
the succession of A* searches reaches the same bound after 12.5 million expansions and
27.4 minutes (about 140-fold speedup by ARA*) and Anytime A* reaches it after over 4
million expansions and 8.8 minutes (over 44-fold speedup by ARA*). Similar results hold
when comparing the amount of work each of the algorithms spend on obtaining a solution
of cost 225. While Figure 4 shows execution time, the comparison of states expanded (not
shown) is almost identical. Additionally, to demonstrate the advantage of ARA* expanding
each state no more than once per search iteration, we compare the ﬁrst searches of ARA*
and Anytime A*: the ﬁrst search of ARA* performed 6,378 expansions, while Anytime
A* performed 8,994 expansions, mainly because some of the states were expanded up to

(a) robot with laser scanner

(b) 3D Map

(c) optimal 2D search

(d) optimal 4D search with A*

after 25 secs

(e) 4D search with ARA*
after 0.6 secs ( = 2.5)

(f) 4D search with ARA*

after 25 secs ( = 1.0)

Figure 5: outdoor robot navigation experiment (cross shows the position of the robot)

seven times before a ﬁrst solution was found.

Figures 4d-f show the results of experiments done on a 20 DOF robot arm, with actions
that have non-uniform costs. All three algorithms start with  = 30. Figures 4d and 4e
show that in 90 seconds of planning the cost of the trajectory found by ARA* and the sub-
optimality bound it can guarantee is substantially smaller than for the other algorithms. For
example, the trajectory in Figure 4d contains more steps and also makes one extra change
in the angle of the third joint from the base of the arm (despite the fact that changing lower
joint angles is very expensive) in comparison to the trajectory in Figure 4e. The graph in
Figure 4f compares the performance of the three algorithms on twenty randomized envi-
ronments similar to the environment in Figure 4d. The environments had random goal lo-
cations, and the obstacles were slid to random locations along the outside walls. The graph
shows the additional time the other algorithms require to achieve the same sub-optimality
bound that ARA* does. To make the results from different environments comparable we
normalize the bound by dividing it by the maximum of the best bounds that the algorithms
achieve before they run out of memory. Averaging over all environments, the time for
ARA* to achieve the best bound was 10.1 secs. Thus, the difference of 40 seconds at the
end of the Anytime A* graph corresponds to an overhead of about a factor of 4.

3.2 Outdoor Robot Navigation
For us the motivation for this work was efﬁcient path-planning for mobile robots in large
outdoor environments, where optimal trajectories involve fast motion and sweeping turns
at speed. In such environments it is particularly important to take advantage of the robot’s
momentum and ﬁnd dynamic rather than static plans. We use a 4D state space: xy position,
orientation, and velocity. High dimensionality and large environments result in very large
state-spaces for the planner and make it computationally infeasible for the robot to plan
optimally every time it discovers new obstacles or modelling errors. To solve this problem
we built a two-level planner: a 4D planner that uses ARA*, and a fast 2D (x, y) planner
that uses A* search and whose results serve as the heuristic for the 4D planner.1

1To interleave search with the execution of the best plan so far we perform 4D search backward.
That is, the start of the search, sstart, is the actual goal state of the robot, while the goal of the search,
sgoal, is the current state of the robot. Thus, sstart does not change as the robot moves and the search
tree remains valid in between search iterations. Since heuristics estimate the distances to sgoal (the
robot position) we have to recompute them during the reorder operation (line 10’, Figure 3).

In Figure 5 we show the robot we used for navigation and a 3D laser scan [3] con-
structed by the robot of the environment we tested our system in. The scan is converted
into a map of the environment (Figure 5c, obstacles shown in black). The size of the envi-
ronment is 91.2 by 94.4 meters, and the map is discretitized into cells of 0.4 by 0.4 meters.
Thus, the 2D state-space consists of 53808 states. The 4D state space has over 20 million
states. The robot’s initial state is the upper circle, while its goal is the lower circle. To
ensure safe operation we created a buffer zone with high costs around each obstacle. The
squares in the upper-right corners of the ﬁgures show a magniﬁed fragment of the map with
grayscale proportional to cost. The 2D plan (Figure 5c) makes sharp 45 degree turns when
going around the obstacles, requiring the robot to come to complete stops. The optimal
4D plan results in a wider turn, and the velocity of the robot remains high throughout the
whole trajectory. In the ﬁrst plan computed by ARA* starting at  = 2.5 (Figure 5e) the
trajectory is much better than the 2D plan, but somewhat worse than the optimal 4D plan.
The time required for the optimal 4D planner was 11.196 secs, whereas the time for
the 4D ARA* planner to generate the plan in Figure 5e was 556ms. As a result, the robot
that runs ARA* can start executing its plan much earlier. A robot running the optimal
4D planner would still be near the beginning of its path 25 seconds after receiving a goal
location (Figure 5d). In contrast, in the same amount of time the robot running ARA* has
advanced much further (Figure 5f), and its plan by now has converged to optimal ( has
decreased to 1).

4 Conclusions
We have presented the ﬁrst anytime heuristic search that works by continually decreasing
a sub-optimality bound on its solution and ﬁnding new solutions that satisfy the bound on
the way. It executes a series of searches with decreasing sub-optimality bounds, and each
search tries to reuse as much as possible of the results from previous searches. The exper-
iments show that our algorithm is much more efﬁcient than any of the previous anytime
searches, and can successfully solve large robotic planning problems.

Acknowledgments
This work was supported by AFRL contract F30602–01–C–0219, DARPA’s MICA pro-
gram.

References
[1] B. Bonet and H. Geffner. Planning as heuristic search. Artiﬁcial Intelligence, 129(1-

2):5–33, 2001.

[2] T. L. Dean and M. Boddy. An analysis of time-dependent planning. In Proc. of the

National Conference on Artiﬁcial Intelligence (AAAI), 1988.

[3] D. Haehnel. Personal communication, 2003.
[4] S. Koenig and M. Likhachev.

Incremental A*.

In Advances in Neural Information

Processing Systems (NIPS) 14. Cambridge, MA: MIT Press, 2002.

[5] R. E. Korf. Linear-space best-ﬁrst search. Artiﬁcial Intelligence, 62:41–78, 1993.
[6] M. Likhachev, G. Gordon, and S. Thrun. ARA*: Formal Analysis. Tech. Rep. CMU-

CS-03-148, Carnegie Mellon University, Pittsburgh, PA, 2003.

[7] J. Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving.

Addison-Wesley, 1984.

[8] R. Zhou and E. A. Hansen. Multiple sequence alignment using A*. In Proc. of the

National Conference on Artiﬁcial Intelligence (AAAI), 2002. Student abstract.

[9] S. Zilberstein and S. Russell. Approximate reasoning using anytime algorithms. In

Imprecise and Approximate Computation. Kluwer Academic Publishers, 1995.

