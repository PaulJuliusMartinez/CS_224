The Use of Pre-Conditioning over Irregular Regions Gene H. Golub'
Computer Science Department Stanford University
Stanford, California 94305 USA
David Mayers Oxford University Computing Laboratory
19 Parks Road Oxford OX1 3PL
England
Abstract Some ideas and techniques for solving elliptic pde.`s over irregular regions are discussed. .The basic idea is to break up the domain into subdomains and then to use the preconditioned conjugate gradient method for obtaining the solution over the entire domain. The solution of Poisson's equaton over a T-shaped region is described in some detail and a numerical example is given.
1. Introduction. In the last several years, there has been great interest in solving a variety of problems using
domain decomposition or svbstructuring (cf. [l], [2], [3], [4], 191). The basic idea is to break up a domain into subdomains and then to solve for an approximate solution on each subdomain. There - is a need then to "paste" the approximate solutions together to get the solution to the original problem.
It turns out that a very effective numerical procedure for obtaining the solution is the preconditioned conjugate gradient method (cf. [3]). It provides a particularly good method because advantage is taken of the eigenvalue structure of certain matrices.
It is obvious that the ideas of domain decomposition are quite applicable to parallel processing. We will not delve into that subject at this time.
We will concentrate on solving Poisson's equation on a T-shaped region. The general ideas expressed here will be applicable to a much wider class of problems.
2. The Pre-Conditioned Conjugate Gradient Method. Our discussion in this section closely follows that given in [3]. Consider the system of equations
(2 . 1) A x = b
where A is a symmetric, positive definite matrix. We re-write (2.1) as
Mx=Nx+b
where 1M is symmetric and positive difinite. We assume that given a vector d it is "easy" to solve the system
(2 l2) Ma=d.
* The work of this author was in part supported by NSF and the DOE. Invited Lecture at Sixth International Conference on Computing Methods in Applied Sciences and Engineering, Versailles, sponsored by INRIA, December 12-16, 1983.
1

The pre-conditioned conjugate gradient (CG) method proceeds as follows.

Algorithm. Let x(O) be a given vector and arbitrarily define p(-`) . For k = 0, 1, . . .
(1) Solve Ma(`) = b - AX(&) E r@) . (2) Compute
b0 = o ,

zWT M zV4 bk = z(k-l)T M zW-l)

k>l

p(k) = z(k) + bk p(w .

(3) Compute

X(k+l) = xck) + ak pck) . Note that in the computation of the numerator of f.zk and bk one need not compute Mz(~) since
M ztk) = rtk). Furthermore, the matrix A need not be known explicitly since we need compute the vector qtk) = A ~(~1. Finally, we note that the residual rtk+l) can be computed by the relationship
rCk+l) = rtk) - ak Ap(k) . The convergence properties of the conjugate gradient method are fairly well understood. Let
K = M - `A and
K: knsx(M-l A) = Amin(M-`A)
where A meX(M-l A) is the largest eigenvalue of the argument and Amin(M-'A) is the smallest. The following is known about the CG method.

(4

(~(~1 - x)~ A (~(`1 - x)_ < 4 (x((O) - x)T A (x(O) - x) -

" '

(B) If K = M-'A has p distinct eigenvalues then the CG method converges in at most p iterations. *
Thus if we wish to solve the system

(A+uuT)x=b (u f 0)
with M = A and N = -u uT, the CG method will converge in at most 2 iterations since M-`N is a matrix of rank one and hence has at most two distinct eigenvalues.
Our aim will be to devise matrices M so that K is as small as possible and that the matrix N be of low rank.
In many cases the matrix A can be written as

where the systems

Ml al = dl and Ml z3 = d2

are easy to solve and for such matrices, it is convenient to choose

Suppose we are given YC~' and we compute YC~'so that Max2(0) =r2(0) .
A short calculation shows that z$) = 0 and hence al = 1. Generalizing a result of Reid [7], it is shown in [3], then for j = 0, 1, . . .
(2 . 3) Using (2.3), we are able to eliminate roughly half the-number of operations (cf. [7]).
3. Some Preliminary Results. In many applications (cf. (51, [6]), matrices of the following structure arise quite naturally:

A= .

B1 B,l l l Q
The matrix A is symmetric and positive definite so that each A; (ni x ni) is also positive definite and so is the p x p matrix Q.
Suppose we wish to solve the system

(3 . 1) A x = b

where

Xl

x=

x2 .:

and b=

XT
it

One possibility is to apply the CG method to (3.1) with

bl
b2 .. .
dr CI

,O . A,`Al
M=
Q

3

Note the matrix M-`N is 2-cyclic and as indicated in Section 2 various simplifications can be made in the procedure and about half the computations can be eliminated.
Let us examine the eigenvalues of M -`N. Consider the matrix equation
U.l M-`Nu=h. 0Then if u = : , a short manipulation shows
UT
V
t x BTAflBiv=X'Qv.
i=l
Instead of solving (3.1) directly, we could eliminate xi, x2,. . . , xv and solve for c. This leads to the equation
(3.3)
We can apply the CG method to (3.3) and if Q is a p x p matrix we will obtain the solution in at most p iterations. The matrix
CSQ-2 BTAflBi
i=l
is the Schur complement of & in A and hence G is symmetric and positive definite. Associated with (3.3)) we can choose a pre-conditioner a. There are various choices of k
One possibility is to choose k = I. Perhaps a more natural pre-conditioner is G = Q. Note if

then the convergence properties of the algorithm are determined by the eigenvalues of A?i fi. Let

lwfiw=+yw.

Then if Q = Q, we have
(3 . 4)

cI BTAf'Biw=rQw.
i=l

Hence the cigenvalues of (3.2) are the squares of the eigcnvalues of (3.4). The rate of convergence of the two procedures are essentially the same because we are able to eliminate h,alf of the numerical operations. In Section 4, WC shall indicate how we can further improve the convergence properties
by choosing various pre-conditioners related to the differential equations.
In order tc solve (3.3) with a = Q, we need to solve the system

Q p) = ~+k BTAfl(Biq-bi)
i=l
4

where q is an approximation to the solution. Note that one need not compute the matrix Br Af'Bi but instead one solves the system
Aip; = Bi rj - bi .
If A; has a simple structure this can be solved without much difficulty.

4. The Model Problem. We shall explain the method by applying it to the problem studied in [3]. We wish to solve

(4 1).

- A u = f (z,y)~T `1L = 9 (z, Y) (5 m

where T is the T-shaped domain shown in Figure 1. We use a uniform square grid of size h, where l/2h is an integer N, and 0 < e < N. Using the standard 5-point finite difference approximation we obtain the system of equations in matrix form

(4.2) (&f g g(+(g).

The vector u comprises the unknown values of the solution at the interior points of the lower square, v comprises the unknowns in the interior of the upper square, and w comprises the unknown values at grid points on the dividing line. Thus P, Q, and R are square matrices of orders (2N- 1)2, (2e - 1)2, and (2e - 1) respectively.
There are several possible pre-conditioners one can use for solving (4.2). The obvious choices
-are

and PO0
The pre-conditioner M(l) requires that for each iteration two fast Poisson solvers are used whereas for pre-conditioner M(*I two fast Poisson solvers are used and a system of equations involving the matrix Q must be solved. Which pre-conditioner is to bc preferred? Using a thcorcm of Varga [8, pg. 901, we see that. the spectral radius is smaller when M(l) is used as the pre-conditioner and hence MC') is to be preferred in many situations. Theorem (Varga):
Let A = M(l) - N(`) = M(2) a N(2) b e two regular splittings of A where A-' > 0. If N(2) > N(l) 2 0, equality excluded, then
1 > p(M(2)-1N(2)) 2 p&f(`)-`N(`)) > 0.
Here p(M(`)-*IV(`)) denotes the spectral radius of the argument. Even though it appears that M(l) is to be preferred over Mt2), we will consider below a pre-conditioner related to Mt2) !
WC obtnin the capacitance matrix by eliminating u and v from these equations, giving
(4 . 3) (R - JTP-'J - KTQ-`K) w = b - JTP-`a - KTQmlc
5

which we shall write
(4 .4 C w = d .
As indicated in Section 3, it follows at once that C is symmetric and positive definite. We now wish to solve (4.4) by a pre-conditioned conjugate gradient method. If we can find a
suitable pre-conditioning matrix this iterative method converges very rapidly, and it will certainly terminate in at most (2e - 1) iterations, this being the order of the matrix C.
We now consider the computation of Cp @I. Returning to the original equation (4.2) it will be seen that the matrices P and Q correspond to the solution of the discrete form of Poisson's equation on the lower and upper squares respectively, given values on the boundaries of these squares. The matrices J and K consist of unit matrices, augmented by blocks of zeros. Hence to compute Cpck) we need to solve the discrete Laplace equation in the two squares, given zero boundary conditions at all boundary points, except at the points on the dividing line between the two squares, where the given boundary values are the elements of the vector ~(~1. The required vector Cptk) then consists.of the residuals of the 5-point formula evaluated at each of these dividing points. The two discrete Laplace problems are both Dirichlet problems on a rectangle, and can be solved very efficiently by a fast Poisson Solver, using some form of Fast Fourier Transform.
For the most rapid rate of convergence we wish the matrix M to be close to C, and in particular for the eigenvalues of M-`C to be clustered as closely as possible. If we examine the elements of C in some particular cases we find that it is quite close to a Toeplitz form, with the element Cii being mainly a function of Ii - jl only, and with the largest element on the diagonal, the elements decreasing quite rapidly as Ii - jl increases.
This suggests that the elements of C do not depend very much on the shape and size of the -two rectangles in Figure 1, and that we could find a useful approximation to C by letting the boundaries of the two squares move away to infinity. We must then find solutions of Laplace's equation in the two half planes. In this approximation the elements Cij are in fact dependent only on Ii - il, and the elements in each row are obtained by solving the discrete Laplace equation in a half plane, the solution being required to vanish at infinity, and also at all points on the axis except at the origin, at which it is equal to 1. We thus wish to solve

ul,d+l + 24 r,s-1+%+&a +'tlr--1,e - 4urs = 0, 9 > 0

ur, + 0 as r+bo,

andas

S-+00

ur,o = 0 (r # 0)

u(),o = 1.

Defining the generating function -

we thus obtain

The general solution of the recurrence relation for & is 6

where Al and A2 are arbitrary constants, and A; and A2 are the roots of P+ (;+5-4)1+1=0.

The condition at infinity eliminates one of these two terms, and the condition 40 = 1 determines the remaining arbitrary constant, giving the solution

(ha(t)= [2-;(tCf)-({2-;(t+~)}*-1)Li2]**

The solution in the other half plane is of course determined by symmetry, and the residuals at the grid points on the axis are given by

pr = UT- I,0 + Ur+l,O + Uf,l + %,-1 - w,o = ut-l&l + Ur+l,O + W,l - 4%,0

for which the generating function is

l

1w> [t= + 17 - 4 $bo + 241

(4 .5)

=-z(

[2-;(t+;)]`-1)`1'-

To determine pr we now expand e(t) in positive and negative powers of t; pr is then the coefficient of tr . The simplest way to do this is to write t = eie, t + $ = 2 cos 6, and then expand +(t) as a Fourier cosine series in cos 8. This gives

ti = -2{[? - CO8 e]* - l}"* 1
= 5Po +2Cp,cosrO.

It is then convenient to make the substitution 8 = 2a and obtain the final result

(4 . 6)

pr

=

1 27r

= I -;

-2

cosd[(i

-

cos

0)'-

l]"*

d6

=

-

4- * cos 2ra sin cr[l + sin* = /0

Q]`/* da.

These integrals are easily evaluated numerically for small values of r. For moderate and large values of r, it is sufficient to integrate twice by parts and obtain the asymptotic approximation

Pt

=32 +0

(

1 -$
>

.

A possible preconditioning matrix M is then obtained by writing

This matrix has in fact been found to give good convergence, but a minor modification improves it still further. Dryja [4] used as pre-conditioning matrix

where K is the matrix

2 -1

K=

-1 2 -1 -1 2 -1

...

In our notation this corresponds to a different generating function

b= [-t+2- f]li2.
This suggests replacing 2 - t - i by K in the generating function. A multiplying scale factor is unimportant for our purpose, so for comparison we use instead of Dryja's form, M = - (4K)`l*, and f?om( 4.5) we suggest trying also

-2{ (l+~K)2-l}1'2~-{4K+K2~1~2.
The eigenvalues and eigenvectors of the matrix K are well known, so it is a simple matter to write down the elements of these matrices giving three possible pre-conditioners:
Mi(j') = Pli-jl where pr is given by (4.6)

M,!i"`= -a gsin(F)sin(E) [l+sin*(g)] s i n ( $ )

.

where n is the order of the matrix, in our case (2! - 1). Here MC") is Dryja's pre-conditioner, and A4t3) is our modification. Owing to the particular form of M(*) and lM(3), the solution of a system of equations Mo = r is particularly simple in these two cases, and can be done by a simple application of the Fast Fourier Transform.
For the model problem of Figure 1 we can now evaluate the eigenvalues of the matrix M`-`C for each of the three prc-conditioning -matrices M. We have used ! = f N, with N = 8 and 16, giving two sets of matrices, of orders 7 and 15 respectively, for which the eigenvalues are given in Table 1. It will be seen that in each case the eigcnvalucs are clustered quite closely about. 1, the clustering being most marked for the matrix Mt3).
Table 2 gives some results which illustrate the rate of convergence of the method. The domain is as in Figure 1, and the function f and the boundary conditions !J are chosen to give a smooth
solution as in [3]. The entries in Table 2 are the values of ( zk, Mz") . which give an indication of how fast zk, and hence rk, is tending to zero. The initial approximation was constructed by simply taking w(O) = 0, corresponding to making u = 0 at these grid points.
Acknowledgment. Much of the work of the first author was performed while a guest at the Oxford University Computing Lab. He is pleased to thank Professor Leslie Fox for his generous hospitality.
The authors wish to thank Gerard Mcurant and I'cttcr Djostnd for their helpful comments.

.

8

(3 + th, 1 + 2t!h) Figure 1

Table 1 Eigenvalues of Me1 C, for three matrices M

M(1)

dimension 7 M(*)

M(3)

1.29279 1.04310 1.00953 1.00135 0.99771 0.98384 0.98265

1.40048 1.36048 1.29815 1.21928 1.13432 1.04073 2.93631.

1 .ooooo 1 .ooooo 0.99999 0.99968 0.99736 0.96727 0.91185

dimension 15

M(l)

M(*)

1.66158 1.15918 1.05216 1.01339 1.01081 1.00885 1.00617 1.09090 0.99844 0.99125 0.99092 0.98361
0.98262
0.97753
0.97735

1.41079 1.40058 1.38385 1.36098 1.33257 1.29930 1.26220 1.22217 1.18079 1.13894 1.09911 1.06133 1.02975 0.96949
0.89807

1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1 .ooooo 1.00000 1.00000 0.99995 0.99971 0.99731 0.98958 0.93837
0.88376

10

Table 2 Values of (a tk), Mz#) , for three pre-conditioning matrices A4

dimension 7

k M(l)

M(*)

M(3)

0.4, -1 0.9, -1 0.2, -4 0.7, -3 0.4, -8 0.4, -5 0.3, -12 0.3, -7
0.6, -10 0.2, -12

0.3, -2 0.5, -6 0.2, -11

dimension 15

k M(l)

M(*)

0.3 0.5, -3 0.2,.5 0.3, -9

0.1
0.1, -2 0.1, -4
0.1, -6
0.1, -8
0.1, -10

0.7, -2 0.3, -5 0.5, -9

11

References

.

[l] Bjorstad, P. and 0. Widlund, "Solving elliptic problems on regions partitioned into substructures,* in Elliptic Problem Solvers, G. Birkhoff and A. Schoenstadt, eds., Academic Press, New York, 1984.

[2] Buzbee, B. L., Dorr, F. W., George, J. A. and Golub, G. H., "The direct solution of the discrete Poisson equation on irregular region," SIAM J. Numer. Anal. 8 (1971), pp. 722-736.

(31 Concus, P., Golub, G. and O'Leary, D., "A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations," in Sparse Matrix Computation, J. Bunch and D. Rose, eds., Academic Press, New York, 1976, pp. 309-322.

[4] Dryja, M., "A capacitance matrix method for Dirichlet problems on polygonal region," Num. Math., (1982), pp. 51-64.

[5] Golub, G., H., Luk, F. and Pagano, M., "A large sparse least squares problem in photogrammetry," Proceedings of the 12th Annual Symposium on the Interface, University of Waterloo, May, 1979.

[6] Golub, G. H. and Plemmons, R., "Sparse least squares problems," Proceedings of the lRIA Fourth hternational Symposium on Computing Methods in Applied Science and Engineering, Versailles, France, Dec. 1979.

[7] Reid, J., "The use of conjugate gradients for systems of linear equations possessing `Property A'," SIAM J. Numer. Anal. 9 (1972), pp. 325-332.

[8] Varga, Richard S., Matrix Iterative Analysis, Prentice-Hall, Englewood Cliffs, New Jersy, 1962.

_ (9) Dingh, Q. V., Glowinski, R. and Periaux, J., "On the solution of elliptic problems by domain decomposition methods Applications," manuscript.

12

