Speech Denoising and Dereverberation Using Probabilistic Models
Hagai Attias John C. Platt Alex Acero Li Deng Microsoft Research 1 Microsoft Way Redmond, WA 98052
hagaia,jplatt,alexac,deng @microsoft.com
Abstract
This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.
1 Introduction
This paper presents a statistical-model-based algorithm for reconstructing a speech source from microphone signals recorded in a stationary noisy reverberant environment. Speech enhancement in a realistic environment is a challenging problem, which remains largely unsolved in spite of more than three decades of research. Speech enhancement has many applications and is particularly useful for robust speech recognition [7] and for telecommunication. The difficulty of speech enhancement depends strongly on environmental conditions. If a speaker is close to a microphone, reverberation effects are minimal and traditional methods can handle typical moderate noise levels. However, if the speaker is far away from a microphone, there are more severe distortions, including large amounts of noise and noticeable reverberation. Denoising and dereverberation of speech in this condition has proven to be a very difficult problem [4]. Current speech enhancement methods can be placed into two categories: singlemicrophone methods and multiple-microphone methods. A large body of literature exists on single-microphone speech enhancement methods. These methods often use a probabilistic framework with statistical models of a single speech signal corrupted by Gaussian noise [6, 8]. These models have not been extended to dereverberation or multiple microphones. Multiple-microphone methods start with microphone array processing, where an array of microphones with a known geometry is deployed to make both spatial and temporal measurements of sounds. A microphone array offers significant advantages compared to single microphone methods. Non-adaptive algorithms can denoise a signal reasonably well, as

long as it originates from a limited range of azimuth. These algorithms do not handle reverberation, however. Adaptive algorithms can handle reverberation to some extent [4], but existing methods are not derived from a principled probabilistic framework and hence may be sub-optimal.

Work on blind source separation has attempted to remove the need for fixed array geometries and pre-specified room models. Blind separation attempts the full multi-source, multimicrophone case. In practice, the most successful algorithms concentrate on instantaneous noise-free mixing with the same number of sources as sensors and with very weak probabilistic models for the source [5]. Some algorithms for noisy non-square instantaneous mixing have been developed [1], as well as algorithms for convolutive square noise-free, mixing [9]. However, the full problem including noise and convolution has so far remained open.

In this paper, we present a new method for speech denoising and dereverberation. We use the framework of probabilistic models, which allows us to integrate the different aspects of the whole problem, including strong speech models, environmental noise and reverberation, and microphone arrays. This integration is performed in a principled manner facilitating a coherent unified treatment. The framework allows us to produce a Bayes-optimal estimation algorithm. Using a strong speech model leads to computational intractability, which we overcome using a variational approach. The computational efficiency is further enhanced by working in the frequency domain and by employing conjugate priors. The
resulting algorithm has complexity Ç´Æ ÐÓ Æ µ. Results on noisy speech show significant
improvement over standard methods.

Due to space limitations, the full derivation and mathematical details for this method are provided in the technical report [3].

Notation and conventions. We work with time series data using a frame-by-frame analysis

 sweixgittnehnaÆldsi.n-pWgooihnveetnrfrÒÒamiseas¼l.soThouÆmsi,ttael½dl.,sWÝigidnteahnlstohtaeenssduapsllyerssstiegcmrniapslt,sea.otgm.aliÝltttÒeidm, ,heÝapvÒoedianenttsoi.mteSesuapploelrimnsctircsirupobtpsshcmorianpyet

become subscripts and vice versa when no confusion arises. The discrete Fourier transform

È  (DFT) of ÜÒ is Ü

Ò ÜÔ´ ÒµÜÒ. We define the primed quantity

¼

Ô
½ 

 

ÒÒ

(1)

Ò½

for variables Ò with Ò ½ Ô.

ÆThe Gaussian distribution for a random vector with mean and precision matrix Î (de-

fined as the inverse covariance matrix) is denoted ´

Î µ. The Gamma distribution for

»  a

non-negative
´ « ¬µ

ra«nd¾ om½

variable
ÜÔ´ ¬

with « degrees of freedom and inverse scale ¬ is denoted
¾µ. Their product, the Normal-Gamma distribution

Æ ´ Î « ¬µ Æ´ Î µ ´ « ¬µ

(2)

turns out to be particularly useful. Notice that it relates the precision of to .

Problem Formulation We consider the case where a single speech source is present and
Å microphones are available. The treatment of the single-microphone case is a special
case of Å ½, but is not qualitatively different.
Let ÜÒ be the signal emitted by the source at time Ò, and let ÝÒ be the signal received at
microphone at the same time. Then

ÝÒ Ò ÜÒ · ÙÒ

ÑÜÒ Ñ · ÙÒ

Ñ

(3)

where Ñ is the as it propagates

impulse response of toward microphone

the ,

filter (of length Ã
is the convolution

Æ ) operating operator, and Ù

on the source Ò denotes the

noise recorded at that microphone. Noise may originate from both microphone responses and from environmental sources.
In a given environment, the task is to provide an optimal estimate of the clean speech signal
Ü from the noisy microphone signals Ý . This requires the estimation of the convolving filters and characteristics of the noise Ù . This estimation is accomplished by Bayesian inference on probabilistic models for Ü and Ù .

2 Probabilistic Signal Models

We now turn to our model for the speech source. Much of the work on speech denoising in the past has usually employed very simple source models: AR or ARMA descriptions [6]. One exception is [8], which uses an HMM whose observations are Gaussian AR models. These simple denoising models incorporate very little information on the structure of speech. Such an approach a priori allows any value for the model coefficients, including values that are unlikely to occur in a speech signal. Without a strong prior, it is difficult to estimate the convolving filters accurately due to identifiability. A source prior is especially
important in the single microphone case, which estimates Æ clean samples plus model coefficients from Æ noisy samples. Thus, the absence of a strong speech model degrades
reconstruction quality.

The most detailed statistical speech models available are those employed by state-of-the-
art speech recognition engines. These systems are generally based on mixture of diagonal
Gaussian models in the mel-cepstral domain. These models are endowed with temporal
Markov dynamics and have a very large ( ½¼¼¼¼¼) number of states corresponding to
individual atoms of speech. However, in the mel-cepstral domain, the noisy reverberant
speech has a strong non-linear relationship to the clean speech.

Physical speech production model. In this paper, we work in the linear time/frequency

domain using a statistical model and take an intermediate approach regarding the model
size. We model speech production with an AR(Ô) model:

Ô
ÜÒ ÑÜÒ Ñ · ÚÒ
Ñ½

(4)

where the coefficients Ñ are related to the physical shape of a "lossless tube" model of the vocal tract.

To turn this physical model into a probabilistic model, we assume that Ú Ò are indepen-

dent zero-mean Gaussian variables with scalar precision . Each speech frame Ü

´Ü¼
of Ü

is

ÜÆ ½µ has
generally a

its own parameters zero-mean Gaussian,

´ Ô´Ü

½µ

ÆÔ

µ. ´Ü

Given
¼ £µ,

, the where

joint
£ is

tdhiestÆribu¢tioÆn

precision matrix. Specifically, the joint distribution is given by the product

Ô´Ü µ Æ ´ÜÒ

ÑÜÒ Ñ µ

ÒÑ

(5)

Probabilistic model in the frequency domain. However, rather than employing this prod-

uct form directly, we work in the frequency domain and use the DFT to write

Ô´Ü

Æ  ½
µ » ÜÔ´ ¾Æ

¼ ¾ Ü ¾µ

(6)

¼

Èwhere
´ Æµ

¼

is

defined in
´Ò Ñµ

(1).

The
¼¾

precision matrix £ is now
. This matrix belongs to

given by an a sub-class

inverse DFT, £ ÒÑ
of Toeplitz matrices

called circulant
to via Ë

Toeplitz.
Ü¾

IÆt fo´llows¼

from
¾µ.

(6)

that

the

mean

power

spectrum

of

Ü

is

related

Conjugate priors. To complete our speech model, we must specify a distribution over the

Æspeech production parameters . We use a Ë-state mixture model with a Normal-Gamma

distribution (2) for
«× ¬×µ. This form
follows. Given the

each component × ½ Ë: Ô´ ×µ ´ ½ Ô ×
is chosen by invoking the idea of a conjugate prior, which is
model Ô´Ü µÔ´ ×µ, the prior Ô´ ×µ is conjugate to Ô´Ü

Î×µ ´
defined as
µ iff the

posterior Ô´ Ü ×µ, computed by Bayes' rule, has the same functional form as the prior.

This choice has the advantage of being quite general while keeping the clean speech model

analytically tractable.

¢It turns out, as discussed below, that significant computational savings result if we restrict

htohafeviÔÒ×ngÎtoÒÔ×Ñipm,rpeaconisdseiwoanonrmekxaipntlriitcchieetscfoÎren×qsuttoreanihncaytv, edwoaemrceairpicnau:rlaamntetTroizeepÔli´tz

structure. To do
×µ in terms of

Ò×thisÒ×winitshteoaudt

Ô´

Ô ½
×µ » ÜÔ´ ¾Ô

×

 

×

¾µ ¡

 

«×
¾

ÜÔ´ 

¬×
¾

µ

¼

(7)

ÈNote that we use a Ô- rather than Æ -point DFT. The precisions are now given by the inverse

DFT ÎÒ×Ñ ´½ Ôµ

´Ò Ñµ × ¾ and are manifestly circulant. It is easy to show

that conjugacy still holds.

Finally, the mixing fractions are given by Ô´×µ

our clean speech model Ô´Üµ in terms of the ×µÔ´×µ. The model is parametrized by Ï

la´teÑ×nt

vaÑ×r×ia.«bT×leh¬ims×ocdoem×lµp.Ôl´eÜtes

the
×µ

specification
Ô´Ü µÔ´

of

Speech model training. We pre-train the speech model parameters Ï using ½¼¼¼¼ sen-

tences of the Wall Street Journal corpus, recorded with a close-talking microphone for 150
male and female speakers of North American English. We used ½ msec overlapping frames with Æ ¾ time points at ½ kHz sampling rate. Training was performed using an EM algorithm derived specifically for this model [3]. We used Ë ¾ clusters and Ô ½¾.
Ï were initialized by extracting the AR(Ô) coefficients from each frame using the autocor-

relation method. These coefficents were converted into cepstral coefficients, and clustered
into Ë classes by -means clustering. We then considered the corresponding hard clusters
of the AR(Ô) coefficients, and separately fit a model Ô´ ×µ (7) to each. The resulting

parameters were used as initial values for the full EM algorithm.

ÈNoise model. In this paper, we use an AR(Õ) description for the noise recorded by micro-

phone , ÙÒ

Ñ ÑÙÒ Ñ · ÛÒ. The noise parameters are

´ Ñ µ, where are

the precisions of the zero-mean Gaussian excitations ÛÒ. In the frequency domain we have

the joint distribution

Ô´Ù

Æ  ½
µ » ÜÔ´ ¾Æ

¼

¾ Ù ¾µ

¼

(8)

As in (6), the parameters determine the spectra of the noise. But unlike the speech
model, the AR(Õ) noise model is chosen for mathematical convenience rather than for its
relation to an underlying physical model.

Noisy speech model. The form (8) now implies that given the clean speech Ü, the distribution of the data Ý is

Ô´Ý

Æ  ½
Üµ » ÜÔ´ ¾Æ

¼

¾Ý  

Ü ¾µ

¼

(9)

ÉThis completes the specification of our noisy speech model Ô´Ýµ in terms of the joint dis-
tribution Ô´Ý ÜµÔ´Ü µÔ´ ×µÔ´×µ.

3 Variational Speech Enhancement (VSE) Algorithm

The denoising and dereverberation task is accomplished by estimating the clean speech Ü,
which requires estimating the speech parameters , the filter coefficients , and the noise
parameters . These tasks can be performed by the EM algorithm. This algorithm receives
the data Ý from an utterance (a long sequence of frames) as input and proceeds iteratively. In the E-step, the algorithm computes the sufficient statistics of the clean speech Ü and
the production parameters for each frame. In the M-step, the algorithm uses the suffi-
cient statistics to update the values of and , which are assumed unchanged throughout
the utterance. This assumption limits the current VSE algorithm to stationary noise and
reverberation. Source reconstruction is performed as a by-product of the E-step.
Intractability and variational EM. In the clean speech model Ô´Üµ above, inference (i.e. computing Ô´× Üµ for the observed clean speech Ü) is tractable. However, in the
noisy case, Ü is hidden and consequently inference becomes intractable. The posterior
Ô´× Ü Ýµ includes a quartic term ÜÔ´Ü¾ ¾µ, originating from the product of two Gaus-
sian variables, which causes the intractability.

To overcome this problem, we employ a variational approach [10]. We replace the exact
posterior distribution over the hidden variables by an approximate one, Õ´× Ü Ýµ, and
select the optimal Õ by maximizing

Õ

×

Ü

Õ´×

Ü

Ýµ ÐÓ

Ô´× Õ´×

Ü Ýµ Ü Ýµ

(10)

w.r.t. Õ. To achieve tractability, we must restrict the space of possible Õ. We use the partially

factorized form

Õ Õ´×µÕ´ ×µÕ´Ü ×µ

(11)

where the Ý-dependence of Õ is omitted. Given Ý, this distribution defines a mixture model for Ü and a mixture model for , while maintaining correlations between Ü and (i.e.,
Õ´Ü µ Õ´ÜµÕ´ µ). Maximizing is equivalent to minimizing the KL distance between Õ and the exact conditional Ô´× Ü Ýµ under the restriction (11).

With no further restriction, the functional form of Õ falls out of free-form optimization, as
shown in [2]. For the production parameters, Õ´ ×µ turns out to have the form Õ´ ×µ

Æ´
Ô´

½ ×µ,

Ô × Î×µ ´ «× ¬×µ. This form is functionally identical to that of the prior
consistent with the conjugate prior idea. The parameters of Õ are distinguished

Æfrom the
the clean

prior's by the symbol. Similarly,
speech, we obtain Gaussians, Õ´Ü

the
×µ

state

responsibilities are
´Ü × £×µ, with

Õ´×µ ×. For
state-dependent

means and precisions.

E-step and Wiener filtering. To derive the E-step, we first ignore reverberation by setting
Ò ÆÒ ¼ and assuming a single microphone signal ÝÒ, thus omitting . The extension to
multiple microphones and reverberation is straightforward.

The parameters of Õ are estimated at the E-step from the noisy speech in each frame, using
an iterative algorithm. First, the parameters of Õ´ ×µ are updated via

where ÊÒ× Ñ

ÈÎ× Ê× · Î×

× Î× ½´Ö× · Î× ×µ

(12)

´½ Æµ

´Ò Ñµ ×´ Ü ¾µ, ÖÒ× ÊÒ× ¼, and × denotes averaging

w.r.t. Õ´Ü ×µ, which is easily done analytically. The update rules for « ×, ¬×, × are shown

in [3].

Next, the parameters of Õ´Ü ×µ are obtained by inverse DFT via

Ò×

½ Æ ½
Æ

¼

Ò ×Ý

£Ò× Ñ

½ Æ ½
Æ

¼

´Ò Ñµ ×

(13)

Èwhere ×
w.r.t. Õ´

×µ.

¼¾
These

×, and steps are

× iterated

¼ ¾ · ×´ ¼ ¾µ. Here
to convergence, upon which

× denotes averaging the estimated speech

signal for this frame is given by the weighted sum Ü

× × ×.

We point out that the correspondence between AR parameters and spectra implies the
Wiener filter form × Ë× ´Ë× · Æ µ, where Ë× is the estimated clean speech spec-
trum associated with state ×, and Æ is the noise spectrum, both at frequency . Hence,

the updated × in (13) is obtained via a state-dependent Wiener filter, and the clean speech is estimated by a sum of Wiener filters weighted by the state responsibilities. The same

Wiener structure holds in the presence of reverberation. Notice that, whereas the conven-

tional Wiener filter is linear and obtained directly from the known speech spectrum, our

filters depend nonlinearly on the data, since the unknown speech spectra and state respon-

sibilities are estimated iteratively by the above algorithm.

M-step. After computing the sufficient statistics of Ü for each frame, and are

updated using the whole utterance. The update rules are shown in [3]. Alternatively, the

can be estimated directly by maximum likelihood if a non-speech portion of the input

signal can be found.

Computational savings. The complexity of the updates for Õ´Ü ×µ and Õ´ ×µ is Æ ÐÓ Æ and ËÔ ÐÓ Ô, respectively. This is due to working in the frequency domain, using

the FFT algorithm to perform the DFT, and by using conjugate priors and circulant preci-

sions. Working in the time domain and using priors with general precisions would result in
the considerably higher complexity of Æ ¾ and ËÔ¿, respectively.

4 Experiments
Denoising. We tested this algorithm on ½ ¼ speech sentences by male and female speakers
from the Wall Street Journal (WSJ) database, which were not included in the training set. These sentences were distorted by adding either synthetic noise (white or pink), or noise recorded in an office environment with a PC and air conditioning. The distortions were applied at different SNRs. All of these noises were stationary. We then applied the algorithm to estimate the noise parameters and reconstruct the original speech signal. The result was compared with a sophisticated, subband-based implementation of the spectral subtraction (SS) technique.
Denoising & Dereverberation. We tested this algorithm on ½¼¼ WSJ sentences, which
were distorted by convolving them with a 10-tap artificial filter and adding synthetic white Gaussian noise at different SNRs. We then applied the algorithm to estimate both the noise
 level and the filter. Here we used a simpler speech model with Ô´ ×µ Æ´ ×µ.
Speech Recognition. We also examined the potential contribution of this algorithm to robust speech recognition, by feeding the denoised signals as inputs to a recognition system. The system used a version of the Microsoft continuous-density HMM (Whisper), with
¼¼¼ tied HMM states (senones), ¾¼ Gaussians per state, and the speech represented via
Mel-cepstrum, delta cepstrum, and delta-delta cepstrum. A fixed bigram language model
is used in all the experiments. The system had been trained on a total of ½ ¼¼¼ female clean speech sentences. The test set consisted of ½ female WSJ sentences, which were
distorted by adding synthetic white non-Gaussian noise. The word error rate was 55.06% under the training-test mismatched condition of no preprocessing on the test set and decoding by HMMs trained with clean speech. This condition is the baseline for the relative performance improvement listed in the last row of Table 1. For these experiments, we compared VSE to the SS algorithm described in [7].
Table 1 shows that the Variational Speech Enhancement (VSE) algorithm is superior to SS at removing stationary noise either measured via SNR improvement or via relative reduction in speech recognition error rate (compared to baseline).

SNR improvement SNR improvement SNR improvement SNR improvement Speech recognition relative improvement

dB noise added
5 10 5 10
10

reverb added
No No Yes Yes
No

SS synthetic
noise 4.3 4.1 6.7 8.3
38.6%

SS real noise 4.3 4.1

VSE synthetic
noise 6.0 5.8 10.2 13.2
65.1%

VSE real noise 5.5 5.1

Table 1: Experimental Results.

5 Conclusion
We have presented a probabilistic framework for denoising and dereverberation. The framework uses a strong speech model to perform Bayes-optimal signal estimation. The parameter estimation and the reconstruction of the signal are performed using a variational EM algorithm. Working in the frequency domain and using conjugate priors leads to great computational savings. The framework applies equally well to one-microphone and multiple-microphone cases. Experiments show that the optimal estimation can outperform standard methods such as spectral subtraction. Future directions include adding temporal dynamics to the speech model via an HMM structure, using a richer adaptive noise model (e.g. a mixture), and handling non-stationary noise and filters.
References
[1] H. Attias. Independent factor analysis. Neural Computation, 11(4):803­851, 1999.
[2] H. Attias. A variational bayesian framework for graphical models. In T. Leen, editor, Advances in Neural Information Processing Systems, volume 12, pages 209­215. MIT Press, 2000.
[3] H. Attias, J. C. Platt, A. Acero, and L. Deng. Speech denoising and dereverberation using probabilistic models: Mathematical details. Technical Report MSR-TR-200102, Microsoft Research, 2001. http://research.microsoft.com/ hagaia.
[4] M. S. Brandstein. On the use of explicit speech modeling in microphone array applications. In Proc. ICASSP, pages 3613­3616, 1998.
[5] J.-F. Cardoso. Infomax and maximum likelihood for source separation. IEEE Signal Processing Letters, 4(4):112­114, 1997.
[6] A. Dembo and O. Zeitouni. Maximum a posteriori estimation of time-varying ARMA processes from noisy observations. IEEE Trans. Acoustics, Speech, and Signal Processing, 36(4):471­476, 1988.
[7] L. Deng, A. Acero, M. Plumpe, and X. D. Huang. Large-vocabulary speech recognition under adverse acoustic environments. In Proceedings of the International Conference on Spoken Language Processing, volume 3, pages 806­809, 2000.
[8] Y. Ephraim. Statistical-model-based speech enhancement systems. Proc. IEEE, 80(10):1526­1555, 1992.
[9] J. C. Platt and F. Faggin. Networks for the separation of sources that are superimposed and delayed. In J. E. Moody, editor, Advances in Neural Information Processing Systems, volume 4, pages 730­737, 1992.
[10] L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean field theory of sigmoid belief networks. J. Artificial Intelligence Research, 4:61­76, 1996.

