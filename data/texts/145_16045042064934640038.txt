Appeared in Proceedings of the Tenth National Conf. on Arti cial Intelligence, pp. 171{176, MIT Press, 1992.

Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta
Richard S. Sutton
GTE Laboratories Incorporated Waltham, MA 02254 sutton@gte.com

Abstract
Appropriate bias is widely viewed as the key to e cient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system|the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate testbeds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact nds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I o er a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation.
Introduction
People can learn very rapidly and generalize extremely accurately. Information theoretic arguments suggest that their inferences are too rapid to be justi ed by the data that they have immediately available to them. People can learn as fast as they do only because they bring to the learning situation a set of appropriate biases which direct them to prefer certain hypotheses over others. To approach human performance, machine learning systems will also need an appropriate set of biases. Where are these to come from?
Although this problem is well known, there are few general answers. The eld of pattern recognition has long known about the importance of feature selection,

and the importance of representations is a recurring theme in AI. But in both of these cases the focus has always been on designing in a good bias, not on acquiring one automatically. This has resulted in an accumulation of specialized and non-extensible techniques. Is there an alternative? If bias is what a learner brings to a learning problem, then how could the learner itself generate an appropriate bias? The only way is to generate the bias from previous learning experience (e.g., Rendell, Seshu, & Tcheng 1987). And this is possible only if the learner encounters a series of di erent problems requiring the same or similar biases. I believe that is a correct characterization of the learning task facing people and real-world learning machines.
In this paper, I present a new algorithm for learning appropriate biases for a linear learning system based on previous learning experience. The new algorithm is an extension of the Delta-Bar-Delta algorithm (Jacobs 1988 Sutton 1982 Barto & Sutton 1981 Kesten 1958) such that it is applicable to incremental tasks|supervised learning tasks in which the examples are processed one by one and then discarded. Accordingly, I call the new algorithm the Incremental Delta-Bar-Delta (IDBD) algorithm. The IDBD algorithm can be used to accelerate learning even on single problems, and that is the primary way in which its predecessors have been justi ed (e.g., Jacobs 1988 Silva & Almeida 1990 Lee & Lippmann 1990 Sutton 1986), but its greatest signi cance I believe is for nonstationary tasks or for sequences of related tasks, and it is on the former that I test it here.
The IDBD Algorithm
The IDBD algorithm is a meta-learning algorithm in the sense that it learns the learning-rate parameters of an underlying base learning system. The base learning system is the Least-Mean-Square (LMS) rule, also known as the delta rule, the ADALINE, the RescorlaWagner rule, and the Widrow-Ho rule (see, e.g., Widrow & Stearns 1985). This learning system is often thought of as a single connectionist unit as shown in gure 1. The unit is linear, meaning that its output y(t), at each time step (example number) t, is a

y



W1 W2

Wn

X1 X2

Xn

Figure 1: The base-level learning system is a single linear unit using the LMS or delta rule.

weighted sum of its real-valued inputs xi(t):
Xn
y(t) = ( )wi t xi(t)
i=1

(1)

wplweueeatrirg,enhyeetr(atwrc)eh,iceaawinsvisd(eots)ccaioaimssteetptdhaoerwfeviistnahilptuuxettiosa.,taxAtigti(mitve)ea,enccthodomtefispmaiureetmdessotodietupist,opatubuhtltee-,

y (t). error

T2(hte),

aim of where

learning is to minimize (t) = y (t) ; y(t), on

the squared future time

steps. The LMS learning rule updates the weights at

each time step according to:

(wi t + 1) = wi(t) + (t)xi(t)

(2)

where is a positive constant called the learning rate.

In the IDBD algorithm there is a di erent learning

1rtoa9t8ea6,)m. iTe,thfaoe-rlbeeaaarscneh-inliengvpepulrtloexcaeir,snsainn(gdcfrt.uhleeHseiasmc1hpasnogne

according & Volper

(wi t + 1) = wi(t) + i(t + 1) (t)xi(t): (3)

The learning rates are a powerful form of bias in this

system. Learning about irrelevant inputs acts as noise,

interfering with learning about relevant inputs. A

rough rule of thumb is that learning time is propor-

tional to the sum of the squares of the learning rates

(assuming all inputs have equal variance see Widrow

& Stearns 1985). In e ect, learning rates are a valu-

able resource that must be distributed carefully. In-

puts that are likely to be irrelevant should be given

small learning rates, whereas inputs that are likely to

be relevant should be given large learning rates.

In the IDBD algorithm, the learning rates are all of

the form

i(t)

=e

i

(t) :

(4)

1The indicate

thiatarteheinirdeuxpeddatbey,

t+1 by a

rather process

than t simply to described below,

occurs before the wi update (see gure 2).

This exponential relationship between the learning

itrmwittahisaoteiyesnicds,uoiarfirnemaiiedc,sner,wsteacuimhnlrviladi,aennmhlngutiatshoeetmse,vhdteeamwf.tuoguoper.p,maimuadowonparvridkaylodlinnradoptglwaodawwgrognaaewenymsobn.smybebFbtyeeeayitrrarps1ittoc0x,hs%esxiiatttdet.iedivpsfTierss.asahtaicSiennsctepaitcio-utsosnuiaind:zrldloeaeiy-,f,fl

sirable others

rbeemcaauinselasrogme enoi

must become xed step-size

very small while would work well

for all The

tIhDeBDi.

algorithm

updates

the

i by

i(t + 1) = i(t) + (t)xi(t)hi(t)

(5)

where is a positive constant, the meta-learning rate,

h iuanpddahteidisbayn additional per-input memory parameter
hi(t+1) = hi(t) 1; i(t+1)x2i (t) ++ i(t+1) (t)xi(t)

where

x]+ is x, if x > 0, else 0.

The

(6) rst term

iss(tnoechifce(T.ottthhchn(+u3eiedsm)1)iattn.)uebextrlrouaTi2mmvit(hteitive)icvneeaeimcqussruiuseednaemmmeostairoeooamnrbnfytadershlieehlshcciyinaeiadniyzsbetdtryotchoehfhcetuaahohsyInrieDgaatlteBoaesdwpsrDetmtoacosrwaaidltywegstiiivihong.zerehgeittfrptrhorcarm.ahoccatdTeiinuoshgncoaeetf,

simple one. portional to

Note that the increment to the product of the current

wieiingh(t5)chisapnrgoe-,

Bb(eytc)oaxmcic(euts)m,puarlnoadptionargttitrohanciaselpotrofodrteuhcceetn,ctotrhwreeelioagvtheitoranclhlbacenhtgawenesge,enhiicn(utr)-i.

rent and recent weight changes. If the current step

is positively correlated with past steps, that indicates

that the past steps should have been larger (and equa-

tisionneg(5a)tivaeclcyorcdoirnrgellayteindcrweiatshespasit).stIefptsh, ethcautrrienndticsatteeps

that the past steps were too large the algorithm is

overshooting the best weight values and then having

to re-correct in the opposite direction (here equation

(5)Tdheecrienatsueistivei).idea of the IDBD algorithm as de-

scribed above is the same as that of Jacob's (1988)

Delta-Bar-Delta algorithm. The primary di erence is

that Jacobs's algorithm can be applied only on a batch-

by-batch basis, with updates after a complete presen-

tation of a training set, whereas here we assume exam-

ples arrive one-by-one and are not necessarily revisited

afterwards. The key to making the new algorithm in-

aiictnrlgegfmoairdneieptnshutmatalwxaiaislysitsohopeinmrlewyspaertynootvt,tehhaseesotivernxeadrtcieecJnaahttcietodhibsasbt'dysetxihnni2e(ettcdh)o.arstrTuecthshhpeeotnnhdedaewt--

cay rate is not a separate free parameter, but is tied to

the current learning rate. The new algorithm in fact

has only one free parameter, the meta-learning rate, ,

whereas Jacobs's algorithm has three free parameters.

Initialize hi to 0, and wi i as desired, i = 1 : : : n
PRepeat for each new example (x1 : : : ,xn y ):
;y in=1 wixi yy
Repeat for i = 1 : : : n: +i i xihi

i ei

;+wi wi + 1 ] +hi hi

i xiixi2 +

i xi

Figure 2: The IDBD Algorithm in Pseudocode

On the other hand, Jacobs's algorithm was designed

for nonlinear networks. While I do not foresee great

di culties extending the IDBD algorithm to the non-

linear case, that is beyond the scope of this paper.

In practice, below by, say,

i;t 1is0,ofttoenpruesveefnutl

taoribthomunedticeaucnhderi

from ows.

In addition, it is any one step to,

prudent to limit the say, 2. However,

tchheasne gbeoiunndiinogns

were not required to obtain the empirical results pre-

sented in the next section.

Results
The capabilities of the IDBD algorithm were assessed using a series of tracking tasks|supervised-learning or concept-learning tasks in which the target concept drifts over time and has to be tracked (cf. Schlimmer 1987). Non-stationary tasks are more appropriate here than conventional learning tasks because we are trying to assess the IDBD algorithm's ability to learn biases during early learning and then use them in later learning. To study this one needs a continuing learning problem, not one that can be solved once and is then nished.

Experiment 1: Does IDBD help?

Experiment 1 was designed to answer the question:

Does the IDBD algorithm perform better than the or-

dinary LMS algorithm without IDBD? The task in-

volved 20 real-valued inputs and one output. The in-

puts were chosen independently and randomly accord-

ing to a normal distribution with mean zero and unit

variance. The target concept was ve inputs, each multiplied either

bthye+s1umoro;f 1th, ei.e.:rst

y = s1x1 + s2x2 + s3x3 + s4x4 + s5x5

+ 0x6 + 0x7 + + 0x20

wtrhaeckreinagllptrhoeblesmi a, reeveerityhe2r0

+1 or ;1. examples

To one

make of the

it a ve

+si1wtaos

;se1le, cotredviacte

random versa.

and switched in Thus, the same

sign, from ve inputs

were always relevant, but their relationship to the tar-

get concept occasionally changed. If the IDBD algo-

rithm can successfully identify which inputs are rele-

Asymptotic Error

0 0.02 0.04 0.06 0.08
8
6 LMS()
4
2 IDBD()
0 0.005 0.01 0.015 0.02
Figure 3: Comparison of the average asymptotic performances of IDBD and LMS algorithms over the relevant ranges of their step-size parameters ( , upper axis, for LMS, and , lower axis, for IDBD). The IDBD algorithm results in less than half the level of error over a broad range of values of its step-size parameter. For parameter values above those shown, both algorithms can become unstable.
vant, then it should be able to track the drifting target function more accurately than ordinary LMS.
Because this is a tracking task, it su ces to perform one long run and measure the asymptotic tracking performance of the algorithms. In this experiment I ran each algorithm for 20,000 examples so as to get past any initial transients, and then ran another 10,000 examples. The average mean-squared error over that 10,000 examples was used as the asymptotic performance measure of the algorithm. The algorithms used were ordinary LMS with a range of learning rates and the IDBD algorithm with a range of meta-learning rshauatcsehsn.tohTaahteeciti=oinn0:ta0hs5ye,mIfDoprBtoaDtlilcailp(geborurfiottrhommf acnwouceerr)se.estehtiisncithioailclye
The results for both algorithms are summarized in gure 3. With its best learning rate, ordinary LMS attains a mean squared error of about 3.5, while the IDBD algorithm performs substantially better over a wide range of values, attaining a mean squared error of about 1.5. The standard errors of all of these means are less that 0.1, so this di erence is highly statistically signi cant. The IDBD algorithm apparently learns biases (learning rates) that enable substantially more accurate tracking on this task.
Exp. 2: Does IDBD nd the optimal ?i
Experiment 1 shows that the IDBD algorithm nds a distribution of learning rates across inputs that is better than any single learning rate shared by all, but it does not show that it nds the best possible distribution of learning rates. While this may be di cult to show as a general result, it is relatively easy to con rm empirically for special cases. To do this for the task used in Experiment 1, I chose a small value for the

LEARNING RATE ()
Asymptotic Error

0.15

2.2

0.1 RELEVANT

2.0 Performance
1.8 with fixed 's

0.05

1.6

IRRELEVANT

1.4

0 0 50 100 150 200 250
TIME STEPS (1000's of Examples)

0.05 0.10 0.15 0.20
's of Relevant Inputs

Figure 4: Time course of learning-rate parameters, under IDBD, for one relevant and one irrelevant input.

meta-learning rate, = 0:001, and ran for a very large

number of examples (250,000) to observe the asymp-

totic distribution of learning rates found by the algo-

rithm (as before, the learning rates were initialized to

0.05). Figure 4 shows the behavior over time of two of

the vant

iin,pounte.

for

a

relevant

input

and

one

for

an

irrele-

After 250,000 steps, the learning rates for the 15 ir-

relevant inputs were all found to be less than 0.007

while the learning rates for the 5 relevant inputs were

all 0:13 0:015. The learning rates for the irrelevant in-

puts were apparently heading towards zero (recall that

they cannot be exactly zero clearly optimal, but what of

uthnelerseslevia=nt;in1pu),tsw?hTichheiys

should all share the same optimal learning rate, but

is it 0:13, as found by the IDBD algorithm, or is it

some other value? We can determine this empirically

simply by trying various sets of xed learning rates.

The irrelevant inputs were all given xed zero learning

rates and the relevant inputs were xed at a variety of

values between 0.05 and 0.25. Again I ran for 20,000

examples, to get past transients, and then recorded the

average squared error over the next 10,000 examples.

The results, plotted in gure 5, show a clear minimum

somewhere near 0:13 0:01, con rming that the IDBD

algorithm found learning rates that were close to opti-

mal on this task.

DerivataiosnGorfatdhieenItDDBeDsceAnltgorithm
Many useful learning algorithms can be understood as gradient descent, including the LMS rule, backpropagation, Boltzmann machines, and reinforcement learning methods. Gradient descent analysis can also be used to derive learning algorithms, and that in fact is the way in which the IDBD algorithm was invented. In this section I derive the IDBD algorithm as gradient descent.
To illustrate the basic idea of gradient-descent analysis, it is useful to rst review how the base learn-

Figure 5: Average error as a function of the learningrate parameter of the relevant inputs (the irrelevant inputs had zero learning-rate parameters). Error is minimized near = 0:13, the value found by the IDBD algorithm.

ing rule, the LMS rule (2), can be derived as gradi-

ent the (t)

ed=xepsyceecn(ttet).d;vRyae(lutc)ae.lloTfthhteahteexwspqeeucaatrreeeddtereryrrironorgratso2a(tm)f,uinnwicmhtieiorznee

of the weights forms a surface. In gradient descent, a

current value for w is moved in the opposite direction

of the slope of that surface. This is the direction in

which the expected error falls most rapidly, the direc-

tion of steepest descent. Because the expected error

issamnoptleitesrerlof rkn2o(wt)n:, we use instead the gradient of the

(wi t

+

1)

=

wi(t)

;

1 2

@ 2(t) ( ):
@wi t

(7)

rsThitoghewheepthsefacasatrnldadthresesiqdcuewena:etni.gtihtTtyhve12ec21toidsrrtomhpeosvsoetusetpin-assitzhwee,e

determining direction of expand the

(wi t + 1)

=

wi(t)

;

1 2

@ 2(t) ( )@wi t

= wi(t) ; = wi(t) ;

(t)

@ (t) ( )@wi t

(t) @

y

(t) ; y(t)] ( )@wi t

(8)

= wi(t) + = wi(t) +

hX(t)

( )@y t ( )@wi t

n

i

( ) ( )@ t @wi t

wj (t)xj (t)
j=1

= wi(t) + (t)xi(t)

thus deriving the LMS rule (2). The derivation for the IDBD algorithm is very sim-
ilar in principle. In place of (7), we start with

i(t + 1) =

i(t)

;

1 2

@ 2(t)
@i

(9)

twiohner,ethneowpar21tiailsdtehrievi(tmiveetaw)itshterpe-sspizeec.t a time index should be interpretted as

In to the

this equadi ewriivtahtoiuvet

It is often not recognized that the size of the step in the direction of the gradient may depend on the current value of the parameters being modi ed. Moreover,

with repect to an in ntesimal change steps. A similar technique is used in

ginradiieantt-adlelstciemnte

even the direction of the step can be changed, as long as it it is in the same general direction as the gradi-

analyses of recurrent connectionist networks (c.f., e.g.,

Williams & Zipser 1989). We then similarly rewrite

and expand (9) as:

i(t + 1) =

X;i(t)

1 2

@ 2(t) ( )@wj t ( )j @wj t @ i

;i(t)

1 @ 2(t) ( )@wi t 2 () :
@wi t @ i

(10)

pfteoinrrrotepa(nneperyotwsipei,tsfia.tv2omeFtiihlonyernoieenfrxcaaprlmegrmoopdrleieunt,chtotmo)n,fes.wcioIitniuhnlofd(au9cat)td,ltdooesxaiopnfbagetrcaittmihoneresanoentfskeeniipny-, progress suggest that some members of this family may be more e cient than the IDBD algorithm at nding optimal learning rates. There is little reason beyond simplicity for prefering the IDBD algorithm over these other possibilities a priori.

The approximation above is reasonable in so far as the pbtihre6=iemroenjaf.ortyrhFeeerwoiemetchtc(awo8nf)eicrgwehhweatnr.kigtnTienohwg(u1ts0th)hweaaeitstah;ssl21eua@@mrwn2ei((ittn@))gw@=jr(iatt)e(ts)hx0oi(uftol)dr

The gradient analysis presented in this section tells us something about the IDBD algorithm, but it may also tell us something about incremental bias-learning algorithms in general. In particular, it suggests how one might derive bias-learning algorithms for other base learning methods, such as instance-based learning methods. In instance-based learning systems an

i(t + 1) i(t) + (t)xi(t)hi(t)

(11)

where hi(t) is de is in turn derived

naesdfoalslo@ww@si:(it)

.

The

update

rule

for

hi

important source of bias is the parameters of the interinstance distance metric. Currently these parameters are established by people or by o ine cross-validation methods. An interesting direction for further research

(hi t + 1) = @wi(t + 1) @i h i= @ wi(t) + e i(t+1) (t)xi(t)

would be to attempt to repeat the sort of derivation presented here, but for instance-based methods.
(12) Conclusion

=

@i
hi(t) + e

i(t+1)

(t)xi(t) + e

i(t+1) @ (t) xi(t) @i

The results presented in this paper provide evidence that the IDBD algorithm is able to distinguish relevant from irrelevant inputs and to nd the optimal learning

using the product rule of calculus. Using the same

approximation as before (in (10)), we write

X@ (t) = ; ( )@y t = ; @

wj (t)xj (t)

h i@ i @ i @ i j

; ;@ wi(t)xi(t) = ( ) ( )hi t xi t :

@i

Finally, plugging this back into (12) yields

rates on incremental tracking tasks. Depending on the problem, such an ability to nd appropriate biases can result in a dramatic reduction in error. In the tasks used here, for example, squared error was reduced by approximately 60%. The IDBD algorithm achieves this while requiring only a linear increase in memory and computation (both increase by roughly a factor of three over plain LMS). This algorithm extends prior work both because it is an incremental algorithm, operating on an example-by-example basis, and because it has

(hi t + 1)

h ihi(t) + e i(t+1) (t)xi(t) ; e i(t+1) ( )2xi t hi(t)
hi(t) 1 ; i(t + 1)x2i (t) + i(t + 1) (t)xi(t)

which, after adding a positive-bounding operation, is

uthpeTdahotereiga(ibn1o1av)leufdopredmaiotenissrtturhlaeetefssoartmhheait,ats(h6(e)5,I)D.wBhiDle

the derived algorithm is

a form of stochastic gradient descent in the learning-

rate tend

ptaorcaamuesetecrhs anig. eIsnaoccthoredr iwngortdos,thtehier

algorithm will e ect on over-

all error. At local optima, the algorithm will tend to be

stable elsewhere, it will tend to reduce the expected

error. These might be considered necessary properties

fewer free parameters that must be picked by the user. Also presented in this paper is a derivation of the IDBD algorithm as gradient descent. This analysis re nes previous analyses by improving certain approximations and by being applicable to incremental training.
On the other hand, only a linear version of the IDBD algorithm has been explored here. In addition, the results presented do not show that the IDBD algorithm is the best or fastest way to nd optimal learning rates. Further work is needed to clarify these points.
A good way of understanding the IDBD algorithm may be as an incremental form of cross validation. Consider the form of cross validation in which one ex-

for a good learning algorithm, but they alone are not

2Such algorithms are no longer steepest-descent algo-

su cient. For example, there is the issue of step-size. rithms, but they are still descent algorithms.

ample is held out, all the others are used for training, and then generalization is measured to the one held out. Typically, this is repeated N times for a training set of size N with a di erent example held out each time, and then the parameters are set (or stepped, following the gradient) to optimize generalization to the one held out, averaged over all N cases. Obviously, this algorithm can not be done incrementally, but something similar can be. At each time step, one could take the new example as the one held out, and see how all the training on the prior examples generalized to the new one. One could then adjust parameters to improve the generalization, as the IDBD algorithm does, and thereby achieve an e ect very similar to that of conventional cross validation. Such methods di er fundamentally from ordinary learning algorithms in that performance on the new example is optimized without using the new example.
The IDBD algorithm is being explored elsewhere as a psychological model. The base learning algorithm used here, the LMS rule, has often been used to model human and animal learning behavior. Although that modeling has been generally successful, there have also been a number of areas of divergence between model and emperiment. In many cases the discrepancies can be signi cantly reduced by augmenting the LMS model with relevance-learning methods (e.g., Kruschke 1992 Hurwitz 1990). The IDBD algorithm is also being explored in this regard, and the initial results are very encouraging (Gluck, Glauthier, & Sutton, in preparation Gluck & Glauthier, in preparation see also Sutton 1982).
One possible use of the IDBD algorithm is to assess the utility (relevance) of features created by constructive-induction methods or other representation-change methods. It is intriguing to think of using IDBD's assessments in some way to actually direct the feature-construction process.
Finally, a broad conclusion that I make from this work has to do with the importance of looking at a series of related tasks, such as here in a non-stationary tracking task, as opposed to conventional single learning tasks. Single learning tasks have certainly proved extremely useful, but they are also limited as ways of exploring important issues such as representation change and identi cation of relevant and irrelevant features. Such meta-learning issues may have only a small, second-order e ect in a single learning task, but a very large e ect in a continuing sequence of related learning tasks. Such cross-task learning may well be key to powerful human-level learning abilities.
Acknowledgements
The author wishes to thank Mark Gluck, without whose prodding, interest, and assistance this paper would never have been written, and Oliver Selfridge, who originally suggested the general approach. I also thank Richard Yee, Glenn Iba, Hamid Benbrahim,

Chris Matheus, Ming Tan, Nick Littlestone, Gregory Piatetsky, and Judy Franklin for reading and providing comments on an earlier draft of this paper.

References

Barto, A.G. & Sutton, R.S. (1981) Adaptation of learning rate parameters, Appendix C of Goal Seeking Com-

ponents for Adaptive Intelligence: An Initial Assessment. Air Force Wright Aeronautical Laboratories/Avionics Laboratory Technical Report AFWAL-TR-81-1070, WrightPatterson AFB, Ohio.

Gluck, M.A. & Glauthier P.T. (in preparation) Representa-

tion of dimensional stimulus structure in network theories

of associative learning.

Gluck, M.A., Glauthier, P.T., & Sutton, R.S. (in prepara-

tion) Dynamically modi able stimulus associability in net-

work models of category learning.

Hampson, S.E. & Volper, D.J. (1986) Linear function neu-

rons: Structure and training. Biological Cybernetics 53, 203-217.

Hurwitz, J.B. (1990) A hidden-pattern unit network model of category learning. PhD thesis, Harvard Psychology Dept.

Jacobs, R.A. learning rate

(a1d9a8p8t)aItniocnre. aNseedurraalteNseotfwcoornkvse1rg,e2n9c5e{3th0r7o.ugh

Kesten, H. (1958) Accelerated stochastic approximation.

Annals of Mathematical Statistics 29, 41{59.

Kruschke, J.K. (1992) ALCOVE: An exemplar-based con-

nectionist model of category learning. Psychological Review.

Lee, Y. & Lippmann, R.P. (1990) Practical characteristics of neural network and conventional pattern classi ers on

arti cial and speech problems. In Advances in Neural Information Processing Systems 2, D.S. Touretzky, Ed., 168{ 177.

Rendell, L.A., Seshu, R.M., and Tcheng, D.K. (1987) Lay-

ered concept learning and dynamically-variable bias man-

agement, Proc. Tenth International Joint Conference on Arti cial Intelligence, 308-314.

Schlimmer, J.C. (1987) Concept acquisition through rep-

resentation adjustment. PhD thesis, University of Califor-

nia, Information and Computer Science Dept., Technical

Report 87-19.

Silva, F.M. & Almeida, L.B. (1990) Acceleration tech-

niques for the backpropagation algorithm. In Neural Net-

works: EURASIP Workshop 1990, L.B. Almeida and C.J.

Wellekens, Eds., 110-119. Berlin: Springer-Verlag.

Sutton, R.S. (1982) A theory of salience change dependent

on the relationship between discrepancies on successive tri-

als on which the stimulus is present. Unpublished working

paper.

Sutton, R.S. (1986) Two problems with backpropagation and other steepest-descent learning procedures for networks. Proceedings of the Eighth Annual Conference of the

Cognitive Science Society, 823{831.

Widrow, B. & Stearns, S.D. Adaptive Signal Processing.
Englewood Cli s, NJ: Prentice-Hall, 1985.

Williams, R.J. & Zipser, D. (1989) Experimental analysis

of the real-time recurrent learning algorithm. Connection

Science 1, 87{111.

