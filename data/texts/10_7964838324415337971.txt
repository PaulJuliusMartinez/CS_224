From: KDD-95 Proceedings. Copyright © 1995, AAAI (www.aaai.org). All rights reserved.
Fuzzy Interpretation of Induction

Results

Xindong Wu and Petter MihlCnt
Department of Software Development, Monssh University 900 Dandenong Road, Melbourne, VIC 3145, Australia xindongQinsect.sd.monash.edu.au
t Department of Numerical Analysis and Computer Science Royal Institute of Technology, Stockholm, Sweden

Abstract
When applying rules induced from training examples to a test example, there are three possible caseswhich demand different actions: (1) no match, (2) single match, and (3) multiple match. Existing techniques for dealing with the fist and third cases are exclusively based on probability estimation. However, when there are continuous attributes in the example space, and if these at-
t"r-i.hl-r.r.tun"n h_.il.v..n" hI-a-aaan A-.,w-.r-c-=--.-t-i.caA i-n-"t-n Lqkprv& he fore induction, fuzzy interpretation of the discre-
tised intervals at deduction time could be very valuable. This paper introduces the idea of using fuzzy borders for interpretation of discretised intervals at deduction time, and outlines the results we have obtained with the HCV (Version 2.0) software.
Introduction
Knowledge discovery in databases (KDD) is a research frontier (Wu 93a) for both database technology and machine learning techniques, and has seen sustained research over recent years. It acts as a link between the two fields, thus offering a dual benefit. Firstly, since database technology has already found wide application in many fields, machine learning research obviously stands to gain from this greater exposure and established technological foundation. Secondly, as databases grow in both number and size, the prospect of mining them for new, useful knowledge becomes yet more enticing. Machine learning techniques can augment the ability of existing DBMSs to represent, acquire, and process a collection of expertise such as those which form part of the semantics of many advanced applications.
Generally speaking, all kinds of attribute-based learning algorithms can be adapted to extract knowledge from databases. It is not difficult to add an induction engine to an existing database system in an ad hoc way to implement rule induction from databases

or design some specific engines to learn from domainspecific data sets. However, when we integrate ma chine learning techniques into database systems, we must face many problems such as:
Efficient induction algorithms are needed. The algorithms should be capable of being applied to realistic databases, e.g. 1 lo6 relational tuples. Exponential or even medium-order polynomial complexity will not be of nr-r-a.-c.t-i-c~a.l- u~s~e._. .
The knowledge learned needs to be tested and/or used back in the learning systems.
Noise (including missing information) has to be effectively handled. Machine learning is different from mathematic induction. We cannot assume that the data in the given databases is complete. There are various sources of noise including missing values in real-world databases. To produce acceptable results for realistic applications, noise handling facilities are often essential to learning algorithms.
Numerical data and symbolic data are equally important in practical application. Existing learning algorithms can be generally divided into two groups: numerical methods including statistical methods and neural networks which are good at processing numerical data in noisy environments, and symbolic AI methods which are more efficient in dealing with symbolic or nominal data. It has been a long term dispute that AI methods (especially the decision trees) are too simple to represent the real world. In the meanwhile, we can also easily argue that numerical methods are not good enough to represent and maninurla--te-. 1_ -lo-~ci-c- r-e~la~t-i.o-n--s-h-.i-n~s~I~ among " sv* m~b~olic values. We need to have induction algorithms which can effectively deal with both types of data.
There are quite a few induction algorithms such as the ID3-like algorithms (Quinlan 86; Quinlan 93) and HCV (Wu 93b; Wu 95) which are low-order polynomial

WU 325

in both time and space. However, since induction from databases relies to a great extent on the quality of the training databases, interpreting induction results (say, rules) to classify a new example needs to face three possible cases which demand different actions:
No match: No rules match the example;
Single match: One or more rules indicate the same class match, and
Multiple match: More than one rule matches the example, and indicates different classes.
The third case does not apply to decision trees produced by IDJ-like algorithms, but when the trees are decompiled into rules, the rules will face the same problems (Quinlan 87; Quinlan 93).
In the single match case, the choice of class to the example is naturally the class indicated by the rules. Existing techniques for dealing with the first and third cases (Wu 95) are exclusively based on probability estimation. Among them, the Measure of Fit for dealing with the no match case and the Estimate of Probability for handling the multiple match case developed in (Michalski et al. 86) have been widely adopted in the KDD community.
The Measure of Fit and Estimate of Probability methods perform quite well with problem domains where no real-valued attributes are involved. However, when a problem contains attributes that take values from continuous domains (i.e. real numbers or integers), the performance of both methods, especially in terms of accuracy, decreases. In existing induction algorithms, dealing with continuous domams is based on discretisation of them into a certain number of intervals. There are quite a few strategies available for discretisation, such as (Wu 95) Bayesian classifiers and the information gain method. Once each continuous domain has been discretised into intervals, the intervals are treated as discrete values in induction and deduction. This is the standard way all existing systems have taken. However, discretisation of continuous domains does not always fit accurate interpretation, To say an age greater than 50 is old or a temperature above 32 centigrades is high is fuzzy. In these kinds of cases, fuzzy interpretation of the discretised intervals at deduction time could be very valuable. Rather
t"hIIaVn.L tUrrUlc'i.n"`a~ t"h.Ae" VrnIt". yn-n.i-n.vta- &d&d by t,E_e rl_~~c.&~~~-
tion methods as sharp borders, we can instead place some kind of curve at each cut point as fuzzy borders. With these fuzzy borders, a value can be classified into a few different intervals at the same time, with varying degrees. This could change a single match case to a multiple match, and a no match case to a single or
326 KDD-9s

even multiple match. Deduction with fuzzy borders of discretised intervals is called fuzzy matching. In the multiple match case, we can take the interval with the greatest degree as the value's discrete value.

Discretisation of Continuous Attributes
When there are both symbolic and continuous attributes in an example set for induction, the standard approach is discretise the numerical domains of these continuous attributes into a certain number of intervals. The discretised intervals can be treated in a similar way to nominal values during induction and deduction.
The most difficult aspect of discretisation is to find the right places to set up interval borders. This section reviews some typical discretisation methods. In the following account, N indicates the number of examples in the training set, c is the number of classes or concepts that these examples are classified into, and d is the number of intervals generated by discretisation. In all these methods, sorting the values of the continuous attribute in question in ascending order is always
u.Aa"a"f~n.l.~ hYnILfn"rI.e2 r.li.a.Yrr"e.t"iVcIaYticnyn"SV.~ aia" "ra..rsr~b~-lI"U "nYdY.

The simplest class-separating method
The simplest discretisation method is to place interval borders between each adjacent pair of examples that are not classified into the same class. Suppose the pair of adjacent values on attribute X are ~1 and es, 2 = (xl+ x2)/2 can be taken as an interval border.
If the continuous attribute in question is very informative, which means that positive and negative examples take different value intervals on the attribute, this method is very efficient and useful. You can find, for example, that Professors and Lecturers at Australian universities have distinctive salary ranges, and the continuous attribute salary is very informative in distinguishing academic positions. However, this method tends to produce too many intervals on those attributes which are not very informative. These intervals can also easily confuse algorithms like HCV (Wu 93b) because a 0.16 difference between a positive example and a negative one on a numerical attribute makes one more interval. The worst case is d = N - 1, where a border has to be set up between every pair of examples.

Bayesian classifiers According to Bayes formula,

e4c.i)ecj) p(cj'z) = Cizl P(+JP(Ck)

(1)

where P(cj 1~) is the probability of an example belonging to class cj if the example takes value z on the

continuous attribute in question, P(zlcj) is the probability of the example taking value z on the attribute if it is classified in the class cj.
P(cj) can be approximated by using one of the following three probability estimation methods: relative frequency, Laplacian Law of Succession (Niblett and Bratko 87), or the m-estimate (Lavrac & Dzeroski 94), and P(cjIt) can take the frequency of cj under e over all the examples in the training set.
Given P(cj) and P(cj/x), we can construct a probability curve,

fjtx) = P(XlCj)P(Cj)

(2)

for each class cj. When the curves for every class have

been constructed, interval borders are placed on each of those points where the leading curves are different

on its two sides. Between each pair of those points

including the two open ends, -00 and +co, the leading curve is the same.

We call a discretisation implemented by the above method a Bayesian classifier (Hong 94).

The information gain heuristic
When the examples in the training set have taken values of 21, .... x, in ascending order on a continuous attribute, we can use the information gain heuristic adopted in ID3 (Quinlan 86) to find a most informative border to split the value domain of the continuous attribute. (Fayyad & Irani 92) has shown that the maximum information gain by the heuristic is always achieved at a cut point (say, the mid-point) between the values taken by two examples of different classes.
We can adopt the information gain heuristic in the following way. Each x = (zi +xi+1)/2 (i = 1, .... n - 1) is a possible cut point if xi and xi+1 have been taken by examples of different classes in the training set. Use the information gain heuristic to check each of the possible cut points and find the best split point. Run the same process on the left and right halves of the splitting to split them further. The number of intervals produced this way may be very large if the attribute is not very informative. (Catlett 91) has proposed some criteria to stop the recursive splitting:

l Stop if the information gain on all cut points is the same,

l Stop if the number of examples to split is less than a certain number (e.g. fourteen), and

l Limit the number of intervals to be produced to a certain number (e.g. eight).

In C4.5 (Quinlan 93), the information gain approach is revised in the following ways. Firstly, each of the

possible cut points is not the midpoint between the two nearest values, but rather the greatest value in the entire training set that does not exceed the midpoint. This ensures that all border values occur in the training data. Each border value in this case is not necessarily the same as the lower of the two neighbouring values since all training examples are examined for the selection. Secondly, C4.5 (Quinlan 93) adopts the information gain ratio rather than the information gain heuristic. Finally, C4.5 does binarization of continuous attributes, which means only one interval border is found for each continuous attribute.

Fuzzy Borders and Fuzzy Interpretation
Rather than taking the cut points set up by discretisation methods as sharp borders, each interval is associated with a specific membership function with fuzzy methods. The membership function measures the degree of a value belonging to the interval. In fact, sharp intervals can be treated as a special case of fuzzy borders: the membership function for an interval with sharp borders takes value 1 iff the value is inside the interval and 0 otherwise, and one value can belong to one interval only. Figure 1 shows the difference between sharp borders and fuzzy ones.

1
___---------I' I' II I'
Ii I' I'
/ 1s

1\\----- \\\\\\\

Figure 1: Sharp and Fuzzy Borders
In Figure 1, xleft and zrisht are the left and right sharp borders of interval Ii respectively, and 1 = Gight - x[~J~ is the original length of the interval. s is the spread parameter, which indicates the length that an interval should be extended at each end. When the parameter is 0.1, for example, the interval in Figure 1 spreads out into adjacent intervals for twenty percent of its original length. In HCV (Version 2.0) (Wu et al 95), s is a user-specified parameter with default being 0.1.
The match of an example taking value x on a specific attribute domain with an interval is defined as the value of the membership function of the interval calculated for x.
wu 327

We have implemented three membership functions

(see Appendix) in HCV (Version 2.0), with which two

methods for calculating the match degree of a value z

with a selector1 or conjunction have been implemented.

The first takes the maximum membership degree of

the value in all of the intervals involved in the selector.

The drawback of this method is that if two adjacent

intervals belong to the same selector, a value close to

the border between the two intervals will get a very

low membership value in both, leading to a low overall

I..I.I.cd;I.I.I.vL~mL.D"l.I:I.I.p

UAGA.&v;%lr;.r,.; C,.."*-Aal.. :IcI :c IV I:,D .w.*-az1,,1 L",".Y..,O".LJGU

l".,.y.eL.,U1I-z Dn*G-

lector. In an attempt to remedy this, the other method

adds with fuzzy plus2 all the fuzzy membership degrees

within one selector.

The HCV (Version 2.0) software
The HCV algorithm (Wu 93b) is a representative of the extension matrix based family of attribute-based induction algorithms, originating with JR. Hong's AEl (Hong 85). By dividing the positive examples (PE) of a specific concept in a given example set into intersecting groups and adopting a set of strategies to find a heuristic conjunctive rule in each group which covers all the group's positive examples and none of the negative examples (NE), HCV can find a rule in the form of variable-valued logic for the concept in low-order polynomial time. If there exists at least one conjunctive rule in a given training example set for PE against NE, the rule produced by HCV must be a conjunctive one. The rules in variable-valued logic generated by HCV have been shown empirically to be more com-
nyycyaurt" t"h1a.Wn .. tYh.n.V Ur-LVrV;~.Yin.n".. tVm."m""c n"1o1. tV.hII&V.. m.ddyuv.a.lmUt..".." r..t"~"r.iYs.i"n.n.
rules produced by the ID3 algorithm (the best-known induction algorithm to date) and its successors (e.g., C4.5) in terms of the numbers of conjunctive rules and conjunctions.
The HCV (Version 2.0) software is a C++ implementation of the HCV algorithm. In this implementation, HCV can work with noisy and real-valued domains as well as nominal and noise-free databases. It also provides a set of deduction facilities for the user to test the accuracy of the produced rules on test examples. The detailed description of the software is inciuded in (Wu et al 95).
In addition to a set of discretisation facilities, such as the Bayesian classifiers and the information gain

'A selector in variable-valued logic (Michalsld 75) takes the general form
[XWI
where X is a variable or attribute, # is a relational operator (such as =, #, <, >, 5, and 21, and R is a list of one or more values (including discretrsed intervals) that X could take on.
2Fuzzy plus $ is defined as follows: a @ b = a + b - ab.

328 KDD-95

heuristic, and the fuzzy borders mentioned above, HCV (Version 2.0) permits the user to specify their own discretisation of real-valued attributes by providing a set of intervals in the structure file, which specifies the attributes (with their order and value domains) and classes used in the data files. This is a very useful way for integrating domain information.
Hybrid Interpretation
Extensive experiments have been carried out with the above fuzzy methods in HCV (Version 2.0) on a large set of databases from the University of California at Irvine Repository of Machine Learning Databases. However, the results were much less encouraging than what we expected when we were trying to justify that fuzzy borders are generally more reliable than sharp borders with numerical domains.
We have analysed the results by fuzzy methods and those with sharp borders, and found that the accuracy of the single matches is in general much better than
n. .n.a .m..aI"tV.rYh,1xI ua.d... `m.*n".lYt.i'yn'l-e" .m..a-"t"r.hI"e"e w.,i.tY.h.. .a"l.lI .m..&YhYn.c.VleUY.
With the multiple match case, the Estimate of Probability (Michalski et al. 86) with the Laplacian Law of Succession (Niblett and Bratko 87) outperforms other methods including fuzzy matching. These observations motivated the development of a hybrid interpretation in HCV (Version 2.0) with fuzzy matching and the Estimate of Probability.
The hybrid method works as follows. In the single match case, we do not provide any probability analysis or fuzzy border_s.:_:In the multiple match case, the Estimate of Probability method with sharp borders is used to find the best class for the example in question. Only in the no match case, fuzzy borders are set up (with the polynomial membership function as default) in order to find a rule which is closest (with the maximum membership degree) to the example in question.
The hybrid method is an option for deduction in HCV (Version 2.0). The user can overrule it by specifying other methods (such as the combination of the Measure of Fit and the Estimate of Probability).
Conclusions
As mentioned above, fuzzy methods, although their results are significant when combined with other deduction methods, do not contribute as much as one can expect to the accuracy of deduction on their own. This is likely because all the experiments have not been specifically conducted with domain dependent information.
Fuzziness is strongly domain dependent. The HCV (Version 2.0) software has provided a way for the user to specify their own intervals and select their own fuzzy functions. This is an important direction to take if we

would like to achieve significant results with specific domains.
References
J. Catlett, On Changing Continuous Attributes into Ordered Discrete Attributes, Proceedings of E WSL91, 1991.
U.M. Fayyad and K.B. Irani, On the Handling of Continuous-Valued Attributes in Decision Tree Generation, Machine Learning, 8(1992), 87-102.
J. Hong, AEl: An Extension Matrix Approximate Method for the General Covering Problem, International Journal of Computer and Information Sciences, 14(1985), 6: 421-437.
J. R. Hong, PKAS: A Practical Knowledge Acquisition System, Unpublished (1994).
N. Lavrac and S. Dzeroski, Inductive Logic Programming - Techniques and Applications, Ellis Horwood, 1994.
R.S. Michalski, Variable-Valued Logic and Its Applications to Pattern Recognition and Machine Learning, Computer Science and Multiple- Valued Logic Theory and Applications, D.C. Rine (Ed.), Amsterdam: North-Holland, 1975, 506-534.
R.S. Michalski, I. Mozetic, J. Hong and N. Lavrac, The Multi-Purpose Incremental Learning System A&15 and Its Testing Application to Three Medical Domains, Proceedings of AAAZ 1986, 1986, 10411045.
T. Niblett and I. Bratko, Learning Decision Rules in Noisy Domains, Research and Development in Expert Systems III, M. A. Bramer (Ed.), Cambridge, New York Cambridge University Press, 1987 pp. 25-34
J.R. Quinlan, Induction of Decision Trees, Machine Learning, 1(1986), 81-106.
J.R. Quinlan, Generating Production Rules from Decision Trees, Proceedings of International Joint Conference on Artificial Intelligence, J. McDermott (Ed.), Morgan Kaufmann Publishers, Inc., 1987,304-307.
J.R. Quinlan, C&?i: Programs for Machine Learning, Morgan Kaufmann Publishers, 1992.
X. Wu, Inductive Learning: Algorithms and Frontiers, Artijiciai inteiiigence Review, 7(i993j, 2: 93108.
X. Wu, The HCV Induction Algorithm, Proceedings of the 2lst ACM Computer Science Conference, S.C. Kwasny and J.F. Buck (Eds.), ACM Press, USA, 1993, 168-175.

X. Wu, Knowledge Acquisition from Data Bases (in press), 1995.
X. Wu, J. Krisar and P. MBhMn, HCV (Version 2.0) User's Manual, Department of Software Development, Monash University, Australia, 1995.

Appendix: Three Fuzzy Membership Functions Implemented in HCV (Version 2.0)
There are three functions in HCV (Version 2.0) which can be used to fuzzify interval borders.
The linear function (see Figure 2) is specified by

A 10 --

.kx+lJ =... ..

/ :

b+n

.. .. . . . ..'
....

%% - F"_uuu .m.-e..m-h-_.dlm .R_m.- c_t,_m..

. ..*
**.. ._ -... -.

Figure 2: The linear membership function
the following expressions where s and 1 have the same meanings as in Figure 1.
k=&> 1 1 a = -heft + 2, b = kx,ie/,t + -2 Einrejt(x) = kx + a lin,i,ht(x) = -kx + b Zin(x) = MAX(0, MTN{l,linr,ft(z),Ein,i,ht(~)}} With the polynomial membership function (see Figure 3), the fuzzy borders are defined by a third-degree polynomial.

P&lejt (g) = wejtx3 + &+x2 + Cleft2 + 4ejt

Polyright (z) = %ghtZ3 + bightX2 + Crightx + dright

where

1

aft

= aright

=

-4&)3

baide = -3a sidexside

cside = saside(x:ide - (lS)2)
dside = -a(&de - 3X:,ide(ls)2 + 2(ls)3)

wu 329

- - - Shup mrnknhip W&km
Figure 3: The polynomial membership function
Figure 4: The arctan membership function
and side E {left, right), XSi& is the sharp border on each side, and I and s are the original interval length and the spread respectively.
if Xleft - IS 5 2 5 X:left + Is if Xright - Is < x < Xright + IS if xleft + 1s 5 X 5 xTight - t.5 otherwise The third membership function (see Figure 4) is the arctan function. The spread of the interval is used to indicate the flatness or linearity of the curve, and the fuzzy membership of an interval takes the minimum of the membership from the left and the one from the right. The function used to calculate the membership is:
arctan = M IN{;atrnqx-5yt) + ;, +an( 2-;;ight)+ ;I*
330 KDD-95

