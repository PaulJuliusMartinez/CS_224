Reinforcement learning
Stuart Russell, UC Berkeley
Stuart Russell, UC Berkeley 1

Outline
 Sequential decision making  Dynamic programming algorithms  Reinforcement learning algorithms
­ temporal difference learning for a fixed policy ­ Q-learning and SARSA  Function approximation  Exploration  Decomposing agents into modules
Stuart Russell, UC Berkeley 2

Sequential decision problems
Add uncertainty to state-space search  MDP Add sequentiality to Bayesian decision making  MDP I.e., any environment in which rewards are not immediate Examples:
­ Tetris, spider solitaire ­ Inventory and purchase decisions, call routing, logistics, etc. (OR) ­ Elevator control ­ Choosing insertion paths for flexible needles ­ Motor control (stochastic optimal control) ­ Robot navigation, foraging
Stuart Russell, UC Berkeley 3

Example MDP

3 +1

2 1 START

-1
0.8
0.1 0.1

1234

States s  S, actions a  A

Model T (s, a, s)  P (s|s, a) = probability that a in s leads to s

Reward function R(s) (or R(s, a), R(s, a, s))

=


  
  

-0.04 ±1

(small penalty) for nonterminal states for terminal states

Stuart Russell, UC Berkeley 4

Solving MDPs
In search problems, aim is to find an optimal sequence In MDPs, aim is to find an optimal policy (s)
i.e., best action for every possible state s (because can't predict where one will end up) The optimal policy maximizes (say) the expected sum of rewards Optimal policy when state penalty R(s) is ­0.04:
3 +1

2 -1

1 1234

Stuart Russell, UC Berkeley 5

Utility of state sequences
Need to understand preferences between sequences of states Typically consider stationary preferences on reward sequences:
[r, r0, r1, r2, . . .]  [r, r0 , r1 , r2 , . . .]  [r0, r1, r2, . . .]  [r0 , r1 , r2 , . . .] Theorem (Koopmans, 1972): there is only one way to combine rewards over time:
­ Additive discounted utility function: U ([s0, s1, s2, . . .]) = R(s0) + R(s1) + 2R(s2) + · · · where   1 is the discount factor
[Humans may beg to differ]
Stuart Russell, UC Berkeley 6

Utility of states
Utility of a state (a.k.a. its value) is defined to be U (s) = expected (discounted) sum of rewards (until termination) assuming optimal actions
Given the utilities of the states, choosing the best action is just MEU: maximize the expected utility of the immediate successors

3 0.812 0.868 0.912

+1

3

+1

2 0.762

0.660

-1

2

-1

1 0.705 0.655 0.611 0.388

1

1234

1234

Stuart Russell, UC Berkeley 7

Utilities contd.
Problem: infinite lifetimes  undiscounted ( = 1) utilities are infinite 1) Finite horizon: termination at a fixed time T
 nonstationary policy: (s) depends on time left 2) Absorbing state(s): w/ prob. 1, agent eventually "dies" for any 
 expected utility of every state is finite 3) Discounting: assuming  < 1, R(s)  Rmax,
U ([s0, . . . s]) = t=0tR(st)  Rmax/(1 - )
Smaller   shorter horizon 4) Maximize average reward per time step
­ sometimes more appropriate than discounting
Stuart Russell, UC Berkeley 8

Dynamic programming: the Bellman equation

Definition of utility of states leads to a simple relationship among utilities of neighboring states:

expected sum of rewards = current reward +  × expected sum of rewards after taking best action

Bellman equation (1957) (also Shapley, 1953):
U (s) = R(s) +  maax sU (s)T (s, a, s)
U (1, 1) = -0.04 +  max{0.8U (1, 2) + 0.1U (2, 1) + 0.1U (1, 1), 0.9U (1, 1) + 0.1U (1, 2) 0.9U (1, 1) + 0.1U (2, 1) 0.8U (2, 1) + 0.1U (1, 2) + 0.1U (1, 1)}

up left down right

One equation per state = n nonlinear equations in n unknowns

Stuart Russell, UC Berkeley 9

Value iteration algorithm

Idea: Start with arbitrary utility values Update to make them locally consistent with Bellman eqn. Everywhere locally consistent  global optimality

Repeat for every s simultaneously until "no change"

U (s)  R(s) +  maax sU (s)T (s, a, s)
1 0.8 0.6

for all s
(4,3) (3,3)
(1,1) (3,1)

Utility estimates

0.4 (4,1)

0.2

0

-0.2

0 5 10 15 20 25 30 Number of iterations

Stuart Russell, UC Berkeley 10

Convergence
Define the max-norm ||U || = maxs |U (s)|, so ||U - V || = maximum difference between U and V Let U t and U t+1 be successive approximations to the true utility U Theorem: For any two approximations U t and V t
||U t+1 - V t+1||   ||U t - V t|| I.e., Bellman update is a contraction: any distinct approximations must get closer to each other so, in particular, any approximation must get closer to the true U and value iteration converges to a unique, stable, optimal solution But MEU policy using U t may be optimal long before convergence of values ...
Stuart Russell, UC Berkeley 11

Policy iteration
Howard, 1960: search for optimal policy and utility values simultaneously Algorithm:
  an arbitrary initial policy repeat until no change in 
compute utilities given  update  as if utilities were correct (i.e., local depth-1 MEU) To compute utilities given a fixed  (value determination):
U (s) = R(s) +  sU (s)T (s, (s), s) for all s
i.e., n simultaneous linear equations in n unknowns, solve in O(n3)
Stuart Russell, UC Berkeley 12

Q-iteration

Define Q(s, a) = expected value of doing action a in state s
and then acting optimally thereafter
= R(s) +  sU (s)T (s, a, s)
i.e., U (s) = maxa Q(s, a)

Q-iteration algorithm: like value iteration, but do

Q(s,

a)



R(s)

+



sT

(s,

a,

s)

max
a

Q(s,

a)

for all s, a

Q-values represent a policy with no need for transition model (unlike U )

Stuart Russell, UC Berkeley 13

General asynchronous dynamic programming
Local value and policy updates steps can be mixed in any order on any states, with convergence guaranteed as long as every state gets updated infinitely often Reinforcement learning algorithms operate by performing such updates based on the observed transitions made in an initially unknown environment
Stuart Russell, UC Berkeley 14

Invariance transformations
Nonsequential behaviour invariant under positive affine transform: U (s) = k1U (s) + k2 where k1 > 0
Sequential behaviour with additive utility is invariant wrt addition of any potential-based reward:
R(s, a, s) = R(s, a, s) + F (s, a, s) where the added reward must satisfy
F (s, a, s) = (s) - (s) for some potential function  on states [Ng, Harada, Russell, ICML 99] Often useful to add "shaping rewards" to guide behavior
Stuart Russell, UC Berkeley 15

Partial observability
POMDP has an observation model O(s, e) defining the probability that the agent obtains evidence e when in state s Agent does not know which state it is in
 makes no sense to talk about policy (s)!! Theorem (Astrom, 1965): the optimal policy in a POMDP is a function
(b) where b is the belief state (P (S|e1, . . . , et)) Can convert a POMDP into an MDP in (continuous, high-dimensional) belief-state space, where T (b, a, b) is essentially a filtering update step Solutions automatically include information-gathering behavior The real world is an unknown POMDP
Stuart Russell, UC Berkeley 16

Other Issues
Complexity: polytime in number of states (by linear programming) but number of states is exponential in number of state variables  Boutilier et al, Parr & Koller: use structure of states (but U , Q summarize infinite sequences, depend on everything)  reinforcement learning: sample S, approximate U /Q/  hierarchical methods for policy construction (next lecture)
Unknown transition model: agent cannot solve MDP w/o T (s, a, s)  reinforcement learning
Missing state: there are state variables the agent doesn't know about  [your ideas here]
Stuart Russell, UC Berkeley 17

Reinforcement learning
Agent is in an unknown MDP or POMDP environment Only feedback for learning is percept + reward Agent must learn a policy in some form:
­ transition model T (s, a, s) plus value function U (s) ­ action-value function Q(a, s) ­ policy (s)
Stuart Russell, UC Berkeley 18

Example: 4 × 3 world

3 2 1 START
12

+1 -1
34

(1, 1)-.04(1, 2)-.04(1, 3)-.04(1, 2)-.04(1, 3)-.04 · · · (4, 3)+1 (1, 1)-.04(1, 2)-.04(1, 3)-.04(2, 3)-.04(3, 3)-.04 · · · (4, 3)+1 (1, 1)-.04(2, 1)-.04(3, 1)-.04(3, 2)-.04(4, 2)-1 .

Stuart Russell, UC Berkeley 19

Example: Backgammon
0 1 2 3 4 5 6 7 8 9 10 11 12
25 24 23 22 21 20 19 18 17 16 15 14 13
Reward for win/loss only in terminal states, otherwise zero TDGammon learns U^ (s), represented as 3-layer neural network Combined with depth 2 or 3 search, one of top three players in world (after 2 million games)
Stuart Russell, UC Berkeley 20

Example: Animal learning
RL studied experimentally for more than 60 years in psychology Rewards: food, pain, hunger, recreational pharmaceuticals, etc. [Details in later lectures] Digression: what is the animal's reward function? Inverse reinforcement learning [Ng & Russell, 2000; Sargent, 1978]:
estimate R given samples of (presumably) optimal behavior Issue: degenerate solutions (e.g., R = 0) Choose R to make observed  "very optimal" or "likely assuming noisy selection"
Stuart Russell, UC Berkeley 21

Example: Autonomous helicopter
Reward = ­ squared deviation from desired state
Stuart Russell, UC Berkeley 22

Temporal difference learning
Fix a policy , execute it, learn U (s) Bellman equation:
U (s) = R(s) +  T (s, (s), s)U (s)
s
TD update adjusts utility estimate to agree with Bellman equation: U (s)  U (s) + (R(s) +  U (s) - U (s))
Essentially using sampling from the environment instead of exact summation
Stuart Russell, UC Berkeley 23

Utility estimates RMS error in utility

TD performance

1 0.8 0.6 0.4 0.2
0 0

(4,3) (3,3) (1,3) (1,1) (2,1)
100 200 300 400 500 Number of trials

0.6 0.5 0.4 0.3 0.2 0.1
0 0

20 40 60 80 100 Number of trials

Stuart Russell, UC Berkeley 24

Q-learning [Watkins, 1989]
One drawback of learning U (s): still need T (s, a, s) to make decisions Learning Q(a, s) directly avoids this problem Bellman equation:
Q(a, s) = R(s) +  T (s, (s), s) max Q(a, s)
s a
Q-learning update: Q(a, s)  Q(a, s) + (R(s) +  max Q(a, s) - Q(a, s))
a
Q-learning is a model-free method for learning and decision making Q-learning is a model-free method for learning and decision making (so cannot use model to constrain Q-values, do mental simulation, etc.)
Stuart Russell, UC Berkeley 25

SARSA [Rummery & Niranjan, 1994]
Instead of using maxa Q(a, s), use actual action a taken in s SARSA update:
Q(a, s)  Q(a, s) + (R(s) +  Q(a, s) - Q(a, s)) Q-learning can execute any policy it wants while still learning the optimal Q SARSA learns the Q-value for the policy the agent actually follows
Stuart Russell, UC Berkeley 26

Function approximation

For real problems, cannot represent U or Q as a table!!

Typically use linear function approximation (but could be anything): U^(s) = 1 f1(s) + 2 f2(s) + · · · + n fn(s) .

Use a gradient step to modify  parameters:

i



i

+



[R(s)

+



U^(s)

-

U^(s)]

U^(s) i

i



i +  [R(s) + 

max
a

Q^  (a,

s)

-

Q^ (a,

s)]

 Q^  (a, i

s)

Often very effective in practice, but convergence not guaranteed (Some narrow results for linear and instance-based approximators)

Stuart Russell, UC Berkeley 27

Policy search
Simplest possible method: ­ parameterized policy theta(s) ­ try it out, see how well it does ­ try out nearby values of , see how well they do ­ follow the empirical gradient
Problems: local maxima, uncertainty in policy value estimates Useful idea [Ng & Jordan, 2000; Hammersley, 1960s]: (in simulated domains) reuse random seed across trial sets for different values of , thereby reducing variance in estimate of value differences
Stuart Russell, UC Berkeley 28

Exploration
How should the agent behave? Choose action with highest expected utility?

RMS error, policy loss

23
RMS error 1.5 Policy loss
2
1

+1 ­1

0.5
1

0 0 50 100 150 200 250 300 350 400 450 500
Number of trials

1234

Exploration vs. exploitation: occasionally try "suboptimal" actions!!

Really an (intractable) "exploration POMDP" where observations give the agent information about which MDP it's in

Stuart Russell, UC Berkeley 29

Functional decomposition
Do RL agents have to be monolithic? Is it possible to have modules for different functions, e.g., navigation, eating, obstacle avoidance, etc.? . . . whilst retaining global optimality?
Stuart Russell, UC Berkeley 30

Command arbitration

Each sub-agent recommends an action, arbitration logic selects among recommended actions (e.g., subsumption architecture (Brooks, 1986))

State
s

 1 a1  2 a2

Arbitrator

 3 a3

Action
a

Stuart Russell, UC Berkeley 31

Command arbitration contd.
Problem: Each recommendation ignores other sub-agents  arbitrarily bad outcomes
Problem: Arbitration is difficult, domain-specific Why not "blend" the recommendations? Problem: Blending "swerve left" and "swerve right" is a bad idea and blending "Ra6" and "Qg7" is meaningless
Stuart Russell, UC Berkeley 32

Q-decomposition [Russell & Zimdars, 2003]

A very obvious idea:  Each sub-agent embodies a local Qj function  Given current state s, sends Qj(s, a) for each a  Arbitrator chooses arg maxa j Qj(s, a)
Q1 Q1( . , s)

State
s

Q Q2( . , s) 2

Arbitrator

Action
a

Q Q3( . , s) 3

Stuart Russell, UC Berkeley 33

Additive rewards
Each sub-agent aims to maximize own Rj(s, a, s) Additive decomposition: R = j Rj Trivially achievable (not relying on any state decomposition) but often Rj may depend on subset of state variables while Qj depends on all state variables
Stuart Russell, UC Berkeley 34

What are the Qjs?
Qj = expected sum of Rj rewards under globally optimal policy Define Qj (s, a) = Es Rj(s, a, s) + Qj (s, (s))  Q(s, a) = Es [R(s, a, s) + Q(s, (s))]
= j Qj(s, a)  arg maxa j Qj (s, a) = arg maxa Q(s, a) I.e., arbitrator decision is globally optimal when local Qj functions anticipate globally optimal behavior Theorem: Local SARSA converges to globally optimal Qj (whereas local Q-learning yields greedy sub-agents)
Stuart Russell, UC Berkeley 35

Example: Fisheries

Fleet of boats, each has to decide how much to catch
350000

300000

250000

Policy value

200000 150000

Local SARSA Global SARSA
Local Q

100000

50000 0 100 200 300 400 500 600 700 800 900 1000
Training episode

Stuart Russell, UC Berkeley 36

Fish population

Fisheries results contd.

160000

140000 120000

Local SARSA Local Q

100000

80000

60000

40000

20000

0 0 10 20 30 40 50 60 70 80 90
Season

Stuart Russell, UC Berkeley 37

Summary
MDPs are models of sequential decision making situations Dynamic programming (VI, PI, QI) finds exact solutions for small MDPs Reinforcement learning finds approximate solutions for large MDPs Work directly from experience in the environment, no initial model Q-learning, SARSA, policy search are completely model-free Function approximation (e.g., linear combination of features) helps RL scale up to very large MDPs Exploration is required for convergence to optimal solutions Agents can be decomposed into modules and still be globally optimal
Stuart Russell, UC Berkeley 38

