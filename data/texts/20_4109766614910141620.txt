Speed and Sparsity of Regularized Boosting

Yongxin Taylor Xi Zhen James Xiang Peter J. Ramadge

Robert E. Schapire

Princeton University

Princeton University

Department of Electrical Engineering

Department of Computer Science

Princeton, NJ 08544

35 Olden Street, Princeton, NJ 08540

Abstract
Boosting algorithms with l1-regularization are of interest because l1 regularization leads to sparser composite classifiers. Moreover, Rosset et al. have shown that for separable data, standard lpregularized loss minimization results in a margin maximizing classifier in the limit as regularization is relaxed. For the case p = 1, we extend these results by obtaining explicit convergence bounds on the regularization required to yield a margin within prescribed accuracy of the maximum achievable margin. We derive similar rates of convergence for the -AdaBoost algorithm, in the process providing a new proof that -AdaBoost is margin maximizing as  converges to 0. Because both of these known algorithms are computationally expensive, we introduce a new hybrid algorithm, AdaBoost+L1, that combines the virtues of AdaBoost with the sparsity of l1regularization in a computationally efficient fashion. We prove that the algorithm is margin maximizing and empirically examine its performance on five datasets.
1 INTRODUCTION
Boosting is a technique for constructing from a finite dataset a composite classifier as a linear combination of a given large set of weak classifiers. Typically, it is assumed that for every distribution, there exists one weak classifier whose error is slightly below chance, and the objective is to construct a composite classifier with a much lower probability of misclassification. AdaBoost was the first practical boosting algorithm (Freund & Schapire, 1997), and
Appearing in Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS) 2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR: W&CP 5. Copyright 2009 by the authors.

many other generalizations and extensions have since been proposed (see, for example, (Meir & Ra¨tsch, 2003) and (Schapire, 2002) for overviews).
Theoretical work on boosting's ability to generalize have centered on two approaches. The first relies on sparsity, i.e., a limit on the number of weak classifiers with nonzero weights, or alternatively, a bound on the magnitude (say, the l1-norm) of those weights (Freund & Schapire, 1997; Lugosi & Vayatis, 2004; Zhang & Yu, 2005). Sparsity may be desirable not only for generalization performance, but also as a means of identifying a small set of useful features. Sparsity can be enforced using early stopping or regularization, although AdaBoost often performs well even when neither of these techniques is employed. In an alternative approach, AdaBoost can be analyzed by showing that it achieves large margins on the training data, and that these large margins are sufficient to guarantee good test performance (Schapire et al., 1998). This paper focuses on the intertwined relationship between these important aspects of boosting, namely, sparsity, regularization and large margins.
Rosset et al. (2004b) have shown that for separable data, standard lp-regularized loss minimization results in a margin maximizing classifier in the limit as regularization is relaxed. So, for datasets that admit classification using a sparse but unknown set of features, l1-regularized loss minimization offers two potential advantages: (1) a sparse composite classifier with (2) a large margin on the training data. The major disadvantage of standard l1-regularized loss minimization over a huge space of weak classifiers is the considerable computational burden. As an alternative, Hastie et al. (2001) studied the -AdaBoost algorithm in which the coefficient of a selected weak classifier is increased by a small amount  at each iteration, instead of the potentially large step size of AdaBoost. This algorithm is slow but simpler to implement and shares some properties of l1-regularized loss minimization. In particular, as  converges to 0, the classifier attains the maximum possible margin on the training data (Zhang & Yu, 2005).
In Section 3 we extend Rosset et al.'s results when p = 1

615

Speed and Sparsity of Regularized Boosting

by deriving explicit bounds on the regularization parameter to ensure the composite classifier margin is within prescribed accuracy of the maximum achievable margin. For completeness, in an appendix we derive similar results for -AdaBoost and give a new proof that it is margin maximizing. (See also (Zhang & Yu, 2005).)
Known methods for solving the regularized loss minimization problem are computationally expensive. To address this, in Section 4 we introduce a new hybrid algorithm, AdaBoost+L1, that combines in a computationally efficient fashion the virtues of AdaBoost with the sparsity produced by l1-regularization. We prove that AdaBoost+L1 is margin maximizing given standard assumptions of weak learnability. We empirically investigate AdaBoost+L1 on five datasets in Section 5.

2 PRELIMINARIES

For

any

integer

k

>

0,

let

k
R

denote

k-dimensional

Eu-

clidean space and k denote the probability simplex in Rk.

Let

z

=

(z1,

.

.

.

,

zk )

denote

the

components

of

z



k
R

and

z 1=

k i=1

|zi|.

Let S = (x1, y1), . . . , (xm, ym) be a sequence of m labelled training examples, where xi  X is an instance and yi  {-1, +1} its label. A binary classifier on domain X is a map h : X  {-1, +1}. The performance of the clas-
sifier h with respect to a distribution d  m on S can be
measured by its edge:

m
e(h, d) = Eid [yih(xi)] = diyih(xi).
i=1

(1)

This value is contained in the interval [-1, 1] and is a measure of the correlation (under d) between the predictions h(xi) and the labels yi. It is linearly related to the weighted error by Prid[yi = h(xi)] = (1 - e(h, d))/2.
Let H = {h1, h2, . . . , hN } be a finite set of N weak classifiers. For example, the weak classifiers might be simple decision stumps or decision trees of bounded size. For our present purposes, we say that the training examples S are weakly learnable under H if there exists  > 0 such that for any distribution d  m one can find hj  H such that e(hj, d)  . This is equivalent to there being a weak classifier that performs better than chance under distribution d. Now for any distribution d, maxj e(hj, d) is the best edge over classifiers in H. So

 = min max e(hj, d)
dm j

(2)

is the best edge over H under the least favorable distri-
bution on the examples, and weak learnability is simply equivalent to  being strictly positive so that, for any d  m, there exists hj with e(hj, d)   > 0.

Now consider the composite classifier h(x) =

N j=1

j hj

(x),

where





RN

.

This

gives

the

binary

clas-

sification y = sign(h(x)). Assume that hj  H implies

-hj  H. Then without loss of generality we can assume

that j  0, j = 1, . . . , N . We say that hj is active with

respect to weight coefficients  if j > 0.

The margin of h on the i-th training example is

µi() = yih(xi)/  1.

(3)

Clearly, µi()  [-1, 1]. µi() is a measure of the robustness or confidence of the composite classifier's decision on the i-th instance. The margin of the classifier h is the least margin over all examples, µ() = mini=1...m µi().
A maximum-margin classifier results by selecting  to maximize µ(). Such a classifier has margin

N
µ = max min jyihj(xi).
N i j=1

(4)

As shown in (Freund & Schapire, 1996) and (Ra¨tsch & Warmuth, 2005), von Neumann's min-max theorem applied to the m × N game matrix M (i, j) = yihj(xi), yields:

N

µ = max min

j yihj (xi)

N i=1,...,m j=1

m

= min max

diyihj (xi) = 

dm j=1,...,N i=1

where the last equality follows from (1) and (2). Hence  > 0 is the maximum achievable margin for any com-
posite classifier.

3 l1-REGULARIZED LOSS MINIMIZATION

We now state the general l1 regularized loss minimization problem and give conditions under which its solution yields a maximum margin classifier as regularization is relaxed. Moreover, we give an explicit bound on the value of the regularization parameter to yield a margin within prescribed accuracy of .
Fix a loss function L : R  R. Common choices include the exponential loss L(z) = e-z, used in AdaBoost, and logistic loss L(z) = ln(1 + e-z). The l1-Regularized Loss Minimization Problem (RLMP) with loss function L and parameter r > 0 is:


mN



min L() =


L yi

j hj (xi)

i=1 j=1

such that  1  r, j  0, j = 1, 2, . . . , N.

616

Xi, Xiang, Ramadge, Schapire

For continuous L, RLMP has at least one solution. If L is convex, then so is L and for any r > 0, RLMP will have the same solution as a convex program of the form


mN



min L yi jhj(xi) +   1

i=1 j=1

for some  > 0.

We first prove a fundamental property: at a solution of RLMP, all active weak classifiers have identical edges.

Lemma 1 (Uniform Learning). Assume the training exam-

ples are weak learnable. Assume L is differentiable and

strictly monotone decreasing, i.e., L (z) < 0 for all z  R.

Then any solution  of RLMP satisfies

N j=1

j

=

r.

Moreover, if we select d  m such that

di =

L (yi

m i=1

L

N j=1

j

hj

(xi

))

(yi

N j=1

j

hj

(xi))

,

i = 1, . . . , m, (5)

(as in AdaBoost), then the edges of all active weak classifiers are equal and no smaller than .

Proof. Using (5) then (1)



L()

m

N

k

= yihk(xi)L yi jhj(xi) (6)
i=1 j=1


mN



= e(hk, d) L yi jhj(xi) . (7)

i=1 j=1

Under weak learnability, there exists hk  H such that

e(hk, d)  . Hence L/k is strictly negative. If

N j=1

j

<

r,

then

the

objective

is

decreased

by

increasing

k; contradicting optimality.

Since  is a solution to RLMP, L()/j must be equal for all j > 0. Otherwise the objective can be further decreased by jittering the coefficients: increase those with smaller derivative and decrease those with larger derivative by the same amount. From (7) we see that the partial derivative with respect to k is proportional to the edge e(hk, d). So the edges of all active weak classifiers are equal.

Alternatively, for convex problems this follows by the

KKT conditions. Let fi() = -i, i = 1, . . . , N and

fN+1() =

N j=1

j -r.

Then

the

problem

can

be

written

as: min L() subject to fi()  0, i = 1, 2, . . . , N + 1.

By the KKT conditions, the optimizer  satisfies L()+

N +1 i=1

ifi()

=

0 with fi()

=

0 or i

=

0,

i = 1, 2, . . . , N + 1. Obviously if i > 0, i = 0. So

for all i

>

0,

L i

=

-N+1, and by (7), the edges of all

active weak classifiers are equal.

Let the edge of all active weak classifiers be . Assume  < . By weak learnability, there exists hk  H and k = 0 such that e(hk, d)  . Increasing k and decreasing any
current active weak classifier by the same small amount
will reduce the objective function, contradicting optimality.

We now use Lemma 1 to significantly extend a result of Rosset et al. (2004b) (for p = 1) by giving an explicit bound on the value r in RLMP to yield a margin within prescribed accuracy of .
Theorem 1. Assume L is convex, differentiable and L (z) < 0 for all z. Let (r) be a solution of RLMP with parameter r and h(r) have margin µ((r)). Then for small  > 0

L

L (r) (r( - ))

<

 m(1 - )



µ((r))





-

.

Thus, if  > 0, limz L (z)/L (z(1 - )) = 0, then
(r) is margin maximizing in the limit, i.e., rk   implies limk µ((rk)) = .

Proof. Consider a solution  of RLMP for fixed r. By
Lemma 1, r = j j. Let µi = µi() denote the margin of the i-th example. Then the edges of the active classifiers
with respect to d, given by (5), satisfy:

  e(hj, d)

=

m
yihj (xi)
i=1

L (yi

N j=1

j

hj

(xi))

m k=1

L

(yk

N j=1

j

hj

(xk

))

=

m
yihj (xi)
i=1

L (rµi)

m k=1

L

(rµk

)

.

Multiplying both sides by (j/  1)

m k=1

L

(rµk ),

summing over j, and combining terms yields

m
L (rµi)(µi - )  0.
i=1

(8)

Next, we split this sum into three sums over µi <  - ,  -   µi < , and µi  . This yields:

0

L (rµi)(µi - )

i:µi < -

+ L (rµi)(µi - )

i: -µi <

+ L (rµi)(µi - )

i:µi 
 -K()L (r( - )) + 0 + m(1 - )L (r).

The first sum uses the convexity of L with K() defined to be the number of examples for which µi <  - . The
second term is at least 0 because L (x) < 0, x. The last

617

Speed and Sparsity of Regularized Boosting

term is at least m(1 - )L (r), using convexity and that µi's are bounded by 1. Thus,

K ()



m(1 - )L (r) L (r( - )) .

(9)

If L (r)/L (r( - )) < /(m(1 - )), then K() < 1, so all example margins are at least (-). Finally, given
the additional assumption stated in the theorem, for any  > 0, K()  0 as rk  . Hence limk µ((rk)) = .

Theorem 1 implies that loss functions with exponential decay tails, such as exponential loss L(x) = e-x and logistic loss L(x) = ln(1 + e-x), are margin maximizing. Polynomial decay functions L(x) = x-k, k > 0 do not belong to
this category. Theorem 1 further provides an explicit rate of convergence of the margin to . For instance, for exponential loss, it shows that if r > (1/) ln(m(1 - )/) then µ((r))   - .
Rosset et al. (2004b) define a non-increasing and nonnegative loss function L to be a margin maximizing loss function if there exists R > 0 (possibly R = ) such that for all  > 0, limz R L(z(1 - ))/L(z) = . For such functions, they show the solution of RLMP maximizes the margin as r  . To see how this relates to our results, we consider separately the two cases that R is finite or infinite. When R = , it must be that limz L(z) = 0. Using l'Ho^pital's rule yields limz L(z(1 - ))/L(z) = limz L (z(1 - ))/L (z). The latter condition matches the condition in Theorem 1. When R < , such as for the hinge loss function, we do not need to let r  , as sug-
gested in (Rosset et al., 2004b), to achieve the maximum
margin. Actually, as the following theorem shows, for loss
functions that decrease monotonically to a certain positive
point and remain constant afterwards, we can always find a maximum margin classifier by solving RLMP with finite r.
Theorem 2. Assume L is continuous, strictly monotone decreasing up to R > 0 and constant afterwards, i.e., L(z) = L(R), z  R. Then a solution of RLMP with r = R/ is margin maximizing.

Proof. There exists  with  1 = 1 such that h is margin maximizing. We first show that  = R/

is a solution to RLMP, and then show that any non-

margin-maximizing composite classifier cannot be a so-

lution to RLMP. First, since  is margin maximizing

yi

N j=1

j hj

(xi

)





for

each

i.

Replacing  with

 yields

N
yi jhj(xi)  R/ = R , i = 1, . . . , m.
j=1

Then for each i, L(yi

N j=1

j

hj

(xi

))

=

L(R).

Thus

L() = mL(R) achieves the least possible value of

L. So  is a solution and mL(R) is the minimum

achievable loss. Assume ¯ is a solution of RLMP with r = R/ that achieves margin ¯ < . Then for at

least one i, yi

N j=1

¯j

hj

(xi

)/

N j=1

¯j

=

¯.

Addition-

ally,

N j=1

¯j

 r = R/.

So for this i, we have

yi

N j=1

¯j

hj

(xi)



¯R/

<

R,

which

makes

L(¯ )

>

mL(R), a contradiction of optimality.

An example is the hinge loss L(x) = max{0, (1 - x)}.
Theorem 2 says RLMP with hinge loss yields a margin maximizing solution when r = 1/.

Finally, if we slightly strengthen the margin maximizing loss function condition of Rosset et al. (2004b), we obtain the following stronger result.
Theorem 3. Assume that L is monotone decreasing and that for z  0 and   0, L((1 - )z)/L(z)  f (z) where f is a strictly monotone increasing function. Then



-

µ((r))



f -1(m) .

r

Proof. We note that

m i=1

L(µi

()



1).

L() Hence

= by

m i=1

L(yih(xi))

monotonicity of L

=

L(µ()  1)  L()  mL(µ()  1).

Let  be margin maximizing with  1 = r. Then by optimality of (r),

L(µ((r))r)  L((r))  L()  mL(r).

Hence L(µ((r))r)/L(r)  m. Let z = r. We can write

L(µ((r))r) = L(zµ((r))/) = L((1 - r)z)
with r = 1 - µ((r))/. Since µ((r))  , r  0. So by the theorem's assumption

f (rz)  L((1 - r)z)/L(z)  m.
Inverting f yields rz  f -1(m) and substituting for r and z gives the result.

The exponential loss function satisfies the assumptions of Theorem 3 with f (z) = ez. Hence for exponential loss,  - µ((r))  ln(m)/r.

4 NEW ALGORITHM: ADABOOST+L1
Although conceptually simple, the direct solution of RLMP and related approximate methods such as -boosting are too slow to be practical for large weak classifier spaces. To address this, we propose a new hybrid algorithm, AdaBoost+L1 (see Figure 1), that efficiently penalizes the

618

Xi, Xiang, Ramadge, Schapire

AdaBoost+L1

1.

Initialize:

select





(0, 1],

set

r0

=

0,

0

=

0



N
R

,

U0

=

,

and

d0(i)

=

1 m

,

i

=

1, . . . , m.

For t = 1, 2, . . .

2. Find hk  H such that e(hk, dt-1)  .

3. Update U and r:

Ut = Ut-1  {k},

rt

=

rt-1

+

 2

ln

.1+e(hk ,dt-1 )
1-e(hk ,dt-1 )

4. Solve the (small) convex minimization problem over

{j }jUt :


m



min exp -yi jhj(xi)

i=1 jUt

s.t. j  rt, j  0, j  Ut
jUt

5. Update the coefficients:

t(j) =

j 0

if j  Ut; otherwise

.

6. Update the distribution:

dt(i) =

,exp(-yi

N j=1

t(j)hj (xi))

m i=1

exp(-yi

N j=1

t(j)hj (xi))

i = 1, . . . , m.

Figure 1: The AdaBoost+L1 algorithm.

l1 norm of the coefficients. The hybrid nature of the algorithm is reminiscent of Grove and Schuurmans' LPAdaBoost algorithm (1998). Steps 2 and 3 of each iteration perform the first part of a standard AdaBoost update with possibly non-optimal weak classifier selection (meaning the selected weak classifier need not have maximum edge, just not worse than ). Before updating the coefficients, step 4 solves a small l1-regularized loss minimization problem using only the coefficients in the set Ut. This can be thought of as a balancing step that seeks to bring the active classifiers into the uniform learning condition. Finally, steps 5 and 6 update the coefficients and the distribution. A key point is that the optimization problem in step 4 is convex and only involves a small number ( N ) of the weak classifiers. The set Ut consists of the indices that have been used at least once up to and including round t. The parameter   (0, 1] controls the speed of relaxation of the regularization, and is essentially the same as Friedman's (2001) use of "shrinkage" with boosting. Smaller values of  impose more aggressive regularization and hence seek sparser solutions.
The next theorem shows that AdaBoost+L1 yields a maximum margin solution as t  .
Theorem 4. Let t be the solution of AdaBoost+L1 after round t and set ~ t = t/ t 1. Then limt µ(t) = , and every limit point of ~ t is margin maximizing.
To prove the theorem, we first introduce a lemma. Let At =

{j : t(j) > 0}.
Lemma 2. The edges e(hj, dt), j  At, have a common value t. Moreover, lim inft t  .

Proof. By the proof of Lemma 1, the edges e(hj, dt), j  At, have a common value t. If t < , then in the next round k / Ut with e(hk, dt)   is selected and
added to Ut+1. But {Ut} is a monotone increasing set se-
quence bounded above by a finite set. Hence it can only be
enlarged a finite number of times. So there exits t0 such that t  , t  t0. Hence lim inft t  .

Proof. (Of Theorem 4) In each round, rt increases by at

least

1 2



ln((1

+

)/(1

-

)).

So

limt rt

=

.

By

Lemma 2 there exists t1 such that t > 0 for t  t1,

where t is as above. Henceforth we restrict attention to

t  t1. Then the edges with j  At are positive, and by

(7), t 1 = rt.

Let Kt() denote the number of examples with margin below (t - ) after round t. Following the steps in the proof of Theorem 1 we obtain

0



Kt()



m(1 L

- t)L (rtt (rt(t - ))

)

.

Now 1 - t is bounded, Kt() is an integer and for L(z) = e-z, limt L (rtt)/L (rt(t - )) = 0. Hence there
exists t()  t1 with Kt() = 0 for t  t(). Thus for t  t(), t -   µ(t)  . Taking the lim inf of each term and using Lemma 2 gives:  -   lim inft(t - )  lim inft µ(t)  lim supt µ(t)  . Since  > 0 is arbitrary, limt µ(t) = . Let subsequence
{~ tk } converge to a limit point  of {~ t = t/ t 1}. Since µ is continuous in ,  = limk µ(tk ) =
limk µ(~ tk ) = µ(limk ~ tk ) = µ().

We note that the Boost+L1 framework can be generalized to other loss functions such as logistic loss, to achieve a maximum margin solution in the limit as regularization is relaxed.

5 EXPERIMENTS
We compared AdaBoost+L1 with AdaBoost on five datasets: a synthetic dataset, Ringnorm, and four datasets from the UCI online machine learning repository. Ringnorm is a binary classification task between two 20dimensional Gaussian distributions: N (0, 4I) and N (µ, I) (µ = (a, a, . . . , a), a = 1/ 20). The three real datasets are Diabetes (detecting diabetes based on medical information), German (determining credit risk based on financial histories), Spam (identifying spam email messages based on word frequencies) and Ionosphere (classifying radar returns from the ionosphere). In Ringnorm we randomly generated 100 examples for training and 5000 examples for

619

Speed and Sparsity of Regularized Boosting

testing; In Diabetes, we used 100 examples for training and the other 668 for testing; In Spam, we used 100 for training and the other 4501 for testing; In German, we used 200 for training and the other 800 for testing; In Ionosphere, we used 100 for training and the other 250 for testing. For each dataset, we ran both algorithms until the error rates stopped decreasing and overfitting occurs. We ran all the experiments 20 times (using different samples of training/testing examples) and averaged the results.
We used simple decision stumps as weak classifiers. This illustrates the relative performance of the new algorithm. Better performance may be achieved by more complex weak classifiers such as decision trees.
Since each algorithm learns at a different rate, direct roundby-round comparison of test performance or sparsity is not a useful metric. Each algorithm will also begin overfitting at different rounds, so comparing results at the final round is also not appropriate. In applications one could use cross validation to determine the stopping point for training that yields the best  for each dataset. Hence it makes more sense to compare the best test error of each algorithm over all rounds and the associated sparsity of the classifier at this "sweet spot".
To this end, in Figure 2 we plot the test error and classifier margin µ() (on training examples) as functions of the number of classifiers chosen. The best error rates achieved at the "sweet spots" and the associated number of base classifiers (average over 20 trials) are shown in Tables 1 and Table 2.

Ringnorm Diabetes German
Spam Ionosphere

AdaBoost
25.5% 26.7% 26.9% 11.3% 12.5%

AdaBoost+L1
25.3% 26.6% 26.7% 11.3% 12.6%

Rel. Improvement
0.8% 0.4% 0.8% 0.2% -0.6%

Table 1: Comparison of error rates

Ringnorm Diabetes German
Spam Ionosphere

AdaBoost
121.8 7.6 22.1 31.3 26.7

AdaBoost+L1
55.0 11.9 16.5 26.5 19.6

Rel. Improvement
54.8% -56.6% 25.2% 15.3% 26.8%

Table 2: Comparison of numbers of classifiers

As shown in the left column of Figure 2 and the tables, AdaBoost+L1 achieves similar best error rates to AdaBoost. But in most cases, AdaBoost+L1 achieves its best error rate with much fewer base classifiers (Table 2). The only exception is the Diabetes dataset. However, it should be noted that the actual numbers in this case are very small,

generalization error

generalization error

0.45 0.4
0.35 0.3
0.250
0.3 0.29 0.28 0.27 0.26
0
0.32 0.3
0.28 0.260
0.25 0.2

Adaboost Adaboost+L1

0.5 0

min Margin

-0.5

-1

50 100 number of classifiers

150 -1.50

Adaboost Adaboost+L1

50 100 number of classifiers

150

(a) Ringnorm dataset

Adaboost Adaboost+L1

0.2 0

min Margin

-0.2

-0.4

10 20 30 number of classifiers

-0.6 -0.8 40 0

Adaboost Adaboost+L1

10 20 30 number of classifiers

40

(b) Diabetes dataset

Adaboost Adaboost+L1

-0.2

min Margin

-0.4

-0.6

10 20 number of classifiers

-0.8 30 -10

Adaboost Adaboost+L1

10 20 number of classifiers

30

(c) German dataset

Adaboost Adaboost+L1

0.5 0

generalization error

min Margin

generalization error

generalization error

0.15
0.10
0.17 0.16 0.15 0.14 0.13 0.120

-0.5

10 20 30 number of classifiers

40

-10

(d) Spam dataset

Adaboost Adaboost+L1

10 20 30 number of classifiers

40

Adaboost Adaboost+L1

0.2 0

min Margin

-0.2

10 20 30 number of classifiers

-0.4 40 -0.60

Adaboost Adaboost+L1

10 20 30 number of classifiers

40

(e) Ionosphere dataset

Figure 2: Experiment results on five datasets. The left column shows the error rate on testing data, the right column shows the minimal margin on training data.

so a small difference results in a large percentage change. The right column of Figure 2 shows that with a fixed number of classifiers, AdaBoost+L1 achieves equal or slightly better margins on the training examples.

620

Xi, Xiang, Ramadge, Schapire

frequency of being selected

1 AdaBoost
0.8 AdaBoost + L1
0.6
0.4
0.2
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57
feature index
Figure 3: The frequencies of different features being selected, Spam Dataset

From Table 1 we can see that the improvement in error rate is modest, or even too small to justify the extra complexity of the algorithm. Therefore AdaBoost+L1 is unnecessary if the only objective is low classification error. On the other hand, as shown in Table 2, except for the Diabetes dataset, AdaBoost + L1 reduces the number of active classifiers by a considerable percentage. Therefore AdaBoost + L1 is preferable in applications where a sparse set of features is advantageous for other purposes.
In the Diabetes and German datasets, the error rate of AdaBoost+L1 becomes higher than that of AdaBoost at the end of the curve. As mentioned above, this does not mean that AdaBoost+L1 has worse performance. In the German dataset, for example, AdaBoost+L1 hits the overfitting watershed earlier than AdaBoost. With 20 classifiers, AdaBoost is still "fitting" the data, while AdaBoost+L1 is already overfitting after passing a sweet spot with better performance.
AdaBoost+L1 takes more rounds to select the same number of classifiers since it repeatedly adjusts the weights of selected classifiers and removes some previously selected classifiers (by zeroing their coefficients). For example, in the Ringnorm experiment, to generate around 120 active classifiers (as shown on the graph), we ran AdaBoost+L1 for 1000 iterations, while AdaBoost reached this number of active classifiers after 700 iterations. This accords with the main goal of the algorithm: to select fewer active classifiers over a given number of rounds while at the same time achieving at least as good generalization performance. As the results indicate, this is achieved in most cases.
The results suggest that AdaBoost+L1 chose more informative classifiers/features. To better understand this, we investigated the Spam experiments in greater detail. For each of the 20 repetitions, we recorded whether a single-feature classifier had been selected by AdaBoost and AdaBoost+L1 at the 60th iteration. For each feature, we then plotted a histogram showing the selection frequency under each algorithm (Figure 3). Both algorithms select important features such as "free", "$" and "!". However, AdaBoost tends to select presumably irrelevant words such as "you" (number 19 on the plot), "mail"

(10) and left parenthesis "(" (50) about 20% more often than AdaBoost+L1. Other words such as "all" (3), "people" (13) and "re" (45) are chosen by AdaBoost about 10% more often. By avoiding these non-discriminative features, AdaBoost+L1 is able to achieve roughly the same classification accuracy using a smaller set of weak classifiers.
6 CONCLUSION
We have shown that l1-regularized boosting (RLMP), under various conditions (Theorems 1, 2, 3), attains the maximum margin as regularization is relaxed. Moreover, we obtained a quantitative relationship between the regularization parameter r and the achievable margin. We provided similar convergence rate results for -AdaBoost and gave a simple proof that it is also margin-maximizing as  vanishes. We observe that approximate uniform learning is also enforced in other iterative margin maximizing algorithms, such as TotalBoost (Warmuth et al., 2006) and LPBoost (Grove & Schuurmans, 1998), under the name "totally corrective learning." The AdaBoost+L1 algorithm provably achieves the maximum margin as t   and has been shown empirically to efficiently yield sparser solutions than AdaBoost with roughly equal "sweet spot" generalization performance. This suggests it is doing better feature selection. A more precise quantitative analysis of the trade-off between sparsity, margin and generalization merits further investigation.
APPENDIX: -ADABOOST
In this appendix, we give an analysis of -boosting (Hastie et al., 2001). This method works much like AdaBoost and related algorithms: a composite classifier h is built up over a sequence of rounds. In each round, a distribution d as in (5) is defined, and a weak classifier hj with large edge e(hj, d) selected. However, unlike AdaBoost, -boosting increases j by only some small  > 0. Rosset et al. (2004a) show that -boosting approximates the solution to the l1-regularized boosting problem, and for  small, maximizes the margin in the limit of convergence (Zhang & Yu, 2005).

621

Speed and Sparsity of Regularized Boosting

-AdaBoost uses the exponential loss L(z) = exp(-z).

Below we provide an explicit bound for -AdaBoost on the difference of the margin from  as a function of  and

the number of rounds. Let t denote the coefficient vector after t rounds with 0 = 0. Theorem 5. Fix 0 <  < . After t rounds, the margin

µ(t) of -AdaBoost satisfies

 - µ(t)





-

ln(1

+

( -) 1- 

)

+

ln m

 t

ln m  + for  small .
t

To prove Theorem 5, we first introduce a lemma.

Lemma 3. For 0 <  < 0.5,

L(t) 

1 -  t 1 - 2 m.

(10)

Proof. At round t, suppose the j-th weak classifier with

edge e(hj, dt)   is selected. Then according to the

algorithm, t = t-1 + ej, where ej is the j-th natural

basis vector. Using Taylor Series Expansion (about  = 0),

we obtain L(t-1 + ej) =

 k=0

C(k) k!

k

,

where

C(k) =

dkL(t-1 + ej ) dk =0

m

= (-yihj (xi))k exp(-yiht-1 (xi))

i=1

=

C(1) k odd L(t-1) k even.

Similar to (7), we have C(1) = -e(hj, dt)L(t-1)  -L(t-1). Therefore,

L(t) = L(t-1 + ej)



L(t-1)

1 -  + 2 - 3 + · · · 2! 3!

 L(t-1)(1 -  + 2 - 3 + · · · )

=

L(t-1)

1 -  1 - 2

.

In the third line, we used the fact that   0.5. Repeating the argument for each round t and noting that L(0) = m gives (10).

Proof. (Of Theorem 5) At round t, let the mar-

gin on the i-th example be µi = µi(t) =

yiht (xi)/ t 1. Now t 1 = t, because every co-

efficient is monotone non-decreasing. Hence L(t) =

m im=1 i=1

exp(-yiht exp(-tµi)

(xi 

)) = exp(-t

m i=1

exp(-

t

mini µi). Using

1µi) µ(t)

= =

mini µi, and combining this inequality with Lemma 3

yields exp(-tµ(t)) 

1-  1-2

t
m.

Taking logs and

rearranging gives (10).

Acknowledgements
This research was supported in part by NSF Grants IIS0325500 and CCR-0325463.
References
Freund, Y., & Schapire, R. E. (1996). Game theory, on-line prediction and boosting. In COLT '96: Proc. ninth annual conf. on Comp. learning theory, pp. 325­332.
Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. J. Computer and System Sciences, 55(1), 119­139.
Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5).
Grove, A. J., & Schuurmans, D. (1998). Boosting in the limit: Maximizing the margin of learned ensembles. In Proc. Fifteenth National Conf. on Artificial Intelligence.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.
Lugosi, G., & Vayatis, N. (2004). On the Bayes-risk consistency of regularized boosting methods. Annals of Statistics, 32(1), 30­55.
Meir, R., & Ra¨tsch, G. (2003). An introduction to boosting and leveraging. In Mendelson, S., & Smola, A. (Eds.), Advanced Lectures on Machine Learning (LNAI2600), pp. 119­184. Springer.
Ra¨tsch, G., & Warmuth, M. (2005). Efficient margin maximization with boosting. J. Machine Learning Research, 6, 2131­2152.
Rosset, S., Zhu, J., & Hastie, T. (2004a). Boosting as a regularized path to a maximum margin classifier. J. Machine Learning Res., 5, 941­973.
Rosset, S., Zhu, J., & Hastie, T. (2004b). Margin maximizing loss functions. In Advances in Neural Information Processing Systems 16.
Schapire, R. E. (2002). The boosting approach to machine learning: An overview. In Nonlinear Estimation and Classification. Springer.
Schapire, R. E., Freund, Y., Bartlett, P., & Lee, W. S. (1998). Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5), 1651­ 1686.
Warmuth, M. K., Liao, J., & Ra¨tsch, G. (2006). Totally corrective boosting algorithms that maximize the margin. In Proc. Int. Conf. on Machine Learning.
Zhang, T., & Yu, B. (2005). Boosting with early stopping: Convergence and consistency. Annals of Statistics, 33(4), 1538­1579.

622

