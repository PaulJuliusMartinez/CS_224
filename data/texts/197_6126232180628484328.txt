FF-Replan: A Baseline for Probabilistic Planning

Sungwook Yoon

Computer Science & Engineering

Arizona State University

Tempe, AZ 85281

Sungwook.Yoon@asu.edu

Alan Fern

Computer Science Department

Oregon State University

Corvallis, OR 97331
afern@cs.orst.edu

Robert Givan

Electrical & Computer Engineering

Purdue University

West Lafayette, IN 47907

givan@purdue.edu

Abstract

FF-Replan was the winner of the 2004 International Proba-
bilistic Planning Competition (IPPC-04) (Younes & Littman
2004a) and was also the top performer on IPPC-06 domains,
though it was not an ofﬁcial entry. This success was quite sur-
prising, due to the simplicity of the approach. In particular,
FF-Replan calls FF on a carefully constructed deterministic
variant of the planning problem and selects actions according
to the plan until observing an unexpected effect, upon which
it replans. Despite the obvious shortcomings of the approach
and its strawman nature, it is the state-of-the-art in probabilis-
tic planning as measured on recent competition benchmarks.
This paper gives the ﬁrst technical description of FF-Replan
and provides an analysis of its results on all of the recent
IPPC-04 and IPPC-06 domains. We hope that this will in-
spire extensions and insight into the approach and planning
domains themselves that will soon lead to the dethroning of
FF-Replan.

Introduction

Probabilistic planning is naturally formulated using Markov
Decision Processes (MDP) (Puterman 1994) and many prob-
abilistic planning techniques have been developed based
on MDP formulations. When the ﬁrst international prob-
abilistic planning competition (Younes & Littman 2004a)
was held in 2004, as expected, most of the entries were
based on MDP solvers. Surprisingly enough, the winner
of the IPPC-04 was a planner based on deterministic plan-
ning techniques, FF-Replan.
In IPPC-06 (Bonet & Givan
2006), FF-Replan was not entered, but tests reported here
shows it outperforms all entrants in the probabilistic track.
FF-replan is thus the currently reigning state-of-the-art tech-
nique for probabilistic planning problems as measured on
planning competition benchmarks.

FF-Replan has obvious shortcomings. FF-Replan ﬁrst de-
terminizes the input domain, removing all probabilistic in-
formation from the problem, and then synthesizes a plan.
During the execution of this plan, should an unexpected state
occur, the planner replans in the same determinization of the
problem. Execution and replanning continue until a goal is
reached. FF-replan is similar in style to contingent plan-
ning (Geffner 1998), although it does not explicitly gener-
Copyright c(cid:2) 2007, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

ate a plan tree. FF-Replan does neither contingent planning
nor conformant planning. FF-Replan does not consider the
multiple potential effects of an action and this is the biggest
weakness of the approach. On the other hand, we believe
that the weaknesses of FF-Replan can motivate research that
compensates for them while maintaining the merits of the
FF-Replan system.

Probabilistic planning and deterministic planning have
historically taken different approaches although they have
similar goals. Leveraging MDP formulations, probabilistic
planning has typically focused on value functions or policies
for the entire state space, while deterministic planning de-
veloped intelligent heuristic functions enabling exploration
of a restricted portion of the state space that is considered to
be enroute to the goal state. By relying on search, state of
the art deterministic planners function with enormous speed
and efﬁciency—this is the key to the moderate success of
FF-Replan.

Cross-fertilization between the deterministic and prob-
abilistic planning communities has been challenging but
steady (Mausam, Bertoli, & Weld 2007; Meuleau & Smith
2003). For example, in the probabilistic planning commu-
nity, (Boutilier, Brafman, & Geib 1998) used reachability
analysis from deterministic planning to prune out unneces-
sary Bellman backup operations. (Karabaev & Skvortsova
2005) considered efﬁcient state space enumeration based on
a plangraph reachability analysis. Another very direct ap-
plication of deterministic planning techniques to probabilis-
tic problems was conducted by Pgraphplan (Blum & Lang-
ford 1999). In the deterministic planning community, prob-
abilistic planning techniques have been applied to ﬁnd plan-
ning domain “solvers” that ﬁnd control information for plan-
ning domains, for example using approximate policy itera-
tion (Fern, Yoon, & Givan 2006). We hope that FF-Replan’s
success will promote further cross-fertilization, especially
the use of deterministic planning techniques in probabilistic
planning.

In what follows, we ﬁrst overview the probabilistic plan-
ning competitions. Next we describe the architecture and
variations of FF-Replan and present its performance on the
domains from the two planning competitions. We discuss
the shortcomings of FF-Replan and potential improvements
to FF-Replan that can overcome these shortcomings.

352

The Probabilistic Planning Competitions

There have been two International Probabilistic Planning
Competitions: the ﬁrst in 2004 run by Michael Littman and
Hakan Younes (www.cs.rutgers.edu/˜mlittman/
topics/ipc04-pt) and the second in 2006 run by Blai
Bonet and Bob Givan (www.ldc.usb.ve/˜bonet/
ipc5). Each has been associated with the International
Conference on Automated Planning and Scheduling in the
same year. Problems for these competitions have been spec-
iﬁed in the Probabilistic Planning Domain Description Lan-
guage (PPDDL) (Younes & Littman 2004b), which deﬁnes
a probabilistic planning problem as a set of probabilistic ac-
tion schemas together with an initial state and a goal de-
scription. Each component is speciﬁed exactly as in PDDL
(McDermott 1998), the widely used deterministic planning
language, except that there is one newly available probabilis-
tic effect for action deﬁnitions

(probabilistic

p1 e1 . . . pn en)

where each ei is a PPDDL effect (either probabilistic or
deterministic) that occurs with probability pi whenever the
overall probabilistic effect occurs. A probabilistic effect rep-
resents a distribution over deterministic effects. The deﬁni-
tion is recursive with the base case being when an ei is a de-
terministic effect, in which case the probability of ei is just
pi. The recursive case is when ei is also a probabilistic ef-
fect, in which case we consider the probability distribution
over deterministic effects deﬁned by ei and multiply those
probabilities by pi. Note that the probabilistic construct can
also be used to specify an initial state distribution.

The planners in each competition were evaluated in an
online planning setting over the internet by connecting
to a competition host server (Younes & Littman 2004a;
Bonet & Givan 2006) that acted as an environment simu-
lator. The planners were evaluated on a ﬁxed set of planning
domains using a ﬁxed set of problems from each domain.
For each problem, the server ﬁrst sent the planner the initial
state and goal. The planner-server interaction then alternated
between the planner sending an action and the server send-
ing an updated state to the planner. For each problem, the
planners were given a ﬁxed real-time limit, in which they
would attempt to solve the problem for up to thirty trials.
The planners were evaluated on a per problem basis accord-
ing to the fraction of trials that reached the goal (out of
thirty) and the average number of actions selected on suc-
cessful trials.

FF-Replan - The Architecture

FF-Replan is an action selection algorithm for online plan-
ning in probabilistic domains. FF-Replan has a very simple
architecture. Given a new probabilistic planning problem,
consisting of a domain description, a goal, and initial state,
FF-Replan ﬁrst generates a deterministic planning problem
as described below. FF-Replan then uses the deterministic
planner FF (Hoffmann & Nebel 2001) to compute a totally-
ordered plan for the generated deterministic problem. Dur-
ing execution of the resulting plan, if confronted with an un-
expected state, the process is repeated with the unexpected

353

state as the initial state, until a goal state is reached. Note
that the determinization process is conducted once before
execution begins and there is a potential improvement of the
system by considering adaptive determinization.

Determinization
FF-Replan determinizes the input domain at the start of plan-
ning. It converts the probabilistic domain deﬁnition into a
deterministic domain, different from the input domain. This
simple idea turns out to be quite useful for many competition
domains.

. . .

We have tried two straightforward methods of deter-
minization. The ﬁrst is single-outcome determinization,
which selects one outcome for each probabilistic construct.
en, the determinization process selects
Among e1
one ei, otherwise ignoring the probabilities pi and other
effects ei. Probabilistic effects can be nested and the de-
terminization process recursively selects one effect among
the nested probabilistic constructs. The single-outcome de-
terminization process produces one deterministic action for
each probabilistic action. One can design several heuristic
methods for selecting the outcome to keep—e.g. choos-
ing the most likely effect or choosing the probabilistic ef-
fect with the most add effects are central candidates. A key
problem with the single-outcome approach is that the per-
formance of the replanner depends critically on the method
for selecting the effect used and that any choice may ne-
glect important possible effects. For any such outcome-
selection heuristic, one can easily design domains where
the resulting planner will perform poorly. The ﬁrst variant
of FF-Replan, that participated in and won IPPC-04, used
the single-outcome approach, using a heuristic that selected
the outcome with highest probability. In our experimental
results, we compare this variant with the later variant de-
scribed below, across the competition domains from both
competitions.

The second approach, which the current FF-Replan takes
as its determinization process, is all-outcomes determiniza-
tion, which considers every probabilistic outcome as a dis-
tinct deterministic action. For each possible outcome ei, FF-
Replan generates one separate action for each effect. If ei is
deterministic then the action corresponding to ei is also de-
terministic. Otherwise for probabilistic ei (i.e. nested prob-
abilities) we recursively apply the all-outcomes procedure.
Thus, the number of deterministic actions is exponential in
the depth of probabilistic nesting. In practice, this nesting is
shallow and has not caused a problem with FF-Replan in the
ﬁrst two competitions.

Since this approach considers every possible probabilis-
tic outcome, the performance does not depend on the choice
of the outcome of each action as is the case for the previ-
ous idea. While the theoretical properties of this approach
are weak, one can at least say that with all-outcomes deter-
minization, FF-Replan will select an action a in state s only
if a is the ﬁrst action of some sequential plan that has non-
zero probability of reaching the goal from s. This also im-
plies that FF-Replan will cease selecting actions whenever
it enters a dead-end state. However, a key potential weak-
ness of this approach is that the planner is given no informa-

tion about probabilistic effects and rather treats them all as
equal. This means that FF-Replan will not explicitly attempt
to avoid actions that have a non-trivial chance of leading to
dead-ends or moving away from the goal. One can easily
develop domains that exploit this weakness.

Both determinization approaches are implemented by
storing a mapping from the generated deterministic actions
back to the underlying (probabilistic) actions from which
they were created. Then FF-Replan maps the determinis-
tic actions back to the available probabilistic actions during
plan execution as it communicates with the server. Consid-
ering probabilistic outcomes as separate deterministic out-
comes has been discussed before in the MDP community
(Boutilier, Dearden, & Goldszmidt 2000).
In that work,
determinization is used in combination with probabilistic
choice (nature’s selection) to facilitate a dynamic program-
ming formulation.

FF-Replan’s determinization has some similarity to hind-
sight optimization (Wu, Chong, & Givan 2002).
In hind-
sight optimization, the future sequence of nature’s choices is
drawn from a simulator and used to provide a non-stationary
determinization of the actions. Planning under this non-
stationary determinization provides an estimate of the dis-
tance to the goal. The mean value of this estimate over mul-
tiple simulations underestimates the true distance and can
be used as a heuristic to guide action selection. This strat-
egy performed quite well in practice as reported in (Mercier
& Van HenTenRyck 2007). FF-Replan’s determinization
strategy is similar to hindsight optimization but FF-Replan’s
strategy is more aggressive. Not only does FF-Replan as-
sume the knowledge of the future (since the domain is deter-
ministic), but also FF-Replan is allowed to select the most
helpful outcome, especially if the deterministic planner is
optimal. When the most helpful outcomes correspond well
to reality, which is often the case in real problems (i.e. the
desired effect is quite likely), the all-outcomes determiniza-
tion approach will often perform well. We will discuss re-
laxing FF-Replan’s strategy toward hindsight optimization
in the discussion section.

Planning
FF-Replan maintains a partial state-action mapping using a
hash-table which is initially empty. When FF-Replan en-
counters a state that is not in the table, then it determinizes
the problem and synthesizes a plan using FF. FF-Replan then
simulates the plan according to the deterministic action def-
initions resulting in a state-action sequence whose pairs are
put in the hash table. The ﬁrst action of the plan is then exe-
cuted in the environment, which returns a new current state.
FF-Replan thus produces a partial policy in an online fash-
ion. Of course due to the deterministic approximation, the
partial policy has no quality guarantees in general. One di-
rection for future improvement might be to adaptively repair
the partial policy—for example, by attempting to break out
of loops.

Standard planning approaches propositionalize existential
goals as disjunctions over instantiations with the available
objects. For example, if the goal is stacking all the blocks in
the Blocksworld, although the quantiﬁed goal is a single sen-

354

tence, ∃x∃y . . .∃z (on x y)∧ . . .∧ (on . z), the proposi-
tionalized goal is a disjunction of any stacked blocks, which
is factorial in the number of blocks involved. The result-
ing disjunction of the instantiated goals is sound in theory
but in practice, state of the art planners have difﬁculty deal-
ing with such goals, due to the popular focus on exploiting
conjunctive goal structure. Because the IPPC competitions
have used such goals, FF-Replan uses a very simple initial
approach to this problem. Rather than deal with the quan-
tiﬁed goal, FF-Replan picks an arbitrary grounded form of
any existential goal. The obvious pitfall of this approach is
that some groundings of the goal are not reachable or are
much more expensive to reach from the initial state. Sim-
ple sampling of the grounded goals and testing with relaxed
reachability analysis would be an immediate but untried im-
provement on this strategy. More extensive improvements
handling this difﬁcult problem should also be considered in
the future.

Competition Results

Here we consider the competition results from IPPC-04 and
IPPC-06 (Younes & Littman 2004a; Bonet & Givan 2006)
extended with new experiments to include both variants of
FF-Replan. Figure 1 contains a brief description of each of
the domains used in these competitions.

IPPC-04
Figure 2 gives the results for IPPC-04. Each column corre-
sponds to a planner and each row corresponds to a problem.
In the competition, 30 trials per problem were conducted
with a total time limit of 15 minutes for the 30 trials. Note
that this time includes the communication time and the plan-
ning time. Each cell of the ﬁgure gives two numbers. The
ﬁrst is the number of successful trials for each planner and
problem pair and the second number in parens is the average
time required to achieve the goal in seconds on successful
trials. The columns labeled FFRs and FFRa correspond to
the single-outcome and all-outcomes variants of FF-Replan
respectively. Note that FFRa was not an ofﬁcial entry in
IPPC-04 and was evaluated on these problems for the pur-
poses of this paper. Also note that the planning domain deﬁ-
nitions, but not speciﬁc problems, had been released prior to
the competition and participants in the learning and control-
knowledge tracks were allowed to learn from the domain
description or provide control knowledge respectively. The
planners shown in boldface in the table used human provided
control knowledge. Classy was the only learning system.

Blocksworld and Boxworld Results. The problems
starting with “bw” are Blocksworld variants and problems
starting with “bx” are Boxworld variants. Both domains
have probabilistic actions that fail to achieve the intended
effect but the failures do not lead to dead-end states or sig-
niﬁcant departures from the expected state. These domain
characteristics are ideal for our replanning approach.
In
particular, the highest-probability heuristic used by FFRs is
the correct heuristic in this domain since the intended ef-
fects are always more probable. Accordingly in these do-
mains the FF-Replan variants dominate all of the regular

entries. Compared to planners that used control knowl-
edge (NMRDPP+Control Knowledge (Gretton & Thiebaux
2004) and J1 (Yoon, Fern, & Givan 2004) ), FFRs was
slightly worse on two problems.

FFRs did fail

to solve some of

the larger sized
Blocksworld variants. The primary reason appears to be
that FF took too much time solving the deterministic ver-
sions generated by FFRs compared to the problems gener-
ated by FFRa. This behavior of FF is somewhat unexpected
since the actions used by FFRs are a subset of those used
by FFRa. Our best explanation of this is that some of the
failure outcomes considered by FFRs act as a sort of macro
action that allows FF to ﬁnd plans more quickly. For exam-
ple, the failure mode of the pickup action is for the block
to end up on the table, which would normally require two
actions. Thus the deterministic action corresponding to the
failure outcome of pickup can be used to quickly get a block
on the table. Of course while this behavior might lead to
computational speedups, the resulting heuristic is likely to
be less accurate.

Both variants of FF-Replan performed very well on Box-
world variants, as its base planner FF is strong on Logistics
style domains.

Remaining Problems.

The exploding-block, g-tire-
problem, and toh-prob-pre have actions that can lead to the
dead-end states and need careful planning that minimizes
the involvement of such actions. These types of domains are
among the worst case scenarios for both of our approaches.
For FFRs the “dead end outcomes” are not selected by the
determinization heuristic, so the planner is completely un-
aware of them. For FFRa the deterministic planner can op-
timistically choose only the desired outcomes required to
reach the goal. Both variants of FF-Replan failed on most
of the trails. Note, however, that for the exploding-block
domain all other planners fail and FFRa is the only plan-
ner that had any successful trials. Thus, even the planners
that explicitly reason about probabilities are not yet up to
the challenge of this domain. For ztravel we again see a
signiﬁcant difference between FFRa, which solves all trials,
and FFRs which always fails. Here the outcome selection
heuristic results in deterministic problems that are not solv-
able, explaining the failure of FFRs. Overall we see that
FFRa outperforms FFRs showing that the outcome selection
heuristic is not as effective in these domains as planning ac-
cording to the most optimistic potential outcomes, showing
the utility of this novel determinization approach.

Finally, the average time results show that FF-Replan is a
practical approach in these domains. Note that the current
implementation is not optimized and there are many direc-
tions for optimization. For example, rather than create a de-
terministic plan from scratch each time an unexpected out-
come occurs, it would be useful to investigate incremental
planning approaches that leverage the most recent determin-
istic plan.

IPPC-06
Figures 3, 4, 5 show the competition results from the IPPC-
06 probabilistic track. There were 4 ofﬁcial participants.
FFRa was an unofﬁcial strawman entry and performed bet-

355

ter than the winner FPG (Buffet & Aberdeen 2006). There
were 9 domains in IPPC-06 and there were 15 problems for
each domain, thus there were 135 problems total. For each
problem, the participants were asked to solve 30 trials as
in IPPC-04. Unlike IPPC-04, no participant utilized control
knowledge or learning.

Figure 3 shows the percent of successful trials for each do-
main. FFRa and FPG were the only performers who solved
at least one or more problems for each domain and both of
these planners exhibit more robust performance across do-
mains than the other planners. Compared to the winner FPG,
FFRa has a higher success rate in all domains with the ex-
ception of schedule. Overall we see that FFRa was the top
performer, often by signiﬁcant margins, in 5 of the 9 do-
mains. Furthermore, across all domains it was never worse
than second best.

There are four dead-end free domains, which are the ones
that we might expect FFRa to perform well on. Indeed, for
two of them, Zeno World and Random, FFRa performed the
best, as these domains are well suited to the replanning idea.
However, for the other two domains, Blocksworld and El-
evators domain, FFRa was second best. The primary rea-
son for the worse performance in Blocksworld and Eleva-
tor, appears to be that the deterministic planning time for the
larger problem instances was signiﬁcant. This is particularly
a problem since currently FF-Replan does not reuse previous
planning effort.

Among the 5 non-dead-end-free domains, on 3 domains,
Exploding Blocksworld, Drive and Pitchcatch, FFRa per-
formed the best. We think that the primary reason for the
replanners’ success here is its scalability compared to the
MDP based techniques. One noticeable feature of these re-
sults is that the performance of FF-Replan on the explod-
ing blocksworld domain in IPPC-04 was much worse than
in IPPC-06. After some investigation we found that the ex-
ploding blocksworld problems in IPPC-06 are signiﬁcantly
easier than the ones in IPPC-04 in the sense that it is not
nearly as important to consider the “exploding outcome”
that results in a dead end. For the rest of the non-dead-end-
free domains, FFRa was close second. We were at ﬁrst sur-
prised at the high success rate of more than 80% for FFRa in
Tireworld. A closer investigation revealed that many of the
instances were not too hard in the sense that there was no
need for careful route selection. For other problems, where
careful route selection was necessary, FFRa appeared to be
fortunate and not get ﬂat tires.

Both the results in exploding blocksworld and Tireworld
suggest that it can be difﬁcult to deﬁne problem distribu-
tions that properly exercise the complexity of probabilistic
domains. One idea for future competitions would be to ex-
plicitly design some of the problem distributions with the
aid of a replanner to help ensure that the replanner does not
do well.

Figure 4 shows the average number of actions used on
successful trials in each domain. In some domains we see
that FF-Replan performs signiﬁcantly worse than planners
that solve a similar number of problems. For example, in
the blocksworld variant, FOALP (Sanner & Boutilier 2006)
solves all of the problems and only requires approximately

IPPC

2004

IPPC

2006

Blocksworld

Boxworld

exploding-block

ﬁle-prob-pre

g-tire-problem

r-tire-problem

toh-prob-pre

ztravel-1-2

Blocksworld

Tireworld

Drive

Pitchcatch

Random

Domain Description

Similar to the traditional Blocksworld except that there is a probability of dropping a
block on the table and the blocks are colored. The goal is to build a tower with a speciﬁed color
pattern, rather than to achieve a tower containing particular blocks as is more usual.
Similar to traditional logistics world except that there is a small chance of driving to an
unintended city. The domain is dead-end free.

Similar to the Blocksworld but with some probability putting down a “not-detonated” block
destroys the target (either table or a block) and detonates the block. When the table or a
goal block is destroyed, we reach a dead-end. Putting down a detonated block does not cause
any problem. The best strategy is to putdown a block onto a non-goal block until it detonates.
and then build the target tower.

Every ﬁle needs to be placed into a folder of its type.
The type of a ﬁle is assigned uniformly among available types when get-type action is conducted.

There are two routes. One route has spare tires at some locations. The other does not.
There is a small chance of having a ﬂat tire when driving. When the tire is ﬂat and there is no spare,
the state is a dead-end.
Similar to g-tire-problem. But can call AAA for spares, with penalty. Dead-end free domain.

A variant of Tower Of Hanoi. Can move two disks at a time with bigger risk of ending up with
a deadend state. Should not use two disks actions.

A variant of Zenotravel IPC3. Each original action has a “complete”-action that must be taken to
realize the effects. Each “complete”-action has a very small probability of success.
A planner needs to repeat the complete action until the effect is realized.

Similar to Blocksworld of IPPC 2004. Additional actions of moving a tower of blocks is available,
with smaller success chance than normal actions. Blocks do not have colors in this version.

Similar to tire-problems of IPPC 2004. Need to consider more routes.

Toyish representation of real-life driving. Need to reach a goal point. Roads are placed similar
to chess board. Before the move-action, need to check the light and wait when the light is RED.
The chance of having RED is different depending on the road type. There is a slim chance of
dying on a wait action. Move-action can cause dying, depending on the length of the road.

Need to catch balls of goal types. Each type can be caught when the pre-deﬁned set of bits are set.
Some bits can be set through “setbit” action with a big chance of dying. When a ball is thrown,
one can catch or pass it. When caught, all the bits are reset. When hard to set bit
is set and the ball is not the right type, one should pass and wait for another ball.

Domain itself is randomly generated with random precondition and effects from randomly generated
predicate symbols. Based on the domain, a problem is generated by random walk from randomly
generated initial state. Then the goal state is the resulting state of the random walk.
Special reset-action can lead any state to the initial state, making it dead-end free.

Exploding Blocksworld

Similar to IPPC 2004 Exploding Blocksworld domain. Need to reduce the number of putdown
actions to raise the success probability. Unlike 2004 version, destroyed table does not
immediately lead to a dead-end state.

ZenoWorld
Elevators

Schedule

Similar to IPPC 2004 Zeno travel domain.

Need to collect coins. Coins are at some positions of some levels. Elevators can move along levels.
At a level one needs to move to the correct positions to get the coins. The move action is probabilistic
and can result in unwanted position with 50% chance. Thus, a planner needs to repeat the move
actions to get to the positions with coins.

Toy simulation of packet scheduling problem. Goal is processing goal types of packets.
Some types are delivered very rarely. Packets are delivered to the queue more frequently than one
can process, and when the queue is overﬂowed, there is a chance of dying.
Need to process those hard-to-come-by types of packets as they are delivered

Figure 1: Domain Description for IPPC 2004 and 2006: Bold faced domains are dead-end free, which is favorable for a
replanner.

356

Domains
bw-c-pc-8

bw-c-pc-nr-8
bw-nc-pc-11
bw-nc-pc-15
bw-nc-pc-18
bw-nc-pc-21
bw-nc-pc-5
bw-nc-pc-8

bw-nc-pc-nr-8
bx-c10-b10-pc-n
bx-c10-b10-pc
bx-c15-b10-pc
bx-c5-b10-pc

bx-c5-b10-pc-nr
exploding-block

ﬁle-prob-pre
g-tire-problem-
r-tire-problem-
toh-prob-pre
ztravel-1-2

NMR C mGPT
30 (1)
0 (-)
0 (-)
30 (1)
30 (0)
30 (4)
30 (7)
30 (18)
0 (-)
23 (38)
19 (45)
0 (-)
30 (0)
30 (0)
30 (0)
30 (1)
0 (-)
30 (1)
0 (-)
30 (28)
30 (29)
0 (-)
0 (-)
14 (60)
30 (6)
30 (7)
0 (-)
30 (8)
0 (-)
0 (-)
0 (-)
30 (2)
16 (0)
0 (-)
30 (0)
0 (-)
0 (-)
0 (-)
0 (-)
30 (0)

NMR
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
30 (0)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)

3 (290)
9 (0)
30 (1)
15 (0)
30 (9)

C
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
30 (1)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
30 (1)
30 (1)
0 (-)

30 (18)

FCP
30 (26)
30 (27)

Results from IPPC-04
R
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
30 (2)
0 (-)

0 (-)
0 (-)
0 (-)
0 (-)
30 (3)
30 (24)
30 (28)

1 (789)

0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
9 (1)
0 (-)
17 (4)
27 (32)

0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)

J1

30 (1)
30 (1)
30 (1)
30 (2)
30 (2)
30 (3)
30 (0)
30 (0)
30 (0)
30 (3)
30 (3)
30 (4)
30 (1)
30 (1)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)

(cid:2)

Cla
30 (1)
30 (1)
30 (1)
30 (2)
15 (3)
30 (3)
30 (0)
30 (0)
30 (0)
20 (4)
20 (4)
0 (-)
30 (1)
30 (1)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)

FFRs
30 (1)
30 (1)
30 (1)
0 (-)
0 (-)

30 (19)
30 (0)
30 (0)
30 (0)
30 (3)
30 (2)
30 (3)
30 (1)
30 (1)
3 (0)
14 (60)
7 (0)
30 (0)
0 (-)
0 (-)

Pro
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)

11 (67)

0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
0 (-)
7 (1)
6 (122)

0 (-)
1 (0)

FFRa
30 (0)
30 (0)
30 (0)
30 (1)
30 (28)
30 (3)
30 (0)
30 (0)
30 (0)
30 (0)
30 (0)
30 (0)
30 (0)
30 (0)
5 (0)
29 (29)
7 (0)
30 (0)
11 (0)
30 (0)

Figure 2: IPPC-04 competition results. Each column corresponds to a planner and each row corresponds to a problem. The
planners were run for up to 30 trials on each problem and given a total of 15 minutes for the 30 trials. Each cell ﬁrst gives the
number of trials for which the goal was achieved and then gives in parens the average time in seconds required for the successful
trials. The planners are as follows: NMRDPPC (NMRC), NMRDPP (NMR), FCPlanner (FCP), J1 (Planner with Human
Control Knowledge), Classy (Cla), FF-Replan with single-outcome determinization (FFRs), Probapop (Pro), FF-Replan with
all-outcomes determinization (FFRa). Note that FFRa was not part of the original competition and these results were obtained
for the purposes of this paper. Also note that planners shown in bold utilize human-provided control knowledge.

half as many actions as FFRa. Pitchcatch and zeno are other
domains where the average plan length of FFRa is appar-
ently far from optimal. For other domains, FFRa produces
signiﬁcantly shorter solutions. For example, in schedule and
random, the solutions are much shorter than those of FPG
though both planners solve approximately the same number
of problems.

Figure 5 shows the average time spent on successful trials
in each domain. In most of the domains, FF-Replan spent
less time than other performers, again showing the beneﬁt
of the determinization process. Note that the average time
is only on solved problems and thus the numbers should be
interpreted in conjunction with Figure 3.

Summary and Future Directions

The deterministic planning community has developed very
efﬁcient and informative planning heuristics. FF-Replan’s
performance on probabilistic planning is primarily due to
these developments, allowing for fast deterministic plan syn-
thesis. We have demonstrated that this approach is state-of-
the-art when evaluated on problems from the ﬁrst two prob-
abilistic planning competitions. It is important to note that
the impressive performance is a result of most competition
problems not exercising the fully difﬁculty that is potential
in the probabilistic setting. This suggests a more careful de-
sign process in future competitions, where there is an ex-
plicit attempt to span the spectrum of complexity ranging
from deterministic problems to highly stochastic problems.

While FF-Replan has obvious shortcomings, our results
have suggested many natural directions for improvement,
some of which we have highlighted in this paper. Two such
directions that we are currently pursuing are discussed in
more detail below.

Hindsight Optimization. FF-Replan’s approach bears
much similarity to hindsight optimization (Wu, Chong, &
Givan 2002).
In the all-outcomes variant of FF-Replan,
the deterministic planner effectively has the ability to select
exactly the probabilistic outcomes that are most useful for
achieving the goal, regardless of how likely those outcomes
are. In comparison, the hindsight optimization approach is
not able to select the future outcomes, but rather is told what
the future outcomes will be effectively allowing it to plan
in “hindsight”. Given knowledge of the future outcomes,
hindsight optimization formulates a plan contingent on those
outcomes. In this sense, hindsight optimization is not as op-
timistic as FF-Replan.

An interesting future direction is to evaluate a hindsight
optimization variant of FF-Replan. One approach to doing
this is as follows. First, at the current state draw a random
number sequence r1 . . . rh until a predeﬁned horizon h. The
predeﬁned horizon h can be increased as the planner fails to
ﬁnd any plan in the horizon. During the planning, the out-
come for each action at future time i would be dictated by
the random number ri. Thus, the corresponding determinis-
tic problem is non-stationary in the sense that the outcome
selected for a particular action can change with the time step.

357

Domains

blocksworld

ex-blocksworld

drive

elevators
pitchcatch
random
schedule

tire
zeno

FFRa
86.22
51.56
71.33
93.33
53.56
100.00
51.33
82.22
100.00

Percent Successful Trials in IPPC-06)
FOALP
Paragraph
100.00
24.22
0.00
100.00
0.00
0.00
0.00
81.56
0.00

FPG
62.89
42.67
56.00
76.22
23.11
64.67
54.22
74.89
26.89

sfDP
29.11
31.33
0.00
0.00
0.00
0.00
0.00
0.00
6.67

0.00
30.74
8.60
0.00
0.00
5.11
0.92
91.14
6.67

FFRs
76.8
51.56

0
93
0
73
0
69
7

Figure 3: IPPC-06 successful trials results. Each row corresponds to a planning domain and each column corresponds to a
planner. Each cell gives the percentage of successful trials in each domain.

Domains

blocksworld

ex-blocksworld

drive

elevators
pitchcatch
random
schedule

tire
zeno

5.07

0.00

Paragraph

FFRs
44.41

sfDP
6.00
1.87

FOALP
37.27
7.47

Average Number of Actions for Successful Trials in IPPC-06
FFRa
73.17
19.17
36.64
47.45
588.41
20.15
105.49
2.70
245.84

FPG
39.27
3.73
48.73
33.13
20.33
103.40
198.20
2.67
45.00

0.07
0.33
3.79
0.00

1.87
3.53

-

-
-
-

-

-

-

1

57.00

-
-
-
-
-
-

21
-

50.3

12.7

2.95

-

-
-

Figure 4: IPPC-06 average number of actions results. Each row corresponds to a planning domain and each column corresponds
to a planner. Each cell gives the average number of actions used for successful trials in each domain.

Average Time for Successful Trials in IPPC-06

Domains

blocksworld

ex-blocksworld

drive

elevators
pitchcatch
random
schedule

tire
zeno

FFRa
75.21
1.17
0.31
0.24
1.19
2.12
0.72
0.05
5.90

FOALP

2.14
0.71

-

2.57

-
-
-

43.11

-

sfDP
9.37
0.69

-
-
-
-
-
-

0.00

FPG
1.73
0.18
2.06
1.43
0.87
10.04
9.92
0.17
1.89

Paragraph

-

0.03
0.57

-
-

0.00
0.01
0.17
0.00

FFRs
3.6
0.36

-

0.17

-

0.32

-

0.04

0

Figure 5: IPPC-06 average solution time results. Each row corresponds to a planning domain and each column corresponds to
a planner. Each cell gives the average solution time on successful trials in each domain.

358

Buffet, O., and Aberdeen, D. 2006. The factored policy
gradient planner. In International Probabilistic Planning
Competition Booklet of ICAPS.
Fern, A.; Yoon, S.; and Givan, R. 2006. Approximate
policy iteration with a policy language bias: Solving re-
lational markov decision processes. Journal of Artiﬁcial
Intelligence Research 25:85–118.
Geffner, H. 1998. Classical, probabilistic and contingent
planning: Three models, one algorithm. In AIPS’98 Work-
shop on Planning as Combinatorial Search.
Gretton, C., and Thiebaux, S. 2004. Non markovian reward
decision process planner, url=http://users.rsise.
anu.edu.au/˜charlesg/nmrdpp/.
Hoffmann, J., and Nebel, B. 2001. The FF planning sys-
tem: Fast plan generation through heuristic search. Journal
of Artiﬁcial Intelligence Research 14:263–302.
Karabaev, E., and Skvortsova, O. 2005. A Heuristic Search
Algorithm for Solving First-Order MDPs. In Bacchus, F.,
and Jaakkola, T., eds., Proceedings of the Conference on
Uncertainty in Artiﬁcial Intelligence (UAI’2005), 292–299.
Edinburgh, Scotland: AUAI Press. ISBN-0-9749039-1-4.
Mausam; Bertoli, P.; and Weld, D. S. 2007. A hybridized
planner for stochastic domains. In IJCAI.
McDermott. 1998. Pddl-the planning domain deﬁnition
language. In The 1st International Planning Competition.
Mercier, L., and Van HenTenRyck, P. 2007. Performance
analysis of online anticipatory algorithms for large multi-
stage stochastic programs. In International Joint Confer-
ence on Artiﬁcial Intelligence.
Meuleau, N., and Smith, D. 2003. Optimal limited contin-
gency planning.
Puterman, M. 1994. Markov Decision Processes. Wiley,
New York.
Sanner, S., and Boutilier, C. 2006. First order approximate
linear programming. In International Probabilistic Plan-
ning Competition Booklet of ICAPS.
Wu, G.; Chong, E.; and Givan, R. 2002. Burst-level con-
gestion control using hindsight optimization. IEEE Trans-
actions on Automatic Control.
Yoon, S.; Fern, A.; and Givan, R. 2004. Learning re-
active policies for probabilistic planning domains. In In-
ternational Probabilistic Planning Competition Booklet of
ICAPS.
Younes, H., and Littman, M. 2004a. International prob-
ablistic planning competition, url = http://www.cs.
rutgers.edu/˜mlittman/topics/ipc04-pt.
Younes, H. L. S., and Littman, M. L. 2004b. Ppddl1.0:
An extension to pddl for expressing planning domains with
probabilistic effects. In Technical Report CMU-CS-04-162.

After solving the non-stationary deterministic problem, the
ﬁrst action in the plan is recorded and the process is re-
peated. After many repetitions the hindsight optimization
approach suggests selecting the action that was recorded the
most times.

There are a number of straightforward ways to ﬁnd plans
for the resulting non-stationary deterministic problems. In
particular, one could encode the non-stationary problem as
a standard stationary problem by having h copies of the op-
erators, modiﬁed to include preconditions and effects that
restrict the selection of an action to its corresponding time.
It is also relatively straightforward to modify certain heuris-
tic search planners and many SAT-based planning encodings
to naturally facilitate for non-stationary actions.

It is likely that the hindsight optimization approach will be
more robust in domains where probabilistic effects are crit-
ical, e.g., the exploding blocksworld domain in (Younes &
Littman 2004a). The cost of hindsight optimization is com-
putation time as it typically calls for drawing a number of
random sequences and solving the corresponding planning
problems for each state encountered. However, it is likely
that incremental variants of this approach could be devel-
oped to signiﬁcantly decrease the cost.

Policy Rollout. A simpler version of this idea is a tech-
nique called policy rollout (Bertsekas & Tsitsiklis 1996).
Policy rollout computes a value estimate for each action in
a state as follows. Each action is simulated from the current
state and then FF-Replan is used to select actions until the
goal is reached or until some horizon. This is done multiple
times and the resulting plan lengths are averaged for each ac-
tion, giving a value estimate. This effectively adds one step
of look ahead on top of the FF-Replan policy, which can help
avoid dead-ends among other types of sub-optimal behavior.
We implemented a prototype of this idea and tested it on the
exploding blocksworld of IPPC-06. The results showed that
policy rollout does indeed improve over FF-Replan by a sig-
niﬁcant margin.

Acknowledgement

We thank the IPPC-04 and IPPC-06 hosts for their efforts on
MDPSIM, domains, and the competition results.

References

Bertsekas, D. P., and Tsitsiklis, J. N. 1996. Neuro-Dynamic
Programming. Athena Scientiﬁc.
Blum, A., and Langford, J. 1999. Probabilistic planning in
the graphplan framework. In ECP, 319–332.
Bonet, B., and Givan, R. 2006. International probablistic
planning competition, url = http://www.ldc.usb.
ve/˜bonet/ipc5/.
Boutilier, C.; Brafman, R. I.; and Geib, C. 1998. Struc-
tured reachability analysis for Markov decision processes.
In Uncertainty in Artiﬁcial Intelligence 1998, 24–32.
Boutilier, C.; Dearden, R.; and Goldszmidt, M. 2000.
Stochastic dynamic programming with factored represen-
tations. Artiﬁcial Intelligence 121(1-2):49–107.

359

