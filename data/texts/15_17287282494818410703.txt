A Gentle Introduction to Support Vector Machines
in Biomedicine
Alexander Statnikov*, Douglas Hardin#, Isabelle Guyon, Constantin F. Aliferis*
(Materials about SVM Clustering were contributed by Nikita Lytkin*)
*New York University, #Vanderbilt University, ClopiNet

Part I
· Introduction · Necessary mathematical concepts · Support vector machines for binary
classification: classical formulation · Basic principles of statistical machine learning
2

Introduction
3

About this tutorial
Main goal: Fully understand support vector machines (and important extensions) with a modicum of mathematics knowledge.
· This tutorial is both modest (it does not invent anything new) and ambitious (support vector machines are generally considered mathematically quite difficult to grasp).
· Tutorial approach: learning problem main idea of the SVM solution geometrical interpretation math/theory basic algorithms extensions case studies.
4

Data-analysis problems of interest

1. Build computational classification models (or "classifiers") that assign patients/samples into two or more classes.
- Classifiers can be used for diagnosis, outcome prediction, and other classification tasks.
- E.g., build a decision-support system to diagnose primary and metastatic cancers from gene expression profiles of the patients:

Patient

Biopsy Gene expression profile

Classifier model

Primary Cancer Metastatic Cancer

5

Data-analysis problems of interest

2. Build computational regression models to predict values of some continuous response variable or outcome.

- Regression models can be used to predict survival, length of stay in the hospital, laboratory test values, etc.

- E.g., build a decision-support system to predict optimal dosage

of the drug to be administered to the patient. This dosage is

determined by the values of patient biomarkers, and clinical and

demographics data:

Patient

1 2.2 3 423 2 3 92 2 1 8
Biomarkers, clinical and

Regression model

Optimal dosage is 5 IU/Kg/week

demographics data

6

Data-analysis problems of interest
3. Out of all measured variables in the dataset, select the smallest subset of variables that is necessary for the most accurate prediction (classification or regression) of some variable of interest (e.g., phenotypic response variable).
- E.g., find the most compact panel of breast cancer biomarkers from microarray gene expression data for 20,000 genes:
Breast cancer tissues
Normal tissues
7

Data-analysis problems of interest
4. Build a computational model to identify novel or outlier patients/samples.
- Such models can be used to discover deviations in sample handling protocol when doing quality control of assays, etc.
- E.g., build a decision-support system to identify aliens.
8

Data-analysis problems of interest
5. Group patients/samples into several clusters based on their similarity.
- These methods can be used to discovery Cluster #1 disease sub-types and for other tasks.
- E.g., consider clustering of brain tumor Cluster #2 patients into 4 clusters based on their gene expression profiles. All patients have the Cluster #3 same pathological sub-type of the disease, and clustering discovers new disease subtypes that happen to have different Cluster #4 characteristics in terms of patient survival and time to recurrence after treatment.
9

Basic principles of classification

· Want to classify objects as boats and houses.

10

Basic principles of classification

· All objects before the coast line are boats and all objects after the
coast line are houses. · Coast line serves as a decision surface that separates two classes.

11

Basic principles of classification
These boats will be misclassified as houses
12

Basic principles of classification
Longitude

Latitude

Boat House

· The methods that build classification models (i.e., "classification algorithms")
operate very similarly to the previous example. · First all objects are represented geometrically.

13

Basic principles of classification
Longitude

Latitude

Boat House

Then the algorithm seeks to find a decision surface that separates classes of objects

14

Basic principles of classification

Longitude
These objects are classified as houses
? ??

??

?

These objects are classified as boats
Latitude
Unseen (new) objects are classified as "boats" if they fall below the decision surface and as "houses" if the fall above it

15

The Support Vector Machine (SVM) approach
· Support vector machines (SVMs) is a binary classification algorithm that offers a solution to problem #1.
· Extensions of the basic SVM algorithm can be applied to solve problems #1-#5.
· SVMs are important because of (a) theoretical reasons:
- Robust to very large number of variables and small samples - Can learn both simple and highly complex classification models - Employ sophisticated mathematical principles to avoid overfitting
and (b) superior empirical results.
16

Main ideas of SVMs
Gene Y

Normal patients

Cancer patients

Gene X

· Consider example dataset described by 2 genes, gene X and gene Y · Represent patients geometrically (by "vectors")

17

Main ideas of SVMs
Gene Y

Normal patients

Cancer patients

Gene X

· Find a linear decision surface ("hyperplane") that can separate patient classes and has the largest distance (i.e., largest "gap" or "margin") between border-line patients (i.e., "support vectors");

18

Main ideas of SVMs

Gene Y

Cancer

kernel

Cancer

Decision surface

Normal

Normal

Gene X

· If such linear decision surface does not exist, the data is mapped

into a much higher dimensional space ("feature space") where the

separating decision surface is found;

· The feature space is constructed via very clever mathematical

projection ("kernel trick").

19

History of SVMs and usage in the literature

· Support vector machine classifiers have a long history of
development starting from the 1960's. · The most important milestone for development of modern SVMs
is the 1992 paper by Boser, Guyon, and Vapnik ("A training
algorithm for optimal margin classifiers")

10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

Use of Support Vector Machines in the Literature

General sciences Biomedicine

8,180 6,660

8,860

4,950 3,530 2,330

30000
Use of Linear Regression in the Literature

25000 20000 15000 10000

General sciences

24,100

Biomedicine
19,200

18,700

19,100

22,200 17,700

20,100 19,500

20,000

19,600

17,700 18,300

14,900 15,500

16,000 14,900

9,770

10,800

12,000

13,500

359 4
1998

621 12

1,430 906
46 99

1999 2000 2001

201
2002

351
2003

521
2004

726
2005

917
2006

1,190 5000

2007

0

1998

1999

2000

2001

2002

2003

2004

2005

2006

2007

20

Necessary mathematical concepts
21

How to represent samples geometrically? Vectors in n-dimensional space ( n)
· Assume that a sample/patient is described by n characteristics ("features" or "variables")
· Representation: Every sample/patient is a vector in n with tail at point with 0 coordinates and arrow-head at point with the feature values.
· Example: Consider a patient described by 2 features: Systolic BP = 110 and Age = 29.
This patient can be represented as a vector in 2:
Age
(110, 29)

(0, 0)

Systolic BP

22

How to represent samples geometrically? Vectors in n-dimensional space ( n)

Age (years)

Patient 3

Patient 4

60

40

Patient 1

Patient 2

20

0 200
150
100

50

00

Patient id
1 2 3 4

Cholesterol (mg/dl)
150 250 140 300

Systolic BP (mmHg)
110 120 160 180

Age (years)
35 30 65 45

200 100
Tail of the vector (0,0,0) (0,0,0) (0,0,0) (0,0,0)

300
Arrow-head of the vector
(150, 110, 35) (250, 120, 30) (140, 160, 65) (300, 180, 45)

23

How to represent samples geometrically? Vectors in n-dimensional space ( n)

Age (years)

Patient 3

Patient 4

60

40

Patient 1

Patient 2

20

0 200
150
100

50

00

200 100

300

Since we assume that the tail of each vector is at point with 0 coordinates, we will also depict vectors as points (where the arrow-head is pointing).
24

Purpose of vector representation

· Having represented each sample/patient as a vector allows

now to geometrically represent the decision surface that

separates two groups of samples/patients.

A decision surface in 2
7

A decision surface in 3
7

66

55
4
4
3
3
2

21

1

0 10

0 01 234567

5 00 1 2 3 4 5 6 7

· In order to define the decision surface, we need to introduce

some basic math elements...

25

Basic operation on vectors in n

1. Multiplication by a scalar Consider a vector a (a1, a2,..., an ) and a scalar c Define: ca (ca1, ca2 ,..., can )
When you multiply a vector by a scalar, you "stretch" it in the same or opposite direction depending on whether the scalar is positive or negative.

a (1,2)

c2

4
ca
3

ca (2,4) 2

a
1

0 1234

a (1,2) c1 ca ( 1, 2)

4 3 2 1

-2 -1
ca

0 -1
-2

a
1234

26

Basic operation on vectors in n

2. Addition Consider vectors a (a1, a2,..., an ) and b Define: a b (a1 b1, a2 b2 ,..., an bn )

(b1, b2 ,..., bn )

a (1,2)

4

b (3,0)

3 2

a b (4,2) 1

0

a ab
123
b

4

Recall addition of forces in classical mechanics.

27

Basic operation on vectors in n

3. Subtraction Consider vectors a (a1, a2,..., an ) and b Define: a b (a1 b1, a2 b2 ,..., an bn )

(b1, b2 ,..., bn )

a (1,2) b (3,0)

4 3

a b ( 2,2)

ab

2 1

-3 -2 -1 0

a
1234
b

What vector do we need to add to b to get a ? I.e., similar to
subtraction of real
numbers.

28

Basic operation on vectors in n
4. Euclidian length or L2-norm Consider a vector a (a1, a2,..., an ) Define the L2-norm: a 2 a12 a22 ... an2
We often denote the L2-norm without subscript, i.e. a

a (1,2) a5
2

3
2.24 2 1

0

a Length of this
vector is
1234

L2-norm is a typical way to measure length of a vector; other methods to measure length also exist.

29

Basic operation on vectors in n

5. Dot product

Consider vectors a (a1, a2 ,..., an ) and b (b1,b2 ,...,bn ) n

Define dot product: a b a1b1 a2b2 ... anbn

aibi

i1

The law of cosines says that a b || a ||2|| b ||2 cos where is the angle between a and b. Therefore, when the vectors
are perpendicular a b 0.

a (1,2) b (3,0) ab 3

4 3 2
a
1
0 1234
b

a (0,2) b (3,0) ab 0

4 3 2
a
1
0 1234
b

30

Basic operation on vectors in n

5. Dot product (continued)

a b a1b1 a2b2 ... anbn

n
aibi
i1

· Property: a a a1a1 a2a2 ... anan

a

2 2

· In the classical regression equation y w x b

the response variable y is just a dot product of the

vector representing patient characteristics (x ) and

the regression weights vector (w) which is common

across all patients plus an offset b.

31

Hyperplanes as decision surfaces
· A hyperplane is a linear decision surface that splits the space into two parts;
· It is obvious that a hyperplane is a binary classifier.

A hyperplane in 2 is a line
7 6 5 4 3 2 1 0
01 234567

A hyperplane in 3 is a plane
7 6 5 4 3 2 1 0 10
5 00 1 2 3 4 5 6

7

A hyperplane in n is an n-1 dimensional subspace

32

Equation of a hyperplane

First we show with show the definition of hyperplane by an interactive demonstration.
Click here for demo to begin
or go to http://www.dsl-lab.org/svm_tutorial/planedemo.html

Source: http://www.math.umn.edu/~nykamp/

33

Equation of a hyperplane

Consider the case of 3:
w

P x x0
x

P0
x0

O

An equation of a hyperplane is defined by a point (P0) and a perpendicular
vector to the plane (w) at that point.

Define vectors: x0 OP0 and x OP , where P is an arbitrary point on a hyperplane.

A condition for P to be on the plane is that the vector x

w (x x0 ) 0 or w x w x0 0 define b

w x0

x0 is perpendicular to w :

wx b 0

The above equations also hold for n when n>3.

34

Equation of a hyperplane

Example
w (4, 1,6)

P0 (0,1, 7) b w x0 (0 1 42)
w x 43 0

43

(4, 1,6) x 43 0

(4, 1,6) (x(1) , x(2) , x(3) ) 43

4x(1) x(2) 6x(3) 43 0

0

+ direction
w
P0
- direction

w x 50 0 w x 43 0 w x 10 0

What happens if the b coefficient changes?
The hyperplane moves along the direction of w.
We obtain "parallel hyperplanes".

Distance between two parallel hyperplanes w x is equal to D b1 b2 / w .

b1

0 and w x b2

0

35

(Derivation of the distance between two parallel hyperplanes)

tw
w x2
x1

w x b2 0 w x b1 0

x2 x1 tw D tw t w
w x2 b2 0 w (x1 tw) b2 0 w x1 t w 2 b2 0 (w x1 b1) b1 t w 2 b2
b1 t w 2 b2 0 t (b1 b2 ) / w 2
D t w b1 b2 / w

0

36

Recap

We know... · How to represent patients (as "vectors") · How to define a linear decision surface ("hyperplane")
We need to know... · How to efficiently compute the hyperplane that separates
two classes with the largest "gap"?
Gene Y
Need to introduce basics of relevant optimization theory

Normal patients

Cancer patients

Gene X

37

Basics of optimization: Convex functions
· A function is called convex if the function lies below the straight line segment connecting two points, for any two points in the interval.
· Property: Any local minimum is a global minimum!

Global minimum
Convex function

Local minimum Global minimum

Non-convex function

38

Basics of optimization: Quadratic programming (QP)
· Quadratic programming (QP) is a special optimization problem: the function to optimize ("objective") is quadratic, subject to linear constraints. · Convex QP problems have convex objective functions. · These problems can be solved easily and efficiently by greedy algorithms (because every local minimum is a global minimum).
39

Basics of optimization: Example QP problem

Consider x (x1,x2)

Minimize

1 2

||

x

||22

subject to x1

x2 1

0

quadratic objective

linear constraints

This is QP problem, and it is a convex QP as we will see later

We can rewrite it as:

Minimize

1 2

(

x12

x22 ) subject to x1

x2 1

0

quadratic objective

linear constraints

40

Basics of optimization: Example QP problem

f(x1,x2)

x1 x2 1 0 x1 x2 1

1 2

(

x12

x22 )

x1 x2 1 0
x2 x1
The solution is x1=1/2 and x2=1/2.

41

Congratulations! You have mastered all math elements needed to
understand support vector machines.
Now, let us strengthen your knowledge by a quiz
42

Quiz

1) Consider a hyperplane shown with white. It is defined by
equation: w x 10 0
Which of the three other hyperplanes can be defined by
equation: w x 3 0 ?
- Orange - Green - Yellow
2) What is the dot product between
vectors a (3,3) and b (1, 1)?

w
P0

4 3 2 1
-2 -1 0 -1 -2

a
1234
b

43

Quiz

3) What is the dot product between
vectors a (3,3) and b (1,0)?
4) What is the length of a vector
a (2,0) and what is the length of
all other red vectors in the figure?

4
3a
2 1
-2 -1 0 b 1 2 3 4
-1 -2

4 3 2 1
-2 -1 0 -1 -2

a
1234

44

Quiz
5) Which of the four functions is/are convex?
12
34

45

Support vector machines for binary classification: classical formulation
46

Case 1: Linearly separable data;

"Hard-margin" linear SVM

Given

training

data:

x1, x2 ,..., xN y1, y2 ,..., yN

Rn { 1, 1}

· Want to find a classifier

(hyperplane) to separate

negative instances from the

positive ones. · An infinite number of such

hyperplanes exist. · SVMs finds the hyperplane that

maximizes the gap between

data points on the boundaries

(so-called "support vectors"). · If the points on the boundaries

Negative instances (y=-1)

Positive instances (y=+1)

are not informative (e.g., due to noise), SVMs will not do well.

47

Statement of linear SVM classifier

wx b 1

wx b 0

The gap is distance between

w x b 1 parallel hyperplanes:

wx b wx b

1 1

and

Or equivalently:

w x (b 1) 0 w x (b 1) 0

Negative instances (y=-1)

Positive instances (y=+1)

We know that
D b1 b2 / w
Therefore:
D 2/ w

Since we want to maximize the gap,

we need to minimize w

or equivalently minimize

1 2

w2

(

1 2

is

convenient

for

taking

derivative

later

on)

48

Statement of linear SVM classifier

wx b 1
Negative instances (y=-1)

wx b 0

In addition we need to

impose constraints that all

w x b 1 instances are correctly
classified. In our case:

w xi b w xi b

1 if yi 1 if yi

1 1

Equivalently:
yi (w xi b) 1

Positive instances (y=+1)

In summary:

Want

to

minimize

1 2

w

2
subject

to

yi (w

xi

b)

1 for i = 1,...,N

Then given a new instance x, the classifier is f (x) sign(w x b)

49

SVM optimization problem: Primal formulation

n

Minimize

1 2

wi2 subject to

i1

Objective function

yi (w xi b) 1 0
Constraints

for i = 1,...,N

· This is called "primal formulation of linear SVMs". · It is a convex quadratic programming (QP)
optimization problem with n variables (wi, i = 1,...,n), where n is the number of features in the dataset.

50

SVM optimization problem: Dual formulation

· The previous problem can be recast in the so-called "dual
form" giving rise to "dual formulation of linear SVMs". · It is also a convex quadratic programming problem but with
N variables ( i ,i = 1,...,N), where N is the number of samples.

N
Maximize i i1

N

1 2

i j yi y j xi x j subject to

i, j 1

N

i 0 and

i yi

i1

Objective function

Constraints

N

Then the w-vector is defined in terms of i: w

i yi xi

N i1

And the solution becomes: f (x) sign( i yi xi x b)

i1

0.
51

SVM optimization problem: Benefits of using dual formulation

1) No need to access original data, need to access only dot

products.

N
Objective function: i
i1 N
Solution: f (x) sign(
i1

N 1 2
i, j 1

i

i yi xi x

j yi y j xi b)

xj

2) Number of free parameters is bounded by the number

of support vectors and not by the number of variables

(beneficial for high-dimensional problems).

E.g., if a microarray dataset contains 20,000 genes and 100 patients, then need to find only up to 100 parameters!
52

(Derivation of dual formulation)

n

Minimize

1 2

wi2 subject to yi (w xi b) 1

i1

Objective function

Constraints

0 for i = 1,...,N

Apply the method of Lagrange multipliers.

Define Lagrangian P w,b,

n
w1 2
2i i1

N
i yi (w xi
i1

b) 1

a vector with n elements a vector with N elements

We need to minimize this Lagrangian with respect to w,b and simultaneously
require that the derivative with respect to vanishes , all subject to the constraints that i 0.

53

(Derivation of dual formulation)

If we set the derivatives with respect to w,b to 0, we obtain:

P w, b, b
P w, b, w

N
0 i yi 0
i1

N

0w

i yi xi

i1

We substitute the above into the equation for P w,b, and obtain "dual formulation of linear SVMs":

NN

D

1 i2

i j yi y j xi x j

i 1 i, j 1

We seek to maximize the above Lagrangian with respect to N
constraints that i 0 and i yi 0 .
i1

, subject to the
54

Case 2: Not linearly separable data; "Soft-margin" linear SVM

What if the data is not linearly separable? E.g., there are outliers or noisy measurements, or the data is slightly non-linear.

0 00 00 0
0

Want to handle this case without changing the family of decision functions.

0

0

0 0

00 0 0

Approach:
Assign a "slack variable" to each instance i 0 , which can be thought of distance from
the separating hyperplane if an instance is misclassified and 0 otherwise.

Want

to

minimize

1 2

w2

N
C

i subject to yi (w xi

b)

1

i1
Then given a new instance x, the classifier is

f (x)

sign(w x

i for i = 1,...,N
b)

55

Two formulations of soft-margin linear SVM

Primal formulation:

nN

Minimize

1 2

wi2 C

i subject to yi (w xi b) 1

i1 i1

Objective function

Constraints

i for i = 1,...,N

Dual formulation:

n
Minimize i
i1

N

1 2

i j yi y j xi x j subject to 0

i, j 1

for i = 1,...,N. Objective function

N

i C and

i yi

i1

Constraints

0

56

Parameter C in soft-margin SVM

Minimize

1 2

w2

N
C

i subject to yi (w xi

b)

1

i1

i for i = 1,...,N

C=100 C=0.15

· When C is very large, the soft-

margin SVM is equivalent to

hard-margin SVM; · When C is very small, we

admit misclassifications in the

training data at the expense of C=1 having w-vector with small

norm; · C has to be selected for the

distribution at hand as it will

be discussed later in this

tutorial.

C=0.1

57

Case 3: Not linearly separable data; Kernel trick
Gene 2

Tumor

Tumor

kernel

?

Normal

? Normal

Gene 1

Data is not linearly separable in the input space

Data is linearly separable in the feature space obtained by a kernel

:RN H

58

Kernel trick

Original data x (in input space) f (x) sign(w x b)

Data in a higher dimensional feature space (x) f (x) sign(w (x) b)

N
w i yi xi
i1

N
w i yi (xi )
i1

N
f (x) sign( i yi (xi ) (x) b)
i1

N
f (x) sign( i yi K (xi ,x) b)
i1

Therefore, we do not need to know explicitly, we just need to define function K(·, ·): N × N .

Not every function N × N

can be a valid kernel; it has to satisfy so-called

Mercer conditions. Otherwise, the underlying quadratic program may not be solvable.

59

Popular kernels

A kernel is a dot product in some feature space:

K (xi , x j ) (xi ) (x j )

Examples:
K (xi , x j ) xi x j K (xi , x j ) exp( xi x j 2 )
K (xi , x j ) exp( xi x j ) K (xi , x j ) ( p xi x j )q
K (xi , x j ) ( p xi x j )q exp( K (xi , x j ) tanh(kxi x j )

Linear kernel Gaussian kernel Exponential kernel
Polynomial kernel
xi xj 2 ) Hybrid kernel Sigmoidal

60

Understanding the Gaussian kernel

Consider Gaussian kernel: K (x, x j ) exp( x x j 2 )
Geometrically, this is a "bump" or "cavity" centered at the
training data point x j :

"bump" "cavity"

The resulting mapping function is a combination of bumps and cavities.

61

Understanding the Gaussian kernel
Several more views of the data is mapped to the feature space by Gaussian kernel
62

Understanding the Gaussian kernel
Linear hyperplane that separates two classes
63

Understanding the polynomial kernel

Consider polynomial kernel: K (xi , x j ) (1 xi x j )3
Assume that we are dealing with 2-dimensional data (i.e., in 2). Where will this kernel map the data?

2-dimensional space
x(1) x(2)
kernel

10-dimensional space

1 x(1)

x(2)

x2 (1)

x2 (2)

x(1) x(2)

x3 (1)

x3 (2)

x(1)

x2 (2)

x2 (1)

x(2)

64

Example of benefits of using a kernel

x(2)
x1 x4 x3
x2

· Data is not linearly separable

in the input space ( 2). · Apply kernel K (x, z) (x z)2

x(1)

to map data to a higher dimensional space (3-

dimensional) where it is

linearly separable.

K(x, z) (x z)2

x(1)

z2 (1)

x(2) z(2)

x z2 2 (1) (1)

2x(1) z(1) x(2) z(2)

x z2 2 (2) (2)

x(1) z(1) x(2) z(2) 2

x2 (1)
2x(1) x(2) x2
(2)

z2 (1)
2z(1) z(2) z2
(2)

(x) (z)
65

Example of benefits of using a kernel

Therefore, the explicit mapping is (x)

x(2) x1 x4 x3
x2

kernel
x(1)

x2 (1)
2x(1) x(2) x2
(2)
x2 (2) x1 , x2
x3 , x4

x2 (1)

2x(1) x(2)

66

Comparison with methods from classical statistics & regression

· Need model to be estimated:

Number of variables 2 10 10 100 100

Polynomial degree 3 3 5 3 5

Number of parameters 10 286 3,003 176,851 96,560,646

Required sample 50 1,430 15,015 884,255 482,803,230

· SVMs do not have such requirement & often require much less sample than the number of variables, even when a high-degree polynomial kernel is used.

67

Basic principles of statistical machine learning
68

Generalization and overfitting
· Generalization: A classifier or a regression algorithm learns to correctly predict output from given inputs not only in previously seen samples but also in previously unseen samples.
· Overfitting: A classifier or a regression algorithm learns to correctly predict output from given inputs in previously seen samples but fails to do so in previously unseen samples.
· Overfitting Poor generalization.
69

Example of overfitting and generalization

There is a linear relationship between predictor and outcome (plus some Gaussian noise).
Algorithm 2

Outcome of Interest Y

Outcome of Interest Y

Algorithm 1
Training Data Test Data

Predictor X

Predictor X

· Algorithm 1 learned non-reproducible peculiarities of the specific sample

available for learning but did not learn the general characteristics of the function

that generated the data. Thus, it is overfitted and has poor generalization.

· Algorithm 2 learned general characteristics of the function that produced the

data. Thus, it generalizes.

70

"Loss + penalty" paradigm for learning to avoid overfitting and ensure generalization
· Many statistical learning algorithms (including SVMs) search for a decision function by solving the following optimization problem:
Minimize (Loss + Penalty)
· Loss measures error of fitting the data · Penalty penalizes complexity of the learned function · is regularization parameter that balances Loss and Penalty
71

SVMs in "loss + penalty" form

SVMs build the following classifiers: f (x) sign(w x b)

Consider soft-margin linear SVM formulation:

Find w and b that

Minimize

1 2

w2

N
C

i subject to yi (w xi

b)

1

i1

i for i = 1,...,N

This can also be stated as:

Find w and b that N
Minimize [1 yi f (xi )]
i1

w

2 2

Loss Penalty ("hinge loss")

(in fact, one can show that = 1/(2C)).

72

Meaning of SVM loss function

N
Consider loss function: [1 yi f (xi )] i1
· Recall that [...]+ indicates the positive part · For a given sample/patient i, the loss is non-zero if 1 yi f (xi ) · In other words, yi f (xi ) 1 · Since yi { 1, 1}, this means that the loss is non-zero if
f (xi ) 1 for yi = +1 f (xi ) 1 for yi= -1
· In other words, the loss is non-zero if
w xi b 1 for yi = +1 w xi b 1 for yi= -1

0

73

Meaning of SVM loss function

· If the instance is negative, it is penalized only in regions 2,3,4
· If the instance is positive, it is penalized only in regions 1,2,3

wx b 1

wx b 0

1

2

3

wx b
4

1

Negative instances (y=-1) Positive instances (y=+1)
74

Flexibility of "loss + penalty" framework

Minimize (Loss + Penalty)

Loss function Penalty function Resulting algorithm

N
Hinge loss: [1 yi f (xi )]
i1
Mean squared error:
N
( yi f (xi ))2
i1
Mean squared error:
N
( yi f (xi ))2
i1
Mean squared error:
N
( yi f (xi ))2
i1
N
Hinge loss: [1 yi f (xi )]
i1

w2 2
w2 2

w 1

1 w1

2

w

2 2

w 1

SVMs Ridge regression Lasso Elastic net 1-norm SVM

75

Part 2
· Model selection for SVMs · Extensions to the basic SVM model:
1. SVMs for multicategory data 2. Support vector regression 3. Novelty detection with SVM-based methods 4. Support vector clustering 5. SVM-based variable selection 6. Computing posterior class probabilities for SVM
classifiers
76

Model selection for SVMs
77

Need for model selection for SVMs

Gene 2

Tumor

Gene 2

Tumor

Normal
Gene 1
· It is impossible to find a linear SVM classifier that separates tumors from normals!
· Need a non-linear SVM classifier, e.g. SVM with polynomial kernel of degree 2 solves this problem without errors.

Normal

Gene 1

· We should not apply a non-linear SVM

classifier while we can perfectly solve

this problem using a linear SVM

classifier!

78

A data-driven approach for model selection for SVMs

· Do not know a priori what type of SVM kernel and what kernel
parameter(s) to use for a given dataset? · Need to examine various combinations of parameters, e.g.
consider searching the following grid:

Polynomial degree d

(0.1, 1) (1, 1) (10, 1) (100, 1) (1000, 1)

(0.1, 2) Parameter
C (0.1, 3)

(1, 2) (1, 3)

(10, 2) (10, 3)

(100, 2) (1000, 2) (100, 3) (1000, 3)

(0.1, 4) (1, 4) (10, 4) (100, 4) (1000, 4)

(0.1, 5) (1, 5) (10, 5) (100, 5) (1000, 5)

· How to search this grid while producing an unbiased estimate

of classification performance?

79

Nested cross-validation

Recall the main idea of cross-validation:

test What combination of SVM

parameters to apply on

data

train train test

training data?

train test

valid train train
train valid

Perform "grid search" using another nested loop of cross-validation.

80

Example of nested cross-validation

Consider that we use 3-fold cross-validation and we want to optimize parameter C that takes values "1" and "2".

data

Outer Loop

P1

Training set

Testing set

C

Accuracy

Average Accuracy

P2 P1, P2 P3 1 89%

...

P1,P3 P2 2 84%

83%

P3 ... P2, P3 P1 1 76%

Training set
P1 P2 P1 P2

Inner Loop

Validation set

C

P2 P1

1

P2 P1

2

Accuracy
86% 84% 70% 90%

Average Accuracy
85%
80%

choose C=1

81

On use of cross-validation
· Empirically we found that cross-validation works well for model selection for SVMs in many problem domains; · Many other approaches that can be used for model selection for SVMs exist, e.g.:
Generalized cross-validation Bayesian information criterion (BIC) Minimum description length (MDL) Vapnik-Chernovenkis (VC) dimension Bootstrap
82

SVMs for multicategory data
83

One-versus-rest multicategory SVM method

Gene 2

Tumor I

?

?

*

****** **

* *

** *

*
* *

Tumor II

Tumor III

Gene 1

84

One-versus-one multicategory SVM method
Gene 2
Tumor I

** *

*

* Tumor II

*

** *

** * *
*

**

?*

?

Tumor III

Gene 1

85

DAGSVM multicategory SVM method

Not AML

AMALMvLs.vAs.LALLTL-cTe-lclell

Not ALL T-cell

AALLLLBB-c-eclellvl sv.sA. ALLLLT-Tc-eclelll

AML vs. ALL B-cell

Not ALL B-cell

Not ALL T-cell Not AML

Not ALL B-cell

ALL T-cell

ALAL LBL-cBe-lclell

AML

86

SVM multicategory methods by Weston and Watkins and by Crammer and Singer
Gene 2

?

?

*

******

* *

* *

* *
*

*
* *

Gene 1

87

Support vector regression
88

-Support vector regression ( -SVR)

Given training data:

x1, x2 ,..., xN y1, y2 ,..., yN

Rn R

y
*

+
** -

*

**

* *

******* *

*

x

Main idea:
Find a function f (x) w x b that approximates y1,...,yN : · it has at most derivation from
the true values yi · it is as "flat" as possible (to
avoid overfitting)

E.g., build a model to predict survival of cancer patients that

can admit a one month error (= ).

89

Formulation of "hard-margin" -SVR

y

*

* *

**

** *

* *

** *

*

*

+ wx b 0

-

Find f (x) w x b

by minimizing

1 2

w2

subject

to constraints:

yi (w x b) yi (w x b)

for i = 1,...,N. x

I.e., difference between yi and the fitted function should be smaller
than and larger than - all points yi should be in the " -ribbon" around the fitted function.

90

Formulation of "soft-margin" -SVR

y

* **

** *

*

* *

* *

** *

*

*

*

+ -

x

If we have points like this (e.g., outliers or noise) we can either:
a) increase to ensure that these points are within the new -ribbon, or
b) assign a penalty ("slack" variable) to each of this points (as was done for "soft-margin" SVMs)

91

Formulation of "soft-margin" -SVR

y

*i

**

** *

*

* *

* *

** *

*

*
* i
*

+ -

x

Find f (x) w x b

by minimizing

1 2

w2

N
C

subject to constraints: i 1

yi (w x b)

i

yi (w x b)

* i

i,

* i

0

for i = 1,...,N.

i

* i

Notice that only points outside -ribbon are penalized!

92

Nonlinear -SVR

y+

* ***

* ** *

-

* **

***

*x

y + ()

** - ( )

*

**

**

****** * *

(x)

Cannot approximate well this function with small !

kernel 1

y

* ***

*

**

*

+ -

* **

***

*

x

93

-Support vector regression in "loss + penalty" form

Build decision function of the form: f (x)

Find w and b that N
Minimize max(0,| yi
i1

f (xi ) |

)

w

2 2

Loss ("linear -insensitive loss")

Penalty

wx

b

Loss function value

Error in approximation

94

Comparing -SVR with popular regression methods

Loss function

Penalty function Resulting algorithm

Linear -insensitive loss:

N
max(0,| yi f (xi ) | )
i1

Quadratic -insensitive loss:

N
max(0, ( yi f (xi ))2
i1

)

Mean squared error:
N
( yi f (xi ))2
i1

Mean linear error: N | yi f (xi ) | i1

w2 2

-SVR

w

2 2

Another variant of -SVR

w2 2

Ridge regression

w 2 Another variant of ridge 2 regression

95

Comparing loss functions of regression methods

Linear -insensitive loss
Loss function value

Quadratic -insensitive loss
Loss function value

- Error in approximation
Mean squared error Loss function value

- Error in approximation
Mean linear error Loss function value

- Error in approximation

- Error in approximation
96

Applying -SVR to real data
In the absence of domain knowledge about decision functions, it is recommended to optimize the following parameters (e.g., by cross-validation using grid-search):
· parameter C · parameter · kernel parameters (e.g., degree of polynomial)
Notice that parameter depends on the ranges of variables in the dataset; therefore it is recommended to normalize/re-scale data prior to applying -SVR.
97

Novelty detection with SVM-based methods
98

What is it about?

· Find the simplest and most compact region in the space of predictors where the majority of data samples "live" (i.e., with the highest density of samples).
· Build a decision function that takes value +1 in this region and -1 elsewhere.
· Once we have such a decision function, we can identify novel or outlier samples/patients in the data.

Predictor Y

Decision function = +1

*******************************************************************************
Decision function = -1

**

Predictor X

99

Key assumptions
· We do not know classes/labels of samples (positive or negative) in the data available for learning this is not a classification problem
· All positive samples are similar but each negative sample can be different in its own way
Thus, do not need to collect data for negative samples!
100

Sample applications
"Normal"

"Novel"

"Novel"

"Novel"

Modified from: www.cs.huji.ac.il/course/2004/learns/NoveltyDetection.ppt 101

Sample applications

Discover deviations in sample handling protocol when doing quality control of assays.

Protein Y
Samples with low quality
* of processing from the * * lab of Dr. Smith
**
Samples with low quality of processing from ICU patients

********************************************

Samples with high-quality of processing

Samples with low quality
*** of processing from infants Protein X

* Samples with low quality of * * processing from patients
with lung cancer
102

Sample applications

Identify websites that discuss benefits of proven cancer treatments.

* ****

Weighted frequency of
word Y
Websites that discuss cancer prevention methods

* * * *Blogs of cancer patients

**********************************************************************

Websites that discuss benefits of proven cancer treatments

Websites that discuss side-effects of

proven cancer treatments

* ** *

Weighted

frequency of

word X

Websites that discuss
*** unproven cancer treatments

103

One-class SVM
Main idea: Find the maximal gap hyperplane that separates data from the origin (i.e., the only member of the second class is the origin).
wx b 0

Origin is the only member of the second class

i
j
Use "slack variables" as in soft-margin SVMs to penalize these instances

104

Formulation of one-class SVM: linear case

Given training data: x1, x2 ,..., xN Rn

Find f (x) sign(w x b)

wx b 0

by minimizing

1 2

w2

subject to constraints:

1N N i1 i

b

i j

wx b i0

i

for i = 1,...,N.

i.e., the decision function should be positive in all training samples except for small deviations

upper bound on the fraction of outliers (i.e., points outside decision surface) allowed in the data
105

Formulation of one-class SVM: linear and non-linear cases

Linear case

Find f (x) sign(w x b)

by minimizing

1 2

w2

1N N i1 i

subject to constraints:

wx b i0

i

for i = 1,...,N.

b

Non-linear case (use "kernel trick")

Find f (x) sign(w (x) b)

by minimizing

1 2

w2

1N N i1 i

subject to constraints:

b

w (x) b i0

i

for i = 1,...,N.

106

More about one-class SVM
· One-class SVMs inherit most of properties of SVMs for binary classification (e.g., "kernel trick", sample efficiency, ease of finding of a solution by efficient optimization method, etc.);
· The choice of other parameter significantly affects the resulting decision surface.
· The choice of origin is arbitrary and also significantly affects the decision surface returned by the algorithm.
107

Support vector clustering
Contributed by Nikita Lytkin
108

Goal of clustering (aka class discovery)
Given a heterogeneous set of data points x1, x2,..., xN Rn Assign labels y1, y2 ,..., yN {1,2,..., K} such that points with the same label are highly "similar" to each other and are distinctly different from the rest
Clustering process
109

Support vector domain description
· Support Vector Domain Description (SVDD) of the data is a set of vectors lying on the surface of the smallest hyper-sphere enclosing all data points in a feature space
­ These surface points are called Support Vectors

kernel

RR R

110

SVDD optimization criterion

Formulation with hard constraints:

Minimize R2 subject to || (xi ) a ||2 R2 for i = 1,...,N

Squared radius of the sphere

Constraints

a' R'

RR aR

111

Main idea behind Support Vector
Clustering
· Cluster boundaries in the input space are formed by the set of points that when mapped from the input space to the feature space fall exactly on the surface of the minimal enclosing hyper-sphere
­ SVs identified by SVDD are a subset of the cluster boundary points

RR aR

1

112

Cluster assignment in SVC
· Two points xi and xj belong to the same cluster (i.e., have the same label) if every point of the line segment (xi, xj ) projected to the feature space lies within the hypersphere
Some points lie outside the hyper-sphere
RR aR

Every point is within the hypersphere in the feature space

113

Cluster assignment in SVC (continued)

· Point-wise adjacency matrix is constructed by testing the line segments between every pair of points
· Connected components are extracted
· Points belonging to the same connected component are assigned the same label

C

AB E D

ABCDE

A11100

B 1100

C 100

D 11

CE

1

AB E D

114

Effects of noise and cluster overlap

· In practice, data often contains noise, outlier points and

overlapping clusters, which would prevent contour

separation and result in all points being assigned to the

same cluster

Noise

Ideal data

Typical data

Outliers

Overlap

115

SVDD with soft constraints

· SVC can be used on noisy data by allowing a fraction of points,

called Bounded SVs (BSV), to lie outside the hyper-sphere

­ BSVs are not considered as cluster boundary points and are not

assigned to clusters by SVC

Noise

Noise

Outliers

Typical data

kernel

RR aR

Outliers

Overlap

Overlap

116

Soft SVDD optimization criterion

Primal formulation with soft constraints:

Minimize R2 subject to || (xi ) a ||2 R2 i i 0 for i = 1,...,N

Squared radius of the sphere
Introduction of slack variables i mitigates the influence of noise and overlap on the clustering process

Soft constraints

Noise

R Ri

R a

Outliers

Overlap

117

Dual formulation of soft SVDD

Minimize W

i K (xi , xi )

i j K (xi , x j )

i i, j

subject to 0 i C for i = 1,...,N
Constraints

· As before, K (xi , x j ) (xi ) (x j )denotes a kernel function · Parameter 0 C 1 gives a trade-off between volume of the sphere and
the number of errors (C=1 corresponds to hard constraints) · Gaussian kernel K (xi , x j ) exp( xi x j 2 ) tends to yield tighter
contour representations of clusters than the polynomial kernel
· The Gaussian kernel width parameter 0 influences tightness of cluster boundaries, number of SVs and the number of clusters
· Increasing causes an increase in the number of clusters

118

SVM-based variable selection
119

Understanding the weight vector w

Recall standard SVM formulation:

wx b 0

Find w and b that minimize

1 2

w 2 subject to yi (w

xi

b)

1

for i = 1,...,N.

Use classifier: f (x) sign(w x b)

Negative instances (y=-1) Positive instances (y=+1)

· The weight vector w contains as many elements as there are input variables in the dataset, i.e. w Rn.
· The magnitude of each element denotes importance of the
corresponding variable for classification task.

120

Understanding the weight vector w

w (1,1)

w (1,0)

x2

1x1 1x2 b 0

x2

1x1 0x2 b 0

x1
X1 and X2 are equally important
x2 w (0,1) 0x1 1x2 b 0
x1
X2 is important, X1 is not

x1
X1 is important, X2 is not

x3 w (1,1,0) 1x1 1x2 0x3 b 0

x2

x1 X1 and X2 are equally important,

X3 is not

121

Understanding the weight vector w

Gene X2

Melanoma Nevi

SVM decision surface
w (1,1) 1x1 1x2 b 0
Decision surface of another classifier
w (1,0) 1x1 0x2 b 0
True model

X1

Gene X1

· In the true model, X1 is causal and X2 is redundant · SVM decision surface implies that X1 and X2 are equally

X2 Phenotype

important; thus it is locally causally inconsistent

· There exists a causally consistent decision surface for this example

· Causal discovery algorithms can identify that X1 is causal and X2 is redundant 122

Simple SVM-based variable selection algorithm
Algorithm: 1. Train SVM classifier using data for all variables to
estimate vector w
2. Rank each variable based on the magnitude of the
corresponding element in vector w
3. Using the above ranking of variables, select the smallest nested subset of variables that achieves the best SVM prediction accuracy.
123

Simple SVM-based variable selection algorithm

Consider that we have 7 variables:
The vector w is: (0.1, 0.3, 0.4, 0.01,

X1, X2, X3, X4, X5, 0.9, -0.99, 0.2)

X6,

X7

The ranking of variables is: X6, X5, X3, X2, X7, X1, X4

Subset of variables

Classification accuracy

X6 X5 X3 X2 X7 X1 X4 X6 X5 X3 X2 X7 X1 X6 X5 X3 X2 X7

0.920 0.920 0.919

X6 X5 X3 X2 X6 X5 X3 X6 X5 X6

0.852 0.843 0.832 0.821

Best classification accuracy
Classification accuracy that is statistically indistinguishable from the best one

Select the following variable subset: X6, X5, X3, X2 , X7

124

Simple SVM-based variable selection algorithm
· SVM weights are not locally causally consistent we may end up with a variable subset that is not causal and not necessarily the most compact one.
· The magnitude of a variable in vector w estimates
the effect of removing that variable on the objective function of SVM (e.g., function that we want to minimize). However, this algorithm becomes suboptimal when considering effect of removing several variables at a time... This pitfall is addressed in the SVM-RFE algorithm that is presented next.
125

SVM-RFE variable selection algorithm
Algorithm: 1. Initialize V to all variables in the data 2. Repeat 3. Train SVM classifier using data for variables in V to
estimate vector w
4. Estimate prediction accuracy of variables in V using the above SVM classifier (e.g., by cross-validation)
5. Remove from V a variable (or a subset of variables) with the smallest magnitude of the corresponding
element in vector w
6. Until there are no variables in V 7. Select the smallest subset of variables with the best
prediction accuracy
126

SVM-RFE variable selection algorithm

10,000 genes

SVM model
Prediction accuracy

5,000 genes
Important for classification

SVM model
Prediction accuracy

5,000 genes

Discarded

Not important for classification

2,500 genes
Important for classification

...

2,500 genes Discarded
Not important for classification

· Unlike simple SVM-based variable selection algorithm, SVM-
RFE estimates vector w many times to establish ranking of the
variables. · Notice that the prediction accuracy should be estimated at
each step in an unbiased fashion, e.g. by cross-validation. 127

SVM variable selection in feature space

The real power of SVMs comes with application of the kernel trick that maps data to a much higher dimensional space ("feature space") where the data is linearly separable.
Gene 2

Tumor

Tumor

kernel

?

Normal
input space

Gene 1

? Normal

feature space

128

SVM variable selection in feature space
· We have data for 100 SNPs (X1,...,X100) and some phenotype. · We allow up to 3rd order interactions, e.g. we consider:
· X1,...,X100 · X12,X1X2, X1X3,...,X1X100 ,...,X1002 · X13,X1X2X3, X1X2X4,...,X1X99X100 ,...,X1003
· Task: find the smallest subset of features (either SNPs or their interactions) that achieves the best predictive accuracy of the phenotype.
· Challenge: If we have limited sample, we cannot explicitly construct and evaluate all SNPs and their interactions (176,851 features in total) as it is done in classical statistics.
129

SVM variable selection in feature space
Heuristic solution: Apply algorithm SVM-FSMB that: 1. Uses SVMs with polynomial kernel of degree 3 and selects M features (not necessarily input variables!) that have largest weights in the feature space.
E.g., the algorithm can select features like: X10, (X1X2), (X9X2X22), (X72X98), and so on. 2. Apply HITON-MB Markov blanket algorithm to find the Markov blanket of the phenotype using M features from step 1.
130

Computing posterior class probabilities for SVM classifiers
131

Output of SVM classifier

1. SVMs output a class label
(positive or negative) for each sample: sign(w x b)

wx b 0

2. One can also compute distance from the hyperplane that separates classes, e.g. w x b. These distances can be used to compute performance metrics like area under ROC curve.

Negative samples (y=-1)

Positive samples (y=+1)

Question: How can one use SVMs to estimate posterior class probabilities, i.e., P(class positive | sample x)?
132

Simple binning method

1. Train SVM classifier in the Training set.
2. Apply it to the Validation set and compute distances from the hyperplane to each sample.

Sample # 1 2 3 4 5 Distance 2 -1 8 3 4

98 99 100 ...
-2 0.3 0.8

3. Create a histogram with Q (e.g., say 10) bins using the above distances. Each bin has an upper and lower value in terms of distance.

Number of samples in validation set

25 20 15 10
5 0 -15 -10

-5 0 Distance

5

10 15

Training set Validation set
Testing set
133

Simple binning method

4. Given a new sample from the Testing set, place it in the corresponding bin.

E.g., sample #382 has distance to hyperplane = 1, so it is placed in the bin [0, 2.5]

25

Number of samples in validation set

20

15

10

5

0 -15 -10

-5 0 Distance

5

10 15

5. Compute probability P(positive class | sample #382) as a fraction of true positives in this bin.

E.g., this bin has 22 samples (from the Validation set), out of which 17 are true positive ones , so we compute P(positive class | sample #382) = 17/22 = 0.77

Training set Validation set
Testing set
134

Platt's method

Convert distances output by SVM to probabilities by passing them

through the sigmoid filter:

P( positive class | sample)

1 1 exp( Ad

B)

where d is the distance from hyperplane and A and B are parameters.

P(positive class|sample)

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 -10 -8 -6 -4 -2 0 2 4 6 8 10
Distance

135

Platt's method

1. Train SVM classifier in the Training set.

2. Apply it to the Validation set and compute distances from the hyperplane to each sample.

Sample # 1 2 3 4 5 Distance 2 -1 8 3 4

98 99 100 ...
-2 0.3 0.8

3. Determine parameters A and B of the sigmoid function by minimizing the negative log likelihood of the data from the Validation set.

4. Given a new sample from the Testing set, compute its posterior probability using sigmoid function.

Training set
Validation set Testing set

136

Part 3
· Case studies (taken from our research)
1. Classification of cancer gene expression microarray data 2. Text categorization in biomedicine 3. Prediction of clinical laboratory values 4. Modeling clinical judgment 5. Using SVMs for feature selection 6. Outlier detection in ovarian cancer proteomics data
· Software · Conclusions · Bibliography
137

1. Classification of cancer gene expression microarray data
138

Comprehensive evaluation of algorithms for classification of cancer microarray data
Main goals: Find the best performing decision support algorithms for cancer diagnosis from microarray gene expression data; Investigate benefits of using gene selection and ensemble classification methods.
139

Classifiers

K-Nearest Neighbors (KNN) Backpropagation Neural Networks (NN) Probabilistic Neural Networks (PNN) Multi-Class SVM: One-Versus-Rest (OVR) Multi-Class SVM: One-Versus-One (OVO) Multi-Class SVM: DAGSVM Multi-Class SVM by Weston & Watkins (WW) Multi-Class SVM by Crammer & Singer (CS) Weighted Voting: One-Versus-Rest Weighted Voting: One-Versus-One Decision Trees: CART

instance-based neural networks
kernel-based
voting decision trees
140

Ensemble classifiers

dataset

Classifier 1

Classifier 2

Prediction 1 Prediction 2

... ...

Classifier N Prediction N

dataset

Ensemble Classifier
Final Prediction

141

Gene selection methods

Highly discriminatory genes Uninformative genes
genes

1. Signal-to-noise (S2N) ratio in one-versus-rest (OVR) fashion;
2. Signal-to-noise (S2N) ratio in one-versus-one (OVO) fashion;
3. Kruskal-Wallis nonparametric one-way ANOVA (KW);
4. Ratio of genes betweencategories to within-category sum of squares (BW).
142

Performance metrics and statistical comparison
1. Accuracy
+ can compare to previous studies + easy to interpret & simplifies statistical comparison
2. Relative classifier information (RCI)
+ easy to interpret & simplifies statistical comparison + not sensitive to distribution of classes + accounts for difficulty of a decision problem
Randomized permutation testing to compare accuracies of the classifiers ( =0.05)
143

Microarray datasets

Number of
Dataset name Sam- Variables Cateples (genes) gories

Reference

11_Tumors 174 12533 11 Su, 2001

14_Tumors 308 15009 26 Ramaswamy, 2001

9_Tumors

60 5726

9 Staunton, 2001

Brain_Tumor1 90 5920

5 Pomeroy, 2002

Brain_Tumor2 50 10367

4 Nutt, 2003

Leukemia1 72 5327

3 Golub, 1999

Leuk emia2

72 11225

3 Armstrong, 2002

Lung_Cancer 203 12600

5 Bhattacherjee, 2001

SRBCT

83 2308

4 Khan, 2001

Prostate_Tumor 102 10509

2 Singh, 2002

DLBCL

77 5469

2 Shipp, 2002

Total:
~1300 samples
74 diagnostic categories 41 cancer types and 12 normal tissue types
144

Summary of methods and datasets

MC-SVM

Cross-Validation Designs (2)
10-Fold CV LOOCV
Classifiers (11)
One-Versus-Rest One-Versus-One
DAGSVM Method by WW Method by CS
KNN Backprop. NN
Prob. NN Decision Trees One-Versus-Rest One-Versus-One

Based on outputs Based on MCof all classifiers SVM outputs

Gene Selection Methods (4)
S2N One-Versus-Rest S2N One-Versus-One Non-param. ANOVA
BW ratio
Ensemble Classifiers (7)
Majority Voting MC-SVM OVR MC-SVM OVO MC-SVM DAGSVM Decision Trees
Majority Voting
Decision Trees

Performance Metrics (2)
Accuracy
RCI

Statistical Comparison
Randomized permutation testing

Multicategory Dx

Gene Expression Datasets (11)
11_Tumors 14_Tumors 9_Tumors Brain Tumor1 Brain_Tumor2 Leukemia1 Leukemia2 Lung_Cancer
SRBCT Prostate_Tumors
DLBCL

Binary Dx

145

WV

100

80

Results without gene selection

60 OVO

PNN 20

PrBBoLsrruaat11naiiLLnn149tgeee______uu_TTTTTCkkDSTuuuuuaeeuRLmmmmmnmmmBBcoooooiiCCoerrrraar21sssrr12LT

0

40

Accuracy, %

OVR

DAGSVM

WW

CS KNN

NN

146

MC-SVM

Results with gene selection

Improvement in accuracy, % Accuracy, %

Improvement of diagnostic performance by gene selection (averages for the four datasets)

Diagnostic performance before and after gene selection

70 60 50 40 30 20 10
OVR OVO DAGSVM WW CS KNN NN PNN SVM non-SVM

100 9_Tumors
80 60 40 20

14_Tumors

100 Brain_Tumor1
80 60 40 20
SVM non-SVM

Brain_Tumor2
SVM non-SVM

Average reduction of genes is 10-30 times

147

Accuracy, %

Comparison with previously published results
Multiclass SVMs 100 (this study)
80
60
Multiple specialized 40 classification methods
(original primary studies)
20
0
148

ProBBLrrsutaaii11naLLtnn149geee______uu_TTTTTCkkTSDuuuuuaeeuLRmmnmmmmmBBcmiiooooorrrrreoCCaarr21sss21LT

Summary of results
Multi-class SVMs are the best family among the tested algorithms outperforming KNN, NN, PNN, DT, and WV. Gene selection in some cases improves classification performance of all classifiers, especially of non-SVM algorithms; Ensemble classification does not improve performance of SVM and other classifiers; Results obtained by SVMs favorably compare with the literature.
149

Random Forest (RF) classifiers
· Appealing properties
­ Work when # of predictors > # of samples ­ Embedded gene selection ­ Incorporate interactions ­ Based on theory of ensemble learning ­ Can work with binary & multiclass tasks ­ Does not require much fine-tuning of parameters
· Strong theoretical claims · Empirical evidence: (Diaz-Uriarte and Alvarez de
Andres, BMC Bioinformatics, 2006) reported superior classification performance of RFs compared to SVMs and other methods
150

Key principles of RF classifiers

Training data
1) Generate bootstrap samples

Testing data

4) Apply to testing data & combine predictions

2) Random gene selection

3) Fit unpruned decision trees

151

Results without gene selection
· SVMs nominally outperform RFs is 15 datasets, RFs outperform SVMs in 4 datasets, algorithms are exactly the same in 3 datasets.
· In 7 datasets SVMs outperform RFs statistically significantly. · On average, the performance advantage of SVMs is 0.033 AUC and 0.057 RCI. 152

Results with gene selection
· SVMs nominally outperform RFs is 17 datasets, RFs outperform SVMs in 3 datasets, algorithms are exactly the same in 2 datasets.
· In 1 dataset SVMs outperform RFs statistically significantly. · On average, the performance advantage of SVMs is 0.028 AUC and 0.047 RCI. 153

2.Text categorization in biomedicine
154

Models to categorize content and quality: Main idea

1. Utilize existing (or easy to build) training corpora

1 2. Simple document

representations (i.e., typically

2

stemmed and weighted words in title and abstract,

Mesh terms if available;

3 occasionally addition of

Metamap CUIs, author info) as 4 "bag-of-words"

155

Models to categorize content and quality: Main idea

Labeled Examples

Unseen Examples

3. Train SVM models that capture implicit categories of meaning or quality criteria

Labeled
4. Evaluate models' performances - with nested cross-validation or other
appropriate error estimators - use primarily AUC as well as other metrics
(sensitivity, specificity, PPV, Precision/Recall curves, HIT curves, etc.)
5. Evaluate performance prospectively & compare to prior cross-validation estimates

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0
Txmt

Diag

Prog

Etio

Estimated Performance 2005 Performance

156

Models to categorize content and quality: Some notable results

Category

Average AUC

Range over n folds

Treatment Etiology Prognosis Diagnosis

0.97* 0.94* 0.95* 0.95*

0.96 - 0.98 0.89 ­ 0.95 0.92 ­ 0.97 0.93 - 0.98

1. SVM models have excellent ability to identify high-quality PubMed documents according to ACPJ gold standard

Method

Treatment - Etiology -

AUC

AUC

Google Pagerank 0.54

0.54

Yahoo Webranks 0.56

0.49

Impact Factor 2005
Web page hit count
Bibliometric Citation Count

0.67 0.63 0.76

Machine Learning Models

0.96

0.62 0.63 0.69 0.95

Prognosis AUC
0.43

Diagnosis AUC
0.46

0.52 0.52

0.51 0.52

0.58 0.57

0.67 0.60

0.95

0.95

2. SVM models have better classification

performance than PageRank, Yahoo ranks,

Impact Factor, Web Page hit counts, and

bibliometric citation counts on the Web

according to ACPJ gold standard

157

Models to categorize content and quality: Some notable results

Gold standard: SSOAB

Area under the ROC curve*

SSOAB-specific filters 0.893

Citation Count

0.791

ACPJ Txmt-specific filters 0.548

Impact Factor (2001)

0.549

Impact Factor (2005)

0.558

3. SVM models have better classification performance than PageRank, Impact Factor and Citation count in Medline for SSOAB gold standard

Treatment - Fixed Sensitivity

0.98 0.98 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Fixed Sens

0.71

0.89

Spec

Query Filters Learning Models

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

Etiology - Fixed Sensitivity
0.98 0.98 0.75
0.44

Fixed Sens Query Filters

Spec Learning Models

Diagnosis - Fixed Sensitivity

0.96 0.96

0.68

0.88

Fixed Sens Query Filters

Spec Learning Models

Prognosis - Fixed Sensitivity

1 0.9

0.8 0.8

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Fixed Sens

0.87 0.71
Spec

Query Filters Learning Models

Treatment - Fixed Specificity

1 0.9

0.95 0.8

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Sens

0.91 0.91 Fixed Spec

Query Filters Learning Models

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

Etiology - Fixed Specificity

0.94

0.91 0.91

0.68

Sens Query Filters

Fixed Spec Learning Models

Diagnosis - Fixed Specificity

0.65

0.82

0.97 0.97

Sens Query Filters

Fixed Spec Learning Models

Prognosis - Fixed Specificity

1

1 0.9

0.8

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.77 0.77

Sens

Fixed Spec

Query Filters Learning Models

4. SVM models have better sensitivity/specificity in PubMed than CQFs at

comparable thresholds according to ACPJ gold standard

158

Other applications of SVMs to text categorization

Model

Area Under the Curve

Machine Learning Models 0.93

Quackometer*

0.67

Google

0.63

1. Identifying Web Pages with misleading treatment information according to special purpose gold standard (Quack Watch). SVM models outperform Quackometer and Google ranks in the tested domain of cancer treatment.
2. Prediction of future paper citation counts (work of L. Fu and C.F. Aliferis, AMIA 2008)

159

3. Prediction of clinical laboratory values
160

Dataset generation and experimental design
· StarPanel database contains ~8·106 lab measurements of ~100,000 inpatients from Vanderbilt University Medical Center.
· Lab measurements were taken between 01/1998 and 10/2002.
For each combination of lab test and normal range, we generated the following datasets.

01/1998-05/2001

06/2001-10/2002

Training

Validation (25% of Training)

Testing
161

Query-based approach for prediction of clinical cab values

Training data
Data model Train SVM classifier
Validation data Performance
These steps are performed for every data model

database
SVM classifier
These steps are performed for every testing sample

Testing data
Testing sample
Optimal data model
Prediction
162

Classification results

Laboratory test Laboratory test

Including cases with K=0 (i.e. samples with no prior lab measurements)

Area under ROC curve (without feature selection)

BUN Ca CaIo CO2 Creat Mg Osmol PCV Phos

>1 75.9% 67.5% 63.5% 77.3% 62.2% 58.4% 77.9% 62.3% 70.8%

Range of normal values <99 [1, 99] >2.5 <97.5 93.4% 68.5% 81.8% 92.2% 80.4% 55.0% 77.4% 70.8% 52.9% 58.8% 46.4% 66.3% 88.0% 53.4% 77.5% 90.5% 88.4% 83.5% 88.4% 94.9% 71.8% 64.2% 67.0% 72.5% 64.8% 65.2% 79.2% 82.4% 91.6% 69.7% 76.5% 84.6% 75.4% 60.4% 68.0% 81.8%

[2.5, 97.5] 66.9% 60.0% 58.7% 58.1% 83.8% 62.1% 71.5% 70.2% 65.9%

Excluding cases with K=0 (i.e. samples with no prior lab measurements)

Area under ROC curve (without feature selection)

BUN Ca CaIo CO2 Creat Mg Osmol PCV Phos

>1 80.4% 72.8% 74.1% 82.0% 62.8% 56.9% 50.9% 74.9% 74.5%

Range of normal values <99 [1, 99] >2.5 <97.5 99.1% 76.6% 87.1% 98.2% 93.4% 55.6% 81.4% 81.4% 60.0% 50.1% 64.7% 72.3% 93.6% 59.8% 84.4% 94.5% 97.7% 89.1% 91.5% 98.1% 70.0% 49.1% 58.6% 76.9% 60.8% 60.8% 91.0% 90.5% 99.2% 66.3% 80.9% 80.6% 93.6% 64.4% 71.7% 92.2%

[2.5, 97.5] 70.7% 63.4% 57.7% 56.3% 87.7% 59.1% 68.0% 67.1% 69.7%

A total of 84,240 SVM classifiers were built for 16,848 possible data models.

163

Improving predictive power and parsimony of a BUN model using feature selection

Model description

Test name

BUN

Range of normal values Data modeling Number of previous measurements

< 99 perc. SRT
5

Use variables corresponding to hospitalization units?

Yes

Number of prior hospitalizations used

2

Dataset description

N samples N abnormal

N

(total)

samples variables

Training set

3749

78

Validation set 1251 27 3442

Testing set

836

16

Frequency (N measurements)

x 104 3

Histogram of test BUN

2.5

2

1.5

1 0.5
0 0

105

normal values

abnormal values

50 100 150 200 Test value

250

Classification performance (area under ROC curve)

Validation set Testing set Number of features

All 95.29% 94.72%
3442

RFE_Linear 98.78% 99.66% 26

RFE_Poly 98.76% 99.63% 3

HITON_PC 99.12% 99.16% 11

HITON_MB 98.90% 99.05% 17

164

Classification performance (area under ROC curve)

Validation set Testing set Number of features

All 95.29% 94.72%
3442

RFE_Linear 98.78% 99.66% 26

RFE_Poly 98.76% 99.63% 3

HITON_PC 99.12% 99.16% 11

HITON_MB 98.90% 99.05% 17

Features
1 2
3
4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

LAB: PM_1(BUN)

LAB: PM_1(BUN)

LAB: PM_1(BUN)

LAB: PM_1(BUN)

LAB: PM_2(Cl)

LAB: Indicator(PM_1(Mg)) LAB: PM_5(Creat)

LAB: PM_5(Creat)

LAB: DT(PM_3(K))

LAB: Test Unit NO_TEST_MEASUREMENT LAB: PM_1(Phos) (Test CaIo, PM 1)

LAB: PM_3(PCV)

LAB: DT(PM_3(Creat))

LAB: Indicator(PM_1(BUN))

LAB: PM_1(Mg)

LAB: Test Unit J018 (Test Ca, PM 3)

LAB: Indicator(PM_5(Creat))

LAB: PM_1(Phos)

LAB: DT(PM_4(Cl))

LAB: Indicator(PM_1(Mg))

LAB: Indicator(PM_4(Creat))

LAB: DT(PM_3(Mg))

LAB: DT(PM_4(Creat))

LAB: Indicator(PM_5(Creat))

LAB: PM_1(Cl)

LAB: Test Unit 7SCC (Test Ca, PM 1) LAB: Indicator(PM_3(PCV))

LAB: PM_3(Gluc)

LAB: Test Unit RADR (Test Ca, PM 5) LAB: Indicator(PM_1(Phos))

LAB: DT(PM_1(CO2))

LAB: Test Unit 7SMI (Test PCV, PM 4) LAB: DT(PM_4(Creat))

LAB: DT(PM_4(Gluc))

DEMO: Gender

LAB: Test Unit 11NM (Test BUN, PM 2)

LAB: PM_3(Mg)

LAB: Test Unit 7SCC (Test Ca, PM 1)

LAB: DT(PM_5(Mg))

LAB: Test Unit RADR (Test Ca, PM 5)

LAB: PM_1(PCV)

LAB: Test Unit 7SMI (Test PCV, PM 4)

LAB: PM_2(BUN)

LAB: Test Unit CCL (Test Phos, PM 1)

LAB: Test Unit 11NM (Test PCV, PM 2)

DEMO: Gender

LAB: Test Unit 7SCC (Test Mg, PM 3)

DEMO: Age

LAB: DT(PM_2(Phos))

LAB: DT(PM_3(CO2))

LAB: DT(PM_2(Gluc))

LAB: DT(PM_5(CaIo))

DEMO: Hospitalization Unit TVOS

LAB: PM_1(Phos)

LAB: PM_2(Phos)

LAB: Test Unit 11NM (Test K, PM 5)

LAB: Test Unit VHR (Test CaIo, PM 1)

165

4. Modeling clinical judgment
166

Methodological framework and study outline

Patients

Guidelines

Physicians

Predict clinical decisions

Physician 1

Patient
1
...
N
...
1
...
N

Feature f1...fm
...
f1...fm

Clinical Diagnosis
cd1
...
cdN
...
cd1
...
cdN

Gold Standard
hd1
...
hdN
...
hd1
...
hdN

same across physicians different across physicians

Identify predictors ignored by physicians
Explain each physician's diagnostic model
Compare physicians with each other and with guidelines

Physician6

Clinical context of experiment
Malignant melanoma is the most dangerous form of skin cancer Incidence & mortality have been constantly increasing in the last decades.

Physicians and patients

Patients N=177 76 melanomas - 101 nevi
Dermatologists N = 6 3 experts - 3 non-experts

Data collection:
Patients seen prospectively, from 1999 to 2002 at Department of Dermatology, S.Chiara Hospital, Trento, Italy
inclusion criteria: histological diagnosis and >1 digital image available
Diagnoses made in 2004

Features

Lesion location

Family history of melanoma

Irregular Border

Max-diameter

Fitzpatrick's Photo-type

Min-diameter Sunburn

Number of colors
Atypical pigmented network

Evolution

Ephelis

Abrupt network cut-off

Age Gender

Lentigos Asymmetry

Regression-Erythema Hypo-pigmentation

Streaks (radial streaming, pseudopods)
Slate-blue veil
Whitish veil
Globular elements Comedo-like openings, milia-like cysts Telangiectasia

Method to explain physician-specific SVM models

FS
Build SVM
SVM "Black Box"
Regular Learning

Apply SVM

Build DT
Meta-Learning

Results: Predicting physicians' judgments

Physicians
Expert 1

All
(features)
0.94 (24)

HITON_PC (features)
0.92 (4)

HITON_MB (features)
0.92 (5)

RFE
(features)
0.95 (14)

Expert 2

0.92 (24) 0.89 (7)

0.90 (7) 0.90 (12)

Expert 3

0.98 (24) 0.95 (4)

0.95 (4) 0.97 (19)

NonExpert 1 0.92 (24) 0.89 (5)

0.89 (6) 0.90 (22)

NonExpert 2 1.00 (24) 0.99 (6)

0.99 (6) 0.98 (11)

NonExpert 3 0.89 (24) 0.89 (4)

0.89 (6) 0.87 (10)

Results: Physician-specific models

Results: Explaining physician agreement

Patient 001

Blue veil
yes

irregular border
no

streaks
yes

Expert 1 AUC=0.92 R2=99%

Expert 3 AUC=0.95 R2=99%

Results: Explain physician disagreement

Patient 002

Blue veil
no

irregular border
no

streaks

number of colors

evolution

yes 3

no

Expert 1 AUC=0.92 R2=99%

Expert 3 AUC=0.95 R2=99%

Results: Guideline compliance

Physician

Reported guidelines

Experts1,2,3, non-expert 1

Pattern analysis

Compliance
Non-compliant: they ignore the majority of features (17 to 20) recommended by pattern analysis.

Non expert 2 ABCDE rule

Non compliant: asymmetry, irregular border and evolution are ignored.

Non expert 3

Non-standard. Reports using 7 features

Non compliant: 2 out of 7 reported features are ignored while some nonreported ones are not

On the contrary: In all guidelines, the more predictors present,
the higher the likelihood of melanoma. All physicians were compliant with this principle.

5. Using SVMs for feature selection
176

Feature selection methods

Feature selection methods (non-causal)

· SVM-RFE · Univariate + wrapper

This is an SVM-based feature selection

· Random forest-based method

· LARS-Elastic Net

· RELIEF + wrapper

· L0-norm

· Forward stepwise feature selection

· No feature selection

Causal feature selection methods

· HITON-PC · HITON-MB · IAMB · BLCD · K2MB

This method outputs a Markov blanket of the response variable (under assumptions)

...

177

13 real datasets were used to evaluate feature selection methods

Dataset name Infant_Mortality

Domain Clinical

Number of variables
86

Number of samples

Target

5,337 Died within the first year

Ohsumed

Text 14,373

5,000 Relevant to nenonatal diseases

ACPJ_Etiology Lymphoma Gisette

Text
Gene expression
Digit recognition

28,228 7,399 5,000

15,779 227

Relevant to eitology 3-year survival: dead vs. alive

7,000 Separate 4 from 9

Dexter

Text 19,999

600 Relevant to corporate acquisitions

Data type discrete continuous continuous continuous continuous continuous

Sylva

Ecology

216

14,394 Ponderosa pine vs. everything else

continuous & discrete

Ovarian_Cancer Proteomics

Thrombin Breast_Cancer Hiva

Drug discovery
Gene expression
Drug discovery

Nova

Text

2,190 139,351 17,816
1,617 16,969

216 2,543 286

Cancer vs. normals

continuous

Binding to thromin

discrete (binary)

Estrogen-receptor positive (ER+) vs. ER- continuous

4,229 Activity to AIDS HIV infection

discrete (binary)

1,929 Separate politics from religion topics

discrete (binary)

Bankruptcy

Financial

147

7,063 Personal bankruptcy

continuous & discrete

178

Classification performance vs. proportion of selected features

Classification performance (AUC) Classification performance (AUC)

Original 1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6 0.55
0.50

HITON-PC with G2 test RFE
0.5 1 Proportion of selected features

Magnified 0.9
0.89
0.88
0.87
0.86
0.85 HITON-PC with G2 test RFE
0.05 0.1 0.15 0.2 Proportion of selected features
179

Statistical comparison of predictivity and reduction of features

SVM-RFE (4 variants)

Predicitivity

Reduction

P-value Nominal winner P-value Nominal winner

0.9754 0.8030 0.1312 0.1008

SVM-RFE SVM-RFE HITON-PC HITON-PC

0.0046 0.0042 0.3634 0.6816

HITON-PC HITON-PC HITON-PC SVM-RFE

· Null hypothesis: SVM-RFE and HITON-PC perform the same; · Use permutation-based statistical test with alpha = 0.05.
180

Simulated datasets with known causal structure used to compare algorithms
181

Comparison of SVM-RFE and HITON-PC
182

Comparison of all methods in terms of causal graph distance

SVM-RFE

HITON-PC based causal
methods

183

HITON-PC based causal
methods
HITON-PCFDR
methods

Summary results

SVM-RFE

184

Statistical comparison of graph distance

Comparison

Sample size =

200

P-value

Nominal winner

Sample size =

500

P-value

Nominal winner

average HITON-PC-FDR with G2 test vs. average SVM-RFE

<0.0001

HITON-PCFDR

0.0028

HITON-PCFDR

Sample size =

5000

P-value

Nominal winner

<0.0001

HITON-PCFDR

· Null hypothesis: SVM-RFE and HITON-PC-FDR perform the same; · Use permutation-based statistical test with alpha = 0.05.

185

6. Outlier detection in ovarian cancer proteomics data
186

Ovarian cancer data

Same set of 216 patients, obtained using the Ciphergen H4 ProteinChip array (dataset 1) and using the Ciphergen WCX2 ProteinChip array (dataset 2).

Cancer
Normal Other Cancer

Data Set 1 (Top), Data Set 2 (Bottom)

Normal

Other

4000

8000 Clock Tick

12000

The gross break at the "benign disease" juncture in dataset 1 and the similarity of the

profiles to those in dataset 2 suggest change of protocol in the middle of the first

experiment.

Experiments with one-class SVM

Assume that sets {A, B} are normal and {C, D, E, F} are outliers. Also, assume that we do not know what are normal and outlier samples.
·Experiment 1: Train one-class SVM on {A, B, C} and test on {A, B, C}: Area under ROC curve = 0.98 ·Experiment 2: Train one-class SVM on {A, C} and test on {B, D, E, F}: Area under ROC curve = 0.98

Cancer
Normal Other Cancer
Normal Other

Data Set 1 (Top), Data Set 2 (Bottom)

4000 8000 Clock Tick

12000

188

Software
189

Interactive media and animations
SVM Applets
· http://www.csie.ntu.edu.tw/~cjlin/libsvm/ · http://svm.dcs.rhbnc.ac.uk/pagesnew/GPat.shtml · http://www.smartlab.dibe.unige.it/Files/sw/Applet%20SVM/svmapplet.html · http://www.eee.metu.edu.tr/~alatan/Courses/Demo/AppletSVM.html · http://www.dsl-lab.org/svm_tutorial/demo.html (requires Java 3D)
Animations
· Support Vector Machines:
http://www.cs.ust.hk/irproj/Regularization%20Path/svmKernelpath/2moons.avi http://www.cs.ust.hk/irproj/Regularization%20Path/svmKernelpath/2Gauss.avi http://www.youtube.com/watch?v=3liCbRZPrZA
· Support Vector Regression: http://www.cs.ust.hk/irproj/Regularization%20Path/movie/ga0.5lam1.avi
190

Several SVM implementations for beginners
· GEMS: http://www.gems-system.org · Weka: http://www.cs.waikato.ac.nz/ml/weka/ · Spider (for Matlab): http://www.kyb.mpg.de/bs/people/spider/ · CLOP (for Matlab): http://clopinet.com/CLOP/
191

Several SVM implementations for intermediate users
· LibSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
General purpose Implements binary SVM, multiclass SVM, SVR, one-class SVM Command-line interface Code/interface for C/C++/C#, Java, Matlab, R, Python, Pearl
· SVMLight: http://svmlight.joachims.org/
General purpose (designed for text categorization) Implements binary SVM, multiclass SVM, SVR Command-line interface Code/interface for C/C++, Java, Matlab, Python, Pearl
More software links at http://www.support-vector-machines.org/SVM_soft.html and http://www.kernel-machines.org/software
192

Conclusions
193

Strong points of SVM-based learning methods

· Empirically achieve excellent results in high-dimensional data

with very few samples

· Internal capacity control to avoid overfitting

· Can learn both simple linear and very complex nonlinear

functions by using "kernel trick"

· Robust to outliers and noise (use "slack variables")

· Convex QP optimization problem (thus, it has global minimum

and can be solved efficiently)

· Solution is defined only by a small subset of training points

("support vectors")

· Number of free parameters is bounded by the number of

support vectors and not by the number of variables

· Do not require direct access to data, work only with dot-

products of data-points.

194

Weak points of SVM-based learning methods
· Measures of uncertainty of parameters are not currently well-developed
· Interpretation is less straightforward than classical statistics
· Lack of parametric statistical significance tests · Power size analysis and research design considerations
are less developed than for classical statistics
195

Bibliography
196

Part 1: Support vector machines for binary classification: classical formulation
· Boser BE, Guyon IM, Vapnik VN: A training algorithm for optimal margin classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory (COLT) 1992:144-152.
· Burges CJC: A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery 1998, 2:121-167.
· Cristianini N, Shawe-Taylor J: An introduction to support vector machines and other kernelbased learning methods. Cambridge: Cambridge University Press; 2000.
· Hastie T, Tibshirani R, Friedman JH: The elements of statistical learning: data mining, inference, and prediction. New York: Springer; 2001.
· Herbrich R: Learning kernel classifiers: theory and algorithms. Cambridge, Mass: MIT Press; 2002.
· Schölkopf B, Burges CJC, Smola AJ: Advances in kernel methods: support vector learning. Cambridge, Mass: MIT Press; 1999.
· Shawe-Taylor J, Cristianini N: Kernel methods for pattern analysis. Cambridge, UK: Cambridge University Press; 2004.
· Vapnik VN: Statistical learning theory. New York: Wiley; 1998.
197

Part 1: Basic principles of statistical machine learning
· Aliferis CF, Statnikov A, Tsamardinos I: Challenges in the analysis of mass-throughput data: a technical commentary from the statistical machine learning perspective. Cancer Informatics 2006, 2:133-162.
· Duda RO, Hart PE, Stork DG: Pattern classification. 2nd edition. New York: Wiley; 2001. · Hastie T, Tibshirani R, Friedman JH: The elements of statistical learning: data mining,
inference, and prediction. New York: Springer; 2001. · Mitchell T: Machine learning. New York, NY, USA: McGraw-Hill; 1997. · Vapnik VN: Statistical learning theory. New York: Wiley; 1998.
198

Part 2: Model selection for SVMs
· Hastie T, Tibshirani R, Friedman JH: The elements of statistical learning: data mining, inference, and prediction. New York: Springer; 2001.
· Kohavi R: A study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI) 1995, 2:1137-1145.
· Scheffer T: Error estimation and model selection. Ph.D.Thesis, Technischen Universität Berlin, School of Computer Science; 1999.
· Statnikov A, Tsamardinos I, Dosbayev Y, Aliferis CF: GEMS: a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data. Int J Med Inform 2005, 74:491-503.
199

Part 2: SVMs for multicategory data
· Crammer K, Singer Y: On the learnability and design of output codes for multiclass problems. Proceedings of the Thirteenth Annual Conference on Computational Learning Theory (COLT) 2000.
· Platt JC, Cristianini N, Shawe-Taylor J: Large margin DAGs for multiclass classification. Advances in Neural Information Processing Systems (NIPS) 2000, 12:547-553.
· Schölkopf B, Burges CJC, Smola AJ: Advances in kernel methods: support vector learning. Cambridge, Mass: MIT Press; 1999.
· Statnikov A, Aliferis CF, Tsamardinos I, Hardin D, Levy S: A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis. Bioinformatics 2005, 21:631-643.
· Weston J, Watkins C: Support vector machines for multi-class pattern recognition. Proceedings of the Seventh European Symposium On Artificial Neural Networks 1999, 4:6.
200

Part 2: Support vector regression
· Hastie T, Tibshirani R, Friedman JH: The elements of statistical learning: data mining, inference, and prediction. New York: Springer; 2001.
· Smola AJ, Schölkopf B: A tutorial on support vector regression. Statistics and Computing 2004, 14:199-222.
Part 2: Novelty detection with SVM-based methods and Support Vector Clustering
· Scholkopf B, Platt JC, Shawe-Taylor J, Smola AJ, Williamson RC: Estimating the Support of a High-Dimensional Distribution. Neural Computation 2001, 13:1443-1471.
· Tax DMJ, Duin RPW: Support vector domain description. Pattern Recognition Letters 1999, 20:1191-1199.
· Hur BA, Horn D, Siegelmann HT, Vapnik V: Support vector clustering. Journal of Machine Learning Research 2001, 2:125­137.
201

Part 2: SVM-based variable selection
· Guyon I, Elisseeff A: An introduction to variable and feature selection. Journal of Machine Learning Research 2003, 3:1157-1182.
· Guyon I, Weston J, Barnhill S, Vapnik V: Gene selection for cancer classification using support vector machines. Machine Learning 2002, 46:389-422.
· Hardin D, Tsamardinos I, Aliferis CF: A theoretical characterization of linear SVM-based feature selection. Proceedings of the Twenty First International Conference on Machine Learning (ICML) 2004.
· Statnikov A, Hardin D, Aliferis CF: Using SVM weight-based methods to identify causally relevant and non-causally relevant variables. Proceedings of the NIPS 2006 Workshop on Causality and Feature Selection 2006.
· Tsamardinos I, Brown LE: Markov Blanket-Based Variable Selection in Feature Space. Technical report DSL-08-01 2008.
· Weston J, Mukherjee S, Chapelle O, Pontil M, Poggio T, Vapnik V: Feature selection for SVMs. Advances in Neural Information Processing Systems (NIPS) 2000, 13:668-674.
· Weston J, Elisseeff A, Scholkopf B, Tipping M: Use of the zero-norm with linear models and kernel methods. Journal of Machine Learning Research 2003, 3:1439-1461.
· Zhu J, Rosset S, Hastie T, Tibshirani R: 1-norm support vector machines. Advances in Neural Information Processing Systems (NIPS) 2004, 16.
202

Part 2: Computing posterior class probabilities for SVM classifiers

· Platt JC: Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Advances in Large Margin Classifiers. Edited by Smola A, Bartlett B, Scholkopf B, Schuurmans D. Cambridge, MA: MIT press; 2000.
Part 3: Classification of cancer gene expression microarray data (Case Study 1)

· Diaz-Uriarte R, Alvarez de Andres S: Gene selection and classification of microarray data

using random forest. BMC Bioinformatics 2006, 7:3.

· Statnikov A, Aliferis CF, Tsamardinos I, Hardin D, Levy S: A comprehensive evaluation of

multicategory classification methods for microarray gene expression cancer diagnosis.

Bioinformatics 2005, 21:631-643.

· Statnikov A, Tsamardinos I, Dosbayev Y, Aliferis CF: GEMS: a system for automated cancer

diagnosis and biomarker discovery from microarray gene expression data. Int J Med

Inform 2005, 74:491-503.

· Statnikov A, Wang L, Aliferis CF: A comprehensive comparison of random forests and

support vector machines for microarray-based cancer classification. BMC Bioinformatics

2008, 9:319.

203

Part 3: Text Categorization in Biomedicine (Case Study 2)
· Aphinyanaphongs Y, Aliferis CF: Learning Boolean queries for article quality filtering. Medinfo 2004 2004, 11:263-267.
· Aphinyanaphongs Y, Tsamardinos I, Statnikov A, Hardin D, Aliferis CF: Text categorization models for high-quality article retrieval in internal medicine. J Am Med Inform Assoc 2005, 12:207-216.
· Aphinyanaphongs Y, Statnikov A, Aliferis CF: A comparison of citation metrics to machine learning filters for the identification of high quality MEDLINE documents. J Am Med Inform Assoc 2006, 13:446-455.
· Aphinyanaphongs Y, Aliferis CF: Prospective validation of text categorization models for indentifying high-quality content-specific articles in PubMed. AMIA 2006 Annual Symposium Proceedings 2006.
· Aphinyanaphongs Y, Aliferis C: Categorization Models for Identifying Unproven Cancer Treatments on the Web. MEDINFO 2007.
· Fu L, Aliferis C: Models for Predicting and Explaining Citation Count of Biomedical Articles. AMIA 2008 Annual Symposium Proceedings 2008.
204

Part 3: Modeling clinical judgment (Case Study 4)
· Sboner A, Aliferis CF: Modeling clinical judgment and implicit guideline compliance in the diagnosis of melanomas using machine learning. AMIA 2005 Annual Symposium Proceedings 2005:664-668.
Part 3: Using SVMs for feature selection (Case Study 5)
· Aliferis CF, Statnikov A, Tsamardinos I, Mani S, Koutsoukos XD: Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification. Part II: Analysis and Extensions. Journal of Machine Learning Research 2008.
· Aliferis CF, Statnikov A, Tsamardinos I, Mani S, Koutsoukos XD: Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification. Part I: Algorithms and Empirical Evaluation. Journal of Machine Learning Research 2008.
205

Part 3: Outlier detection in ovarian cancer proteomics data (Case Study 6)
· Baggerly KA, Morris JS, Coombes KR: Reproducibility of SELDI-TOF protein patterns in serum: comparing datasets from different experiments. Bioinformatics 2004, 20:777-785.
· Petricoin EF, Ardekani AM, Hitt BA, Levine PJ, Fusaro VA, Steinberg SM, Mills GB, Simone C, Fishman DA, Kohn EC, Liotta LA: Use of proteomic patterns in serum to identify ovarian cancer. Lancet 2002, 359:572-577.
206

Thank you for your attention! Questions/Comments?
Email: Alexander.Statnikov@med.nyu.edu URL: http://ww.nyuinformatics.org
207

