Several tools -originally intended for developing "classical" compilers have also proved useful in a variety of application programs. /4
I 11
U
Language Development Tools on the Unix System
U
Stephen C. Johnson Bell Laboratories

I

Every computer program defines an input language, albeit a primitive one. Too often, the same programmers who decry the lack of regular, powerful programming languages for their own use turn around and provide their users with rigid, restrictive, and awkward input languages. Although regular syntax, flexible input, and powerful error detection and recovery lead to improved reliability and productivity, too many programs force fixed-length fields and fixed column positions and provide little or no error checking and recovery. Features such as arithmetic expressions have proved their worth in other areas of programming, but rarely appear in application programs.
To a considerable extent, these problems are inherited from the 1/0 facilities of current programming languages. For example, Fortran format statements encourage rigid, card-column-oriented input. Although we may wish to allow the expression "2 + 3" wherever the input "5" is legal, writing such a program in Fortran or Cobol is so difficult that few try and even fewer succeed. Such improvements eat up valuable programming resources and will not be done if seen as costly. In particular, improvements will almost certainly not be done if parsing theory must be learned in order to do them.
On the Unix* system, several tools are available which allow nonspecialists to define and process rich input languages. Although originally intended for the development of "classical" compilers, these tools have proved useful in a wide variety of application programs as well. This article discusses the design and implementation of these tools, as well as the environment which has grown up to support them.
'Unix is a trademark of Bell Laboratories..

Tools
Over the years, a number of general utility programs have grown up on the Unix system. Many of these tools may be viewed as an encapsulation of a piece of specialized knowledge-anything from driving a piece of hardware to performing a utility task. They may be as simple as a sin subroutine, or as complex as a disk driver. By using tools, programmers are able to gain access to facilities and algorithms that they do not have to understand in detail. Not surprisingly, constructing programs by using available tools and pieces has proved to be very successful:
* The resulting products are produced quickly. * They are likely to work correctly. * They are often quite flexible and adaptable as ap-
plications change. * Inter-machine portability is often enhanced by using
tools. * Tool usage encourages a natural modularity in the
resulting program. * Using tools constructed by experts, programmers get
the use of expert algorithms. The keys to useful tools are high-quality specialized knowledge and high-quality packaging. Without underlying knowledge, tools are ponderous and unacceptably slow, offering little over hand-coding. Without proper packaging, tools remain unused because they solve the wrong problem or are too difficult to apply to real situations. In many cases, theoretical insights and algorithms are essential to the success of a tool. Applying theory directly from textbooks, however, is likely to lead to problems.

0018-9162/80/0800-0016$00.75 '.' 1980 IEEE

COM PUTER

Theoretical models are, after all, models. Real problems sions.2 Figure 3 gives a sufficient fragment of Lex input to

are often nearly, but not precisely, modeled by the theory. Theory provides insights into the structure of hard problems, leading to techniques for structuring these problems. To paraphrase Hamming, the purpose of theory is insight-not theorems. The tool builder's challenge is to make the theoretical insights apply to the 90 to 95 percent of the problem the model fits, without making the remaining 5 to 10 percent impossible to.do at all.
A particularly interesting form of tool is the program generator, schematically depicted in Figure 1. The user supplies a description of the problem to be solved, and the program generator then constructs a program to solve the problem. Of course, not all problems can be solved in

deal with the example used in Figure 2. Issues such as treatment of blanks, reserved words, and
operator definitions can each be dealt with in a line or two of specification. When one of the patterns on the left (see Figure 3) is matched, an associated action is invoked to process the matched text. This action is a fragment of code written in C, 3,4 the dominant programming language on the Unix system. Usually, this action returns an indication of what token was seen. However, when a blank is seen, the action does nothing: the blank is ignored, and the next token sought. When an identifier is seen, the action can enter the name into a name table, or make a symbol table entry or reference, etc., before returning the

such a fashion. Automatic generation of a complete com- token ID.

piler or operating system is still beyond us. Nevertheless,

the approach works excellently for certain parts of such

larger systems. In fact, one of the strengths of program

generators is that they need not do an entire application.

They limit themselves-and thus the restrictions of their

models-to one part of a larger program. Moreover, the

specification of the problem may (and should) be in-

dependent of the operating system and target language of Figure 1. A program generator-one way of packaging

the resulting program, thus enhancing program portabili- knowledge.

ty. Finally, the programmer becomes insulated from im-

plementation techniques. Theory can be packaged and

used, and the generator improved, without intervention

by the application programmer. As discussed below, pro-

gram generators are a valuable way of packaging compiler

construction tools.

Compiler construction. For the moment, we will concentrate on the front end of compilers, since this part is the one that is best understood theoretically. When a compiler sees an input like

let force = mass * accel

it first breaks the input into lexical tokens; we might have

LET ID ASSIGN ID MUL ID Here, LET is a token representing the key word let; ASSIGN and MUL represent the assignment and multi-

sioFctinargluutctrhtoeeukm2re.,enAsa(f,ustsetusrhaehtllohpyewarncasoteamrrbpeoieiv)lmeecp,roorhbsrayeesssabprarroogankrndegaiinmnnmgtgahttetoihictneahplmeuitrsitnilrnaauttcoedtrlaueItrxnae-tended use in the compiler.

plication operators; and ID represents the identifiers,

force, mass, and accel. With each of the ID tokens, there

also would be associated additional information-for ex-

ample, a name table index-which would describe the

particular identifier seen.

This part of the compiling process is done by the lexical

analyzer. The next step is to impose a grammatical struc-

ture on the tokens. As shown in the example given in

Figure 2, the parser takes the tokens and arranges them in

a data structure, usually a tree, that corresponds to their

later intended use in the compiler. Lexical analyzers and

parsers are well understood theoretically, and this theory

can be packaged in useful tools. On the Unix system, a

program called Lex generates lexical analyzers, and a pro-

gram called Yacc generates parsers.

Lex. LexI is a program generator. The input specification language is based on the notation of regular expres-

Figure 3. Lex input.

August 1980

17

Lex solves problems that often cause trouble in hand- recognizes that the top token on the stack, ID, matches

written lexical analyzers. For example, the reserved word the right side of a rule; this token can be reduced to the

let also matches the pattern for an identifier, but since Lex symbol expr. The tokens matching the right side (in this

chooses the first such pattern match, we need not worry case, the single token ID) are popped from the stack, and

that let will get into our name table. Another troublesome the symbol expr is pushed in their stead. No further reduc-

issue involves tokens such as identifiers and constants, tions are possible, so the lexical analyzer is asked for addi-

where finding the end of such a token requires reading up tional tokens. MUL and ID are returned and stacked, and

to a character which is not a letter or digit. That character the ID token is reduced to expr, as before. Now the top

must be stashed away where it can be read again when the three elements on the stack are

next token is needed. Lex provides this service automati-

cally. expr MUL expr

Although Lex was designed for use in compiler con-

struction, it has been used in many nontraditional ways as matching the right side of another rule for expr. This rule

well. Some jobs, such as a program to remove typesetting is reduced, the three elements on the top of the stack are

commands from a document and produce i word list, are popped, and expr pushed in their stead. Now, the stack

sufficiently simple that no parser is needed. Lex has been contains

used as part of a portable peephole optimizer, in which

the regular expressions are used to adapt the optimizer to

LET ID ASSIGN expr

many different assemblers. In addition, a Lex program

has been used to process optically scanned input as part of This is the right side of rule deriving statement; this rule is

an image processing project. 5

reduced, the four top stack elements popped, and state-

As these applications indicate, packaging the theory of ment pushed in their stead. This process continues until

regular expressions as a program generator creates a very the entire input is correctly parsed or an error is detected.

useful tool. It would have been difficult to extract these Parsing actions are done when a rule is reduced. Before

algorithms from a monolithic "compiler-writing" sys- the tokens matching the right side of a rule are popped,

tem. the code fragment associated with the rule is executed.

The lexical analyzer may return values associated with

Yacc. The program generator Yacc generates parsers tokens recognized. For example, the token ID probably

from an input specification language that describes the would have a value that would describe the particular

desired syntax. Figure 4 provides an example of Yacc in- name that was seen. In turn, the parsing actions can

put. Once again, each major construct can be dealt with in return values as well. The parameters $n are used to access

a couple of lines. As with Lex, each rule is associated with the values returned by the lexical analyzer or earlier rules;

a fragment of C code, invoked as the rule is recognized. the parameter $$ describes the value returned by the cur-

Yacc handles the recognition of rules and invokes the pro- rent rule. In the example, we postulate a tree-building

gram fragments when the associated rules are recognized. function, build, which is invoked to construct the parse

The parsers generated by Yacc consist of a finite-state tree of Figure 2. In compiler applications, a data structure

machine and a stack. To parse the example in Figure 4, the such as a parse tree is typically constructed by the parser

parser would first call the lexical analyzer and obtain the and passed to an optimizer or code generator. In more

first token, LET. This token is placed on the stack, the unusual applications, however, the parsing actions may

parser changes state, and asks for another token; ID is returned. Once again, the token is pushed onto the stack,

compute and return values directly.
Yacc is based on the theory of LALR(l) parsers.2'6 The

and the parser changes state. Similarly, ASSIGN and ID user is given a reasonable interface, and it is up to Yacc to

are pushed onto the stack. At this point, the parser take care of getting the parsing job done in good order.

While most users eventually learn something about how

the parser operates, few know the theory on which the

%token

LET ID MUL ASSIGN . ..

parser is constructed. The theoretical power of the method allows some ad-

vantages to be passed on to the users. A large, interesting

class of grammars is handled. For example, the user need

statement: LET ID ASSIGN expr

not avoid left recursive rules. Yacc also permits controlled

use of ambiguous grammars, with disambiguating rules,

* * a making it much easier to specify operator precedence and

expr expr MUL expr

handle some classic language design problems, including

{$$=build (MUL,$1,$3);}

the IF-THEN-ELSE ambiguity. Moreover, Yacc parsers have very good error detection properties. This theoreti-

ID

cal power is useful, however, only to the extent that it makes the tool easier for the nonspecialist to use.

Although Yacc and Lex were designed to work

together, Yacc can be used separately in those cases (e.g.,

Fortran) where the lexical analyzer must perform obscene

acts of a distinctly nontheoretical nature (for example,

Figure 4. Yacc input.

processing Hollerith constants). Although the major use

18 COMPUTER

of Yacc has been in the construction of compilers, many when program generators are not used and nearly in-

of the compilers are rather unconventional-for exam- dispensable when they are.

ple, an equation typesetting language. 7 Yacc also has been

used to provide interactive front ends for desk calculator

programs, data bases, etc. Once again, monolithic com- Testing

piler-writer systems would be difficult or uneconomical to

adapt to many of these applications; the result would be A number of general aspects of the Unix system en-

ad hoc input languages, probably of less power and re- vironment contribute to the process of testing compilers.

quiring more effort to implement.

A few of the most useful are mentioned below.

A class of subtle bugs in compilers arise from a process

Other compiler tools. It would be pleasant to report of "faulty generalization": a particular correct section of

that the other parts of a compiler could be similarly the compiler may do something sensible, but it may be ap-

generated from specifications, but this is not so. Symbol plied in instances when it should not be. This difficulty is

table management, optimization, code generation, most apparent when bugs are tracked down and fixed, for

diagnostics, etc., still require thought andl handwritten if the generalization was faulty the first time, it has a good

code. A certain amount of theory exists, but it is far from chance of being botched a second time. A specific bug fix

complete. In code generation, for example, a compiler may break other working code sequences. More subtle,

must deal with issues such as asymmetric registers (e.g., but quite common in my experience, is a bug fix that

register pairs), operator side effects (condition codes and causes other parts of the compiler to generate correct, but

assignment operators), the knotty issue of the sizes and much less efficient, code. This last form of bug is par-

types of operands, overflow, base register addressability, ticularly insidious, since a later attempt to improve the

calling sequences, stack frame organizations, and so on. code quality often reintroduces the original bug.

Theory has little to offer directly on most of these issues, Of course, we assume that the programmer has taken

except the occasional confirmation that the problems are all reasonable steps to design and build compilers without

indeed difficult. However, the compiler writer.cannot bugs. One of the crucial issues appears to be structuring

evade these problems.

the compiler so that the maintainer can retain intellectual

While the theory is not yet packaged into general- control over the design. Nevertheless, a solid program of

purpose tools, it has nevertheless proved useful as a struc- testing is wise.

turing principle. In the portable C compiler, 8'9 theoretical Particularly useful are regression tests. These tests in-

results helped identify the hard parts, suggested a number volve running new and old versions of the compiler on

of code generation techniques, and identified a number of large bodies of input, and then comparing the outputs. A

generally useful functions. Moreover, the theory is begin- number of Unix system utilities make this staightforward.

ning to affect hardware design, leading to machines that The program Diff'3 takes two arbitrary text files, com-

are better matched to current compiling techniques. pares them, and produces a list of differences between

There are already some good, albeit limited, algorithms them. The command interpreter or shell 14 allows for itera-

for code generation.'0"1l As machines become more tion, testing, selection, and execution of saved command

theoretically tractable and the theory of code generation files. Thus, regression tests can be set up with a few lines

develops further, better tools are inevitable.

of shell script, invoked like a command, and (for exam-

ple) run in the background or overnight in the case of

large tests.

Dependencies

Figure 6 shows the output of a hypothetical compiler

test. The load of a literal zero and store sequence has been

The use of program generators leads to some in- replaced by a clear. A very flexible system of regression

teresting problems in organizing programs. For example,

Figure 5 shows the dependency graph for a simple pro-

gram using Yacc and Lex. It is a nontrivial job to

remember, for example, that after changing the Yacc in-

put file, one must recompile all programs which use the

declaration file produced by Yacc, but need not rerun

Lex.

The utility program Make 12 addresses this problem; it

might be called a command generator by analogy with a

program generator. The user supplies a dependency list,

together with commands that cause the desired transfor-

mations. Whenever Make is invoked, it scans the

dependency list. If it finds a file that has been changed

later than those things which depend on it, Make invokes

the appropriate commands to bring thirigs into line.

The overhead of Make is small enough, for all but the

most trivial commands, to waste less real time than it

would take to type in the proper commands at the ter-

minal. For this reason, Make has become popular even Figure 6. Dependencies among programs using Yacc and Lex.

August 1980

of theoretical results can lead to extremely clever tools

testi: which solve the wrong problems. This last point deserves more discussion. The
test2: theoretical work in software and algorithm design is ex-

test3:

ceptionally relevant to many practical programming tasks. Even such "negative" areas as the work on NP-

< clear x

complete problems can provide a great deal of insight into the structure of practical computing problems, suggesting

interesting approaches toward their effective solution.

> load = 0

Nevertheless, behind these algorithms and results lie certain assumptions about the nature of the underlying

> store x

machine. Often, certain data structures or representa-

test4:

tions of the problem are assumed, and the behavior of the algorithm may be hopeless if these assumptions are not

DONE

met. Many times we have seen the clever program whose performance is dominated by output line printing. Much

Figure 6. Output of a hypothetical compiler test'in which the load of a literal zero and store sequence has been replaced by a clear.

simpler algorithms would have saved programming time and had equivalent performance. Many attractive algorithms depend on certain operations such as set union and intersection-being cheap; practical programs

using these algorithms may find themselves spending all

of their time allocating and garbage-collecting sets, with

tests can be established in a dozen or so lines of shell script, without requiring system programming or special privileges. This technique has proved most useful in catching compiler bugs before they infest users' code.

little or no time in the algorithm. The list could be extended indefinitely. The key point is
that the models underlying the algorithms must be understood as well as, or better than, the algorithm itself. Much of the art of tool construction consists of creating,

in software. a close enough representation of the underly-

Tooling up

ing model so that an interesting algorithm behaves as well as expected.

What is required to construct tools such as these? Since the philosophy of building and using software tools is discussed elsewhere,15 the topic will not be belabored here. Nevertheless, there is a certain amount of mystery about the process by which an algorithm in a theory textbook becomes a useful software tool.
Not only is it difficult to construct useful generalpurpose tools in the heat of a development project, but tool construction must be viewed as an investment in the future. The payoff may be big, but it will not come in the month, or even the year, in which the tool is built. For any given project, it will probably be cheaper to do the job by hand. Only when the cost is averaged over a number of projects stretching into the future does the tool become cost-effective. This fact implies a freedom from deadlines and a commitment by management to this kind of intellectual investment. We have been fortunate to enjoy this freedom during the development of the Unix system.
There is also a breadth of outlook which is very useful

We have had very poor luck "designing" tools in the traditional sense. Something which spreads across the spectrum from application and user concerns through software techniques to theory is very hard to get right the first time. We have had our best luck with a form of
iterative refinement of the tools. A first version is thrown
together and presented to the users. At this stage, it helps to make florid promises to the users about future performance improvements; in return, the users freely tell the designer why the tool solves the wrong problem and gives a lousy solution at that (while at the same time complaining that it is too slow!). With successive redesign the tool comes to solve useful problems in a reasonable time and often makes greater use of theoretical insights. For example, Yacc has been extensively rewritten over a dozen times, improving in both expressive power and perfor-
mance with each iteration. The performance, in particular, has increased over a thousandfold in the six years in which Yacc has been widely used.

in a tool-builder. Ideally, the tool builder is a user who is

also a skilled software designer, knowing a fair amount of theory on the side. A good tool usually represents a compromise among what the users need, what can be programmed in reasonable time and space, and what is theoretically possible. To ignore or weight too heavily any of the above factors is imprudent, Giving the users exactly what they say they want can lead to endless expensive projects, chartered to do impossible tasks. Skimping on resources to make a tool quickly can slow development, since the tool must first be learned and then painfully abandoned as unusable. Finally, while ignoring good algorithms can lead to terrible tools, uncritical acceptance

B-uilding a set of tools has enormous advantages,
primarily flexibility. The tools encourage modularity and are often useful outside of the context where they were defined. While theory is desirable and perhaps necessary to build good tools, it is important that escape hatches be
left for those parts of the job which don't fit the
theoretical models. Using the theory to provide insight, even when it is not yet developed enough to provide tools, seems to encourage both theoretical development and the development of tools. Finally, tool-building goes handin-hand with the general programming environment.-

20 COM PUTER

Acknowledgments
Mike Lesk has actively participated in the tool-building described in this article. Many of the ideas presented above were developed with him in earlier work;'6 My other colleagues at Bell Labs have been cheerfully but unfailingly intolerant of my dumber ideas, aiding me greatly. The cajolery of Al Aho is particularly appreciated.

13. J. W. Hunt and M. D. Mcilroy, "An Algorithm for Differential File Comparison," Computer Science Technical Report No. 41, Bell Laboratories, Murray Hill, N.J., June 1976.
14. S. R. Bourne, "UNIX Time-Sharing System: The UNIX Shell," Bell System Technical J., Vol. 57, No. 6, JulyAug. 1978, pp. 1971-1990.
15. B. W. Kernighan and P. J. Plauger, Software Tools, Addison-Wesley, Reading, Mass., 1976.

References

16. S. C. Johnsori and M. E. Lesk, "UNIX Time-Sharing System: Language Development Tools," Bell System Technical J., Vol. 57, No. 6, July-Aug. 1978, pp. 2155 -2175.

1. M. E. Lesk, "Lex-A Lexical Analyzer Generator," UNIX Programmer's Manual 2, Section 20, Jan. 1979.
2. A. V. Aho and J. D. Ullman, Principles of Compiler Design, Addison-Wesley, Reading, Mass., 1977.
3. B. W. Kernighan and D. M. Ritchie, The C Programming Language, Prentice-Hall, Englewood Cliffs, N.J., 1978.
4. D. M. Ritchie et al., "UNIX Time-Sharing System: The C Programming Language," Bell System Technical J., Vol. 57, No. 6, July.Aug. 1978, pp. 1991-2019.

Stephen C. Johnson has been employed by Bell Telephone Labs in Murray Hill, New Jersey, since 1967. There he has worked in the areas of psychometrics,-algebraic manipulation, compiler construction, program portability, and hardware design. He received his PhD in pure mathematics (category theory) from Columbia University in 1968 and is a member of the ACM i AAAS, and the Sine Nomine Singers.

5. J. F. Jarvis, "Feature Recognition in Line Drawings Using Regular Expressions," Proc. 3rd Int'l Joint Conf. Pattern Recognition, 1976.*

6. A. V. Aho and S. C. Johnson, "LR Parsing," Computing Surveys, Vol. 6, No. 2, June 1974, pp. 99-124.
7. B. W. Kernighan and L. L. Cherry, "A System for Typesetting Mathematics," Comm. ACM, Vol. 18, Mar. 1975, pp. 151-157.
8. S. C. Johnson and D. M. Ritchie, "UNIX Time-Sharing System: Portability of C Programs and the UNIX System," Bell System Technical J., Vol. 57, No. 6, JulyAug. 1978, pp. 2021-2048.
9. S. C. Johnson, "A Portable Compiler: Theory and Practice," Proc. 5th ACM Symp. Principles of Programming Languages, Jan. 1978, pp. 97-104.
10. R. Sethi and J. D. Ullman, "The Generation of Optimal Code for Arithmetic Expr.essions," J. ACM, Vol. 17, No. 4, Oct. 1970; pp. 715-728; reprinted in Compiler Techniques, B. W. Pollack, ed., Auerbach, Princeton, N.J., 1972, pp. 229-247.
11. A. V. Aho and S. C. Johnson, "Optimal Code Generation for Expression Trees," J. ACM, Vol. 23, No. 3, Mar. 1975, pp. 488-501; also in Proc. ACM Symp. on Theory of Computing, 1975, pp. 207-217.
12. S. 1. Feldman, "Make-A Program for Maintaining Computer Programs," Software-Practice & Experience, Vol. 9, Apr. 1979, pp. 255-265.

Y124MOPI1~I liEI ~~
PURCHASEE 112-24 MONTH F-ULL 36 MONTh 1
PLAN OWNERSHIP PLAN ILEASE PLAN

LA36 DECwriter S1,695 $162 $DESCRIPrION

PUPRRCIHCAESE 12 MOS. PE2R4 MMOOSN.TH 3 MOS

VVLLLLATTAAA-113131B-34042-82-00-DDEEDDCC~~CCEER~R-wwCCrTTr-rpwiirrDDttiiEEe.net6.rCtre.e6sr.IIrIccI.VVIool-lIFnp...oeeK-....rS...m.R....s....-....C-.t....r....l.-I......--122211I,,,-,,-204028999999-555555

110254 221038092 2--20

116195497900 S 4467P17050
110212 8638

DT80/1 DATAMEDIA CRT ... 1,995 191 106 72

TTTTTl11l1788786421255050 BRKPKuoDSSbrRRbtPlarPePbirrlniiMetnneettTrmeeerrorrm..yi..n..Ta..el..6r...m....i1...n...a...l.. .21211,,,,,, 551859999955555

153 214829 125130

11048165 11875

-695847 5797

AAADDDMMM34312A CCCRRRTTT TTTeeerrrmmmiiinnnaaalll

..I........ ......

21,,814759505

21318940

1471877

735293

OQUUMMEE LLeetttteerr QQuuaalliittyy RKOSR ....... 23,,829955

321768

117565

119 105

HAZELTINE 1420 CRT ....... 945 91 51

HAZELTINE 1500 CRT HAZELTINE 1552 CRT

..... .....

11,,129955

112145

6694

HHeewwlleetttt--PPaacckkaarrdd 22662211AP CCRRTT 21.,649550 124544 14820

434734 5946

'These proceedings are available from the IEEE Computer Society Publications Office, 5855 Naples Plaza, Suite 301, Long Beach, CA
90803.

August 1980

Reader Service Number 10 9

