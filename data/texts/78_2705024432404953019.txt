This research shows that the systematic organization provided by table-driven methods provides many more benefits than the ad hoc code generation techniques of the past.

Table-Driven Code Generation Susan L. Graham University of California, Berkeley

A compiler is a computer program which translates has become not only easier but more reliable. It is easier to

programs, written in a particular source programming check that a grammar is an accurate syntax description

language, into executable code for target computers of a than it is to check the implicit description embodied in the

particular design. Although the design of compilers has logic of a syntax analysis program. In addition, it is possi-

been studied for many years and many compilers have ble to prove once and for all that if the description is ac-

been written, certain design issues have not been satisfac- curate, then the table-driven syntax-processing sub-

torily solved. One of these is code generation.

routine produced by the syntax-analyzer-generator is cor-

The traditional approach to code generation has been rect.

to provide, for each kind of operator or operand in the Recently, researchers have turned their attention to the

source language, a collection of routines to produce a se- later stages of compilation. In attempting to provide tools

quence of target instructions which carry out the com- to automate code generation, they have again turped to

putation. Incorporated in the code generation task are table-driven methods. In subsequent sections, we de-

storage allocation for intermediate results, register scribe a method for generating instructions algorithmical-

management, code "optimization" (i.e., replacement of ly from tabular information about the functional proper-

longer or slower computations by equivalent shorter or ties of the target machine. This approach to code genera-

faster ones), and instruction selection. Because of the tion has many of the same advantages as table-driven syn-

complexity of the mapping from source language to target tax analysis: reliability, ease of use, and the flexibility

machine, and of the need for efficiency of various kinds, needed for a compiler-writing tool.

code synthesizers are large, complicated programs. Fur- By using a table-driven syntax analyzer, we can shift

thermore, because of the ad hoc way in which many of the analysis to a new language by giving a new grammar

these programs are written, they are difficult to debug, to the table-building program and then providing the

modify, and maintain; hence, their reliability is new table to the analyzer. Similarly, by presenting a

sometimes questionable.

description of a new target machine to the code gener-

Since compilers incorporate considerable knowledge ator's table-building program, we can hope to retarget

about the structure and meaning of both the source the code generator to produce code for the new

language and the target machine, a new compiler must be machine.

produced for each new combination of source and target. Our method depends in part on the way it is embed-

Because both languages and architectures continue to ded in a compiler. In our view, one reason that code

proliferate, there is a continuing need for new compilers. generation methods have not been improved more

Researchers have attempted to ease the work of produc- rapidly has been the lack of a modular approach to code

ing compilers by developing methods to automate com- synthesis. We will describe for the reader the assump-

piler writing.

tions we are making about the processing done by other

Computer scientists have had considerable success in parts of the compiler.

automating production of the syntax analysis portion of Since the method described here represents research in

compilers.' By using table-driven analysis methods and progress, it may be fruitful to compare it with other

programs to construct the tables from a syntax descrip- researchers' work. In particular, the investigations of

tion (usually a grammar), that aspect of compiler writing compiler-building methods being conducted at Car-

August 1980

0018 9162/80/0800-0025$00.75 9"1980 IEEE

25

negie-Mellon University,2 at IBM Research, Yorktown program must decide what methods ofrepresentation and Heights,3 at the Technical University of Munich,4 and at access will be used for source language types such as arBell Laboratories, Murray Hill,5 are providing useful in- rays, boolean values, sets, and records. Run-time support

formation about approaches to code generation and the for dynamic storage allocation and reclamation, environ-

other aspects of compilation on which this method ment switching caused by procedure and function calls or

depends. A subsequent section relates our work to these tasking, and run-time exceptional condition and error

projects.

handling must be designed. We assume that such deci-

sions have been made.

The setting

One of the code synthesis tasks is to allocate storage to program variables and constants, and to compiler-

generated entities such as intermediate values, actual

We assume that a compiler has an analysis phase, or parameters, return addresses, and the target code itself.

"front end," in which lexical and syntax analysis are per- Typically, relative addresses within data blocks, pro-

formed, type-checking and other kinds of diagnostics are gram segments, or stack frames are assigned during

done, a symbol table is constructed, and other static compilation; actual locations are chosen by a loader or

semantic actions are carried out. The analysis phase yields run-time allocator. We assume that assignment of stor-

an intermediate form of the source program in which age locations (i.e., addresses in the computer memory) is details of the external representation of the program logically separate from other aspects of code synthesis,

(comments, identifier spellings, spacing, etc.) have been and that there are compiler routines to carry out this

removed, and in which the phrase structure determined task.

by syntax analysis is apparent. The intermediate form is However, the more limited resources, e.g., processor usually either a sequence of tuples or some sort of tree registers, are a separate issue. For the present, our target

structure, sometimes termed the abstract syntax tree (Figure 1). In a one-pass compiler there may be no ex-

machine model will be an essentially sequential generalregister machine such as the PDP-l1 or the IBM 370.

plicitly constructed intermediate form; however, the We will return to the topic of machine architecture

traversal of such a form is implicit in the sequence of steps toward the end of the article.

of the syntax analyzer.

In a general-register machine, the registers are a limited

The elements of the intermediate form usually can be and computationally valuable resource; their effective

divided into operands and operators. Typically, the operands, and sometimes the operators as well, possess associated attributes such as type, print-name, or scope. This information may appear as decorations on the tree nodes, or as tuple components, or as entries in the symbol table.
The choice of representation of the intermediate form,

utilization can greatly improve the speed of a target program.* Since optimal register assignment is computationally intractable, it is conventional to use various assignment heuristics. Sometimes, certain registers have dedicated roles, perhaps for stack pointers, base addresses, or subroutine linkage. These conventions simplify, but do not resolve, the register management

while an important aspect of compiler design, is not germane to our discussion. Our concern here is the issue of what information is available to the code synthesis phase.
It is also outside the scope of this discussion to consider the algorithmic mappings chosen for the source language. For example, the compiler writer or compiler-generating

problems. Another aspect of the code synthesis problem is deter-
mining the order of the operations. In the tree shown in Figure 1, the order of evaluation is only partially specified. Both the multiplication and the division must precede the addition, and both the subscripting and the addition must precede assignment. The subscripting,

multiplication, and addition operations, however, may

source language: xlii := a * b + a/b

occur in any order. A typical compiler generates code to evaluate opera-

tions as soon as possible after they occur, that is, as soon

intermediate form:

as code has been generated to evaluate their arguments.

However, better register utilization and more efficient

target programs often can be obtained by rearranging

the order of computation. One reason why such a rear-

rangement helps is that the intermediate results of

evaluating subexpressions need not be stored in

ri-irsubscript

+
I

memory, since these results have no further use after they are used as arguments. (We will defer the issue of common subexpressions.) Consequently, it is better to evaluate subexpressions as close as possible to the use of

their values. It is also advantageous to evaluate the

arguments of an n-ary operation in decreasing order of

complexity, since each result already evaluated will be

a ba b

Figure 1. An example of the intermediate form.

*Storage requirements can also be reduced by intelligent register managenment

26 COMPUTER

"tying up" a register until it is used. Every compiler must (4) There may be complex instructions which carry

include some algorithm to order the computation.

out more than one of the operations in a particular sub-

Having decided the algorithmic mappings, allocated tree. (For instance, an "add 1 to memory location" does

the resources (except, perhaps, for the registers), and both an add and a store.) The algorithm will not find

determined the order of evaluation, the remaining task such instructions.

is the selection and generation of a sequence of instruc- (5) It may be necessary to have the operands in cer-

tions to perform the computation. Even at this stage, in- tain registers, or to have certain additional registers

struction selection is nontrivial. On virtually any ma- available. Typical examples are multiplication or divi-

chine, there is a choice of instructions to carry out the sion using register pairs, or the CDC 6000 series load

same operation. The choice may concern the location or and store conventions. If such registers are present,

addressing mode of the operands, the existence of these subgoals can be achieved by register-to-register

special-purpose instructions, or the use of alternative moves, although such moves are "extra" instructions.

operations in some situations. In addition, there is nor- (6) An operation higher in the tree may disregard part

mally a large amount of detail. The many choices for the of its operand, which consequently need not have been

many operators create a complex situation out of in- produced. For instance, a conditional branch uses the

dividually simple cases.

truth value of the associated conditional expression but

Let us assume that we are generating code from an in- perhaps not a bit pattern representing the value. In the

termediate form such as that in Figure 1. Since operators expression "a + b < 0," the sign of the sum is needed but

cannot be applied until their operands are evaluated, a its value is not. To store an operand in memory, the ad-

simple code-generation strategy might be a bottom-up, dress of the destination is used but its (previous) value is

left-to-right traversal of the tree in which, as each node not.

is visited, code is generated to evaluate it. If we assume a

general-register machine, the code for a leaf might load Many of these complications for code generation

the indicated value into a register, and the code for an could be eliminated by using a different repertoire of in-

intermediate node might carry out the operation in a structions. However, the compiler writer usually cannot

register, taking both operands from registers. If we change the target machine. Consequently, a more com-

assume a sufficient number of registers, an architecture plicated code generation strategy is needed.

in which the appropriate register-register instructions ex- We defer for the moment the first two issues. The re-

ist, and a tree whose operations correspond to single in- maining four limitations all stem from a lack of infor-

structions, that algorithm would yield correct code. mation about the context in which the local evaluation

However, as most experienced programmers will takes place. Whether an operand needs to be in a

recognize, the algorithm normally would not yield very register depends in general on the operation and the

good code. The deficiences arise from several sources: location of the other operand(s). Similarly, whether an

(1) The same computation might be repeated more than once, because so-called common subexpressions

operand must be in a particular set of registers depends on the operation. Clearly, issues (4) and (6) depend on context.

had not been detected. To simplify our discussion, The context information can be provided to the code

assume that a logically separate code optimization phase generator in various ways. One is to indicate the context

will replace all but the first use of a common subexpres- explicitly in each node of the tree. That solution will

sion with a reference to its computed value.* Assume make the tree larger, of course, and may require addi-

further that the code optimization phase pulls invariant tional computation in constructing the intermediate

computations out of loops, propagates constants, and representation.

so on. Allen7 provides a summary of typical optimiza- Another way to provide context is to abandon the no-

tions. tion of a strictly bottom-up traversal. If information is

(2) It is unrealistic to assume that there will always be propagated down the tree, then local decisions can be enough registers. The strategies for handling this prob- based on goals generated by the context. If, for exam-

lem divide into on-the-fly strategies and preplanning ple, the code generator has seen an add operator, the

strategies. In an on-the-fly strategy, a register manage- goal for its left subtree might be to ensure that the value

ment routine provides a register when requested, represented by the subtree was addressable by one of the

perhaps by storing and subsequently reloading its add instructions. The possibilities might include

previous value. A preplanning strategy inspects the com- registers, stack locations, and indexed memory loca-

putation before generating instructions and determines tions. From another point of view, we could regard the

which intermediate values are to be stored in memory or code generator as being in a certain state in which code

recomputed, in order that the supply of registers not run for a particular node is generated. Using this approach,

out. we can propagate context as the traversal takes place

(3) It may be possible to evaluate some of the rather than altering the representation in advance.

operators without having the operands in registers or

leaving the results in a register. Such computation may

avoid load instructions and reduce the number of Top-down algorithms

registers in use. The strategy outlined above precludes

such instructions.

Continuing to defer the issues of code optimization and

*This assumption is really an oversimplification of the problem.6

register allocation, let us consider generating code for the

August 1980

27

intermediate tree in Figure 2. The tree represents the Pascal statement
A Bt.Y + C, where A and C are integer variables, B is a pointer to a record with an integer field Y, t is the dereferencing operator, and = is the assignment operator. The nodes of the tree are numbered for reference. We use a low-level intermediate language in which the storage mapping is explicit. Thus, r2 designates the display register (stack pointer) and a,b,c the relative addresses for the usual stack implementation of a language supporting recursion.* In the intermediate form, the distinction between addresses and values is explicit; the symbol t again designates the dereferencing, or "contents of," operator.
Figure 3 shows a simplified set of instructions for a hypothetical computer. It gives the assembly language form of each instruction; a tree illustrates the computation carried out by each instruction. For instructions which leave their results in registers, such as "add," a store operation is not explicit in the tree. Rather, the location of the result is indicated by a bracketed label at the root. Thus, the meaning of = is "store the value described by the right subtree in the memory location described by the left subtree." By convention, the first operand in the assembly language instruction is the destination for loads and adds and the source for stores. The constants are all integers. The notation -const denotes a literal value (rather than an address).
If the operation symbols used in the intermediat'e form have the same meanings as they have in describing the instructions, we can use a top-down traversal of the
*We assume that r2 is a dedicated register assigned by an earlier stage of the
compilation.

tree in Figure 2 to derive context information with which to select instructions from Figure 3. By assuming that the meanings of the symbols are the same, we can reduce the search for instructions to symbolic pattern matching.
The algorithm would proceed as follows. Starting at the root in Figure 2, we will need to generate an instruction to compute the ": =" operation, namely, a store instruction. As subgoals, the instructions which compute a + r2 must leave that number (an address) in a form usable by one of the store instructions, i.e., in a form which matches a left subtree of one of the store instructions. Also, the instructions to compute "t(t(b + r2) + y) + t(c + r2)" must leave the result in a register. By continuing the traversal, we discover that "a +r2" already
STORE INSTRUCTIONS

F

reg const reg reg

reg

const reg store reg, const(reg)

store reg,const

store reg O(reg)

LOAD INSTRUCTIONS (Q ]CONTAINS DESTINATION)

const [reg]

t [reg]

reg [reg]

= (1)

-I

const reg

+ (2) + (5)

load reg, = const load reg,const(reg) move reg,reg

FI-I-,~~~~~~~I~. ~~~~~~~~~~~~A~D~D I~NS~TR~UC~TI~ON~S~~~~~~~~~~~~~~~~~~~~~~~~~~

a (3) r2 (4) t (6)

t (13)

+ [reg]

+ [reg]

+ [reg]

Fl',+ (7) + (14)

rliH,rH

const reg t

reg reg

reg

t (8) y(12) c(15) r2(16)

+ (9)
F-Ill
b (10) r2 (11)
Figure 2. Input to code generator. 28

const reg add reg,=const ,add reg,const(reg)
Figure 3. Instruction descriptions.

add reg,reg COMPUTER

matches the left subtree of the first store instruction, since a is a constant and r2 is a register. In addition, we might discover that we could generate the first add instruction and then use the third store instruction. However, the register management routine presumably would rule that out because it would destroy r2, the display pointer. Another possibility is to use the first load instruction followed by the third add instruction to put the value of "a+ r2" in some other register.
The next subgoal is to generate instructions which compute the right subtree of ": =" in Figure 2, leaving the result in a register. We examine all the instructions which put their results in registers. Since node 5, the root of the right subtree of ":=", is a +, we choose as subgoals at node 5 the add instructions, all of which leave their result in a register.
In order to use an add instruction, the subgoals at nodes 6 and 13 are to generate instructions to compute the arguments of +, leaving the results in a form (actually an addressing mode) matching one argument of the add instructions. Since add is commutative, either subtree can match either argument, as long as the combination matches an instruction.
Looking first at the left subtree, we have two choices (i.e., two subgoals) since node 6 is a t. The first possibility is to use the second add instruction by matching its left subtree. The second possibility is to generate instructions which leave the value of the subtree rooted at node 6 in a register. (We can rule out a subgoal destination "const" by observing that no instruction yields a compile-time constant as its destination.) Further inspection shows that there is an instruction which leaves its result in a register and also does an "t" operation, namely, the second load operation.
Moving to node 7, we find that both candidate instructions (the second load and the second add) continue to match. At node 8, by noticing that node 12 is a constant and again appealing to the commutativity of " +", we again have as a subgoal an instruction which computes t and leaves its value in a register. The second load instruction is now the only possibility and matches the remaining subtree. At this point, we generate the load which computes t(b + r2) and puts the result in a register (this register, which we will call r3, is provided by a register allocator). That instruction computes the subtree rooted at node 8. Returning to node 6, we can either generate a load instruction to compute t(r3 + y), leaving open the possibility of using any of the add instructions, or indicate a match of the first subtree of the second add instruction. In the former case, the corresponding subgoal at node 13 is any operand of an add instruction; in the latter case, the subgoal is a register. In the remainder of the traversal, we discover that the subtree for t(c + r2) rooted at node 13 matches the first subtree of the second add instruction and that it can also be computed by the second load instruction, leaving the result in a register.
Thus, if we disregard other possibilities for the assignment destination, the algorithm finds three sequences of instructions (Figure 4).*
The first instruction is the same in all three sequences. That was apparent when the instruction was generated,

since there was a single subgoal. A choice at node 6 to generate the load instead of "working on" the second add instruction yields both the worst sequence and the best. The third sequence is worse than the others because it has an additional instruction. When the third load is generated, it is clear from the subgoal information that the add could be performed without the load. Consequently the load need never be generated. The
first sequence is better than the second because it has the same set of instructions but uses fewer registers. However, that is true only because the second argument of the addition can be evaluated without using a register, a fact not known until the second operand is scanned.
We will return to this example. Suppose we wish to have a code generator which, for
simplicity and efficiency, fixes the order of traversal and chooses instructions as it finds them rather than summarizing all the possibilities first. There must then be a strategy for making choices on-the-fly. One plausible heuristic is not to generate an instruction if it can be avoided. In the example, when the traversal returns to the "t" at node 6, having traversed its subtree, there is a choice between generating the load or continuing to "work on" the second add instruction. Applying the heuristic would cause the load to be rejected.
No matter what heuristic is used, an algorithm of this kind may block, i.e., prevent possible choices at some later stage because of an earlier choice. If the second operand of the second add instruction in Figure 3 were a constant, for example, the algorithm would block unless the load had been chosen at node 6. Note that the load would be a safe choice in this example, since it would leave open all possibilities for the second operand to add. In general, however, there need not be a safe choice.
There are several ways to deal with this difficulty. One is to modify the algorithm so that if it blocks, it goes back and makes a previous choice differently. This is essentially a backtracking solution. Another possibility is to find a sufficient condition under which blocking
*We also disregard other possible sequences by reordering the same instructions, although this approach could generate them. Note that there would be many more possibilities if the instruction set had a nonindexed load.

load r3, b(r2) {subtree rooted at node 8} load r3, y(r3) {root at node 6} add r3, c(r2) {root at node 5} store r3, a(r2) {root at node 1 }

load r3, b(r2) {subtree rooted at node 8}

load r4, c(r2) {root at node 13}

add store

r4, r4,

y(r3) a(r2)

{root at node 5} {root at node 11

load r3, b(r2) {subtree rooted at node 8} load r3, y(r3) {root at node 6} load r4, c(r3) {root at node 13}
add r3, r4 {root at node 5} store r3, a(r2) {root at node 1

Figure 4. Possible instruction sequences.

August 1980

29

cannot occur for a given instruction set, and to use the algorithm only when that condition is satisfied. Another

reg.1 ::= + t + const.1 reg.2 add reg.1, const.l (reg.2)

possibility is to identify those situations in which block- This description carries more information than we

ing is possible and to either make a safe choice or use more information (for example, a traversal of subsequent subtrees, i.e., lookahead) to make the decision.

saw in Figure 3. The tree describing the computation has

b":e:en="r,epwlahciecdh

by the sequence
correspond to a

of nodes to the prefix tree walk.

right The

of in-

struction destination is to the left of "::-". The

A top-down deterministic algorithm

qualifications ".1" and ".2" on "const" and "reg" serve two purposes-to show which quantities in the in-

The preceding discussion motivates the method of code generation we have been studying.8-'0 In collaboration with R. S. Glanville, we have developed a code generation algorithm which traverses a sequence of trees representing the low-level intermediate form of a program. The traversal is depth-first, left-to-right, without backup, i.e., a prefix walk of the tree." Since there is no backup, each tree can be represented by a linear sequence of nodes in prefix form, i.e., with each operator (interior node) preceding its operands (subtrees). For instance, the nodes in the example in Figure 2 would appear in the order in which they are labeled.*
Conceptually, the choices made by the algorithm occur after a subtree has been traversed. A decision is made either to generate an instruction to compute the subtree (and, if so, which instruction to generate) or to incorporate the subtree computation in a larger subtree of which it is a part. In the current implementation, the first choice is always to avoid generating an instruction (by using an instruction for a larger subtree) and otherwise to choose the "best" instruction. The best instruction is the one which is cheapest by whatever measure. Thus, in the example the choice for the left subtree of ":#" would be to incorporate the computation in the first store instruction (rather than loading "a" and doing a register-register add)> The load for the subtree rooted at node 8 would be generated (since there is no choice). The load at node 6 would not be generated; consequently, a load would be generated at node 13, yielding the second code sequence.
Notice that if the right subtree of node 5 were traversed before the left subtree, the first code sequence in Figure 4 would be generated. Thus, as one would expect, the order of traversal can affect the quality of the code that is produced, since instructions are generated on-the-fly. A preplanning strategy for this sort of code generator would be to choose the appropriate traversal order and provide the corresponding sequence of nodes as input to the code generator.
The primary reasons for using a left-to-right, no backup traversal and a simple rule for making choices are efficiency and automation. The process described above-determining subgoals, seeing which subgoals can be satisfied, and then choosing among them-can be carried out by a table-driven algorithm and an automatically constructed table.
The table construction program takes as input a description of the target machine. We do not give a detailed description here. As an example, the second

struction tree correspond to which ones in the assembly

language instruction, and to enforce the rule that repeti-

tions of the same qualification on the same kind of node

designate the same quantity. Because of the latter, the

destination register is the same as the second argument

register. Other parts of the machine description would

indicate that the operator + is commutative, that the

possible that the

constants are integers registers are designated

in in

a a

certain range, particular way

and
(RO,

RI, R2, etc.). Other kinds of qualification are also

possible. For example, an instruction may incorporate a

particular constant such as 1 or a particular register such

as RO, may require that two registers be different, or

may use register pairs.

Although most instruction descriptions correspond to

single machine instructions, it is also possible to have a

description with no accompanying instruction (to record

a change of state in the code generator) or one with a se-

quence of instructions. For instance, an instruction

description could be associated with an operation of the

intermediate language that was not directly represented

in the hardware of a given target machine.

The resemblance of the instruction description to a

context-free grammar rule is more than coincidental.

Many of the ideas behind the table construction and in-

struction selection algorithms are drawn from LR pars-

ing,1 although the details differ significantly. The

subgoal generation and pattern matching described

previously are carried out as a form of syntax analysis.

The ambiguity stemming from choosing among matches

is resolved by fixed rules built into the table construc-

tion. Therefore, there is no search or tree walk.

The instruction descriptions could include instruc-

tions having identical computation trees. (They would

haanveadddiffienrsitnrgucqtuiaolnifaicnadtiaonns,inhcorweemvneenrt.)inFsitgruurceti5onshwoiwtsh

similar trees. The increment instruction is a constrained

form of add in which the constant has the value 1.

The qualifications on the instructions are checked

after an instruction match has occurred, in a fashion

analogous to compiler semantic rules (although these

"semantics" are automatically generated). If there is

more than one rule with the same computation tree (ex-

cept for qualifications), the rules are ordered by some

cost measure. If that tree is selected, the various sets of

qualifications are checked in the determined order and

the first instruction whose qualifications are satisfied is

generated. The table

generator

ensures

that

for

each

syntactic

in-

add instruction of Figure 3 would be described as

struction tree there is some unrestricted instruction or

*The return to handled by the

an interior node after traversing
code generator internally and need

one not

of its subtrees is be reflected in the

input sequence.

sequence of instructions to generate. For example, the add instruction in Figure 5 is unrestricted, since *he qualifications only indicate fields in the assembly

30 COMPUTER

language instruction. The absence of an unrestricted sequence would constitute another kind of blocking.
The table generator checks the machine description for a sufficient condition which, if satisfied, guarantees that blocking cannot occur. At present, in the rare case that the no-block condition fails, no code generator is produced. However, in this case it would also be possible to have instead a backtracking or lookahead version of the code generator.
In addition to their speed, the table generator and table-driven code generator have several other advantages.
First, we have proven that the code generator never loops and always produces correct code for wellformed input., By "correct code" we mean correct as long as the machine description is accurate. Input is well-formed if it corresponds to a well-formed tree; i.e., the operators have the correct number of operands of the proper kinds. By analyzing the instruction set, the table generator can determine the conditions satisfied by well-formed input. These conditions are easy to express and check. The code generator can easily check, as the input is read, that it is well-formed. Alternatively, the implementer can ensure that the compiler "front end" generates only well-formed input.
Second, due to the nature of the search process, the code generator considers all the instructions. It will exploit addressing hardware and special-purpose instructions when the opportunity arises. It can also find and use machine "idioms" by describing them as pseudoinstructions. For example, multiplication by a power of two can be a description of a shift instruction on some machines.
Third, the code generator appears to be very fast. The quality of the code it produces is very good, given the amount of information available about subsequent input (i.e., the algorithm generates locally good code and finds the more powerful special-purpose instructions).
One can use such an algorithm during compiler development. Initially, the code generator can be incorporated without any preplanning or optimization. A strategy such as that described by Ammann'2 can be used to allocate registers and to choose which ones to spill if there are not enough.
The compiler phase, which produces input to the code generator, can be modified or rewritten to provide optimization, expression reordering, or preplanning. Those modifications might add auxiliary information to the code generator input, such as common expression identification or usage counts, but would not greatly change the code generator. Meanwhile, the code generated without the benefit of the code improvement techniques under development can provide valuable feedback about what sort of code improvements to try to obtain.
Compilers using these code generation techniques can be retargeted in a reasonably straightforward way. Retargeting has several aspects. Changing the code generator to generate instructions for the new machine can be accomplished by writing a new machine description, running the table generator to produce a new table for the code generator, and replacing the old table with

the new. The other part of retargeting-changing the source-to-target mappings to reflect the semantics of the source language and to exploit the architecture-involves modification to earlier phases of the compiler.*
It has been our experience that an implementer using a machine manual can easily write new instruction descriptions. Hence, although one could develop a program which generates the descriptions automatically from a lower-level representation such as ISP,13 we have not found the need for such a tool.
The portable C compiler
The portable C compiler5 is a translator for the Unix** system's programming language C. It was written by S. C. Johnson with the goals that it be easily retargetable and provide code of reasonable quality. It has been retargeted successfully on over a dozen machines.
The compiler uses a table-driven code generation method somewhat different from the method discussed above. A preplanning strategy is used to avoid having the code generator run out of registers. So-called SethiUllman numbers'4 are assigned to the nodes of expression trees to determine which temporary values should be stored. Using those numbers, the compiler generates code for the subtrees rooted at nodes representing stored temporary values before it generates code for the expressions which use those values. Thus, the code generator has as input a tree for which it will not run out of registers.
The code generator uses a table containing a collection of templates. In simplified form, each template contains a pattern to search for in the input tree; some subgoals (i.e., destinations) attained by the code associated with the template; resources, such as registers, used by the associated code; a rewriting rule for the input tree; and an encoded form of the assembly language instructions to be generated. The code generator is driven by a template-matching algorithm which attempts to find a template and an associated goal that matches a portion of the input tree and its associated goal. When a match is found, code is generated and the tree is transformed to reflect the computation.
*For example, subprogram linkage conventions may have to be redesigned. **Unix is a trademark of Bell Laboratories.

ADD INSTRUCTION
+ [regi1]

INCREMENT INSTRUCTION
+ [regi]

const. 1

reg.1

add reg. 1, = const. 1

const = 1

reg.1

incr reg.1

Figure 5. Similar instruction trees.

August 1980

31

Unlike our algorithm, in which an efficient matching process is obtained by preprocessing the instruction descriptions and by requiring that they satisfy a sufficient condition, the portable C compiler uses a more general heuristic search mechanism over a more general class of templates. The templates are written by the implementer rather than being generated automatically. Both the quality of the generated code and the speed of the code generator can be significantly affected by choices in the design of the templates.
The added generality appears to be both a strength and a weakness. The primary strength lies in the handling of situations which might be problematical in our more restricted setting. The weaknesses are slow searching and no other way than the usual compiler testing to ensure that correct code is generated or that the code generator will not block. One can prevent blocking by adding a template to cover the case in question, but the missing template is not discovered unless the input that needs it is provided.
In this code generator as well as in our own, there is a considerable benefit because the information about the target machine is contained in a table or data base rather than being more extensively scattered throughout the compiler code. Not only does such an organization facilitate retargeting, but it also has made it much easier for implementers other than Johnson to understand, modify, and debug versions of the compiler.
Code generation in POCC
The Production-Quality Compiler-Compiler project described elsewhere in this issue (pp. 38-49) is an attempt to design a compiler-generator system which yields compilers capable of generating very good code. Its approach to code generation'5 has many similarities to our own.
PQCC code generation is, again, table-driven and, like the portable C compiler, relies on a preplanning strategy.
The phases prior to actual code generation, notably DELAY and TNBIND, go through a pseudocode genera-
tion process to determine addressing modes and to allocate temporary storage and registers.
Like our generator, the PQCC code generator uses trees as patterns. The patterns, which are part of productions, are derived by a code-generator-generator'6"'7 from the production-like input/output assertions describing the instructions. As in the portable C compiler, generation is carried out by a goal-directed heuristic search which matches pattern trees to the tree input to the code generator. A match may cause the generation of more than one instruction. In the case of more than one match, a cost criterion is applied.
The code-generator-generator-CGG-is similar to our table generator. The input/output assertions describing each machine instruction are analogous to our instruction descriptions, except that there can be more than one assertion for a given instruction (often indicating a side effect).
One of the tasks of the CGG is to produce a separate code generator pattern for each effect of the instruction, producing extra actions to compensate if necessary. An

example from Cattell'7 is, given an instruction that stores into both memory and a register, to describe the register store alone and precede the generated instruction with allocation of a dummy location. In our method, the separate descriptions would be provided by the implementer.
The other patterns constructed by the CGG augment the input/output assertions by means of patterns which ensure that there are
* productions to transfer between every pair of addressing modes,
* at least one production for each operator of the PQCC intermediate language TCOL, and
* productions for the control operators such as whiledo and if-then-else.
The new productions are derived by running the code generation algorithm on "built-in" input trees that describe the situations to be covered, and by applying axioms to exploit the properties of the TCOL operations.
It is interesting to contrast the design of the CGG with that of our table generator. Our test for blocking automatically ensures that it is possible to generate code for all the necessary changes of access mode. It Would be
easy for the implementer to use an instruction descrip-
tion to describe each transfer requiring more than one machine instruction, but such cases occur rarely in practice.
We have not designed a fixed set of operations for an intermediate language. In using a particular machine description and set of intermediate operations, it is easy to check whether each operation occurs in an instruction
description. The blocking test then determines whether the possible operands are general enough. The im-
plementer must supply the required additional descriptions, which might correspond to more than one actual instruction. It also would be possible to add the control
descriptions, although our present formulation assumes that the intermediate language is already at the level of labels and jumps.
WVe have described three different forms of table-
driven code generation. In our presentation we have focused on their similarities rather than their differences. One of the chief differences is the role played by the implementer in designing and changing code generators.
The portable C compiler demands the most from the implementer, who must specify the template tables and ensure that their interaction with the code generator is correct. This design is the least automated of the tliree but, because it has yielded a large number of working compilers, is the most extensively validated.
The Production-Quality Compiler-Compiler demands the least from the implementer. The TCOL input language has been specified and is intended to be sourcelanguage-independent. The code-generator-generator has complete responsibility for devising a code generator from an automatically generated machine description.
All aspects of the TCOL/code generator interface are

August 1980

33

handled by the CGG; in particular, the implementer provides no information about how the instructions might best correspond to the intermediate language.
Our approach lies somewhere between these extremes. We have left it to the implementer to provide the instruction descriptions in a way that has proven to be quite easy. The table is produced automatically from those descriptions and is used by an algorithm which generates locally good, correct code. However, the implementer can incorporate special knowledge about uses of the instructions. The table generator computes from the machine description the simple properties that the input to the code generator must satisfy. The implementer can fix discrepancies in the interface either by adding instruction descriptions or by modifying an earlier stage of the compiler. The implementer could exploit a hardware multidimensional array-indexing instruction, for example, either by introducing a high-level indexing operator used both in the intermediate language and in the instruction description, or by providing a low-level description of the instruction. The former solution would be more efficient; the latter would be more general. Both could coexist in the code generator, of course.
In making decisions which trade generality for efficiency, we have been more willing than the others to sacrifice potential loss of generality for gains in simplici-
ty and efficiency. In part, oi&r motive is to see how far
we can push this approach. However, our design goals also stem from a belief that it should not be necessary to design large complex code generators to produce highquality code for baroque instruction sets. If we can identify the properties that make an instruction set a natural target for translation of high-level languages, we can provide useful information-beyond mere intuition-on which computer architects can base their designs.
The table-driven approach to code generation appears feasible and worthwhile. However, more work needs to be done. Our methods and those of other researchers can probably be integrated into an even better system. Controlled experiments are needed to identify the remaining problems. Nevertheless, it is already clear that the systematic and modular organization provided by table-driven methods is far better than the ad hoc code generation techniques of the past.E

2. B. W. Leverett et al., "An Overview of the Production Quality Compiler-Compiler Project," Tech. Report CMU-CS-79-105, Carnegie-Mellon University, Feb. 1979.

3. F. E. Allen et al., "The Experimental Compiling Systems Project," Report RC6718, Computer Sciences Dept., IBM Thomas J. Watson Research Center, Yorktown Heights, N.Y., Sept. 1977.

4. "Introduction to the Compiler Generating System MUG2," Report TUM-Info 7913, Institut fur Informatik, Technische Universitat Muinschen, May 1979.

5. S. C. Johnson, "A Portable Compiler: Theory and Practice," Conf. Record 5th Ann. ACM Symp. Principles of Programming Languages, Jan. 1978.

6. A. V. Aho, S. C. Johnson, and J. D. Ullman, "Code Generation for Expressions with Common Subexpressions," J. ACM, Vol. 24, No. 1, Jan. 1977, pp. 146-160.

7. F. E. Allen and J. Cocke, "A Catalog of Optimizing Transformations," in Design and Optimization of Compilers, Prentice-Hall, Englewood Cliffs, N.J., 1972.

8. R. S. Glanville, "A Machine Independent Algorithm for

Code PhD

Generation and Its dissertation, Univ.

Use of

iCnalRieftoarrngieat,abBleerkCeolmepyi,leDresc,."

1977.

9. R. S. Glansville and S. L. Graham, "A New Method for Compiler Code Generation," Conf. Record Fifth ACM Symp. Principles ofProgramming Languages, Jan. 1978.

10. S. L. Graham and R. S. Glanville, "The Use of a Machine Description for Compiler Code Generation," Proc. Third Jerusalem Conf. Information Technology, North Holland Pub. Co., Aug. 1978.

11. D. E. Knuth, The Art of Computer Programming, Volume I-Fundamental Algorithms, Addison-Wesley, Reading, Mass., 1968.

12. Upi.leAr,m"mSaonfnt,war"eO-nPrCaocdteicGeen&erEaxtpieornieinncea,PVAoSl.CA7,LNCoo.m3-, June/July 1977, pp. 391-423.

13. C. G. Bell and A. Newell, Computer Structures: Readings and Examples, McGraw-Hill, New York, 1971.

14. R. Sethi and J. D. Ullman, "The Generation of Optimal Code for Arithmetic Expressions," J. ACM, Vol. 17, No. 4, Oct. 1970, pp. 715-728.

15. R. G. G. Cattell, J. M. Newcomer, and B. W. Leverett, "Code Generation in a Machine-Independent Compiler,"
ACM Sigplan Symp. Compiler Construction, Boulder, Colo., Aug. 1979.

16. R. G. G. Cattell, "Formalization and Automatic Derivation of Code Generators," PhD dissertation, CarnegieMellon Univ., Pittsburgh, Pa., Apr. 1978.

17. R. G. G. Cattell, "Automatic Derivation of Code Generators from Machine Descriptions," ACM Trans. Programming Languages and Systems, Vol. 2, No. 2, Apr. 1980.

Acknowledgment

We did much of our recent work on code generation in collaboration with Robert Henry, who provided helpful
comments during the preparation ofthis article. This article is based on research sponsored by the National Science Foundation, under grant MCS74-07644-A04.
References
1. A. V. Aho and J.D. Ullman, Principles of Compiler Design, Addison-Wesley, Reading, Mass., 1977.

Susan L. Graham is an associate professor

Coaflicfoomrpnuiat,erBesrckieelnecye.

at the University of
Her research inter-

ests include programming language im-

plementation and design and the design of

programming tools. Graham received the

AB degree in mathematics from Harvard

University and the MS and PhD degrees in

computer science from Stanford University. A member of ACM and the IEEE Com-

puter Society, she is editor-in-chief of the A CM Transactions on

Programming Languages and Systems.

34 COMPUTER

