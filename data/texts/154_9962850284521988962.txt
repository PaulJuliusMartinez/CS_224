[In Proceedings of Workshop on Models versus Exemplars in Computer Vision, CVPR 2001]

Combining Models and Exemplars for Face Recognition: An Illuminating Example

Terence Sim

Takeo Kanade

Robotics Institute

Carnegie Mellon University

Pittsburgh, PA 15213

Abstract
We propose a model- and exemplar-based approach for face recognition. This problem has been previously tackled using either models or exemplars, with limited success. Our idea uses models to synthesize many more exemplars, which are then used in the learning stage of a face recognition system. To demonstrate this, we develop a statistical shape-fromshading model to recover face shape from a single image, and to synthesize the same face under new illumination. We then use this to build a simple and fast classifier that was not possible before because of a lack of training data.
Keywords: model-based, exemplar-based, shape-fromshading, face recognition, eigenspace
1. Introduction
The task of automatic face recognition has been actively researched in recent years, with researchers employing both model-based and exemplar-based techniques. Although great strides have been made after almost three decades, the task remains unsolved in general. Current systems work very well whenever the task image to be recognized is captured under conditions similar to those of the training images. However, they are not robust enough if there is variation between task and training images [14]. Changes in incident illumination, head pose, facial expression, hairstyle (including facial hair), cosmetics (including eyewear) and age, all confound the best systems today.
As a general rule, we may categorize approaches used to cope with variation in appearance into three kinds: invariant features, canonical forms, and variation-modelling. The first approach seeks to utilize features that are invariant to the changes being studied. For instance, the Quotient Image [15] is (by construction) invariant to illumination and may be used to recognize faces (assumed to be Lambertian) when lighting conditions change. The second approach attempts to "normalize" away the variation, either

by clever image transformations or by synthesizing a new image (from the given task image) in some "canonical" or "prototypical" form. Recognition is then performed using this canonical form. Examples of this approach include [23, 24]. In [23], for instance, the task image under arbitrary illumination is re-rendered under frontal illumination, and then compared against other frontally-illuminated prototypes. The third approach of variation-modelling is selfexplanatory: the idea is to learn, in some suitable subspace, the extent of the variation in that space. This usually leads to some parameterization of the subspace(s). Recognition is then performed by choosing the subspace closest to the task image, after the latter has been appropriately mapped. In effect, the recognition step recovers the variation (e.g. pose estimation) as well as the identity of the person. For examples of this technique, please see [6, 8, 9, 21].
Despite the plethora of techniques, and the valiant effort of many researchers, face recognition remains a difficult, unsolved problem in general. While each of the above approaches works well for the specific variation being studied, performance degrades rapidly when other variations are present. For instance, a feature invariant to illumination works well as long as pose or facial expression remains constant, but fails to be invariant when pose or expression is changed. This is not a problem for some applications, such as controlling access to a secured room, since both the training and task images may be captured under similar conditions. However, for general, unconstrained recognition, none of these techniques are robust enough. Moreover, it is not clear that different techniques can be combined to overcome each other's limitations. Some techniques, by their very nature, exclude others. For example, the Symmetric Shape-from-Shading method of [23] relies on the approximate symmetry of a frontal face. It is unclear how this may be combined with a technique that depends on side profiles, where the symmetry is absent.
We can make two important observations after surveying the research literature: (1) There does not appear to be any

feature, set of features, or subspace, that is simultaneously invariant to all the variations that a face image may exhibit. (2) Given more training images, almost any technique will perform better. These two factors are the major reasons why face recognition is not widely used in real-world applications. The fact is that for many applications, it is usual to require the ability to recognize faces under different variations, even when training images are severely limited.
Another way to categorize face recognition techniques is to consider whether they are based on models or exemplars. Models are used in [15] to compute the Quotient Image, and in [6] to derive their Active Appearance Model. These models capture prior class information (the class of faces), and provide strong constraints when dealing with appearance variation. At the other extreme, exemplars may also be used for recognition. The ARENA method in [18] simply stores all training images and matches each one against the task image. Obviously, scalability is an issue. However, the visitor identification system reported in [19] that uses ARENA appears to naturally handle illumination changes, as exemplars under new lighting conditions are added to the system. Some subspace-methods are also based on exemplars. For example, the eigenface approach in [20] and the Fisherface method in [3] embed exemplars in their respective subspaces, and recognition is performed via nearestneighbor classification in these subspaces.
As far as we can tell, current methods that employ models do not use exemplars, and vice versa. This is surprising, since these two approaches are by no means mutually exclusive, and we can leverage the strengths of each to better solve the problem. In this paper, we propose a way of combining models and exemplars for face recognition. We will use models to synthesize additional training images, which can then be used as exemplars in the learning stage of a face recognition system. To illustrate this, we develop a model that recovers the shape of a face from a single image under an arbitrary but unknown illumination, and that re-renders the same face under new illumination [17]. We then synthesize many more images to be used for learning in a simple eigenface classifier.
2. Shape-from-Shading Model
We begin by developing a model that allows us to recover the face shape, in terms of surface normals, from a single image. We assume that the image is taken under a single point light source at infinity. The direction of the light source is unknown. This shape-from-shading problem has had a long history [10], and is in general underconstrained: many 3D shapes give rise to the same 2D image. To overcome this, we build a statistical model to guide the shaperecovery process. Similar work is reported in [23, 7, 8], but we differ in a few key areas. Unlike [23], we do not ex-

plicitly depend on face symmetry, so our method will work for other non-frontal poses too. And unlike [8], we do not require multiple images to be available. We work with as few as one image, which means we cannot use photometric stereo. In [7], a method is also presented to recover surface normals from a single image, using symmetry and integrability constraints. By contrast, both these contraints are implicit in our statistical model, rather than explicit. We argue that this affords us greater latitude to use our method for other non-symmetric objects, or even to exploit other symmetries in our statistical model which may not be readily apparent. Another difference is that we do not require faces to be strictly Lambertian; instead, we model the nonLambertian component statistically. In terms of rendering the recovered face under new illumination, our technique is similar to the Quotient Image of [15] in that we also rely on class-based information. In their case, it takes the form of face images taken under 3 linearly independent illumination directions. In ours, the class-based information is a statistical model of the variation of surface normals from person to person, and of the non-Lambertian component of surface reflection. Our statistical model is explained in detail in [17], but for completeness we will describe it here.
2.1. Augmented Lambertian equation
At the heart of our method is the following equation, which is the standard Lambertian equation [11] augmented with an additive term. This is done because the standard Lambertian equation does not handle shadows nor specular reflections, which occur naturally in face images. The augmented model is then:

 ¢¤¡ £¦¨¥ §©¡¤£ ¥¨

(1)

£

w !h#ic" h says that at pixel position , the pixel intensity,

, cluding

'iaslb(ree"dlaot% e) dattothdaot tppixreold,u©$ct#of"&)th% 0e, as" unrdfatchee

normal (insingle light

source,

, plus an error term

. The purpose of

this error term is to model shadows and specular reflections,

without explicitly reco1ve2§ rin3 g4¡ £6th57e8¥ full 3D shape (depth) of

tehnecefabceet.wIteiesncl  eaanrdth©at



.

, since it is just the differWe assume that surface normals

eartrdo3ir¡¤f@ftee75 rr8em¥ nthpoiwxeelvsera,rewienadse@spuemndeetnhtaot f3o¡¤£9n57e8 a¥ niostihne3 dr¡4.e£6pF5eoA n¥ rdtehnet of for other pixels ,A but correlated with under some other illumination . This assumption allows us to

synthesize more realistic images while remaining computa-

tionally tractable. More details will be given in Section 2.2.

eNqoutaetitohnatfoErqeuvaetiroynpi(x1e)lipsorseiatilolyna£&s§CystDB e5FmEGEGoEHf57eI qiunatthioenims, aognee.

Note also that unlike [15], our model does not assume that

every person has the same face shape, nor constant albedo,

Figure 1: Two Yale faces under the same set of four illumination directions.

but allows both to vary from pixel to pixel and from person

to person.

I

Given only a single underconstrained: the© re

-apreix PelIQinpRutuinmkangoew, nEsq (utahteiocno(m1p) oiI s-

nents of the vectors and , and the scalars ) but only

equations. MoreovTR erUS, thR ere is an inherent ambiguity, since

we ma©y to get



inW seX§ rt a© n y¡

YV !V `Ha

i¥nWveb§ rti4¡b© le

ma¥ct¡ rix V
V V!H` a

8 a¥dn§fd it© e s

ine verse . We

will use knowledge about the class of faces in general to

saosltvaetisftoircatlhemuondkelnofowrn©s.4¡ ¦£ M¥ aonrde

3sp¡4£6ec57i8 fi¥ cfarlolym,

we will a set of

learn boot-

strap images. This is the Yale dataset [8], comprising 15

people each taken under roughly 60 known illumination di-

rections (Figure 1). Some basic preprocessing was done: all

faces were aligned to an arbitrarily chosen reference face by

manually marking 3 points (the centers of the eyes, and the

base of the nose) and performing an affine warp; the gray-

scale images wereS then cropped to remove the background and scaled to 120 100 pixels.

2.2. Learning the statistical model

Our statistical©m¤¡ ¦£ o¥del isgto4¡ £6le5hai r¥ n the probability density func-

tion (pdf) for and

. We assume that the pdf's are

Gaussian distributions of unknown means and covariances,

which we will estimate using maximum likelihood. This

turns out to be the sample mean and sample covariance,

cp omputITabS)leq from the bootstrap images. MoreI precisely, let abAimneldsbaaogelyR,eeatlsSe ttI a kbmeebanaetruIaimnx¨S1R dawteq S rhrixoqimlslwueamthmcraooiixtsnlrueioaxmtfcioootnhlfnsuetamdherierenrreisotlchlratueirtomeevnretimshncetasr o.tFirTo8ss-8tndhr swuied©mv ni4¡are¦£fe.noc¢¥ Lsrttiieeootavnncx asa hl.,

person in the bootstrap set, we compute the least-squares

solution for x and  as follows:

p x
and 

§  x 
§ ¡  ¥ p   H` a 
§ Cp   x



(2)

¡¤£ ¥

From this we compute the samp¡¤le¦£ ¥ mean ©ve¤¡ £ ct¥ or 9

a3 n¤¡ 6£d5h8sa¥ mple covariance matrix  for . S¤¡ £9in57c8e¥

is a scalar, we co¡46£ m75 p8u¥ te the sample mean  

and sample variance 6

. To produce better syn7s thetic

imagesg , 4¡ w£6e5h as l¥so com3¡4p£6u75te  th¥ e correlation coefficient  be-

t3w¡¤£6ee5hn s ¥

and 3 ¤¡ £9an57d ¥

. Statistically, we are modelling as a7s  jointly Gaussian distribution, with

correlation coefficient  . We will make use of this in Sec-

tion 3.4.

3. Using the Model

Having learned the statistical model, we make use of it in

the following algorithm:


1. Given an image, estimate the unknown illumination

(which may be different from those in the bootstrap

set).

2. Compute 9 ¡46£ 578¥ and H  ¡¤6£ h5 8 ¥

© ¡4£¦¥ £

3. Recover

at each by computin©g th¤¡e£¦¥ max§ -

imum a
de7yf 'g dih 3j

pi okgsl tee r4¡ i©o¡¤r£ i ¥Gm(h  M¡4¦£ Aw¥ ¥ P.)

estimate,

MAP

 opn ¡¤£ ¥

4. Sinynthesiz© e a n¡4¦£ew¥ image

under novel il3lu¡¤£9m75 i8n¥ ation

3 ¡¤£9u75sinn ¥g MAP and the j on int statistics of

and

. Note again that may be different from the

illumination in the bootstrap set.

The next few sections elaborate on these steps.

3.1. Estimating q

Estimating the unknown illumination is a well-studied prob-

lem [22, 25], since it is part of the shape-from-shading prob-

lem. This turns out to be easier than recovering the shape.

For our purposes, we use the simple method of kernel re-

gression [1]. We note that since the bootstrap set is labeled with known illumination, we can recover by viewing it as

a continuous-valued we first store all the

r

cltarsasiinfiicnagtiiomnapgreosb, l8r esmsi.t8Mswt v or,eaplornecgisweiltyh,

their labeled illumination, we recover its illumination

r

Gis t8swt v

a
. Given a new image, u ,

usinga simple kernel regression:

v§ w t x is c 8s yz¡ w t x is ¥ ws v ws v

aB

a

where x

Ys §|{ ~h 6}  

¡40 ¡ 5 is ¥7y is ¥  u s  G

 ¡ 5 si¥§C  s 

and u s

u s  , the   norm

(3)

We use Gaussian kerns els of widths  s , which control the extent of influence of s . These values are pre-computed so that appB!roS xims ately ten spercent of the bootstrap images lie within  at each s . Basically, kernel regression is a smooth interpolation method in which bootstrap images near u get weighted more than those farther away. How accurate is this method of estimating ? We compared the estimated illumination of the 60 images of a test face against their actual values. Details of the test are given in [17], but ozn R a verage, the estimated differ zRfr og m the actual value by
, with a standard deviation of . Our method is thus reasonably accurate.

3.2. Computing the statistics of  ozq 

¡¤6£ 5hc 8s ¥

Our sta¡4t6£is75tic cs8a¥ l mo3d4¡e£6l h75 cais s¥ learned the statistics (i.e.   G s

and H

) of

at the known illuminations . We

need a way t o compute these same statistics for any new

illumination . A gain, we use kernel regression. The mean

vanalduveasraitanr c es it awst tv

are smoothly interpolated from the known . The kernel regression equation used here

is similar to that a in (3).

We also need a was7y to interpolate from the known cor-

relation cient  oa 

cobeeftfiwceieenntsill umitnoatoibotnasin

a
a

new and



correlation coeffi (one or both of

which may be different from the bootstrap set). To do this,

we need a slight modification. We view th¡4Ae¥&pro&" bl( emno" w

as interp4¡ oA lat¥vin§ g the (unknA own) function 

,

wt o h:ienrA teae r p§ola a t ea



75    ¡¤A¥

oa  
.



and a  is the concatenation of a and . We will now use ke§rnel regression Note that b¤¡ Aeca¥ use  oa  ¡4A ¥  ¢ a , we can

actually obtain two estimates  a  and  ha . By averag-

ing these two values, we obtain a better (smaller variance)

estimate.

3.3. Computing the MAP estimate

© ¡4£¦¥§

Using Bayes'
de7yf gTd h 3 j i k l

re u¡¤l¢  e4¡ ,¦£ t¥Ghm ©e ¡¤M£¥A¥ Pl

e

e¤¡s©ti¤¡m£a¥t¥ e .

becomes MAP The second term

is

simply the Gaussian pdf learned in the previous section,

while t©h¡¤e£¥first term may be obtainh  e4¡ d¦£ ¥ from Equation (1).

Given variable

, Equation (1) says that with¡¤£6G5h8a¥ussian pdf of mean

© ¤¡ £ ¥

i s

a 

scala¤¡ r£6r5ha8n¥ dom   and

variance H 

. It can be shown [17] that the MAP esti-

mate may be computed from solving linear matrix equations

of the form:

©§ V MAP

where

§ V

and

§ u

u

B

8  

4¡    

  `Ha ¥

  



  ` a 

(4)

Note that ou© r¡4~@ a¥lgorithm recovers each ©¤¡ ¦£ ¥ independently of other . We have not explicitly used any other

constraints such as smoothness or symmetry. These con-

straints are implicit in our statistical model, and we have

found them to be sufficient for our purpose. Imposing the

symmetry constraint explicitly, for example, will limit the

applicability of our method to symmetric objects only. Be-

sides, faces are not perfectly symmetric [13], so this con-

straint may actually hurt our algorithm.

© ¡¤£ If¥ we have multiple images from which to recover vddd eu77ee yffycret'g'go.,rdiid whhoLfell eqteec4¡¡ax©x ni¡¤¤¡n¡4££e£¦tG¥eaG¥¥mnsmx ©isliy§¤¡¤¡ t££y¥¥e¥¥xv, l taea lwenu¡¤¡¤£deh© s¥¢ic¤¡5t£hha  et¥ ¥¡¤b£¦p.Myic¥ xTG5ABeFEhlPaGE ey6E £ ese5w.ess  tc'u iomWr¡¤nu£adelt¥ ei ton enboremwbpceorisomsectehaeekess-

before, while sian pdf with

mtheeanfirst

t© erm

is ,

now a multivariate Gausand covariaRncS e mq atrix  .

The new variables are whose columnsq areS thB e

defined as:  , a estimated illuminations

of

mthaetrq ix

images; means 

 

¤¡ ,6£ ah5 n
a

¥cG5 EGFE E 5



m4¡ e6£ a5hn


u

ve¥ ctor containinq g thS2e q , and  , an

scalar co-

vHa r4¡ i6£an57cea ¢¥ mF5 EGaEGt6E ri5xH w ¡4h£6o5hs eu 7s d¥ ,iaagnodnawl heonst3 re¡4ie£6os57ffa-s rd¥eiatghoenvag la4¡re6£inah5 nt rci ee¥ ss

are the covariances  between

and .

Anogwa`Hinat,h¡ xethteersmo6¥ slu tairoenq: `HataVk e.s§

the form
¡`HaG

 of 

Equation (4),  H` a , and u

bu§ t

Note that these images must differ only in illumina-

tion. Identity, facial expression, etc., must remain the same,

otherwise the method may fail. Since it is difficult in real-

world applications to acquire several images of one person

that differ only in illumination and nothing else, the oppor-

tunity to use multiple images is rare. Nevertheless, we are

pleased that our statistical model is theoretically able to han-

dle multiple input images.

3.4. Synthesizing new images

We now have tn he tools to synthesize images under any new

illumination tian equation

a.ndWceomcapnutseim© pMAlyP 4¡ £¦us¥ e

ithn e .

standard LamberBut this generates

images that do not look realistic: specularities and shadows

are not properly synthesized. This is because real faces are

not perfectly Lambertian.

© ¤¡ £¥

To do better, we note that having computed MAP , w e

can o!bt§ain3¤¡t£9he578 a¨¥ c§tua ¢l¤¡ £ e¥ rr or©

from

in §¢3¤¡ £6h5 i £n ¥

at th¤¡ £ e¥
MAP

o r ig. iWnael

illumination , now ask fo r the

most other

probable words, we

want

given

d7e f 'g id h i n

¥¤

l 

e

k4¡ n8 o¦n wm i ¥ledge about . Since we

. In have

modelled the joint pdf of tion, th8isn turns o ut to be §



8an¦nm  d ,

as the

a Gaussian conditional

distribuexpecta-

tion of given . From basi¤¡ c£95p@zro¥ bability theory, we know

that if two random variables

are jointly Gaussian with

means   and 6 ¨ , variances   and H ¨ , and correlation co-

Figure 2: Image rendered using the strict Lambertian equation (left) versus one that uses the error term (right). Specular reflection on the cheeks are more accurately rendered in the right image.

e f¨ fic ie nt ¨  variances



a n¨¨ d,¡ t ch`«oeª©rnrª eH@ ¥ l,am £ tainiosdnacvlsoaoerifaafinGcciaeeunHststiear§ nmpsHda¨ fr¡weB citohm3mp eu¨ at¥ ne.dTuhs§ e-

itnhgeskizeerdneilmraeggereissstihounsa:so  ¬nd¡¤e£ s¥ cr§2ibe© dMAiPn¡4¦£ S¥e cti iogn n 3§ .2 8 .¦n Tm  h .e synThe effect of the error term may be seen in Figure 2,

which compares an image rendered under strict Lamber-

tian assumption versus one rendered using the additive error

term. Clearly, the use of the error term produces a more vi-

sually pleasing result.

4. Face Recognition
Let's recall our proposed approach for face recognition. We proceed in two stages: first, we will use models to generate additional synthetic images; these are then be used as exemplars to train a face recognition system. Using our synthesis method of the previous section, we can easily render images under many illumination directions. The effect of multiple light sources is also readily produced: since light is additive, we simply add the images created under single light sources. From our literature survey, we note that having more training images will almost always improve the performance of any classification system. It is the dearth of training images that cripples many a classifier. However, we are particularly interested in using these synthetic training images in an exemplar-based classifier.

4.1. Exemplar-based classifiers
The simplest exemplar-based method is to use the training images directly in a  -nearest-neighbor search [19]. The obvious drawback is the enormous amount of storage space required, and the time it takes to perform a single classification. Clearly, this method is not readily scalable. We can improve things somewhat by reducing the dimensionality of the problem. A standard way to do this is via Principal Components Analysis (PCA) [5].

From the seI¡t Sof we comq pute a 

I

-dimensional training vecto¡I rsS r B projection matrix ¯ and a

£Dt sw® v , meaa n

vector . We then project all the tr#@ain§ ing

dbiemneefintsiiosntahlaPt CofAtensu, b sp± aceI

using , allowing

¯
us

d at¡4a£

in to°q a ¥

lower. The

to greatly reduce

the dimensionality while preserving the i¦@nft ormation in 6£tht e original images. We can then work with r instead of r ,

since they are just compressed versions of the original. The

problem is that given the size of our images, and the fact that

we are synthesizing thousands more, the usual way of com-

putin£gt ¯ from the Singular Value Decomposition (SVD)

of r be¡¬c´³ omi% ¥es intracp¡ t³ able¥ . The time and space complex-

ity are ²

and ²  respectively. Instead, we need

a way to compute ¯ incrementally, as new images are syn-

thesized.

We use the method of [4]. The idea is to incrementally update the SVD (and hence the PCA subspace) as more images are added. Furthermore, not every additional image will affect the SVD; only those that are significantly outside the PCA subspace need to be considered, others can be safely ignored since they are well represented in the subspace. Using this technique and requiring that the reconstruction error to be no more than 1%, we found that the PCA subspace so computed spans only 40 dimensions, much smaller than the 12000 dimensions of our original images.

Despite this, working in this reduced subspace still requires a substantial amount of memory. The main culprit is the fact that we are synthesizing so many images. One way to overcome this is to embed the exemplars in their own subspace, one for each person to be recognized. This isq readily done by computing individual PCA subspaces (0¯ µ , µ ) for each person ¶ . This is different from the global PCA subspace ¯ described above, which is computed from all the exemplars, regardless of identity. Recognition is now performed using reconstruction error, as follows:

1.

Given a task im@ age§ subspace using µ

£ ¯

,

p ro¡4£ ject
µ

q

it

µ

in¥ to .

each

individual

£ § @ ) q

2. Reconstruct the image µ¨  U¯  µ '§ µ  £  µ £ , an d compute

the reconstruction error µ 

µ .

 
3. Pick the subspace that has the smallest µ  .

The memory requirements for this individual eigenspace approach is linear in the number of classes, and does not depend on number of synthetic exemplars generated. Recognition speed is also fast: linear in the number of classes, and independent of the number oQ f exe mplars. In terms of pattern recognition theory, each µ  is just a quadratic discrimination function [5].

4.2. Analysis

An important and natural question to ask is this: instead

of synthesizing all these training images and then design-

ing a classifier from them, can we exploit the structure of

the synthesis procedure to more directly produce an equiva-

lent classifier? If we can do so, we will have completely

avoided the synthesis step and its concomitant problems

described in the previous section. We shall attempt to do

this for the individual PCA approach. We begin by writ-

i(u¢ne· xg§ptrhex essseyd n·at sh eas6Iis¸¥-w¹deº i¸ qm,uweanthisoe°RironenS(oa¢u flI·

Section  3.4 in matrix form: is the th synthesized image column vector) under new il-

lumination · ; x normals; and 9 ¸

¹

is
º¸

§ the
§

 8n¥m 



matrix of recovered surface is the column vector of the

conditional mean of original illumination



the .

error

term

given

the

error

at


the

To aid our analysis, we assume that the illumination · is

moved around in front of the face (there is no need to illuminate from behind). Specifically, assume that · is varied

on the surface of a hemisphere of unit radius, centered at the

face. To compute the individual PCA, we needp the§ eigent -

vectors of the covariance matrix of the images

8r u · .

We should therefore see how varying · contributes to the

covariance. Appendix A derives the mean and covariance

to be:

 » § §  u ·  § x  §   ·   §  6 ¸¥¹º ¸ 

G¼ 8½ ¾  p  §

x



§



·



 ·

x



x



§

  ·4

 ¥¸ ¹wº ¸ 



§

 6¸

¹

º¸



 ·

x



§

 6¸ ¹ º 7¸ 

 ¸o¹º ¸





 » »

B

where

§  · §

¿

5¿

5



 

§



·



 ·



§ÁÂÀ

%a ¿

¿ %a

¿ ÄÃ ¿

¿ ¿ %a

(5)

Observe that all expectation terms are constant of x .

They do not change for different people, and can there-

fore be pre-computed. Thus, to determine the eigenspace for each person, we simply compute

¼cin½8d¾ ivp id ubayl

the above equations, and then solve for its eigenvectors.

However, there are several problems with this approach.

Aside from the two expectations computed as shown, the

others are too difficult to obtain analytically as they involve

fairly complicated functions of random variables (see Ap-

pendix A). We can obtain numerical estimates for them us-

ing simulation or Monte Carlo techniques, but even this is

vptrorioxlvb, elwedmh. iFacthoicrfoibnresoctauanurcsieem, oatghf eetshteeorfmhduig§me esn¥¸ is¹ziºeo7¸ sna¸o¹lfiº ¸tyt hiI¡es §Cam&IaBitr S iD¿c¿Ies¿ mirnea---

quires about 1 Gb of memory. Assuming we can overcome

this problem (e.g. by reducing the size of the images), we

would still like a way to quickly compute the eigenvectors.

Unfortunately, there is no obvious relationship between the

eoifg¼ce½8n¾ vep ct o. rNs eoifththeer

individual is there an

terms in Equation 5 and those easy way to compute the next

set of eigenvectors when x If we knew the rank of

c¼ ½8c¾ha p ng ,esorfoervdeniffaenreunptppeerobpoleu.nd

on the rank, we can speed up our eigenvector calculationI s. Assuming that the upper bound,  , is much smaller than ,

there are meth¡ odI3s¥ to compute the  dominant 4¡ eI ig% ¥envectors

quickly, in ²  time instead of the usual ²

time for

standard eigenvector algorithms. See ARPACK [12] for ex-

ample. wrong,

Looking at to conclude

the that

ac¼ b½8o¾ v ep

equation, it  has a rank

is tempting, of at most 5

but be-

cause it is a sum of five terms containing rank-1 outer prod-

eu§§¼cv½8c e¾tE sn·  pcot·a fh nov,euaeawfgccfthhheoicrchtsh· a.t shh·T erahashrenaabknrseekeraÅ ansonofRcknio,t1imsts.hapetTrhugfahtuoetumdutshreattehhnbeetot.xevfiprAermestctcothatahbastierseoeirneonaftoneprpkraoemniÅrnkasttoo3Iisfr,,

while the last term has rank 1. We can of course numeri-

cally estimate § riseq ui§ remÆ entB s¿ ..

 6 o¸ ¹º
Then If ±

7¸an¸¥I u¹w,pº ¸wp eearncdbanoitunsnordwanorkne,stÆ oh,retbrtaoarnrAiknRgoPfmAc¼ eC8½ m¾ Ko p rtoy

compute the eigenvectors; otherwise it does not appear that

we can do much better.

5. Experiments and Results
5.1. Synthesis experiments
We tested our shape-from-shading algorithm by comparing the synthesized images against the actual ones. Testing the recovered surface normals is not particularly meaningful because we do not have ground truth data. Even if we used Equation (2) to attempt to recover the ground truth, what we would end up computing is just the least-squares estimate of the true surface normals. Furthermore, since our purpose is to generate more exemplars, what matters is how realistic our synthetic images are, not how well intermediate parameters are recovered. Quantitative results of our synthetic images are reported in [17], and show that the more extreme the illumination (whether in the task image or in the synthetic ones), the greater the error. These quantitative errors, do not, however, tell us about the visual quality of the synthetic images. For this we can only rely on human judgement. Figure 3 shows some synthetic images, compared to the actual ones.
A more interesting and instructive test case is when the task image is completely black. Our algorithm proceeds to recover and synthesize a face, in effect hallucinating one into existence [2]. This is the natural consequence of using class-based prior information. Our algorithm is generating the most probable face, based on other bootstrap faces it has seen. Not surprisingly, this turns out to be the average

Figure 3: Synthesized images (top row) versus actual images under the same illumination (bottom row).

Figure 5: (Left) Input image: face illuminated from one side. (Middle) Face recovered and re-rendered under frontal lighting. The side of the face in shadow is hallucinated from the model, resulting in a mixed identity. (Right) Actual image.

Figure 6: Two PIE faces under four illumination directions. These subjects are different from those in the Yale dataset. The illumination is also different.

Figure 4: (Left) Hallucinated from an all-black image. (Right) Average of all bootstrap faces.

 §
of all the bootstrap images (Figure 4): substituting

¿

(© becau§ se there is no illumination) into Equation (4) yields MAP   . This effect also comes into play when large
regions of the face are occluded, or in shadow. For instance,

a task image illuminated from one side (and therefore has

the other side in the dark) produces a synthetic face with

mixed identity. The side that is illuminated is one person,

while the side that is not visible is replaced with the average

face (Figure 5).

Overall, our algorithm synthesizes reasonable faces, es-

pecially when illumination is not extreme. For the purpose

of face recognition, this seems adequate. A natural question

to ask is therefore: how does the quality of the synthetic im-

ages affect recognition accuracy? This is something we will

investigate in future.

5.2. Recognition experiments
We built three simple exemplar-based classifiers and compared them. The first classifier performs single nearestneighbor search directly in image space. We call this 1NN. The second classifier searches for the nearest exemplar in

the global PCA subspace (dimensionality 40). We denote

this as globalPCA. Finally, the third classifier computes in-

dividual PCA subspaces from the exemplars (dimensional-

ity between 35 and 45), and classifies using the smallest re-

construction error. Call this indivPCA. All PCA subspaces

were computed to have an average reconstruction error of

1%.

For our recognition experiments, we used a subset of

ten people from the CMU PIE dataset [16]. These persons

are distinct from those in the Yale dataset (see Figure 6).

Each person has 21 images taken under different illumina-

tion (different from those in the Yale dataset). All images

were pre-processed as in the Yale dataset: affinely aligned

and tightly cropped. For each person, one image was ar-

bitrarily selected as the training image, and from this 900

additional exemplars were synthesized. These synthetic im-

ages from

w eÇ re¿ 

creat edÇ to

¿

u nder illumination in both azimuth

directions that range and elevation angles

on a hemispherical surface in front of the face. The remain-

iNcnaogntne2o0tthbiamyt pabagesecssasupysenerthopeuesrriszioimnngawgimeesraeagureesseladanrdtgoecto(emI stp§ÈtuhteeiB c¼c l½8¿Da¾ D¿ss¿ p i)fi, ewdrsie-.

rectly using Equation 5.

The results are as follows: 39% recognition accuracy for

both 1NN and globalPCA, and 95% accuracy for indivPCA.

We postulate that the 1NN and globalPCA classifiers are

distracted by "noise": many faces under extreme illumina-

tion look alike because large regions are shadowed. We further speculate that the 40 eigenfaces in the globalPCA subspace capture mostly illumination variation, rather than identity. The eigenfaces beyond these 40 (which have been excluded from globalPCA) are probably more discriminating. As for indivPCA, we pleased to see that it is a viable classifier. For this method to work, large numbers of training images must be available for each person to be recognized. This is required in the computation of the individual PCA subspaces, otherwise the computed subspaces will not be accurate. Since training images are often scarce in face recognition applications, this technique is not commonly used. By using our approach of synthesizing exemplars from even a single training image, we can overcome this limitation. indivPCA now becomes a feasible classifier.
6. Discussion
The results from our simple experiments are encouraging. They show that our model- and exemplar-based approach for face recognition can overcome the limitations that plague other methods. We must emphasize that our approach is not tied to specific synthesis techniques nor classifier designs. We could have used the Quotient Image [15] to generate new face images, or a neural network for our classifier, and it would not change our idea. The synthesis method in Section 3.4 and the indivPCA classifier in Section 5.2 were chosen merely to illustrate our idea of combining models and exemplars, and to show that some mathematical analysis can be done when we know the structures of both the synthesis method and the classifier. In fact, a number of major points may be raised at this juncture.
Firstly, whereas other classification algorithms focus on features ­ finding discriminating and invariant features, and extracting them from images ­ we focus on synthesizing training images and designing an exemplar-based classifier that is feature-free.1 As we have noted in our introduction, finding discriminating features that are truly invariant to all types of appearance variation is very hard, if not impossible. Even if these features are found, it is time-consuming and error-prone to locate them on a face image, extract and process them for recognition. By avoiding features in our classifier, we also avoid these problems. To be sure, our synthesis method does require some features ­ locating three points on the face for image alignment ­ but this is confined to the synthesis stage. The classifier itself is featurefree. In general, we expect the model-building and synthesis stage to require features. But as long as the resulting classifier does not explicitly depend on features, we will not be bogged down by feature-processing during recognition.
1Although we have not stated it explicitly, it should be obvious that any exemplar-based classifier does not rely on features directly. Rather, the exemplars themselves provide the information necessary to discriminate one pattern from another.

Secondly, our method need not be fully automatic. Note that there is no reason for our model-based synthesis not to require manual intervention from a human expert. As long as our classifier works automatically, the learning stage can use as much manual help as possible to create realistic images. For example, in Figure 3, we could have manually corrected the deficiency of our algorithm and improved the quality of the synthetic images before using them in our classifier. Doing so will only help indivPCA learn a more accurate eigenspace. Other classification schemes try to make both the learning and recognition stage fully automatic, but we feel that this is unnecessarily burdensome.
Thirdly, our approach has the potential to handle many more kinds of appearance variation, simply by synthesizing images that exhibit those variations. For example, to simultaneously cope with pose, illumination and expression changes, synthesize training images under many different combinations of lighting, pose and expression. The exemplar-based classifier will learn them all, regardless of variation. Of course, we have yet to demonstrate this conclusively, but the extension is logical and waiting to be explored. We envisage using different models to synthesize different kinds of variations: the one in this paper for illumination, another for pose, yet a third for facial expression. We can even imagine synthesizing mustaches, beards, and eye glasses. Obviously, the number of synthetic images will grow combinatorially with the kinds of variations to be synthesized. This imposes some constraints on the exemplar-based classifier: (1) it must be able to learn incrementally, as more training images are synthesized; and (2) its storage requirements and classification time must not increase as quickly as the number of exemplars. Classifiers such as 1NN and globalPCA clearly will not work, whereas indivPCA is good candidate.
Finally, we should point out that our approach is applicable to other non-face objects as well. In fact, our approach should be seriously considered for recognizing nonrigid, deformable objects, where finding suitable features has proven to be difficult. As long as models can be built to synthesize images of these objects realistically, we can combine them with an appropriate exemplar-based classifier to do the job.
7. Conclusion
We conclude by summarizing the key points in this paper: We have proposed a new approach for face recognition. The main idea is to utilize models, statistical or otherwise, to synthesize many more images from a given few, which can then be used to train an exemplar-based classifier. We demonstrated this idea by showing how a statistical shapefrom-shading model may be used to synthesize images under novel illumination, and next by using this set of aug-

mented training images to build a simple, exemplar-based classifier. We also analyzed the mathematical structure behind the synthesis and classification scheme and suggested ways to improve the construction of the classifier. We note that our synthetic images are not perfect; they degrade under extreme illumination, but we can expect better techniques to correct for this in future. Indeed, the computer graphics community is relentlessly perfecting the art of synthesizing realistic faces. We should be able to leverage their techniques to further our own goals, that of perfecting face recognition.
In the near future, we intend to further develop this approach in several ways. On the synthesis side, we hope to use graphics models to generate pose and expression variations. As for classifier design, we want to explore classifiers that can compactly represent all the synthetic exemplars without combinatorially exploding, or those that were never possible before due to a lack of training data. Where the structure permits, we intend to mathematically analyze both the synthesis algorithm and classification method in order to design better classifiers.

A. Derivation of É p and ÊHyË Ì'pÍ CÎ Ï

We first compute §

 · 

and §



·



 ·

.


Consider ·

to be a

point moving randomly on th e surface of a unit hemisphere.

We assume that the pdf of · is uniform f§Ôor B all points on

this surface. Let this be ¶ . Then: iÐ dÑ ¶ÒÓ

, where  is

the surface of the hemisphere. Using Spherical coordinates,

with Õ and Ö denoting azimuth and elevation angles, re-

spectively, and noting that the elemental area on the surface

is ×cgØ Ù~ÖÚÒzÕ#~Ò Ö , this may be written as:

ÝDÛ Ü  ÝÛ Ü  §ÞB
v¶ ×cØÙ3ÚÖ ÒzÕ~Ò Ö

DÛ Ü ÛÜ

` ` 

ÝDÛ Ü  ÝÛDÜ 

 à¶ âáß

B ÒzÕ9Gåã ä âáß

§ÞB G× Ø Ù~ÚÖ ~Ò Ö6Gãå ä

Û Ü ÛÜ

`

`

 

¤¡ æ6G¥ ¡p ¥ Þ§ B5 ¶

B

so that

§ ¶

 æ


N§ o w·  , §let  §

·£

§ 

5

§



9£ 5@w5 ç £b @  5§ §



ç

 

 è  

in Cartesian coordinates. Then: . We con5zvì@ert§ into SphericalçÔco§ -

ordinates using

wÙ ëé êÕc× ØÙzÖ

wÙ éëê Ö and

×cØÙ~¡Õ c× ØgÙ~Ö . So for instance,

§ £  § § §

ÝÛÜ  ÝDÛ Ü  
¶ Ùèé ê íÕ G× Ø Ù~!Ö ×GØÙ~ÚÖ ÒzÕ ~Ò Ö

DÛ Ü DÛ Ü ` `  B ÝÛDÜ   i æ âáß

Ù èé êÕ#zÒ Õ9åã ä

ßâá

ÝDÛ Ü  c× Ø Ù  ÚÖ ~Ò Öåã ä

ÛDÜ B ` æ i æ ¡ ¿ c¥ ¡  ¥

§¿

ÛDÜ `

wNa¿ r,eoeatagneeodttfh:§ tahtç et hh§ eemsaei.csopHnhedenrcG×eeØ. ,Ù~S§ Öim  tie· l ram§rlyc, ¿ow5m¿ee5 msa af ryo.mcAomtshfpeouretel§ e§m   ·e@  n · t§a l,

£ £î@ î£ ç ÄÃ ÃÄ

§



·



 ·



§

§

ÀÂ ÂÀ î£ @ £¦ç

@ 3@ ç

~@ ç ç



We compute the expectation of each element of the matrix as before, by converting into Spherical coordinates. This results in:

§



·

8  ·



§ïÂÀ

%a ¿

¿ %a

¿ ÄÃ ¿

¿ ¿ %a

Proceeding on, we have 6»

§

§ u · §

§ x

  ·

6 ¥¸ ¹wº ¸  §

x  §   ·   §  6¸¥w¹ º ¸  . From Section 3.4,

g ¡¬8 ¥ 

§

E

¸

6¸¥w¹ º ¸  ¸ ¹  ¸ ¹¥ð ¸  ¸ ¹ ñ

¸ ò

55
Note that  ¸ ¹  ¸ o¹ ð ¸ and  ¸ ¹ are computed using kernel regression (see Section 3.2). Because of this, these terms are

random variables, since they are functions of the random vector · . Recall, however, that is the estim ated illumination of the input image, which is constant as · varies on the hemisphere. Thus,

3¡¬i¥ 

§  6 ¸ ¹ º ¸  §

§



 ¥¸ ¹ 

§

  o¸ ¹ ð ¸

E 

o¸ ¹ ñ

¸

¸ ò

And finally,

¼c8½ ¾  p  § §

§

 ¢u ¦· u

 ·





 » »

  §#ó¢ô¬x · 6¥¸ o¹ º ¢¸ õYpô x

  ·

ö  6 ¸¥w¹ º ¸¢õ

  »  »

§

§ x

   · ·x

    x ·p ¸¥¹oº ¸





6¸ ¹ º ¸h ¸oo¹ º ¸   »  »

 6¸ ¹ º ¸ · x



§

x



§



·



 ·

x



x



§

  ·4

 ¸o¹ º ¸





§

 6¸

¹

º¸



 ·

x



§

 6¸ ¹ º 7¸ 

 ¸¥¹º ¸ 



 » »

Acknowledgements
We are grateful to Rahul Sukthankar for his helpful comments and prompt feedback on this manuscript, as well as for his early contributions to our ideas.
References
[1] C.G. Atkeson, A.W. Moore, and S. Schaal. Locally Weighted Learning. Artificial Intelligence Review, 1996.
[2] S. Baker and T. Kanade. Hallucinating Faces. In AFGR, 2000.
[3] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection. PAMI, 19(7), 1997.
[4] S. Chandrasekaran, B. Manjunath, Y. Wang, J. Winkeler, and H. Zhang. An Eigenspace Update Algorithm for Image Analysis. In CVGIP, 1997.
[5] R. Duda, P. Hart, and D. Stork. Pattern Classification, 2nd edition. John Wiley and Sons, 2000.
[6] G.J. Edwards, T.F. Cootes, and C.J. Taylor. Face Recognition Using Active Appearance Models. In ECCV, 1998.
[7] R. Epstein, A.L. Yuille, and P.N. Belhumeur. Learning Object Representations From Lighting Variations. Lecture Notes in Computer Science, 1144:179­??, 1996.
[8] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman. From Few to Many: Generative Models for Recognition Under Variable Pose and Illumination. In AFGR, 2000.
[9] D.B. Graham and N.M. Allinson. Face Recognition from Unfamiliar Views: Subspace Methods and Pose Dependency. In AFGR, 1998.
[10] B.K.P. Horn and M.J. Brooks. Shape from Shading. MIT Press, 1989.
[11] R. Jain, Kasturi R., and B. Schunck. Machine Vision. McGraw Hill, 1995.
[12] R.B. Lehoucq, D.C. Sorensen, and C. Yang. ARPACK Users' Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods. SIAM Publications, Philadelphia, 1998.
[13] Yanxi Liu, R.L. Weaver, Karen Schmidt, N. Serban, and Jeffrey Cohn. Facial Asymmetry: A New Biometric. Technical Report CMU-RI-TR-01-23, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, August 2001.
[14] U.S. Department of Defense. Facial Recognition Vendor Test, 2000. http://www.dodcounterdrug.com/facialrecognition /FRVT2000/frvt2000.htm.
[15] T. Riklin-Raviv and A. Shashua. The Quotient Image: Class Based Recognition and Synthesis Under Varying Illumination Conditions. In CVPR, pages II:566­571, 1999.
[16] T. Sim, S. Baker, and M. Bsat. The CMU Pose, Illumination, and Expression (PIE) Database of Human Faces. Technical Report CMU-RI-TR-01-02, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, January 2001.

[17] T. Sim and T. Kanade. Illuminating the Face. Technical Report CMU-RI-TR-01-31, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 2001.
[18] T. Sim, R. Sukthankar, M. Mullin, and S. Baluja. Memorybased Face Recognition for Visitor Identification. In AFGR, 2000.
[19] R. Sukthankar and R. Stockton. Argus: The Digital Doorman. IEEE Intelligent Systems, 16(2), 2001.
[20] M. Turk and A. Pentland. Eigenfaces for Recognition. Journal of Cognitive Neuroscience, 3(1), 1991.
[21] L. Wiskott, J.M. Fellous, N. Kruger, and C. von der Malsburg. Face Recognition by Elastic Bunch Graph Matching. PAMI, 19(7), July 1997.
[22] R. Zhang, P.S. Tsai, J.E. Cryer, and M. Shah. Shape from Shading: A Survey. PAMI, 21(8), August 1999.
[23] W. Zhao and R. Chellappa. Robust Face Recognition using Symmetric Shape-from-Shading. Technical Report CARTR -919, 1999., Center for Automation Research, University of Maryland, College Park, MD, 1999.
[24] L. Zheng. A New Model-based Lighting Normalization Algorithm and its Application in Face Recognition. Master's thesis, National University of Singapore, 2000.
[25] Q. Zheng and R. Chellappa. Estimation of Illuminant Direction, Albedo, and Shape from Shading. PAMI, 13(7), July 1991.

