Convergence and Consistency of
Regularized Boosting Algorithms with Stationary -Mixing Observations

Aure´lie C. Lozano Department of Electrical Engineering
Princeton University Princeton, NJ 08544 alozano@princeton.edu

Sanjeev R. Kulkarni Department of Electrical Engineering
Princeton University Princeton, NJ 08544 kulkarni@princeton.edu

Robert E. Schapire Department of Computer Science
Princeton University Princeton, NJ 08544 schapire@cs.princeton.edu

Abstract
We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identically distributed (i.i.d.) but come from empirical processes of stationary -mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classifiers resulting from a regularization achieved by restricting the 1-norm of the base classifiers' weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.
1 Introduction
A significant development in machine learning for classification has been the emergence of boosting algorithms [1]. Simply put, a boosting algorithm is an iterative procedure that combines weak prediction rules to produce a composite classifier, the idea being that one can obtain very precise prediction rules by combining rough ones. It was shown in [2] that AdaBoost, the most popular Boosting algorithm, can be seen as stage-wise fitting of additive models under the exponential loss function and it effectively minimizes an empirical loss function that differs from the probability of incorrect prediction. From this perspective, boosting can be seen as performing a greedy stage-wise minimization of various loss functions empirically. The question of whether boosting achieves Bayes-consistency then arises, since minimizing an empirical loss function does not necessarily imply minimizing the generalization error. When run a very long time, the AdaBoost algorithm, though resistant to overfitting, is not immune to it [2, 3]. There also exist cases where running Adaboost

forever leads to a prediction error larger than the Bayes error in the limit of infinite sample size. Consequently, one approach for the study of consistency is to modify the original Adaboost algorithm by imposing some constraints on the weights of the composite classifier to avoid overfitting. In this regularized version of Adaboost, the 1-norm of the weights of the base classifiers is restricted to a fixed value. The minimization of the loss function is performed over the restricted class [4, 5].
In this paper, we examine the convergence and consistency of regularized boosting algorithms with samples that are no longer i.i.d. but come from empirical processes of stationary weakly dependent sequences. A practical motivation for our study of non i.i.d. sampling is that in many learning applications observations are intrinsically temporal and hence often weakly dependent. Ignoring this dependency could seriously undermine the performance of the learning process (for instance, information related to the time-dependent ordering of samples would be lost). Recognition of this issue has led to several studies of non i.i.d. sampling [6, 7, 8, 9, 10, 11, 12].
To cope with weak dependence we apply mixing theory which, through its definition of mixing coefficients, offers a powerful approach to extend results for the traditional i.i.d. observations to the case of weakly dependent or mixing sequences. We consider the mixing coefficients, whose mathematical definition is deferred to Sec. 2.1. Intuitively, they provide a "measure" of how fast the dependence between the observations diminishes as the distance between them increases. If certain conditions on the mixing coefficients are satisfied to reflect a sufficiently fast decline in the dependence between observations as their distance grows, counterparts to results for i.i.d. random processes can be established. A comprehensive review of mixing theory results is provided in [13].
Our principal finding is that consistency of regularized Boosting methods can be established in the case of non-i.i.d. samples coming from empirical sequences of stationary -mixing sequences. Among the conditions that guarantee consistency, the mixing nature of sampling appears only through a generalization of the one on the growth of the regularization parameter originally stated for the i.i.d. case [4].

2 Background and Setup

2.1 Mixing Sequences

Let W = (Wi)i1 be a strictly stationary sequence of random variables, each having the

same distribution by W1, . . . , Wl.

P on D  Rd. Let Similarly, let l+k

1l =

=  (W1, W2, . . .  (Wl+k, Wl+k+1,

, .

Wl) ..,)

be the . The

-field generated following mixing

coefficients characterize how close to independent a sequence W is.

Definition 1. For any sequence W , the -mixing1 coefficient is defined by W (n) = supk E sup |P A|1k - P (A) | : A  k+n ,
where the expectation is taken w.r.t. 1k.

Hence W (n) quantifies the degree of dependence between 'future' observations and 'past' ones separated by a distance of at least n. In this study, we will assume that the sequences

1To gain insight into the notion of -mixing, it is useful to think of the -field generated by a random variable X as the "body of information" carried by X. This leads to the following interpretation of -mixing. Suppose that the index i in Wi is the time index. Let A be an event happening in the future within the period of time between t = k + n and t = . |P (A|1k) - P (A)| is the absolute difference between the probability that event A occurs, given the knowledge of the information generated by the past up to t = k, and the probability of event A occurring without this knowledge. Then, the greater the dependence between 1k (the information generated by (W1, . . . , Wk)) and k+n (the information generated by (Wk+n, . . . , W)), the larger the coefficient W (n).

we consider are algebraically -mixing. This property implies that the dependence between observations decreases fast enough as the distance between them increases.
Definition 2. A sequence W is called -mixing if limn W (n) = 0. Further, it is algebraically -mixing if there is a positive constant r such that W (n) = O (n-r ) .
The choice of -mixing appears appropriate given previous results that showed "uniform convergence of empirical means uniformly in probability" and "probably approximately correct" properties to be preserved for -mixing inputs [11]. Some examples of -mixing sequences that fit naturally in a learning scenario are certain Markov processes and Hidden Markov Models [11]. In practice, if the mixing properties are unknown, they need to be estimated. Although it is difficult to find them in general, there exist simple methods to determine the mixing rates for various classes of random processes (e.g. Gaussian, Markov, ARMA, ARCH, GARCH). Hence the assumption of a known mixing rate is reasonable and has been adopted by many studies [6, 7, 8, 9, 10, 12].
2.2 Classification with Stationary -Mixing Training Data
In the standard binary classification problem, the training data consist of a set Sn = {(X1, Y1) , . . . , (Xn, Yn)}, where Xk belongs to some measurable space X , and Yk is in {-1, 1}. Using Sn, a classifier hn : X  {-1, 1} is built to predict the label Y of an unlabeled observation X. Traditionally, the samples are assumed to be i.i.d., and to our knowledge, this assumption is made by all the studies on boosting consistency. In this paper, we suppose that the sampling is no longer i.i.d. but corresponds to an empirical process of stationary -mixing sequences. More precisely, let D = X × Y, where Y = {-1, +1}. Let Wi = (Xi, Yi). We suppose that W = (Wi)i1 is a strictly stationary sequence of random variables, each having the same distribution P on D and that W is -mixing (see Definition 2). This setup is in line with [7]. We assume that the unlabeled observation is such that (X, Y ) is independent of Sn but with the same marginal.

3 Statistical Convergence and Consistency of Regularized Boosting for Stationary -Mixing Sequences

3.1 Regularized Boosting

We adopt the framework of [4] which we now recall. Let H denote the class of base classifiers h : X  {-1, 1}, which usually consists of simple rules (for instance decision stumps). This class is required to have finite VC-dimension. Call F, the class of functions f : X  [-1, 1] obtained as convex combinations of the classifiers in H:

tt

F = f (X) = jhj (X) : t  N, 1, . . . , t  0, j = 1, h1, . . . , ht  H .

j=1 j=1

(1)

Each fn  F defines a classifier hfn = sign (fn) and for simplicity the generalization

error 1/n

Lni=(h1fIn[h)fnis(Xdie)n=oYtie]d.

minimizing the indicator

by L (fn). Then the training error is denoted by Ln (fn) = Define Z (f ) = -f (X) Y and Zi (f ) = -f (Xi) Yi. Instead of of misclassification (I[-f(X)Y >0]), boosting methods are shown

to effectively minimize a smooth convex cost function of Z(f ). For instance, Adaboost

is based on the exponential function. Consider a positive, differentiable, strictly in-

creasing, and strictly convex function  : R  R+ and assume that  (0) = 1 and

that tion

limx-  (x) = 0. The corresponding cost are respectively C (f ) = E (Z (f )) and Cn (f )

function = 1/n

and empirical cost func-

n i=1



(Zi

(f ))

.

Note

that

L (f )  C (f ), since I[x>0]   (x).

The iterative aspect of boosting methods is ignored to consider only their performing an

(approximate) minimization of the empirical cost function or, as we shall see, a series of

cost functions. To avoid overfitting, the following regularization procedure is developed for

the choice of the cost functions. Define responding empirical and expected cost

 such that  > 0  (x) functions become Cn (f ) =

=
1 n

 (x) . The cor-

n i=1



(Zi

(f

))

and C (f ) = E (Z (f )) . The minimization of a series of cost functions C over the

convex hull of H is then analyzed.

3.2 Statistical Convergence

The nature of the sampling intervenes in the following two lemmas that relate the empirical cost Cn (f ) and true cost C (f ). Lemma 1. Suppose that for any n, the training data (X1, Y1) , . . . (Xn, Yn) comes from a stationary algebraically -mixing sequence with -mixing coefficients  (m) satisfying  (m) = O (m-r ), m  N and r a positive constant. Then for any  > 0 and b  [0, 1),

E sup |C (f ) - Cn (f ) |
f F



4

()

c1 n(1-b)/2

+ 2 ()

1 nb(1+r )-1

+

2 n1-b

.

(2)

Lemma 2. Let the training data be as in Lemma 1. For any b  [0, 1), and   (0, 1 - b), let n = 3(2c1 + n/2) ()/n(1-b)/2. Then for any  > 0

P sup |C (f ) - Cn (f ) | > n  exp(-4c2n) + O(n1-b(r+1)).
f F

(3)

The constants c1 and c2 in the above lemmas are given in the proofs of Lemma 1 (Section 4.2) and Lemma 2 (Section 4.3) respectively.

3.3 Consistency Result

The following summarizes the assumptions that are made to prove consistency.

Assumption 1.

I- Properties of the sample sequence: The samples (X1, Y1) , . . . , (Xn, Yn) are assumed

to come from a stationary algebraically -mixing sequence with -mixing coefficients

X,Y (n) = O (n-r ), r being a positive constant.

II- Properties of the cost function :  is assumed to be a differentiable, strictly convex,

strictly increasing cost function such that  (0) = 1 and limx-  (x) = 0.

III- Properties of the base hypothesis space: H has finite VC dimension. The distri-

bution of (X, Y ) and the class H are such that lim inffF C (f ) = C, where F = {f : f  F} and C = inf C (f ) over all measurable functions f : X  R.

IV- Properties of the smoothing parameter: We assume that 1, 2, . . . is a sequence

of positive numbers satisfying n   as n  , and that there exists a constant

c

1 1+r

,

1

such that n (n) /n(1-c)/2  0 as n  .

Call f^n the function in F which approximatively minimizes Cn (f ), i.e. f^n is such that

Cn(f^n) n  .

 inffF Cn (f ) The main result is

+ the

n = inffF following.

1 n

n i=1



(Zi

(f ))

+

n, with

n  0 as

Theorem 1. Consistency of regularized boosting methods for stationary -mixing se-

quences. Let fn der Assumption 1,

= f^nn  limn L

F(h,fnwh=erseigfn^n(nfn()a)pp=roxLimaaltmivoelsyt)sumrienlyimainzeds

Cnn (f ) . Unhfn is strongly

Bayes-risk consistent.

Cost functions satisfying Assumption 1.II include the exponential function and the logit function log2(1 + ex). Regarding Assumption 1.II, the reader is referred to [4](Remark on

(denseness assumption)). In Assumption 1.IV, notice that the nature of sampling leads to a generalization of the condition on the growth of n (n) already present in the i.i.d. setting [4]. More precisely, the nature of sampling manifests through parameter c, which is limited by r. The assumption that r is known is quite strict but cannot be avoided (for
instance this assumption is widely made in the field of time series analysis). On a positive note, if unknown, r can be determined for various classes of processes as mentioned
Section 2.1.

4 Proofs

4.1 Preparation to the Proofs: the Blocking Technique

The key issue resides in upper bounding

n

sup Cn (f ) - C (f ) = sup 1/n  (-f (Xi) Yi) - E (-f (X1) Y1) , (4)

f F

f F

i=1

where F is given by (1). Let W = (X, Y ), Wi = (Xi, Yi). Define the function g by g (W ) = g (X, Y ) =  (-f (X) Y ) and the class G by G = {g : g (X, Y ) =  (-f (X) Y ) , f  F} . Then (4) can be rewritten as

supfF Cn (f ) - C (f ) = supgG n-1

n i=1

g

(Wi)

-

Eg

(W1)

.

Note that the class G is uniformly bounded by  (). Besides, if H is a class of measurable functions, then G is also a class of measurable functions, by measurability of F.

As the Wi's are not i.i.d, we propose to use the blocking technique developed in [12, 14] to
construct i.i.d blocks of observations which are close in distribution to the original sequence W1, . . . , Wn. This enables us to work on the sequence of independent blocks instead of the
original sequence. We use the same notation as in [12]. The protocol is the following. Let (bn, µn) be a pair of integers, such that

(n - 2bn)  2bnµn  n.

(5)

Divide the segment W1 = (X1, Y1) , . . . , Wn = (Xn, Yn) of the mixing sequence into 2µn blocks of size bn, followed by a remaining block (of size at most 2bn). Con-

sider the odd blocks only. If their size bn is large enough, the dependence between

them is weak, since two odd blocks are separated by an even block of the same size

bn. Therefore, the odd blocks can be approximated by a sequence of independent blocks

with the same within-block structure. The same holds if we consider the even blocks.

Let (1, . . . , bn ) , (bn+1, . . . , 2bn ) , . . . , (2µn-1)bn , . . . , 2µnbn be independent blocks

such that jbn+1, . . . , (j+1)bn =D Wjbn+1, . . . , W(j+1)bn , for j = 0, . . . , µn - 1.

For j = 1, . . . , 2µn, and any g  G, define

Zj,g :=

jbn i=(j -1)bn +1

g

(i)

-

bnEg

(1)

,

Z~j,g :=

jbn i=(j -1)bn +1

g

(Wi)

-

bnEg

(W1)

.

Let Oµn = {1, 3, . . . , 2µn - 1} and Eµn = {2, 4, . . . , 2µn}.

Define Zi,j (f ) as Zi,j (f ) := -f (2j-2)bn+i,1 · (2j-2)bn+i,2, where k,1 and k,2 are respectively the 1st and 2nd coordinate of the vector k. These correspond to the

Zk(f ) = -f (Xk) Yk for k in the odd blocks 1, ..., bn, 2bn + 1, ..., 3bn, ....

4.2 Proof sketch of Lemma 1

A. Working with Independent Blocks. We show that

E sup
gG

1 n

n

g (Wi)-Eg (W1)

i=1

 2E sup
gG

1 n

Zj,g + ()

jOµn

µnW

(bn)+

2bn n

.

(6)

Proof. Without loss of generality, assume that Eg (W1) = Eg (1) = 0.

Then, E supg

1 n

n i=1

g

(Wi)

= E supg

1 n

Oµn Z~j,g + Eµn Z~j,g + R , where R

is the remainder term consisting of a sum of at most 2bn terms. Noting that g 

G,

|g|



 (),

it

follows

that

E

supg

|

1 n

n i=1

g

(Wi

)

|



E(supg

|

1 n

Oµn Z~j,g|) +

E(supg

|

1 n

Eµn

Z~j,g |)

+

()(2bn n

)

.

We

use

the

following

intermediary

lemma.

Lemma 3 (adapted from [15], Lemma 4.1). Call Q the distribu-

tion of (W1, . . . , Wbn , W2bn+1, . . . , W3bn , . . .) and (1, . . . , bn , 2bn+1, . . . , 3bn , . . .). For any measurable

Q the distribution of function h on Rbnµn with

bound H, |Qh (W1, . . .) - Qh (1, . . .) |  H (µn - 1) W (bn) . The same result holds

for (Wbn+1, . . . , W2bn , W3bn+1, . . . , W4bn . . .).

Using this with h(W1, . . .) respectively, and noting

=

supg

|

1 n

that H =

Oµn Z~j,g| and h(Wbn+1  () /2, we have

,. E

. .) = supg |

supg

|

1 n

1n

n i=1

g

Eµn
(Wi)

Z~j,g | |

E supg
As the

|

1 n

Zj,g

Oµn Zj,g | +
's from odd

() 2
and

µnW

(bn) +

E

supg

|

1 n

Eµn

Zj,g| +

() 2

µn

even blocks have the same distribution, we

W (bn
obtain

)

+

()(2bn n

(6).

)

.

B. Symmetrization. The odd blocks Zj,g's being independent, we can use the standard symmetrization techniques. Let Zj,g's be i.i.d. copies of the Zj,g's. Let Zi,j(f )'s be the corresponding copies of the Zi,j(f ). Let (i) be a Rademacher sequence, i.e. a sequence of independent random variables taking the values ±1 with probability 1/2. Then by [16], Lemma 6.3 (Proof is omitted due to space constraints), we have

E sup
g

1 n

Zj,g

jOµm

 E sup
g

1 n

j

jOµn

Zj,g - Zj,g

.

(7)

C. Contraction Principle. We now show that

E sup
gG

1 n

Zj,g

jOµn

 2 · bn () E sup
f F

1 n

µn

j Z1,j (f ) .

j=1

(8)

Proof. As Zj,g =

bn i=1

(Zi,j (f )),

and

the

Zi,j (f )'s

and

Z

i,j (f )'s

are

i.i.d.,

with

(7)

E supg

1 n

2bnE supg

1 n

jOµµnn j=1

Zj,g

 E supg

1 n

j ( (Z1,j(f ))-1)

µn j=1

j

bn i=1

. By applying

 (Zi,j (f )) -  Zi,j (f ) the "Comparison Theorem",

 The-

orem 7 in [17], to the contraction  (x) = (1/ ()) ( (x) - 1), we obtain (8).

D. Maximal Inequality. We show that there exists a constant c1 > 0 such that

E sup
f F

1 n

µn

j Z1,j (f )

j=1



c1µn n

.

(9)

Proof.

Denote (h1, . . . , hN ) by hN1 .

One

can

write

E supfF

|

1 n

µn j=1

j

Z1,j

(f

)|

=

1 n

E

supN 1

suphN1 HN

sup1,...,N

|

µn j=1

N k=1

kj (1,j),2hk

(2j -2)bn +1,1

|. Since

(2j-2)bn+1,2 and (2j -2)bn+1,2 are i.i.d. for all j = j (they come from different blocks),

and (j ) is a Rademacher sequence, then j (2j-2)bn+1,2hk (2j-2)bn+1,1 j=1,...,µn

has the same distribution as j hk (2j-2)bn+1,1 j=1,...,µn . Hence

E sup
f F

1 n

µn

j Z1,j (f )

j=1

=

1 n

E

sup
N 1

sup
hN1 HN

sup
1 ,...,N

µn N
j khk
j=1 k=1

(2j -2)bn +1,1

.

By the same argument as used in [4], p.53 on the maximum of a linear function over a convex polygon, the supremum is achieved when k = 1 for some k. Hence we get

E supfF

1 n

µn j=1

j

Z1,j

(f

)

=

1 n

E

suphH

µn j=1

j

h

(1,j),1

. Noting that for all

j = j , h((2j-2)bn+1,1) and h((2j -2)bn+1,1) are i.i.d. and that Rademacher processes are sub-gaussian, we have by [18], Corollary 2.2.8

1 n

E

sup
hH

µn
j h
j=1

(2j -2)bn +1,1



1 n

E

sup
hH{0}

µn
j h
j=1

(2j -2)bn +1,1



c µn n


(log sup N (
0P

, 2,Pn , H  {0}))1/2d

,

where c is a constant and N ( , 2,Pn , H  {0}) is the empirical L2 covering number.

As H has finite VC-dimension (see Assumption 1.III), there exists a positive constant

w such that supP N ( , 2,Pn , H  {0}) = OP ( -w)(see [18], Theorem 2.6.1). Hence

 0

(log supPn

N

(

, 2,Pn , H



{0}))1/2d

< . and (9) follows.

E. Establishing

E supgG

1 n

(2). Combining (6),(8),

n i=1

g

(Wi)

-

Eg

(W1

)

and4b(n9),we(h)avc1enµn

+

 ()

µnW

(bn)+

2bn n

.

Take bn = nb, with 0  b < 1. By (5), we obtain µn  n1-b/2. Besides, as we assumed that the sequence W is algebraically -mixing (see Definition 2), W (n) = O (n-r ).

Then µnW (bn) = O n1-b(1+r) , and we arrive at (2).

4.3 Proof Sketch of Lemma 2

A. Working with Independent Blocks and Symmetrization. For any b  [0, 1),  

(0, 1 - b), let

n = 3(2c1 + n/2) ()/n(1-b)/2.

(10)

PWegsusGhpown1

n
g (Wi)-Eg (W1)
i=1

>

n  2P

sup
gG

1 n

Zj,g

jOµn

>

n/3

+O(n1-b(1+r ) ).

(11)

Proof. By [12], Lemma 3.1, we have that for any n such that ()bn = o(n n),

P

supgG

1 n

n i=1

g

(Wi)

-

Eg (W1)

>

n



2P

supgG

1 n

jOµn Zj,g >

n/3 + 4µnW (bn). Set bn = nb, with 0  b < 1. Then µnW (bn) = O(n1-b(1+r))

(for the same reasons as in Section 4.2 E.). With n as in (10), and since Assumption 1.II implies that  ()  () - 1, we automatically obtain ()bn = o(n n).

B. McDiarmid's Bounded Difference Inequality. For n as in (10), there exists a constant c2 > 0 such that,

P

sup
gG

1 n

Zj,g

jOµn

>

n/3

 exp(-4c2n).

(12)

Proof. The Zj,g's of the odd block being independent, we can apply McDiarmid's bounded

difference inequality ([19], Theorem 9.2 p.136) on which depends of Z1,g, Z3,g . . . , Z2µn-1,g. Noting

the that

function

supgG|

1 n

changing the value

of

jOµn Zj,g | one variable

does not change the value of the function by more that bn () /n,we obtain with bn = nb

that for all > 0,

P

supgG

1 n

jOµn Zj,g

> E supgG

1 n

jOµn Zj,g +

 exp

-4 2n1-b ()2

.

Combining (8) and (9) from the proof of Lemma 1, and with bn = nb, we have

E supgG

1 n

jOµn Zj,g  2 () C/n(1-b)/2. With = n/2 ()/n(1-b)/2, we

obtain n as in (10). Pick 0 such that 0 < 0 < . Then, since  ()  () - 1, (12)

follows with c2 = (1 - 1/(0))2.

C. Establishing (3). Combining (11) and (12) we obtain (3).

4.4 Proof Sketch of Theorem 1

Let f¯ a function in F minimizing C. With fn = f^nn , we have

Since

C n

(nfn)  ,

- C = (Cn (f^nn ) - Cn (f¯n )) the second term on the right-hand

+ (inffnF C side converges

(f ) to

-C zero

). by

Assump-

tion 1.III. By [19], Lemma 8.2, we have Cn (f^nn ) - Cn f¯n  2 supfF |Cn (f ) -

Cnn (f ) |. By Lemma 2, supfF |Cn (f ) - Cnn (f ) |  0 with probability 1 if, as

n  , n (n) n(+b-1)/2  0 and b > 1/(1 + r). Hence if Assumption 1.IV

holds, C (nfn)  C with probability 1. By [4], Lemma 5, the theorem follows.

References
[1] Schapire, R.E.: The Boosting Approach to Machine Learning An Overview. In Proc. of the MSRI Workshop on Nonlinear Estimation and Classification (2002)
[2] Friedman, J., Hastie T., Tibshirani, R.: Additive logistic regression: A statistical view of boosting. Ann. Statist. 38 (2000) 337­374
[3] Jiang, W.: Does Boosting Overfit:Views From an Exact Solution. Technical Report 00-03 Department of Statistics, Northwestern University (2000)
[4] Lugosi, G., Vayatis, N.: On the Bayes-risk consistency of boosting methods. Ann. Statist. 32 (2004) 30­55
[5] Zhang, T.: Statistical Behavior and Consistency of Classification Methods based on Convex Risk Minimization. Ann. Statist. 32 (2004) 56­85
[6] Gyo¨rfi, L., Ha¨rdle, W., Sarda, P., and Vieu, P.: Nonparametric Curve Estimation from Time Series. Lecture Notes in Statistics. Springer-Verlag, Berlin. (1989)
[7] Irle, A.: On the consistency in nonparametric estimation under mixing assumptions. J. Multivariate Anal. 60 (1997) 123­147
[8] Meir, R.: Nonparametric Time Series Prediction Through Adaptative Model Selection. Machine Learning 39 (2000) 5­34
[9] Modha, D., Masry, E.: Memory-Universal Prediction of Stationary Random Processes. IEEE Trans. Inform. Theory 44 (1998) 117­133
[10] Roussas, G.G.: Nonparametric estimation in mixing sequences of random variables. J. Statist. Plan. Inference. 18 (1988) 135­149
[11] Vidyasagar, M.: A Theory of Learning and Generalization: With Applications to Neural Networks and Control Systems. Second Edition. Springer-Verlag, London (2002)
[12] Yu, B.: Density estimation in the L norm for dependent data with applications. Ann. Statist. 21 (1993) 711­735
[13] Doukhan, P.: Mixing Properties and Examples. Springer-Verlag, New York (1995)
[14] Yu, B.: Some Results on Empirical Processes and Stochastic Complexity. Ph.D. Thesis, Dept of Statistics, U.C. Berkeley (Apr. 1990)
[15] Yu, B.: Rate of convergence for empirical processes of stationary mixing sequences. Ann. Probab. 22 (1994) 94­116.
[16] Ledoux, M., Talagrand, N.: Probability in Banach Spaces. Springer, New York (1991)
[17] Meir, R., Zhang, T.:Generalization error bounds for Bayesian mixture algorithms. J. Machine Learning Research (2003)
[18] van der Vaart, A.W., Wellner, J.A.: Weak convergence and empirical processes. Springer Series in Statistics. Springer-Verlag, New York (1996)
[19] Devroye, L., Gyo¨rfi L., Lugosi, G.: A Probabilistic Theory of Pattern Recognition. Springer, New York (1996)

