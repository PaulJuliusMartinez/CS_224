Correspondence Analysis, with Special Attention to the Analysis of Panel Data and Event History Data Author(s): Peter G. M. van der Heijden and Jan de Leeuw Source: Sociological Methodology, Vol. 19 (1989), pp. 43-87 Published by: American Sociological Association Stable URL: http://www.jstor.org/stable/270948 Accessed: 23/04/2009 20:45 Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/action/showPublisher?publisherCode=asa. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. We work with the scholarly community to preserve their work and the materials they rely upon, and to build a common research platform that promotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org.
American Sociological Association is collaborating with JSTOR to digitize, preserve and extend access to Sociological Methodology.
http://www.jstor.org

Correspondence Analysis, with Special Attention to the Analysis of Panel Data and Event History Data
Peter G. M. van der Heijden* and Jan de Leeuwt
Wepresent correspondence analysis as an exploratory method that uses graphical representationsto study the relation between rows and columns of a two-way table with non-negative entries. Wepresent multiplecorrespondence analysis (MCA) as ordinary correspondence analysis of a so-called superindicator matrix. In this matrix, objects (e.g. persons) are in the rows, and each category of each variable has a separate column. MCA uses only the bivariate marginal frequencies to derive a representation for the columns. Therefore, it can handle data sets with many variables with many categories. We give special attention to panel data and event history data. We show how these types of data can be coded in three-way superindicator matrices with objects in the rows, categories of the variables in the columns, and time points in the layers.
This paper is a rewritten and adapted version of some chapters in van der Heijden's Correspondence Analysis of Longitudinal Categorical Data (D.S.W.O. Press, Leiden, 1987). For helpful comments, we are indebted to three anonymous reviewers, and we are grateful to Henriette Meerens for kindly providing us with her data. This research was partially supported by a grant from the Netherlands Organization for the Advancement of Pure Research (Z.W.O.) to the first author.
*University of Leiden tUniversity of California at Los Angeles
43

44 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
To analyze these data with MCA, we construct two-way codings. We discuss the implications of choosing specific twoway codings for the results of MCA. We compare Markov chain modeling of panel data with statisticalmethodsfor event history data. We show that MCA can easily handle data sets with many variables, categories, and time points and that it can be used to study individual differences, unlike most statistical approaches, in which persons are usually treated as replications. MCA is a veryflexible exploratory tool of data analysis, and much research in this area is needed.
1. INTRODUCTION
In the last 15 years, there has been a growing interest in correspondenceanalysis (CA), a tool for the analysisof categorical data. CA has quite a long history (de Leeuw 1983). Since its development in 1935, it has been reinvented several times under different names. These approachesare formally identical, but their objectives, rationales, and procedures can be quite different. CA (also called simple CA) is intended for the analysis of two-way tables, and it is (formally)identicalto reciprocalaveraging,canonical correlation analysis, and simultaneouslinear regressions(see, e.g., Nishisato 1980; Greenacre 1984). Multiplecorrespondenceanalysis (MCA) is a version of CA that is meant for the analysis of more than two variables; it is also known as homogeneity analysis and the quantificationmethod (see also Tenenhaus and Young 1985). CA and MCA are considered the same in optimal scaling (Bock 1960)and in dual scaling(Nishisato 1980). The term correspondence analysis originated in France, where it is very popular. Compared with other approaches, CA places a heavy emphasison geometrical representations, which is probablyone of the reasons it became so popular, at least in France.
CA can handle many different types of data, such as pairedcomparison, ranking, rating, and sorting data (see, e.g., Nishisato 1986). Data are coded into an appropriate two-way matrix (not necessarily a contingency table) so that CA of this matrix reveals some important aspects of the original data. The matrix is approximated in a least squares sense by a matrix of lower rank. This lower-rank approximation is studied in graphical

CORRESPONDENCE ANALYSIS

45

representations. Such representations show, among other things, the relation between the row and column entries of the matrix. In
many applications the rows are individuals, and MCA gives a representationof individualdifferences;i.e., it shows how individuals differ in their relation to the columns. CA also finds optimal quantificationsfor the rows and columns of the matrixunder study.
In this paper we will focus upon CA of longitudinal data. We speak of longitudinaldata if one or more objects or phenomena are observed more than once using one or more variables. Our objective is to show how CA can be used to analyze longitudinal data and to compare it with more common approachesfor analyzing these data. For discrete time, many models can be formulated as loglinear models (see Bishop, Fienberg, and Holland 1975; Plewis 1985). For continuous time, we find statistical models for the analysis of event histories (see Tuma and Hannan 1984; Allison 1984). In both the discrete and the continuous time approaches, the number of categories of the variable(s) under study may not be too large because of empty-cell problems. We will show that CA can easily deal with variables having a very large number of categories. We will concentrate on the analysis of panel data and event history data.

2. CORRESPONDENCE ANALYSIS OF CROSSTABLES

We will introduce correspondence analysis as a method for the analysis of crosstables. For details and proofs, we refer to the standardworks of Nishisato (1980), Gifi (1981), Greenacre (1984), and Lebart, Morineau, and Warwick(1984). Considerthe crosstable
P, having values pj,, where i (i=1, ... , I) indexes the I rows, j
(=1, . ... , J) indexes the J columns, and pijRO.We denote the margins as pi, and p+jfor the rows and columns respectively (a
"+" replaces an index when summed over the corresponding variable). In principle we can use CA to analyze any two-way matrix with non-negative entries. One type of crosstable for which CA is particularlysuited is a contingency table. In this section we will assume that P is a table with proportions that add up to one: p+ +=1. At the end of this section, we will discuss what tables can
be analyzed with CA.

46 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
CA gives a graphical representation of P. In this representation, each row and each column of P has a separate point, and the configurationof points tells us what is going on in the data. To understandwhat is going on, we must know how distances between points relate to aspects in the data. The distance between two row points i and i' is a function of the differences between so-called profiles of these rows. The profile for row i is defined as the vector of conditional proportionspijlp,+.The differences between the two profiles are weighted by (1/p+i), thus bringingdown the influence of the better-filled columns:

82(i,i) = >(1/p+j) (pijpi+-pi

pi )2.

(1)

Distances 8(i,i') are called chi-squared distances. When 8(i,i') is large, the profilesof rows i and i' differ much;whereaswhen 8(i, i') is small, the profiles of row i and i' are similar, and we conclude that i and i' are related in the same way to the columns. The chisquared distances between the rows show the dependencein matrix P: When the matrix is independent, all row profiles are identical
and equal to the average row profile with values p+j/p ++=p+j, i.e., the marginalcolumn proportions.
We want to find a representationof the row points in lowdimensional euclidean space. We can obtain a solution as follows. Without loss of generality, let I>J. First define Dr and D, as diagonal matrices with marginal row and column proportionsp,+
and p+j respectively. Now D 'P is the matrix with row profiles. The I rows can be plotted as points in a J-dimensional space by using row vectors of D 'P as coordinates. By defining a weighted
euclidean metric Dc' for this space, we get chi-squared distances between the rows (see Greenacre 1984 for more details). We can center this configuration of row points by subtracting from each row profile the average row profile with values p+j. This can be done by using the matrixDr'(P-E) as coordinates, where E is an independent matrix: e,i=pi+p+j. Dr1E is a matrix with (identical) profiles of the column totals. By this centering, the rows span a (J-1)-dimensional subspace in the J-dimensional space, and the center of this space can be interpretedas the point for the average row profile.

CORRESPONDENCE ANALYSIS

47

Similarly, we can plot the J columns as points in an (I-1)dimensional space with a weighted euclidean metric defined by D7' by using D'l (P-E)T as a matrix with coordinates. Since we assumed that I>J, and the columns are centered, the column points span a (J-1)-dimensional subspaceof the I-dimensionalspace. The center can be interpreted as the point for the average column profile. In this way we can also study the dependence in P from the representation of the column points.
This study of the space of the rows and the columns is simplified greatly when a large part of the distances is displayed in a low-dimensional space. In CA this is accomplished as follows. First, weights are defined for the row and column points by pi, and p+j respectively. Then, new axes are defined in such a way that weighted squared projected distances to these axes are maximized for subsequent dimensions. If we denote the row coordinates in this new coordinate system as ri, for row i on dimension a, A2= pripi+f2 is maximized for dimension 1, then A2
= lipi+r2 is maximized for dimension 2, and so for further dimensions;the coordinateson distinctdimensionsare uncorrelated.
The same criterion is used for the columns. The weighting by pi, is performed so that categories with large marginal proportions have more influence on the determination of the new axes. The
new axes are found for the rows and columns simultaneously
by performing a generalized singular value decomposition (see Greenacre 1984 for details). The generalized singular value decomposition is performed of

Dr,(P-E)Dc' = RACT,

(2)

where A is a diagonal matrix with (J-1) singular values Aa in decreasing order; RTDrR = I = CTDcC; R is of order Ix(J-l) and
has elements ri for category i on dimension a; C is of order Jx(J-1) and has elements cja for category j on dimension a. Euclidean distances between the rows are chi-squareddistances if scores R = RA are used as coordinates, and euclidean distances between the columns are chi-squared distances if scores C = CA are used as coordinates. By rewriting (2) using RTDrR = I =
CTDcC, we can find

R = RA = D,'PC,

(3a)

48 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW

C = CA = D T'PTR,

(3b)

since D- 'EC = 0 = D-'ETR. Equations (3a) and (3b) are known as the transitionformulas. Equation (3a) shows that row scores R can be derived from the column scores C by placing the row points in the weighted averages of the column points, where weights are defined by profile elements. Equation (3b) shows that column scores C can be derived from the row scores R by placing the column points in the weighted averages of the row points. Thus, joint plots of row and column points are sometimes constructed with coordinates (R,C) or (R,C). If the first choice is used, only the distances between row points in full-dimensionalspace are chisquared distances; if the second choice is used, only the distances between columnpointsarechi-squareddistances.Another possibility is to use the pair (R,C) as coordinates, so that both distances between rows and distances between columns are chi-squared distances. However, the disadvantage of this is that the singular values are used twice. This disadvantage is solved in the last possibility by using (RA1/2, CA"12),but then neither the distances between the rows nor the distances between the columns are chi-
squared distances. The transition formulas (3a) and (3b) are a third way to
understandhow CA displays the dependence in P: When for row i the profile value p,l/pi is larger than the average profile value p j, row i "pulls"more than average on j, causingj to be placed nearer to i. By multiplying both values by pi,, we find that i is nearerto j whenPi,>pi+p+i = eij,i.e., when the observed proportion for cell (i,j) is largerthan the independentproportion.Independence can be seen to be baseline model in CA: When P is independent, all rows and columns fall into the origin, and the singular values
are all zero.
Interpretation of geometrical CA solutions is often made easier by studyingtables of contributionsof dimensions and points. In these contributions, the squared singular values play a crucial role. First, it can be proved that the singular values relate in the following way to the Pearson chi-square statistic X2 for testing
independence:

trace A2 X-l/n,

(4)

CORRESPONDENCE ANALYSIS

49

where n is the sample size. This shows that CA splits the dependence in the matrixinto a numberof dimensions. The proportiondisplayed on dimension a is equal to AX/ZA2,. Second, for each dimension a, A2 = ipi+ra = Zjp+ca, so the squared singular value can for each dimension be split up over the rows and columns by studying proportions pi+rJi2liP i+ri2 and p+J2,,1lp+cj2, respectively. These proportions add up to one for each dimension. A third type of contribution can be derived for a specific point by dividing its squared projected distance to the origin on a specific dimension by its total squared distance to the origin in full-dimensional space: Values ria/Z-aria show how good point i is represented on
dimension a.
Another way to make interpretation easier is to use supplementary information (if available) on the rows or columns of the crosstable (see, e.g., Greenacre 1984). This supplementary information is fitted into the CA solution as a second step, after this solution has been derived from the crosstable. Consider a
matrix P of order IxJ and a supplementarymatrix S of order LxJ with extra information on the columns. Now, the L row profiles of S, contained in D 'S, are fitted into the solution with an equation similar to the transition formula (3a):

Rs= D-ylSCA ',

(5a)

where R, contains the coordinates of the L supplementary rows. The position of these supplementary points can give us further understandingof the configurationof the column points, from which they are derived. Similarly, we can derive coordinates for L* supplementary columns using

Cs= DS7'RA-',

(5b)

where Dc STis a matrixof orderJx L* that contains the L* profiles
of supplementary columns. So far, we have presented CA as a tool for making graphical
representationsof the dependence in a crosstable. It can be proved (see, e.g., de Leeuw 1973; Nishisato 1980; Greenacre 1984) that CA is formally identical to canonical analysis of contingency tables (see Kendall and Stuart 1967, ch. 33), an approachthat emphasizes

50 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW

the quantificationof the categories of a two-way contingency table. For canonical analysis, we can rewrite (2) as

P = E+DrRAC7Dc = Dr(l + RACT)D,

(6)

which is also known as the reconstitutionformula. Now, the first column of R gives optimal scores for the row categories, the first column of C gives optimal scores for the column categories, and the first singularvalue A/ gives the maximized correlationbetween the optimally scored row and column variables. The second column of R (and C) gives optimal scores under the restriction that they are orthogonal to the first column of R (and C), and these optimal scores give the second maximized correlation X2, and so on for further dimensions.
There is a large interest in the relation between CA and other techniques for the analysis of categorical data. Goodman (1981) found that when the frequencies are derived from an underlying bivariate normal distribution (or a distribution that is bivariate normal after a suitable transformationof the rows and
columns), the scores in the firstcolumnof R andC are approximately the same as the scores in the rows and columns of the log-
multiplicative RC association model. Goodman (1985, 1986) discusses this in more detail and also introduces forms of CA as a
model (see also Gilula and Haberman 1986). In our own work (van der Heijden and de Leeuw 1985), we have given more attention to the interpretation of (nonstatistical) CA as a tool for the analysis of residuals from independence, an aspect that is clearly seen in equation (6). Escofier (1984) extends CA by usingthe generalization

P = Q + SrRACTS,

(7)

where Q is a matrix with estimates of expected proportions under
some model that is less restrictive than independence, and Sr and
S. are diagonal matrices that do not necessarily consist of the marginsof P. Thus, in van der Heijden and de Leeuw (1985), we use CA for the analysisof residualsfrom loglinearmodels in higherway tables. We start by coding higher-way tables into two-way tables by stackingthe categories of the originalvariables(i.e., when two of the original variables have 2 and 3 levels, the new row variable has 2x3=6 levels). It is easy to see that in this way the values of the matrixE are equal to estimatesof expected frequencies

CORRESPONDENCE ANALYSIS

51

for the loglinear model in which the set of row variables are
independent of the set of column variables. Instead of the independent matrixE, one could choose, for example, a conditional independence model and decompose residuals with (7). Other examples of this approach use quasi-independence, symmetry, and quasi-symmetrymodels. Overviews of this approach can be found in van der Heijden (1987) and van der Heijden, de Falguerolles,
and de Leeuw (1989).
Goodman's approach to CA as a model and our residual analysis approach to CA are most useful when the table to be analyzed is a contingency table with frequencies derived under Poisson or (product-)multinomialsampling. But CA is also useful for many other types of tables, for example, the Burt table and the superindicatormatrix, to be discussed in the next section. Another table will be analyzed as an example. Generally speaking, CA is a
useful technique when the chi-squareddistance is a useful measure for the (dis)similaritybetween the rows or between the columns of
the matrix under study.

2.1. Example

For an example, we will analyze the matrix displayed in Table 1. It is a matrix of order 18x(5x5), having hours from 6:00 A.M. until midnight in the columns and the joint behavior of husbands and wives in the rows. In the cells, we find the total
number of minutes in an hour that the husbands and wives in 326
couples spent at home (H), at work (W), travelling (T), shopping (S), and in other activities (O). The column totals all equal (326x60), because each of the 326 couples spends 60 minutes in an hour. The husband-wife pairs come from the National Travel Survey of 1980 conducted by the Dutch CentralBureau of Statistics (Moning 1983). From this survey, we selected the 326 couples in which both partners worked more than 25 hours a week and in which both kept diaries of their weekday activities. For more details, see van der Heijden (1987).
Table 1 tells us the specific hours in which activities were performed. We use CA to study this. In CA, each column profile is compared with the average column profile, having values 1/18. Each row profile is compared with the profile of the column

TABLE 1 The Number of Minutes in Each Hour that 326 Couples Spend in Various
6 7 8 9 10 11 12 13 14 15 16 17 1

HH HW HS0 HT HO WH WW WS WT WO SH SW
SS ST SO0 TH TW TS TT TO OH OW OS OT0 00

17,345 11,013 865 2,377 0 820 1,787 10 39 120 337 60 1,118 0 20 585 (0 00 0 00 (0 ( 170 883 0 416 (0 15(1 96)0 ( 0 30 ( 15
O( (
00

4,226 3,280
0 466 9( 1,463 6,773
0 704 5()
( ( 0
643 1,156
629 (
30 5()
0 (

3.027 2,262
0 146 60
1,927 10,547
0 4(X) 120 25 55 35
( ( 97 429 0 175 0
13() 11()
( 15 (

2,261 2,191 3.414 1,970 1.896 2,365
70 1() 25 119 187 284 20 0 60 1,625 1,527 1.632 11,178 11,354 8,186 46 82 50 326 393 556 240 248 397 222 231 305 195 185 326 30( 15 85 10) 50 31
00 128 133 282 330 3(X) 828
5 160 163 193 1( () 25 65 ( 43 438 455 284
(9 30) 15 26 75 125 149

3.279 2,704 2,556 3,896 2,656 2,552 2,478 2,040
( 905 240() 314 330() 985 25 35 65 284
1.530() 1,720() 1,681 1,273 8.474 9,483 8,740 5,279
6() 170 155 75 621 287 409 1,221 165 264 305 19( 140() 19( 198 164 410() 453 542 33() 60 52 85 221 35 50 75 173
5 0 30() 180 1(11 115 597 829 482 7()08 963 25 25 15 22 395 203 352 866 26 1 22 171 50 55 43 15() 288 345 474 429
( 2( ( 6 2() 22 44
67 25 16() 176

9,378 13,548

1.393 1,255 137 1

1,379 1,002 348 506

1,177 498

1,272 486

37

451 31

55 0

320() 144 155 198

120 20 5 0

25 0

754 644

431 175

2

1,050 352

1()2 24 174 252

178 135

2() 1(

97 7( 283 417

Totals 19,560 19,56( 19,56() 19.560( 1',548* 19,560 19,56() 19,560 19,56() 19,56() 19,560 19,56() 19,56
Note. Bivariate categories are in the rows, hours (starting with the hour from 6-7 A.M.) ar H = at home, W = at work, T = travelling, S = shopping, and 0 = other activities. Each joint acti the behaviorof the wife, the second the behaviorof the husband,
*12 minutesmissing.

CORRESPONDENCE ANALYSIS

53

margins. Notice that this latter marginshows us that males generally spend more time working, whereas women spend more time shopping, etc. CA of Table 1will not show this, since it concentrates on the departure from this margin.
CA gives eigenvalues Al = 0.47 (76 percent of chi square), 2 = 0.06 (10 percent), and A3= 0.04 (0.06 percent). To decide upon the number of dimensions that we are going to study, we can use the elbow or scree criterion, known from dimension-reduction
techniques like factor analysis. It tells us that we can restrict attention to the first dimension only. Results are summarized in Figure 1. The hours are presented horizontally and the quantifications on the firstdimension are presented vertically.The activities are represented on the vertical line on the left, and the hours are
represented by points that are connected for adjacent hours. The morning hours 6, 7, and 8 (i.e., from 6:00 A.M. until 9:00 A.M.) and the evening hours 18 to 23 are quantifiednegatively; the other hours are quantified positively. Thus, in the morning and evening, the couples perform activities different from the activities they perform in the hours between.
The quantifications of the row categories show what these activities are. We use the transition formulas (3a) and (3b). To interpret the row categories, it is helpful to study the contributions of these points to the first dimension. This shows contributions of 0.38 for HH (both partners at home) and 0.43 for WW (both partners at work). Thus, more than 80 percent of the chi square displayed on this dimension stems from the fact that rows HH and WW depart from the average, being the profile with values 1/18. We now understand the quantificationof the hour points: In the morning and in the evening, husbandsand wives are both at home more than the average for the whole day, whereas during working hours, they are both at work more than the average for the whole day. The time-point quantifications, which all have about equal contributions, are determined mainly by these two points. There is a dip duringlunch time because some persons go home then. Other states do not contributemuch. Only the state in whichboth partners do other things (00) takes account of another0.04 and is quantified very negatively, since these activities are performed more than average in the evening. This does not mean that we should not interpret the position of the other states; they do not contribute

54 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW

WS ---WW

'-

0 C

-sw E

'I

c-wowh o
tw X

wt -

-ts I

et
-/ ow _sh

cr

hw -ss

-hs

-tt
th
=SO
-to _ht
-OS
-ot
-hh -oh -ho -oo

7 9 11 13 15 17 19 21 23 hours startingfrom 6.00 AM
FIGURE 1 Analysis of data in Table 1. Quantifications on first dimension for the rows and columns. The J activity quantifications are displayed on the vertical line on the left; the P period quantifications are connected by a solid line on the right.
much because they have a lower mass than the categories HH, WW, and 00. For example, the state in which one partner shops while the other works (WS and SW) occurs more than average duringworking hours, whereas shopping together (SS) occurs more than average after working hours but before the evening (in the Netherlands most shops close at 6:00 P.M.). Furthermore, discrepanciesbetween "opposed"states such as HT and TH can be

CORRESPONDENCE ANALYSIS

55

studied: This shows, for example, that in the morning, women stay at home somewhat longer than men and that in the evening, they are at home again earlier. These latter interpretations are more
speculative, because these points have little influence on the
solution, and it is possible that higher dimensions lead us to a
different interpretation.Therefore, it is generallyadvisableto check conclusions like these in the data or to study whether these points have large contributions on higher dimensions. CA is used here to
obtain only a global idea of the most important departure from
independence (76 percent is displayed). Higher dimensions show mainly unstructuredpeculiarities in the departure.
Note that it would have been unusual to study Table 1 with loglinear analysis, because the assumption of independent observations (minutes) is clearly violated. If the observations in Table 1 were independent, CA could be interpreted as giving a representation of the residuals from the loglinear model in which hours are independent of the joint behavior of the couples (see above, and van der Heijden and de Leeuw 1985).

3. MULTIPLE CORRESPONDENCE ANALYSIS

There are many ways to introduce multiple correspondence analysis (MCA). Standardreferences are Benzecri (1973), Nishisato (1980), Gifi (1981), Greenacre (1984), and Lebart et al. (1984). Here we will discuss three ways. First, we will introduce MCA as CA of a so-called indicatormatrix.In this context we will emphasize a chi-squared distance interpretation of MCA. Second, we will show that MCA can be interpretedas principalcomponents analysis (PCA) of categorical data. Here the quantification interpretation of MCA will be stressed. Third, we will introduce MCA as CA of a so-called Burt matrix. This will make it easier to relate MCA to the decomposition of chi square and to loglinear analysis.
3.1. CorrespondenceAnalysis of a SuperindicatorMatrix
Data are very often coded into a matrixwith objects as rows and variablesas columns. The objects can be, for example, persons, schools, countries, or household units. The variablesdenote aspects on which the objects are measured. We will assume that the

56 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
TABLE 2 A SmallExampleof a CategoricalDataMatrix(PanelA) andits Superindicator
Matrix(Panel B)
Panel B

Panel A

abcpqr uvw

aqw

arw

crv

bqu

ar w

bp
cr

u
w

cp v cqw

ap v

1000 1000 1 10000 100 1 00 10 100 10 0 100 10 100 10000 10 0 1 0 10 100 100
0 0 10 0 10 0 1
00 11000 10
00 10 1000 1 10 0 1000 10

Note. There are ten objects (rows), and three variables, each with three categories.

resulting measures are categorical. In Table 2, panel A, we present an example of a data matrix for ten persons and three variables. The entries of the matrix, X, are simply letters: i.e., a, b, c for
variable 1, p, q, r for variable 2, and u, v, w for variable 3. The
matrix has columns Xm (m=l, .. ., M) and elements xim, where i (i=, . . ., I) indexes the n rows. We can transform each column
(variable) Xm into a so-called indicator matrix G'" with elements g', where gT = 1 if object i falls into category j (/=1, . .., J) of variable m, and g' = 0 otherwise. Variable m has km categories. By concatenating the M indicator matrices G"' horizontally, we get a so-called superindicator matrix, denoted by G. G has n rows and k = Emkm columns. In our example, the matrix G has nine columns (see Table 2, panel B). CA of G is known as MCA. We will now apply this by discussing the properties of CA of G.
CA gives a graphical representation of both row profiles and column profiles of a matrix. So the n row profiles in Dr'G are plotted as points in a k-dimensional space. Notice that because each row in G has m Is, Dr = I/m, and a row profile is a vector with m values equal to 1/m, the other values being 0. Also, all row profiles

CORRESPONDENCE ANALYSIS

57

have identical weights. The metric of this space is defined by D,.',
where D,.has diagonal elements (E2g.i)/lnm, /ig7bi eing the marginal frequency of category j of variable m, and nm being the total number of Is in the superindicatormatrix. Two row points will be near each other when they have similar profiles, i.e., when the objects fall into the same categories of the m variables. The column profilesof G are plotted as points in n-dimensionalspace. Associated with these points are masses (EigT')/nmthat are proportionalto the marginalfrequencies of the categories. Two column profiles will be near each other when there are many objects that fall either into both of these categories or into neither of these categories. The dimensionalityof the subspaces that are spanned by the row points and the column points is min((n-1), (k-m)), where (n-1) is the dimensionality of the centered n-dimensional column space and (k-m) is the dimensionality of the centered row space, since for each variable m, Ejg'i = 1, and therefore centering eliminates m dimensions. Related to this is a special property of the category scores. For each variable on each dimension, they add up to zero when we weight them with the marginal frequencies of the categories: Ejgi,'c'"= 0 for each dimension a and each variable m, where c7j is the quantification of category j of variable m on
dimension (a.
Because G is a binary matrix, the relation between the row points and column points simplifies. Equation (3b) now shows that if the normalization(R,C) is chosen, a column point, i.e. a category, is in the centroidof the objects that fall into that category. Equation (3a) shows that if (R,C) is chosen, an object point is in the average of its categories. In most applications, normalization (R,C) is
chosen.
In most applications, supplementary information on the objects (e.g., sex, place of birth) is also available. When the supplementary information is (made) categorical, we can code it into a (super)indicatormatrix S and apply equation (5b). Thus, we find supplementary category points that are in the average of all objects that fall into these categories (see Greenacre 1984).

3.2. Principal ComponentsAnalysis of CategoricalData
One way to define PCA is as follows. Consider a set of m quantitative variables collected in a matrix Q having columns Q"'.

58 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
Then, the first principalcomponent, R', is the linear combination of the m variables Qll that maximizes the average of the squared correlations A2 = E, (cor(R,Qm))2/m. The average I2 is called the first eigenvalue, and the correlations cor(R1,Qm)are known as component loadings. The second principal component, R2, is the linear combination that maximizes A2 = Em(cor(R2,Qm))2/m under the restriction that it is uncorrelatedwith the first, and so on for
further dimensions.
MCA can easily be fitted into this framework. Consider the matrixX, with categoricalmeasures in the cells (see Table 2, panel A, for an example). Suppose we have analyzedthe table G, derived from X, with CA. This gives us scores collected in R and C and eigenvalues collected in A2. Consider now only the first dimension of CA, i.e., the first column of row scores RI and column scores
C1, where the score of category j of variable m is denoted with cjl. We can use the category scores cj' to derive from the original matrix X a quantifieddata matrix Q by replacingthe categories in X by their correspondingquantificationsin Cl. The relationbetween PCA and MCA is such that these category scores are optimal in the sense that X2 = Zm(cor(Rl,Qm))2/m is maximized. Using C2, we can construct a newly quantified data matrix Q by filling in these category scores, and these category scores are optimal in the sense that X2 = Em(cor(R2,Q,n))2/m is maximized under the restriction that RI is uncorrelatedwith R2, and so on for further dimensions.
This shows that MCA is a PCA for categorical variables that are optimally quantified for each subsequent principalcomponent.
In the description of ordinaryCA, we discussed three types of contributionsthatcould be computedto simplifythe interpretation of the CA solution. The firsttype was the proportionof chi square that was decomposed in each dimension. This type of contribution is less useful in MCA, since the last dimensions of the MCA
solution can be shown to be artifacts due to linear dependencies (see, e.g., Greenacre 1984, ch. 5). Therefore, distances on these last dimensions should not be interpreted.The second type was the decomposition into subsequentdimensionsof the distanceof a point to the origin in full-dimensionalspace. Now that we know that the last dimensions in the MCA solution are artifacts, this type of contribution is not very useful either. Only the third type of contributionis useful: the contributionof individualrow and column

CORRESPONDENCE ANALYSIS

59

points to a dimension. For the rows of G, this contributionis easily derived, because all row weights, defined by D,, are identical. For
the columns of G, it is most useful to sum these contributionsper variable so that it is clear how much a particularvariable (instead of a particular category) contributes to a specific dimension. The
contribution (grj/nm)( j)2/A2 shows the contribution of category j of variable m to dimension a. By adding up over the categories j in variable m, we have the contribution of variable m: lZ(g+j/nm)(j?)2/A2. This quantity is closely related to the squared correlation between the quantified variable m and the row scores for dimension a, since (cor(Ra,Qm))2= Yj(gmj/n)(j)2 (for a proof,
see Tenenhaus and Young 1985).

3.3. CorrespondenceAnalysis of the Burt Matrix

Furtherinsight into MCA can be obtained when we consider it as CA of the so-called Burt matrix B=GTG. Table 3 displays the Burt matrix for our little example. The Burt matrix can be considered a concatenation of all univariatemargins (as diagonal submatrices on the diagonal of the Burt matrix) and bivariate
margins. CA of the matrixG is related to CA of the matrixB=GTG in the following way: Column scores C derived from analysis of G are equal to column (and row) scores C derived from analysis of B. The only difference is in the singularvalues: For these, we have
Aa(B) = A2(G), where Aa(B) is the singular value for the analysis of

TABLE 3 The Burt Matrix for the Example in Table 2
abcpqr uv

w

a 4 0 0 1120 13 b 020 110200 c 004 112022 p 1 1 13 00 120 q 11 10 30 102 r 20 20040 13
u 020110200 v 10 2 20 10 30 w 302023005

60 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
the Burt matrix, and Aa(G) is the singular value for the analysis of the superindicatormatrix G.
OrdinaryCA decomposes the departurefrom independence. When we applythis to the Burtmatrix,we see that an "independent" matrixderived from the Burt matrixis in fact a matrixwith marginal independence for all pairs of variables. So MCA studies the departurefrom bivariatemarginalindependence,restrictingattention to the bivariate marginal dependence. Now, the relation with chi square displayed in equation (4) needs to be changed: This relation becomes EaAa(B)= aAa2(G)= (k-m)+m2m, Xmm /n, where Xmm is the chi square for testing independence in the marginal table of variablesm and m'. It also shows, in loglinearanalysisterminology, that second- and higher-orderinteractions are ignored. There are circumstances in which this is a serious drawback. However, by restrictingattention to marginalbivariatedependence, we can study the relation between many variables with many categories without running into empty-cell problems. Therefore, MCA has a large range of possible applications.
The Burt matrix also gives us a further understandingof the quantificationinterpretationof MCA. In CA the categories of the two variables are quantifiedin such a way that the correlations are maximized. By quantifying, we can derive for each dimension a correlation for the contingency table. In MCA the variables are all quantified optimally under the restriction that the quantifications of the categories of a variable are identical for the relations with all other variables. With these quantifications, a correlation coefficient can be derived from each subtable of the Burt matrix, and this matrix can be reduced to a correlation matrix for each
dimension.
3.4. Missing Data
So far, we have discussed MCA with no missing values. When there are missing values, it is not known in which category an object falls. Most often this is remedied by coding 0 in the indicator matrix for all categories of the missing variable. In many instances, formulaswill change a little, because with missingvalues, row margins are not the same for all rows. MCA with missing

CORRESPONDENCE ANALYSIS

61

values is treated in detail by Meulman (1982; see also Nishisato 1980, ch. 8; Greenacre 1984, ch. 5).

4. THREE-WAY SUPERINDICATOR MATRICES

In the sequel, we concentrate on the analysis of longitudinal data. We speak of longitudinal data when a group of objects is observed more than once at the same time points on the same (group of) variable(s). We assume that the measuresare categorical.
Therefore, longitudinal data can be coded into a three-way superindicatormatrixG, havingelements g/, whereg, = 1 if object i falls into category j of variable m at time point t (t = 1, . ., T), and g, = 0 if not. The three modes of G are the object mode indexed by i, the category mode indexed by a combinationof j and m, and the time mode indexed by t, so only the last mode is new. This way of coding longitudinal data is not very restrictive. The only restrictive feature is that in principle, all objects must be measured at the same (and not at different) time points. If they are not, we have to code some objects at specific time points as missing, or use an extra category for them. However, in both possibilities, results can be influenced to some extent by the fact that a number of objects are missing at specific time points.
The coding is useful not only in discrete time applications. Observations in continuous time can be coded into a three-way superindicatormatrix when we keep in mind that in practice, the precision with which time is observed is limited: Continuous time
observations are measured in days, hours, minutes, or seconds. Large precision implies only that the number of time points is very large, making the time mode of the matrix G very large. Threeway superindicatormatriceswere firstintroducedby Saporta(1981); see also de Leeuw, van der Heijden, and Kreft (1985) and van der
Heijden (1987).
Our approachto the analysis of G is to recode the three-way data in G into a two-waymatrixand to analyze this two-way coding with MCA. In principle there are three ways to construct two-way codings: First, we can concentrate on separate slices of the threeway data block. Second, we can concatenate these slices. Third, we can concentrate on margins of the three-way matrix G. In this

62 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
paper we will not analyze slices but concentrate instead on concatenations of slices and on marginaltables.
There are three ways to concatenate slices. First, we can concatenate the T time-point slices horizontally so that we obtain a broad matrixwith objects in the rows and categories of variables at specific time points in the columns. We will refer to this matrix as the BROAD matrix and denote it as G'(j', having elements gi(j,).Second, we can concatenate the T time-point slices vertically, yielding a LONG matrixwith categories of variablesin the columns and object-time-point combinations in the rows. The LONG matrix will be denoted as G(i')i,with elements g,)j. Third, we can
constructa matrixwith time points in the columnsand combinations of objects and categories of variables in the rows. However, relations between the rows and columnsof this matrixare not easily interpretable; therefore, we will skip this possibility.
For the marginal tables, we also have three possibilities. First, we can add up over the time points. This gives a matrix denoted as G" with objects in the rows and categories of variables in the columns. The entries g7l+of G" are frequencies indicating the number of times that object i falls into category j of variable m. Second, we can add up over the objects so that we have a matrix with time points in the rows and categories of variables in the columns. This matrix will be denoted as GTJ, and entries g+j, are frequencies indicating the number of objects that fall into a specificcategory at a specifictime point. The third marginalmatrix, obtained by adding up over the category mode, is the uninteresting matrix with all entries equal to gK, = m, the numberof variables.
So we conclude that the three-way matrix G gives us four interestingtwo-way matrices, namely, the BROAD and the LONG matricesobtained by concatenatingslices, and the marginalmatrices G" and GTJ. Below, we will discuss MCA of panel data and of event history data. Specifically, we will discuss the aspects of the data that are revealed when we analyze each of the four possible two-way matrices and compare this to more usual techniques for the analysis of these data.

CORRESPONDENCE ANALYSIS

63

5. MULTIPLE CORRESPONDENCE ANALYSIS OF PANEL DATA

We speak of panel data if a group of objects is measured more than once at specific time points using one or more variables. Such data can always be coded into the three-way superindicator matrix G. We will first consider univariate panel data. Then we will discuss the analysis of multivariatepanel data.
5.1. UnivariatePanel Data

We can consider panel data as univariate when either the
number of variables is one or a composite variable is constructed
by stacking the categories of two or more variables into one new
variable. For the general case, we saw that the three-way superindicatormatrix G could be reduced to four interesting two-
way codings of the data. For this specific univariate application, the LONG matrix G(0') gives a trivial solution with all singular values equal to one, since G(i')jhas rows with one 1, the other
values being zero.
The most interesting analysis is the analysis of BROAD
matrix Gi('t).This matrix has one row for each object and one
column for each category at each time point. Each row has T values of one, the others being zero. The chi-squaredistanceinterpretation of MCA indicates that each object (row) is compared with the average row profile, being the profile of the column margins g+j, (we skip the superscriptm, since there is only one variable). These
column margins specify the distributions of objects over the categories at the T time points. The profile of these column margins is placed in the origin. Note that we do not study the structure within the column margins, only the departure from this average. Objects are placed near each other when their profiles depart in the same way from the average profile, and they are placed far from each other when they depart in different ways. A separate analysisof the column marginsof Gi('t)can be useful to complement the analysis of Gi(t) itself. In this way we can understandfrom what
the objects depart. The columnmarginsare collected in the marginal matrixGTJ. This matrixshows how categories relate to time points. CA of GTJcompares the T distributionsof objects with the average

64 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
distribution over all time points given by the column margins of
GTTJ
When we interpret the MCA of G'"ulas a PCA of nominal variables,the variablethat is measuredat Ttime points is considered in Gi'') as a set of T distinct variables that will simultaneously receive an optimal quantification.So MCA gives T distinct sets of quantificationsof this variable, one set for each time point t. This interpretation of MCA of G'i(' is closely related to MCA of the marginal matrix G". Gifi (1981, ch. 10) shows that equality constraints (i.e., under which two or more rows/columns receive identicalquantifications)can be obtainedwithordinaryCA programs by simply replacing the two or more rows/columns by one row/ column that is the sum of the original rows/columns. Therefore, the analysis of the marginal matrix G" gives a solution for the analysis of G'i'' in which categoryj obtains identical quantifications for each time point t. The transition formula (3a) shows that if in the analysis of Gi0) the category quantificationsof corresponding categories do not differ much, the row scores R (i.e., the scores for the objects) for the analysisof G'i(' will be approximatelyequal to those for G7.
We conclude that for the three-way superindicatormatrix, there are three interesting two-way matrices to be analyzed. The analysisof G'it' is in general most interesting, because it shows how objects depart in different ways from the average. This analysiscan be supplemented by the analysis of GTJ, giving more insight into this average. The analysis of G" shows us a constrainedanalysis of
Gi'O).
So far, we have only considered matricesthat can be derived from the three-way matrix G by adding up or concatenating slices of the data block. For the special case in which the numberof time points T=2, we can also construct a transition matrix with the categories for t=l in the rows and the categories for t=2 in the columns. It can be proven that an ordinaryCA of such a transition matrixcomes to the same as MCA of the matrixGi')', in the sense that the former solution can be derived from the latter (see, e.g., Gifi 1981; Greenacre 1984). This property holds for contingency tables in general. We can also use CA to analyze panel data with three or more waves by creating a contingency table with a new composite variable, with the stacked categories of the firstfew time

CORRESPONDENCE ANALYSIS

65

points in the rows and the categories of the last time point in the columns. For an example, and its relation to Markovchain models,
see van der Heijden (1987). We will now discuss an MCA example for three time points and then discuss the relation of MCA of univariate panel data to Markov chain models.

5.2. Example

In 1987 and 1988, Meerens, Boer, and Tan (1988) conducted
a panel studyof the decreasein income resultingfrom unemployment or medical disability. The first interview was conducted in January 1987, the second in July 1987, and the third in January1988. There were 402 respondents at all three time points. Longitudinal informationwas collected for many variables (for more details, see
Meerens et al. 1988). We concentrate here on the question, "Consider a person receiving money from social security. How much may such a person according to your standards earn in a month without notifying the social security office?" (In the Netherlands, earning extra money without notifying the social security office is illegal.) At time 1, the possible answers are "no idea," "nothing," "less than flO," "between flOO and f249," "between f250 and f499," "more than f500," and "unlimited, or
could not specify an amount." (One Dutch guilder, denoted as fl, equals roughly $0.5.) Because of the divers answersfalling into this last alternative, at time points 2 and 3 two extra alternatives were added to this list: "the same amount as when I worked" and "I
know that it is allowed, but I don't know how much." So there are
7x9x9=567 possible response patterns, only 204 of which were used. For this reason, more traditional approaches that use the three-way matrixof order 7x9x9 cannot be applied. The sequence of analyses below is in our opinion typical for an adequate study of panel data with MCA.
We start by coding the 402 profiles into a three-wayindicator matrix of order 402x9x3, of which at time 1 two categories are empty by design. The first analysis we perform is the analysis of the marginal matrix GTJof order 9x3. This matrix is displayed in Table 4. Although this matrix can easily be studied by eye only, we will do a CA for expository purposes. CA gives a solution that is dominated by a distinction between time 1 and times 2 and 3:

66 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW

TABLE 4
Marginal Frequencies of Responses at Times 1, 2, and 3 to the Question About Social Security Income

Response

Time 1 Time 2 Time 3

No idea
Nothing Less than flOO Between flOOand f249 Between f250 and f500 More than f500 Unlimited Same amount as when I worked I know it is allowed, but I
don't know how much Totals

50 6 13 61 66 56 19 19 21 64 68 56 110 92 87 77 45 55 21 13 16
68 57 - 25 41
402 402 402

This is due to the two categories that are zero by design at time 1 and to the "no idea" category, which has a high frequency at time 1 and a low frequency at times 2 and 3. So this analysis shows a somewhat trivial result. In de Leeuw and van der Heijden (1988), we discuss a procedure for the analysis of incomplete tables that is closely related to the loglinear quasi-independence model. Practically, it implies that independent values are imputed into the cells that make the table incomplete. For our example, these independent values are 81.97 and 43.28. When we analyze the table with the
imputed values, we find a dominant eigenvalue of 0.13 (93 percent). The time points are quantified as -0.50, 0.49, and 0.16 for time 1 to time 3. Now the main difference is between the profiles for times 1 and 2, time 3 being in between. Categories "no idea," "greater than f500," and "unlimited" receive negative quantifications, showing that frequencies of these categories are particularly large at time 1 and become smaller at time 2 (and to a lesser extent, at
time 3). On the other hand, the categories "nothing," "less than flOO," and "flO0-f249" are quantified positively, showing that the number of persons that fell into these categories at time 2 (and to a lesser extent, at time 3) became larger.

CORRESPONDENCE ANALYSIS

67

Now that we have some insight into the changes at the group level, we can study individual differences in changes with MCA. We do this by analyzingthe BROAD matrixof order 402 x 25 (we can omit the two empty columns). MCA gives firstfew eigenvalues 0.621, 0.557, 0.514, 0.467, 0.441, and 0.411. We study only two dimensions here, thus using MCA only to show the most important information in the data. The first two dimensions are shown for
the persons (rows) in Figure 2 and for the categories (columns) in Figure 3. These two figures are related in the sense that each category point is in the average of the persons that fall into it. In Figure 2, each different response pattern is represented by a point, so there are 204 distinct points. For this data set, the points are placed in a triangularconfiguration,and most points are located in the left corner. The chi-squareddistance interpretationimplies that response patterns (i.e. persons) are placed closer together when they have many elements in common (here, they are identicalwhen

C\J
0
C
E 2.5

a.
m

1.5_

*mm

. .-

0.5 _

*

. -

aI * -

i?

-..: ,,.

-0.5 . . Ua * .

?+~k~~?~m~II*~0~~00~.m~m~ l

. ..??

-2.5 -1.5

I
-0.5

'I
0.5

It
1.5

II I I
2.5 3.5

dimension 1

FIGURE 2 Row points (for objects/response patterns) in two dimensions.

68
\ 2.5
0
.E
1.5 _

P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
t2 , less than f100
13

f250-f499 /

100-f249

0.5_

.>f5t0t0i - t<

-same as in workingposition t3

t tt2

-0.5. 13

1t2 unluimnliitmedi

t OK,amount ?

no idea /

-1.5 -1.5

/ nothing LJ
-0.5 0.5 1.5 2.5 dimension 1
FIGURE 3 Category points in two dimensions.

they have three elements in common), and they are placed farther apart when they have little in common. We can interpretthe form
of the configurationof points when we study the plot of category points. Here we see in the upper right corner the categories for "less than flOO,"in the lower right corner "nothing," and in the left corner the other categories. Category points are close together when they have many persons in common; therefore, for example,
the categories "less than flOO"at times 1, 2, and 3 have many persons in common. This shows that persons who fall into "less than flO0"at one of the time points very often fall into this category at other time points. When these persons change categories, they are most likely to change to nearbycategories, since these categories have the most persons in commonwith the "lessthanf100"category. For the persons falling at least once in "less than flOO,"these are the categories for "f100-f249."Persons falling at least once in "less than f100" will seldom change to other categories, because "less than flOO"is relatively far from all other categories. The configurationof the remainingpoints can be interpretedin a similar

CORRESPONDENCE ANALYSIS

69

fashion:A curvecan be drawnfrom "lessthanf100"via "f 00-f249,"
"f250-f499," "f500 or more," "unlimited,""no idea" to "nothing."
Figure 2 shows that most persons have a response pattern with categories that are in adjacent clusters in this curve. However, there are several exceptions: There are also points between the upperrightand lower rightcornersrepresentingpersonswho change from "nothing"to "less than flOO."That the distance between "less than flOO"and "fl00-f249" is much largerthan the distance between "fl00-f249" and "f250-f499" indicates that more changes occur between categories of the latter pair than between those of the former pair. Most changes occur between the categories in the left corner. The new categories "the same amount as when I worked" and "I know that it is allowed, but I don't know how much" are
between all other categories, indicatingthat they are found together with all other categories.
For all categories, the points for times 1, 2, and 3 are near each other, indicating that relatively many persons remain in the same state. It is dangerousto interpretthe smalldifferencesbetween the locations of a category at times 1, 2, and 3 because we are looking at a two-dimensional representationof a high-dimensional configuration, and although these two dimensions are most important, much information is hidden in higher dimensions. However, on both dimensions it is generally true that the points for time 2 are further away from the center than the points for times 1 and 3. This is also shown by the contributionsof the time points to the dimensions: For times 1, 2, and 3 respectively, these are 0.317, 0.358, and 0.326 for dimension 1 and 0.307, 0.388, and 0.305 for
dimension 2. This shows that the separationof person points is due more to their scores at time 2 than to their scores at times 1 and
3 (see the next section on Markov chain models and multiple CA). These differences are very small. This is shown by the fact that the analysis of G'J(a constrainedanalysisof the BROAD matrix) gives almost the same configuration of person points: For the first dimension, the correlation between the person scores for the constrained and the unconstrainedanalyses is 0.996; for the second dimension, it is 0.989.
As a second step in the multiple CA of the BROAD matrix, we will relate supplementary information to the solution: the age of the persons (in four categories), their sex, and whether they

70 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
stopped working for medical reasons (M) or became unemployed (U). We relate this information to the person points to find out whether persons with specific characteristics can be found in a specific part of the configuration.Thus, we use the supplementary information to explain differences between persons' scores on the longitudinalvariable.We have derivedfromthe three supplementary variablesone new composite variablehaving4x 2x 2=16 categories, and we have computed averages for each of these 16 categories. (This strategy can only be applied when the product of all levels of the supplementary variables is not too large. If the product is large, we have to compute averages for each level of each variable separately.) See Figure 4. The categories discriminatemost in the direction from the left corner to the lower right corner. In general, medically unfit persons more often state "nothing," whereas unemployed persons more often mention high amounts. There doesn't seem to be much difference by sex: Corresponding male and female points are quite near each other. There also seems to be a tendency for some unemployed groups to lie near the upper right corner (i.e., a small amount is allowed).
5.3. Multiple CorrespondenceAnalysis and Markov Chain Models
In the presentation of MCA as a CA of the Burt matrix, we explained that MCA concentrates on the bivariate marginal dependence in the higher-way contingency table. In terms of loglinear analysis, a Burt matrixgives an adequate summaryof the higher-way contingency table if a model with only first-order interactions fits the data reasonably well. If the bivariate margins are not sufficient for an adequate description of the data, then some of the information in the higher-waytable is ignored.
Let's now consider Markov chain models. For discrete time, Markov chain models can be formulated in terms of loglinear models (see Bishop et al. 1975; Plewis 1985) and hence in terms of fitted margins. Assume that we have three-wave panel data coded into a three-way contingency table with elements fi,k. If the data in the three-way matrix can be described by a nonstationary first-orderMarkovchain, then the two marginaltables with elements fij+ and f+jk are sufficient to describe the data (cf. Plewis 1985).

CORRESPONDENCE ANALYSIS

c0 0.4 0- 0.3-.
E
-
0.2
0.1 -

male/23-30/m

* male/23-30/u

mae/31-40/u female/23-30/um
female/41-50/m

female/31-40/u male/41-50/u
female/>50/u

0.0- *male/>50/m

-0.1 -

male/31-40/m

female/>50/m

female * 23-30/m

-0.2 -

male/>50/u,

male/41-50/m

female/31-40/m. ~-0.3

~female/41-50/u?

--00.6.6 --00.4.4 --00.2.2 00.0.0 00..22 00.4.4 dimension 1
FIGURE 4 Supplementary points in two dimensions.

71

Adjacent time points are dependent, while times 1 and 3 are independent given 2, and hence the marginal matrix f +k can be derived as fi+k = jifj+f+jk/f+j+. If the data in the three-way matrix can be adequately described by a stationary first-order Markov chain, then we need only one transition matrix and the initial proportions at time 1 to describe the data. For the three-wave example, this transitionmatrixcan be estimated using the marginal tables with elements fij+ and f+jk.
The Burt matrix uses all possible transition matrices for two time points. MCA ignores three- and higher-waytransitionmatrices.
If the data follow a nonstationary or stationary first-order Markov
chain, then all the information necessary to describe this chain is present in the Burt matrix, and MCA does not ignore any useful information. The sufficientmarginsare in the submatricesadjacent to the diagonal matrices. The other matricesin the Burt matrixcan be derived from these sufficient margins. For the stationary Burt matrix, we find that the submatrix farthest from the diagonal (in
Table 3, this is the matrix with elements fi+k) approximates independence the most. This follows fromthe limiting-statebehavior of Markov chains: If the time passed is taken to be large enough, the initial state of an object provides no information about the present state. A transitionmatrixfrom time 1 to the last time point would show a matrix with equal rows, i.e., an independent matrix.

72 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
So the farther apart two time points are, the more unrelated they are. The implicationfor MCA is that in a solution, the intermediate time points, which are the time points that are most related to the largestnumberof other time points, will have the largestcontribution to the solution. For nonstationaryMarkov chains, similar results can be shown to hold under some conditions.
5.4. MultivariatePanel Data
So far, we have discussed the analysis of univariate panel data. Multivariatepanel data could be dealt with in this framework by stacking the categories of these variables into a new composite variable and by dealing with this variable as if it were an ordinary variable. Another possibility, on which we will focus in this section, is to code the multivariatepanel data into the three-way superindicatormatrixand to analyze this matrixby first recoding it into two-way matrices. This differs from the situation for univariate panel data in that we are now able to work with the LONG matrix, because we have more than one 1 in each row. So we now have
four possible matrices that could be analyzed: (a) the BROAD matrix G'i0', in which the T slices of the three-way data block are concatenated horizontally; (b) the LONG matrix Gt0'i, in which the T slices of the three-waydata block are concatenatedvertically; (c) the marginal matrix G", which can be considered again as a constrained analysis of the BROAD matrix but which also gives a constrained analysis of the LONG matrix, in which the T sets of object scores are constrained to be identical; and (d) the marginal matrix GTJ, which consists of the column margins of the BROAD matrix.
The analysisof the BROAD matrixG'i') can perhapsbe most easily understood when we think of it as a PCA for nominal variables. For quantitativelongitudinal data, it is not unusual to construct BROAD matrices (cf. Visser 1985) and to analyze them with PCA or factor analysis. Many types of information are to be found in the solution of the MCA of G'(). First, it can show the relation between distinct variablesat identical time points. Second, it can show the relation between identical variables at distinct time points. And third, it can show the relationbetween distinctvariables at distinct time points. One type of informationis not displayed in

CORRESPONDENCE ANALYSIS

73

the solution, namely, the relation between categories and time
points. This information is in the marginal matrix GTJ and has to be analyzed separately, as we analyzed the univariate data. The
analysis of the marginal matrix G' can again be considered as providing a constrained solution for the BROAD matrix-con-
strained in the sense that each category receives identical quantifications for all time points.
MCA of the LONG matrix gives only one set of scores for all the categories but T sets of scores for the objects, namely, one set for each time point. This analysis can also best be understood as a PCA for nominal variables. A possible drawbackof MCA of the LONG matrixis that category quantificationscan be found that distinguish in the first place the different time points and not the different individuals;i.e., there are T sets of object scores that are
approximately similar in each set but very different between sets.
This might happen when the distributions of the categories differ considerablyover time points. Such a solution might be considered
rather uninteresting, because a similar-and more easily interpretable-solution might be obtained from the analysis of the marginal matrix GTJ. (This gives a solution in which object scores at identical time points are restricted to be the same.)
In the context of quantitativevariables, a similar problem is that average differences in the analysisof LONG matricescan lead to spuriouscorrelations.This is sometimescircumventedby vertically concatenating T slices that are each in deviation from the mean (see Bentler 1973). Escofier (1988) recently proposed a comparable procedure, which she called conditional MCA (see also van der Heijden 1987, ch. 7). Using the generalizationsof CA (see equation (7)), we decompose not the departure from independence for the total matrix G(it)jbut the departure from independence in each of the T submatrices. So, for P in equation (7), we take the observed LONG matrix G(')i, whereas the matrix Q has elements
g(it)+g(+)jlg(+t)+W. hen for Sr and Sc simply the margins of P are taken, it can be shown that the generalized CA solution can be obtained with ordinary CA programs by analyzing the matrix (P-Q+E). In fact, conditional MCA can be used in a similar way in the more general situation in which we want subgroupsof objects to have an average of zero (see also van der Heijden 1987 and van der Heijden and Meijerink 1989 for more details).

74 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
6. MULTIPLE CORRESPONDENCE ANALYSIS OF EVENT HISTORY DATA
Event history data specify not only the sequence of states (categories) that apply to an object but also how long each object is in a specific state. Event history data can be coded into the familiarthree-waysuperindicatormatrixG by going fromcontinuous time to discretetime points and making a superindicatormatrix for each time point. So, g,,,= 1 if object i falls into categoryj of variable m at time point t, and gm,=0 otherwise. In this approach, event history data are just a special form of panel data, namely, a form that tells us for each time point the category in which an object falls.
In general, the number of time points T will be very large; for example, T=1,440 if we consider one day divided into minutes. Therefore, it is difficult to analyze the data without imposing any restrictions. We restrict the analysis by aggregatingthe data over time points within larger periods. For example, we can subdivide a day into hours, and instead of 1,440 time points, we then construct a matrix G with 24 time periods p (p=, . . ., P), where elements gp denote the number of minutes in hour p that object i spent in categoryj of variable m. This idea was first worked out by Deville and Saporta (1980, 1983); see also Saporta (1981, 1985), de Leeuw et al. (1985), and van der Heijden (1987). Let's again consider the analysis of the BROAD matrix GiP), the LONG matrixG('P),and the two marginalmatrices G'J and GPJ.
In principle, the analysis of these matrices does not differ much from the analysis of panel data. Therefore, we will not distinguish between the univariate and multivariate cases. The BROAD matrix GiIP)shows how the objects spend their time in the periods chosen. MCA of the BROAD matrix will show how objects depart from the average time spent. Unlike the analysis of G'i(, in the analysis of Gi(P) the category quantifications are restricted to be identical within each period. It will be clear that to save information, we must choose the periods in such a way that within periods, objects do not change states much but that betweenperiods, the distributionsare as different as possible. This consideration can lead to a choice of periods of unequal length (see, e.g., de Leeuw et al. 1985).

CORRESPONDENCE ANALYSIS

75

The average from which the objects depart is collected in the column margins of the BROAD matrix, i.e., the matrix GPJ.
This matrix shows that some states are better filled in some periods and that other states are better filled in others. An analysis of this matrix supplements the analysis of the BROAD matrix. In the marginalmatrix G'J, the number of periods is reduced to one. This matrix shows how the objects spend their time. In an MCA of G'J, the category quantificationsare restricted to be the same over the
complete time range. The situation for the LONG matrix is slightly different from
what it was for univariatepanel data: If we have event history data for one variable and if the periods are chosen so that state changes occur, the analysis of the LONG matrix will not give a trivial solution. In general, however, the analysis of the LONG matrix will approach the (uninteresting) analysis of a diagonal matrix if the number of periods is chosen to be sufficiently large, so that time periods will be short and the number of objects in only one state will be large.

6.1. Example

In the first section, we discussed the analysis of a matrix of 18 hours by 25 categories, with each cell representing the number of minutes in an hour that 326 couples spend in a specific category. This matrix can be considered as the margin GTJ of the three-way data block G of order 326x25x18. We will now analyze the BROAD matrix Gi('), having order 326x(25x18). In fact, the number of the columns of the matrix is slightly smaller than 25x18=425; it is 343. Not all joint activities are displayed at each hour (see zero frequenciesin Table 1), and we findno quantifications for such joint activities.
MCA of the BROAD matrix gives as first few eigenvalues 0.422, 0.390, 0.339, 0.294, 0.271, 0.259, 0.249. A plot of the 326 objects (the couples) can be found in Figure 5. The cloud of points has a peculiar form. On the first dimension, a distinction is made between two groups: A large number of couples appears on the left, and a smaller number appearson the right. Low on the second dimension, an even smaller group of couples is visible. This phenomenon goes on in higher dimensions. In each dimension, a

HH HW HS HT HO WH WW WSWT WO SH SW
SS STSO TH TW TS TT TO OH OW OSOT 00-

TABLE 5 Category Quantifications on Dimension 1
6 7 8 9 10 11 12 13 14 15 16 17

0.04 -0.31
-0.20 0.18
-0.24 -0.03
-0.81 -
-0.52
--0.58
-
-

0.31 -0.28
-0.28 0.97
-0.46 -0.61
-0.63
-
-0.38 -0.62
-0.50 -
2.10 -0.93
-
-

1.34 0.02
-0.06 1.09
-0.27 -0.61
-0.70
0.12 -
0.00 -0.58
-0.32 -
2.40 -0.68
-

1.89 0.25
0.94 1.88 -0.09 -0.62
-0.52
0.18 2.13 -0.18 1.99
-
1.04 -0.17
0.54 -
1.26 -0.40
0.93
-

2.07 0.39 2.19 1.85 1.88 -0.02 -0.62 -0.56 -0.34 0.03 2.02 0.00 2.06 1.78
1.82 0.04
1.39 1.88 -0.13 0.11
-
0.94 1.78

2.11 0.38 2.23 1.38
0.01 -0.61 -0.60 -0.26 0.04 2.04 -0.29 1.52 1.78
1.51 -0.19
1.68
0.37 0.83 1.76

1.64 0.07 2.13 0.82 0.39 -0.20 -0.67 -0.28 -0.41 -0.37 1.03 -0.60 0.38 -0.54
0.45 -0.48 -0.44 0.39 -0.27 -0.32 -0.(6 -0.65 0.25 1.60

1.80 0.05
-
0.67 1.63 -0.19 -0.68 -0.09 -0.42 -0.16 1.26 -0.45 -0.11 -0.63 -0.71 0.79 -0.41 2.13 0.58 -0.64
0.55 0.03
1.46

1.97 0.06 2.07 0.68 0.78 -0.19 -0.67 -0.16 0.05 0.38 1.58 -0.28 0.64 -0.18
1.72 -0.36 -0.41 1.30 1.92 2.13 0.37 2.13 2.13 1.12

1.88 0.11
0.70 0.73 -0.17 -0.68 -0.60 -0.25 0.13 0.99 -0.30 1.97 1.06 1.95 1.22 -0.38 1.05 1.01 -0.40 1.47 0.30
1.07 1.39

1.19 -0.09
2.06 -0.25
0.96 -0.24 -0.67 -0.78 -0.62 -0.40
1.09 -0.39
1.43 0.13
0.04 -0.56 -0.93 -0.22 0.03 0.91 0.37 0.41 0.39 1.69

0.28 -0.49
0.06 -0.32
0.87 -0.52 -0.66 -0.97 -0.75 -0.37
0.32 -0.27
0.92 -0.52 -0.62 -0.21 -0.52 -0.58 -0.24 -0.29
1.15 -0.04 -0.09
0.64 1.20

0. -0.4 -0.5 -0.3
0. -0.4 -0.6
-0.9
0. 0. 2.
-(.3 -(.4
0. -0.5
(. -0.5 -0.0
(0 0.

Note. See note to Table 1.

CORRESPONDENCE ANALYSIS

C

EO
E0 -

_s

? 'LA

4 at k - M- ^ ^

-2a ' '
AA

77

-3- .

-4
-1.5

1I 0I
-0.5

A
d' .5
0.5

1. .
1.5 2.5
dimension 1

FIGURE 5 Row points in two dimensions.

small group of couples is separated from the rest: Couples in a separated cluster differ in the same respect from the average event history that is summarized in GTJ and that is dominated by the larger group of couples. This is also reflected in the eigenvalues, which drop from 0.422 only very gradually. We conclude that the bulk of couples have quite the same event histories, probably because we selected couples in which both partnerswork more than
25 hours a week and we are only studying working days. The category quantifications in Table 5 and their plot in
Figure 6 show how the group on the right in Figure 5 differs from the larger group. In Figure 6, the hours are presented along the horizontal, and the quantificationson dimension 1 are presented along the vertical. For the five categories in which both partners performthe same activity,we have connected the points for adjacent hours. To determine the points on which we must concentrate, we study the contribution of each individual point to dimension 1;

CORRESPONDENCE ANALYSIS

79

thus, we find a subdivision (in proportions)of the eigenvalue 0.422. By adding up over the 18 hours, we find contributions of the 25
joint activities to dimension 1; by adding up over the 25 joint activities, we find contributions of the 18 hours on dimension 1.
When we focus on the hours, we find that the hours between 8:00
A.M. and 5:00 P.M. clearly contribute most; thus, on dimension 1, the couples are distinguished mainly on the basis of their departure from the average joint activities in these hours, say working hours. When we then focus on the joint activities,we find that the HH category (both partnersat home) and the WWcategory (both partnersat work) contributemost (57 percent and 24 percent, respectively). So the couples on the right in Figure 5 are separated from the couples on the left mainly because they depart in opposite ways on these joint activities. Figure 6 shows that during working hours, the couples on the right are much more often both at home and much less often both at work than the couples on the left. Since we know from the sample that both partners work at least 25 hours a week, the couples on the right probablyhave a day off. After working hours, the couples on the right do not differ much from the couples on the left. When we study Table 5 and Figure 6 more carefully, we find that most of the joint activities in which at least one of the partners is working (i.e., one W) are quantified negatively in all hours; so for the objects on the left, at least one of the partners is working. These couples travel more often in the morning (TT has negative quantifications),probably to their work, and also more between 4:00 P.M. and 6:00 P.M., probably from work. We can also see a lunch dip for both partners travelling or shopping: In the morning and afternoon, these activities are performed more by the couples having a day off, whereas during lunch time, these behaviors are evenly spread between the couples on the right and those on the left. To interpret Figure 6, we first studied the activities that contribute much. We did not necessarily give much attention to the most extremely quantified activities, such as SS in the evening, since these activities may have a very low frequency. (SS is performed only 150 minutes after 6:00 P.M. However, the couples doing this are all quantified extremely
positive.) For the second dimension, we find that the hours between
8:00 A.M. and 4:00 P.M. contribute between 5 percent and 10

80 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
percent each, the evening hours contributingabout 3 percent each. The activities contributingmuch are WH (58 percent) and HW (9 percent), WO (4 percent) and OW (5 percent), WW (5 percent), SH (5 percent) and TH (4 percent). A table similarto Table 5 (not displayed here for reasons of space) shows that the couples quantified negatively in Figure 5 are couples in which the wives work duringordinaryworking hours and the husbandsare at home (WH) or doing other things (WO). During working hours wives also travel more while the husbands are at home (TH). For the rest of these couples, we find that the husbands work more than average in the evening while the wives are at home (HW) or doing other things (OW) and that wives shop more in the early evening while the husbands are at home (SH). We can classify the couples quantified negatively on dimension 2 roughly as couples in which the wives work more than average during ordinaryworking hours while the husbandsdo things other than working and the husbands work more in the evening while the wives do things other than
working. As a last step, we could study the relation between
supplementaryinformation and the object scores in the same way we did for panel data (compare Figure4). Because of lack of space, we did not do this (but see van der Heijden 1987).
In sum, we think that bivariate and multivariate event histories can very well be studied by means of MCA. A possible limitation is the number of objects compared with the number of activities. If the former is small compared with the latter, the solution can become unstable. In general, however, solutions can be made more stable by restricting the quantifications of each category to be identical within larger time periods. This amounts to separating the total time range into only a few time periods, thus reducing the number of columns of the BROAD matrix.
6.2. MCA Compared with StatisticalEvent History Analysis
We now compare MCA of event history analysis with statistical approaches, such as those summarizedin Allison (1984) and Tuma and Hannan (1984). Allison discusses a number of dimensions along which the statistical approaches differ, and we will use some of these to place MCA into his framework. A first

CORRESPONDENCE ANALYSIS

81

dimension is that between regression methods and distributional
methods. In MCA we use both aspects in an analysis. First,
homogeneous groups of objects are formed on the basis of the differences in distributionsof behavior over time. This gives us one
set of object scores for each dimension. Second, supplementary information is related to these sets of object scores by computing
averages over supplementary categories. In MCA, we use a twostep procedure to find object scores and then relate these to the supplementaryinformation. This is done in one step in the methods described by Allison. It is also possible to do this in a combined step for MCA using canonical CA (ter Braak 1986). With canonical CA we can find object scores that are restricted to be a linear
combination of a set of supplementaryvariables. However, this has not yet been applied to the analysis of event history data.
A second dimension discussed in Allison is that of repeated versus nonrepeatedevents, and a third dimension is that of single versus multiple kinds of events. Single nonrepeatedevents can be dealt with in MCA by using in our three-way indicator matrix two
categories: "event did not yet happen" and "event happened." Similarly, "censored" can be defined as a separate category as an alternative to "event happened." (Another approach using simple CA is proposed by Nakache, Asselain, and Lasry [1984] and will
be discussed below.) In our approach,single repeatedevents can be dealt with by counting the number of times an event maximally happened, x, and defining x+ 1 categories, namely, "event did not yet happen," "event has happened once," to "event has happened x times." In fact, this is a way to analyze point processes with MCA. We deal with point processes here not by definingthe points themselves but by defining the time between the points as distinct states. If the number of times an event happens for an object is very large and the numberof objects to which this happens is small, MCA can form homogeneous groups of objects by placing this small number of objects in the periphery of the plot of object scores. However, this can be remedied by definingthe state "event happened more than x times" in such a way that the number of objects in it is large enough to reduce the number of categories. When there are multiple kinds of events, we can deal with those in the same way, namely, by defining states an object is in. So, if we deal with unrepeated multiple kinds of events and there are, say,

82 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
K kinds of events, we define K+1 categories, namely, "nothinghas happened yet" to "state K has happened." With repeatedmultiple kinds of events where a specific kind of event can follow itself, we define categories like "nothing has happened," "event k has happened one time," "event k has happened two times," "event k* has happened one time," etc. When we deal with repeated multiple kinds of events where a specific event cannot follow itself, we can count the number of times events have happened, but we can also define only K possible categories, indicating the state an object is in. In fact, this is the case for the example that we analyzed in this paper, and it is also the only type of example that is published (see Deville and Saporta 1983; de Leeuw et al. 1985; van der Heijden 1987). However, we have practical experience with some of the other cases mentioned above, where MCA gave satisfactoryresults.
A fourth dimension mentioned by Allison (1984) is that of discrete time versus continuous time. MCA is applicable in both cases: In the case of discrete time, we work with panel data, and in the case of continuous time, we use the fact that continuous time
is always measured with discrete precision, defining the state an object is in at a specific time point. Because the time dimension of the three-way (super)indicator matrix becomes very large when precise measurements are used, we reduce this matrix by defining time periods.
Another problem occurs when objects are not measuredover
the same time range (apart from the fact that, of course, the time
scale can be defined in various ways). This situation can be dealt with by defining an extra category, "not observed yet" or "not observed anymore." However, the drawbackis that MCA can find a solution that distinguishesobjects on the basis of these categories. Therefore, it is probablybetter to define a person as missingduring the period he/she is not observed and to code all categories in this period as 0. We have not used this to analyze event history data, but this approach to missing data works quite well in the context of MCA of "ordinary"incomplete data.
Nakache et al. (1984) perform simple CA on a special
contingency table to analyze unrepeated single event histories. This work is similar to the work of Laird and Olivier (1981), who also use contingency tables to show that survival analysis can be performedusing loglinear models. Nakache et al. constructa matrix in which the columns are the categories of the explanatoryvariables

CORRESPONDENCE ANALYSIS

83

and the rows are specific grouped lengths of survival times, split up for censored individuals and noncensored individuals. In the example they use, there are ten rows: five for censored persons and five for noncensored persons. The five survivallengths are 0-6 months, 6-24 months, 24-60 months, 60-120 months, and more
than 120 months. In each cell of the matrix, we find the number
of individuals who fall into a specific category of an explanatory variable and who have a specific survival time. This matrix is analyzed with simple CA. For the example Nakache et al. use, the survival time periods are quantified monotonically on the first dimension, both for the censored persons and for the noncensored persons. The profiles of the explanatory variables for the noncensored individual objects are fitted into this solution (this can be done with transition formula (3), where C is the matrix with category scores for the explanatoryvariablesand Dr 'P is the matrix with profiles for the individuals), thus obtaining scores for each individual. Nakache et al. correlated these scores with the scores
for these noncensored individuals using the Cox regression model and found a correlation of 0.88, which gives some justificationfor
the exploratory procedure they applied. As we have tried to describe above, MCA of event history
data is a very flexible exploratory approach. Much research in this area is needed. For example, practical experience of MCA with many types of event history data is limited, and it is not clear how or whether exploratory CA can be used prior to confirmatory survival analysis. In principle, it seems that it can be used for all sorts of event histories, and it can easily handle data with a large set of states (in our example, we used 25). Computationally,neither a large amount of objects nor a large amount of time periods creates a problem. However, results might become unstable when the number of time periods and categories is too small compared with the number of objects. This problem can be solved by making the periods larger, thus making the number of columns smaller, thus restrictingthe quantificationsof categories to be constant for a longer time span.

7. CONCLUSIONS

We have discussed some of the properties of CA and MCA, concentratingon MCA of panel and event history data. Compared

84 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
with more usual data analysis approaches, MCA is less hampered by empty-cell problems because it uses only the bivariate marginal dependence in the data. Therefore, it can analyze data sets with many variables, categories, and time points that cannot be analyzed with approaches like Markov chain models or processes. On the other hand, MCA sometimes ignores information that is too important to ignore. However, we have seen that if a first-order Markov model fits adequately, MCA uses all information in the
analysis. Another differencebetween MCA and more usualapproaches
is that MCA finds scores for objects; therefore, it is possible to study individual differences.This is rather different from the more usual approaches, in which objects are treated as replications and differences between objects are usually neglected or considered to be the result of random error. There, if the differences between
objects become too large, objects are often split up into a few subgroups and are considered again as replications in these subgroups.This is one of the waysto linksupplementaryinformation (like sex, age, SES) to the process under study. It is not possible to link all these supplementaryvariables at the same time because of empty-cell problems. On the other hand, in MCA we find quantificationsfor objects that emphasizedifferencesbetween them. To these scores we can easily relate a largenumberof supplementary variables by computing averages for each category.
We think that MCA can be useful in obtaining a first idea of what is going on in the data. No assumptionshave to be fulfilled for the technique to work. If we concentrate on the bivariate margins, interesting information might be ignored, but the range of possible applicationsbecomes very large. Bivariate marginscan be reasonably filled even though the total number of objects is much smaller than the total number of possible profiles (i.e., cells in the multiwaycontingencytable). This is one importantadvantage of MCA. MCA is also a good alternative to many modeling techniques when the numberof categories of the variablesis large, making interpretation of parameters quite difficult. In this sense also, the MCA approach is far more exploratoryin nature and we think in many applications also far more realistic, especially when no theory is available to use a specific model or distribution.

CORRESPONDENCE ANALYSIS
REFERENCES

85

Allison, P. D. 1984. Event History Analysis. Beverly Hills: Sage. Bentler, P. M. 1973. "Assessment of Developmental Factor Change at the
Individual and the Group Level." Pp. 145-74 in Life-Span Developmental Psychology: Methodological Issues, edited by J. R. Nesselroade and H. W. Reese. New York: Academic Press.
Benzecri, J. P. et collaborateurs. 1973. Analyse des donnees. 2 vols. Paris: Dunod.
Bishop, Y. M. M., S. E. Fienberg, and P. W. Holland. 1975. Discrete Multivariate Analysis. Cambridge, MA: MIT Press.
Bock, R. D. 1960. Methods and Applications of Optimal Scaling. Research Memorandum No. 25. Chapel Hill: University of North Carolina, Psychometric Laboratory.
de Leeuw, J. 1973. "Canonical Analysis of Categorical Data." Ph.D. diss., University of Leiden. . 1983. "On the Prehistory of Correspondence Analysis." Statistica Neerlandica 37: 161-64.
de Leeuw, J., and P. G. M. van der Heijden. 1988. "Correspondence Analysis of Incomplete Tables." Psychometrika 53: 223-33.
de Leeuw, J., P. G. M. van der Heijden, and I. Kreft. 1985. "Homogeneity Analysis of Event History Data." Methods of Operations Research 50: 299-316.
Deville, J.-C., and G. Saporta. 1980. "Analyse harmonique qualitative." Pp. 375-89 in Data Analysis and Informatics, edited by E. Diday. Amsterdam: North Holland.
. 1983. "Correspondence Analysis, with an Extension Towards Nominal Time Series." Journal of Econometrics 22: 169-89. Escofier, B. 1984. "Analyse factorielle en reference a un modele: Application a l'analyse de tableaux d'echanges." Revue de Statistique Appliquee 32: 25-36.
.1988. "Analyse des correspondances multiples conditionelles." Pp. 333-42 in Data Analysis and Informatics 5, edited by E. Diday. Amsterdam: North Holland.
Gifi, A. 1981. Nonlinear MultivariateAnalysis. Leiden: University of Leiden, Department of Datatheory.
Gilula, Z., and S. J. Haberman. 1986. "Canonical Analysis of Contingency Tables by Maximum Likelihood." Journal of the American Statistical Association 81: 780-88.
Goodman, L. A. 1981. "Association Models and Canonical Correlation in the Analysis of Cross-Classifications Having Ordered Categories." Journal of the American Statistical Association 76: 320-44.
. 1985. "The Analysis of Cross-Classified Data Having Ordered and/or Unordered Categories: Association Models, Correlation Models, and Asymmetry Models for Contingency Tables With or Without Missing Entries." Annals of Statistics 13: 10-69.

86 P. G. M. VAN DER HEIJDEN AND J. DE LEEUW
.1986. "Some Useful Extensions of the Usual Correspondence Analysis Approach and the Usual Log-Linear Models Approach in the Analysis of Contingency Tables." InternationalStatisticalReview 54: 243-309. Greenacre, M. J. 1984. Theory and Applications of Correspondence Analysis. New York: Academic Press.
Kendall, M. G., and A. Stuart. 1967. The Advanced Theory of Statistics. 2d ed. London: Griffin.
Laird, N., and D. Olivier. 1981. "Covariance Analysis of Censored Survival Data Using Log-Linear Analysis Techniques." Journal of the American Statistical Association 76: 231-40.
Lebart, L., A. Morineau, and K. M. Warwick. 1984. Multivariate Descriptive Statistical Analysis: Correspondence Analysis and Related Techniques for Large Matrices. New York: Wiley.
Meerens, H., L. Boer, and A. Tan. 1988. Gevolgen inkomensachteruitgang. Leiden: University of Leiden, Werkgroep A&W.
Meulman, J. 1982. Homogeneity Analysis of Incomplete Data. Leiden: D.S.W.O. Press.
Moning, H. 1983. The National Travel Survey in the Netherlands. Heerlen: Central Bureau of Statistics.
Nakache, J.-P., B. Asselain, and C. Lasry. 1984. "Contribution de l'analyse factorielle des correspondances a l'etude multidimensionelles de donnees tronques." Pp. 99-108 in Data Analysis and Informatics 3, edited by E. Diday. Amsterdam: North Holland.
Nishisato, S. 1980. Analysis of Categorical Data: Dual Scaling and its Applications. Toronto: University of Toronto Press. .1986. "Classification with a Variety of Categorical Data." Pp. 353-59 in Classification as a Tool of Research, edited by W. Gaul and M. Schrader. Amsterdam: North Holland.
Plewis, I. 1985. Analyzing Change: Measurement and Explanation Using Longitudinal Data. New York: Wiley.
Saporta, G. 1981. "Methodes exploratoires d'analyse de donnees temporelles." Cahiers du Buro 37-38.
.1985. "Data Analysis for Numerical and Categorical Individual Time-Series." Applied Stochastic Models and Data Analysis 1: 109-19. Tenenhaus, M., and F. W. Young. 1985. "An Analysis and Synthesis of Multiple Correspondence Analysis, Optimal Scaling, Dual Scaling, Homogeneity Analysis, and Other Methods for Quantifying Categorical Multivariate Data." Psychometrika 50: 91-119. ter Braak, C. J. F. 1986. "Canonical Correspondence Analysis." Ecology 67: 1167-79.
Tuma, N. B., and M. T. Hannan. 1984. Social Dynamics: Models and Methods. New York: Academic Press.
van der Heijden, P. G. M. 1987. Correspondence Analysis of Longitudinal Categorical Data. Leiden: D.S.W.O. Press.
van der Heijden, P. G. M., and J. de Leeuw. 1985. "Correspondence Analysis Used Complementary to Loglinear Analysis." Psychometrika 50: 429-47.

CORRESPONDENCE ANALYSIS

87

van der Heijden, P. G. M., A. de Falguerolles, and J. de Leeuw. 1989. "A Combined Approach to Contingency Table Analysis Using Correspondence Analysis and Loglinear Analysis." Applied Statistics, forthcoming.
van der Heijden, P. G. M., and F. Meijerink. 1989. "Generalized Correspondence Analysis of Multiway Contingency Tables and Multiway (Super-) Indicator Matrices." In The Analysis of Multiway Data Matrices, edited by
R. Coppi and S. Bolasco. Amsterdam: North-Holland. Visser, R. A. 1985. The Analysis of Longitudinal Data in Behavioural and
Social Research. Leiden: D.S.W.O. Press.

