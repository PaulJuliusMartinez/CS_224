Contextual Bandits with Linear Payoff Functions

Wei Chu Yahoo! Labs Santa Clara, CA, USA chuwei@yahoo-inc.com

Lihong Li Yahoo! Labs Santa Clara, CA, USA lihong@yahoo-inc.com

Lev Reyzin Georgia Institute of Tech.
Atlanta, GA, USA lreyzin@cc.gatech.edu

Robert E. Schapire Princeton University Princeton, NJ, USA schapire@cs.princeton.edu

Abstract
In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions. For T rounds, K actions, and d dimensional feature vectors, we prove an O T d ln3(KT ln(T )/) regret bound
that holds with probability 1 -  for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem. We also prove a lower bound of ( T d) for this setting, matching the upper bound up to logarithmic factors.
1 INTRODUCTION
In the contextual bandit problem, on each of T rounds a learner is presented with the choice of taking one of K actions. Before making the choice of action, the learner sees a feature vector associated with each of its possible choices. In this setting the learner has access to a hypothesis class, in which the hypotheses take in action features and predict which action will give the best reward. If the learner can guarantee to do nearly as well as the prediction of the best hypothesis in hindsight (to have low regret), the learner is said to successfully compete with that class.
In this paper, we study the contextual bandit setting with linear payoffs. This setting was introduced by Abe et al. [2003] and developed by Auer [2002]. In this contextual bandit setting, the learner competes with the set of all linear predictors on the feature vectors. The set of linear predictors is both ex-
Appearing in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR: W&CP 15. Copyright 2011 by the authors.

pressive enough to yield good real-world performance, yet yields to a succinct representation that makes it a tractable case.
An example application for contextual bandits with linear payoffs is the Internet advertisement selection problem [Abe et al., 2003], where advertisement and webpage features are used to construct a linear function to predict the probability of a user clicking on a given advertisement. This setting has been used for other applications including making article recommendations on web portals [Agarwal et al., 2009, Li et al., 2010].
In this paper, we give a theoretical analysis of a variant of LinUCB, a natural upper confidence bound algorithm introduced and experimentally demonstrated to be effective by Li et al. [2010]. We use a technique first developed by Auer [2002] by decomposing LinUCB into two algorithms: BaseLinUCB and SupLinUCB (which uses BaseLinUCB as a subrou-
tine). We then show a O T d ln3(KT ln(T )/)
high-probability regret bound for SupLinUCB. Finally, a lower bound of ( T d) for the contextual bandit problem with linear payoffs are also given.
2 PREVIOUS WORK
In the traditional, non-contextual, multiarmed bandit problem, the learner has no access to arm features and simply competes with pulling the best of K arms in hindsight. In this setting, when the rewards are i.i.d. from round to round, upper confidence bound (UCB) algorithms proved both efficient and optimal [Lai and Robbins, 1985, Agrawal, 1995, Auer et al., 2002a]. The idea of confidence bound algorithms is to keep upper bounds on the plausible rewards of the arms and to pull the arm with the highest UCB. These approaches are known to give near-optimal algorithms that provide high probability guarantees on the regret suffered by the learner.
Our setting, in comparison, focuses on bandit prob-

208

Contextual Bandits with Linear Payoff Functions

lems with features on actions. As opposed to traditional K-armed bandit problems, action features in contextual bandits may be useful to infer the conditional average payoff of an action, which allows a sequential experimenter to even improve the average payoff over time. The name contextual bandit is borrowed from Langford and Zhang [2008], but this setting is also known by other names such as bandit problems with covariates [Woodroofe, 1979, Sarkar, 1991], associative reinforcement learning [Kaelbling, 1994], associative andit problems [Auer, 2002, Strehl et al., 2006], and bandit problems with expert advice [Auer et al., 2002b], among others.

It is useful to mention that one could imagine trying to solve the contextual bandit problem with linear payoffs using Exp4-type approaches [Auer et al., 2002b] that are made to work with an arbitrary hypothesis set. For N experts, Exp4 gives a O( KT ln N ) bound on the regret. One could imagine discretizing the linear hypotheses into an epsilon-net of experts and running Exp4 on them. However, this would have two disadvantages compared to our approach. First, the algorithm would run in time exponential in d, the number of features. Second, its regret bound would have a K dependence (this may be due to the fact that Exp4 works in an adversarial setting while UCB algorithms require rewards to be i.i.d.). For these reasons, it is useful to explicitly take advantage of the linear structure of the predictors in the contextual bandit problem with linear payoffs and to tackle it directly.

Most notably, Auer [2002] considered the contextual

bandit problem with linear payoffs (under the name

"associative reinforcement functions") and presented

learning LinRel,

twhiethfirlsint eOa~r(vaTlude)

algorithm for this problem. LinUCB has a couple of

practical advantages over LinRel: first, it is simpler to

state and implement; second, it uses ridge regression

as its core operation, which is often easier to solve with

standard software packages and may be less prone to

numerical instability issues compared to the eigen de-

composition step used in LinRel.1 While LinUCB has

been experimentally tested [Li et al., 2010, Pavlidis

et al., 2008], no theoretical analysis has been carried

out. Indeed, it was previously conjectured that Lin-

UCB does not enjoy the same regret bound as Lin-

Rel [Auer, 2002, footnote 5]. Our analysis thus an-

swers the question affirmatively.

Our regret lower bound improves the earlier result (T 3/4K1/4) by Abe et al. [2003]. This new result matches the best known regret upper bounds up to logarithmic factors.

1Even in the worst case, the computation complexity of matrix inversion is no worse than that of eigen decomposition.

Recently, the study of stochastic linear optimization under bandit feedback has received a lot of attention (e.g., Dani et al. [2008], and Rusmevichientong and Tsitsiklis [2010]). Their setting is similar to ours but is different. On each round, instead of having to pick one of K arms with given contextual information, their algorithms are allowed to pick a point in an infinitely large context space (called decision space in the literature). In contrast to the results discussed above, the matching upper and lower regret bounds for this setting is on the order of d T .

3 PROBLEM SETTING

Let T be the number of rounds and K the number of possible actions. Let rt,a  [0, 1] be the reward of action a on round t. On each round t for each action a the learner observes K feature vectors xt,a  Rd, with xt,a  1, where · denotes the 2-norm. After observing the feature vectors the learner then selects
an action at and receives reward rt,at .
We operate under the linear realizability assumption; that is, there exists an unknown weight vector   Rd with   1 so that

E[rt,a | xt,a] = xt,a

for all t and a. Hence, we assume that the rt,a are independent random variables with expectation xt,a.
Let xt,a be the feature vector of action a at step t, and define the regret of an algorithm A to be

TT

rt,at -

rt,at ,

t=1 t=1

where at = arg maxa xt,a is the best action at step t according to  and at is the action selected by A at step t.

We note that in our setting, the context vectors xt,a can be chosen arbitrarily by an oblivious adversary
as long as the rewards rt,a are independent given the context.

4 LINUCB
The LinUCB algorithm is motivated by the UCB algorithm of Auer et al. [2002a] and the KWIK algorithm of Walsh et al. [2009]. LinUCB is also similar to the LinRel algorithm of Auer [2002]--the main idea of both algorithms is to compute the expected reward of each arm by finding a linear combination of the previous rewards of the arm. To do this, LinUCB decomposes the feature vector of the current round into

209

Wei Chu, Lihong Li, Lev Reyzin, Robert E. Schapire

Algorithm 1 LinUCB: UCB with Linear Hypotheses
0: Inputs:   R+, K, d  N 1: A  Id {The d-by-d identity matirx} 2: b  0d 3: for t = 1, 2, 3, . . . , T do 4: t  A-1b 5: Observe K features, xt,1, xt,2, · · · , xt,K  Rd 6: for a = 1, 2, . . . , K do
7: pt,a  t xt,a +  xt,aA-1xt,a {Computes
upper confidence bound}
8: end for
9: Choose action at = arg maxa pt,a with ties broken arbitrarily
10: Observe payoff rt  {0, 1} 11: A  A + xt,at xt,at 12: b  b + xt,at rt 13: end for

a linear combination of feature vectors seen on previous rounds and uses the computed coefficients and rewards on previous rounds to compute the expected reward on the current round.
LinRel, however, is not only a more complicated algorithm but also requires solving an SVD (or eigendecomposition) of a symmetric matrix, while LinUCB only requires inverting the same matrix.

5 REGRET ANALYSIS

In this section we UCB, and prove

introduce its regret

aismbooduifineddedvebrysioOn~(of dLTin)-.

While experiments show LinUCB is probably suffi-

cient in practice Li et al. [2010], there is technical dif-

ficulty in analyzing it. In our analysis, we need the

predicted set of rewards on the current round to be

computed from a linear combination of rewards that

are independent random variables, in order to apply

the Azuma/Hoeffding inequality.

However, LinUCB has the problem that predictions in later rounds are made using previous outcomes. To handle this problem, we modify the algorithm into BaseLinUCB (Algorithm 2) which assumes statistical independence among the samples, and then use a master algorithm SupLinUCB (Algorithm 3) to ensure the assumption holds. This technique is similar to the LinRel/SupLinRel decomposition by Auer [2002].

Algorithm 2 BaseLinUCB: Basic LinUCB with Linear Hypotheses at Step t
0: Inputs:   R+, t  {1, 2, · · · , t - 1} 1: At  Id +  t x,a x,a 2: bt   t r,a x,a 3: t  At-1bt 4: Observe K arm features, xt,1, xt,2, · · · , xt,K  Rd 5: for a  [K] do
6: wt,a   xt,aAt-1xt,a
7: r^t,a  t xt,a 8: end for

5.1 Analysis for BaseLinUCB For convenience, define

Then,

st,a =

xt,aA-t 1xt,a  R+

Dt = x,a  t  R|t|×d yt = [r,a ] t  R|t|×1

At = Id + Dt Dt and bt = Dt yt
Furthermore, we denote the eigendecomposition of At by At = UttUt , where t = diag(t,1, t,2, . . . , t,d) contains eigenvalues of At in the diagonal entries, and Ut  Rd×d is a unitary matrix. Note that 1,j = 1 for all j as At is initialized to Id.
Lemma 1. Suppose the input index set t in BaseLinUCB is constructed so that for fixed x,a with   t, the rewards r,a are independent random variables with means E[r,a ] = x,a . Then, with probability at least 1 - /T , we have for all a  [K] that
r^t,a - xt,a  ( + 1)st,a.

Proof. Using notation in the algorithm descriptions of BaseLinUCB, we have

r^t,a - xt,a = xt,at - xt,a = xt,aA-t 1bt - xt,aA-t 1 Id + Dt Dt  = xt,aA-t 1Dt yt - xt,aA-t 1  + Dt Dt = xt,aAt-1Dt (yt - Dt) - xt,aA-t 1,
and since   1,

r^t,a - xt,a

 xt,aA-t 1Dt (yt - Dt) (1) + A-t 1xt,a .

210

Contextual Bandits with Linear Payoff Functions

Algorithm 3 SupLinUCB (adapted from Auer [2002])
0: Inputs: T  N
1: S  ln T 2: ts   for all s  [T ] 3: for t = 1, 2, . . . , T do 4: s  1 and A^1  [K]
5: repeat 6: Use BaseLinUCB with st to calculate the
width, wts,a, and upper confidence bound, r^ts,a + wts,a, for all a  A^s. 7: if wts,a  1/ T for all a  A^s then 8: Choose at = arg maxaA^s r^ts,a + wts,a 9: Keep the same index sets at all levels:
st+1  st for all s  [S]. 10: else if wts,a  2-s for all a  A^s then 11: A^s+1  {a  A^s | r^ts,a + wts,a 
maxa A^s r^ts,a + wts,a - 21-s} 12: s  s + 1.
13: else 14: Choose at  A^s such that wts,at > 2-s. 15: Update the index sets at all levels:

st+1 

ts  {t}, ts ,

if s = s .
otherwise

16: end if 17: until an action at is found. 18: end for

The right-hand side above decomposes the prediction
error into a variance term (first) and a bias term (sec-
ond). Due to statistical independence of samples indexed in t, we have E[yt - Dt] = 0, and, by Azuma's inequality,

Pr xt,aA-t 1Dt (yt - Dt) > st,a



2 exp

-

22st2,a DtA-t 1xt,a

2

 2 exp(-22)

 =,
TK

where the last inequality is due to the following fact:

st2,a = xt,aAt-1xt,a = xt,aAt-1(Id + Dt Dt)A-t 1xt,a  xt,aA-t 1Dt DtAt-1xt,a = DtA-t 1xt,a 2 .

Now applying a union bound, we can guarantee, with probability at least 1-/T , that for all actions a  [K],

xt,aAt-1Dt (yt - Dt)  st,a.

We next bound the second term in Equation 2:

A-t 1xt,a

= 

=

xt,aA-t 1IdA-t 1xt,a xt,aA-t 1(Id + Dt Dt)At-1xt,a xt,aA-t 1xt,a = st,a.

Combining the two upper bounds above finishes the proof.

Lemma 2 (Auer [2002], Lemma 11). Suppose t+1 =

t  {t} in BaseLinUCB. Then, the eigenvalues of

At+1 can be arranged so that t,j  t+1,j for all j

and

st2,at



d
10
j=1

t+1,j - t,j . t,j

Lemma 3. Using notation in BaseLinUCB and assuming |T +1|  2, we have

st,at  5 d |T +1| ln |T +1|.
tT +1

Proof. The proof is similar to the proof of Auer [2002, Lemma 13], but modified to handle the difference between our algorithms. For convenience, define  = |T +1|. Lemma 2 implies

st,at =

tT +1

tT +1

d
10
j=1

t+1,j - 1 . t,j

The function

f=
t

d
(htj - 1)
j=1

is maximized under the constraints

d

htj  1 and

htj  C,

j=1 t

when

C 1/|| htj = d

for all t   and j  [d], according to Lemma 8.2

In our context, we have

t+1,j  1 t,j
2The claim was made by Auer [2002] without proof.

211

Wei Chu, Lihong Li, Lev Reyzin, Robert E. Schapire

and

d t+1,j j=1 tT +1 t,j

d
= T +1,j
j=1
= xtat 2 + d
tT +1
  + d,

and so

 st   10d
tT +1

+d

1/
-1

d

   10d ( + 1)1/ - 1.

When   2, according to Lemma 9, we have ( + 1)1/ - 1  2.5 ln , 
and hence

st  25d ln  = 5 d ln .
tT +1

5.2 Analysis for SupLinUCB
In this section, we make use of lemmas in the previous subsections and complete the regret analysis for SupLinUCB. The following lemma is critical for our application of BaseLinUCB. Lemma 4 (Auer [2002], Lemma 14). For each s  [S], each t  [T ], and any fixed sequence of feature vectors xt,at with t  st , the corresponding rewards rt,at are independent random variables such that E[rt,at ] = xt,at . Lemma 5 (Auer [2002], Lemma 15). With probability 1 - S, for any t  [T ] and any s  [S], the following hold:
1. |r^t,a - E[rt,a]|  wt,a for any a  [K],
2. at  A^s, and 3. E[rt,at ] - E[rt,a]  23-s for any a  A^s.
Lemma 6. For all s  [S],
Ts +1  5 · 2s 1 + 2 d Ts +1 .
Proof. The proof is a small modification of that in Auer [2002, Lemma 16].

Observing that 2-s  1 , given the previous lemmas,
T
the main theorem follows using a similar argument as Auer [2002]. Theorem 1. If SupLinUCB is run with
1 2T K  = ln ,
2 then with probability at least 1 - , the regret of the algorithm is
O T d ln3(KT ln(T )/) .

6 A MATCHING LOWER BOUND

In this section, we prove the following lower bound that matches the upper bound in Theorem 1 up to logarithmic factors.
Theorem 2. For the contextual bandit problem with linear payoff functions, for any number of trials T and K actions (where T  K  2), for any any algorithm A choosing action at at time t, there is a constant  > 0, for d2  T a sequence of d-dimensional vectors xt,a, such that

T T

E

max
a

xt,a

-

rt,at   T d.

t=1 t=1

The following lemma will be useful for our lower bound proof:

Lemma 7 (Auer et al. [2002b], Theorem 5.1). There is a constant  > 0 s.t. for any bandit algorithm A choosing action at at time t and any T  K  2, if arm a is chosen uniformly at random and the proba-

bility

slot

machine

pays

1

is

set

to

pi

=

1 2

+

1 4

K T

and

the

rest

pay

1

w.p.

1 2

then

T
E piT - rt,at
t=1

   KT.

Proof. We will use a technique similar to Abe et al. [2003] to prove the lower bound for our setting.
We divide our T rounds into m = (d - 1)/2 groups of T = 2T /(d - 1) rounds such that each group 1, 2, . . . , m has a different best action. We will then use Lemma 7 for each group independently.
We say time step t belongs in group r if t/m = r. For all t and all actions, we let xt,a have a 1/2 in the first component and also 1/2 in components 2r and 2r + 1, and 0 in the remaining components.
The true vector  will have a 1/2 in its first coordinate, and values 1/T for either coordinate 2r or

212

Contextual Bandits with Linear Payoff Functions

2r + 1 for each group r (and 0 in the other)--setting  in this manner constrains d2  T , as otherwise we would violate the condition that   1.
Our choice of  induces expected rewards that corresponds to a bandit problem where, in each group, one of two arms (either the rth or r + 1st) pays off with probability 1/4 + 1/(2T ) and the other with probability 1/4.

Scaling by 2 and applying Lemma 7 (with K=2) inde-

pendently for each group, we get a per-group regret of



 T or 

T d

for some constants

 ,

> 0. Sum-

ming over the (d - 1)/2 groups finishes the proof.

We note that this bound is within a polylogarithmic factors of LinUCB and LinRel.
We can compare this with work of Abe et al. [2003] that gives regret of (K1/4T 3/4) for this problem; they also give an algorithm with an almost-matching upper bound on regret of O(K1/2T 3/4)--this dependence on T 3/4 shows that our ( dT ) lower bound requires the constraint d2  T .

7 CONCLUSIONS

In this paper we analyze a polynomial-time algorithm

for the contextual bandit problem with linear payoffs.

The algorithm is simpler and more robust in practice

than its precursor. that a regret of O~

We also give a lower bound showing T d cannot be improved, modulo

logarithmic factors.

However, like Auer [2002] and Abe et al. [2003], we
make the realizability assumption that there exists a vector  for which E[rt | xt,a] = xt,a. Ideally, we would like to be able to handle the agnostic case. Com-
peting with linear predictors without assuming one
perfectly predicts the expected rewards remains an in-
teresting open problem.

Acknowledgements
We thank an anonymous referee from an earlier version of this paper for correcting our proof of Theorem 2.
This work was done while Lev Reyzin and Robert E. Schapire were at Yahoo! Research, New York. Lev Reyzin acknowledges that this material is based upon work supported by the National Science Foundation under Grant #0937060 to the Computing Research Association for the Computing Innovation Fellowship program.

References
Naoki Abe, Alan W. Biermann, and Philip M. Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37(4):263­293, 2003.
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango, Nitin Motgi, Seung-Taek Park, Raghu Ramakrishnan, Scott Roy, and Joe Zachariah. Online models for content optimization. In Advances in Neural Information Processing Systems 21 (NIPS-08), pages 17­24, 2009.
Rajeev Agrawal. Sample mean based index policies with o(log n) regret for the multi-armed bandit problem. Advances in Applied Probability, 27(4): 1054­1078, 1995.
Peter Auer. Using confidence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3:397­422, 2002.
Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2­3):235­256, 2002a.
Peter Auer, Nicolo` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32 (1):48­77, 2002b.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the Twenty-First Annual Conference on Learning Theory (COLT-08), pages 355­366, 2008.
Leslie Pack Kaelbling. Associative reinforcement learning: Functions in k-DNF. Machine Learning, 15(3):279­298, 1994.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4­22, 1985.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances in Neural Information Processing Systems 20, pages 1096­1103, 2008.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In WWW, pages 661­670, 2010.
Nicos G. Pavlidis, Dimitris K. Tasoulis, and David J. Hand. Simulation studies of multi-armed bandits with covariates. In Proceedings on the Tenth International Conference on Computer Modeling and Simulation (UKSIM-08), pages 493­498, 2008. Invited paper.

213

Wei Chu, Lihong Li, Lev Reyzin, Robert E. Schapire

Paat Rusmevichientong and John N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395­411, 2010.
Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, 19(4):1978­ 2002, 1991.
Alexander L. Strehl, Chris Mesterharm, Michael L. Littman, and Haym Hirsh. Experience-efficient learning in associative bandit problems. In Proceedings of the Twenty-Third International Conference on Machine Learning (ICML-06), pages 889­896, 2006.
Thomas J. Walsh, Istva´n Szita, Carlos Diuk, and Michael L. Littman. Exploring compact reinforcement-learning representations with linear regression. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI-09), pages 591­598, 2009. A corrected version is available as Technical Report DCS-tr-660, Department of Computer Science, Rutgers University, December, 2009.
Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American Statistics Association, 74(368):799­806, 1979.

A TECHNICAL LEMMAS

We state and prove two technical lemmas that were useful for the regret analysis in Section 5.
Lemma 8. The function

f=
t

d
ctj
j=1

is maximized, under the constraints

d

ctj  0 and

(ctj + 1)  C,

j=1 t

for some constant C > d when

ctj =

C

1/||
-1

d

for all t   and j  [d].

Letting the partial derivative w.r.t. ci be 0, we have

 -1/2

L 1 d



 c i

=

2  cj 
j=1

+

c i

+

1

(cti
t

+

1)

=

0.

It can then be seen that at a stationary point, all ctj are identical, yielding

ctj =

C

1/||
- 1.

d

It can be seen easily that ctj > 0.
We now compute the Hessian matrix, 2f , which is clearly negative definite at the stationary point, thus proving the stationary point is indeed a maximizer of the constrained optimization problem.
Lemma 9. If   2, then

( + 1)1/ - 1  2.5 ln . 

Proof. It is equivalent to show

ln( + 1)  ln

2.5 1 + ln 

.



Since   2 and ln(+1)/ ln  is a decreasing function of , it suffices to show

ln 3 ln   ln

2.5 1 + ln 

,

ln 2 



which can be verified.

Proof. Compute the Lagrangian function using the second constraint, and we have

L() =
t


dd



ctj +  

(ctj + 1) - C .

j=1

j=1 t

214

