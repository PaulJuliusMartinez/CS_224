In Advances in Neural Information Processing Systems 10 (Proceedings of NIPS'97), pp.1050­1056. MIT Press.
Multi-time Models for Temporally Abstract Planning
Doina Precup, Richard S. Sutton University of Massachusetts Amherst, MA 01003 dprecup rich @cs.umass.edu
Abstract
Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a gridworld planning task.
The need for hierarchical and abstract planning is a fundamental problem in AI (see, e.g., Sacerdoti, 1977; Laird et al., 1986; Korf, 1985; Kaelbling, 1993; Dayan & Hinton, 1993). Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making (Peng & Williams, 1993, Moore & Atkeson, 1993; Sutton and Barto, 1998). However, current model-based reinforcement learning is based on one-step models that cannot represent common-sense, higher-level actions. Modeling such actions requires the ability to handle different, interrelated levels of temporal abstraction. A new approach to modeling at multiple time scales was introduced by Sutton (1995) based on prior work by Singh , Dayan , and Sutton and Pinette . This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. However, that work was concerned only with predicting the environment. This paper summarizes an extension of the approach including actions and control of the environment [Precup & Sutton, 1997]. In particular, we generalize the usual notion of a

primitive, one-step action to an abstract action, an arbitrary, closed-loop policy. Whereas prior work modeled the behavior of the agent-environment system under a single, given policy, here we learn different models for a set of different policies. For each possible way of behaving, the agent learns a separate model of what will happen. Then, in planning, it can choose between these overall policies as well as between primitive actions.
To illustrate the kind of advance we are trying to make, consider the example shown in Figure 1. This is a standard gridworld in which the primitive actions are to move from one grid cell to a neighboring cell. Imagine the learning agent is repeatedly given new tasks in the form of new goal locations to travel to as rapidly as possible. If the agent plans at the level of primitive actions, then its plans will be many actions long and take a relatively long time to compute. Planning could be much faster if abstract actions could be used to plan for moving from room to room rather than from cell to cell. For each room, the agent learns two models for two abstract actions, one for traveling efficiently to each adjacent room. We do not address in this paper the question of how such abstract actions could be discovered without help; instead we focus on the mathematical theory of abstract actions. In particular, we define a very general semantics for them--a property that seems to be required in order for them to be used in the general kind of planning typically used with Markov decision processes. At the end of this paper we illustrate the theory in this example problem, showing how room-to-room abstract actions can substantially speed planning.

ROOM
AA1 AA2

HALLWAYS G?

4 unreliable primitive actions

up

left

right

Fail 33% of the time

down

8 abstract actions
(to each room's 2 hallways)

Figure 1: Example Task. The Natural abstract actions are to move from room to room.

1 Reinforcement Learning (MDP) Framework

In reinforcement learning, a learning agent interacts with an environment at some discrete,
lowest-level time scale Ø ¼ ½ ¾ On each time step, the agent perceives the state of

the environment, ×Ø, and on that basis chooses a primitive action, Ø. In response to each primitive action, Ø, the environment produces one step later a numerical reward, ÖØ·½, and a next state, ×Ø·½. The agent's objective is to learn a policy, a mapping from states to

probabilities of taking each action, that maximizes the expected discounted future reward

from each state ×:

Ò½

¬Ó

Ú ´×µ

ØÖØ·½

¬ ¬

×¼

×

Ø¼

where  ¾ ¼ ½µ is a discount-rate parameter, and

denotes an expectation implicitly

conditional on the policy being followed. The quantity Ú ´×µ is called the value of state ×

under policy , and Ú is called the value function for policy . The value under the optimal

policy is denoted:

Ú£´×µ Ñ Ü Ú ´×µ

Planning compute

vinalureeinfufonrccteiomnesn, tplaeratircnuinlagrlryefÚe£r.s

to

the

use

of

models

of

the

effects

of

actions

to

¾We assume that the states are discrete and form a finite set, ×Ø ½ ¾ Ñ . This

is viewed as a temporary theoretical convenience; it is not a limitation of the ideas

pasrecsoenlut.mTnhvisecatsosrusm, ÚptioanndalÚlo£w, esaucsh

to alternatively denote the value functions, Ú and having Ñ components that contain the values of

wÚ£e,
the

Ñ states. In general, for any Ñ-vector, Ü, we will use the notation Ü´×µ to refer to its ×th

component.

¢The model of an action, , whether primitive or abstract, has two components. One is an
Ñ Ñ matrix, È , predicting the state that will result from executing the action in each
state. The other is a vector, , predicting the cumulative reward that will be received along
the way. In the case of a primitive action, È is the matrix of 1-step transition probabilities of the environment, times :

È Ì ´×µ  ×Ø·½ ×Ø × Ø

×

where È Ì ´×µ denotes the ×th column of È Ì (these are the predictions corresponding to
state ×) and ×Ø denotes the unit basis Ñ-vector corresponding to ×Ø. The reward prediction,
, for a primitive action contains the expected immediate rewards:

´×µ ÖØ·½ ×Ø × Ø

×

For any stochastic policy, , we can similarly define its 1-step model,

Ò¬

Ó

È Ì ´×µ 

×Ø·½

¬ ¬

×Ø

×

and

Ò¬

´×µ

ÖØ·½

¬ ¬

×Ø

È as:
Ó
××

(1)

2 Suitability for Planning

In conventional planning, one-step models are used to compute value functions via the

Bellman equations for prediction and control. In vector notation, the prediction and control

Bellman equations are

Ú · È Ú and Ú£ Ñ Ü · È Ú£

(2)

respectively, In planning,

where the Ñ Ü
these equalities

function is are turned

applied component-wise
into updates, e.g., Ú ·½

in

the

c·onÈtrolÚeq, uwathioicnh.

converge to the value functions. Thus, the Bellman equations are usually used to define

and compute value functions given models of actions. Following Sutton (1995), here we

reverse the roles: we take the value functions as given and use the Bellman equations to

define and compute models of new, abstract actions.

In particular, a model can be used in planning only if it is stable and consistent with the

¢Bellman equations. It is useful to define special terms for consistency with each Bellman

equation. Let È denote an
this model is said to be valid

arbitrary model (an Ñ-vector and an Ñ
for policy [Sutton, 1995] if and only

if

ÐÑÑmat½rixÈ).

The¼n

and Ú · È Ú

(3)

Any valid model can be used to compute Ú via the iteration algorithm Ú ·½

·ÈÚ .

This is a direct sense in which the validity of a model implies that it is suitable for planning.

We introduce here a parallel definition that expresses consistency with the control Bellman

peqousiattiivoene. lTemheenmtso,dÐeÑl

È is said
½È

to¼,baenndon-overpromising

(NOP)

if

and

only

if

È

has

only

Ú£ · È Ú£

(4)

where erator

the in the

relation control

holds component-wise. If a NOP model is added Bellman equation (2), this condition ensures that

inside the Ñ Ü
the true value,

oÚp£-,

will not be exceeded for any state. Thus, any model that does not promise more than it

is achievable (is not overpromising) can serve as an option for planning purposes. The

one-step models of primitive actions are obviously NOP, due to (2). It is similarly straight-

forward to show that the one-step model of any policy is also NOP.

For some purposes, it is more convenient to write a model È as a single ´Ñ·½µ¢´Ñ·½µ

matrix:

¾½   ¼  ¿

ÅÈ

We say that the model Å has been put in homogeneous coordinates. The vectors corre-
sponding to the value functions can also be put into homogeneous coordinates, by adding
an initial element that is always 1.

Using this notation, new models can be combined using two basic operations: composition

and averaging. Two models Å½ and Å¾ can be composed by matrix multiplication, yield-

ing a new model Å
diagonal matrices

,

Å½Å¾.
such that

AÈset

of

models Å can be Á, to yield a new

averaged,
model Å

weÈightedÅby.aSsuetttoonf

(1995) showed that the set of models that are valid for a policy is closed under compo-

sition and averaging. This enables models acting at different time scales to be mixed to-
gether, and the resulting model can still be used to compute Ú . We have proven that the set

of NOP models is also closed under composition and averaging [Precup & Sutton, 1997].

These operations permit a richer variety of combinations for NOP models than they do for

valid models because the NOP models that are combined need not correspond to a particu-

lar policy.

3 Multi-time models
The validity and NOP-ness of a model do not imply each other [Precup & Sutton, 1997]. Nevertheless, we believe a good model should be both valid and NOP. We would like to describe a class of models that, in some sense, includes all the "interesting" models that are valid and non-overpromising, and which is expressive enough to include common-sense notions of abstract action. These goals have led us to the notion of a multi-time model.
The simplest example of multi-step model, called the Ò-step model for policy , predicts the Ò-step truncated return and the state Ò steps into the future (times Ò). If different Ò-
step models of the same policy are averaged, the result is called a mixture model. Mixtures are valid and non-overpromising due to the closure properties established in the previous section. One kind of mixture suggested in [Sutton, 1995] allows an exponential decay of
the weights over time, controlled by a parameter ¬.

AGBG

S CDE

F

AGBF
S CDE G

Figure 2: Two hypothetical Markov environments

Are mixture models expressive enough for capturing the properties of the environment?
In order to get some intuition about the expressive power that a model should have, let
us consider the example in figure 2. If we are only interested if state G is attained, then
the two environments presented should be characterized by significantly different models.
However, Ò-step models, or any linear mixture of Ò-step models cannot achieve this goal.
In order to remediate this problem, models should average differently over all the different
trajectories that are possible through the state space. A full ¬-model [Sutton, 1995] can

distinguish between these two situations. A ¬-model is a more general form of mixture model, in which a different ¬ parameter is associated with each state. For a state , ¬
can be viewed as the probability that the trajectory through the state space ends in state
. Although ¬-models seem to have more expressive power, they cannot describe Ò-step
models. We would like to have a more general form of model, that unifies both classes.
This goal is achieved by accurate multi-time models.

Multi-time models are defined with respect to a policy. Just as the one-step model for a
policy is defined by (1), we define È to be an accurate multi-time model if and only if

È Ì ´×µ

Ò½

ÛØ Ø ×Ø

¬

¬ ¬

×¼

Ó
×

Ø½

´×µ

Ò

½

ÛØ ´Ö½

· Ö¾ · ¡ ¡ ¡ · Ø ½ÖØµ

¬

¬ ¬

×¼

Ó
×

Ø½

 dmfÛthoiØesertanrssiosbutmurae¼tteieooaØfnnh,tdhtahfesÈoanritmo½Ødaplwel½opeÛ×reit,ngaØdhnastcneadosgns½fiolo.vycreiTonasonthtemoedstetwwahteseieteishØgq-huvitthtei.ssnsIictftaeaerÛdteeoØarofatfnroatd½hrnoedbmoteÈrmfavojØaer wreci¼½taetoÛbiirgmlyeh,.estasIØl,ncl.hÛtpohTa½serhetrÛnieecm¾uwalcaaecirino,girihnfdstguiÛnÛcwgØhØetitioghsh¼aaatt,
along the trajectory is given to state Ø. The effect is that state ×Ø is the "outcome" state for
the trajectory.

The random weights along each trajectory make this a very general form of model. The

only necessary constraint is that the weights depend only on previously visited states. In

particular, we can choose weighting sequences that generate the types of multi-step models

¼d,esØcribeÒd,

in [Sutton, 1995]. If the
we obtain Ò-step models.

weighting variables are such A weighting sequence of the

that ÛÒ=1, form ÛØ

¥anØ d¼½Û¬Ø

Ø,

where ¬ is the parameter associated to the state visited on time step , describes a full

¬-model.

The main result for multi-time models is that they satisfy the two criteria defined in the previous section. Any accurate multi-time model is also NOP and valid for . The proofs of these results are too long to include here.

4 Illustrative Example
In order to illustrate the way in which multi-time models can be used in practice, let us return to the gridworld example (Figure 1). The cells of the grid correspond to the states of the environment. From any state the agent can perform one of four primitive actions, up, down, left or right. With probability 2/3, the actions cause the agent to move one cell in the corresponding direction (unless this would take the agent into a wall, in which case it stays in the same state). With probability 1/3, the agent instead moves in one of the other three directions (unless this takes it into a wall of course). There is no penalty for bumping into walls.
In each room, we also defined two abstract actions, for going to each of the adjacent hallways. Each abstract action has a set of input states (the states in the room) and two outcome states: the target hallway, which corresponds to a successful outcome, and the state adjacent to the other hallway, which corresponds to failure (the agent has wandered out of the
room). Each abstract action is given by its complete model Û ÈÛ , where is the optimal policy for getting into the target hallway, and the weighting variables Û along any trajectory
have the value 1 for the outcome states and 0 everywhere else.

Iteration #1

Iteration #2

Iteration #3

Iteration #4

Iteration #5

Iteration #6

Figure 3: Value iteration using primitive and abstract actions

The goal state can have an arbitrary position in any of the rooms, but for this illustration let

us suppose that state is 1, there

the are

goal is two no rewards

steps along

down from the right hallway. the way, and the discounting

The value
factor is 

of

¼the.gWoael

performed planning according to the standard value iteration method:

Ú ·½ Ñ Ü · È Ú

where Ú¼´×µ ¼ for all the states except the goal state (which starts at 1). In one experi-
ment, ranged only over the primitive actions, in the other it ranged over the set including both the primitive and the abstract actions.

When using only primitive actions, the values are propagated one step away on each iteration. After six iterations, for instance, only the states that are at most six steps away from the goal will be attributed non-zero values. The models of abstract actions produce a significant speed-up in the propagation of values at each step. Figure 3 shows the value function after each iteration, using both primitive and abstract actions for planning. The area of the circle drawn in each state is proportional to the value attributed to the state. The first three iterations are identical with the case when only primitive actions are used. However, once the values are propagated to the first hallway, all the states in the rooms adjacent to that hallway will receive values as well. For the states in the room containing the goal, these values correspond to performing the abstract action of getting into the right hallway, and then following the optimal primitive actions to get to the goal. At this point, a path to the goal is known from each state in the right half of the environment, even if the path is not optimal for all states. After six iterations, an optimal policy is known for all the states in the environment.

The models of the abstract actions do not need to be given a priori, they can be learned from experience. In fact, the abstract models that were used in this experiment have been learned during a 1,000,000-step random walk in the environment. The starting point for

learning was represented by the outcome states of each abstract action, along with the
htToyhpleeoagtrhrneeettihdceyalopupottliiilmcityiaelwssitUtahtear-seasscoptcieiocanttetvdoalwÉuieÍ£thfutnhicestsiteohnestÉaptoeÍ£lsi.cyWaasessosuocscieaidatetQedd-wlweiatihrtnheianthcgeh[aWabbasstttkrraaincctts,aa1cc9ttii8oo9nn]..
At the same time, we used the ¬-model learning algorithm presented in [Sutton, 1995]
to compute the model corresponding to the policy. The learning algorithm is completely online and incremental, and its complexity is comparable to that of regular 1-step TDlearning.
Models of abstract actions can be built while an agent is acting in the environment without any additional effort. Such models can then be used in the planning process as if they would represent primitive actions, ensuring more efficient learning and planning, especially if the goal is changing over time.
Acknowledgments
The authors thank Amy McGovern and Andy Fagg for helpful discussions and comments contributing to this paper. This research was supported in part by NSF grant ECS-9511805 to Andrew G. Barto and Richard S. Sutton, and by AFOSR grant AFOSR-F49620-96-1-0254 to Andrew G. Barto and Richard S. Sutton. Doina Precup also acknowledges the support of the Fulbright foundation.
References
Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5, 613­624.
Dayan, P. & Hinton, G. E. (1993). Feudal reinforcement learning. In Advances in Neural Information Processing Systems, volume 5, (pp. 271­278)., San Mateo, CA. Morgan Kaufmann.
Kaelbling, L. P. (1993). Hierarchical learning in stochastic domains: Preliminary results. In Proceedings of the Tenth International Conference on Machine Learning ICML'93, (pp. 167­173)., San Mateo, CA. Morgan Kaufmann.
Korf, R. E. (1985). Learning to Solve Problems by Searching for Macro-Operators. London: Pitman Publishing Ltd.
Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986). Chunking in SOAR: The anatomy of a general learning mechanism. Machine Learning, 1, 11­46.
Moore, A. W. & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13, 103­130.
Peng, J. & Williams, J. (1993). Efficient learning and planning within the Dyna framework. Adaptive Behavior, 4, 323­334.
Precup, D. & Sutton, R. S. (1997). Multi-Time models for reinforcement learning. In ICML'97 Workshop: The Role of Models in Reinforcement Learning.
Sacerdoti, E. D. (1977). A Structure for Plans and Behavior. North-Holland, NY: Elsevier.
Singh, S. P. (1992). Scaling reinforcement learning by learning variable temporal resolution models. In Proceedings of the Ninth International Conference on Machine Learning ICML'92, (pp. 202­ 207)., San Mateo, CA. Morgan Kaufmann.
Sutton, R. S. (1995). TD models: Modeling the world as a mixture of time scales. In Proceedings of the Twelfth International Conference on Machine Learning ICML'95, (pp. 531­539)., San Mateo, CA. Morgan Kaufmann.
Sutton, R. S. & Barto, A. G. (1998). Reinforcement Learning. An Introduction. Cambridge, MA: MIT Press.
Sutton, R. S. & Pinette, B. (1985). The learning of world models by connectionist networks. In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, (pp. 54­64).
Watkins, C. J. C. H. (1989). Learning with Delayed Rewards. PhD thesis, Cambridge University.

