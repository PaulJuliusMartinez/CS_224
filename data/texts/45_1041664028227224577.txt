From: ISMB-94 Proceedings. Copyright © 1994, AAAI (www.aaai.org). All rights reserved.

Optimally Parsing a Sequence into Different Classes
Based on Multiple Types of Evidence
Gary D. Stormo1 a2nd David Haussler
1 Dept. of Molecular, Cellular and Developmental Biology University of Colorado, Boulder, CO80309-0347 phone: 303-492-1476, FAX: 303-492-7744 stormo@beagle.colorado.edu 2 Dept. of Computer and Information Sciences
University of California at Santa Cruz, Santa Cruz, CA95064 phone: 408-459-2015, FAX: 408-459-4829 haussler@cse.ucsc.edu

Abstract
Weconsider the problem of parsing a sequence into different classes of subsequences.Twocommonexamples are finding the exons and introns in genomicsequences and identifying the secondary structure domainsof protein sequences. In each case there are various types of evidencethat are relevant to the classification, but none are completely reliable, so we expect some weighted average of all the evidence to provide improved classifications. For example, in the problemof identifying codingregions in genomic DNAt,he combineduse of evidence such as codon bias and splice junction patterns can give more reliable predictions than either type of evidence alone. Weshowthree mainresults:
1. For a given weighting of the evidence a dynamic programmingalgorithm returns the optimal parse and any number of sub-optimal parses.
2. For a given weightingof the evidence a dynamic programmingalgorithm determines the probability of the optimal parse and any numberof sub-optimal parses under a natural BoltzmannGibbs distribution over the set of possible parses.
3. Given a set of sequences with knowncorrect parses, a dynamic programmingalgorithm allows one to apply gradient descent to obtain the weightsthat ma~dmiztehe probability of the correct parses of these sequences.
Introduction
Biological macromoleeules, such as DNA,RNAand protein, are long strings from a few types of monomeric
units; DNAand RNAare each made from four different nucleic acid bases and proteins are made from twenty different aminoacids. It is both convenient and appropriate to consider the macromolecular strings as

being "texts" derived from their alphabets and methods developed for string analysis can be usefully applied (Sankoff & Kruskal 1983). The strings can often be divided into substrings, or domains, based on structural and/or functional criteria. For example,
DNAsequences contain regions that code for genes, regulatory regions that affect the expression of those genes, intergenic regions between the genes and a variety of other types of regions such as replication origins, repetitive DNAsegments and structural domains like centromeres and telomeres. RNAsequences can be divided into structural domains such as helices and loops, and pre-mRNAsequences can also be divided into functional domains of coding regions (exons) and intervening sequences (introns). Protein sequences can be divided into structural domains such as a-helices and B-sheets, and into functional domains described by various motifs (Henikoff & Henikoff 1991).
Dividing a sequence into its separate domains, or
parsing it, can be very useful in understanding important properties of the macromolecule. Auger and Lawrence (Auger & Lawrence 1989) and Sankoff
(Sankoff 1992) have published dynamic programming methods to optimally parse a sequence into domains given some types of patterns that describe the different classes of domains. These are closely related to our first result, except that we extend the method to obtain ranked suboptimal solutions. Gelfand and
Rotyberg (Gelfaud & Roytberg 1993) have also published a method for identifying exons and introns in genomic DNAthat allows the identification of suboptimal solutions. In addition to that, we explicity develop the method to utilize mutliple types of evidence in arriving at the optimal parses. Snyder and
Stormo (Snyder & Stormo 1993) have previously given a dynamic programming algorithm for optimally parsing a DNAsequence into exons and introns given the evidence from multiple statistical tests for each

Stormo 369

type. Our first result is closely related to theirs and to further work by those authors (Snyder 1994;
Batra 1993). The use of multiple types of evidence is important
in these problems. Often there are several different types of sequence features that can be used, but none are completely reliable. Using them in combination can improve classification performance. For example, in the problem of identifying the exons and introns in DNAsequences, there are ambiguous nucleotide patterns that serve as "signals" at the juntions of the two types of sequences. However, these patterns are ambiguous enough that there are manyfalse positive occurences for every true occurence. Exons and introns also differ in certain "content" statistics; they have different overall patterns of nucleotide distribu-
tions (see (Fickett & Tung1992) for several examples of content statisitics). However,the distributions overlap such that statistical tests are not sufficient for re-
liable classification. Used in combination, the "signal" and "content" methods provide increased reliability (Snyder & Stormo 1993; Stormo 1987; Brunak, Engelbrecht, &: Knudsen1991; Fields & Soderlund 1990; Guigo et al. 1992; Uberbacher & Mural 1991).
The main difficulty with employing multiple types
of evidence is determining howto weight the various types. In GRAIL(Uberbacher & Mural 1991) a neural network is used to weight different "sensors" that each monitor a statisical measure of exon content. Snyder and Stormo (Snyder & Stormo 1993) also use a neural network to obtain weights that optimize tile performance of the dynamic programmingalgorithm. In this paper we introduce a probabilistic method to optimize these weights, and give a more general interpretation of the objective function. Our intent is to provide a useful theoretical frameworkto help guide the design of these types of parsing methods for biosequences.
For simplicity this paper will use the example problem of identifying exons and introns in genomic DNA sequences. However,it should be kept in mind that the same methodology can easily be applied to the problem of protein secondary structure prediction, and many other biosequence analysis problems.
Notation
First we fix a sequence S of letters from the alphabet
A,C,G,T. The letters of S are numbered from 1 ...l, where 1
is the length of S. Fromthe sequence S several T matrices are created,
one for each statistic/~ in a collection "exonstatistics" and one for each statistic ,¢ in a collection of "intron statistics". For each i and j, where 1 < i < j < l, T~(i,j) is a real number(possibly negative) that call the "score" for the exonstatistic/~ over the region i...j of the sequence S. If/~ is a good exon statistic, then the morethe region of S from i... j looks like an
exon, the larger Tg(i, j) will be. Statistics that measure

similarity to introns are also defined in an analogous way, and these are stored in matrices T~(i,j). Note that these, and other matrices, need only be stored as half-matrices for which j > i.
The set of T matriccs are combined into two other half-matrices: 1
L) E(i,j) = ~wuTu(i,j ~t
n1(i,j) = E w,T~(i,j)
These represent composite exon and intron statistics, respectively, based on the underlying statistics {Tu} and {T~}. The weights {wu} and {w~} are adjustible real numbers, i.e. parameters of the composite exon and intron statistics.
A parse ¢ of the sequence S is a partition of S into consecutive nonemptyregions that are alternately labeled as either exon or intron. A parse can start with either an exon or an intron region, and can also end with either an exon or an intron region. IIence there are 2t possible parses of a sequenceS of length I (each possible assignment of "exon" and "intron" to the individual letters of S corresponds to a distinct and unique 2parse.)
The score of a parse ~, denoted @(¢), is defined be the sum of the LE and LI values for the regions of S that make up the parse. An example is provided in Figure 1; the parse has exons from 1..h, i..j and k..l, and introns in between. The score is the sum of the LE and LI matrix elements corresponding to those regions:

®(0) LE(1,h)+ L1 (h+ l, i- 1) + LE (i,j) +LI(j + 1,k- 1) + LE(k,l)

(1)

Finding the Optimal Parse
Given the L matrices, a simple dynamic programming algorithm can find the parse of S with the highest score
(called the "best parse"). The constraints that apply are that exons and introns alternate and are adjacent
1Withoutloss of generality, wecan assumethat one of the exonstatistics is the constantfunction1, i.e. T(i,j) = for all i,j for this statistic. Theweightfor this statistic then plays the role of the additive constant, or bias, for exons. The analogous thing can be assumedfor the intron statistics.
2For simplicity weassumethat thc entire sequencewill be parsed into exonand intron sequences. In reality there are likely to be regionson both endsof the sequencethat do not constitute completeexonsand introns so that the correct parse mayonly cover a "local" region of the sequence rather than the complete sequence. The methodpresented here, for the "global" parse of the sequence,can easily be extendedto do "local" parses, as described in (Snyder Stormo1993).

37O ISMB-94

i 1E

I h]

i

i EJ

ii I k EI

i O,J)

(]+1,k-1)
Figure 1: The horizontal line at the top represents the sequence with length I. The sequence is parsed into exons (E) in the regions 1..h, i..j, k..l and has introns (I) between. The two half-matrices store, for all regions, the log-likelihood values for the region being an exon or an intron in LE and Lx, respectively. The LE and LI elements that correspond to the parse shownat the top are indicated.

and non-overlapping. Suppose we have already parsed the prefix of S beginning at position 1 and ending at position j. Sucha parse is called a partial parse. Let the value DE(j) denote the score for the best partial parse that ends with an exon that terminates at position j, and the value DI(j) denote the score for the best partial parse that ends with an intron that terminates at position j. The following recursive definitions of DE(j) and DI(j) can be converted into a dynamic programming algorithm (see Auger and Lawrence (Auger & Lawrence 1989), Sankoff (Sankoff 1992) and Snyder and Stormo (Snyder & Stormo 1993)).
DE(O)= DI(O)
DE(j) maxl<[kL<Ej(k,j) + D,(k - 1)
DI(j)max [L ,(k,j) + DE(k - i)
l<k<j
Thescoreforthebestparseis max[DE(1),D1(1)].
The actual best parse can be determined by backtracking, whichis simplified if the k-values used in the maxima are kept along with the D vectors.
Although it gives the same best parse, we will see below that it is also useful to be able to do the recursions in the reverse direction:
D'E(I + 1) D'i(i + 1): 0
DE(j) = m<~<<,[LE(j,k)+ D'1(k 1)
D'I(j) = max[L,(j, k) + D'E(k 1) j<k<l
These D' vectors store the values of the best solutions beginning at position j, and the score of the best solution is max[D~(X),D}(1)]. Adding the plementary vectors together gives, for every position, the score of the best solution with that position as a boundary. In particular, the score for the best solution with an exon-intron boundary where the exon ends at position j is
AE(j) : DE(j) + D}(j 1)
and the analogous score for an intron-exon boundary is
Al(j) = D,(j) + D'E(j 1)
Note that all of the boundaries that contribute to the optimal solution will have the same A-values. If the parse in Figure 1 is the best parse Chest, then:
= DE(I)= D~(1) = LE(1,h)+LI(h+I,i-1)+LE(i,j)
+LI(j+ 1,k- 1) + LE(k,l) -_ AE()h= AE(j)= AE(i)= AI(O) : Al(i- 1) = A,(k- 1)

Stormo 371

1

hi

j j'

k

Figure 2: As in Figure 1, the horizontal line corresponds to the sequence of length I. Plotted above and belowthe sequence line are lines whoselengths are proportional to AE(j) and At(j), respectively. An is a region boundedby a lower line on the left and an upper line on the right, and introns are the reverse. Lines are shownonly for the optimal parse and for the highest suboptimal parse.

A simple plot of A(j) vs. j can be used similarly to Zuker's suboptimal plots of RNAstructure and sequence alignments (Zuker 1989; 1991) to shownot only the optimal solution but the best solution for all possible junctions. Figure 2 plots j versus AE(j) and --AI(j) for the optimal solution (from the example in Figure 1) and for the highest suboptimal solution, which moves the end of the second exon from j to f. Additional suboptimals can be plotted on the same graph to indicate which regions are predicted consistently and which regions have multiple parses of approximately equal score (Snyder 1994).

Calculating the Probability

of a Parse

Wenowdefine a probability distribution on the set of all parses of a given sequence. This can be used to measure how suboptimal a given suboptimal parse is, and to detect whenmost of the probability is concentrated on the single best parse (a good case, since suboptimal parses can be ignored here) or, in contrast, whenthere

are a large numberof parses that are all nearly as likely as the best parse (and hence suboptimal parses need

to be considered, e.g. for deciding if a given region is an exon).
Wedefine the probability of a parse by interpreting the score 0 as an unnormalized log-likelihood of the

joint probability of the sequence S and the parse ¢. Then for any parse ¢ of S, we define

Pr(¢IS)

--

)-~-all

Pr(S, ¢) parses ¢'

Pr(S,

°e(¢) ¢9 - Z

(2)

where Z = ~-~all parses ¢' e°(¢')" This has a natural
statistical physics interpretation as a Boltzmann-Gibbs distribution. Let H(¢) = -0(¢). The quantity If(¢) corresponds to the energya of the state configuration (parse) ¢. The normalizing constant Z is called the
3If wewantto makesure that energyis never negative, then we can define H(¢) = -O(¢) + c for a suitably large constant c. This doesn't changethe theory, since subtract-

partition function in statistical physics. Calculating the probability of a given parse, including the best parse, requires calculating Z. While in most statistical physics models this calculation rapidly becomes intractable, here it is remarkably simple. It can be done with a modified version of the dynamic programruing algorithm given above.
Let liE(j) be the sumof eO(~) over all partial parses ¢ of the prefix of S from the beginning up to position j and ending with an exou at j. Let I/~(j) be defined similarly for introns. These new Vvectors can be computed according to the following recursive equations:
rE(0)= V(0)

S E(J) - X~ er'E(k'J)v1(h-
l <k<_j

v)(j)
l <_k<_j
It is easily verified that

=

z =t'E(I)

Theprobability of the best parse is easily calculated from this result and equation (2), as is the sumof the probabilities of any numberof parses. In particular, in analogy with the function A defined above, we can calculate the probability that the parse of S has an exon ending at position j by summingthe probability of all parses of S that have this property. In order to do this calculation efficiently, we do the reverse recursive calculations of the Vvalues just as we did reverse calculations of the Dvalues in the previous section. Call these reverse V-values ~'~(j + 1) ( the sum of °(¢) over all partial parses ¢ of the suffix of S fromposition
j + 1 to the end that start with an exon at. position j + 1) and 1,~' (defined similarly for introns). Then is easily verified that the probability that the parse of S has an exon ending at position j is given by

PE(J) = I/E(J)i~Z(J + 1)
This method of calculating these probabilities is closely related to the use of the forward-backwardalgorithm to computeanalogous probabilities in IIidden Markov Models (Rabiner 1989).
Finally, we note that in practice all of the probabilities discussed in this section would be computedand displayed as log probabilities or negative log probabilities, to avoid problems with the large dynamicrange of the raw probabilities. The negative log probability of a parse yields a positive score function that is minimized to obtain the best parse.
ing a constant from ®(4)) for each ¢ does not change value of Pr(tk) in equation(2).

3"I2 ISMB-94

Determining the Best Weights
Givena collection of Nsequences, S1,. ¯., SN, for each
of which we knowthe correct parse (these sequences are the "training set"), we want to find the weighting of the different types of evidence that optimizes the performance of the algorithms described above. Snyder and Stormo (Snyder & Stormo 1993) describe the use of a neural network to obtain weights that maximize the proportion of the optimal parses that are correct. They start with arbitrary weights and obtain the optimal parse. Optimal parses that differ from the correct parse are used as training examples to find new weights that give higher scores to the correct parses. Those weights are used in another round of obtaining optimal parses which generate new training examples. This procedure is iterated until there is a plateau in the
performance, which is judged by the fraction of optimal parses that are correct. Our objective is somewhat different. Wetake a maximumlikelihood approach, so we want to find the weights that maximize the probability of the correct parse so that, even in cases where the optimal parse is not correct, the correct parse has as high a probability as possible.
Assumethat sequence Si has length li, with the bases numbered consecutively from 1...li. Let w de-
note the vector of all parameters {wv}and {w~}. For any parse ¢ of the sequence Si, the probability of this parse is denoted Pr(¢[Si, w), and is defined as in equa-
tion (2). Wecall the correct parse ¢~ for each training sequence Si. Weuse the training set to computemaximumlikelihood estimates for the parameters {wu } and {w~}, which are those that minimize
N
~- In Pr(CZlS,,w)
i=1
This might be accomplished by many methods; we will describe the method of gradient descent. This means that we will repeatedly loop through all the training examples, and for each training example S with correct parse ¢*, we will modify each parameter w in w by the rule

w:=w+r/

01nPr(¢*lS, w) 0w

(3)

where r/> 0 is a suitable "learning rate". To do this,

we need to calculate the above partial derivatives. For

simplicity, we will drop tile conditioning on S and w

in our notation. Wehave

OlnPr(¢*) Ow

_ 0 lne-°--(~') 00(¢*)

Ow Z
0o(¢-) z'

Ow

- O~ z

0 in Z Ow
(4)

where

Z' = 0e(¢) eo(~). all parses ¢ 0w

This equation tells us that when adapting the weights, the change in w should be proportional to
the derivative of the score of the correct parse minus the derivative of the average score over all parses.
The derivative of the score of the best parse is easy
to calculate, since the score function ® is linear in each adjustible weight parameter w. To calculate the derivative of the average score, we again use dynamic programming.]Are state the recursive equations for this in their most general form, to show that the method also works for more complex score functions in which the weight parameters enter in a nonlinear fashion, e.g. as in multilayer neural nets. The only requirement is
that the score O(¢) be defined as the sum of the and L; values of the exons and introns in ¢ as above, and that the derivatives of these LE and Lt values with respect to each of the adjustible parameters w be efficiently computed.
The derivative of the average score is Z'/Z. This can be computed by first computing Z, which we did in the previous section, and then Z', which is the unnormMizedweighted average of the derivative. Before deriving the algorithm to computeZ', it is worthwhile considering a simple example. Imagine a parse ¢ that contains within it an exon from k..j. We will denote the partial parse that ends with that exon by ¢(j) and its score by

~(¢(j)) = O(¢(k - LE(k,j ),

the score for the exon from k..j and the score of the partial parse ending at k - 1. The unnormalized likelihood of that partial parse is
e°(¢(/)) = e°(¢(k-1))eLE(k,j )

and the derivative of the score of that partial parse is

00(¢(j)) _ 00(¢(k- 1)) + OLE(k, j)
Ow Ow Ow
The unnormalized weighted derivative is just product of those last two equations:

the

00(¢(j))eO(¢(~)) = eL~(k,i)[00(¢(k-- 1))ee(~(k_~)) Ow Ow

-~OLOF'w(k'J)ee(¢(k-U)]

(5)

Now,in order to compute Z' let us define UE(j) to

be the unnormalized weighted average derivative over

all partial parses with an exon ending at position j, and define UI(j) similarly for introns. Weuse the no-

tation ¢(j)z for the partial parse ending with an exon

at position j, and similarly ¢(j)I for the partial parse

ending with an intron at j. Then

UE(O=)U,(O)

and using equation (5)

u (j)

Oo(¢(j) eO(~(~))

Stormo 373

= Z:: Z:: l_<k_<¢j(./)~
aLE(k, j) eO(¢(~_1))] q" Ow

-

used in IIidden MarkovModeltraining algorithms (Rabiner 1989) (see also (Bourlard & Morgan1993)). other direction is to explore more complexscore functions. Wehave already mentioned that the methods described here can easily be extended to score functions

=
l<_k<j

,p(t-1)~

aw

in which the LE and Lz values are smooth nonlinear functions of the weights w. It is also possible to handle more complexconstraints on the order in which differ-

÷aLE(k,j)aw E eO(¢(k-1))]

ent kinds of domains can appear in the parse. In this paper we just considered alternating exons and introns. However,in moregeneral situations, with several kinds

=
1<~_<j

aLE(k, j) VI(k - 1)] of domains, we can use a grammarto specify which or-

aw

derings of the domains are possible, and then restrict attention to only the parses that can be derived from

where Vt is the function computedin the previous section. Similarly,

the grammar. If we make the grammar probabilistic, this leads to a mutual generalization of this work and the workon stochastic context-free grammarsfor biose-

u~(j)

=

~ eL'(k'~)[UE(k--1)+
l<k<j

aL~(Ok,wj)

VE(k-1)].

Finally, it is clear that

quence analysis (Sakakibara el al. 1994). The details of this remain to be workedout.
A much more complex situation arises when distant pairs of domains in the parse can interact, such as the fl-strands in a parse of a protein sequence.

Z' = UE(I) + U~(l),
hence these recursive equations give an efficient method to compute Z'.

A good score function should take into account the evidence for such interactions. However, it appears that whenone defines such a score function, then the simple dynamic programming methods developed here

can no longer be used. Somepossibilities for reme-

Conclusions
Wehave addressed the problem of parsing a sequence into a set of classes. Weassume that for each class we have somestatistical evidence, in the form of loglikelihoods, that can help decide if any subsequence belongs to that class. Weshow that, given a weight-

dying this problem might be found by somehowmarrying our approach with either the branch-and-bound
approach taken Lathrop and Smith in their work on
threading/parsing proteins (Lathrop & Smith 1994)
the Gibbs sampling approach (Geman & Geman1984; Lawrence et al. 1993; Bryant & Lawrence 1993).

ing of the evidence, we can find the optimal parse and any number of ranked suboptimal parses. Wefurther showthat the probability of any parse, or collection
of parses, can also be calculated efficiently. Finally, we show that, given a set of sequences with known parses, we can use them as a training sct to arrive at the weights for the different types of evidence that maximize the probablity of the correct parses. While
the example we used was parsing genomic DNAinto exons and introns, the method can be applied to many

Acknowledgements
We thank Anders Krogh, Yoram Singer, Eric Snyder and Sajeev Batra for discussions of this work and for implementing and testing someof the ideas presentcd. We thank Chip Lawrence for comments on a draft. G.D.S. was supported by grants from NIH (ITG00249) and DOE (ER61606). D.H. was supported by NSF grants CDA-9115268and IRI-9123692.

related problems as well, such as the problem of parsing protein sequences into c~-helix, fl-strand and coil

References

regions. Wecan also use an analogous method to find

Auger, I. E., and Lawrence, C. E. 1989. Algorithms

the substitution and indel parameters that maximize the probability of the correct alignments of a set of sequences.
Apart from extensive testing of this method, this workstill leaves a numberof directions for further research. One would be to try to extend tile training algorithm so that it can use some unparsed training sequences along with the ones for which parses are available, as these unparsed sequences will in general

for the optimal identification of segment neighborhoods. Bull. Math. Biol. 51:39-54.
Batra, S. 1993. A new algorithm for protein structure prediction: using neural nets with dynamicprogramming. Master's thesis, University of Colorado, Boulder.
Bourlard, H., and Morgan, N. 1993. Connectionist Speech Recognition, a hybrid approach. Kluwer.

be much easier to obtain. One possibility would be to try an Expectation-Maximization algorithm, treating the parse as missing data. Analogous methods are

Brunak, S.; Engelbrecht, J.; and Knudsen, S. 1991. Prediction of humanmrna donor and acceptor sites from the dna sequence. J. Mol. Biol. 220:49-65.

374 ISMB-94

Bryant, S. H., and Lawrence, C. E. 1993. An empirical energy function for threading protein sequence through the folding motif. Proteins 16:92-112.
Fickett, J. W., and Tung, C.-S. 1992. Assessment of protein coding measures. Nucl. Acids Res. 20:64416450.
Fields, C. A., and Soderlund, C. A. 1990. gm: a practical tool for automating DNAsequence analysis. ComputerAppl. Biol. Sci. 6:263-270.
Gelfand, M. S., and Roytberg, M. A. 1993. Prediction of the exon-intron structure by a dynamic programming approach. BioSystems 30:173-182.
Geman, S., and Geman, D. 1984. Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images. IEEETrans. on Pattern Analysis and Machine Intelligence 6:721-742.
Guigo, R.; Knudsen, S.; Drake, N.; and Smith, T. 1992. Prediction of gene structure. J. Mol. Biol. 226:141-157.
Henikoff, S., and Henikoff, J. G. 1991. Automated asembly of protein blocks for database searching. Nucl. Acids Res. 23:6565-6572.
Lathrop, R. H., and Smith, T. F. 1994. A branch-andbound algorithm for optimal protein threading with pairwise (contact potential) aminoacid interactions. In Proceedings of the 27th Hawaii International Conference on System Sciences. Los Alamitos, CA: IEEE Computer Society Press.
Lawrence, C. E.; Altschul, S.; Boguski, M.; Liu, J.; Neuwald, A.; and Wootton, J. 1993. Detecting subtle sequence signals: A Gibbs sampling strategy for multiple alignment. Science 262:208-214.
Rabiner, L. R. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE 77:257-286.
Sakakibara, Y.; Brown, M.; Mian, I. S.; Underwood, R.; and Haussler, D. 1994. Stochastic context-free grammars for modeling RNA.In Proceedings of the Hawaii International Conference on System Sciences. Los Alamitos, CA: IEEE Computer Society Press.
Sankoff, D., and Kruskal, J. 1983. Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. Addison-Wesley.
Sankoff, D. 1992. Efficient optimal decomposition of a sequence into disjoint regions, each matched to some template in an inventory. Math. Biosci. 111:279-293.
Snyder, E. E., and Stormo, G. D. 1993. Identification of coding regions in genomic DNAsequences: an application of dynamic programming and neural networks. Nucl. Acids Res. 21:607-613.
Snyder, E. E. 1994. Identification of Protein Coding Regions in Genomic DNA. Ph.D. Dissertation, University of Colorado, Boulder.

Stormo, G. D. 1987. Identifying coding sequences. In Nucleic Acid and Protein Sequence Analysis, A Practical Approach, 231-258. IRL Press. Uberbacher, E. C., and Mural, R. J. 1991. Locating protein-coding regions in humandna sequences by a multiple sensor-neural network approach. Proc. Natl. Acad. Sci. USA88:11261-11265. Zuker, M. 1989. Onfinding all suboptimal foldings of an R.NAmolecule. Science 244:48-52. Zuker, M. 1991. Suboptimal sequence alignment in molecular biology alignment with error analysis. J. Mol. Biol. 221:403-420.
Stormo 375

