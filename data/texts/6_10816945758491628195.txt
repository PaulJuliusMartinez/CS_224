Tensor Network Computations in Quantum Chemistry
Charles F. Van Loan Department of Computer Science
Cornell University
Joint work with Garnet Chan, Department of Chemistry and Chemical Biology, Cornell University

The Matrices are Big
The Google Matrix 
H
2d-by-2d d = 30 (now), d = 100 (soon), d = 1000 (eventually)

Modelling Electron Interactions
Have d "sites" (grid points) in physical space. The goal is compute a wave function, an element of a 2d Hilbert space.
The Hilbert space is a product of d 2-dimensional Hilbert spaces. (A site is either occupied or not occupied.)
A (discretized) wavefunction is a d-tensor, 2-by-2-by-2-by-2...

What a Sample H Matrix Looks Like

d = 8:

H = i,j ij и Pij + i,j,k, ijk и Qijk

P3,7 = I2  I2  D  E  E  E  C  I2 Q1,3,5,7 = D  E  D  E  C  E  C  I2



E

= 

1 0



0 -1

,



D

= 

0 0


10 ,



C

= 

0 1


00 

H is data sparse. It is defined by d4 numbers.

The Curse of Dimensionality
Wavefunction-related computations lead to Hx = x and Hx = b problems.
However x is SO BIG that it cannot be stored explicitly.
Idea: Approximate x with a tensor network that captures the essence of the electron interactions.

Outline: и What is a Tensor Network? и Three Illustrative Tensor Network Computations
High-level Message: Big n via Big d Requires Tensor-Based Computational Thinking

The Next Chapter Is About to Be Written...
Scalar-Based Computational Thinking (1960's)  (1960's)
Matrix-Based Computational Thinking (1980's)  (1980's)
Block-Matrix-Based Computational Thinking (2000's)  (2000's)
Tensor-Based Computational Thinking

What Is a Tensor Network?

A tensor network is a tensor of high dimension that is built up from many sparsely connected tensors of low-dimension.
A(2) A(5) A(3) A(4) A(6) A(7)
A(1) A(8) A(10) A(9)
Nodes are tensors and the edges are contractions.

A 5-Site Linear Tensor Network
A(1) A(2) A(3) A(4) A(5)
A(1) : 2 О m A(2) : m О m О 2 A(3) : m О m О 2 A(4) : m О m О 2 A(5) : m О 2
m is a parameter, typically around 100.

If a(1:2, 1:2, 1:2, 1:2, 1:2) is 5-site LTN then...
a(1,1,1,1,1) A(1) A(2) A(3) A(4) A(5)

If a(1:2, 1:2, 1:2, 1:2, 1:2) is 5-site LTN then...
a(2,1,1,1,1) A(1) A(2) A(3) A(4) A(5)

If a(1:2, 1:2, 1:2, 1:2, 1:2) is 5-site LTN then...
a(1,2,1,1,1) A(1) A(2) A(3) A(4) A(5)

If a(1:2, 1:2, 1:2, 1:2, 1:2) is 5-site LTN then...
a(2,2,1,1,1) A(1) A(2) A(3) A(4) A(5)

If a(1:2, 1:2, 1:2, 1:2, 1:2) is 5-site LTN then...
a(1,1,2,1,1) A(1) A(2) A(3) A(4) A(5)

LTN(5,m): Scalar Definition
a(n1, n2, n3, n4, n5) =
mmmm

i1 = 1 i2 = 1 i3 = 1 i4 = 1
A(1)(n1, i1)  A(2)(i1, i2, n2)  A(3)(i2, i3, n3)  A(4)(i3, i4, n4)  A(5)(i4, n5)
A length-2d vector that is represented by O(dm2) numbers.

LTN(5,m): BlockVec Product Definition





a(1, a(2, a(1,
a(1, a(2,

1, 1, 1, 1, 2, 1,
.. 2, 2, 2, 2,

1, 1, 1,
2, 2,

1) 1) 1)
2) 2)



=

 

A(1)(1, A(1)(2,

:) :)

 



 

A(2)(:, A(2)(:,

:, :,

1) 2)

 



 

A(3)(:, A(3)(:,

:, :,

1) 2)

 



 

A(4)(:, A(4)(:,

:, :,

1) 2)

 



 

A(5)(:, A(5)(:,

1) 2)

 

The Block Vec Product 



F1G1   FF12  GG21 = FFF122GGG122  

 





A 10-Site General Tensor Network

A(2)

A(5)

A(3) A(4) A(6) A(7) A(1)
A(8) A(10)

A(9)
At each site there is a tensor. Its dimension is k + 1 where k is the number of site neighbors. E.g.,

A(2) = A(2)(1:m, 1:m, 1:2) A(4) = A(4)(1:m, 1:m, 1:m, 1:m, 1:2)

A 10-Site General Tensor Network

A(2)

A(5)

A(3) A(4) A(6) A(7) A(1)
A(8) A(10)

A(9)

Each edge represents a contraction, e.g.,

m i23=1

A(2)(i23,

i25,

n2)



A(3)(i13,

i23,

i34,

n3)

a(n1, n2, n3, n4, n5, n6, n7, n8, n9, n10)

= i1,3 , i1,8 , i2,3 , i2,5 , i3,4 , i4,5 , i4,6 , i4,10 , i6,7 , i7,10 , i8,9 , i8,10 , i9,10

A(1)(i1,3 , i1,8 , n1)  A(2)(i2,3 , i2,5 , n2)  A(3)(i1,3 , i2,3 , i3,4 , n3)  A(4)(i3,4 , i4,5 , i4,6 , i4,10 , n4)  A(5)(i2,5 , i4,5 , n5)  A(6)(i4,6 , i6,7 , n6)  A(7)(i6,7 , i7,10 , n7)  A(8)(i1,8 , i8,9 , n8)  A(9)(i8,9 , i9,10 , n9)  A(10)(i4,10 , i7,10 , i9,10 , n10)

и Order of Operations
и Blocking и Transposition и Tensor BLAS

Sample Tensor Network Computations

H times a Tensor Network
H = i,j ij и Pij + i,j,k, ijk и Qijk The Pij and Qijk are d-fold Kronecker products of 2-by-2 matrices, many of which are I2 The "action" of Q3,6,19,50 is decoupled from the action of Q2,20,27,48.
Parallelization opportunities in the style of parallel Jacobi.

2-norm of a Tensor Network


 = w  X  и и и  X  z1TT  w X X z2


2122 





1,d-1 2,d-1




21 





=

(w1



w1

+

w2



w2)T

d-1
 (X1k



X1k

+

X2k




X2k )

(z1



z1

+

z2



z2)

k=2

2-norm of a Tensor Network


 = w  X  и и и  X  z1TT  w X X z2


1222 





1,d-1 2,d-1




21 





=

(w1



w1

+

w2



w2)T

d-1
 (X1k



X1k

+

X2k




X2k )

(z1



z1

+

z2



z2)

k=2

An ability to reason at the index-level about contractions and the order of their evaluation.
An ability to reason at the block level in order to expose fast, underlying Kronecker product operations.

QR/SVD's of Tensor Network-Related Matrices

Let's compute the QR factorization of this:

    

M = BB  CC  FF  GG21 

12 

21 

21 

Assume every matrix is m-by-m.

Recall...



F1G1   FF21  GG12 = FFF212GGG212  

 





and note...



Q1RG1







QQ21 R  GG12 = QQQ212RRRGGG221 = QQ21  RRGG12

 









    

 

QR/SVD's of Tensor Network-Related Matrices

Let's compute the QR factorization of this:

    

M = BB  CC  FF  GG21 

12 

21 

12 







BB = QQ R21 

1B B  2B

QR/SVD's of Tensor Network-Related Matrices

Let's compute the QR factorization of this:

 

 



M = Q  R C  F  G1B  Q R C F G2B

BB 12 

12 

12 

QR/SVD's of Tensor Network-Related Matrices

Let's compute the QR factorization of this:

 

 



M = Q  R C  F  G1B  Q R C F G2B

BB 12 

21 

12 







RR CC = QQ RBB 21 

1C C  2C

QR/SVD's of Tensor Network-Related Matrices

Let's compute the QR factorization of this:

  





M = Q  Q  R F  G1B  Q Q R F G2B

1C  2C

CC 21 

12 

QR/SVD's of Tensor Network-Related Matrices

Done!

       

M = QQ  QQ  QQ  QQ R21BB 

21CC 

12FF 

12GG M 

(SVD of RM can be used to obtain SVD of M .)
In general, can get QR (or SVD) of M  IRm2dОm in O(dm3) flops.
Lots of product-decompositions when working with tensor networks.

Superpositioning

Given

A

=

 

A(1)(1, A(1)(2,

:) :)

 



 

A(2)(:, A(2)(:,

:, :,

1) 2)

 

 иии



 

A(d-1)(:, A(d-1)(:,

:, :,

1) 2)

 



 

A(d)(:, A(d)(:,

1) 2)

 

B

=

 

B(1)(1, B(1)(2,

:) :)

 



 

B(2)(:, B(2)(:,

:, :,

1) 2)

 



иии



 

B(d-1)(:, B(d-1)(:,

:, :,

1) 2)

 



 

B(d)(:, B(d)(:,

1) 2)

 

find

C

=

 

C (1) (1, C (1) (2,

:) :)

 



 

C (2) (:, C (2) (:,

:, :,

1) 2)

 



иии



 

C (d-1) (:, C (d-1) (:,

:, :,

1) 2)

 



 

C (d) (:, C (d) (:,

1) 2)

 

so that (A + B) - C F = min Something better that alternating least squares?

Summary
The tensor network paradigm in quantum chemistry is a great venue to promote the idea of tensor-based computational thinking:
и Data structures. How do we lay out a tensor network in memory?
и Identifying important kernel operations and developing BTAS.
и Low rank representations to handle intermediate contractions.
и Nearness problems and Multilinear Optimization.

