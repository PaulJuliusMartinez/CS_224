JMLR: Workshop and Conference Proceedings vol 35:1­26, 2014

Follow the Leader with Dropout Perturbations

Tim van Erven De´partement de Mathe´matiques, Universite´ Paris-Sud, France

TIM@TIMVANERVEN.NL

Wojciech Kotlowski

WKOTLOWSKI@CS.PUT.POZNAN.PL

Institute of Computing Science, Poznan´ University of Technology, Poland

Manfred K. Warmuth Department of Computer Science, University of California, Santa Cruz

MANFRED@CSE.UCSC.EDU

Abstract
We consider online prediction with expert advice. Over the course of many trials, the goal of the learning algorithm is to achieve small additional loss (i.e. regret) compared to the loss of the best from a set of K experts. The two most popular algorithms are Hedge/Weighted Majority and Follow the Perturbed Leader (FPL). The latter algorithm first perturbs the loss of each expert by independent additive noise drawn from a fixed distribution, and then predicts with the expert of minimum perturbed loss ("the leader") where ties are broken uniformly at random. To achieve the optimal worst-case regret as a function of the loss L of the best expert in hindsight, the two types of algorithms need to tune their learning rate or noise magnitude, respectively, as a function of L.
Instead of perturbing the losses of the experts with additive noise, we randomly set them to 0 or 1 before selecting the leader. We show that our perturbations are an instance of dropout -- because experts may be interpreted as features -- although for non-binary losses the dropout probability needs to be made dependent on the losses to get good regret bounds. We show thatthis simple, tuning-free version of the FPL algorithm achieves two feats: optimal worst-case O( L ln K + ln K) regret as a function of L, and optimal O(ln K) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a gap between the expected loss of the best expert and all others.
A number of recent algorithms from the Hedge family (AdaHedge and FlipFlop) also achieve this, but they employ sophisticated tuning regimes. The dropout perturbation of the losses of the experts result in different noise distributions for each expert (because they depend on the expert's total loss) and curiously enough no additional tuning is needed: the choice of dropout probability only affects the constants. Keywords: Online learning, regret bounds, expert setting, Follow the Leader, Follow the Perturbed Leader, Dropout
1. Introduction
We address the following online sequential prediction problem, known as prediction with expert advice (see e.g. Littlestone and Warmuth, 1994; Vovk, 1998): in each trial t = 1, 2, . . . the algorithm (randomly) chooses one of K experts as its predictor before being told the loss vector t  [0, 1]K of all experts. Let LT,k be the cumulative loss of the k-th expert after T rounds, and let L = mink LT,k be the cumulative loss of the best expert. Then the goal for the algorithm is to minimize the difference between its expected cumulative loss and L, which is known as the regret. In this paper, we consider not just the standard worst-case setting, in which losses are generated by an
© 2014 T. van Erven, W. Kotlowski & M.K. Warmuth.

VAN ERVEN KOTLOWSKI WARMUTH
adversary that tries to maximize the regret, but also a setting with easier data, in which loss vectors are sampled from a fixed probability distribution.
The simplest algorithm is to Follow the Leader (FTL), i.e. to choose an expert of minimal cumulative loss, with ties broken uniformly at random. This algorithm works well for probabilistic data, but unfortunately its regret will grow linearly with L and T on worst-case data, which means that it is not able to learn. This problem can be avoided by the Weighted Majority algorithm or its stratified version, the Hedge algorithm (Littlestone and Warmuth, 1994; Freund and Schapire, 1997), which chooses expert k with probability proportional to the exponential weight exp(-Lt-1,k) in trial t, for some positive learning rate . For the appropriate tuning of , Hedge is guaranteed to achieve sublinear regret of order O( L ln K + ln K), which is worst-case optimal (Cesa-Bianchi et al., 1997; Jiazhong et al., 2013).
In addition to using exponential weights, there is a second method that can achieve sublinear worst-case regret: perturb the cumulative losses of the experts by independent additive noise from a fixed distribution and then apply FTL to the perturbed losses. For exponentially distributed noise with its parameter appropriately chosen as a function of L, the resulting Follow the Perturbed Leader (FPL) algorithm can also guarantee regret O( L ln K + ln K) (Kalai and Vempala, 2005). FPL can also exactly simulate Hedge with any learning rate  by using log-exponential random variables for the noise that are scaled as a function of  (Kuzmin and Warmuth, 2005; Kalai, 2005).
Instead of using additive noise, we perturb the loss vector of the current trial: for binary losses t  {0, 1}K, we set each component t,k to zero with fixed probability . As we explain in Section 2, this is equivalent to dropout (Hinton et al., 2012), because t,k may be interpreted as the k-th feature value at trial t. Surprisingly, this simple method achieves the same optimal regret bound for any value of   (0, 1), without tuning.
We can also handle the general case when the loss vector t takes values in the continuous range [0, 1]K. In this case, we show that, on worst-case data, the vanilla dropout procedure that simply sets coordinates of the loss vector to zero, gets a suboptimal (K) dependence on the number of experts K. However, by binarizing the dropout loss, by which we mean setting the loss t,k of the k-th expert to 1 with probability (1 - ) t,k and to 0 otherwise, FPL again achieves the optimal regret of O( L ln K + ln K).
Most Hedge or FPL algorithms depend on a learning rate or noise parameter  > 0, which needs to be tuned in terms of L to get the worst-case bound O( L ln K + ln K). A simple way is to maintain an estimate of L and tune  based on the current estimate. As soon as the loss of the best expert exceeds the current estimate, the estimate is doubled and the algorithm re-tuned (see e.g. Cesa-Bianchi et al., 1996, 1997). This method achieves the optimal regret for Hedge and the related previous versions of FPL, albeit with a large constant factor.
Much fancier tuning methods were introduced by Auer et al. (2002). Amongst others, their techniques have recently led to the AdaHedge and FlipFlop algorithms (van Erven et al., 2011; de Rooij et al., 2014), which work well in the case when the loss vector is drawn i.i.d. from a fixed distribution. In this case they achieve constant regret O(ln K) when there is a gap between the loss of the best expert and the expected loss of all others. Surprisingly, we can show that FPL based on the binarized dropout perturbations also achieves regret O(ln K) in the i.i.d. case, without the need of any tuning.
On the other side of the spectrum, there exists another simple perturbation based on Random Walk Perturbation (RWP) (Devroye et al., 2013): simply add an independent Bernoulli coin flip to each loss component in each trial (without any tuning). We highly benefited from the techniques
2

DROPOUT PERTURBATIONS

 used in the analysis of this version of FPL. However, we can show that RWP already has ( T ) regret in the noise-free case L = 0, where T is the number of trials. In contrast, FPL with binarized dropout perturbation achieves constant regret of order O(ln K). It seems that since the perturbation of the loss of expert k depends on the total current loss of expert k, this makes FPL more adaptive and no additional tuning is required.
Our research opens up a large number of questions regarding tuning-free methods for learning with the linear loss.
1. Does FPL with dropout perturbations lead to efficient algorithms with close to optimal regret for shifting experts or for various combinatorial expert settings such as the online shortest path problem (Takimoto and Warmuth, 2003)? See (Devroye et al., 2013) for a similar discussion.
2. Are there other versions of dropout (such as dropping the entire loss vector with probability ) that achieve the same feats as the dropout perturbations used in this paper?
3. There is a natural matrix generalization of the expert setting (Warmuth and Kuzmin, 2011). Is there a version of dropout perturbation that avoids the expensive matrix decompositions used by all algorithms with good regret bounds for this generalization (Hazan et al., 2010) and replaces the matrix decompositions by maximum eigenvector calculations?
4. Wager et al. (2013) consider a Follow the Regularized Leader type algorithm for the batch setting. They use a regularizer that is a variation of the squared Euclidean distance, which they motivate as FTL on an approximation to the expected dropout loss. Although this use of dropout is entirely different from ours, it would be interesting to know whether it leads to novel regret or generalization bounds.
Outline The paper is organized as follows. In Section 2 we formally define dropout perturbation, and explain how dropping loss components t,k is the same as dropping features. Section 3 contains the core of ourpaper: We prove that the regret of FPL with binarized dropout perturbations is bounded by O( L ln K + ln K). Even though our algorithm is simple to state, this proof is quite involved at this point. We construct a worst-case sequence of losses using a sequence of reductions. This sequence consists of two regimes and we prove bounds for each regime separately, which we then combine. We also clarify the difference to the RWP algorithm, which provably does not achieve this bound. Then, in Section 4, we show that dropout perturbation automatically adapts to i.i.d. loss vectors, for which it gets constant regret of order O(ln K). Some of the proof details are postponed to the appendix.

2. Dropout Perturbation

Setting Online prediction with expert advice (Cesa-Bianchi and Lugosi, 2006) is formalized as

a repeated game between the algorithm and an adversary. At each iteration t = 1, . . . , T , the

algorithm randomly picks an expert k^t  {1, . . . , K} according to a probability distribution wt =

(wt,1, . . . , wt,K) of its choice. Then, the loss vector t  [0, 1]K is revealed by the adversary, and

the algorithm suffers loss t,k^t. Let LT,k =

T t=1

t,k

be the cumulative loss of expert k.

Then

the goal of the algorithm is to minimize its regret, which is the difference between its expected

3

VAN ERVEN KOTLOWSKI WARMUTH

Algorithm 1: Follow the Leader with Binarized Dropout Perturbation (BDP)
Parameter : Dropout probability   (0, 1) Initialization: L0,k = 0 for all k = 1, . . . , K.
for t = 1 to T do Pick expert k^t = argmink Lt-1,k (with ties broken uniformly at random). Observe loss vector t and suffer loss t,k^t. Draw t,k according to (2.1), independently for all k. Update Lt,k = Lt-1,k + t,k for all k.
end

cumulative loss and the cumulative loss of the best expert chosen in hindsight:

T

RT = E

t,k^t - L,

t=1

where L = min LT,k.
k

The expectation is taken with respect to the random choices of the algorithm, i.e. E

T t=1 t,k^t

=

T t=1

wt

·

t. For the process that generates the losses, we will consider two different models: In

the worst-case setting, studied in Section 3, we need to guarantee small regret for every possible

sequence of losses 1, . . . , T  [0, 1]K, including the sequence that is the most difficult for the

algorithm; in the independent, identically distributed (i.i.d.) setting, considered in Section 4, we

assume the loss vectors t  [0, 1]K all come from a fixed, but unknown probability distribution,

which makes it possible to give stronger guarantees on the regret.

Follow the Perturbed Leader The general class of Follow the Perturbed Leader (FPL) algorithms (Kalai and Vempala, 2005; Hannan, 1957) is defined by the choice

k^t = argmin Lt-1,k + t-1,k ,
k
where t-1,k is an additive random perturbation of the cumulative losses, which is chosen independently for every expert k. Kalai and Vempala (2005) consider exponential and uniform distributions for t-1,k that depend on a parameter  > 0, and in the RWP algorithm of (Devroye et al., 2013) t-1,k  Binomial(1/2, t) - t/2 is a binomial variable on t outcomes with success probability 1/2 that is centered around its mean.

Binarized Dropout Perturbations We introduce an instance of FPL based on binarized dropout

perturbation (BDP). For any dropout probability   (0, 1), the binarized dropout perturbation of

loss t,k is defined as:

t,k =

1 0

with probability (1 - ) t,k, otherwise.

(2.1)

Let LT,k =

T t=1

t,k be the cumulative BDP loss for expert k.

Then the BDP algorithm (see

Algorithm 1) chooses

k^t = argmin Lt-1,k,

k

4

DROPOUT PERTURBATIONS

with ties broken uniformly at random. BDP is conceptually very simple. Computationally, it might be of interest that it does sparse updates, which only operate on non-zero features: if t,k = 0, then t,k = 0 as well, so, if one uses the same variable to store Lt-1,k and Lt,k, then no update to the internal state of the algorithm is required. There is a parameter , but this parameter only affects the constants in the theorems, so simply setting it to  = 1/2 without any tuning already gives good performance. BDP may be viewed as an instance of FPL with the additive data-dependent perturbations
t-1,k = Lt-1,k - Lt-1,k.
If the losses are binary,  = 1/2 and the cumulative loss grows approximately as Lt-1,k  t - 1 for all experts k, then t-1,k is approximately distributed as Binomial(1/2, t-1)-(t-1). Consequently, in this special case BDP is very similar to RWP because shifts by a constant do not change the minimizer k^t.

Standard Dropout Perturbations Dropout is normally defined as dropping hidden units or fea-

tures in a neural network while training the parameters of the network on batch data using Gradient

Descent (Hinton et al., 2012; Wang and Manning, 2013; Wager et al., 2013). In each iteration,

hidden units or features are dropped with a fixed probability. We are in the single neuron case, in

which each expert may be identified with a feature and the standard dropout perturbations (without

binarization) become1:

s t,k

=

t,k
0

with probability 1 - , otherwise.

For binary losses, standard dropout perturbation is the same as BDP, but for general losses in the interval [0, 1] they are different. We initially tried to prove our results for standard dropout perturbation, but it turns out that this approach cannot achieve the right dependence on the number of experts:

Theorem 2.1 Consider the FPL algorithm based on standard dropout perturbation with parameter

  (0, 1). Then, for any B > 0 and any K  2, there exists a loss sequence 1, . . . , T with

t



[0, 1]K

for

which

L



B,

while

the

algorithm

incurs

regret

at

least

1 2

(K

-

1).

Proof Take and  to be very small positive numbers specified later. The sequence is constructed as follows. First, all experts except expert 1 incur losses for a number of iterations that is sufficiently large that, with probability (w.p.) at least 1 - , the algorithm will choose expert 1 as a leader. Note that the required number of iterations depends on , K and , but it does not depend on . Then, expert 1 incurs unit loss (and so does the algorithm w.p. 1 - ), while all other experts incur no loss. Next, all experts except expert 2 incur losses for sufficiently many iterations that, w.p. at least 1 - , the algorithm will choose expert 2 as a leader. Then, expert 2 incurs unit loss (and so does the algorithm w.p. 1 - ), while all other experts incur no loss. This process is then repeated for k = 3, 4, . . . , K - 1. At the end of the game, the algorithm incurred loss at least K - 1 w.p.

1. The loss w · t in round t that we consider here is linear in the weights w. For a general convex loss of the form ft(w · xt), where xt is the feature vector at round t, learning the weights w is often achieved by locally linearizing the loss with a first-order Taylor expansion: ft(wt · xt) - ft(w · xt)  wt · t - w · t for the surrogate loss t,k = ft(wt · xt)xt,k. (See the discussion in Section 4.1 of Kivinen and Warmuth (1997) and Section 2.4 of ShalevShwartz (2011).) Thus, experts correspond to features, and setting the k-th feature xt,k to 0 is equivalent to setting t,k to 0.

5

VAN ERVEN KOTLOWSKI WARMUTH

at least 1 - (K - 1), while expert K incurred no more loss than T . Taking  small enough that



<

1 2(K -1)

,

the

expected

loss

of

the

algorithm

is

at

least

1 2

(K

-

1).

Finally,

we

take

small enough

to satisfy 0 < T  B, which gives L  B.

 For the case that L  B  ln K, a better bound of order O( L ln K + ln K) = O(ln K) is possible. So we see that standard dropout leads to a suboptimal algorithm, and therefore we use binarized dropout for the rest of the paper.

3. Regret Bounded in Terms of the Cumulative Loss of the Best Expert
 We will prove that the regret for the BDP algorithm is bounded by O( L ln K + ln K):
Theorem 3.1 For any sequence of losses taking values in [0, 1] with mink LT,k = L, the regret of the BDP algorithm is bounded by

RT



1 2(1 - )

4

2L ln(3K) + 3 ln(1 + L)

ln(3K) 3

+

(1

-

)2

+

. 

 Since RWP is similar to BDP and is known to have regret bounded by O( T ln K), one might try to show that the regret for RWP satisfies the stronger bound O( L ln K + ln K) as well, but this

turns out to be impossible:

Theorem 3.2 For every T > 0, there exists a loss sequence 1, . . . , T for which L = 0, while RWP suffers ( T ) regret.

This theorem, proved in Section A.1 of the appendix, shows that there is a fundamental differ-
ence between RWP and BDP, and that the data dependent perturbations used in BDP are crucial to obtaining a bound in terms of L.
We now turn to proving Theorem 3.1, starting with the observation that it is in fact sufficient to
prove the theorem only for binary losses:

Lemma 3.3 Let a  0 and b be any constants. If the regret for the BDP algorithm is bounded by

 RT  a L + b

(3.1)

for all sequences of binary losses, then it also satisfies (3.1) for any sequence of losses with values in the whole interval [0, 1].

Proof Let 1, . . . , T be an arbitrary sequence of loss vectors with components t,k taking values in the whole interval [0, 1]. We need to show that the regret for this sequence satisfies (3.1). To this

end, let 1, . . . , T be an alternative sequence of losses that is generated randomly from 1, . . . , T by letting

t,k =

0 1

with probability 1 - t,k, with probability t,k,

independently for all t and k. Accordingly, let a prime on any quantity denote that it is evaluated on

the alternative losses. For example, LT,k =

T t=1

t,k is the cumulative alternative loss for expert

k. Let wt be the probability distribution on experts induced by the internal randomization of the

6

DROPOUT PERTURBATIONS

BDP algorithm, i.e. wt,k = P (k^t = k), and let wt be its counterpart on the alternative losses. Then, because the BDP algorithm internally generates a binary sequence of losses with the same

probabilities as

t,k, we have E

1,...,

t-1 [wt]

= wt, and, independently, E

[
t

t] =

t. Consequently,

E[wt · t] = wt · t and E[LT ] = LT , where LT and LT are the expected (with respect to the

internal randomization) cumulative losses of the algorithm on the original and the alternative losses,

respectively. Applying (3.1) for the alternative losses and taking expectations, it now follows by

Jensen's inequality that

E[LT

]

-

E[min
k

LT ,k ]



a

E

min
k

LT

,k

+ba

E[LT

]

-

min
k

E[LT ,k ]



a

min
k

E[LT

,k ]

+

b

LT - min LT,k  a min LT,k + b.
kk

E[min
k

LT

,k

]

+

b

Thus, from now on, we assume the losses are binary, i.e. t,k  {0, 1}. Following the standard approach for FPL algorithms of Kalai and Vempala (2005), we start by bounding the regret by applying the so-called be-the-leader lemma to the perturbed losses. Like in the analysis of Devroye et al. (2013), the result simplifies, because we can relate the expectation of the perturbed losses back to the original losses:

Lemma 3.4 (be-the-leader) algorithm is bounded by

For any sequence of loss vectors 1, . . . ,

RT



1

1 -



T

E

t=1

t,k^t - t,k^t+1 ,

T , the regret for the BDP

where 1, . . . , T are the dropout perturbed losses and k^t is the random expert chosen by the BDP algorithm based on the perturbed losses before trial t.

Proof The expected regret is given by

T
RT = E
t=1

T1

t,k^t

- min LT,k =
k

1-E

t=1

1

t,k^t

-

min
k

1

-



E

Lt,k

 

1T

1T

 1- E

t,k^t

- min LT,k
k



1-

E

t=1 t=1

 t,k^t - t,k^t+1  ,

where the second equality holds because k^t only depends on 1, . . . , t-1, which are independent

of t, and E t,k = (1 - ) t,k; and the last inequality is from the be-the-leader lemma, applied

to the perturbed losses, which shows that

T t=1

t,k^t+1



mink LT,k (Kalai and Vempala, 2005,

Equation 4), (Cesa-Bianchi and Lugosi, 2006, Lemma 3.1).

7

VAN ERVEN KOTLOWSKI WARMUTH

Every single term in the bound from Lemma 3.4 may be bounded further as follows:

E t,k^t - t,k^t+1

K
= (1 - ) Pr(k^t = k = k^t+1 | t,k = t,k) E t,k^t - t,k^t+1 | k^t = k = k^t+1, t,k = t,k
k=1

K
 (1 - ) Pr(k^t = k = k^t+1 | t,k = t,k) t,k.

(3.2)

k=1

In the case considered by Kalai and Vempala (2005), this expression can be easily controlled, because, for their specific choice of perturbations, the conditional probability

Pm,k = Pr(k^t+1 = k | k^t = k, t,k = t,k, min Lt,k = m)
j=k

is small, uniformly for all m (see the second display on p. 298 of their paper), which is easy to show, because Pm,k depends only on the cumulative loss and the perturbations for a single expert k. Thus their choice of perturbations is what makes their analysis simple. Unfortunately, in our case and also for the RWP algorithm of Devroye et al. (2013), this simple approach breaks down, because, for the perturbations we consider, the probability Pm,k may be large for some m (although such m will have small probability). We therefore cannot use a uniform bound on Pm,k, and as a consequence our proof is more complicated.
We also cannot follow the line of reasoning of Devroye et al., because it relies on the fact that, at any time t, the perturbation for every expert has a standard deviation of order t. For our algorithm this is only true if the cumulative losses for the experts grow at least at a linear rate ct for some absolute constant c > 0, which need not be the case in general. Put differently, if the expert losses grow sublinearly, then our perturbations are too small to use the approach of Devroye et al..
Thus, we cannot use any of the existing approaches to control (3.2) for arbitrary losses, and, instead, we proceed as follows:

1. Given any integers L1, . . . , LK, we find a canonical worst-case sequence of losses, which maximizes the regret of the BDP algorithm among all binary loss sequences of arbitrary length T such that LT,k = Lk for all experts k.

2. We bound (3.2) on this sequence.

3.1. The Canonical Worst-case Sequence
We will show that the worst-case regret is achieved for a sequence of unit vectors: let uk  {0, 1}K denote the unit vector (0, . . . , 0, 1, 0, . . . , 0) where the 1 is in the k-th position. This restriction to unit vectors has been used before (Abernethy and Warmuth, 2010; Koolen and Warmuth, 2010), but here we go one step further. We build a canonical worst-case sequence of unit vectors from the following alternating schemes:
Definition 3.5 For any k  {1, . . . , K}, we call a sequence of loss vectors uk, . . . , uK a k..Kalternating scheme.

8

DROPOUT PERTURBATIONS

To simplify the notation in the proofs, we will henceforth assume that the experts are numbered from best to worst, i.e. L1  . . .  LK. In the canonical worst-case sequence, alternating schemes are repeated as follows:

Definition 3.6 (Canonical Worst-case Sequence) Let L1  . . .  LK be nonnegative integers. Among all sequences of binary losses of arbitrary length T such that LT,k = Lk for all k, we call the following sequence the canonical worst-case sequence:

· First repeat the 1..K-alternating scheme L1 times;

· then repeat the 2..K-alternating scheme L2 - L1 times;

· then repeat the 3..K-alternating scheme L3 - L2 times;

· and so on until finally we repeat the K..K-alternating scheme (which consists of just the unit vector uK ) LK - LK-1 times.

Note that the canonical worst-case sequence always consists of LK alternating schemes. For example, the canonical worst-case sequence for cumulative losses L1 = 2, L2 = 3, L3 = 6 consists of the following L3 = 6 alternating schemes:

         

1 0 01 0 0

00

000

0 , 1 , 0, 0 , 1 , 0, 1 , 0 , 0, 0, 0 .

         

0 0 10 0 1

01

111

1..3-alternating scheme 1..3-alternating scheme 2..3-alternating scheme 3..3-a.s. 3..3-a.s. 3..3-a.s.
For the BDP algorithm, the following three operations do not increase the regret of a sequence of binary losses (see Section A.2 of the appendix for proofs):
1. If more than one expert gets loss in a single trial, then we split that trial into K trials in which we hand out the losses for the experts in turn (Lemma A.1).
2. If, in some trial, all experts get loss 0, then we remove that trial (Lemma A.2).
3. If, in two consecutive trials t and t + 1, t = uk and t+1 = uk for some experts k = k such that Lt-1,k  Lt-1,k , then we swap the trials, setting t = uk and t+1 = uk (Lemma A.3).
These operations can be used to turn any sequence of binary losses into the canonical worst-case sequence:
Lemma 3.7 Let L1  . . .  LK be nonnegative integers. Among all sequences of binary losses of arbitrary length T such that LT,k = Lk for all k, the regret of the BDP algorithm is (non-uniquely) maximized by its regret on the canonical worst-case sequence.
Proof Start with an arbitrary sequence of binary losses such that LT,k = Lk. By repeated application of operations 1 and 2, we turn this loss sequence into a sequence of unit vectors. We then repeatedly apply the swapping operation 3 to any trials t and t + 1 such that t = uk and t+1 = uk for k = k where we have strict inequality Lt-1,k > Lt-1,k . We do this until no such consecutive trials remain. This arrives at the canonical worst-case sequence except that the unit vectors within each alternating scheme may be permuted. However, by applying property 3 for the case Lt-1,k = Lt-1,k , we can sort each alternating scheme into the canonical order.
Together with Lemma 3.3, the previous lemma implies that we just need to bound the regret of the BDP algorithm on the canonical worst-case loss sequence.

9

VAN ERVEN KOTLOWSKI WARMUTH

3.2. Regret on the Canonical Worst-case Sequence

By Lemma 3.4 and (3.2), the regret for the BDP algorithm is bounded by

TK

RT 

Pr(k^t = k = k^t+1 | t,k = t,k) t,k.

t=1 k=1

(3.3)

On the canonical worst-case sequence, t,k will be zero for all but a single expert k, which we can exploit to abbreviate notation: let k^t+ denote the leader (with ties still broken uniformly at random) on the perturbed cumulative losses if we would add 1 unit of loss to the perturbed cumulative loss
Lt-1,k^t of the expert selected by the BDP algorithm; and define the event

At = k^t = k = k^t+

for the expert k such that t,k = 1.

Then (3.3) simplifies to

T

RT  Pr(At).

(3.4)

t=1

We split the LT,K alternating schemes of the canonical worst-case sequence into two regimes, and use different techniques to bound Pr(At) in each case. Let r  LT,K - LT,1 be some nonnegative integer, which we will optimize later:

1. The first regime consists of the initial LT,1 + r alternating schemes, where the difference in the cumulative loss between all experts is relatively small (at most r).

2. The second regime consists of the remaining LT,K - r - LT,1 alternating schemes. Now any expert that is still receiving losses will have a cumulative loss that is at least r larger than the cumulative loss of expert 1, and r will be chosen large enough to ensure that, with high probability, no such expert will the leader.

The First Regime

Define the lead pack

Dt = {k : Lt-1,k < min Lt-1,j + 2},
j

which is the random set of experts that might potentially be the leader in trials t or t+1. As observed by Devroye et al. (2013), there can only be a leader change at time t if Dt contains more than one expert. Our goal will be to control the probability that this happens using the following lemma (proved in Section A.3.1), which is an adaptation of Lemma 3 of Devroye et al.:

Lemma 3.8 Suppose Lt-1,k  L > 0 for all k. Then

Pr(|Dt|

>

1)



1 (1 -

)

2 ln K 3 +.
LL

Unfortunately, we cannot apply Lemma 3.8 immediately: in the first regime there are K trials for each of the first LT,1 repetitions of the 1..K-alternating scheme, and applying Lemma 3.8 to all of these trials would already give a bound of order K LT,1 ln K, which overshoots the right rate by a factor of K. To avoid this factor, we only apply the lemma once per alternating scheme, which is made possible by the following result:

10

DROPOUT PERTURBATIONS

Lemma 3.9 Suppose an alternating scheme starts at trial s and ends at trial v. Then

v

Pr(At)



1 (v


-

s

+

1) Pr(Av+1)



1 

Pr(k^v+1

=

k^v++1)



1 

Pr(|Dv+1|

>

1).

t=s

Proof The first inequality follows from Lemmas A.4 and A.5 in Section A.3.2 of the appendix,

which show that

Pr(As)



Pr(As+1)



...



Pr(Av )



1 

Pr(Av+1).

Let k = K - (v - s) be the first expert in the alternating scheme. At time v + 1 a new alternating

scheme will start, so that the situation is entirely symmetrical between all experts that are part of the

current alternating scheme:

Pr(Av+1) = Pr(k^v+1 = k = k^v++1) for all k  {k, . . . , K}.

This implies the second inequality:

K

(v - s + 1) Pr(Av+1) =

Pr(k^v+1 = k = k^v++1)

k =k

= Pr(k^v+1  {k, . . . , K}, k^v+1 = k^v++1)  Pr(k^v+1 = k^v++1).

Finally, the last inequality follows by definition of the lead pack.

Applying either Lemma 3.8 or the trivial bound Pr(|Dt| > 1)  1 only to the trials v + 1 that immediately follow an alternating scheme, we obtain the following result for the first regime of the canonical worst-case sequence (see Section A.3.3 for the proof):

Lemma 3.10 Suppose the first regime ends with trial v. Then

v

Pr(At)



1 

1 (1 - ) 2

2LT,1 ln K + 3 ln(1 + LT,1) + r + 1

.

t=1

The Second Regime
We proceed to bound the probability of the event At during the second regime of the canonical worst-case sequence, using that
Pr(At)  Pr(k^t = k) for the expert k such that t,k = 1.
This probability will be easy to control, because during the second regime the difference in cumulative loss between expert 1 and the experts that are still receiving losses, is sufficiently large that the BDP algorithm will prefer expert 1 over these experts with high probability.

Lemma 3.11 Suppose the second regime starts in trial s. Then

T

Pr(At)



K

2LT ,1 2(1 -

+ 3r )2r

exp

t=s

-2(1 - )2r2 2LT,1 + r

.

11

VAN ERVEN KOTLOWSKI WARMUTH

Proof Let t be a trial during the L-th alternating scheme, for some L in the second regime, and let kt be the expert that gets loss in trial t. Then, by Hoeffding's inequality,

Pr(At)  Pr(k^t = kt)  Pr Lt-1,kt  Lt-1,1  exp

-2(1 - )2(L - LT,1 - 1)2 L + LT,1 - 1

= exp

-2(1 - )2(L - LT,1 - 1)2 2LT,1 + L - LT,1 - 1

 exp

-2(1 - )2r(L - LT,1 - 1) 2LT,1 + r

,

where

the

last

inequality

follows

from

the

fact

that

x a+x

is

increasing

in

x

for

x



0

and

a

>

0.

Let

s(L) and v(L) denote the first and the last trial in the L-th alternating scheme. Then, summing up

over all trials in the second regime, we get

T

LT ,K

v(L)

LT ,K

v(L)

Pr(At) =

Pr(At) 

exp

t=s L=LT,1+r+1 t=s(L) L=LT,1+r+1 t=s(L)

-2(1 - )2r(L - LT,1 - 1) 2LT,1 + r

LT ,K
 K exp
L=LT ,1 +r+1

-2(1 - )2r(L - LT,1 - 1) 2LT,1 + r

LT,K -LT,1-1
= K exp
x=r

-2(1 - )2rx 2LT,1 + r

 K exp

-2(1 - )2r2

+K


exp

-2(1 - )2rx

.

2LT,1 + r

x=r+1

2LT,1 + r

Here the second term is bounded by:


exp
r

-2(1 - )2rx 2LT,1 + r

dx

=

2LT,1 + r 2(1 - )2r

exp

-2(1 - )2r2 2LT,1 + r

.

Putting things together we obtain

T
Pr(At)  K exp
t=s

-2(1 - )2r2 2LT,1 + r

+

K

2LT,1 + r 2(1 - )2r

exp



K

2LT ,1 2(1 -

+ 3r )2r

exp

-2(1 - )2r2 2LT,1 + r

,

-2(1 - )2r2 2LT,1 + r

which was to be shown.

The First and the Second Regime Together Combining the bounds for the first and second regimes from Lemmas 3.10 and 3.11, and optimizing r, we obtain the following result, which is proved in Section A.3.4 of the appendix:

Lemma 3.12 On a canonical worst-case sequence of any length T for which LT,1 = L,

T1 Pr(At)  2(1 - ) 4

2L ln(3K) + 3 ln(1 + L)

ln(3K) 3

+

(1

-

)2

+

. 

t=1

Plugging this bound into (3.4) completes the proof of Theorem 3.1.

12

DROPOUT PERTURBATIONS

4. Constant Regret on IID Losses

In this section we show that our BDP algorithm has optimal O(ln K) regret when the loss vectors are drawn i.i.d. from a fixed distribution and there is a fixed gap  between the expected loss of the best expert and all others.
Theorem 4.1 Let   (0, 1] and   (0, 1] be constants, and let k be a fixed expert. Suppose the loss vectors t are independent random variables such that the expected differences in loss satisfy

min E[
k=k

t,k

-

t,k ]  

for all t.

(4.1)

Then, with probability at least 1 - , the regret of the BDP algorithm is bounded by a constant:

8 8K RT  (1 - )22 ln (1 - )22 + 3.

(4.2)

As discussed in the proof in Section B.1, it is possible to improve the dependence on  at the cost of getting a more complicated expression.

Acknowledgments
Tim van Erven was supported by NWO Rubicon grant 680-50-1112, Wojciech Kotlowski by the Foundation for Polish Science under the Homing Plus Program, and Manfred K. Warmuth by NSF grant IIS-1118028.

References
Jacob Abernethy and Manfred K. Warmuth. Repeated games against budgeted adversaries. In Neural Information Processing Systems (NIPS), pages 1­9, 2010.
Peter Auer, Nicolo` Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48­75, 2002.
Nicolo` Cesa-Bianchi and Ga´bor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
Nicolo` Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth. Worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent. IEEE Transactions on Neural Networks, 7(2):604­619, May 1996.
Nicolo` Cesa-Bianchi, Yaov Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427­485, 1997.
Steven de Rooij, Tim van Erven, Peter D. Gru¨nwald, and Wouter M. Koolen. Follow the leader if you can, hedge if you must. Journal of Machine Learning Research. To appear., 2014.
Luc Devroye, Ga´bor Lugosi, and Gergely Neu. Prediction by random-walk perturbation. In Conference on Learning Theory (COLT), pages 460­473, 2013.

13

VAN ERVEN KOTLOWSKI WARMUTH
Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55:119­139, 1997.
James Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of Games, 3:97­139, 1957.
Elad Hazan, Satyen Kale, and Manfred K. Warmuth. On-line variance minimization in O(n2) per trial? In Conference on Learning Theory (COLT), pages 314­315, 2010.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.
Nie Jiazhong, Wojciech Kotlowski, and Manfred K. Warmuth. On-line PCA with optimal regrets. In Algorithmic Learning Theory (ALT), pages 98­112, 2013.
Adam Kalai. A perturbation that makes Follow the Leader equivalent to Randomized Weighted Majority. Private communication, December 2005.
Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291­307, 2005.
Jyrki Kivinen and Manfred K. Warmuth. Additive versus Exponentiated Gradient updates for linear prediction. Information and Computation, 132(1):1­64, 1997.
Wouter M. Koolen and Manfred K. Warmuth. Hedging structured concepts. In 23rd Annual Conference on Learning Theory - COLT 2010, pages 93­104. Omnipress, June 2010.
Dima Kuzmin and Manfred K. Warmuth. Optimum follow the leader algorithm. In Conference on Learning Theory (COLT), pages 684­686, 2005. Open problem.
Nick Littlestone and Manfred K. Warmuth. The Weighted Majority algorithm. Information and Computation, 108(2):212­261, 1994.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107­194, 2011.
Eiji Takimoto and Manfred K. Warmuth. Path kernels and multiplicative updates. Journal of Machine Learning Research, 4:773­818, 2003.
Tim van Erven, Peter D. Gru¨nwald, Wouter Koolen, and Steven de Rooij. Adaptive hedge. In Neural Information Processing Systems (NIPS), pages 1656­1664, 2011.
Volodya Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences, 56(2):153­173, 1998.
Stefan Wager, Sida Wang, and Percy Liang. Dropout training as adaptive regularization. In Neural Information Processing Systems (NIPS), pages 351­359, 2013.
Sida I. Wang and Christopher D. Manning. Fast dropout training. In International Conference on Machine Learning (ICML), pages 118­126, 2013.
14

DROPOUT PERTURBATIONS

Manfred K. Warmuth and Dima Kuzmin. Online variance minimization. Machine Learning, 87(1): 1­32, 2011.

Additional Material for "Follow the Leader with Dropout Perturbations"

Appendix A. Proof Details for Section 3

A.1. Proof of Theorem 3.2

Proof We prove the theorem by explicit construction of the sequence. We take K = 2 experts.

The loss sequence is such that expert 1 never gets any loss (hence L = LT,1 = 0), while expert 2

gets losses

1,2, . . . ,

T , where

t

=

1 . t

This

means

that

its

cumulative

loss

Lt,2

=

t s=1

1 s

is

bounded by 2 t + 1 - 2  Lt,2  2 t + 1. We now bound the total loss of the RWP algorithm

from below.



Consider the situation just before trial t. Then expert 2 has accumulated at most Lt-1,2  2 t

loss so far. The expected loss of the algorithm in this iteration is t times the probability that

expert 1 becomes the leader. This probability is at least P (t-1,1 > t-1,2 + 2 t), where t,k 

Binomial(1/2, t)

-

t/2.

Consequently,

Ut

=

t-1,1

-

t-1,2

+

t

is

distributed

as

Binomial(

1 2

,

2t).

Therefore, the expected loss of the algorithm in iteration t is lower bounded by

E

 t,k^t  tP (Ut > 2 t + t) = tP

Ut

-

t

>

 22

.

t/2

We first give a rough idea of what happens. Due to Central Limit Theorem, the probability on the right hand side is approximately P (Y > 2 2), where Y  N (0, 1). Summing over trials we get that the expected cumulative loss of the algorithm is approximately LT,2 · P (Z > 2 2) = ( T ).
To be more precise, we use the Berry-Esse´en theorem, which says that for X  Binomial(n, p), and Y  N (0, 1), for any z,

P X - np > z - P (Y > z)  f(p) ,

np(1 - p)

n

where

f (p)

depends

on

p,

but

not

on

n.

Using

this

fact

with

n

=

2t

and

p

=

1 2

results

in

P

Ut

-

t

>

 22

- P (Y

>

 2 2)



- c

t/2 t

for some constant c. Therefore the expected cumulative loss of the algorithm can be lower bounded

by

P (Y

T > 2 2)LT,2 -
t=1

t

c t



P (Y

 > 2 2)LT,2 - c(ln T + 1) = ( T ),

which proves the theorem.

15

VAN ERVEN KOTLOWSKI WARMUTH

A.2. Operations to Reduce to the Canonical Worst-case Sequence
In this section we prove that the three operations on losses from Section 3.1 can only ever increase the regret of the BDP algorithm. As proved in Lemma 3.7, this implies that the canonical worstcase sequence maximizes the regret among all sequences of binary losses with the same cumulative losses L1, . . . , LK .

THE UNIT RULE HOLDS
We can assume without loss of generality that in every round exactly one expert gets loss. We call this the unit rule. It follows from two results, which will be proved in this subsection: first, Lemma A.1 shows that, if multiple experts get loss in the same trial, then the regret can only increase if we split that trial into multiple consecutive trials in which the experts get their losses in turn. Secondly, it is shown by Lemma A.2 that rounds in which all experts get zero loss do not change the regret, and can therefore be ignored in the analysis. (Although we only need them for binary losses, both lemmas hold for general losses with values in [0, 1].)

Lemma A.1 (One Expert Gets Loss Per Trial) Suppose 1, . . . , t-1, t, t+1, . . . , T is a sequence

of losses. Now consider the alternative sequence of losses

1, . . . ,

t-1,

t1, . . . ,

K t

,

t+1, . . . ,

T

with

k t,k

=

t,k
0

if k = k, otherwise,

for k, k = 1, . . . , K. Then the regret of the BDP algorithm on the original losses RT never exceeds
its regret on the alternative losses RT : RT  RT .

Proof The cumulative loss of the best expert L is the same on both sequences of losses, so we only

need to consider the cumulative loss of the BDP algorithm. On trials 1, . . . , t - 1 and t + 1, . . . , T

the algorithm's probability distributions on experts are the same for both sequences of losses, so we

only need to compare its expected loss on

t with its expected losses on

t1, . . . ,

K t

.

To

this

end,

we

observe that the algorithm's probability of choosing expert k given losses 1, . . . , t-1 is no more

than its probability of choosing that expert given losses 1, . . . , t-1, 1t , . . . , tk-1, because during

the trials

t1, . . . ,

k-1 t

expert

k

has

received

no

loss

whereas

experts

1,

.

.

.

,

k

-

1

have

respectively

received losses t,1, . . . , t,k-1, which implies that the algorithm's expected loss can only increase

on the alternative sequence of losses.

Lemma A.2 (Ignore All-zero Loss Vectors) Suppose 1, . . . , t-1, t, t+1, . . . , T is a sequence of losses such that t,k = 0 for all k. Then the regret of the BDP algorithm is the same on the subsequence 1, . . . , t-1, t+1, . . . , T with trial t removed.
Proof Both the best expert and the BDP algorithm have loss 0 on trial t. Trial t also does not influence the actions of the BDP algorithm for any trial t = t. Hence the regret does not change if trial t is removed.

16

DROPOUT PERTURBATIONS

SWAPPING ARGUMENT
We call a loss vector t a unit loss if there exists a single k such that t,k = 1 while t,k = 0 for all k = k.
Let k^t denote the expert that is randomly selected by the BDP algorithm (based on some past losses 1, . . . , t-1) to predict trial t.
Let Binomial(p, n) denote the distribution of a binomial random variable on n trials with success probability p.

Lemma A.3 (Swapping) Suppose that 1, . . . , t-1, t, t+1, t+2, . . . , T is a sequence of unit losses with t,k = t+1,k = 1 for k = k and that Lt-1,k  Lt-1,k. Now consider the alternative sequence of losses 1, . . . , t-1, t+1, t, t+2, . . . , T in which t and t+1 are swapped. Then the regret of the BDP algorithm on the original losses RT never exceeds its regret on the alternative losses RT :
RT  RT .
Proof The cumulative loss of the best expert L is the same on both sequences of losses, so we only need to consider the cumulative loss of the BDP algorithm. On trials 1, . . . , t - 1 and t + 1, . . . , T the algorithm's weights are the same for both sequences of loss, so we only need to compare its loss on t, t+1 in the original sequence with its loss on t+1, t in the alternative sequence.
Let Pr and Pr respectively denote probability with respect to the algorithm's randomness on the original and on the alternative losses. Since we assumed unit losses, we need to show that

Pr(k^t = k) + Pr(k^t+1 = k )  Pr (k^t = k ) + Pr (k^t+1 = k).

(A.1)

As will be made precise below, we can express all these probabilities in terms of binomial random variables Lt-1,j  Binomial(1 - , Lt-1,j) for j = 1, . . . , K that represent the cumulative perturbed losses for the experts after t - 1 trials, and an additional Bernoulli variable X  Binomial(1 - , 1) that represents whether the next loss is dropped out (depending on context, X
is either t,k or t,k ). It will also be convenient to define the derived values

M = min Lt-1,j,
j=k,k

C = |{j = k, k : Lt-1,j = M }|,

which represent the minimum perturbed loss and the number of experts achieving that minimum among all experts except k and k . We will show that

Pr(k^t = k | M = m, C = c, X = x) + Pr(k^t+1 = k | M = m, C = c, X = x) - Pr (k^t = k | M = m, C = c, X = x) - Pr (k^t+1 = k | M = m, C = c, X = x) (A.2)

17

VAN ERVEN KOTLOWSKI WARMUTH

is nonpositive for all m, c and x, from which (A.1) follows. In terms of the random variables we have defined, these conditional probabilities are

Pr(k^t = k | M

= m, C = c, X = x) = Pr(Lk < Lk , Lk < m) + Pr(Lk = Lk

1 < m)
2

11

+

Pr(Lk

=

m

<

Lk

) c

+

1

+

Pr(Lk

=

m

=

Lk

) c

+

, 2

Pr(k^t+1 = k

|M

= m, C = c, X = x) = Pr(Lk

< Lk + x, Lk

< m) + Pr(Lk

1 = Lk + x < m) 2

11

+ Pr(Lk

=

m

<

Lk

+

x) c

+

1

+

Pr(Lk

= m = Lk + x) c + 2 ,

Pr

(k^t = k

|M

= m, C = c, X = x) = Pr(Lk

< Lk, Lk

< m) + Pr(Lk

1

=

Lk

<

m) 2

11 + Pr(Lk = m < Lk) c + 1 + Pr(Lk = m = Lk) c + 2 ,

Pr

(k^t+1 = k | M

= m, C

= c, X = x) = Pr(Lk < Lk

+ x, Lk

< m) + Pr(Lk

= Lk

1 + x < m)
2

11

+ Pr(Lk = m < Lk

+

x) c

+

1

+

Pr(Lk

=

m

=

Lk

+ x) , c+2

where for notational convenience, we skipped the subscript t - 1 in Lt-1,j for all j. For x = 0, we see immediately that (A.2) is 0, so it remains only to consider the case x = 1. For that case, (A.2) simplifies to

Pr(Lk < Lk , Lk < m) - Pr(Lk < Lk + 1, Lk < m)

+ Pr(Lk < Lk + 1, Lk < m) - Pr(Lk < Lk, Lk < m)

1 +
2

Pr(Lk = Lk + 1 < m) - Pr(Lk = Lk + 1 < m)

1 +
c+1

Pr(Lk = m < Lk ) - Pr(Lk = m < Lk + 1)

1

+ c+1

Pr(Lk = m < Lk + 1) - Pr(Lk = m < Lk)

1 +
c+2

Pr(Lk = m = Lk + 1) - Pr(Lk = m = Lk + 1)

= - Pr(Lk = Lk < m) + Pr(Lk = Lk < m)

1 +
2

Pr(Lk = Lk + 1 < m) - Pr(Lk = Lk + 1 < m)

-1 c+1

Pr(Lk = m = Lk ) + Pr(Lk = m = Lk)

1 +
c+2

Pr(Lk = m = Lk + 1) - Pr(Lk = m = Lk + 1)

1 m-1 =
2

Pr(Lk = a = Lk + 1) - Pr(Lk = a = Lk + 1)

a=1

1 +
c+2

Pr(Lk = m = Lk + 1) - Pr(Lk = m = Lk + 1) .

18

DROPOUT PERTURBATIONS

To prove that this is nonpositive, it is sufficient to show that

Pr(Lk = a = Lk + 1) - Pr(Lk = a = Lk + 1)  0

(A.3)

for all nonnegative integers a. Abbreviate L = Lt-1,k and L = Lt-1,k . If a = 0 or a > L , then the left-most probability is 0 and (A.3) holds. Alternatively, for a  {1, . . . , L }, the left-hand side
of (A.3) is equal to

L a

L -L

L

a-1

a-1 a

(1 - )2a-1L+L -2a+1,

so it is enough to show that

L a

L a-1

-

L a-1

L  0. a

But this holds, because the left-hand side is equal to

L !L! - L !L! a!(L - a)!(a - 1)!(L - a + 1)! (a - 1)!(L - a + 1)!a!(L - a)!

= a!(L

L !L! - a + 1)!(a - 1)!(L - a + 1)!

(L - a + 1) - (L - a + 1)

L !L! =

L -L

a!(L - a + 1)!(a - 1)!(L - a + 1)!

 0,

because L = Lt-1,k  Lt-1,k = L by assumption.

A.3. Bounding Leader Changes on the Canonical Worst-case Sequence
A.3.1. PROOF OF LEMMA 3.8
Proof Within this proof, abbreviate Lk = Lt-1,k. Let Vk  Binomial(1 - , L) and Wk  Binomial(1 - , Lt-1,k - L), and assume Lk = Vk + Wk. Also define p(v) = Pr(V1 = v). Then

K KL

Pr(|D| = 1) = Pr(min Lj  Lk + 2) =

p(v) Pr(min Lj  v + Wk + 2)

j=k j=k

k=1 k=1 v=0

L-2 K

LK

 p(v) Pr(min Lj  v + Wk + 2) = p(v - 2) Pr(min Lj  v + Wk)

j=k j=k

v=0 k=1

v=2 k=1

=

L

p(v - 2) p(v)

K

p(v) Pr(min Lj  v + Wk) .
j=k

v=2 k=1

(A.4)

f (v)

19

VAN ERVEN KOTLOWSKI WARMUTH

Let S = {k : Lk  minj Lj} be the set of leaders. Then

K
f (v) = Pr(min Lj  Lk, Vk = v)  Pr(k : min Lj  Lk, Vk = v)
j=k j=k k=1

=

Pr(k :

min Lj
j



Lk, Vk

=

v) 

Pr(min
kS

Vk

=

v),

where the first inequality follows by the union bound, and the second because the latter event implies

the former. We remark that S is never empty, so that the minimum is always well-defined. We also

have

p(v - 2) =
p(v)

L v-2

(1 - )v-2L-v+2

L v

(1 - )vL-v

=

2 (1 - )2

·

v(v - 1)

(L

-

v

+

2)(L

-

v

+

. 1)

Let

g(v)

=

max{

v(v-1) (L-v+2)(L-v+1)

,

0}.

Then,

since

g(0)

=

g(1)

=

0,

(A.4)

is

at

least

as

large

as

2 L

2

(1 - )2

g(v)

Pr(min
kS

Vk

=

v)

=

(1

-

)2

E[g(V

)]

v=0

for V = minkS Vk. The function g(v) may be written as

g(v) = h1(v)h2(v) for

v

h1(v)

=

L

-

v

+

, 2

v-1

h2(v)

=

L

-

v

+

. 1

For v  1, both h1 and h2 are nonnegative, nondecreasing and convex, which implies that g(v) also has these properties. Moreover, since g(v) = 0 for v  [0, 1] all properties extend to all v  0. Suppose that E[V ]  (1 - )L - B for some B  0. Then Jensen's inequality and monotonicity of g imply that

(1

2 - )2

E[g(V

)]



(1

2 - )2 g(E[V

])



(1

2 - )2 g

(1 - )L - B



L

-

 1-

B

L

-

 1-

(B

+

1)

=

1-

1 1-

B

+

2

(L + B + 2)(L + B + 1)

L + B + 2

1-

1 1-

(B

+

1)

L + B + 1

1-

1 1-

B

+

2

-

1 1-

(B

+ 1)



1-

2B + 3 .

L + B + 2 L + B + 1

(1 - )L

Putting everything together, we find that

Pr(|Dt|

>

1)

=

1

-

Pr(|Dt|

=

1)



2B + 3 (1 - )L ,

so that it remains to find a good bound B. Let Yk = L - Vk  Binomial(, L). Then

(A.5)

- E[V ] + (1 - )L = E

max
kS

(1 - )L - Vk

 E max (1 - )L - Vk
k

= E max Yk - L
k



L ln K =: B,
2

where the last inequality follows by a standard argument for sub-Gaussian random variables (see, for example, Lemmas A.13 and A.1 in the textbook by Cesa-Bianchi and Lugosi (2006)). Plugging this bound into (A.5) leads to the desired result.

20

DROPOUT PERTURBATIONS

A.3.2. PROOF DETAILS FOR LEMMA 3.9
Suppose that t is a trial during the first regime in which expert k gets a unit of loss. First we consider the case that k is not the last expert in a round of the alternating scheme:

Lemma A.4 Suppose t is a trial during the first LT,1 + r repetitions of the alternating scheme in which t,k = 1 for some k < K. Then

Pr(At)  Pr(At+1).

Proof For j = k, k + 1, let Lt-1,j  Binomial(1 - , Lt-1,j) be the perturbed cumulative losses for all experts except experts k and k + 1. Also define

M = min Lt-1,j,
j=k,k+1

C = |{j = k, k + 1 : Lt-1,j = M }|.

By definition of the canonical worst-case sequence, expert k gets a unit of loss in trial t, expert k + 1 will get a unit of loss in trial t + 1, and Lt-1,k = Lt-1,k+1. We will construct the perturbed cumulative losses for experts k and k + 1 from the following variables: V, W  Binomial(1 - , Lt-1,k) and X  Binomial(1 - , 1). To express Pr(At), we define the perturbed cumulative losses Lt-1,k = V and Lt-1,k+1 = W , but to express Pr(At+1) we let Lt-1,k = W + X and
Lt-1,k+1 = V . This leads to

Pr(At |M = m, C = c)

= Pr(V = m - 1, W > m) c

+ Pr(V

= m - 1, W

c+1 = m)

c+1

c+2

11

+ Pr(V = m, W > m) + Pr(V = m, W = m)

c+1

c+2

+

Pr(V = W - 1, W < m) + Pr(V = W, W < m)

1 ,

2

Pr(At+1 |M = m, C = c)

= Pr(V = m - 1, W + X > m)

c

+ Pr(V

= m - 1, W

+X

c+1 = m)

c+1

c+2

11

+ Pr(V = m, W + X > m) + Pr(V = m, W + X = m)

c+1

c+2

+ Pr(V = W + X - 1, W + X < m) + Pr(V = W + X, W + X < m) 1 2

for any m and c. Thus

Pr(At+1 | M = m, C = c) - Pr(At | M = m, C = c) =  Pr(At+1 | M = m, C = c, X = 0) - Pr(At | M = m, C = c, X = 0) + (1 - ) Pr(At+1 | M = m, C = c, X = 1) - Pr(At | M = m, C = c, X = 1) = (1 - ) Pr(At+1 | M = m, C = c, X = 1) - Pr(At | M = m, C = c) , (A.6)

21

VAN ERVEN KOTLOWSKI WARMUTH

where

Pr(At+1 | M = m, C = c, X = 1) - Pr(At | M = m, C = c) = Pr(V = m - 1, W + 1 > m) - Pr(V = m - 1, W > m) c c+1 + Pr(V = m - 1, W + 1 = m) - Pr(V = m - 1, W = m) c + 1 c+2 + Pr(V = m, W + 1 > m) - Pr(V = m, W > m) 1 c+1 + Pr(V = m, W + 1 = m) - Pr(V = m, W = m) 1 c+2

+ Pr(V = W, W + 1 < m) + Pr(V = W + 1, W + 1 < m)

- Pr(V = W - 1, W < m) - Pr(V = W, W < m)

1 .

2

Using that V and W have the same distribution, so that we may switch their roles, this simplifies to

Pr(At+1 | M = m, C = c, X = 1) - Pr(At | M = m, C = c)
= Pr(V = m - 1, W = m) c - c + 1 + 1 c+1 c+2 c+2
+ Pr(V = W = m - 1) c + 1 - 1 c+2 2
+ Pr(V = W = m) 1 - 1 c+1 c+2
 0.

Substituting back into (A.6), we see that

Pr(At+1 | M = m, C = c) - Pr(At | M = m, C = c)  0 for all m and c. Hence Pr(At+1) - Pr(At)  0 also holds unconditionally, from which the lemma follows.

Secondly, we consider the case that k is the last expert in a round of the alternating scheme:

Lemma A.5 Suppose t is a trial during the first LT,1 + r repetitions of the alternating scheme in

which t,K = 1. Then

Pr(At)



1 

Pr(At+1).

(To make Pr(At+1) well-defined in case t = T , we adopt the convention that expert K gets a unit of loss in trial T + 1.)
Proof Let k be the expert that gets a unit of loss in trial t + 1 so that t+1,k = 1 and t+1,k = 0 for k = k. Trial t + 1 is at the beginning of a round of the alternating scheme, so by symmetry between the experts that are part of the alternating scheme, Pr(At+1) would remain the same if we changed t+1 so that t+1,K = 1 and t+1,k = 0 for all k = K. But then we would have

Pr(At+1 | t,K = 0) = Pr(At),

22

DROPOUT PERTURBATIONS

where t,K is the perturbed loss for expert K in round t, and consequently Pr(At+1) =  Pr(At+1 | t,K = 0) + (1 - ) Pr(At+1 | t,K = 1)   Pr(At+1 | t,K = 0) =  Pr(At),
from which the lemma follows.

A.3.3. PROOF OF LEMMA 3.10

Proof Let v(L) denote the last trial in the L-th alternating scheme. Then, by Lemmas 3.9, 3.8 and the trivial bound Pr(|Dv(L)+1| > 1)  1,

v

Pr(At)



1 

LT ,1 +r
Pr(|Dv(L)+1|

>

1)



1 


1  (1 -

)

LT ,1

t=1 L=1

L=2

2 ln K 3 +
LL

1 

1 (1 - ) 2

2LT,1 ln K + 3 ln(1 + LT,1) + r + 1

,

 + r + 1

where the last step follows from the fact that

LT ,1 L=2

f (L)



LT ,1 1

f

(L)dL

for

any

nonincreasing

function f .

A.3.4. PROOF OF LEMMA 3.12

Here we show how to optimize r to obtain Lemma 3.12:

Proof

Abbreviate f (r) =

2(1-)2r2 2L+r

.

Then

the

bound

for

the

second

regime

from

Lemma

3.11

can

be written as

T

Pr(At)



Kr f (r)

·

2L + 3r 2L + r

e-f

(r)



3Kr e-f(r). f (r)

t=s

We will choose r to be the smallest nonnegative integer such that f (r)  ln(3K)  1, so that

T
Pr(At)  r.
t=s

(A.7)

(It is no problem if this makes r exceed its maximal value LT,K - LT,1, because in that case the second regime is empty, so (A.7) still holds, and since the bound for the first regime from Lemma 3.10 is increasing in r it also still holds.)
To find r, we need to take the largest solution to

2(1 - )2r2 2L + r = ln(3K),

and round it up to the nearest integer. Abbreviating a = 2(1 - )2 and b = ln(3K), this gives



r = b + b2 + 8aLb  b + 2a a

2Lb

ln(3K )

a + 1 = 2(1 - )2 +

L ln(3K) 1 -  + 1,

23

VAN ERVEN KOTLOWSKI WARMUTH

where

the

inequality

follows

from

 x+y



 x

+

y

for

nonnegative

x, y

and

x

 x + 1.

Combining this bound on r with (A.7) and the bound for the first regime from Lemma 3.10, we find

that

T

Pr(At)



1 

t=1

1 

1 

1 (1 - )

 2 2L ln K + 3 ln(1 + L)

+r+1

+r

1 (1 - )

 2 2L ln K + 3 ln(1 + L)

+ 2r + 1

1 (1 - )

 2 2L ln K + 3 ln(1 + L)

ln(3K) 2 + (1 - )2 +

L ln(3K) 1- +3

1 

1 (1 - ) 4

2L ln(3K) + 3 ln(1 + L)

ln(3K ) + (1 - )2 + 3

,

which is equivalent to the statement of the lemma.

Appendix B. Proofs for Section 4

B.1. Proof of Theorem 4.1
Proof [Theorem 4.1] We will show that the conditions of Lemma B.1 below, with c = /2, are satisfied with probability at least 1 -  if  is chosen as

8 8K  = (1 - )22 ln (1 - )22 .

Because the second term in the bound from Lemma B.1 is bounded by

4K (1 - )22

(1 - )22 exp -

  1, 4

this shows that

RT  R+1 + 1

with probability at least 1 - . We now get the inequality in (4.2) from the trivial bound R+1 
 + 1 and x  x + 1. Alternatively, one might also get a better dependence on  by applying Theorem 3.1 to R+1 and using that L   + 1, which leads to

RT = O

ln K 1 +


ln

1 2

ln K

.

To verify that the conditions of Lemma B.1 are satisfied with sufficient probability, define the following events for t   + 1:

At,k

:

Lt,k



E[Lt,k ]

-

 t
4

for k = k,



Bt :

Lt,k



E[Lt,k ]

+

t, 4

24

DROPOUT PERTURBATIONS

and let Dt = Bt  k=k At,k be the event that they all hold simultaneously. By the assumption in (4.1) we have that



Lt,k

- Lt,k



E[Lt,k] - E[Lt,k ] -

t 2



t 2

on Dt,

so that the conditions of Lemma B.1 are satisfied with c = /2 if Dt holds for all t   + 1. By Hoeffding's inequality, the probabilities of the complementary events A¯t,k and B¯t are all
bounded by exp(-2t/8) and hence by the union bound the probability of D¯t is bounded by K exp(-2t/8). Combining this with another application of the union bound, we find that the
probability that Dt fails to hold for any t   + 1 is bounded by

Pr

D¯ t 

T

Pr D¯t  K

T

2 exp(- t)
8

t= +1,...,T

t= +1

t= +1

K

 t=

exp(- 2 t)dt 8

=

8K 2

exp(- 2  ). 8

The reader may verify that, for our choice of  , this probability is bounded by , which completes the proof.

Lemma B.1 Suppose that, for some k,

Lt,k - Lt,k  ct for all t   + 1 and k = k,

where c > 0 is a constant, and  is a nonnegative integer. Then the regret for the BDP algorithm is bounded by a constant:

RT



R +1

+

K (1 - )2c2

exp

- (1 - )2c2 .

Proof From the assumption of the lemma we know that expert k will be the best expert at least for all t   + 1. Consequently the regret is bounded by

T

RT  R+1 +

Pr(k^t = k).

t= +2

For any t   + 2 and any k = k, Hoeffding's inequality implies that

Pr(k^t = k)  Pr(Lt-1,k  Lt-1,k ) = Pr Lt-1,k - Lt-1,k - E[Lt-1,k - Lt-1,k]  (1 - )(Lt-1,k - Lt-1,k )  exp -2(1 - )2(Lt-1,k - Lt-1,k )2  exp - (1 - )2c2(t - 1) . 2(t - 1)

25

VAN ERVEN KOTLOWSKI WARMUTH

Consequently, by the union bound,

TT

T

Pr(k^t = k) 

Pr(k^t = k)  (K - 1)

exp - (1 - )2c2(t - 1)

t= +2

t= +2 k=k

t= +2

T -1



= (K - 1)

exp - (1 - )2c2t  (K - 1) exp - (1 - )2c2t dt

t= +1





(1

K - )2c2

exp

- (1 - )2c2 ,

from which the lemma follows.

26

