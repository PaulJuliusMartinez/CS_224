Hybrid Neural Network based on models of Multi-
Layer Perceptron and Adaptive Resonance Theory 

Novosibirsk State Technical University, 

630087, Novosibirsk, Nemirovich-Danchenko, 136, Russia 

A.V. Gavrilov 

avg@vt.cs.nstu.ru 

Abstract. The model of the hybrid neural network is considered. 
This  model  consists  of  model  ART-2  for  clusterization  and 
perceptron for preprocessing of images. The perceptron provides 
invariant recognition of objects. This model can be used in mobile 
robots for recognition of new objects or scenes in sight the robot 
during his movement. 

I. INTRODUCTION 

Application  of  model  Adaptive  Resonance  Theory  of 
Grossberg-Carpenter  [1]  (in  particular  ART-2)  is  rather 
attractive 
to  problem  of  solving  of  classification  and 
clusterization, because this model combines in itself properties 
of plasticity and stability, and also it does not demand a priori 
knowledge  of  the  fixed  quantity  of  necessary  classes.  Many 
different modifications of this model and its combinations with 
other  neural  networks  are  known  [2,3,4,5].  One  of  hybrid 
model  of  neural  network  based  on  model  ART-2  and 
perceptron with error back propagation training algorithm are 
suggested in this paper. 

Model ART-2 has essential disadvantage. It assumes usage 
of  only  one  layer  of  neurons  (not  including  entry,  associated 
with sensors). It results to that the neural network works only 
with  the  metrics  of  primary  features  and  calculates  distance 
between images (for classification or creation of a new cluster - 
output neuron), using usually  Euclidean distance. It results to 
that for many applications the model ART-2 is a little used. For 
example,  for  clusterization  and  pattern  recognition  by  the 
mobile  robot  [6]  it  is  required  to  recognize  the  object  in 
different foreshortenings and allocating in different parts of a 
field of vision, i.e. recognition should be invariant concerning 
conversions of the map, such as shifts, rotations and others. 

Invariancy  of  recognition  provide  multilayer  perceptrons 
because  in  them  on  intermediate  layers  in  learning  process 
secondary  features  are  forming.  It  is  possible  to  tell,  that  in 
perceptrons  each  layer  provides  conversion  of  one  metrics  to 
another. 

In  this  report  the  hybrid  model  is  offered.  It  combines  of 
advantages  of  multilayer  perceptron  with  learning  by  error 
back propagation [7] and model ART-2. 

II. HYBRID MODEL OF THE NEURAL NETWORK 

In  suggested  model  the  first  some  layers  of  neurons  are 
organized as perceptron with forward connections. Its outputs 
are inputs of model of Grossberg-Carpenter ART-2. Perceptron 
provides  conversion  of  the metrics  of  primary  features in  the 
metrics of secondary features in space of considerably smaller 
dimension.  Neural  network  "ART-2"  classifies  images  and 
uses  secondary  features  to  do  it.  Training  of  perceptron  by 

 
back propagation algorithm provides "attraction" of an output 
vector of perceptron to centre of recognized cluster by ART-2. 
It  is  possible  to  tell,  that  the  recognized  class  is  a  context  in 
which  other  images  from  learning  sampling  are  recognizing, 
and in some limits the system “is ready to recognize” it and in 
the further until then in learning sampling the input vector of 
perceptron  will  be  converted 
to  a  vector  very  much 
distinguished from vector - centre of "cluster-context".  

Action of the suggested model is described by the following 

algorithm in which training without the teacher is realized: 

1.  In  the  perceptron  the  weights  of  links  equal  to  half  of 
quantity  of  neurons  in  the  previous  layer  are  formed.  The 
quantity of output neurons Nout of ART-2 is considered equal 
zero. 

2.  The  next  example  is  shown.  Outputs  of  perceptron  are 

calculating. 

3. If Nout=0, it is formed output neuron with the weights of 
links  equal  to  values  of  inputs  of  model  ART-2  (outputs  of 
perceptron). 

4. If Nout> 0, in model ART-2 the algorithm of calculation of 
distances  between  its  input  vector  and  centers  of  existing 
clusters (weight vectors of output neurons) is executing: 

d

j

=

(

x

∑ −
i

i

w
ij

2)

, 

where: xi – ith digit of input vector of ART-2, 

wij – ith digit of weight of jth output neuron (cluster). 

for 

links 

If distance for the neuron-winner more than defined R value 
(vigilance  or  radius  of  cluster),  the  new  cluster  as  well  as  in 
step 3 is formed. 

5. If  the distance  for  the  neuron-winner  is  less R  in  model 
the  neuron-winner  are 
ART-2  weights  of 
enumerating,  approximating  centre  of  a  cluster  to  the  input 
vector of model ART-2: 
w
im

where: Nm – number of recognized input vectors of mth cluster. 
And  for  perceptron  recalculation  of  weights  by  algorithm 
“error  back  propagation”  (EBP)  are  executing.  Thus  a  new 
vector of weights of output neuron-winner of model ART-2 is 
viewed as output vector-example for EBP, and the quantity of 
iterations  can  be  little  (in  particular,  there  can  be  only  one 
iteration). 

6. The algorithm repeats from step 2 while there are learning 

w
im

w
im

1()

N

, 

−

+

+

(

x

i

)

m

=

examples. 

Action of the suggested model is explained in a figure 1. 
Here the space of secondary features in which by points are 
represented  output  vector  perceptronа  (input  vector  of  model 

ART-2),  centers  of  clusters  are  shown.  In  a  figure  the 
following points are represented: 

 

 

Distance between input vector of ART-2 and 

center of recognized cluster

2 

4 

3 

11 

R1 

5 

Figure 1. Explanation of action of hybrid model 

1  -  a  new  image  for  which  the  cluster  with  radius  R  is 

2 - the new image recognized as concerning to this cluster, 
3  -  the  new  centre  of  a  cluster  calculated  in  step  5  of 

 

created, 

algorithm, 

4  -  a  new  output  vector  of  perceptron,  approximated  to 
centre of a cluster as a result of executing of algorithm “error 
back propagation”, 

5 - the new image recognized as inhered to other cluster.  

III. EXPERIMENTS 

For  research  of  the  suggested  model  the  program  of 
generation of a sequence of patterns with shift on one of axes 
and  the  program  model  of  the  neural  network  handling  this 
sequence  of  patterns  has  been  developed.  In  a  figure  2  two 
examples of images used in experiments are shown. 

 

1) 
 

 2)

 

Figure 2. The images used in experiments 

 

The  suggested  algorithm  of  training  without  the  teacher  in 
experiments has shown good results at rather minor changing 
of each next example in a taught sequence. Thus the following 
parameters of model have been used:  

Quantity of input neurons (pixels) - 10000 (100х100), 
Quantity of neurons in hidden layer of perceptron - 10, 
Quantity  of  neurons  in  an  output  layer  of  perceptron  (in 

input layer of ART-2) Nout - 10, 

Radius of cluster R = 0.01, 
Activation  function  of  neurons  of  perceptron  is  rational 

sigmoid  with parameter a=1,  

perceptron is from 1 to 10. 

Number  of 

iterations  of  recalculation  of  weights  of 

Some results of experiments are shown in figures 3 and 4. 

0.012

0.01

0.008

0.006

0.004

0.002

0

1

2

Nbp=1
Nbp=3
Nbp=5
Nbp=10

8

9

4

3
7
Number of shifted image

5

6

Figure 3. For sequence of image 1 with horizontal shift 

Distance between input vector of ART-2 and 

center of recognized cluster

0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0

1

2

Nbp=1
Nbp=3
Nbp=5
Nbp=10

8

9

4

3
7
Number of shifted image

5

6

 

 

Figure 4. For sequence of image 2 with horizontal shift 

IV. CONCLUSION 

The  suggested  hybrid  model  of  the  neural  network  can  be 
used  in  the  mobile  robot  when  it  is  necessary  to  watch 
sequence  of  the  images  visible  by  the  robot  during  its 
movement,  and  to  extract  in  it  new  images  (objects),  i.e. 
essential changes in the scene visible by the robot. More over, 
this model may be used in security systems for recognition of 
new objects or persons in sight of camera. Modification of this 
algorithm  can  be  algorithm  in  which  the  quantity  of  created 
clusters  is  limited.  In  this  case,  if  the  quantity  of  clusters 
(output neurons) has achieved a limit, arises the problem what 
to do with images which are not recognized, i.e. which cannot 
be related to any existing cluster. In this case may be offered 
increase of parameter R (radius of clusters) and to try to apply 
again algorithm of recognition and so until the new image will 
not be related to one of clusters. After that, it is necessary to 
reduce  quantity  of  clusters  (output  neurons),  uniting  clusters 
with the centers which appear in one cluster, and accordingly 
changing weights  of  links between outputs of perceptron and 
outputs neurons-clusters. 

Modification of this algorithm can be algorithm of training 
with  the  teacher  in  whom  before  to  apply  the  procedure  of 
increase of radius and decrease of quantity of output neurons, 
the  system  requests  "teacher"  what  to  do  -  to  apply  this 
procedure or to create a new cluster. As "teacher" there can be 
not  only  inquiry  to    user,  but  also  any  additional  test  for 
novelty of an image. 

The  following  further  researches  of  the  suggested  hybrid 

model of the neural network are planned: 

- A mathematical substantiation of suggested algorithms, 
- Research of influence of perceptron and ART-2 parameters 

on results of action of the neural network, 

- Testing of the suggested model on program model of the 

mobile robot and the real robot, 

- Research of various modifications of algorithm of hybrid 

model. 

REFERENCES 

[1]  Carpenter  G.,  A.,  Grossberg  S.  Pattern  Recognition  by  Self-Organizing 

Neural Networks, Cambridge, MA, MIT Press, 1991. 

[2]   \Shie-Jue  Lee,  Chun-Liang  Hou.  An  ART-Based  Construction  of  RBF 
Networks. - IEEE Trans. on Neural Networks, Vol. 13, No. 6, November, 
2002. 

[4] 

[3]  Baxter  R.A.  Supervised  Adaptive  Resonance  Networks.  –  Proc.  of  the 
conference on Analysis of neural network applications, 1991. – Pp. 123-
137. 
Jung-Hua Wang, Jen-Da Rau, Wen-Jeng Liu. Two-Stage Clustering via 
Neural  Networks.  -  IEEE  Trans.  on  Neural  Networks,  Vol.  14,  No.  3, 
May 2003. – Pp. 606-615. 

[5]  Hahn-Ming Lee, Member, IEEE, Chih-Ming Chen, and Yung-Feng Lu. A 
Self-Organizing  HCMAC  Neural-Network  Classifier.  -  IEEE  Trans.  on 
Neural Networks, Vol. 14, No. 1, January 2003. – Pp.15-27. 

[6]  Gavrilov A.V., Gubarev V.V., Jo K.-H., Lee H.-H. Hybrid Neural-based 
Control  System  for  Mobile  Robot.  -  Int  Symp.  KORUS-2004,  Tomsk, 
2004. - Vol. 1, Pp. 31-35. 

[7]  Parallel  Distributed  Processing:  Explorations  in  the  microstructure  of 
cognition.  Rumelhart  D.E.,  McClelland  J.L.  (eds.),  v.I,  II,  MIT  Press, 
1986 

 
.

 

