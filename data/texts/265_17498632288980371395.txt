A Generalization of Principal Component Analysis to the Exponential Family

Michael Collins

Sanjoy Dasgupta

Robert E. Schapire

AT&T Labs Research

f g180 Park Avenue, Florham Park, NJ 07932 mcollins, dasgupta, schapire @research.att.com

Abstract
Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.

1 Introduction

Principal component analysis (PCA) is a hugely popular dimensionality reduction tech-

nique that attempts to find a low-dimensional subspace passing close to a given set of
x x 2points 1; : : : ; n Rd. More specifically, in PCA, we find a lower dimensional subspace xthat minimizes the sum of the squared distances from the data points i to their projections

i in the subspace, i.e.,

Xn jjxi ijj2:

(1)

i=1

This turns out to be equivalent to choosing a subspace that maximizes the sum of the

squared lengths of
x 0these projections if

the projections the data happens

i, to

which is the be centered at

same as the (empiPrical)
the origin (so that i i

variance
= ).

of

PCA also has another convenient interpretation that is perhaps less well known. In this

xprobabilistic interpretation, each point i is thought of as a random draw from some un-

2known
then of

distribution P
PCA is to find

i, where P denotes a
the set of parameters

unit Gaussian with mean
1; : : : ; n that maximizes

the

Rd. The purpose
likelihood of the

x xdata, subject to the condition that these parameters all lie in a low-dimensional subspace. In

other
1; : :

w: ;orndsw, h1ic;h:

::;
lie

n
in

are considered to be a subspace; the goal

noise-corrupted versions of is to find these true points,

some true points and the main as-

sumption is that the noise is Gaussian. The equivalence of this interpretation to the ones

given above follows simply from the fact that negative log likelihood under this Guassian

model is equal (ignoring constants) to Eq. (1).

This Gaussian assumption may be inappropriate, for instance if data is binary-valued, or integer-valued, or is nonnegative. In fact, the Gaussian is only one of the canonical distributions that make up the exponential family, and it is a distribution tailored to real-valued

data. The Poisson is better suited to integer data, and the Bernoulli to binary data. It seems

natural to consider variants of PCA which are founded upon these other distributions in

place of the Gaussian.

f gWe extend PCA to the rest of the exponential family. Let P be any parameterized set

of distributions from the exponential family, where is the natural parameter of a distribu-
2tion. For instance, a one-dimensional Poisson distribution can be parameterized by R,

corresponding to mean = e and distribution P (n) = en e =n! = e n=n!; n 2

f0; 1; 2; :
which lie

::;
in

g: Given data x1; :
a low-dimensional

: : ; xn 2 Rd, the goal is
subspace and for which

nthoewlotog-filinkdelpiharoaomdePterislog1

;: P

:

i:(;xin)

is maximized.

Our unified approach effortlessly permits hybrid dimensionality reduction schemes in
xwhich different types of distributions can be used for different attributes of the data. If
the data i have a few binary attributes and a few integer-valued attributes, then some coordinates of the corresponding i can be parameters of binomial distributions while others are parameters of Poisson distributions. (However, for simplicity of presentation, in this
abstract we assume all distributions are of the same type.)

The dimensionality reduction schemes for non-Gaussian distributions are substantially dif-
xferent from PCA. For instance, in PCA the parameters i, which are means of Gaussians,
lie in a space which coincides with that of the data i. This is not the case in general, and therefore, although the parameters i lie in a linear subspace, they typically correspond to a nonlinear surface in the space of the data.

xThe discrepancy and interaction between the space of parameters and the space of the
data is a central preoccupation in the study of exponential families, generalized linear models (GLM's), and Bregman distances. Our exposition is inevitably woven around these three intimately related subjects. In particular, we show that the way in which we generalize PCA is exactly analogous to the manner in which regression is generalized by GLM's. In this respect, and in others which will be elucidated later, it differs from other variants of PCA recently proposed by Lee and Seung [7], and by Hofmann [4].

We show that the optimization problem we derive can be solved quite naturally by an algorithm that alternately minimizes over the components of the analysis and their coefficients; thus, the algorithm is reminiscent of Csisza´r and Tusna´dy's alternating minization procedures [2]. In our case, each side of the minimization is a simple convex program that can be interpreted as a projection with respect to a suitable Bregman distance; however, the overall program is not generally convex. In the case of Gaussian distributions, our algorithm coincides exactly with the power method for computing eigenvectors; in this sense it is a generalization of one of the oldest algorithms for PCA. Although omitted for lack of space, we can show that our procedure converges in that any limit point of the computed coefficients is a stationary point of the loss function. Moreover, a slight modification of the optimization criterion guarantees the existence of at least one limit point.
MSome comments on notation: All vectors in this paper are row vectors. If is a matrix, mwe denote its i'th row by i and its ij'th element by mij.

2 The Exponential Family, GLM's, and Bregman Distances

2.1 The Exponential Family and Generalized Linear Models

In the exponential family of distributions the conditional probability of a value x given
parameter value takes the following form:

log P (xj ) = log P0(x) + x G( ):

(2)

Hxreeaisrles1.,.GFi(rsot)mhiesth"ainsfauittunfrcoatillolponawrtshamathteaettneGrs"u(roef)st=thhealdtoigtshtPerisbxuu2mtXio(nPin,0ta(enxgd)reacxla)no. uf sPu(axlljy

take any value in the
) over the domain of

XWe use to denote the domain of x. The sum is replaced by an integral in the continuous Xcase, where P defines a density over . P0 is a term that depends only on x, and can

usually be ignored as a constant during estimation. The main difference between different
members of the family is the form of G( ). We will see that almost all of the concepts of the PCA algorithms in this paper stem directly from the definition of G.

taAothhnuaifidtstcrGoiissstm(eauexs)msau.=maeIlmnplylbte2hew=iirss2roi.actftaAnestnohnereoamXtsehaxel=loprdgocifnsoP0temr;(ni1xbmtigujao.tlin)Tofanch=ma,eswipeliyrtiohlsowbmagaitbBpehiael2nlirotnygooPaufnl0l(xd(ixxdu2)insiXt=trivb)ia2sur=tiulia2oosn.guncaIpeflto,l2ycwratwhnhirecbihtcetxeah2vnsa=eesP2roia,fi(fdxebedji=pnnt)sahir=tayyt, px(1 p)(1 x) where p is a parameter in 0; 1]. This is a member of the exponential family with P0(x) = 1, = log 1pp, and G( ) = log(1 + e ). EtpAhaepcxBerjiret.ir]c,nBatolhyuefludleinixfccfpateiesroecentnEatitisiaoxttnihjneog]fd=Gxer(puiv.n)aIdtnei=vrtehPleGo(ggx0e(Pjne))x,r.a2wIlXnchatiPhcs0eeh(,nxwEo)eremxxwjai,ll]liditdsisiertsenrifoebeaturesrtieialodysntv,goEe(arsi)fixthetjhder]o"t=heuxagtphego,cu(atnta)tdtihoi=inns parameter", and g defines a function from the natural parameter values to the expectation
parameter values.

Our generalization of PCA is analogous to the manner in which generalized linear mod-

els (GLM's) [8] provide a unified treatment of regression for the exponential family by

generalizing least-squares regression to loss functions that are more appropriate for other

xmembers of this family. The regression set-up assumes a training sample of ( i; yi) pairs,

x 2 2where i Rd is a vector of attributes, and yi R is some response variable. The pa-

2 xrameters of the model are a vector

approximation
x= arg min

of yi
2Rd

.PIni(lyeiast

squares
i)2

Rd. The dot product

i is

regression the optimal parameters

.

taken are

to be set to

an be

x x xx jwtIpitishnaaiertlsGhai"mmmLcfioapeMxndtleeyo'edrslns,,Pvichawaari(rhelilaeoldnirgneicrkPeeh",i0c,)(Gtilywsiy(siht)ahte)ap+erkep=ierynhnovixet2=ior=ms2eagapt,aoeipngfdrdotbbhxieyGteiimnf("ogallitlnteohkwethifs,ideu)eean.earncxisdvtIipilanoeyttnhcitvt"ethhaeeat[liG8otoc]gtna0.h-(slepeiAk)am.eornalfaIimahnxatoiuemtotrhneaduiorlsmrPmocc-hfalaiositlkelhiocedeteglihisehPeitxsori(npobtyaooduitnutcuxierorsniaine)--l

terion is equivalent to the least squares criterion. Another interesting case is logistic re-

Pgreislsoiogn

where
1+e

G(
yi

)= xi

log(1 + e ), and the negative where yi = 1 if yi = 1, yi =

log-likelihood
1 if yi = 0.

for

parameters

is

2.2 Bregman Distances and the Exponential Family

!Let F :

R be a differentiable and strictly convex function defined on a closed, convex

2set R. The Bregman distance associated with F is defined for p; q to be

BF (p k q) =: F (p) F (q) f(q)(p q)

where f (x) = F 0(x). It can be shown that, in general, every Bregman distance is nonneg-

ative and is equal to zero if and only if its two arguments are equal.

For the exponential family the log-likelihood log P (xj ) is directly related to a Bregman

X
G( ) g( ) F (x) f(x) = g 1(x)
BF (p k q) BF (x k g( ))

normal
R 2=2

Bernoulli
f0; 1g
log(1 + e )

Poisson
f0; 1; 2 : : : 1g
e

x2=2

e
x(1l+oeg()x) + (1 x) log(1 x)

e
x log(x) x

x
(p

q)2=2

plolgog1

x
+x
p q

(1

p)

log

1 1

p q

log x

p log

p q

+

q

p

(x )2=2 log(1 + e x ) where x = 2x 1 e x + x log x x

Table 1: Various functions of interest for three members of the exponential family

distance. Specifically, [1, 3] define a "dual" function F through G and g:

F (g( )) + G( ) = g( )

(3)

It can be shown under fairly general conditions that f (x) = g 1(x). Application of these

identities implies that the negative log-likelihood of a point can be expressed through a

Bregman distance [1, 3]:

log P (xj ) = log P0(x) F (x) + BF (x k g( )) :

(4)

In other words, negative log-likelihood can always be written as a Bregman distance plus a term that is constant with respect to and which therefore can be ignored. Table 1 summarizes various functions of interest for examples of the exponential family.

We will find it useful to extend the idea of Bregman distances to divergences between vec-

x y A Btors and x y A Bas BF (

maktric)es=. IfPi,

BFar(exviecktoyrsi,)aanndd

,
BF

(

are

kmatr)ic=es,PthienPwjeBoFve(raloijadkthbeijn)o.ta(Ttiohne

notion of Bregman distance as well as our generalization of PCA can be extended to vec-

tors in a more general manner; here, for simplicity, we restrict our attention to Bregman

distances and PCA problems of this particular form.)

3 PCA for the Exponential Family

We now generalize PCA to other members of the exponential family. We wish to find i's

xthat are "close" to the i's and which belong to a lower dimensional subspace of parameter

v vspace. Thus, our approach is to find a basis v xthe linear combination of these elements i =

1P; : :
k

:; aik

`

in Rd and to represent
k that is "closest" to i.

each

i as

X x VLet be the n v A AVwishoks,eain'dthlerot w

d
be
is

matrix
the n

w`hmosaetrii'xthwriothweilsemie.nLtsetaik

be the . Then

`

d matrix whose k'th row = is an n d matrix

i as above. This is a matrix of natural parameter values which define

Xthe probability of each point in .

Following the discussion in Section 2, we consider the loss function taking the form
L(V; A) = log P (XjA; V) = X X log P (xijj ij) = C+X X( xij ij+G( ij))

ij

ij

where C is a constant term which will be dropped from here on. The loss function varies

Xdepending on which member of the exponential family is taken, which simply changes the
form of G. For example, if is a matrix of real values, and the normal distribution is

appropriate for the data, then G( ) = 2=2 and the loss criterion is the usual squared loss

for PCA. For the
then L(V; A) =

PBeirnPoujllloi gd(is1tr+ibuetioxnij,

G( ij ).

)

=

log(1 +

e

).

If

we

define

xij

=

2xij

1,

From the relationship between log-likelihood and Bregman distances (see Eq. (4)), the loss

can

also

be

written as
L(V; A)

=

X

X

BF

(xij

k

g(

ij)) = X BF (xi

k

g(

i))

ij

i

V(where we allow g to be applied to vectors and matrices in a pointwise manner). Once A x 2and have been found for the data points, the i'th data point i Rd can be represented a aas the vector i in the lower dimensional space R`. Then i are the coefficients which xdefine a Bregman projection of the vector i:

ai

=

arg

min a2R`

BF

(xi

k

g(aV)) :

(5)

The generalized form of PCA can also be considered to be search for a low dimensional

V xbasis (matrix ) which defines a sunrface that is close oto all the data points i. We define the

V V aV a Vset of points Q( ) to be Q( ) = g( ) j V x qimizes the sum of projection distances:
Note that for the normal distribution g( ) =

2
=

R` arg

. The
minV

Popitimmianlqv2aQlu(eVf)oBr F

then min-
( i k ).

and the Bregman distance is Euclidean dis-

V a x VVQtance so
( ) is

that the projection operation in Eq. (5) is a simple linear projection ( i also simplified in the normal case, simply being the hyperplane whose

=i
basis

is

T

). .

To summarize, once a member of the exponential family -- and by implication a convex
function G( ) -- is chosen, regular PCA is generalized in the following way: The loss function is negative log-likelihood, log P (xj ) = x + G( ) + constant.
AVThe matrix = is taken to be a matrix of natural parameter values. AVThe derivative g( ) of G( ) defines a matrix of expectation parameters, g( ).
A function F is derived from G and g. A Bregman distance BF is derived from F .
The loss is a sum of Bregman distances from the elements xij to values g( ij).
V Q VPCA can also be thought of as search for a matrix that defines a surface ( ) which
is "close" to all the data points.
The normal distribution is a simple case because g( ) = , and the divergence is Euclidean
Q Vdistance. The projection operation is a linear operation, and ( ) is the hyperplane which Vhas as its basis.

4 Generic Algorithms for Minimizing the Loss Function

We now describe a generic algorithm for minimization of the loss function. First, we con-
centrate on the simplest case where there is just a single component so that ` = 1. (We drop the k subscript from aik and vjk.) The method is iterative, with an initial random choice for
V V A Vtthe value of . Let (t), (t), etc. denote the values at the 'th iteration, and let (0) be the A V Ainitial random choice. We propose the iterative updates (t) = arg minA L( ;(t 1) ) V V Aand (t) = arg minV L( ; )(t) . Thus L is alternately minimized with respective to
its two arguments, each time optimizing one argument while keeping the other one fixed, reminiscent of Csisza´r and Tusna´dy's alternating minization procedures [2].

It is useful to write these minimization problems as follows:

For i = 1 : : : n, For j = 1 : : : d,

a(t) i
v(t) j

= =

arg arg

mina2R minv2R

P Pj
i

BF BF

xij k g(avj(t )1)

xij

k

g

a( (t) i

v

)

.

We can then see that there are n + d optimization problems, and that each one is essen-
tially identical to a GLM regression problem (a very simple one, where there is a single
parameter being optimized over). These sub-problems are easily solved, as the functions
are convex in the argument being optimized over, and the large literature on maximum-
likelihood estimation in GLM's can be directly applied to the problem.

These updates take a simple form for the nor-

A X V V V A X Ajj jj jj jj= ( ) = = ( ) =mal distribution: (t)

(t 1) T

(t 1) 2, and (t)

(t) T

(t) 2. It

V V X Xfollows that =(t) (t 1) T =C, where C is a scalar value. The method is then equivX Xalent to the power method (see Jolliffe [5]) for finding the eigenvector of T with the Vlargest eigenvalue, which is the best single component solution for . Thus the generic

algorithm generalizes one of the oldest algorithms for solving the regular PCA problem.

The loss is convex in either of its arguments with the other fixed, but in general is not convex in the two arguments together. This makes it very difficult to prove convergence to the global minimum. The normal distribution is an interesting special case in this respect -- the power method is known to converge to the optimal solution, in spite of the non-convex nature of the loss surface. A simple proof of this comes from properties of eigenvectors
H(Jolliffe [5]). It can also be explained by analysis of the Hessian : for any stationary point Hwhich is not the global minimum, is not positive semi-definite. Thus these stationary
points are saddle points rather than local minima. The Hessian for the generalized loss function is more complex; it remains an open problem whether it is also not positive semidefinite at stationary points other than the global minimum. It is also open to determine under which conditions this generic algorithm will converge to a global minimum. In preliminary numerical studies, the algorithm seems to be well behaved in this respect.
A VMoreover, any limit point of the sequence =(t) (t) (t) will be a stationary point.

However, it is possible for this sequence to diverge since the optimum may be at infinity.

To avoid such degenXeraXte chBoiFce(sxoijf

, we can use a modified loss
k g( ij)) + "BF ( 0 k g( ij))]

ij

where " is a for which g

small positive constant,
1( 0) is finite). This is

and 0 is any value roughly equivalent

in to

the range of g (and
adding a conjugate

therefore prior and

finding the maximum a posteriori solution. It can be proved, for this modified loss, that the

sequence (t) remains in a bounded region and hence always has at least one limit point

which must be a stationary point. (All proofs omitted for lack of space.)

There nent.

are We

gviavreioounsewaalygsortiothomptiwmhiizcehthcyeclloessstfhurnocutgiohnthweh`encothmepreonisenmtso,roeptthimanizoinneg

compoeach in

turn while the others are held fixed:

A 0 V 0//Initialization
Set = , =

//Cycle through ` components N times
For n = 1; : : : ; N , c = 1; : : : ; `:

v kk//NowIFnooiFrFtpiootatirrl=mizjiei=1=z;e:1c(1t0:;h):::e:;r::acc:dn'o;td,nnhvo,emcroglmye,npacovanenc(i(dctjte))snet==t wsaaiijtrrhgg=ommtPhiinnekavr=622cRRcoamPPikpvojiknBBjeFFnts

fixed
xij xij

( + )g avc(tj 1) sij ( + )g ai(ct)v sij

The modified Bregman projections now include
the ` 1 fixed components. These sub-problems

a term sij
are again

representing the contribution of a standard optimization problem

regarding Bregman distances, where the terms sij form a "reference prior".

