Particle filter in vision tracking
17th August 2005
Erik Cuevas1,2, Daniel Zaldivar1,2 and Raul Rojas1 1Freie Universität Berlin, Institut für Informatik Takustr. 9, D-14195 Berlin, Germany 2Universidad de Guadalajara
Av. Revolucion No. 1500, C.P. 44430, Guadalajara, Jal., Mexico {cuevas, zaldivar, rojas}@inf.fu-berlin.de Technical Report B 05-13
Abstract The extended Kalman filter (EKF) has been used as the standard technique for performing recursive nonlinear estimation in vision tracking. In this report, we present an alternative filter with performance superior to that of the EKF. This algorithm, referred to as the Particle filter. Particle filtering was originally developed to track objects in clutter (multi-modal distribution). We present as results the filter behavior when exist objects with similar characteristic to the object to track.
1

1 Introduction
The extended Kalman filter (EKF) has been used as the standard technique for performing recursive nonlinear estimation in vision tracking [6]. The EKF algorithm, however, provides only an approximation to optimal nonlinear estimation. In this report, we present an alternative filter with performance superior to that of the EKF. This algorithm, referred to as the Particle filter (Condensation filter, as it is known in the vision community). The basic difference between the EKF and Particle filter stems from the manner in which random variables are represented for propagating through system dynamics. In the EKF, the state distribution is approximated by a gaussian random variable which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed gaussian random variables which may lead to suboptimal performance and sometimes divergence of the filter. The Particle filter address this problem by using a deterministic sampling approach. The state distribution is approximated by a random variable (not necessarily gaussian), but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the random variable and, when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to second order (Taylor series expansion) for any nonlinearity. The EKF, in contrast, only achieves first-order accuracy. No explicit Jacobian or Hessian calculations are necessary for the Particle filter. Remarkably, the computational complexity of the particle filter is the same order as that of the EKF.

2 Optimal recursive estimation

Given observations yk, the goal is to estimate the state xk. We make no assumptions about the nature of the system dynamics at this point. The optimal
estimate in the minimum mean-squared error (MMSE) sense is given by the
conditional mean:

x^k = E[ xk| Y0k]

(1)

where Y0k is the sequence of observations up to time k. Evaluation of this expectation requires knowledge of the a posteriori density p( xk| Y0k) . Given this density, we can determine not only the MMSE estimator, but any "best" estimator under a specified performance criterion. The problem of determining the a posteriori density is in general referred to as the Bayesian approach, and can be evaluated recursively according to the following relations:

p( xk| Y0k)

=

p( xk| Y0k-1)p( yk| xk) p( yk| Y0k-1)

(2)

2

where

p( xk| Y0k-1) = p( xk|xk-1)p( yk| xk)dxk-1 and the normalizing constant p( xk| Y0k) is given by

(3)

p( yk| Y0k-1) = p( xk|Y0k-1)p( yk| xk)dxk

(4)

This recursion specifies the current state density as a function of the previ-
ous density and the most recent measurement data. The state-space model
comes into play by specifying the state transition probability p( xk| xk-1) and measurement probability or likelihood, p( yk| xk) . Specifically, p( xk| xk-1) is determined by the process noise density p(wk) with the state-update equation

xk+1 = f (k, xk) + wk

(5)

For example, given an additive noise model with Gaussian density, p(wk) = N(0, Rv), then p( xk| xk-1) = N(F(xk-1), Rv). Similarly, p( yk| xk) is determined by the observation noise density p(vk) and the measurement equation

yk = h(k, xk) + vk

(6)

In principle, knowledge of these densities and the initial condition p( x0| y0) =

p( y0|x0)p(x0) p(y0 )

determines

p( xk| Y0k)

for

all

k.

Unfortunately, the multidimen-

sional integration indicated by Eqs. (2)-(4) makes a closed-form solution in-

tractable for most systems. The only general approach is to apply Monte Carlo

sampling techniques that essentially convert integrals to finite sums, which con-

verge to the true solution in the limit.

Particle filtering [1-3] was originally developed to track objects in clutter or a variable of interest as it evolves over time, typically with a non-Gaussian and potentially multi-modal probability density function (pdf). The basis of the method is to construct a sample-based representation of the entire pdf (equation 2). A series of actions are taken, each one modifying the state of the variable of interest according to some model (equation 5). Moreover at certain times an observation arrives that constrains the state of the variable of interest at that time.

Multiple hypothetical state (particles) of the variable of interest xk are used, each one associated with a weight that signifies the quality of that specific particle. An estimate of the variable of interest is obtained by the weighted sum of all the particles. The particle filter algorithm is recursive in nature f (k, xk) (prediction stage), including the addition of random noise wk in order

3

to simulate the effect of noise on the variable of interest. Then, each particle's

weight is re-evaluated based on the latest measurements available (update stage).

At times the particles with small weights are eliminated, with a process called

resampling. More formally, the variable of interest (in this case the object

position xk = [ xk yk ]) at time k is represented as a set of M samples (the "particles") Ski = [ xki bki ] : i = 1, 2, ..., M , where the index i denotes the particle number, each particle consisting of a hypothetical value of the variable

of interest xk and a weight b that defines the contribution of this particle to

the overall estimate of the variable, where

M i=1

bki

=

1.

The

figure

1

shown

the

process carried out by the particle filter.

If at time k we know the pdf of the system at the previous instant k -1 then
we model the movement effect with f (k, xk) to obtain a prior of the pdf at time k (prediction). In other words, the prediction phase uses a model in order
to simulate the effect that a movement has on the set of particles with the
appropriate noise added wk. The update phase uses the information obtained from the measurements to update the particle weights in order to accurately
describe the moving object's pdf. Algorithm 1 presents a formal description of
the particle filter algorithm.

3 Particle filter in Vision Tracking
Robust real-time tracking of non-rigid objects in computer vision is a challenging task. Particle filtering has proven very successful for non-linear and non-Gaussian estimation problems. Particle filtering was originally developed to track objects in clutter, that is to say, one of its main characteristics represents the possibility to track objects although exists the presence of other objects that have similar characteristic.
We want to apply a particle filter in a color-based context. Color distributions are used as target models as they achieve robustness against non-rigidity, rotation and partial occlusion. Suppose that the distributions are discretized into m-bins. The histograms are produced with the function h(xi) , that assigns the color at location to the corresponding bin (considering xi the pixel coordinates (x,y) ). In our experiments, the histograms are typically calculated in the RGB space using 8x8x8 bins. To make the algorithm less sensitive to lighting conditions, the HSV color space could be used instead with less sensitivity to V (e.g. 8x8x4 bins).
We determine the color distribution inside an upright circular region centered in with radius r. To increase the reliability of the color distribution when boundary pixels belong to the background or get occluded, smaller weights are assigned to the pixels that are further away from the region center by employing a weighting function

4

Si k -1 xi k -1
Predict
Ski

p(xk-1 yk -1) [xik -1 bki -1]

Resampling

xk

= f (k,

x

i k

-1

)

+

w

k

-1

Measure

Observation density
[xik bki ]
p(xk yk )

Figure 1: Process carried5out by the particle filter.

Algorithm 1 Particle Filter Algorithm From the particles at time-step k -1, Ski -1 = {xki -1, bki -1 i = 1, 2, ..., M }. 1. For each particle we calculate the cumulative probability as

c0k-1 = 0 cki -1 = cki--11 + bki -1

i = 1, 2, ..., M

we have in this way Ski -1 = {xik-1, bik-1, cik-1 i = 1, 2, ..., M }. 2. We select M states (they can repeat) starting from Ski -1 (resampling), carrying out the following procedure

· We generate a random number r  [0, 1], uniformly distributed.

· We find , the smallest j for which cjk-1 · The elected state is ­xki -1 = xjk-1

r.

3. We spread the states { ­xik-1 i = 1, 2, 3..., M } using the model xki = f (k, ­xik-1) + wk. 4. For each new state xik we find their corresponding b starting from the measurement p( y| x)obtained for each hypothesis.

5. We carry out the normalization

M i=1

bik

=

1

and

build

the

particles

Ski

=

{xik, bik i = 1, 2, ..., M }.

6. Once the M samples have been constructed: estimate, if desired, moments

of the tracked position at time k as

E[Ski ] =

M i=1

bik

xki

6

xi
e

Figure 2: Configuration of the density of the particles, centered in xidependent of the distance e.

k(e) =

1 - e2 e < 1 0 otherwise

(7)

where e is the distance from the region center. Thus, we increase the reliability of the color distribution when these boundary pixels belong to the background or get occluded. The figure 2 shows the advantage of using the distance e to improve the reliability of the measurement.
The color distribution py = {p(yu)}u=1,2,3,...m. at location y is calculated as

I

p(yu) = f

k

i=1

y - xi r

[h(xi) - u]

(8)

where I is the number of pixels in the circular region,  is the Kronecker delta function and the normalization factor

f=

1

I i=1

k

|y-xi | r

(9)

ensures that

m u=1

p(yu)

=

1.

In a tracking approach, the estimated state is updated at each time step by incorporating the new observations. Therefore, we need a similarity measure which is based on color distributions. A popular measure between two distributions p(u) and q(u) is the Bhattacharyya coefficient [4, 5].

[p, q] = p(u)q(u)du

(10)

Considering discrete densities such as our color histograms p = {p(u)}u=1,2,3,...m. and q = {q(u)}u=1,2,3,...m.the coefficient is defined as

7

m
[p, q] =
u=1

p(u)q(u)

(11)

The larger  is, the more similar the distributions are. For two identical normalized histograms we obtain  = 1, indicating a perfect match. As distance between two distributions we define the measure

d = 1 - [p, q]

(12)

which is called the Bhattacharyya distance.
The proposed tracker employs the Bhattacharyya distance to update the a priori distribution calculated by the particle filter. Each sample of the distribution represents an ellipse and is given as

xki = [ xik yki xik yki ]

(13)

where x, y specify the location of the ellipse, x and y the motion. As we consider a whole sample set the tracker handles multiple hypotheses simultaneously.
The sample set is propagated through the application of a dynamic model

xki +1 = f (k, xki ) + wki

(14)

where f (k, xki ) defines the deterministic component of the model and wki is a multivariate Gaussian random variable. In this work we currently use an unconstrained Brownian model for describing the region movement with velocity x, y and radius r.

 

xk+1 yk+1 xk+1 yk+1

  = 

exp exp

--4114((xykk exp - exp -

+ 1.5xk)

+ 1.5yk)

1 41 4

xk yk

  + wk

(15)

To weight the sample set, the Bhattacharyya coefficient has to be computed

between the target histogram and the histogram of the hypotheses. Each hypo-

thetical region is specified by its state vector Ski . Both the target histogram q

and the candidate histogram centered at the origin of the

cpirSckiualarre

calculated region.

from

Eq.

3

where

the

target

is

As we want to favor samples whose color distributions are similar to the target model, small Bhattacharyya distances correspond to large weights

8

bi =  1 exp 2

-

(1

-

[pSki 2

,

q

])

(16)

that are specified by a Gaussian with variance . During filtering, samples with a high weight may be chosen several times, leading to identical copies, while others with relatively low weights may not be chosen at all. The programming details for one iteration step are given in the Algorithm 1.

To illustrate the distribution of the sample set, Figure 3 shows the samples distribution considering the flesh color as target histogram q. The samples are located around the maximum of the Bhattacharyya coefficient which represents the best match to the target model.

Given a particle distribution Ski , we need to find the state which defines with accuracy the object position. Three different methods of evaluation have been

used in order to obtain an estimate of the position. First, the weighted mean

(x^k 

M i=1

bik

xki

)

be

used;

second,

the

best

particle

(the

xjk

such

that

bjk

=

max(bik) : i = 1, 2, ...M ) and, third, the weighted mean in a small window

around the best particle (also called robust mean) can be used. Each method

has its advantages and disadvantages: the weighted mean fails when faced with

multi-modal distributions, while the best particle introduces a discretization er-

ror. The best method is the robust mean but it is also the most computationally

expensive. In cases where the object to track is surrounded of objects whose

characteristics are similar the best method is to use as state that defines the

object position the best particle.

9

maximum1

maximum2

(a)

(b) Figure 3: a) Distribution of the sample set and b) generated multi-modal probability density function.
10

References
[1] M. Isard and A. Blake, Contour Tracking by Stochastic Propagation of Conditional Density, European Conference on Computer Vision (1996) 343-356.
[2] M. Isard and A. Blake, A Mixed-State Condensation Tracker with Automatic Model-Switching, International Conference on Computer Vision (1998) 107112.
[3] M. Isard and A. Blake, CONDENSATION (Conditional Density Propagation for Visual Tracking), International Journal on Computer Vision 1(29) (1998) 5-28.
[4] F. Aherne, N. Thacker and P. Rockett, The Bhattacharyya Metric as an Absolute Similarity Measure for Frequency Coded Data, Kybernetika 32(4) (1997) 1-7.
[5] T. Kailath, The Divergence and Bhattacharyya Distance Measures in Signal Selection, IEEE Transactions on Communication Technology COM-15(1) (1967) 52-60.
[6] Cuevas E., Zaldivar D. and Rojas R., Kalman Filter for vision tracking, Technical Report B 05-12, Freie Universität Berlin, Fachbereich Mathematik und Informatik, 2005.
11

