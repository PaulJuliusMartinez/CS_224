Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

University of Basel Spring Semester 2014

THE BEGINNING of the MONTE CARLO METHOD
by N. Metropolis

T he year was 1945. Two earthshaking events took place: the successful test at Alamogordo and the building of the first electronic computer. Their combined impact was to modify qualitatively the nature of global interactions between Russia and the West. No less perturbative were the changes wrought in all of academic research and in applied science. On a less grand scale these events brought about a renascence of a mathematical technique known to the old guard as statistical sampling; in its new surroundings and owing to its nature, there was no denying its new name of the Monte Carlo method.
This essay attempts to describe the details that led to this renascence and the roles played by the various actors. It is appropriate that it appears in an issue dedicated to Stan Ulam.
Los Alamos Science Special Issue 1987

Some Background
Most of us have grown so blase about computer developments and capabilities -even some that are spectacular--that it is difficult to believe or imagine there was a time when we suffered the noisy, painstakingly slow, electromechanical devices that chomped away on punched cards. Their saving grace was that they continued working around the clock, except for maintenance and occasional repair (such as removing a dust particle from a relay gap). But these machines helped enormously with the routine, relatively simple calculations that led to Hiroshima.
The ENIAC. During this wartime period, a team of scientists, engineers, and technicians was working furiously on the

first electronic computer--the ENIAC-- at the University of Pennsylvania in Philadelphia. Their mentors were Physicist First Class John Mauchly and Brilliant Engineer Presper Eckert. Mauchly, familiar with Geiger counters in physics laboratories, had realized that if electronic circuits could count, then they could do arithmetic and hence solve, inter alia, difference equations--at almost incredible speeds! When he'd seen a seemingly limitless array of women cranking out firing tables with desk calculators, he'd been inspired to propose to the Ballistics Research Laboratory at Aberdeen that an electronic computer be built to deal with these calculations.
John von Neumann, Professor of Mathematics at the Institute for Advanced Study, was a consultant to Aberdeen and to Los Alamos. For a whole host of
125

Monte Carlo
Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

University of Basel Spring Semester 2014

reasons, he had become seriously interested in the thermonuclear problem being spawned at that time in Los Alamos by a friendly fellow-Hungarian scientist, Edward Teller, and his group. Johnny (as he was affectionately called) let it be known that construction of the ENIAC was nearing completion, and he wondered whether Stan Frankel and I would be interested in preparing a preliminary computational model of a thermonuclear reaction for the ENIAC. He felt he could convince the authorities at Aberdeen that our problem could provide a more exhaustive test of the computer than mere firing-table computations. (The designers of the ENIAC had wisely provided for the capability of much more ambitious versions of firing tables than were being arduously computed by hand, not to mention other quite different applications.) Our response to von Neumann's suggestion was enthusiastic, and his heuristic arguments were accepted by the authorities at Aberdeen.
In March, 1945, Johnny, Frankel, and I visited the Moore School of Electrical Engineering at the University of Pennsylvania for an advance glimpse of the ENIAC. We were impressed. Its physical size was overwhelming--some 18,000 double triode vacuum tubes in a system with 500,000 solder joints. No one ever had such a wonderful toy!
The staff was dedicated and enthusiastic; the friendly cooperation is still remembered. The prevailing spirit was akin to that in Los Alamos. What a pity that a war seems necessary to launch such revolutionary scientific endeavors. The components used in the ENIAC were jointarmy-navy (JAN) rejects. This fact not only emphasizes the genius of Eckert and Mauchly and their staff, but also suggests that the ENIAC was technically realizable even before we entered the war in December, 1941.
After becoming saturated with indoctrination about the general and detailed structure of the ENIAC, Frankel and I returned to Los Alamos to work on a model
126

that was realistically calculable. (There was a small interlude at Alamogordo!) The war ended before we completed our set of problems, but it was agreed that we continue working. Anthony Turkevich joined the team and contributed substantially to all aspects of the work. Moreover, the uncertainty of the first phase of the postwar Los Alamos period prompted Edward Teller to urge us not only to complete the thermonuclear computations but to document and provide a critical review of the results.

The Spark. The review of the ENIAC results was held in the spring of 1946 at Los Alamos. In addition to Edward Teller, the principals included Enrico Fermi, John von Neumann, and the Director, Norris Bradbury. Stanley Frankel, Anthony Turkevich, and I described the ENIAC, the calculations, and the conclusions. Although the model was relatively simple, the simplifications were taken into account and the extrapolated results were cause for guarded optimism about the feasibility of a thermonuclear weapon.
Among the attendees was Stan Ulam, who had rejoined the Laboratory after a brief time on the mathematics faculty at the University of Southern California. Ulam's personality would stand out in any community, even where "characters" abounded. His was an informal nature; he would drop in casually, without the usual amenities. He preferred to chat, more or less at leisure, rather than to dissertate. Topics would range over mathematics, physics, world events, local news, games of chance, quotes from the classics--all treated somewhat episodically but always with a meaningful point. His was a mind ready to provide a critical link.
During his wartime stint at the Laboratory, Stan had become aware of the electromechanical computers used for implosion studies, so he was duly impressed, along with many other scientists, by the speed and versatility of the ENIAC. In ad-

Stanislaw Ulam
dition, however, Stan's extensive mathematical background made him aware that statistical sampling techniques had fallen into desuetude because of the length and tediousness of the calculations. But with this miraculous development of the ENIAC--along with the applications Stan must have been pondering--it occurred to him that statistical techniques should be resuscitated, and he discussed this idea with von Neumann. Thus was triggered the spark that led to the Monte Carlo method.
The Method
The spirit of this method was consistent with Stan's interest in random processes--from the simple to the sublime. He relaxed playing solitaire; he was stimulated by playing poker; he would cite the times he drove into a filled parking lot at the same moment someone was accommodatingly leaving. More seriously, he created the concept of "lucky numbers," whose distribution was much like that of prime numbers; he was intrigued by the theory of branching processes and

Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

Monte Carlo
University of Basel Spring Semester 2014

contributed much to its development, including its application during the war to neutron multiplication in fission devices. For a long time his collection of research interests included pattern development in two-dimensional games played according to very simple rules. Such work has lately emerged as a cottage industry known as cellular automata.
John von Neumann saw the relevance
of Ulam's suggestion and, on March 11, 1947, sent a handwritten letter to Robert Richtmyer, the Theoretical Division leader (see "Stan Ulam, John von Neumann, and the Monte Carlo Method"). His letter included a detailed outline of a possible statistical approach to solving the problem of neutron diffusion in fissionable material.
Johnny's interest in the method was contagious and inspiring. His seemingly relaxed attitude belied an intense interest and a well-disguised impatient drive. His talents were so obvious and his cooperative spirit so stimulating that he garnered the interest of many of us. It was at that time that I suggested an obvious name for the statistical method--a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he "just had to go to Monte Carlo." The name seems to have endured.
The spirit of Monte Carlo is best conveyed by the example discussed in von Neumann's letter to Richtmyer. Consider a spherical core of fissionable material surrounded by a shell of tamper material. Assume some initial distribution of neutrons in space and in velocity but ignore radiative and hydrodynamic effects. The idea is to now follow the development of a large number of individual neutron chains as a consequence of scattering, absorption, fission, and escape.
At each stage a sequence of decisions has to be made based on statistical probabilities appropriate to the physical and geometric factors. The first two decisions occur at time t = O, when a neutron is selected to have a certain velocity and a cer-

tain spatial position. The next decisions are the position of the first collision and the nature of that collision. If it is determined that a fission occurs, the number of emerging neutrons must be decided upon, and each of these neutrons is eventually followed in the same fashion as the first. If the collision is decreed to be a scattering, appropriate statistics are invoked to determine the new momentum of the neu-
John von Neumann
tron. When the neutron crosses a material boundary, the parameters and characteristics of the new medium are taken into account. Thus, a genealogical history of an individual neutron is developed. The process is repeated for other neutrons until a statistically valid picture is generated.
Random Numbers. How are the various decisions made? To start with, the computer must have a source of uniformly distributed psuedo-random numbers. A much used algorithm for generating such numbers is the so-called von Neumann "middle-square digits." Here, an arbitrary n-digit integer is squared, creating a 2n-digit product. A new integer is formed by extracting the middle n-digits from the product. This process is iterated over and over, forming a chain

127

Monte Carlo
Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

University of Basel Spring Semester 2014

example, see the section entitled "The learned that Fermi took great delight in versed. When a material boundary was

Monte Carlo Method" in "A Primer on astonishing his Roman colleagues with crossed, another choice was made appro-

Probability, Measure, and the Laws of his remarkably accurate, "too-good-to-be- priate to the new material. The device

Large Numbers.") Since its inception, lieve" predictions of experimental results. could accommodate two neutron energies,

many international conferences have been After indulging himself, he revealed that referred to as "slow" and "fast." Once

held on the various applications of the his "guesses" were really derived from again, the Master had just the right feel

method. Recently, these range from the statistical sampling techniques that he for what was meaningful and relevant to

the conference, "Monte Carlo Methods used to calculate with whenever insomnia do in the pursuit of science.

and Applications in Neutronics, Photon- struck in the wee morning hours! And

ics, and Statistical Physics," at Cadarache so it was that nearly fifteen years earlier, The First Ambitious Test. Much to

Castle, France, in the spring of 1985 to Fermi had independently developed the the amazement of many "experts," the

the latest at Los Alamos, "Frontiers of Monte Carlo method.

ENIAC survived the vicissitudes of its

Quantum Monte Carlo," in September,

200-mile journey. In the meantime Rich-

I 1985.

ard Clippinger, a staff member at Ab-

erdeen, had suggested that the ENIAC

I Putting the Method into Practice

had sufficient flexibility to permit its controls to be reorganized into a more conve-

Let me return to the historical account.

nient (albeit static) stored-program mode

In late 1947 the ENIAC was to be moved

of operation. This mode would have a

to its permanent home at the Ballistics

capacity of 1800 instructions from a vo-

Research Laboratory in Maryland. What

cabulary of about 60 arithmetical and log-

a gargantuan task! Few observers were

ical operations. The previous method of

of the opinion that it would ever do an-

programming might be likened to a gi-

1 other multiplication or even an addition.

ant plugboard, that is to say, to a can

It is a tribute to the patience and skill

of worms. Although implementing the

of Josh Gray and Richard Merwin, two

new approach is an interesting story, suf-

fearless uninitiated, that the move was a

fice it to say that Johnny's wife, Klari,

success. One salutary effect of the inter-

and I designed the new controls in about

ruption for Monte Carlo was that another

two months and completed the implemen-

distinguished physicist took this occasion

tation in a fortnight. We then had the

to resume his interest in statistical studies. Enrico Fermi helped create modern

Enrico Fermi

opportunity of using the ENIAC for the first ambitious test of the Monte Carlo

physics. Here, we focus on his inter-

method--a variety of problems in neu-

est in neutron diffusion during those ex-

It was then natural for Fermi, during tron transport done in collaboration with

citing times in Rome in the early thir- the hiatus in the ENIAC operation, to Johnny.

ties. According to Emilio Segre, Fermi's dream up a simple but ingenious ana-

Nine problems were computed corre-

student and collaborator, "Fermi had in- log device to implement studies in neu- sponding to various configurations of ma-

vented, but of course not named, the tron transport. He persuaded his friend terials, initial distributions of neutrons,

present Monte Carlo method when he was and collaborator Percy King, while on a and running times. These problems, as

studying the moderation of neutrons in hike one Sunday morning in the moun- yet, did not include hydrodynamic or ra-

Rome. He did not publish anything on tains surrounding Los Alamos, to build diative effects, but complex geometries

the subject, but he used the method to such an instrument--later affectionately and realistic neutron-velocity spectra

solve many problems with whatever cal- called the FERMIAC (see the accompa- were handled easily. The neutron histo-

culating facilities he had, chiefly a small nying photo).

ries were subjected to a variety of statisti-

mechanical adding machine."*

The FERMIAC developed neutron ge- cal analyses and comparisons with other

In a recent conversation with Segre, I nealogies in two dimensions, that is, in a approaches. Conclusions about the effi-

plane, by generating the site of the "next cacy of the method were quite favorable.

collision. " Each generation was based It seemed as though Monte Carlo was

Company from From X-Rays to Quarks by Emilio Segre.

on a choice of parameters that charac- here to stay.

terized the particular material being tra-

Not long afterward, other Laboratory

128

Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

Monte Carlo
University of Basel Spring Semester 2014

THE FERMIAC
The Monte Carlo trolley, or FERMIAC, was invented by Enrico Fermi and constructed by Percy King. The drums on the trolley were set according to the material being traversed and a random choice between fast and slow neutrons. Another random digit was used to determine the direction of motion, and a third was selected to give the distance to the next collision. The trolley was then operated by moving it across a twodimensional scale drawing of the nuclear device or reactor assembly being studied. The trolley drew a path as it rolled, stopping for changes in drum settings whenever a material boundary was crossed. This infant computer was used for about two years to determine, among other things, the change in neutron population with time in numerous types of nuclear systems.

staff members made their pilgrimages to ENIAC to run Monte Carlo problems. These included J. Calkin, C. Evans, and F. Evans, who studied a thermonuclear problem using a cylindrical model as well as the simpler spherical one. B. Suydam and R. Stark tested the concept of artificial viscosity on time-dependent shocks; they also, for the first time, tested and found satisfactory an approach to hydrodynamics using a realistic equation of state in spherical geometry. Also, the distinguished (and mysterious) mathematician C. J. Everett was taking an interest in Monte Carlo that would culminate in a series of outstanding publications in collaboration with E. Cashwell. Meanwhile, Richtmyer was very actively running Monte Carlo problems on the socalled SSEC during its brief existence at IBM in New York.
In many ways, as one looks back, it was among the best of times.
Rapid Growth. Applications discussed in the literature were many and varied and spread quickly. By midyear 1949 a

symposium on the Monte Carlo method, sponsored by the Rand Corporation, the National Bureau of Standards, and the Oak Ridge Laboratory, was held in Los Angeles. Later, a second symposium was organized by members of the Statistical Laboratory at the University of Florida in Gainesville.
In early 1952a new computer, the MANIAC, became operational at Los Alamos. Soon after Anthony Turkevich led a study of the nuclear cascades that result when an accelerated particle collides with a nucleus. The incoming particle strikes a nucleon, experiencing either an elastic or an inelastic scattering, with the latter event producing a pion. In this study particles and their subsequent collisions were followed until all particles either escaped from the nucleus or their energy dropped below some threshold value. The "experiment" was repeated until sufficient statistics were accumulated. A whole series of target nuclei and incoming particle energies was examined.
Another computational problem run on the MANIAC was a study of equations

of state based on the two-dimensional motion of hard spheres. The work was a collaborative effort with the Tellers, Edward and Mici, and the Rosenbluths, Marshall and Arianna (see "Monte Carlo at Work"). During this study a strategy was developed that led to greater computing efficiency for equilibrium systems obeying the Boltzmann distribution function. According to this strategy, if a statistical "move" of a particle in the system resulted in a decrease in the energy of the system, the new configuration was accepted. On the other hand, if there was an increase in energy, the new configuration was accepted only if it survived a game of chance biased by a Boltzmann factor. Otherwise, the old configuration became a new statistic.
It is interesting to look back over twoscore years and note the emergence, rather early on, of experimental mathematics, a natural consequence of the electronic computer. The role of the Monte Carlo method in reinforcing such mathematics seems self-evident. When display units were introduced, the temptation to exper-
129

Monte Carlo
Dr. Martin O. Steinhauser Graduate Lecture

Handout 17 16 May 2014

University of Basel Spring Semester 2014

iment became almost irresistible, at least for the fortunate few who enjoyed the luxury of a hands-on policy. When sharedtime operations became realistic, experimental mathematics came of age. At long last, mathematics achieved a certain parity-the twofold aspect of experiment and theory-that all other sciences enjoy.
It is, in fact, the coupling of the subtleties of the human brain with rapid and reliable calculations, both arithmetical and logical, by the modern computer that has stimulated the development of experimental mathematics. This development will enable us to achieve Olympian heights.
The Future
So far I have summarized the rebirth of statistical sampling under the rubric of Monte Carlo. What of the future-- perhaps even a not too distant future?
The miracle of the chip, like most miracles, is almost unbelievable. Yet the fantastic performances achieved to date have not quieted all users. At the same time we are reaching upper limits on the computing power of a single processor.
One bright facet of the miracle is the lack of macroscopic moving parts, which makes the chip a very reliable bit of hardware. Such reliability suggests parallel processing. The thought here is not a simple extension to two, or even four or eight, processing systems. Such extensions are adiabatic transitions that, to be sure, should be part of the immediate, short-term game plan. Rather, the thought is massively parallel operations with thousands of interacting processors-even millions!
Already commercially available is one computer, the Connection Machine, with 65,536 simple processors working in parallel. The processors are linked in such a way that no processor in the array is more than twelve wires away from another and the processors are pairwise connected by a number of equally efficient
130

routes, making communication both flexible and efficient. The computer has been used on such problems as turbulent fluid flow, imaging processing (with features analogous to the human visual system), document retrieval, and "common-sense" reasoning in artificial intelligence.
One natural application of massive parallelism would be to the more ambitious Monte Carlo problems already upon us. To achieve good statistics in Monte Carlo calculations, a large number of "histories" need to be followed. Although each history has its own unique path, the underlying calculations for all paths are highly parallel in nature.
Still, the magnitude of the endeavor to compute on massively parallel devices must not be underestimated. Some of the tools and techniques needed are: q A high-level language and new archi-
tecture able to deal with the demands of such a sophisticated language (to the relief of the user); q Highly efficient operating systems and compilers; q Use of modern combinatorial theory, perhaps even new principles of logic, in the development of elegant, comprehensive architectures; q A fresh look at numerical analysis and the preparation of new algorithms (we have been mesmerized by serial computation and purblind to the sophistication and artistry of parallelism). Where will all this lead? If one were to wax enthusiastic, perhaps--just perhaps--a simplified model of the brain might be studied. These studies, in turn, might provide feedback to computer architects designing the new parallel structures. Such matters fascinated Stan Ulam. He often mused about the nature of memory and how it was implemented in the brain. Most important, though, his own brain possessed the fertile imagination needed to make substantive contributions to the very important pursuit of understanding intelligence. s

Further Reading
S. Ulam, R. D. Richtmyer, and J. von Neumann. 1947. Statistical methods in neutron diffusion. Los Alamos Scientific Laboratory report LAMS­551. This reference contains the von Neumann letter discussed in the present article.
N. Metropolis and S. Ulam. 1949. The Monte Carlo method. Journal of the American Statistical Association 44:335-341.
S. Ulam. 1950. Random processes and transformations. Proceedings of the International Congress of Mathematicians 2:264-275.
Los Alamos Scientific Laboratory. 1966. Fermi invention rediscovered at LASL. The Atom, October, pp. 7-11.
C. C. Hurd. 1985. A note on early Monte Carlo computations and scientific meetings. Annals of the History of Computing 7:141­155.
W. Daniel Hillis. 1987. The connection machine. Scientific American, June, pp. 108­1 15.
N. Metropolis received his B.S. (1937) and his Ph.D. ( 1941) in physics at the University of Chicago. He arrived in Los Alamos, April 1943, as a member of the original staff of fifty scientists. After the war he returned to the faculty of the University of Chicago as Assistant Professor. He came back to Los Alamos in 1948 to form the group that designed and built MANIAC I and II. (He chose the name MANIAC in the hope of stopping the rash of such acronyms for machine names, but may have, instead, only further stimulated such use.) From 1957 to 1965 he was Professor of Physics at the University of Chicago and was the founding Director of its Institute for Computer Research. In 1965 he returned to Los Alamos where he was made a Laboratory Senior Fellow in 1980. Although he retired recently, he remains active as a Laboratory Senior Fellow Emeritus.

