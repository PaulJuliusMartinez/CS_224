INFORMEDIA DIGITAL VIDEO LIBRARY

Michael Christel, Scott Stevens, and Howard Wacthr
Carnegie Mellon University Pittsburgh, PA 15213 macfi?sei.cmu.edu

Vastdigital libraries of information will soon be available on the nation's Information Superhighway as a result of emerging technologies for multimedia computing. These libraries wiU profoundly impact the conduct of business, professional, andpersonal activity. However, it is not enough to simply store and play back
video (ss in currently envisioned commercial video-on-demand services); to be most effective, new technology is needed for searching through these vast data collections and retrieving the most relevant selections.
The Informedia Project is developing these new technologies for data storage, search, and retriewd, and in collaboration with QED Communications is embedding them in a video library system for use in education, txaining, and entertainment. The Inforrnedia Project leverages efforts from many Carnegie Mellon University computing research activities, including:
q Sphinx-II speech recognition q Image Understanding Systems Laboratory q Center for Machine Translation (information
retrieval) q Software Engineering Institute (information
modeling)
The Informedia Project is developing intelligen~ automatic mechanisms that provide full-content search and retrieval from digital video, audio, and text libraries. The project integrates speech, image, and language understanding for the creation and exploration of such libraries. l%e initial library will be built using WQED'S vi&o assets.
LIBRARY CREATION
`l'he Informedia system uses Sphinx-II to transcribe narratives and dialogues automatically. Sphhx-11 is a large vocabulary, speakerindependent, continuous speech recognize developed at Carnegie Mellon. With recent advances in acoustic and language modeling, it has achieved a 95% suczess rate on standardized tests for a 5000-word, general dictation task. By relaxing time constraints and allowing transcripts to be generated off-line, Sphinx-II will be adapted to handle the video library domain's larger vocabulary and diverse audio sources without severely degrading recognition rates.
In addkion to annotating the vi&o library with text transcripts, the videos will be segmented into smaller subsetfsor faster access and retrieval of relevant information. Some of this segmentation is pnssible via the time-based transcript generated from the audio information. `l'be workat CMU'S Image Understanding Systems Laboratory focuses on segmenting video clips via visual content. Rather

than manually reviewing a file frame-by-frame around an index entry point machine vision methods that interpret image sequences can be used to automatically locate beginning and end points for a scene or uxwrsation. This segmentation process can be improved through the use of contextual information supplied by the transcript and language understanding. Figure 1 gives an overview of the InforMedia system.

TV Footage -o Extra Footage --p New Video Footage ~

Raw Video Material

audio

video

e
Speech & Lan u@ Interpretation and In%exing

Vldei Segmentation and Compression

l;tex:d

Database

-

Distribution o;Sale to Users

CREATION

q OFFLINE

lEEEiE3----
Video Libray

Natural Langu>ge ~earching

\ Interactive Video

EXPLORATION

ONLINE

mFigure 1. Informedia System Overview

LIBRARY EXPLORATION
Finding desired items in a large information base poses a major challenge. The Inforrnedia Project goes kyond simply searching

480

the transcript text and will, in addition, apply natural-language understanding for knowledge-based search and retrieval. One strategy employs computational linguistic techniques from the Center for Machine Translation for indexing, browsing, and retrieving baaed on identification of noun phrases in a written document. Other techniques from the Center include statistical weighting, term selection heuristics, and natural-language processing. More complex than individual words, these linguistic units provide a conceptually richer domain for subsequent processing.
The Informedia system is extending this technology for spoken language and applying it to correct and index the automaticallytranscribed soundtracks. Other tasks will include identification of topics and subtopics in transcript collections, and a rich natural language retrieval interface. A second thrust is developing robust techniques for matching transcribed words and phrases that sound alike when spoken. This integrated approach will significantly increase the Informedia system's ability to locate a particular videa wgment quickly, despite transcription errors, inadequate keywords, and ambiguous sounds.
Along with improving query capabilities, the Informedia Project is researching better ways to present infornmtion from a given video library. Interface issues include helping the user identify desired video when multiple objects are returned, adjusting the length of video objects returned, and letting the user quickly skim video objects to locate sections of interest.
Cinematic knowledge can enhance the composition and reuse of materials from the video library. For example, the 1ibrary may contain hours of interview footage with experts in a certain topic area. Rather than simply presenting a series of disassociated video windows in response to user queries, this interview footage could be leveraged to produce sur interface in which the user becomes the interviewer. The natural language techniques mentioned above are usedtoparse the user's questions, and scenes from the interview footage are composed dynamically to present relevant answers. Such an interface is designed to engage the user into more fully exploring and interacting with the video library in search of information as an active interviewer.
UBRARY DEMONSTRATION

The Informedia Project currently has a demonstration based on a small database (one gigabyte) of texL ~aphics, video, and audio material &awn from WQED'S "Space Age" series.A sample display of this demonstration appears as Figure 2 for the reader's reference. T?re demonstration was carefully scripted to illustrate the following points (listed in temporal order, as they occur in the demonstration):
. parsing the user's input according to an appropriate grammar for that domain allows for more nattrrrd, less cumbersome queries
q natural language understanding of botb a user's query and the video library trarssmipts enables the efficient retrieval of relevant information
q the location wifhin a video object is identified relevant to a user query via the text transcript
q the video "paragraph", or size of the video object, is determinable based on language understanding of the transcript and image understanding of the video contents
q a larger video object can be "skimmed" in an order of magnitude less time, while coherently presenting all of the important information of the original object
 video clips can be reused in different ways, e.g., to create an interactive simulated interview, as shown in Figure 3
I Rrltmr. what da uou thlrrk qbaut trauol to Mssrs? I

Figure 3. Simulated Interview with Arthur C. Clarke
The demonstration does not show Sphinx-II in action performing automatic transcription nor does it document the process of video segmentation or natural language parsing. It shows the benefits of such automatic indexing and segmentation, illustrating the accurate search and selective retrievaf of audio and video materials appropriate to users' needs and desires, It shows how users can preview as well as scan video at variable rates of speed and presentation, akin to skimming written material. Finally, it demonstrates the concept of combining speech, language, and image understanding technologies to create entertaining educational experiences.
Figure 2. Screen Dump of Informediam Demonstration
481

