The Minimal Disagreement Parity Problem as a Hard Satisfiability Problem
James M. Crawford Computational Intelligence Research Laboratory
1269 University of Oregon Eugene, OR 97403
Michael J. Kearns Robert E. Schapire AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974
February 1, 1994
1 Introduction
One common approach to generating hard instances for SAT algorithms is to encode other NP-complete problems into SAT. An obvious problem with this approach is that many NP-complete problems are trivially solvable in the average case [4]. One solution to this problem is to choose problem classes which are randomly self-reducible. As we show in section 3, randomly self-reducible problems are as hard in the average case as they are in the worst case.
The minimal disagreement parity (MPD) problem is a simple example of a randomly self-reducible problem.1 Informally the problem is the following: given a set of sample input vectors and a set of sample parities, find the bits of the input vectors on which the parities were computed. In the absence of noise this problem has a known polynomial solution. In the presence of a moderate amount of noise (i.e., corruption of around 1/4 of the parity bits), however, there is reason to believe that the problem is quite hard.
1An MDP problem is defined by choices from three distributions. For one of these distributions it is randomly self-reducible. For the other two distributions we expect MDP to be hard for other reasons (discusses in section 3).
1

In this paper we formally define the parity learning problem and give evidence for why it may be a hard problem. We overview the relevant work in coding theory and learning theory. We then give the translation to satisfiability used to create the parity problems in the Dimacs benchmark set. Finally, we provide experimental results for two simple satisfiability algorithms (depth-first search with unit-resolution, and iterative-sampling with unit-resolution).

2 Problem statement

Ttohbroeuogvheor uthtethfiieslpdaGpeFr ,2th¡ ,ei.aer.i,ththmeefiteicldoopfeirnatteiognesrsomf aoddduiltoio2n.

and multiplication
Thus, if ¢¤¦£ ¨¥ § © 0£

are assumed
1 then ¢¥

is the equal
$&! for

parity of ¢ and ¥ . Moreover, if a §© 0£ 1  to the parity of a subset of the variables $ 1 £ which ¢ !  1). It is functions of this type

then
... £ $

the inner product a  x   (specifically, the subset



!#" 1 ¢ %! $&! is
of variables

that we consider in this paper. Specifically,

we are interested in the problem of finding the parity function that is maximally consistent

with a sample of input/output pairs. This problem, which we call the minimal disagreement

parity problem (MPD) is defined formally below:

Instance:
Sample Inputs: x! '§ © 0£ 1  for )(  1£ . . . £10 . Sample outputs: 2 ! § © 0£ 1 for (3 1£ . . . £10 . Error tolerance: integer 4 .
Find: a vector a §5© 0£ 1 such that the parity function represented by a disagrees with the input out sample on at most 4 points, i.e., for which 6 7© ( : a  x9! @8 2 !  B6 A ¤4 C

Here the x! represent 0 sample inputs over D Boolean variables, the 2 ! the (potentially incorrect) value of the unknown parity function on each of the m inputs, and 4 represents a
bound on the acceptable number of errors.
The hard part of this problem is to identify the subset of the D Boolean variables over which the parity function is computed. This is represented by the a: for those variables ( that are part of the parity function, ¢ !  1 (and for those that are not ¢ !  0). Of course it is
not necessary to identify the correct set of variables, as long as the parity function over the
set that is identified gets a sufficient number of the samples (i.e., all but 4 of them) correct.
In this paper, we consider instances of MDP that are generated at random by the
following process (where D , 0 and 4 are given):
1. First, a random target vector s § © 0£ 1 is generated uniformly at random.

2

2. Next, a set of sample inputs x! §E© 0£ 1 are generated uniformly at random for 3(  1£ . . . 1£ 0 .
3. Finally, a set of "noise bits" F ! §© 0£ 1 are generated by randomly choosing exactly 4 of these bits to be 1, and the rest are set to 0. Each sample output 2 ! is then computed as 2 !  s  x! GF ! .
The resulting instance is then given by the sample inputs x! , the sample outputs 2 ! and the error tolerance 4 .

3 Theoretical evidence for the hardness of the parity problem

In this section, we review some of the theoretical evidence suggesting the hardness of the

MDP problem. To begin with, it is known [1] that the MDP problem is NP-complete, and so is likely
to be intractable in the worst case. However, this fact in itself is not an indication of the hardness of solving a random instance of the problem since there are many problems that are known to be NP-complete, and yet are trivially solvable in the average case (see,

for instance, Johnson's survey article [4]). We have no proof that the parity problem is

intractable for the random distributions that we consider; for instance, it is unknown if

the problem is "average-case complete" in the sense of Levin [6, 3]. Nevertheless, there

is considerable circumstantial evidence that solving random instances of the problem is

computationally intractable.

As noted above, the random distribution of instances of the parity problem can be decomposed into three distributions: (1) the distribution of target vectors s; (2) the distribution
of example vectors x! ; and (3) the distribution on noise bits F ! . For the first of these, we
are able to make a "random self-reducibility" argument that the average case (in which s is generated uniformly at random) is as hard as the worst case in which an all-powerful adversary is allowed to choose the vector s. Put another way, we show that any algorithm

for the MDP problem may assume without loss of generality that the target vector s has
been generated uniformly at random.
To see that this is so, consider an instance of MDP that has been generated by an arbitrary
target vector s, sample inputs x1 £ . . . £ xH , and noise bits F 1 £ . . . £1FIH , yielding sample outputs 2 !  s  x! G F ! . We show that this instance can be reduced to one in which the generating

target vector is in fact chosen uniformly at random. To make the reduction, we choose a

vector t addition

§P© 0
over

£

1 
GF

 

uniformly at random, and let sQR s 2¡ ). Then sQ is uniformly distributed.



t (where 
We replace

denotes usual vector
each sample output 2 !

by 2 !Q S 2 !  t  x!    s  tT¡  x! GF !  sQ  x! U F ! C

Thus, each 2B!Q can be computed using t (without knowing the target vector s), and, moreover, we can regard 2 !Q as having been generated by sQ . Finally, we note that a solution aQ to this

3

transformed instance can be converted back to a solution a to the original instance by setting
a  aVQ  t since
aQ  x! W2 Y!Q X   a  t¡T x! ` 2 !  t  x! X a  x! W 2 ! C

This argument shows that if we have an algorithm that can solve instances of MDP in

which the target vector s is generated at random, then we can solve instance in which s is

chosen by an adversary. We conclude that there is no method of choosing "hard" target

vectors that is better than simply choosing them uniformly at random.

For a different reason, we believe that the method we use of choosing sample inputs
x! leads to hard instances of MDP. Specifically, by choosing the x! vectors uniformly at

random, we create a sample in which the solution space is extremely "flat" in the sense that,

except for the single target vector s, all vectors a have high disagreement on the sample.
More precisely, if a is different from s, then the expected number of vectors x! for which a  x! a2 ! is exactly 0cb 2 (where, as usual, 2 !  s  x! WF ! , and F ! is a random noise bit). This follows from the fact that, for a randomly chosen vector $d$&! , the probability that a  x!  s  x! is exactly 1b 2 if a 8 s. Thus, no gradient descent type method is likely to be

an effective tool for solving such MDP problems since, as we search through the solution

space, it is impossible to tell whether progress is being made until we actually locate the

target vector s.

Finally, evidence comes from the area of computational learning theory that the ran-

domly generated instances of MDP that we consider are intractable. Specifically, this

problem is essentially equivalent to the problem of "learning" a parity function with noise.

rIanntdhoismseexttainmgp,leas(orafnthdeomfo)rmtar  gxe£ tsv excetorVF ¡

s ,

is chosen,
where F is

and the learner a noise bit. The

has goal

access to noisy of the learner is

to identify the target vector s.2 In the absence of noise (or in the presence of very low levels

of noise), this learning problem can be solved by linear-algebraic techniques [2]. However,

it is unknown if parity functions can be efficiently learned in the presence of a constant
rate of noise (for example, in which the noise bit F is 1 with probability 1b 8). Moreover,

Kearns [5] has proved that it is impossible to learn noisy parity functions using a broad

class of "statistical" techniques. Since the class of techniques covered by Kearns' result

includes all of the known techniques for learning in the presence of noise, it appears quite

unlikely that noisy parity functions can be learned efficiently.

Finally, we note that the MDP problem is closely related to the problem of efficiently

decoding random linear codes, which is a long-standing open problem. [1].

2More generally, the learner's goal may be to find a function that approximates the behavior of the function
s f x. However, it can be shown that an algorithm which achieves this goal can be converted into one that
achieves the seemingly harder goal of exactly identifying the target vector s.

4

4 Encoding Parity Learning as a Satisfiability Problem

4.1 The Translation

Essentially we generate a propositional theory saying "a is a solution to this parity learning problem". Variables representing a appear explicitly in the theory generated so one can read out the "answer" by looking at the values assigned to them by a model.
The translation proceeds by the following steps:
1. Randomly choose the vectors x1£ . . . £ xH , s, and F 1 £ . . . g£ FIH . Set 2 !  s  x! h F ! . For the problems in the Dimacs challenge we used an error rate of 1b 8.3
2. Generate propositional formula that "calculate" the parities of a  x! . We do this with a matrix i of propositional variables. We generate propositional formula equivalent
to the following:

For () 1 to 0 :

i

!
0

W 2

!

For pq 1 to D : i9r! 5si !r1t 1 u

¢ r3v $ r!

These formula force is!  0 iff a  x! S2 ! .

3. Generate propositional formula to sum the is! . We do this using matrices w (sum)

a n d
2

 

x

(carry) of propositional variables. The
0 ¡ : Tw ! @ ( th bit of sum of first  terms.

invariant

here

is

that

for

y( 

For ()

 
0 to

2   0c¡

:

w

!0



0.

For pq 1 to 0 :

0 to

These

Fx¦r0or5 (si  r 0 to For ( 1 to
formula force w

    2  
2
H to

0¡ 0¡
be

: w !r  w !rgt 1 u x !r .

: x r!

w

!1r t t

1 1



x !r t 1.

a base two representation

of

6

7© (

:a

x9! S8 2

!

6
.

4. We generate propositional formula saying "w H is at most 4 . We currently choose 0

to be a power of two check to just a check

and that

corrupt one eighth the high order bits

of of

tw hH e

parity bits. are not set.

This

reduces

this

last

5 Some Experimental Results
We are in the process of running experiments using iterative sampling with unit-resolution and depth-first search with unit-resolution on these problems. This should provide a basic benchmark against which to judge the more complex algorithms developed in the challenge.
3In the limit the hardest problems arguably will occur when the error rate is 1 4. However, for the "small" problems we generated for the Dimacs challenge an error rate of 1 4 gave theories with too many models.
5

Acknowledgements
We would like to acknowledge Haym Hirsh for his help with developing the translation of the parity problem into a satisfiability problem.
References
[1] E. Berlekamp, R. McEliece, and H. van Tilborg. On the inherent intractability of certain coding problems. IEEE Transactions on Information Theory, 24, 1978.
[2] Paul Fischer and Hans Ulrich Simon. On learning ring-sum-expansions. SIAM Journal on Computing, 21(1):181­192, February 1992.
[3] Yuri Gurevich. Average case completeness. Journal of Computer and System Sciences, 42(3):346­398, 1991.
[4] David S. Johnson. The NP-completeness column: An ongoing guide. Journal of Algorithms, 5(2):284­299, June 1984.
[5] Michael Kearns. Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, pages 392­401, 1993.
[6] Leonid A. Levin. Average case complete problems. SIAM Journal of Computing, 15(1):285­286, February 1986.
6

