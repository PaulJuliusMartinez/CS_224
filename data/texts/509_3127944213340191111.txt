Empirical Processes in M-Estimation by
Sara van de Geer

Handout at New Directions in General Equilibrium Analysis

Cowles Workshop, Yale University June 15-20, 2003

Version: June 13, 2003

1

Most of the first part can be found in van de Geer (2000) Empirical Processes in M-Estimation, Cambridge University Press (see also the references therein). The second part also contains more recent work.
Contents Part 1
Empirical processes and asymptotic normality of M-estimators 1. Introduction 1.1. Law of large numbers for real-valued random variables 1.2. Rd-valued random variables 1.3. Definition Glivenko-Cantelli classes of sets. 2. Which classes are Glivenko-Cantelli classes? 2.1. General Glivenko-Cantelli classes 2.2. Vapnik-Chervonenkis classes 3. Convergence of means to their expectations 3.1. Uniform law of large numbers for classes of functions 3.2. VC classes of functions 3.3. Exercises 4. Uniform central limit theorems 4.1. Real-valued random variables 4.2. Rd-valued random variables 4.3. Donsker's theorem 4.4. Donsker classes 5. M-estimators 5.1. What is an M-estimator? 5.2. Consistency and uniform laws of large numbers 5.3. Asymptotic normality of M-estimators
2

5.4. Conditions a, b and c for asymptotic normality

5.5. Asymptotics for the median

5.6. Conditions A, B and C for asymptotic normality

5.7. Exercise

Part 2 Empirical processes and regularization of M-estimators

1. A classical approach 2. The sequence space model 3. Sparse signals 4. A concentration inequality 5. Hard and soft thresholding 6. The oracle 7. Discretization 8. General penalties 9. Application to the classical penalty 10. Robust regression 11. Density estimation 12. Classification 13. Some references for Part 2

3

Part 1 Empirical processes and asymptotic normality of M-estimators
1. Introduction. Let X1, . . . , Xn, . . . be i.i.d. copies of a random variable X with values in X and with distribution P .

1.1. Law of large numbers for real-valued random variables. Consider the case X = R. Suppose the mean
 = EX

exists. Define the average

X¯n

=

1 n

n

Xi, n  1.

i=1

Then, by the law of large numbers, as n  ,

X¯n  , a.s.

Now, let

F (t) = P (X  t), t  R,

be the theoretical distribution function, and

Fn(t) =

1 n

#{Xi

 t,

1  i  n},

t  R,

be the empirical distribution function. Then by the law of large numbers, as n  ,

Fn(t)  F (t), a.s. for all t.

The Glivenko-Cantelli Theorem says that

sup |Fn(t) - F (t)|  0, a.s.
t
This is a uniform law of large numbers.

Application: Kolmogorov's goodness-of-fit test. We want to test

H0 : F = F0.

Test statistic: Reject H0 for large values of Dn.

Dn = sup |Fn(t) - F0(t)|.
t

1.2. Rd-valued random variables. Questions: (i) What is a natural extension of half-intervals in R to higher dimensions? (ii) Does Glivenko-Cantelli hold for this extension?

4

1.3. Definition Glivenko-Cantelli classes of sets. Let for any (measurable1) A  X ,

Pn(A) =

1 n

#{Xi



A,

1  i  n}.

We call Pn the empirical measure (based on X1, . . . , Xn).

Let D be a collection of subsets of X .

Definition 1.3.1. The collection D is called a Glivenko-Cantelli (GC) class if sup |Pn(D) - P (D)|  0, a.s.
DD
Example. Let X = R. The class of half-intervals D = {l(-,t] : t  R}
is GC. But when e.g. P = uniform distribution on [0, 1] (i.e., F (t) = t, 0  t  1), the class B = {all (Borel) subsets of [0, 1]}
is not GC.
2. Which classes are Glivenko-Cantelli classes?
2.1. General GC classes. Let D be a collection of subsets of X , and let {1, . . . , n} be n points in X .
Definition 2.1.1. We write D(1, . . . , n) = card({D  {1, . . . , n} : D  D} = the number of subsets of {1, . . . , n} that D can distinguish.
That is, count the number of sets in D, when two sets D1 and D2 are considered as equal if D1 D2  {1, . . . , n} = . Here
D1 D2 = (D1  D2c)  (D1c  D2)
1We will skip measurability issues, and most of the time do not mention explicitly the requirement of measurability of certain sets or functions. This means that everything has to be understood modulo measurability.
5

is the symmetric difference between D1 and D2.
Remark. For our purposes, we will not need to calculate D(1, . . . , n) exactly, but only a good enough upper bound.

Example. Let X = R and Then for all {1, . . . , n}  R

D = {l(-,t] : t  R}. D(1, . . . , n)  n + 1.

Example. Let D be the collection of all finite subsets of X . Then, if the points 1, . . . , n are
distinct, D(1, . . . , n) = 2n.

Notation. The simultaneous distribution of (X1, . . . , Xn, . . .) is denoted by P = P ×...×P ×....

Theorem 2.1.2. (Vapnik and Chervonenkis (1971)). We have

if and only if

sup |Pn(D) - P (D)|  0 a.s.
DD

1 n

log

D (X1 ,

.

.

.

,

Xn)

P

0.

2.2. Vapnik-Chervonenkis classes.

Definition 2.2.1. Let
mD(n) = sup{ D(1, . . . , n) : 1, . . . , n  X }.
We say that D is a Vapnik-Chervonenkis (VC) class if for certain constants c and r, and for all n,
mD(n)  cnr, i.e., if mD(n) does not grow faster than a polynomial in n.

Important conclusion: For sets, VC  GC.

Examples.

6

a) X = R, D = {l(-,t] : t  R}. Since mD(n)  n + 1, D is VC.

b) X = Rd, D = {l(-,t] : t  Rd}. Since mD(n)  (n + 1)d, D is VC.

c) X = Rd, D = {{x : T x > t},

 t

 Rd+1}.

Since

mD(n)  2d

n d

,

D

is

VC.

The VC property is closed under measure theoretic operations:

Lemma 2.2.2. Let D, D1 and D2 be VC. Then the following classes are also VC: (i) Dc = {Dc : D  D}, (ii) D1  D2 = {D1  D2 : D1  D1, D2  D2}, (iii) D1  D2 = {D1  D2 : D1  D1, D2  D2}.

Examples.

- the class of intersections of two halfspaces,

- all ellipsoids,

- all half-ellipsoids,

- in R, the class {{x : 1x + . . . + rxr  t} :

 t

 Rr+1}.

There are classes that are GC, but not VC.

Example. Let X = [0, 1]2, and let D be the collection of all convex subsets of X . Then D is not VC, but when P is uniform, D is GC.

Finally, we will not deny you the following definition and a very nice lemma.

Definition 2.2.3. The VC dimension of D is V (D) = inf{n : mD(n) < 2n}.

Lemma 2.2.4. We have that D is VC if and only if V (D) < .

3. Convergence of means to expectations. 7

Notation. For a function g : X  R, we write gdP = Eg(X),
and 1n
gdPn = n i=1 g(Xi).
3.1. Uniform law of large numbers for classes of functions. Let G be a collection of real-valued functions on X .
Definition 3.1.1. The class G is called a Glivenko-Cantelli (GC) class if sup | gdPn - gdP |  0, a.s.
gG

Example. G = {lD : D  D} is GC if D is GC.

3.2. VC classes of functions.
Definition 3.2.1. The subgraph of a function g : X  R is subgraph(g) = {(x, t)  X × R : g(x)  t}.
A collection of functions G is called a VC class if the subgraphs {subgraph(g) : g  G} form a VC class.

Examples (X = Rd).

a) G = {g(x) = 0 + 1x1 + . . . + dxd :   Rd+1},

b) G = {g(x) = |0 + 1x1 + . . . + dxd| :   Rd+1} .

  c) d = 1, G = g(x) =

a + bx d + ex

 

if x  c if x > c

,



a b c d e





 R5,

d) d = 1, G = {g(x) = ex :   R}.

Definition 3.2.2. The envelope G of a collection of functions G is defined by G(x) = sup |g(x)|, x  X .
gG
8

Theorem 3.2.3. Suppose G is VC and that GdP < . Then G is GC.
3.3. Exercises. Exercise 1
Are the following classes of sets (functions) VC? Why (not)? Which classes are GC for all P ? 1) The class of all rectangles in Rd.
2) The class of all monotone functions on R.
3) The class of functions on [0, 1] given by G = {g(x) = aebx + cedx : (a, b, c, d)  [0, 1]4}.
4) The class of all sections in R2 (a section is of the form {(x1, x2) : x1 = a1 + r sin t, x2 = a2 + r cos t, 1  t  2}, for some (a1, a2)  R2 and some 0  1  2  2). 5) The class of all star-shaped sets in R2 (a set D is star-shaped if for some a  D and all b  D also all points on the line segment joining a and b are in D).

Exercise 2
Let G be the class of all functions g on [0, 1] with derivative g satisfying |g|  1. Check that G is not VC. Show that G is GC by using partial integration and the Glivenko-Cantelli Theorem for the empirical distribution function.

4. Uniform central limit theorems. 4.1. Real-valued random variables. Let X = R.

Central limit theorem in R. Suppose EX = , and var(X) = 2 exist. Then

 Pn

X¯n -  

z

 (z), f or all z,

where  is the standard normal distribution function.

Notation.

 n

X¯n -  

L N (0, 1),

9

.

or n(X¯n - ) L N (0, 2).

4.2. Rd-valued random variables. Let X1, X2, . . . be i.i.d. Rd-valued random variables, copies of X (X  X = Rd), with expectation  = EX, and covariance matrix  = EXXT - T .

Central limit theorem in Rd. We have n(X¯n - ) L N (0, ),

i.e.

 n

aT (X¯n - )

L N (0, aT a), f or all a  Rd.

.

4.3. Donsker's Theorem. Let X = R. Recall the definition of the distribution function F and the empirical distribution function Fn:

F (t) = P (X1  t), t  R,

Fn(t) =

1 n

#{Xi

 t,

1  i  n},

t  R.

By the central limit theorem in R (Section 4.1), for all t

 n(Fn(t)

-

F

(t))

L

N

(0,

F

(t)(1

-

F

(t)))

.

Also, by the central limit theorem in R2 (Section 4.2), for all s < t,

 n

Fn(s) - F (s) Fn(t) - F (t)

L N (0, (s, t)),

where

(s, t) =

F (s)(1 - F (s)) F (s)(1 - F (t)) F (s)(1 - F (t)) F (t)(1 - F (t))

.

We are now going to consider the stochastic process Wn = {Wn(t) : t  R}. The process Wn is called the (classical) empirical process.

Definition 4.3.1. Let K0 be the collection of bounded functions on [0, 1]. The stochastic process B(·)  K0, is called the standard Brownian bridge if

- B(0) = B(1) = 0,

 B(t1)  - for all r  1 and all t1, . . . , tr  (0, 1), the vector  ...  is multivariate normal with mean

zero,

B (tr )

10

- for all s  t, cov(B(s), B(t)) = s(1 - t). - the sample paths of B are a.s. continuous.

Donsker's theorem. Consider Wn and WF = B  F as elements of the space K of bounded
functions on R. We have Wn L WF ,

that is,

Ef (Wn)  Ef (WF ),

for all continuous and bounded functions f .

Reflection. Suppose F is continuous. Then, since B is almost surely continuous, also WF = B  F is almost surely continuous. So Wn must be approximately continuous as well in some sense. Indeed, we have for any t and any sequence tn converging to t,
|Wn(tn) - Wn(t)| P 0.
This is called asymptotic continuity.

4.4. Donsker classes. Let X1, . . . , Xn, . . . be i.i.d. copies of a random variable X, with values in the space X , and with distribution P . Consider a class G of functions g : X  R. The (theoretical) mean of a function g is
gdP = Eg(X),

and the (empirical) average (based on the n observations X1, . . . , Xn) is

gdPn

=

1 n

n i=1

g(Xi).

Here Pn is the empirical distribution (based on X1, . . . , Xn).

Definition 4.4.1. The empirical process indexed by G is 
n(g) = n gd(Pn - P ), g  G.

Let us recall the central limit theorem for g fixed. Denote the variance of g(X) by

2(g) = var(g(X)) = Eg2(X) - (Eg(X))2.

If 2(g) < , we have

n(g) L N (0, 2(g)).

11

The central limit theorem also holds for finitely many g simultaneously. Let gk and gl be two functions and denote the covariance between gk(X) and gl(X) by

(gk, gl) = cov(gk(X), gl(X)) = Egk(X)gl(X) - Egk(X)Egl(X).

Then, whenever 2(gk) <  for k = 1, . . . , r,  n(g1)   ...  L N (0, g1,...,gr ), n (gr )

where g1,...,gr is the variance-covariance matrix

 2(g1) . . . (g1, gr) 

()

g1,...,gr = 

...

...

...  .

(g1, gr) . . . 2(gr)

Definition 4.4.2. Let  be a Gaussian process indexed by G. Assume that for each r  N and for each finite collection {g1, . . . , gr}  G, the r-dimensional vector
 (g1)   ... 
 (gr ) has a N (0, g1,...,gr )-distribution, with g1,...,gr defined in (*). We then call  the P -Brownian bridge indexed by G.
Definition 4.4.3. Consider n and  as bounded functions on G. We call G a P -Donsker class if n L ,
that is, if for all continuous and bounded functions f , we have
Ef (n)  Ef ().

Definition 4.4.4. The process n on G is called asymptotically continuous if for all g0  G, and all (possibly random) sequences {gn}  G with (gn - g0) P 0, we have
|n(gn) - n(g0)| P 0.

We will use the notation i.e., · is the L2(P )-norm. Remark. Note that (g)  g .

g 2 = g2dP, 12

Definition 4.4.5. The class G  L2(P ) is called totally bounded if for all  > 0 the number of balls with radius  necessary to cover G is finite.
Theorem 4.4.6. Suppose that G is totally bounded. Then G is a P -Donsker class if and only if n (as process on G) is asymptotically continuous.

Theorem 4.4.7. Suppose that G is a VC-graph class with envelope

satisfying

G = sup |g|
gG
G2dP < . Then G is P -Donsker.

Remark. Thus, a VC class G with square integrable envelope G is asymptotically continuous. In particular, suppose that such a class G is parametrized by  in some parameter space   Rr (say), i.e. G = {g :   }. Let zn() = n(g). Question: do we have that for a (random) sequence n with n  0 (in probability), also
|zn(n) - zn(0)| P 0 ?
Indeed, if (g -g0) converges to zero as  converges to 0, the answer is yes. And so g -g0  0 (mean square convergence) suffices for a yes answer.

5. M-estimators.
5.1. What is an M-estimator? Let X1, . . . , Xn, . . . be i.i.d. copies of a random variable X with values in X and with distribution P .

Let  be a parameter space (a subset of some metric space) and let  : X  R,
be some loss function. We estimate the unknown parameter

0 = arg min


 dP,

by the M-estimator

^n

=

arg

min


 dPn .

Here, we assume that 0 exists and is unique and that ^n exists.

Examples.

13

(i) Location estimators. X = R,  = R, and

(i.a) (x) = (x - )2 (estimating the mean),

(i.b) (x) = |x - | (estimating the median).

(ii) Maximum likelihood. Let {p :   } be a family of densities w.r.t. a -finite dominating measure , and
 = - log p.
If dP/d = p0, 0  , then indeed 0 is a minimizer of dP ,   .

(ii.a) Poisson distribution:

p (x)

=

e

x x!

,

x  {1, 2, . . .},

 > 0.

(ii.b) Logistic distribution:

p(x) =

(1

e-x + e-x)2

,

x  R,

  R.

5.2. Consistency and the law of large numbers. Define for   ,
() = dP,
and n() = dPn.

We first present an easy proposition with a too stringent condition (·).
Proposition 5.2.1. Suppose that   () is continuous. Assume moreover that (·) sup |n() - ()|  0, a.s.,

i.e., that { :   } is a GC class. Then ^n  0 a.s.
The assumption (·) is hardly ever met, because it is close to requiring compactness of . We give a lemma, which replaces (·) by a convexity assumption. This works out well when  is finite dimensional.
Lemma 5.2.2. Suppose that  is a convex subset of Rr, and that   ,    is convex. Then ^n  0, a.s.

14

5.3. Asymptotic normality of M-estimators. In this section, we assume that we already showed that ^n is consistent, and that 0 is an interior point of   Rr.

Definition 5.3.1. The (sequence of) estimator(s) ^n of 0 is called asymptotically linear if we

may write

n(^n

-

0)

=

 n

ldPn + oP(1),

where

 l1  l =  ...  : X  Rr,

lr

satisfies ldP = 0 and lk2dP < , k = 1, . . . , r. The function l is then called the influence function. For the case r = 1, we call 2 = l2dP the asymptotic variance.

Definition 5.3.2. Let ^n,1 and ^n,2 be two asymptotically linear estimators of 0, with asymptotic

variances 12 and 22 respectively. Then

e1,2

=

22 12

is called the asymptotic relative efficiency (of ^n,1 as compared to ^n,2).

5.4. Conditions a, b and c for asymptotic normality. We start with three conditions a, b and c, which are easier to check but more stringent. We later relax them to conditions A, B and C.

Condition a. There exists an x, with derivative

> 0 such that    is differentiable for all | - 0| <

(x) =

 

 (x),

x  X.

and all

Condition b. We have as   0, ( - 0)dP = V ( - 0) + o(1)| - 0|,
where V is a positive definite matrix.

Condition c. There exists an > 0 such that the class

{ : | - 0| < }

has envelope   L2(P ) and is Donsker. Moreover,

lim
0

 - 0

= 0.

15

Lemma 5.4.1. Suppose conditions a, b and c. Then ^n is asymptotically linear with influence
function l = -V -10 ,

so n(^n - 0) L N (0, V -1J V -1),

where

J = 0 T0 dP.

Example: Huber estimator. Let X = R,  = R. The Huber estimator corresponds to the loss function
(x) = (x - ),
with (x) = x2l{|x - |  k} + (2k|x| - k2)l{|x| > k}, x  R.
Here, 0 < k <  is some fixed constant, chosen by the statistician. We will now verify a, b and c.
a) 2k if x -  = k
(x) = -2(x - ) if |x - |  k . -2k if x -   k

b) We have

d d

dP = 2(F (k + ) - F (-k + )),

where F (t) = P (X  t), t  R is the distribution function. So

V = 2(F (k + 0) - F (-k + 0)).

c) Clearly  :   R is a VC class, with envelope   2k.

So the Huber estimator ^n is asymptotically linear, with influence function

 

-

F

k (k+0)-F

(-k+0

)

l(x)

=



x-0 F (k+0)-F (-k+0)
k

F (k+0)-F (-k+0)

if x - 0  -k if |x - 0|  k . if x - 0 > k

The asymptotic variance is

2

=

k2F (-k

+

0)

=

k+0 -k+0

(x

-

0

)2

dF

(x)

+

k2(1

(F (k + 0) - F (k + 0))2

-

F (k

+

0)) .

5.5. Asymptotics for the median. The median (see Example (i.b)) can be regarded as the limiting case of a Huber estimator, with k  0. However, the loss function (x) = |x - | is not
16

differentiable, i.e., does not satisfy condition a. We give here a direct evaluation of the median. This serves as a preparation to relaxing a, b and c, to A, B and C.

Let X  R have distribution F , and let Fn be the empirical distribution. The population median 0 is a solution of the equation
F (0) = 0.

We assume this solution exists and also that F has positive density f in a neighborhood of 0. We consider now for simplicity only even sample sizes n and let the sample median ^n be any solution
of Fn(^n) = 0.

Then we get

0 = Fn(^n) - F (0)

= Fn(^n) - F (^n) + F (^n) - F (0)

= 1n Wn(^n) + F (^n) - F (0) ,

where

Wn

=

 n(Fn -F )

is

the

empirical

process.

Since

F

is

continuous

at

0,

and

^n



0,

we

have

by the asymptotic continuity of the empirical process (Section 4.3), that Wn(^n) = Wn(0) + oP(1).

We thus arrive at

 0 = Wn(0) + n

F (^n) - F (0)

+ oP(1)

=

Wn(0)

+

 n[f (0)

+

o(1)][^n

-

0].

In other words,

n(^n

-

0)

=

-

Wn(0) f (0)

+

oP(1).

So the influence function is

l(x) =

-

f

1 (0)

+

f

1 (0)

if x  0 if x > 0

,

and the asymptotic variance is

2

=

4f

1 (0

)2

.

We can now compare median and mean. It is easily seen that the asymptotic relative efficiency of the mean as compared to the median is

e1,2

=

1 402f (0)2

,

where 02 = var(X). So e1,2 = /2 for the normal distribution, and e1,2 = 1/2 for the double exponential (Laplace) distribution. The density of the double exponential distribution is

f (x) =



1 202

exp

-

2|x - 0| 0

, x  R.

17

Exercise. Suppose X has the logistic distribution with location parameter  (see Example (ii.b)). Show that the maximum likelihood estimator has asymptotic variance equal to 3, and the median has asymptotic variance equal to 4. Hence, the asymptotic relative efficiency of the maximum likelihood estimator as compared to the median is 4/3.

5.6. Conditions A, B and C for asymptotic normality. In this section, we again assume that we already showed that ^n is consistent, and that 0 is an interior point of   Rr. We are
now going to relax the condition of differentiability of .

Condition A. (Differentiability in quadratic mean.) There exists a function 0 : X  Rr, with components in L2(P ), such that

lim
0

 - 0 - ( - 0)T 0 | - 0|

= 0.

Condition B. We have as   0,

()

-

(0)

=

1 2

(

-

0)T V

(

-

0)

+

o(1)|

-

0|2,

with V a positive definite matrix.

Condition C. Define for  = 0,

g

=

( |

- -

0 0|

)

.

Suppose that for some > 0, the class {g : 0 < | - 0| < } has envelope G  L2(P ) and that it is a Donsker class.

Lemma 5.6.1. Suppose conditions A, B and C are met. Then ^n has influence function l = -V -10,

and so where J = 00T dP .

n(^n - 0) L N (0, V -1J V -1),

5.7. Exercise. Let (Xi, Yi), i = 1, . . . , n, . . . be i.i.d. copies of (X, Y ), where X  Rd and Y  R.

Suppose that the conditional distribution of Y given X = x has median m(x) = 0+1x1+. . . dxd,

with

 0   =  ...   Rd+1.

d

Assume moreover that given X = x, the random variable Y - m(x) has a density f not depending

on x, with f positive in a neighborhood of zero. Suppose moreover that

=E

1 X

X XXT

18

exists. Let

^n

=

arg min
aRd+1

1 n

n i=1

|Yi

-

a0

-

a1Xi,1

-

...

-

adXi,d| ,

be the least absolute deviations (LAD) estimator. Show that

 n(^n

-

)

L

N

0,

4f

1 2(0)

-1

,

by verifying conditions A, B and C.

19

Part 2 Empirical processes and regularization of M-estimators
1. A classical approach.

Yi = f0(xi) + i, i = 1, . . . , n.
· 1, . . . , n independent N (0, 2). · xi = i/n, i = 1, . . . , n. · f0 : [0, 1]  R unknown function. · Yi  R observations.

"Classical" estimator:

f^n = arg min
f

1 n

n

|Yi - f (xi)|2 + n2

i=1

1
|f (x)|2dx
0

.

Continuous version: f^ = arg min
f

11
|y(x) - f (x)|2dx + 2 |f (x)|2dx .
00

Lemma 1.1. Solution:

f^(x) = C cosh( x ) + 1 x y(u) sinh( u - x )du,

  0



where with

C = Y (1) -

11

1-u



Y (u) sinh(
0



)du

/

sinh(

1 

),

x
Y (x) = y(u)du.
0

Choice of regularization parameter ? E.g.,

f^n = arg min min
f

1 n

n

|Yi - f (xi)|2 +

i=1

2

1
|f (x)|2dx +

c

0 n

.

20

Here is the a MATLAB script for calculating the solution given in Lemma 1.1:
function smooth = smooth1(data,lambda) % SMOOTH1 requires an input vector DATA and a regularization parameter LAMBDA % SMOOTH1 calculates the smoothed version of the input vector DATA % it uses a penalty on the squared L2 norm of the derivative of the function % n=the number of observations % the output FIT is the squared (normalized) L2 norm of the difference between % DATA and fitted function
lambda sigma=1/lambda;
n=size(data)*[1,1]'-1
dt = 1/(n-1); x = [0:dt:1];
plot(x, data, 'r'); hold on;
size_x = size(x); temp = zeros(size_x(2), 1); temp2 = 0;
DATA = 0; for j = 1:size_x(2)
DATA = DATA + data(j) * dt; end;
C = 0; for j = 1:size_x(2)
C = C + data(j) * (cosh(sigma * (-1 + j * dt))); end; C = (-DATA * (0) + C * dt) / sinh(sigma);
for i = 1:size_x(2) temp2 = 0; for j = 1:i temp2 = temp2 + data(j) * sinh(sigma * (j - i) * dt); end; temp(i) = C * sigma * cosh(sigma * i * dt) + sigma * temp2 * dt;
end;
plot(x, temp, 'g'); fit=(data-temp')*(data'-temp)/n
hold off
21

0

-0.02

-0.04

-0.06

-0.08

-0.1

-0.12

-0.14

-0.16 0

10 20 30 40 50 60 70 80 90 100

True f

0.02 0
-0.02 -0.04 -0.06 -0.08
-0.1 -0.12 -0.14 -0.16 -0.18
0

10 20 30 40 50 60 70 80 90 100

Noise added, noise level = 0.01

22

0.02 0
-0.02 -0.04 -0.06 -0.08
-0.1 -0.12 -0.14 -0.16 -0.18
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Denoised, lambda=0.2 Fit=9.0531e-04

0.02 0
-0.02 -0.04 -0.06 -0.08
-0.1 -0.12 -0.14 -0.16 -0.18
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Denoised, lambda=0.1 Fit=3.4322e-04

23

0.02 0
-0.02 -0.04 -0.06 -0.08
-0.1 -0.12 -0.14 -0.16 -0.18
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Denoised, lambda=0.1 Error=2.8119e-04

0.02 0
-0.02 -0.04 -0.06 -0.08
-0.1 -0.12 -0.14 -0.16 -0.18
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Denoised, lambda=0.05 Error=7.8683e-05

24

2. The sequence space model.

Yi = f0(xi) + i, i = 1, . . . , n.

with 1, . . . , n independent N (0, 2), and xi  X , Yi  R, i = 1, . . . , n.

Let

1 Qn = n

n

Xi .

i=1

Let 1, . . . , n be an orthonormal basis of L2(Qn) (j : X  R, j = 1, . . . , n). Define

Y~j

=

1 n

n

Yij(xi), j = 1, . . . , n,

i=1

~j

=

1 n

n

ij(xi), j = 1, . . . , n,

i=1

j

=

1 n

n

f0(xi)j(xi), j = 1, . . . , n.

i=1

Then

Y~j = j + ~j, j = 1, . . . , n,

with

~1, . . . , ~n

independent

N (0,

2 n

).

25

· Fourier · Wavelets · Trigonometric · Polynomial · ....

3. Sparse signals.

Yj = j + j, j = 1, . . . , n,

with

1, . . . ,

n

independent

N (0,

2 n

),

Define

n



2 n

=

|j|2,   Rn.

j=1

Definition 3.1. Let 0  r < 2. The signal  is called r-sparse if
n
|j|r  1.
j=1

Lemma 3.2. Suppose  is r-sparse. Then #{|j| > }  -r.

Lemma 3.3. Let If  is r-sparse we have

j =

j 0

if |j| >  if |j|  

.

 - 

2 n



2-r .

26

A sparse signal......
27

4. A concentration inequality.

Lemma 4.1. Let 1, . . . , n be independent N (0, 1). Then

max |
1jn

j|



6 log n, a.s.,

for n sufficiently large.

Proof. We have for all a > 0, and  > 0, by Chebyshev's inequality

P(

j

>

a)



E exp[ j] exp[a]

Take  = a to arrive at So
 Take a = 6 log n to get

=

exp[

1 2

2

-

a].

P(

j

>

a)



exp[-

1 2

a2

].

P

max |
1jn

j|

>

a

 2n exp[- 1 a2]. 2

P

max |
1jn

j|

>

6 log n



2n

exp[-3

log

n]

=

2

exp[-2

log

n]

=

2 n2 .

Since the result follows.

n

2 n2

<

,

Remark. The result can be improved to

for n sufficiently large.

max |
1jn

j|



2 log n, a.s.,

5. Hard and soft thresholding.

with

1, . . . ,

n

i.i.d.

N (0,

2 n

).

Yj = j + j, j = 1, . . . , n,

Let   0 be some threshold (= regularization parameter).

Hard thresholding:

^j(hard) =

Yj 0

if |Yj| >  if |Yj|  

,

j = 1, . . . , n.

28

Soft thresholding:



 Yj - 

^j (soft)

=



Yj 0

+



if Yj >  if Yj < - , j = 1, . . . , n. if |Yj|  

Lemma 5.1. ^(hard) minimizes
n
(Yj - j)2 + 2#{j = 0}.
j=1

Lemma 5.2. ^(soft) minimizes

nn
(Yj - j)2 + 2 |j|.
j=1 j=1

We refer to #{j = 0} =

n j=1

|j |0

as

the

0-penalty and

n j=1

|j |

as

the

1-penalty.

The estimators ^(hard) and ^(soft) have similar oracle properties (see Section 6 for the explanation of this terminology). We will prove this for the soft thresholding estimator.

Define

N () = #{j = 0}.

Lemma 5.5. Let ^ = ^(soft),  = n = 2 log n/n. On the set {max1jn | j|  n}, we have

for all 0 <   1

^ - 

2 n

 (1 + 2) min


-

2 n

+

64 2

n2 N

()

.

Proof. Let  be arbitrary. Write with

n
pen() = 2n |j|
j=1
= pen1() + pen2(),
pen1() = 2n |j|,
j=0
pen1() = 2n |j|.
J =0

29

Then

n

^ - 

2 n

2

j(^j - j) + pen() - pen(^) +

 - 

2 n

j=1

n

 2n

|^j - j| + pen() - pen(^) +

 - 

2 n

j=1

 pen1(^ - ) + pen2(^ - ) + pen() - pen(^) +

 - 

2 n

 4pen1(^ - ) +

 - 

2 n

 4n

N () ^ -  n +

 - 

2 n

 4n

N () ^ -  n + 4n

N ()  -  n +

 - 

2 n

= I + II + III.

If (I + II)  III we now have

^ - 

2 n

 (1 + )III

= (1 + )

 - 

n2 .

If (I + II)  III and I  II we have

III



1 

(I

+ II)



2 

I

I,

which implies

 - 

2 n



4 2

16n2

N

(

).

But then

I

+

II

+

III



21

+ 

II

=

21

+ 

 4n

N ()  -  n



4(

1

+ 



)2162nN

().

If (I + II)  III and I  II we get

which gives

I

+

II

+

III



1 

(I

+

II)



2 

I

,

^ - 

n



2 

4n

N ().

6. The oracle.

with

1, . . . ,

n

i.i.d.

N (0,

2 n

).

Let J  {1, . . . , n}. Define

Yj = j + j, j = 1, . . . , n,

^j(J ) =

Yj 0

if j  J if j / J

.

30

Lemma 6.1. We have

E

^(J ) - 

2 n

=

j2

+

|J |2 n

.

j/J

Oracle oracle:





oracle - 

2 n

=

J

min
{1,...,n}



j2

+

|J |2 n

 

.

j/J

7. Discretization.

Yi = f0(xi) + i, i = 1, . . . , n.

with 1, . . . , n independent N (0, 1), and xi  X , Yi  R, i = 1, . . . , n.

Let F be a finite collection of functions, and

f^ =

arg min
f F

1 n

n i=1

|Yi

-

f0(xi)|2.

Define

f  - f0 n = min f - f0 n.
f F

Lemma 7.1. We have for all  > 0,

P

f^ - f0

2 n



2

f  - f0

2 n

+

5762

+

576

log n

|F |

 exp[-n2].

Proof. We have

f^ - f0

2 n



2n n i=1

i(f^(xi) - f (xi)) +

f  - f0

2 n

,

and

1

P

max

n

f F ,

f -f 

2n2882+

288

log n

|F

|

n i=1

if (xi) - f (xi)

f - f

2 n



1 12

 exp

log

|F |

-

n 288

[2882

+

288

log n

|F| ]

 exp[-n2].

We also have, if

f - f0

2 n



2

f  - f0

2 n

+

5762

+

576

log n

|F| ,

31

then

f - f

2 n



2882

+

288

log n

|F| .

Let {Fm}mM be a collection of nested finite models, and let F = mMFm. Define

pen(f ) = min c log |Fm| .

m: f Fm

n

Let

f^ = arg min
f F

1 n

n

|Yi - f (xi)|2 + pen(f )

,

i=1

and

f  = arg min
f F

f - f0

2 n

+

pen(f

)

.

Lemma 7.2. We have

E[

f^ - f0

2 n

+

pen(f^)]



2[

f  - f0

2 n

+

pen(f )]

+

c n

.

8. General penalties.

Yi = f0(xi) + i, i = 1, . . . , n.

with 1, . . . , n independent N (0, 1), and xi  X , Yi  R, i = 1, . . . , n.

Let

f^ = arg min
f F

1 n

n i=1

|Yi

-

f (xi)|2

+

pen(f )

,

and

f  = arg min
f F

f - f0 2 + pen(f ) .

Definition 8.1. The -entropy H(, F) is the logarithm of the minimum number of balls with radius  necessary to cover F.

Lemma 8.2. For

nn2  c

n

H1/2(u, {

f - f

2 n

+

pen(f

)



n2 })du



n

,

0

we have

E[

f^ - f0

2 n

+

pen(f^)]



2[|f



-

f0

2 n

+

pen(f )

+

n2 ]

+

c n

.

32

9. Application to the classical penalty.

Yi = f0(xi) + i, i = 1, . . . , n. with 1, . . . , n independent N (0, 1). and xi = i/n, i = 1, . . . , n.

Let

pen(f ) = min


2

1 0

|f

(x)|2dx

+

c n

.

We will apply Lemma 8.1.

10. Robust regression. Let Yi depend on some covariable xi, i = 1, . . . , n. Assume Y1, . . . , Yn are independent. Let  : R  R be a convex loss function satisfying the Lipschitz condition
|(x) - (y)|  |x - y|, x, y  R.

Examples. - (x) = |x|, x  R, - (x) = |x|l{x < 0} + (1 - )|x|l{x > 0}, x  R. Here 0 <  < 1 is fixed.

We consider the estimator



f^n

=

arg f =Pmjn=in1 j j

1 n

n i=1

(Yi

-

f (xi))

+

n

n j=1

 |j| .

We will derive an oracle inequality for f^n, using the same arguments as in Lemma 5.5.

11. Density estimation. Let X1, . . . , Xn be i.i.d. with distribution P on X , and suppose the denisty p0 = dP/d w.r.t. some -finite measure  exisits. Let

 = {f  L2(P ) : f dP = 0}.

Define

b(f ) = log ef d,

33

and F = {f   : b(f ) < }. We assume that

f0 = log p0 - log p0dP  F .

Consider now an orthonormal system {j}  F in L2(). Let pf = exp[f - b(f )], f  F , and consider the (penalized maximum likelihood) estimator





f^n

=

arg

mPax
f = j j j



  log pf dPn - n |j| .
j

We will derive an oracle inequality for f^n, using the same arguments as in Lemma 5.5.

12. Classification. Let (X1, Y1), . . . , (Xn, Yn) be i.i.d. copies of (X, Y ), where X  X is an instance and Y  {0, 1} is a label. A classifier is a subset G  X . Using the classifier G, we predict
Y = 1 whenever X  G. Bayes rule is to use the classifier

G0 = {x : (x) > 1/2},

with

(x) = E(Y |X = x), x  X .

This classifier minimizes the prediction error

R(G) = P (Y lG(X) < 0) = E|Y - l.G(X)|.

Consider the excess risk

R(G) - R(G0) =

|2 - 1|dQ.

G G0

Here, Q denotes the distribution of X.

The empirical risk is

Rn(G)

=

1 n

n

|Yi - lG(Xi)|.

i=1

Let HB(, G, Q) denote the -entropy with bracketing, for the measure Q, of a class G of subsets of X.

Theorem 12.1. (Mammen and Tsybakov(1999)) Let

G^n

=

arg

min
GG

Rn(G),

where G is a class of subsets of X , satisfying for some constants c > 0 and 0 <  < 1,

HB(, G, Q)  c-,  > 0.

34

Moreover, suppose that for some   1 and 0 > 0, R(G) - R(G0)  Q(G G0)/0, G  X .

Let Then

R(G) = min R(G).
GG

E(R(G^n) - R(G0))  Const.

R(G)

-

R(G0)

+

n-

 2+-1

.

In Tsybakov and van de Geer (2003) one can find an oracle inequality for a penalized empirical risk estimator, which shows adaptivity to both the margin parameter  as well as to the complexity parameter . The method is based on the same arguments as in Lemma 5.5.

35

13. Some references for Part 2.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. Proceedings 2nd International Symposium on Information Theory, P.N. Petrov and F. Csaki, Eds., Akademia Kiado, Budapest, 267-281
Barron, A., Birg´e, L. and Massart, P. (1999). Risk bounds for model selection via penalization. Prob. Theory Rel. Fields 113 301-413.
Devroye, L. , Gyo¨rfi, L. and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer, New York, Berlin, Heidelberg.
Donoho, D.L., and Johnstone, I.M. (1994). Ideal spatial adaptation via wavelet shrinkage. Biometrika 81, 425-455
Donoho, D.L. (1995). De-noising via soft-thresholding. IEEE Transactions in Information Theory 41, 613-627
Edmunds, E., and Triebel, H. (1992). Entropy numbers and approximation numbers in function spaces. II. Proceedings of the London Mathematical Society (3) 64, 153-169
H¨ardle, W., Kerkyacharian, G., Picard, D. and Tsybakov, A. (1998). Wavelets, Approximation and Statistical Applications. Lecture Notes in Statistics, vol. 129. Springer, New York, Berlin, Heidelberg.
Hastie, T., Tibshirani, R.,and Friedman, J. (2001). The Elements of Statistical Learning. Data Mining, Inference and Prediction. Springer, New York
Koenker, R., and Bassett Jr. G. (1978). Regression quantiles. Econometrica 46, 33-50
Koenker, R., Ng, P.T. and Portnoy, S.L. (1992). Nonparametric estimation of conditional quantile functions. L1 Statistical Analysis and Related Methods, Ed. Y. Dodge, Elsevier, Amsterdam, 217-229
Koenker, R., Ng, P.T. and Portnoy, S.L. (1994). Quantile smoothing splines. Biometrika 81, 673-680
Koltchinskii, V. and Panchenko, D. (2002). Empirical margin distributions and bounding the generalization error of combined classifiers. Ann. Statist. 30 1-50.
36

Korostelev, A. P. and Tsybakov, A. B. (1993). Minimax Theory of Image Reconstruction. Lecture Notes in Statistics 82, Springer, New York, Berlin, Heidelberg.
Massart, P. (2000). Some applications of concentration inequalities to statistics. Ann. Fac. Sci. Toulouse 9, 245-303
Loubes, J.-M., and van de Geer S. (2002). Adaptive estimation, using soft thresholding type penalties. Statistica Neerlandica 56, 453-478
Ledoux, M., and Talagrand, M. (1991). Probability in Banach Spaces, Isoperimetry and Processes. Springer, Berlin
Mammen, E. and Tsybakov, A. B. (1999). Smooth discrimination analysis. Ann. Statist. 27 1808 - 1829.
Pinkus, A. (1985) n-widths in Approximation Theory. Springer, New York
Portnoy, S. (1997). Local asymptotics for quantile smoothing splines. Ann. Statist. 25, 414-434
Portnoy, S., and Koenker, R. (1997). The Guassian hare and the Laplacian tortoise: computability of squared error versus absolute-error estimators, with discussion. Stat. Science 12, 279-300
Scho¨lkopf, B. and Smola, A. (2002). Learning with Kernels, MIT Press, Cambridge.
Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6, 461-464
Tibshirani, R. (1996). Regression analysis and selection via the LASSO. Journal Royal Statist. Soc. B 58, 267-288
Tsybakov, A.B. and van de Geer, S. (2003). Square root penalty: adaptation to the margin in classification, and in edge estimation. Pr´epublication PMA-820, Laboratoire de Probabilit´es et Mod`eles Al´eatoires, Universit´e Paris VII.
van de Geer, S. (2000). Empirical Processes in M-Estimation. Cambridge University Press
van de Geer, S. (2001). Least squares estimation with complexity penalties. Mathematical Methods of Statistics 10, 355-374.
van de Geer, S. (2002). M-estimation using penalties or sieves. J. Statist. Planning Inf. 108, 55-69
37

van de Geer, S. (2003). Adaptive quantile regression. Techn. Report MI 2003-05, University of Leiden. van der Vaart, A.W., and Wellner, J.A. (1996). Weak Convergence and Empirical Processes, with Applications to Statistics. Springer, New York Vapnik, V. N. (1998). Statistical Learning Theory. Wiley, New York.
38

