Efficient Reinforcement Learning in Factored MDPs

Michael Kearns AT&T Labs
mkearns@research.att.com

Daphne Koller Stanford University koller@cs.stanford.edu

Abstract
We present a provably efficient and near-optimal algorithm for reinforcement learning in Markov decision processes (MDPs) whose transition model can be factored as a dynamic Bayesian network (DBN). Our algorithm generalizes the recent E3 algorithm of Kearns and Singh, and assumes that we are given both an algorithm for approximate planning, and the graphical structure (but not the parameters) of the DBN. Unlike the original E3 algorithm, our new algorithm exploits the DBN structure to achieve a running time that scales polynomially in the number of parameters of the DBN, which may be exponentially smaller than the number of global states.
1 Introduction
Kearns and Singh (1998) recently presented a new algorithm for reinforcement learning in Markov decision processes (MDPs). Their E3 algorithm (for Explicit Explore or Exploit) achieves near-optimal performance in a running time and a number of actions which are polynomial in the num-
ber of states and a parameter T , which is the horizon time
in the case of discounted return, and the mixing time of the optimal policy in the case of infinite-horizon average return. The E3 algorithm makes no assumptions on the structure of the unknown MDP, and the resulting polynomial dependence on the number of states makes E3 impractical in the case of very large MDPs. In particular, it cannot be easily applied to MDPs in which the transition probabilities are represented in the factored form of a dynamic Bayesian network (DBN). MDPs with very large state spaces, and such DBN-MDPs in particular, are becoming increasingly important as reinforcement learning methods are applied to problems of growing difficulty [Boutilier et al., 1999].
In this paper, we extend the E3 algorithm to the case of DBN-MDPs. The original E3 algorithm relies on the ability to find optimal strategies in a given MDP -- that is, to perform planning. This ability is readily provided by algorithms such as value iteration in the case of small state spaces. While the general planning problem is intractable in large MDPs, significant progress has been made recently on approximate solution algorithms for both DBN-MDPs in

particular [Boutilier et al., 1999], and for large state spaces in general [Kearns et al., 1999; Koller and Parr, 1999]. Our new DBN-E3 algorithm therefore assumes the existence of a
procedure for finding approximately optimal policies in any
given DBN-MDP. Our algorithm also assumes that the quali-
tative structure of the transition model is known, i.e., the un-
derlying graphical structure of the DBN. This assumption is
often reasonable, as the qualitative properties of a domain are
often understood. Using the planning procedure as a subroutine, DBN-E3 ex-
plores the state space, learning the parameters it considers
relevant. It achieves near-optimal performance in a running
time and a number of actions that are polynomial in T and the
number of parameters in the DBN-MDP, which in general is
exponentially smaller than the number of global states. We
further examine conditions under which the mixing time T of
a policy in a DBN-MDP is polynomial in the number of parameters of the DBN-MDP. The "anytime" nature of DBN-E3
allows it to compete with such policies in total running time
that is bounded by a polynomial in the number of parameters.

2 Preliminaries

We begin by introducing some of the basic concepts of MDPs

and factored MDPs. A Markov Decision Process (MDP) is

defined as a tuple S; A; R; P  where: S is a set of states; A

issucahsetht aotfRactsionres;pRresiesnatsretwhearrdewfuanrdctoiobntaRine:dSb!7y the0;aRgemnatxin,

sPtatse0

sj

1;
s;

P a

is a transition represents the

model P : S  A !7 S,
probability of landing in

such state

sth0 aift

the agent takes action a in state s.

Most simply, MDPs are described explicitly, by writing

down a set of transition matrices and reward vectors -- one
for each action a. However, this approach is impractical

Xfor describing complex processes. Here, the set of states
is typically described via a set of random variables =
iwdfnoXesmt1ua;ansi:etn:iaV:Vt;aiaoXllnnYXygi,awst.oshIingdenergenseoeantaeevcrahatlhlu,Xeefsoixertta2akosefVesptaooolnsfXsvvibaarllfeiuaoeibrnsleseivtnsaeYnsryotimaXteioX2finn,sYiattnoe;

1A reward function is sometimes associated with (state,action) pairs rather than with states. Our assumption that the reward depends only on the state is made purely to simplify the presentation; it has no effect on our results.

Y. A state in this MDP is an assignment x 2 ValX; the

total number of states is therefore exponentially large in the

number of variables. Thus, it is impractical to represent the

transition model explicitly using transition matrices.

The framework of dynamic Bayesian networks (DBNs) al-

lows us to describe a certain important class of such MDPs in

a compact way. Processes whose state is described via a set of

variables typically exhibit a weak form of decoupling -- not

all of the variables at time t directly influence the transition

of a variable Xi from time t to time
simple robotics domain, the location

t + 1.
of the

For example, in
robot at time t +

a
1

may depend on its position, velocity, and orientation at time
t, but not on what it is carrying, or on the amount of paper

in the printer. DBNs are designed to represent such processes

x xtXcnroeaixmLntseapitttatiimcatohtnleey2.mscteouApdrr.eeblnTePthaetinmt0raeacjntasinoitd;nioa. nX.Wmi0 LeodedefitenrloXstfteiowrdthaaencenttoivotteaonrtiashapbewelecviialfalryticaottbhhnleee-

sist of two parts -- an underlying transition graph associ-
ated with a, and parameters associated with that graph. The

gnftrrXoaadnp10es;hist:ia:oar:rne;eXdgfirn0raXegpc;1ht;en:dios:tf:era;oXtmh2an-ltn;aXowydee10re;sa:dr:iienr:e;afcXsXtsenu0d1mg;.a:icn:yg:Ac; ltXlihlcanteggdtrhgtaoeeprshneoiwandrehestohnsiinoes pseidatgrioeennstsbgeroatfwpXheei0fnoinrvatahrieaspbgelreacspifihwebsityhtihPneaaaqtuiXamli0ieta.stlIiinvceteu.intWaivtueerlyed,eotnhfoetpetrroathnbe--

abilistic dependencies in a single time step -- namely, the
x x x Q u utvntiPthiaeyverweti,saa0esebbtejalltteecitnshi(;ngCaingniPonoTdiPsfe)atXPoXhafeaiiX0ntdhXidies0epei0a.vfiesjannsPrTdoeioaasdcbiaotamlonteXeaslbdyki0eienwonP.tihtTiahtihPashaeeadXccteoxruip0an0irenrd.jnesiindttiteioionsnn,ceawetpltirhpqnoerugboraaebonbaftibilittihiatilyes-We also need to provide a compact representation of the

reward function. As in the transition model, explicitly spec-

ifying a reward for each of the exponentially many states is

impractical. Again, we use the idea of factoring the repre-

sentation of the reward function into a set of localized re-

ward functions, each of which only depends on a small set of

variables. In our robot example, our reward might be com-

posed of several subrewards: for example, one associated

with location (for getting too close to a wall), one associated

x C C CfAfRMwuXbiinotuh1ctrase;tiikt:onhpe:ngers:e;pnfRcXoriosiirtnenatiltgthsyei,eo,rasnlspsuet,satcaworhtRtcueitosahwbft(aeeftitdholRalreuwsilsseeiitetsttathtoaRienfafigvufeucnpclncautctposittoetroieorncrdnorfesuorrnornfRoemvsto1epa;uVor:ttiah)n:a,led:ba;ilvnneRadsgilkuts;etoooeitaohIRcnaiht...

x x P xThe is

reward function associated
then defined to be R  =

witkih=1thRe iD BN2-M0D; PRamtaax

state .

The following definitions for finite-length paths in MDPs

will be of repeated technical use in the analysis. Let M be

a Markov decision process, and let  be a policy in M . A

ttTiroa-ivpnseasdr)tsehoenfidonMtieMnd:MPpisM=uapspxoe1nq=;us:et:an:rckTt;ei=xnp1TgPo;inxfxTTstk+a++1te1.1xTjs1hxtaekatn;epsdro(ebtxxhakeabctiuli.istti,ynTgthtparoat nlpisciiys-

There are three standard notions of the expected return en-

joyed by a policy in an MDP: the asymptotic discounted re-

turn, the asymptotic average return, and the finite-time aver-

age return. Like the original E3 algorithm, our new general-

ization will apply to all three cases, and to convey the main

ideas it suffices for the most part to concentrate on the finite-

time average return. This is because our finite-time average

return either

result can be applied to
the horizon time 1=1

,the

asymptotic returns through
 for the discounted case,

or the mixing time of the optimal policy in the average case.

(We examine the properties of mixing times in a DBN-MDP

in Section 5.)
Let M be a Markov decision process, let  be a policy in M , and let p be a T -path in M . The average return along p in M is

UM p = 1=T Rx1 +    + RxT+1:

PmoTUTphM-atpeixamxtThaf;slU-TsTpMte-ispn=txeM;p(eTaxvptgphee.raPcattMegsdeta)prretUatauvMtrenrxapf.greoFwmurrhextehtrueiernrnmtMhoefrrebos,ymuwmUeMsitdsaetxoefiv;nTeexrtah=ilesl

poAlicnyimptohrattanact hpireovbelsemoptiinmMalDrePtusrinsipnlaangnivinegn:MfiDndP.inIng

the our

case, we are interested in achieving the optimal T -step av-

erage return. The complexity of all exact MDP planning al-

gorithms depends polynomially on the number of states; this

property renders all of these algorithms impractical for DBN-

MDPs, where the number of states grows exponentially in

the size of the representation. However, there has been re-

cent progress on algorithms for approximately solving MDPs

with large state spaces [Kearns et al., 1999], particularly on

ones represented in a factored way as an MDP [Boutilier et

al., 1999; Koller and Parr, 1999]. The focus of our work is

on the reinforcement learning task, so we simply assume that

we have access to a "black box" that performs approximate

planning for a DBN-MDP.

Definition 2.1:A -approximation T -step planning algo-

rithm for a DBN-MDP is one that, given a DBN-MDP

MUM,

pxr;oTduces

a
1

,(comUpaMctlxy;rTep.resented)

policy



such

that

We will charge our learning algorithm a single step of computation for each call to the assumed approximate planning algorithm. One way of thinking about our result is as a reduction of the problem of efficient learning in DBN-MDPs to the problem of efficient planning in DBN-MDPs.
Our goal is to perform model-based reinforcement learning. Thus, we wish to learn an approximate model from experience, and then exploit it (or explore it) by planning given the approximate model. In this paper, we focus on the problem of learning the model parameters (the CPTs), assuming that the model structure (the transition graphs) is given to us. It is therefore useful to consider the set of parameters that we wish to estimate. As we assumed that the rewards are deterministic, we can focus on the probabilistic parameters. (Our results easily extend to the case of stochastic rewards.) We define a transition component of the DBN-MDP to be a

udistribution PaXi0 uular instantiation

j
to

 for some
the parents

aPcatiaonXai0ainndtshoemtreanpsairttiiocn-

model.
Pmost

Na;iojtVe athlaPt athaeXnui0mjb,ebruotfmtraaynsbietiomnuccohmlopwoneernwtshiesnaat

variable's behavior is identical for several actions.

3 Overview of the Original E3

Since our algorithm for learning in DBN-MDPs will be a di-

rect generalization of the E3 algorithm of Kearns and Singh

-- hereafter abbreviated KS -- we begin with an overview of

that algorithm and its analysis. It is important to bear in mind

that the original algorithm is designed only for the case where
the total number of states N is small, and the algorithm runs in time polynomial in N .

E3 is what is commonly referred to as an indirect or model-

based algorithm: rather than maintaining only a current pol-

icy or value function, the algorithm maintains a model for

the transition probabilities and the rewards for some subset
of the states of the unknown MDP M . Although the algorithm maintains a partial model of M , it may choose to never build a complete model of M , if doing so is not necessary to

achieve high return.

The algorithm starts off by doing balanced wandering: the

algorithm, upon arriving in a state, takes the action it has tried

the fewest times from that state (breaking ties randomly). At

each state it visits, the algorithm maintains the obvious statis-

tics: the reward received at that state, and for each action,

the empirical distribution of next states reached (that is, the

estimated transition probabilities).

A crucial notion is that of a known state -- a state that

the algorithm has visited "so many" times that the transition

probabilities for that state are "very close" to their true val-
ues in M . This definition is carefully balanced so that "so

many" times is still polynomially bounded, yet "very close"

suffices to meet the simulation requirements below. An im-

portant observation is that we cannot do balanced wandering

indefinitely before at least one state becomes known: by the

Pigeonhole Principle, we will soon start to accumulate accu-

rate statistics at some state.

SThe most important construction of the analysis is the
known-state MDP. If is the set of currently known states,

the known-state MDP is simply
Sinduced on by the full MDP

an
M

MDP MS that is naturally
. Briefly, all transitions in

SM between states in transitions in M are

"readrierepcrteesde"rviendMinSMtoS

, while lead to

all other a single

new, absorbing state that intuitively represents all of the un-

known and unvisited states. Although E3 does not have direct

access it does

thoavMeSa,gboyovdiartpuperoofxitmheatdioefinnMi^tiSo.n

of

the

known

states,

The KS analysis hinges on two central technical lemmas.

mTTtshthe-oaseptdteMefir^plertSsorutefrhtinMusariscnn,agoMflolofeorSdadtn.htsyhTaimetphoupuSllasiia,rmcttayiuotoilnfanantMaiMyoc^nctStiuhmLriaaseetcm,ctylMhm:o^esatSeha, altiagostnoiaidsrti,sutithstehemxeefpsuet"elaxkcbpptnleaeoidrscwthtTiesead"s-l

very well.

The second central technical lemma is the "Explore or Ex-
ploit" Lemma. It states that either the optimal (T -step) policy

in M achieves its high return by staying (with high probaSbility) in the set of currently known states, or the optimal Spolicy has significant probability of leaving within T steps.

Most importantly, the algorithm can detect which of these two

is the case; in the first case, it can simulate the behavior of the

tohpetipmaartlipalomlicoydeblyMfi^nSd,ianngdainhitghhe-rseetcuornndexcpasloe,itiattcioann

policy in replicate

the behavior of the optimal policy by finding an exploration

penthiiotnelhgipecarcyroattmihawapltamuyqtoauttdoiiceoklgnlMesy^toSnrnee.aMaT^crhh-Soeu,psstt,htibhmeyeaalplagedroredfrtioiuttirrhomnmnifaniolsgragttbwuhsaeoorranobnefiftxne-tgleiTdnsettaosptteefilapnonsdf-,

or a way to improve the statistics at an unknown or unvisited

state within the next ensures near-optimal

T steps.
return in

KS time

pshoolywnotmhaitalthinisNal.gorithm

4 The DBN-E3 Algorithm

Our goal is to derive a generalization of E3 for DBN-MDPs,

and to prove for it a result analogous to that of KS -- but
with a polynomial dependence not on the number of states N , but on the number of CPT parameters ` in the DBN model.

Our analysis closely mirrors the original, but requires a sig-

nthifie csatrnutcgteunreeroafliazaDtiBonNo-Mf tDheP,SaimmuoldatiifioendLceomnsmtraucthtiaotnexopflMo^itSs

that can be represented as a DBN-MDP, and a number of al-

terations of the details.

Like the original E3 algorithm, DBN-E3 will build a model

of the unknown DBN-MDP on the basis of its experience, but

now the model will be represented in a compact, factorized

x xfo,remx.ecMutoerseapcrtieocnisael,ya, nsdupaprroisveesthinatsotautrealg0.oTrihthismexispienriestnactee

x u u uumwoithnfioeltdlio.mebblWeev--siueoPs^uwneasdailwmxltoai0aeylljusy,op,wdaimhlaletahetrihaenasetlaablesisetnuhteisncemuoauaaupptlnepdtsrsaioPt^Cpeiasdrai.taxhxt0iee0ij;sCePtiitTinoagerfneotthfruiepPedsnaauaotmfeXdobuiei0nrr Recall that a crucial element of the original E3 analysis was

the notion of a known state. In the original analysis, it was ob-
served that if N is the total number of states, then after ON 

experiences some state must become known by the Pigeon-

hole Principle. We cannot hope to use the same logic here,

as we are now in a DBN-MDP with an exponentially large

number of states. Rather, we must "pigeonhole" not on the

number of states, but on the number of parameters required

uttohespCePciTfyetnhteryDPB^aN-xMi0 Dj P.

Towards this
i is known

goal, we if it has

will say that been visited

"enough" times to ensure that, with high probability

jPaxi0 j ui , P^ax0i j uij  :

We now would like to establish that if, for an appropriate choice of , all CPT entries are known, then our approximate DBN-MDP can be used to accurately estimate the expected return of any policy in the true DBN-MDP. This is the desired generalization of the original Simulation Lemma. As in the original analysis, we will eventually apply it to a gener-
alization of the induced MDP MS , in which we deliberately
restrict attention to only the known CPT entries.

4.1 The DBN-MDP Simulation Lemma
Let M and M^ be two DBN-MDPs over the same state space
uwtteahhvpieeetphrtrsyroataxhpmnieosmeisstasairiotembinwoleengavtrrordaaafplnufhMsuesin,txicfoii0tofinrooffgneovXrsra.epir0ey,hvTseshrefeytontrianwcegvteieorsnyaoyfaaPctathainaoadtnXMna^oi,0dai,ensadnXadnwi0fioitnh-r

jPaxi0 j u , P^axi0 j uj 

where Pa j  and P^a j  are the CPTs of M and M^ ,
respectively.

Lemma 4.1: Let M be any DBN-MDP over with ` CPT entries in the transition model,

n state
and let

vMa^ribaeblaens

-approximation of

TUhM^enxf;oTr ajny

policy
:

M, ,

where and for

=
any

O
state

x=,TjU2`MRmxa;xT2,.

x ux u x x x x Qx ufwPiPttooyrhrrosefaoar0nPefcy:jtao(nrSxie;izki0xaieetssjtsccttvhhoaie)tnaieLttsaheeii0esttntausiCsnnsmaPgdfiTaaxols-nlfseayamrsPpaatPaochlatllaiiocnfXayn0ci0ajt.o,arti;nhNniadefoatstr=e.ntaaynttLehsoiaeftitti.ioPatunRsastpCerxscraPaoai0nyTlbsjliaftttbihhaoiicaaln--tt.

probability may actually be quite small itself (exponentially

small in n)
Our first

without goal is

necessarily containing a to show that trajectories

-small
in M

faanctdorM.^

that cross transitions containing a -small CPT factor can be

x"thrown away" without much error. Consider a random tra-
jectory of T steps in M from state following policy . It

x xcan be shown
cross at least

otnheatttrhaenspitrioobnabPility0

tjhat;

such a trajectory
a that contains

will a-

small factor is at most T ` . Essentially, the probability that

at any step, any particular -small transition (CPT factor) will

be taken by any particular variable Xi is at
ple union argument over the CPT entries and

most
the T

. A simtime steps

x xctghaivenedbsieftfhseehreodnwecsneirtjeoUdMbbeoautn; mdT.oTs,thTeUr2eMR^fomrea;,xTt`hej

total contribution to
by these trajectories
+ . We will thus

ignore such trajectories for now.

The key advantage of eliminating -small factors is that

we can convert additive approximation guarantees into mul-
tiplicative ones. Let p be any path of length T . If all the relevant CPT factors are greater than , and we let  = = ,

it can be shown that

1 , TnPM p  P^M p  1 + TnPM p :

In other tions on

words, ignoring paths induced by

-small CPT
 in M and

fMa^ctaorres,qtuhietedsiismtriiblaur-.

From this it follows that, for the upper bound,2

UM^ x; T   1 + TnUM x; T  + T 2Rmax` + 2 : For the choices = p , = O =T 2`Rmax 2 the
lemma is obtained.

2The lower bound argument is entirely symmetric.

Returning to the main development, we can now give a

precise definition of a known CPT entry. It is a simple ap-

tCpilvaiecaxet0iri;orounrioafteCxmchoeesertdnsofOfwbi1oth=unpdr2oslbotoagbs1ihl=iotwyat,thPal^etaapsxrto0i1vji,dueid.thhWaesecaotdhudunis-t

say that this CPT entry is known if its count exceeds the given
bound for the choice = O =T 2nvRmax 2 specified by

the DBN-MDP Simulation Lemma. The DBN-MDP Simula-

otiuornaLppemromxiamsahteowmsodtheal tM^if

all can

CPT entries are be used to find a

known, then near-optimal

policy in the true DBN-MDP M .

u u x uvitwhsihaauNtinctowhkhteniealoltctwiohlneuancatn,rstbewtsayoesCnteaceaktahonixnef0igi;ctdhoaeercinrtatei.iossfspnyTooacwhniuadhwstiie,encdhgitfhCcCewoPnPueTTonabteretnCeantiartnariteixeasasni0P;^saeatxraieptxe.eki0rTnijeohnwfucoisner,
in analogy with the original E3, as long as we are encoun-

tering unknown CPT entries, we can continue taking actions

that increase the quality of our model -- but now rather than

increasing counts on a per-state basis, the DBN-MDP Simu-

lation Lemma shows why it suffices to increase the counts on

a per-CPT entry basis, which is crucial for obtaining the run-

ning time we desire. We can thus show that if we encounter

unknown CPT entries for a number of steps that is polyno-
mial in the total number ` of CPT entries and 1= , there can

no longer be any unknown CPT entries, and we know the true

DBN-MDP well enough to solve for a near-optimal policy.

However, similar to the original algorithm, the real diffi-

culty arises when we are in a state with no unknown CPT

entries, yet there do remain unknown CPT entries elsewhere.

Then we have no guarantee that we can improve our model

at the next step. In the original algorithm, this was solved by
defining the known-state MDP MS , and proving the afore-

mentioned "Explore or Exploit" Lemma. Duplicating this

step for DBN-MDPs will require another new idea.

4.2 The DBN-MDP "Explore or Exploit" Lemma

In our context, when we construct a known-state MDP, we

must satisfy the additional requirement that the known-state

MDP preserve the DBN structure of the original problem, so

that if we have a planning algorithm for DBN-MDPs that ex-

ploits the structure, we can then apply it to the known-state

MDP3. Therefore, we cannot just introduce a new "sink state"
to represent that part of M that is unknown to us; we must

also show how this "sink state" can be represented as a set-

ting of the state variables of a DBN-MDP.

We present a new construction, which extends the idea of

ut"hkantoawntrasntastietiso"ntocotmhepoidneeantoPf a"kXnoi0wjn

transitions".
 is known

We say if all of

its CPT entries are known. The basic idea is that, while it is

impossible to check locally whether a state is known, it is easy

Tto check locally whether a transition component is known. Let be the set of known transition components. We de-
fine the known-transition DBN-MDP MT as follows. The

3Certain approaches to approximate planning in large MDPs do not require any structural assumptions [Kearns et al., 1999], but we anticipate that the most effective DBN-MDP planning algorithms eventually will.

model behaves identically to M as long as only known transi-

svtioaomlnuseeavwrae,rtiwaakbheliench.XAwi0,setshioneotvnroaadrsiuaacbneleuinXntkoi0nttoahwkeenmstoroandneasli.tnieTownheiwstatraankndesenirtiifnoognr

model is defined so that, once a variable takes on the value
w, its value never changes. The reward function is defined so

that, once at least one variable takes on the wandering value,

the total reward is nonpositive. These two properties give us

the same overall behavior that KS got by making a sink state

for the set of unknown states.

TDefinition 4.2:Let M be a DBN-MDP and let be any sub-

Tset of the transition components in the model. The induced DBN-MDP on , denoted MT , is defined as follows:

sMinetTMofhTva,saeltuaheceshsVvaaamrlMieabselXet ioXf,isathanatesew, vinvaraailaudbedliewtsio.ans

M ; however,
to its original

upMiP,oaMTan=neTdnhPtaxusiaiMstj2ihnueXTVsi0a=a;jmliMnu0eaftlPorilfaraontaathslhileXetxircoi0iocnra2rs,geerVwssap,apeolPhMnhsaMdaaviXTnsegiMwtth.raa.jtnuFsPioatMri=oeTna1ccX,hoami0nad-j,

MT
and
For

ochtha2esrtVvheaeclsMtoamrsCeci,se,wtweRehahavasevMethta.htaFRtoiMRr TMieaTcchc=i

= 1; : : :; k
=,RRmiMaxc. .

With this definition, we can prove the analogue to the "Explore or Exploit" Lemma (details omitted).
TLemma 4.3:Let M be any DBN-MDP, let be any subset of x x xelttMUhheiMtaeeDhsptePtrrroaoo;tnnbhnTesaeiMrtbtrieio,ale.inntxsycFiis,ottohtimsoroarnpataoanntphnyooweetlrnaiiecntlsky2eTxooiffSseiMtTx,nscaMse,anteaeydpTpnsoTsdslif,ulco=ecaylthlnoMdktwhT+iaainnntgb1yMUe1MTTtwhTReismlulinact;hxadTkut.echea0adt,t

This lemma essentially asserts that either there exists a policy that already achieves near-optimal (global) return by stay-
ing only in the local model MT , or there exists a policy that
quickly exits the local model.

4.3 Putting It All Together
We now have all the pieces to finish the description and analysis of the DBN-E3 algorithm. The algorithm initially executes balanced wandering for some period of time. After some number of steps, by the Pigeonhole Principle one or more
xtransition components become known. When the algorithm
reaches a known state -- one where all the transition components are known -- it can no longer perform balanced wandering. At that point, the algorithm performs approximate off-line policy computations for two different DBN-MDPs. The first corresponds to attempted exploitation, and the sec-
Tond to attempted exploration. Let be the set of known transitions at this step. In the attempted exploitation computation, the DBN-E3 algorithm would like to find the optimal policy on the induced DBN-
MgoDriPthmM.TT.huCsl,ewarelyu,stehiistsDapBpNro-MxiDmPatiiosnnMo^t Tk,nwowhenretoththeetraule-

transition probabilities are replaced with their current approx-
imation in the model. The definition of MT uses only the
CPT entries of known transition components. The Simula-

tion Lemma now tells us that, for an appropriate choice of

-- a choice that will result in a definition of known transition

itwhniat1th=rine,qunoi,rfevsi,tstahnreedtcuTorrnr--eisnptMohneTdri.entgWurcenowuonfiltlatnsoypbepceoifolyinclayycphooilniycnMe^ofmToriiasl
later (which in turn sets the choice of and the definition of

known state).

Let us now consider the two cases in the "Explore or Ex-

x xploit" Lemma.
 in MT such

tIhnatthUeMexTploi;tTation cUasMe,th;eTree,xists.

a policy (Again,

x x x xwvLagmpaueeolpamuswrrteoam1inxila,nlti,emdeMwi^adsetacTetwuhhsapaiassvtylaeUgtfnhutrMneh^oa0iaTmrcntahgnUtohtaMi;ee^cleTgTeodoporttfoiit;mhbTmae1ble,aprloeomtwlUuiuc.rM)lynUtisFipMnrlaoiM;cm^TpaTot;tilTvh,.iceeTyhf,Saui+cmst0,ouwwr+l.aeohOtfoiaousraneert.

Therefore, in the exploitation case, our approximate planner

is guaranteed to return a policy whose value is close to the

optimal value.

tshiteiIronenftohwreeiteihxnipnMl^oTrTas)tittoehpnastcawissiegt,hutahsroearmneteeeexmdisittnosimatapukomeliacpnyroubnakinbnioMliwtyTn.

(and
tranOur

goal now is to use our approximate planner to find such a pol-

c CistHcitcryeau.rlecI,ttniofoootnrhrdMaeetarcTo0htfo(iMMd^=oTT0

t)h. aTt,hwe etrnaneesidtiuosnesatruslcitguhrtelyofdiMffeT0reinsticdoenn--

.1;H:o: w: ;ekvearn,dthe2rewVaarlMdsarein, owwe

different. have that

c c cRtNohnoiMawttTh0aelTeDt-sB=t0eNbp0-ewM;tfahoDlekrPpofooMth^llileTco0ryw.vrieIentctgtucoarrnns0ewbd,eiblwlsyhteaookhuwearvnaaetpthltpeharaatostxtthRiomeniMpaeTtrueonbpkalanb=noilnwi1etnyr.

transition is at least 1 ,  =k + 1T Rmax , =kT .

prUaenrMnoTtbeoxsaxtbpa;siltTuloeimittxyam,atitaoorlrniezi1ateps:,fitonplidco=syuarUn1Mae,ipxnppxrlM;oo^TxiTtia=mt,itaohtknaet+ppoe+1lnlaijcnToynyReisnrmfarMaoec^xitmtTu0ha,eoltrhurarefit=tcnhukudarTrnss-

of improving our statistics at an unknown transition in the
next T steps. Appropriate choices for and yield our main

theorem, which we are now finally ready to describe.

Recall that for expository purposes we have concentrated
on the case of T -step average return. However, as for the orig-
inal E3, our main result can be stated in terms of the asymp-

totic discounted and average return cases. We omit the details

of this translation, but it is
suffices to set T to be either

a1s=im1p,le

matter of
 log1=

arguing that it
 (discounted)

or the mixing time of the optimal policy (average).

Theorem 4.4: (Main Theorem) Let M be a DBN-MDP with ` total entries in the CPTs.

(Undiscounted case) Let T be the mixing time
icy achieving the optimal average asymptotic

orefttuhrenpUol-

in M . There exists an algorithm DBN-E3 that, given ac-

cess to a -approximation planning algorithm for DBN-

MDPs, and given inputs ; ; `; T and U , takes a num-

nbpexoercormeboeiaafdblaiiniclngittiy1oU=nast1,al,enads. tco;11m,=pu,t1a,=taioc,hn`i,teiTvme,esabntoodtuaRnldmaeacdxtub, ayalnardepwtouilrtynh(Discounted case) Let V  denote the value function for

the policy with the optimal expected discounted return

in M . There exists an algorithm DBN-E3 that, given

access to a DBN-MDPs,

-approximation
and given inputs

p,lan,n`inagndalVgor,itthamkesfoar

npoumlybnoermoiaf laicntio1n=s1an,d

computation time bounded
; 1= ; 1= ; `, the horizon

by a time

T1th,a=t V1,M=^wi1xll,haltV,iannadxsRta,mteaxx.,,aannddwoiuthtppurtoabapboilliictyy

at
^,

least such

Some remarks:

The loss in policy quality induced by the approximate planning subroutine translates into degradation in the running time of our algorithm.
As with the original E3, we can eliminate knowledge of the optimal returns in both cases via search techniques.
Although we have stated our asymptotic undiscounted average return result in terms of the mixing time of the optimal policy, we can instead give an "anytime" algorithm that "competes" against policies with longer and longer mixing times the longer it is run. (We omit details, but the analysis is analogous to the original E3 analysis.) This extension is especially important in light of the results of the following section, where we examine properties of mixing times in DBN-MDPs.

5 Mixing Time Bounds for DBN-MDPs
As in the original E3 paper, our average case result depends
on the amount of time T that it takes the target policy to mix.
This dependence is unavoidable. If some of the probabilities are very small, so that the optimal policy cannot easily reach the high-reward parts of the space, it is unrealistic to expect the reinforcement learning algorithm to do any better.
In the context of a DBN-MDP, however, this dependence is more troubling. The size of the state space is exponentially large, and virtually all of the probabilities for transitioning from one state to the next will be exponentially small (be-
cause a transition probability is the product of n numbers that are 1). Indeed, one can construct very reasonable DBN-
MDPs that have an exponentially long mixing time. For example, a DBN representing the Markov chain of an Ising model [Jerrum and Sinclair, 1993] has small parent sets (at most four parents per node), and CPT entries that are reasonably large. Nevertheless, the mixing time of such a DBN can
be exponentially large in n.
Given that even "reasonable" DBNs such as this can have exponential mixing times, one might think that this is the typical situation -- that is, that most DBN-MDPs have an exponentially long mixing time, reintroducing the exponential
dependence on n that we have been trying so hard to avoid.
We now show that this is not always the case. We provide a

tool for analyzing the mixing time of a policy in a DBN-MDP, which can give us much better bounds on the mixing time. In particular, we demonstrate a class of DBN-MDPs and associated policies for which we can guarantee rapid mixing.
Note that any fixed policy in a DBN-MDP defines a Markov chain whose transition model is represented as a DBN. We therefore begin by considering the mixing time of a pure DBN, with no actions. We then extend that analysis to the mixing rate for a fixed policy in a DBN-MDP.
cixSDxshjiea=fiii-,nnnm,ifttiahxixnjoie1jsdnd;M:l5ea:.tta1:.rf;:tkLixXmoesvtget.cQmghL1ta=beiin1tfe.rmeaWjpatrbexreaesisn;ejatsnhyjitPteittohhsnXetaatstmitttoahotnedea=eMrolyfaxftprohjkrreoobjvcaahXcbaMhiilnaai1.itrnykLooQ=evft

Our bounds on mixing times make use of the coupling method [Lindvall, 1992]. The idea of the coupling method
is as follows: we run two copies of the Markov chain in parallel, from different starting points. Our goal is to make the
states of the two processes coalesce. Intuitively, the first time
the states of the two copies are the same, the initial states have been "forgotten", which corresponds to the processes having
mixed.
sscatpnhaadaMtcienfeosZrfpSeoatrpcgerQe1tS=cSi,,1.setsbhlLuyoece,tnhthcQotethhvneoasitlbsdveieeefpraaafacrtacYrttoareanrdtdnsisi;MntiZitgoiaonrttnkomomQgva1ta.t=crtLrih1xiaexiitsonQvstehbofreevYtehMthreteasgsrorktt1ama=ontve1e-
dom variable that represents the coupling time -- the smallest
m for which Y m = Zm. The following lemma establishes
the correspondence between mixing and coupling times.

Lemma 5.2: For any , let m be such that for any i; j =

1; : : :; s, P 
-mixed at time

m m.

j

Y

1

=

xi;

Z 1

=

xj 



. Then Q is

Thus, to show that a Markov chain is -mixed by some
time m, we need only construct a coupled chain and show that the probability that this chain has not coupled by time m decreases very rapidly in m.

The coupling method allows us to construct the joint chain
over Y t; Zt in any way that we want, as long as each

of the two chains in isolation has the same dynamics as the
original Markov chain Q. In particular, we can correlate the

transitions of the two processes, so as to make their states

coincide faster than they would if each was picked indepen-
dently of the other. Y ZThat is, we choose t +1 and t +1

to be equal to each other whenever possible, subject to the

constraints on the transition probabilities. More precisely, let

Y t = xi and
make the event

Zt = Y t +1

xj. =

bPiliXty0

that is
= xk

tjheXsm=allxejro. f

For any
xPi;ZX0t+=1
Compare

value x 2 S, we can

= xk

xjj

have
X=

a proba-
xi and

this to the probability

of this event if the two processes were independent, which

is the product of these two numbers rather than their mini-

mum. Overall, by correlating the two processes as much as

possible, and considering the worst case over the current state

of the process, we can guarantee that, at every step, the two

Xprocesses couple with
mi;ijn k min P X0 =

probability at least
xk j X = xi; P X0

=

xk

j

X

=

xj

!In general, a variable pair can only be stable if their parents XYar2e10; Zatol2soocuasrntatbrsaltean.bsiitSliioozenwimhnaoitdsoehlla?aptpiIoennnt.shiiTfshwceayeseac,danndetoihtnheleyerdsYgt1ae;bZXil1i2zenoirf
they couple simultaneously.

This quantity represents the amount of probability mass that

This discussion leads to the following definition.

any two transition distributions are guaranteed to have in Definition 5.3:Consider a DBN over the state variables

common. It is called the Dobrushin coefficient, and is the
contraction rate for L1-norm [Dobrushin, 1956] in Markov

chains.

XNow, consider a DBN over the state variables =

fX1;
cess,

l:e:t:ti;nXgnYg1.;

As
:::

above, we
; Yn denote

ponent of the coupled Markov

create two copies of the prothe variables in the first com-
chain, and Z1; : : : ; Zn denote

those in the second component. Our goal is to construct a
Y Z Y ZMarkov chain over ; such that both and separately

Xhave the same dynamics as in the original DBN.

Our construction of the joint Markov chain is very simi-

lar to the one used above, except that will now choose the
transition of each variable pair Yi and Zi so as to maximize

the probability that they couple (assume the same value). As
above, we can guarantee that Yi and Zi couple at any time t

with probability at least

8: X u u =;9i

=

min ; 2 Xu u0

Val Pa

0 i



min P xi j
2x Xi Val i

; P xi j

0

trXthhecee1tr;tee:rda:ins:cs;yaiXctidloniicnr.eggTcrrthaeaepdphhdeedwopgfheetonhfsdereeoDnnmcBoydXNegisfrrataooprmehXXDXj 1iif;fto:otrh:Xt:eh;rje0eX.DinsBaaNnndeidswgaheedirine-

Hence, there is a directed path from Xi to Xj in

influences Xjt0 for some t0 t. Dtion graph of the DBN always has
every node in has a self-loop.

We assume
arcs Xi !

tXhai0,t

D iff Xit
the transi-
so that the

nenLtesti,n1D; :, :s:o;r,teldbseotthheatmiafxiimajl,stthreornegalyrecnoonndeicretecdtecdoemdgpeos-

ba,lfirnozio'duemsdngidb,n=;yjastumtimomcaco,eexrisetj.s.jir,IOoenfinjujo.nr.reAad(dWnesaarsenlufyoamnslrioyes,tsetiiwhsttaioihltslas,pbtta1oewb;sb:iesl:iaibpzs:lee;reod,,.v)aiol,iLldn1oeestfhtoatanhvble=eiylviazaamlilrnirsiaogntbauitblgheihesi-

need to
at time we can

mtcoowuvipetlheopnarttoeobxasatbcaitblliyitlyitzhiengsa,mgi.+e 1At.imsWes.ohoeTnnhiaaslsle,tvheienst,tahib'asiplhpizaeevnses,

stabilized, we are done.

This coefficient was defined by [Boyen and Koller, 1998] in
their analysis of the contraction rate of DBNs. Note that i
depends only on the numbers in a single CPT of the DBN.

Theorem 5.4:For any 0, the Markov chain corresponding to a DBN as described above is -mixed at time m pro-

Assuming that the transition probabilities in each CPT are not too extreme, the probability that any single variable couples will be reasonably high.

vided

m



8l
g

log1=

:

Unfortunately, this bound is not enough to show that all of the variable pairs couple within a short time. The prob-
lem is that it is not enough for two variables Yit and Zit
to couple, as process dynamics may force us to decouple

Thus, the mixing time of a DBN grows exponentially with the
size of the largest component in the dependency graph, which may be significantly smaller than the total number of variables in a DBN. Indeed, in two real-life DBNs -- BAT [Forbes

them at subsequent time slices. To understand this issue,
! j j ! !ahmcXY--oa1u1tnstrsPsatcindoss=XeueiXrtlp20ieo2l0axce.nt1dsAxYgiamw1sr2ns;aiptudxpt+lhmh2e1Zevwp;1aatrZtinhlotud2ahcette=+Ptasxh1tse2tXx,iwfme10r20bid.oteughmtteA,xtsYwtt01twh1X;oettxoh1vv;2edaZa,rirn1fiirfaetaeexbbsrXtllepheenea1t0spitcs,matXdXiinveri1ose2Y;ttslr,yX2lii.btsc2oueT;,,tZhiXtaohwu2nna2t0sdest,,
our sampling process may be forced to give them different

et al., 1995] with ten state variables, and WATER [Jensen et al., 1989] with eight -- the maximal cluster size is 3­4.
It remains only to extend this analysis to DBN-MDPs,
where we have a policy . Our stochastic coupling scheme must now deal with the fact that the actions taken at time t
in the two copies of the process may be different. The diffi-
culty is that different actions at time t correspond to different ttmrraaonndsseiilttiiioonnnddmiifsoftedrrieeblnsut.titorIafnnaisfivttaihoreinaabgclrteiaopXnhsiisPhnaaos,tiatthwdeiifslflaemuresene.taHtrdeainnffcseeirtieXonnit

values, decoupling them again.

cannot stabilize until we are guaranteed that the same action

As this example clearly illustrates, it is not enough for a is taken in both copies. That is, the action must also stabilize.

variable pair to couple momentarily. In order to eventually The action is only guaranteed to have stabilized when all of

couple the two processes as a whole, we need to make each the variables on which the choice of action can possibly de-

variable pair a stable pair -- i.e., we need to guarantee that pend have stabilized. Otherwise, we might encounter a pair

our sampling process can keep them coupled from then on. In
poasluersso.eoxAnanmadspoiltenc,coetuhYpe1lpe; saZ.ir1HYios1ws;tZeavb1eleri,s, itfshtYean2b;lYeZ2a2;sZcos2ouwopnliellasaswlsihtoifilberesYts1tca;obZul1e-

of states in which we are forced to use different actions in the two copies.
We can analyze this behavior by extending the dependency graph to include a new node corresponding to the choice of

is not yet stable, then the sampling process cannot guarantee action. We then see what assumptions allow us to bound

stability.

the set of incoming and outgoing edges. We can then use

the same analysis described above to bound the mixing time.

The outgoing edges correspond to the effect of an action. In

many processes, the action only directly affects the transition

model of a small number of state variables in the process. In

aonthderPwaoXrdi sj,

fPoaramXaniyvaarreiatbhleessaXmie,

we for

have that
all a. In

PaaXi
this case,

the new action node will only have outgoing edges to the re-

maining variables (those for which the transition model might

differ). We note that such localized influence models have a long history both for influence diagram [Howard and Mathe-

son, 1984] and for DBN-MDPs [Boutilier et al., 1999].

Now, consider outgoing edges. In general, the optimal

policy might well be such that the action depends on every

variable. However, the mere representation of such a pol-

icy may be very complex, rendering its use impractical in a

DBN-MDP with many variables. Therefore, we often want to

restrict attention to a simpler class of policies, such as a small

finite state machine or a small decision tree. If our target pol-

icy is such that the choice of action only depends on a small

number of variables, then there will only be a small number of

incoming edges into the action node in the dependency graph.

Having integrated the action node into the dependency

graph, our analysis above holds unchanged. The only differ-

ence from a random variable is that we do not have to include
the action node when computing the size of the ,i that con-
tains it, as we do not have to stochastically make it couple;

rather, it couples immediately once its parents have coupled.

Finally, we note that this analysis easily accommodates

DBN-MDPs where the decision about the action is also

decomposed into several independent decisions (e.g., as

in [Meuleau et al., 1998]). Different component decisions

can influence different subsets of variables, and the choice

of action in each one can depend on different subsets of vari-

ables. Each decision forms a separate node in the dependency

graph, and can stabilize independently of the other decisions.

The analysis above gives us techniques for estimating the

mixing rate of policies in DBN-MDPs. In particular, if we

want to focus on getting a good steady-state return from DBN-E3 in a reasonable amount of time, this analysis shows

us how to restrict attention to policies that are guaranteed to

mix rapidly given the structure of the given DBN-MDP.

6 Conclusions
Structured probabilistic models, and particularly Bayesian networks, have revolutionized the field of reasoning under uncertainty by allowing compact representations of complex domains. Their success is built on the fact that this structure can be exploited effectively by inference and learning algorithms. This success leads one to hope that similar structure can be exploited in the context of planning and reinforcement learning under uncertainty. This paper, together with the recent work on representing and reasoning with factored MDPs [Boutilier et al., 1999], demonstrate that substantial computational gains can indeed be obtained from these compact, structured representations.
This paper leaves many interesting problems unaddressed. Of these, the most intriguing one is to allow the algorithm to learn the model structure as well as the parameters. The

recent body of work on learning Bayesian networks from data [Heckerman, 1995] lays much of the foundation, but the integration of these ideas with the problems of exploration/exploitation is far from trivial.
Acknowledgements We are grateful to the members of the DAGS group for useful discussions, and particularly to Brian Milch for pointing out a problem in an earlier version of this paper. The work of Daphne Koller was supported by the ARO under the MURI program "Integrated Approach to Intelligent Systems," by ONR contract N66001-97-C-8554 under DARPA's HPKB program, and by the generosity of the Powell Foundation and the Sloan Foundation.
References
[Boutilier et al., 1999] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 1999. To appear.
[Boyen and Koller, 1998] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proc. UAI, pages 33­42, 1998.
[Dobrushin, 1956] R.L. Dobrushin. Central limit theorem for nonstationary Markov chains. Theory of Probability and its Applications, pages 65­80, 1956.
[Forbes et al., 1995] J. Forbes, T. Huang, K. Kanazawa, and S.J. Russell. The BATmobile: Towards a Bayesian automated taxi. In Proc. IJCAI, 1995.
[Heckerman, 1995] D. Heckerman. A tutorial on learning with Bayesian networks. Technical Report MSR-TR-95-06, Microsoft Research, 1995.
[Howard and Matheson, 1984] R. A. Howard and J. E. Matheson. Influence diagrams. In R. A. Howard and J. E. Matheson, editors, Readings on the Principles and Applications of Decision Analysis, pages 721­762. Strategic Decisions Group, Menlo Park, California, 1984.
[Jensen et al., 1989] F.V. Jensen, U. Kjærulff, K.G. Olesen, and J. Pedersen. An expert system for control of waste water treatment--a pilot project. Technical report, Judex Datasystemer A/S, Aalborg, 1989. In Danish.
[Jerrum and Sinclair, 1993] M. Jerrum and A. Sinclair. Polynomialtime approximation algorithms for the Ising model. SIAM Journal on Computing, 22:1087­1116, 1993.
[Kearns and Singh, 1998] M. Kearns and S.P. Singh. Near-optimal performance for reinforcement learning in polynomial time. In Proc. ICML, pages 260­268, 1998.
[Kearns et al., 1999] M. Kearns, Y. Mansour, and A. Ng. A sparse sampling algorithm for near-optimal planning in large markov decision processes. In these proceedings, 1999.
[Koller and Parr, 1999] D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In these proceedings, 1999.
[Lindvall, 1992] T. Lindvall. Lectures on the Coupling Method. Wiley, 1992.
[Meuleau et al., 1998] N. Meuleau, M. Hauskrecht, K-E. Kim, L. Peshkin, L.P. Kaelbling, T. Dean, and C. Boutilier. Solving very large weakly coupled Markov decision processes. In Proc. AAAI, pages 165­172, 1998.

