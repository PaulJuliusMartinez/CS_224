Analysis and Synthesis of Structural Textures

Laurent Lefebvre

Pierre Poulin

D´epartement d’informatique et de recherche op´erationnelle

Universit´e de Montr´eal

flefebvla, pouling@iro.umontreal.ca

Abstract

With the advent of image based modeling techniques, it
becomes easier to apply textures extracted from reality
onto virtual worlds. Many repetitive patterns (structural
textures) in human constructions can be parametrized
with procedural textures. These textures offer a powerful
alternative to traditional color textures, but they require
the artist to program the desired effects. We present a
system to automatically extract from photographs values
for parameters of structural textures, giving the user the
possibility to guide the algorithms. Two common classes
of procedural textures are studied : rectangular tilings and
wood. The results demonstrate that synthesizing textures
similar to their real counterpart can be very interesting for
computer-augmented reality applications.

Key words: procedural textures, texture mapping, image
based modeling, feature extraction, wood texture, rectan-
gular tiling, brick layout, computer-augmented reality.

1 Introduction
Repetitive patterns such as layouts of bricks and tiles are
ubiquitous in human constructions. It is therefore not sur-
prising to observe their frequent apparition as textures in
computer graphics scenes. These textures are mainly pro-
duced by two methods : color textures created by artists
or extracted from photographs, and procedural textures
generated by algorithms.
1.1 Color Textures
Traditional texture mapping takes an image (often a pho-
tograph) of a pattern and applies it to 3D models [10].
We refer to these as color textures in this paper. While
popular, the technique suffers from several shortcomings.
First, the extracted texture presents perspective deforma-
tions that must be corrected. The texture must ﬁt exactly
the 3D model, otherwise extending or tiling the texture
can be noticed even if adjacent tiles are seamless. The
color for each texel results also from the real scene illu-
mination, and it is very difﬁcult to factor out the illumina-
tion effects from these colors. Finally, the texture remains
of a ﬁxed resolution.

With the advent of image based modeling for aug-

mented reality applications, textures and models are both
extracted from photographs of a real scene, leading to
a closer relationship between the model and its texture.
Several methods to exploit this relationship have been in-
troduced [9, 3, 4, 20, 18, 19].

They give good results, but are not perfect. For exam-
ple, visibility discontinuities on two adjacent faces would
produce artifacts (see Figure 12 left). Consider also the
situation where objects occlude at least partly the desired
object from all angles. It is virtually impossible to remove
the occluders from the extracted textures, or to determine
what lies behind. Lighting artifacts due to camera ﬂash,
or shadows and reﬂections of the cameraman can occur
in photographs. Extracting the texture augmented with its
BRDF [24] can reduce the artifacts, unfortunately with-
out removing them all. Moreover, extracting the BRDF
is a difﬁcult and lengthy operation that can currently be
done only in highly controlled environments.

1.2 Procedural Textures
Procedural textures [16, 17] are generated by algorithms.
As repetitive patterns are often easily deﬁned by such al-
gorithms, it falls under the responsibility of the artist to
design the proper algorithm to reach his goal. Unfortu-
nately, not all computer artists are necessarily efﬁcient
programmers.

Another direction consists in analyzing a texture to ex-
tract its parameters in order to synthesize a new texture
with similar characteristics. All undesired features can
thus be removed during the analysis phase, and a poten-
tially smaller set of parameters is kept to generate the new
texture at the desired resolution.

This approach is popular in image processing and
texture recognition, and has been extended by multi-
resolution techniques or by non-parametric sampling [11,
12, 2, 5, 6]. These techniques give very good results on a
large class of stochastic or semi-stochastic textures such
as rocks, grass, stucco, etc. They have been applied to
replace portions of textures in reconstructed 3D scenes
[15]. Their generality frees the user from any interven-
tion. However they usually fail in the case of a repetitive
pattern such as bricks, where the semantics of the features

                        

            

a

b

c

d

Figure1:Synthesizingawallofbricks: (a)Originalimage;(b)SynthesisfromDeBonet[2];(c)SynthesisfromEfros
andLeung[6],implementationofWei[22];(d)Synthesisfromouralgorithm

(mortar lines, brick size and shape, regular tiling) is lost
(see Figure 1).

Very little work has been devoted to synthesize struc-
tural textures with a priori information on the structure of
the texture [23, 25]. The situation is similar in the feature
extraction from structural textures, except on specialized
applications such as road tracking by vehicles [8, 14].
1.3 Extraction and Synthesis of Structural Textures
We present in this paper our solution to the problem of
generating textures adapted to a scene reconstructed from
photographs. We are not attempting to generate a pro-
cedural texture identical to its corresponding color tex-
ture. However our textures should be sufﬁciently similar
to them to adequately replace the color texture in order
to remove its artifacts such as illumination from the real
scene, occlusions, ﬁxed texture resolution, etc. Because
of their frequent use, we concentrate our efforts on algo-
rithms for the analysis and synthesis of rectangular tilings
and wood textures.

The process can be summarized by the following steps.
A 3D model is created from photographs using our in-
teractive photogrammetry reconstruction software [18],
and a color texture is associated with each polygon. For
each texture, the user selects the texture type and pro-
duces with various image processing tools [21] a binary
mask representing the texture with black and white pix-
els. Given the color texture, the binary mask, and the tex-
ture class, the system analyzes the features and ﬁts values
to the parameters associated with this texture class. Fi-
nally a new procedural texture can generate adapted high
resolution textures for the 3D models. The system allows
user intervention at all steps, while automatically provid-
ing extracted values for all parameters.

Our system can create quality structural textures free
from artifacts generally present in the photographs. Be-
cause of our knowledge of the structure of the texture,
the resulting procedural texture can be further reﬁned in

several ways, including the addition of a full reﬂection
model, a bump or displacement map, etc.

In the next two sections, we detail our procedural mod-
els for rectangular tilings and wood. Section 4 shows how
these models can be augmented to create more sophisti-
cated textures. Finally we conclude and discuss exten-
sions for this work.

2 Rectangular Tiling
The ﬁrst texture class we present is the rectangular tiling.
Because of its ubiquity in most man-made constructions,
many textures fall in this category : brick walls, ceramic
tiles, skyscraper windows, hanging ceilings, etc.

tile
center

tile width

relative
offset

tile center

mortar thickness

tile height

orientation

Figure2: Ourrectangulartilingmodel

We start by segmenting the color texture into a binary
mask in order to facilitate parameter extraction. Sev-
eral tools, including Sobel and pyramidal edge detectors,
histogram segmentation thresholding, and morphological
operators (opening, closing, eroding, etc.) are provided to
the user to segment and clean up the color texture in or-
der to create the binary mask. Most of these tools are in-
terfaced from the C image processing library CVIP [21].
This segmentation step is fundamental because some of
our parameter extraction algorithms can be sensitive to

noise. Fortunately, the user can observe the quality of his
mask and reﬁne it if necessary. Typically, an experienced
user would not require more than a few minutes to pro-
duce a reasonable mask.

Our tiling model is simple enough to automatically ex-
tract all its parameters from a binary mask (created by
the user) representing the texture. Our model consists of
6 scalar parameters (some illustrated in Figure 2) : width
and height of the tiles, orientation of the pattern, mortar
thickness, number of bricks along a wall, and relative off-
set between adjacent tiles on successive rows.

The Fourier transform (actually FFT) of the binary
mask (see Figure 3) gives important clues in relation to
the height of the tiles and their orientation.

Figure3:Colortexture,segmentedtexture(binarymask),
andscalednegativeoftheFFTofthemask

2.1 Height and Orientation
In a FFT image of resolution 2R  2R, the center of the
brightest group of points (^x; ^y) other than the center of
the image (DC frequency), corresponds to the major fre-
quency in the texture. In a tiling pattern, this frequency
is associated with the repetition of the base lines of the
tile layers. The relation between the spectral and the fre-
quency domains provides the height of the tiles measured
as a proportion of the total image height as

R

:

(1)

Height =

2^x   R2  ^y   R2

Because the orientation of the base lines in the tiling
pattern is invariant under the Fourier transform,
the
brightest point (^x; ^y) in the FFT image also provides the
orientation angle of the tiling pattern as

Angle = aca(cid:18) ^x   R
^y   R(cid:19)

(in radians).

(2)

The sign of the angle is given by the sign of (^x   R).

The height and orientation of the tiles are extracted
with very good precision because the FFT is a global
transformation which resists well to noise. These two
values are the ﬁrst extracted and serve as a basis for the
extraction of the remaining parameters.

The pattern formed by the vertical mortar lines is usu-
ally more difﬁcult to extract accurately from the FFT im-
age, as illustrated by ﬁnding the second brightest clus-
ter of points in Figure 3. The next section describes our
spatial algorithms to extract the remaining information.
While ﬁlters applied vertically or autocorrelation tech-
niques could prove useful, these spatial algorithms have
shown to be sufﬁciently reliable and efﬁcient for our pur-
pose.
2.2 Width and Relative Offset
The remaining parameters are extracted using local and
more efﬁcient spatial algorithms, even though they are
more prone to the effects of noise. For simplicity in sub-
sequent steps, we rotate the mask backward by the angle
previously measured in order to align it with the axes of
the image.

To estimate the width of the tiles, we trace a number
of horizontal scanlines, counting the number of contin-
uous white pixels (tiles) separated by black pixels (mor-
tar). Scanlines within the mortar horizontal regions are
discarded as well as tiles not bounded by mortar. Every
tile width is sorted and the median is taken as the width
of the tiles.

In presence of patterns formed with tiles of a small set
of different lengths, this approach can ﬁnd the length of
each type of tile by separating them into categories. Al-
though we did not investigate further this direction, one
should be able to apply results in pattern detection [13] to
determine more sophisticated tiling patterns.

The mortar thickness is similarly computed, but in-
stead with vertical scanlines. This orientation is less af-
fected by noise due to the segmentation. If horizontal and
vertical thicknesses are different, this approach can easily
be extended to satisfy a new model.

The relative offset between successive rows of tiles is
also extracted with horizontal scanlines. A tile is selected
randomly and its width is estimated as above with hori-
zontal scanlines. An adjacent tile on a row above or un-
der is found by tracing a vertical scanline crossing only
one mortar region. The width of this second tile is also
estimated like for the ﬁrst tile. The relative offset is com-
puted from the center of both tiles. The process is re-
peated for several tiles, discarding outliers, and averag-
ing the median offsets over several selected tiles. If the
relative offset divided by the tile width is close to zero,
we assume we are in presence of an aligned tiling pattern
often occurring in ceramic tiles.

2.3 The Number of Tiles
The number of tiles on the 3D polygon is not always auto-
matically extracted. For instance, we often use a close-up
view of the texture which includes more details than the
extracted texture. In this case, the number of tiles in the
close-up view is different from the actual number of tiles
on the wall. Also, in many photographs taken to recon-
struct a scene, the mortar regions are often too small to
automatically and reliably determine the number of tiles.
The user therefore might want to specify how many
tiles should appear in a row on the 3D polygon. Once
this value is detected or entered, the system can lay tiles
with the right width and the proper aspect ratio over the
3D polygon.

2.4 Filling the Interior
Interiors of tiles and mortar regions have often a more
stochastic nature, and are therefore more suitable for
automatic analysis and synthesis algorithms [11, 2, 6].
We use the C implementation by El-Maraghi [7] of the
Heeger and Bergen algorithm [11].

The user selects one or more regions to represent tiles
and one or more regions to represent mortar. He should
avoid unsatisfactory regions with artifacts such as occlu-
sions, highlights, shadows, poor resolution, aliasing, etc.
The Heeger and Bergen algorithm then processes auto-
matically the selected regions to generate similar high
quality textures at the desired resolution.

Sometimes, the distribution of tiles presents itself a
special color pattern, or tiles in some regions might be
more dirty than others. Once all parameters of the tiling
model are extracted, it is easy to decide which selected
region to generate from, according to the location of the
current tile being treated.

2.5 Results of Rectangular Tiling
The results in this section use our method to analyze and
synthesize tiling textures. Roughness, specularity, and
reﬂectance are not currently extracted, but represent one
of our topics for future work. At this moment, the user
must enter values to set the corresponding parameters.

The left part of Figure 4 shows a bump mapped syn-
thesized brick wall generated from the real wall of Fig-
ure 1a. All parameters were automatically extracted from
the photograph. In the right part of the ﬁgure, some noise
is added to displace the surface in order to increase the
realism.

The same method is applied on a real ceramic wall on
the left of Figure 5. The user selected analysis regions
outside the specular highlight, thus avoiding mixing these
colors in the synthesis. The presence of such highlights
introduces difﬁculties to most analysis and synthesis al-
gorithms [11, 2, 6], having a direct impact on the quality

            

            

Figure 4: Synthesized bricks with bump map (left) and
synthesizedbrickswithdisplacementmap(right)

of the resulting synthesized textures. A bump map func-
tion and a specular coefﬁcient are applied on the gener-
ated tiles, which then behave properly under new illumi-
nation (Figure 5 right).

            

            

Figure5: Realandsynthesizedceramicwalls

3 Wood
Wood textures, although not as frequent as rectangular
tilings, represent nevertheless a great deal of textures in
our environments. Wood is used for ﬂoors, doors, furni-
ture, stairs, window frames, etc. It can be modeled from
a simple growth pattern that can be extracted from pho-
tographs, and then generated onto 3D models. Another
approach consists of creating an anisotropic 3D texture
volume using a pair of orthogonal photographs [5] and
mapping this volumetric texture onto 3D models.

While more sophisticated and complete wood models
exist [1], we chose to keep our model simple enough to
automatically extract values for its parameters from a sin-
gle photograph.

The trunk of a tree oriented along the Z axis is modeled
as a series of identical thickness concentric rings with al-
ternating colors between early and late wood types. The
distribution of colors follows a periodic sine function of
frequency !, starting from the central axis (xc; yc; z) of
the trunk. The color of any 3D point (x; y; z) is computed
by thresholding the following equation :

i(cid:16)!x   xc2  y   yc2(cid:17) :

A wood plank is modeled as a rectangular cut into this
trunk. A tilt angle and a rotation angle are associated with

(x  , y  )

c

c

W
E
I
V
E
D
I
S

 

TOP VIEW

Plank

FRONT VIEW

Y

Z

Y

X

Z

Rotation angle

Early wood color
Late wood color
Tilt angle

X

Growth frequency

Figure6: Oursimplewoodmodel

the plank, as illustrated in Figure 6.

The 10 scalar parameters of our wood model are suf-
ﬁcient to reproduce a fair range of wood types, notwith-
standing knots. They are : trunk center (xc; yc), tilt and
rotation angles, growth frequency, early/late wood ratio
and colors, and turbulence intensity and frequency.

Once again, the user starts by converting the original
texture into a binary mask image with thresholding and
edge detection operators (see Figure 7).

            

            

Figure7: Realwoodtextureandsegmentedmask

3.1 Early and Late Wood Colors and Ratio
The binary mask is ﬁrst traversed to determine the ratio of
early wood pixels in the texture. In this paper, late wood
is represented as black pixels and early wood as white
pixels.

While traversing the corresponding pixels in the orig-
inal texture, we sort the colors for each wood type. The
25th percentile of the early wood color and the 75th per-
centile of the late wood color provide satisfying colors for
each wood type. This empirical solution avoids to wash
out the colors resulting from the binary thresholding, and
they are modiﬁable by the user if necessary.

3.2 Wood Growth Frequency and Rotation Angle
The center of the brightest group of points (other than the
DC frequency) in the Fourier transform of the mask cor-
responds to the major frequency in the texture. It provides
the growth frequency. The orientation of this frequency
determines the rotation angle of the plank in the trunk.
These computations are given by Equations (1) (actually
its inverse) and (2) of Section 2 to determine the principal
frequency and orientation of the wood.

Once the orientation is determined, the mask and its
texture are rotated backward to be aligned with the image
axes for the extraction of the tilt angle and other parame-
ters.

3.3 Tilt Angle
The appearance of the wood texture is highly inﬂuenced
by the tilt angle of the plank in the trunk. The tilt an-
gle controls mainly the shape of ellipses in the texture,
and therefore these ellipses can be used to determine this
angle.

Our wood model does not handle the presence of knots
in the texture. Detecting and synthesizing knots is part of
our future work. Unfortunately, large knots can consider-
ably bias most automatic algorithms because they behave
similarly to the trunk itself, but at a smaller scale.

Our current solution is approximative, but simple and
efﬁcient.
It is based on the observation that as the el-
lipses curve, less and less late wood (or respectively of
early wood) appear in successive horizontal scanlines. If
the tilt angle is null, the texture shows only straight lines
with an identical number of early wood pixels. We there-
fore proceed by counting the number of late wood pixels
in successive horizontal scanlines, and associate an em-
pirically measured factor with this variation.

Obviously, this method is sensitive to noise in the mask
and suffers in presence of a large number of knots. How-
ever on our test set of wood textures, the results were
surprisingly visually satisfying.

We are currently investigating another direction, where
the wood grain is traced in the mask. If an elliptic path is
identiﬁed, we compute its center and eccentricity. By ran-
domly selecting paths in the mask, we expect that statis-
tics on the resulting ellipses should more reliably estimate
the tilt angle.
3.4 Turbulence Intensity and Frequency
We observe that turbulence in wood patterns has usually
a fairly low frequency. We set a typical value by default,
that the user can modify if necessary.

The intensity of this turbulence is estimated in regions
outside high curvature ellipses. We compute the varia-
tion of wood type between vertical scanlines of pixels. If
there is no variations, we are in presence of parallel ver-

tical wood grain, thus the intensity of the turbulence is
set to null. Otherwise, it is set according to the degree of
variation.
3.5 Trunk Center
The coordinates of the trunk center (xc; yc) relative to the
plank are also estimated statistically.

Assume one tangent of the plank is oriented along Y .
If the plank is tilted, ellipses will appear in the image.
The center of these ellipses corresponds to the Y coordi-
nate (yc) of the center of the trunk. To ﬁnd this center, we
compute the color variance of each column of pixels in
the image. The column with the largest variance (i.e., in-
tersecting more ellipses) is associated with the trunk cen-
ter. If the variance is smaller than a certain threshold, a
default value outside the image is assigned to the Y coor-
dinate.

The X coordinate is more difﬁcult to extract accu-
rately. For a trunk located at xc 6= 0, the width of suc-
cessive layers of wood type varies, as illustrated in Fig-
ure 8. Once again, statistics on rows of pixels in the im-
age are used to indicate the location of the X coordinate
(xc) of the trunk center. As a general rule of thumb, ex-
cept for the central layer which can be thinner, the larger
the widths become, the closer to the center we are.

Y

X

Figure8: Settingthe X coordinateofthetrunkcenter

3.6 Results of Wood Textures
Although our extraction algorithms are only approxima-
tive for a number of parameters, and that many of these
parameters are closely interrelated, they usually perform
very well when visually comparing real wood textures
and the synthesized textures. Figure 9 shows a typical
result of our method.

An attractive aspect of this model is that the values we
extract from a single 2D image of wood, are producing
a complete 3D procedural texture, which has an inﬁnite
resolution and can be applied on any surface, even curved
surfaces.

In order to evaluate the precision of our parameter ex-
traction algorithms, we generated a number of textures
with our procedural wood model, and compared the val-
ues extracted with the real values. We generated ten dif-

Figure9: Realwoodandsynthesizedwood

ferent wood patterns for which we modiﬁed each param-
eter, one at a time. After each session, we kept the differ-
ence between the estimated value and the real one. The
results appear in Table 1 as percentages of the difference
with the real value. Note that these percentages are com-
puted on the maximum scale values for most of the pa-
rameters. For all our generated textures, we found that
the error was proportional to the real value for the tilt
angle and the frequency. For these two parameters, the
percentages are computed with respect to the real value
instead of the maximum scale value.

The parameter with the worst estimated value is the in-
tensity of the turbulence. While some perturbation ap-
pears in wood, the exact value of this perturbation is
less important. We observed that our extracted values
were usually quite visually satisfying. Since the early/late
wood ratio, the early wood color, and the late wood color
are all extracted using the mask, errors are caused mainly
by differences between the masks created by the user for
each texture.

Real wood planks are usually cut either almost parallel
or almost perpendicular to the wood grain. Considering
this fact in our generated textures, we only made small
modiﬁcations to the tilt angle around 0Æ and 90Æ to eval-
uate the tilt angle extraction algorithm. Our extraction
algorithm is less precise when the cut is made at angles
not near 0Æ or 90Æ. For example, we generated a texture
in which the tilt angle was 30Æ and the extracted value had
an error of 36 (this result is not included in Table 1). In
such cases, since the shape of the ellipses is deﬁned by
the tilt angle, the user can modify the tilt value to achieve

Wood Model

Parameter

Tilt

Orientation
Frequency

Maximum
Scale Value



180Æ



Ratio Early/Late

Turbulence Intensity

8
30
512
512
256
256
 Proportional to the real value.

Early Intensity
Late Intensity

xc

yc

8.09
0.35
2.47
5.5
10.3
2.71
2.27
3.18
2.79

Statistics on Differences (%)
Average

STD
4.56
0.24
4.81
3.5
7.16
2.46
2.21
0.76
0.94

Max
14.29
0.6
12.34
10.5
20.63
6.56
6.50
4.94
3.38

Table1: Extractingvaluesforsimulatedwood

the desired effet.

For the remaining parameters, our algorithm is usually
sufﬁciently precise with average errors around 2-5% (see
Table 1). In the case of an unusual error, the user can
correct the problem in a few steps. For example, for the
parameter yc, the user can easily specify the center of the
ellipses in the original image.

4 Improving the Procedural Models
Modifying the appearance of a digitized texture requires
much work by the artist. Once we have extracted values
for our simple models, adding new features speciﬁc to
the resulting textures is much simpler because we have a
structural deﬁnition of the texture. It is therefore simple
to add a mirror reﬂection only on the interiors of tiles, to
displace the mortar lines below the tiles depth, to raise
the roughness of early wood type, etc.
4.1 Augmenting the Realism with Various Maps
Bump or displacement maps can be applied to the struc-
tural models according to the nature of each structure. For
instance, one can apply a bump map on individual bricks,
while forcing a deeper displacement map on the mortar
regions. Figures 5 and 11 (right) show bump maps ap-
plied to a ceramic wall and a hardwood ﬂoor. Figure 10
shows a displacement map on brick walls.

Similarly, different specularity coefﬁcients and rough-
nesses can be applied on speciﬁc elements of a structural
model to create highlights on tiles but not on mortar. Mir-
ror reﬂections on hardwood ﬂoors can provide a clean
varnished appearance (Figure 13).
4.2 Matching Textures on Adjacent Polygons
Another advantage of a procedural texture is that one
can set constraints to satisfy the structural nature of the
texture, or parameters that must be shared by several in-
stances of the texture.

Figure 10 shows a brick layout texture ensuring all

mortar lines match, as well as setting the relative offset
of the bricks to match the width of a brick.

            

            

Figure10: Brickwallsonatowerandclose-upview

4.3 Procedural Textures and Color Textures
While procedural textures offer a powerful method to
generate structural textures, many textures such as paint-
ings or text cannot be modeled by procedures.

When this happens, we give the user the possibility to
choose in the texture space which part should be gen-
erated procedurally, and which part should be extracted
as a traditional color texture. This is achieved by letting
the user paint a mask to indicate the regions associated
with each texture. The picture frame in Figure 12 and
the metal screen of the ﬁreplace in Figure 13 are color
textures. Tiles, bricks, and the hardwood ﬂoor in these
ﬁgures are procedural.
4.4 Multiple Structural Textures
On appropriate models, multiple structural textures can
be applied on the same 3D surface. Hardwood ﬂoors ex-
ploit the two texture types described in this paper.

The rectangular tiling method extracts fairly reliably
with the FFT the width of the planks, and the wood

be used, they offer great advantages that should be ex-
ploited. Although advanced synthesis algorithms such as
the one proposed by Efros and Leung [6] are more gen-
eral, they often take hours to synthesize structural tex-
tures and the results are not always semantically correct.
Our algorithm running on an SGI Onyx R4400 processor
takes only a few minutes for both tuning the parameters
and synthesizing new textures. All the textures contained
in this paper were created with at most three iterations
for extracting the parameters (in the case of the hardwood
ﬂoor of Figure 11). The longest time for texture synthe-
sis was one hour for the ﬁreplace, mainly because many
bricks had to be synthesized using the Heeger and Bergen
algorithm [11].

6 Future Work
While the current results are encouraging, we feel there
is still much information in photographs that could be ex-
ploited to improve our structural textures.

The geometry recovered by our image based model-
ing tool can easily indicate regions in umbra, potential
highlights, reﬂections, etc. These phenomena can sig-
niﬁcantly alter the colors of the extracted procedural tex-
tures. Rather than just neglecting these regions, one could
develop techniques to extract values for these phenom-
ena, thus obtaining a more complete structural model
from the photographs.

Acknowledgements
We acknowledge ﬁnancial support from NSERC, and an
equipment donation from the DiTER of the Universit´e de
Montr´eal. Martin Blais developed an important portion
of our current reconstruction system. We thank Jeremy
De Bonet for providing the original and the synthesized
textures in Figure 1. We also thank Li-Yi Wei [22] and
T. El-Maraghi [7] for making available their respective
implementations of Efros [6] and Heeger and Bergen [11]
algorithms.

modeling method ﬁlls in the interior of each plank. Be-
cause less information can be extracted from a plank than
from a large wood panel, the similarity of the generated
wood texture might not be as precise. However we found
that the nature of wood leads to satisfying results even
with narrow planks. Figure 11 gives an example of this
method.

When the photographs do not provide enough informa-
tion to generate detailed wood textures, one can extract
the necessary information from different photographs,
and apply the result on the ﬂoor layout.

5 Conclusion
Color textures extracted from photographs can suffer
from several artifacts resulting from different resolutions,
misalignments, undesired occluding objects, missing por-
tions in the texture, shadows, highlights, etc.

We have presented a technique to extract structural tex-
tures from photographs, and synthesize similar new tex-
tures from the corresponding procedural textures. Unlike
random textures which can be generated automatically,
the features in structural textures are signiﬁcant and must
be modeled accurately.

We focused our efforts on tiling patterns and wood
patterns, both frequent in man-made constructions. For
both patterns, we presented techniques to determine
automatically values for their parameters, while giv-
ing the user the possibility to guide the algorithms
or modify any value that was extracted.
Struc-
textures inspired by photographs were gener-
tural
ated as examples.
Other results can be accessed
from the web site associated with this paper from
www.i.	ea .ca/ ab/ifgahie/ae.

Our simple textural models lead to good results where
no satisfying color textures could have been extracted.
The procedural generation of these textures has many ad-
vantages.
It can be adapted to the desired resolution,
compressed more efﬁciently than color textures of sim-
ilar quality, manipulated to generate different results, and
augmented by other familiar rendering techniques to in-
crease their realism.

While our parameter extraction techniques were devel-
oped speciﬁcally for our structural models, many features
of these techniques could remain appropriate for parame-
ter extraction in other structural models. For instance, the
FFT is frequently used to ﬁnd orientations and frequen-
cies in image processing. We have tried our algorithms
on several different tile patterns and wood styles, and we
are conﬁdent that they should apply well to other such
patterns.

Procedural textures cannot replace all color textures in
computer graphics applications. However when they can

            

            

            

Figure11: Real (left)andsynthesized(centerandright)hardwoodﬂoor. A small bumpmapis appliedtocurvethe
edgesofeachlinearpieceintherightﬁgure.

            

            

            

Projected color textures

Synthesized view

Substituting the ﬂoor texture

Figure12: Reconstructionofstructuraltextures(synthesizedtiles)fromthreephotographsofashowcasebathroom.
Thesystemletstheusertryoutothercombinationoftilingpatternsontotheimage-basedmodeled3Dscene.

            

            

Real ﬁreplace

Synthesized views

Figure 13: Reconstruction of structural textures (bricks and varnished hardwood ﬂoor) synthesized from a single
photographofaﬁreplace. Themetalscreeninfrontoftheﬁreplaceisacolortextureextractedfromthephotograph.
Smallobjectsonthechimneyareproperlyremovedfromthephotographwithoutcausinganyholes.

References
[1] J.W. Buchanan. Simulating wood using a voxel ap-
proach. Eurographics ’98, 17(3):C105–C112, 1998.

[2] J.S. De Bonet. Multiresolution sampling procedure
for analysis and synthesis of texture images. In SIG-
GRAPH 97 Conference Proceedings, Annual Con-
ference Series, pages 361–368, August 1997.

[3] P.E. Debevec, C.J. Taylor, and J. Malik. Modeling
and rendering architecture from photographs: A hy-
brid geometry- and image-based approach. In SIG-
GRAPH 96 Conference Proceedings, Annual Con-
ference Series, pages 11–20, August 1996.

[4] P.E. Debevec, Y. Yu, and G.D. Borshukov. Efﬁ-
cient view-dependent image-based rendering with
projective texture-mapping. In Ninth Eurographics
Workshop on Rendering, pages 105–116, Vienna,
Austria, June 1998.

[5] J.M. Dischler, D. Ghazanfarpour, and R. Freydier.
Anisotropic solid texture synthesis using orthogonal
2d views. Eurographics ’98, 17(3):C87–C95, 1998.

[6] A.A. Efros and T. K. Leung. Texture synthesis
by non-parametric sampling.
In IEEE Interna-
tional Conference on Computer Vision (ICCV’99),
sep 1999.

[7] T.

El-Maraghi.

An
and Bergen’s

implementation
of Heeger
texture
analy-
sis/synthesis algorithm. University of Toronto,
www.cs.utoronto.ca/(cid:24)tem/2522/texture.html,
September 1997.

[8] D. Geman and B. Jedynak. An active testing
model for tracking roads in satellite images. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 18(1):1–14, January 1996.

[9] P. Havaldar, M.-S. Lee, and G. Medioni. View syn-
thesis from unregistered 2-D images. In Graphics
Interface ’96, pages 61–69, May 1996.

[10] P.S. Heckbert. Survey of texture mapping.

IEEE
Computer Graphics and Applications, 6(11):56–67,
November 1986.

[11] D.J. Heeger and J.R. Bergen. Pyramid-based tex-
ture analysis/synthesis. In SIGGRAPH 95 Confer-
ence Proceedings, Annual Conference Series, pages
229–238, August 1995.

[12] A.N. Hirani and T. Totsuka. Combining frequency
and spatial domain information for fast interactive
image noise removal.
In SIGGRAPH 96 Confer-
ence Proceedings, Annual Conference Series, pages
269–276, August 1996.

[13] R. Karp, R. Miller, and A. Rosenberg. Rapid iden-
tiﬁcation of repeated patterns in strings, trees, and
arrays. In Proc. of the ACM Symposium on the The-
ory of Computing, pages 125–136, 1972.

[14] W. Kasrpzak and H. Niemann. Adaptive road recog-
nition and ego–state tracking in the presence of ob-
stacles. International Journal of Computer Vision,
28(1):5–26, 1998.

[15] C. Loscos, M.-C. Frasson, G. Drettakis, B. Walter,
X. Granier, and P. Poulin. Interactive virtual relight-
ing and remodeling of real scenes. In Tenth Euro-
graphics Workshop on Rendering, pages 329–340,
Granada, Spain, June 1999.

[16] D.R. Peachey. Solid texturing of complex surfaces.
Computer Graphics (SIGGRAPH ’85 Proceedings),
19(3):279–286, July 1985.

[17] K. Perlin.

An image synthesizer.

In Com-
puter Graphics (SIGGRAPH ’85 Proceedings), vol-
ume 19, pages 287–296, July 1985.

[18] P. Poulin, M. Ouimet, and M.-C. Frasson. Interac-
tively modeling with photogrammetry. In Ninth Eu-
rographics Workshop on Rendering, pages 93–104,
Vienna, Austria, June 1998.

[19] C. Rocchini, P. Cignoni, and C. Montani. Multiple
textures stitching and blending on 3D objects.
In
Tenth Eurographics Workshop on Rendering, pages
119–130, Granada, Spain, June 1999.

[20] Y. Sato, M.D. Wheeler, and K. Ikeuchi. Object
shape and reﬂectance modeling from observation.
In SIGGRAPH 97 Conference Proceedings, Annual
Conference Series, pages 379–388, August 1997.

[21] S.E. Umbaugh. Computer Vision and Image Pro-

cessing. Prentice-Hall inc., 1998.

[22] L. Wei. An implementation of Alyosha Efros’
texture synthesis algorithm. Stanford University,
www.graphics.stanford.edu/(cid:24)liyiwei/project/texture
/efros, January 2000.

[23] R. Yokoyama and R.M. Haralick. Texture synthe-
sis using a growth model. Computer Graphics and
Image Processing, 8(3):369–381, December 1978.
[24] Y. Yu, P. Debevec, J. Malik, and T. Hawkins. In-
verse global illumination:
recovering reﬂectance
models of real scenes from photographs. In SIGGR-
RAPH 99 Conference Proceedings, Annual Confer-
ence Series, pages 215–224, 1999.

[25] S.W. Zucker. Toward a model of texture. Com-
puter Graphics and Image Processing, 5(2):190–
202, June 1976.

