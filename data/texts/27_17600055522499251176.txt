16th ASCE Engineering Mechanics Conference July 16-18, 2003, University of Washington, Seattle

LARGE-SCALE NORTHRIDGE EARTHQUAKE SIMULATION USING OCTREE-BASED MULTIRESOLUTION MESH METHOD
Eui Joong Kim 1 Jacobo Bielak 2, Member ASCE
and Omar Ghattas 3

ABSTRACT

Large scale ground motion simulation in realistic basins can benefit greatly from the use of parallel

supercomputing systems in order to obtain reliable and useful results within reasonable elapsed time

due to its large problem size. We present a parallel octree-based multiresolution finite element method-

ology for the elastodynamic wave propagation problem and develop a framework for large scale wave

propagation simulations. The framework is comprised of three parts; (1) a mesh generator (Euclid,

Tu et al. 2002), (2) a parallel mesh partitioner (ParMETIS, Karypis et al. 2002), and (3) a parallel

octree-based multiresolution finite element solver (QUAKE, Kim 2003). The octree-based multires-

olution finite element method reduces memory use significantly and improves overall computational

performance. The numerical methodology and the framework have been used to simulate the seismic

response quencies

of of

the greater up to 1 Hz

Los Angeles basin and domain size of

for 80

 a

m80ai ns3h0ockkmo¡ f.tThehr1o9u9g4hNsiomrtuhlraitdiognesEfaorrthseqvuearkael,mfoordferles-,

ranging in size from 400,000 to 300 million degrees of freedom on the 3000-processors HP-Compaq

AlphaServer Cluster at the Pittsburgh Supercomputing Center (PSC), we achieve excellent performance

and scalability. Keywords: Octree-Based

Multiresolution

Mesh

Method

(OBM ¢

),

Northridge

Earthquake,

Ground Motion Simulation

INTRODUCTION Wave propagation simulations for earthquake-induced ground motion have been performed
over the last 30 years to gain a better understanding of the distribution of the earthquake ground motion in urban regions in space and time. Such insight has contributed to develop building codes in which a seismic prone region is divided into different zones of comparable seismic hazard. The dramatic improvement of supercomputing performance has more recently enabled seismologists and earthquake engineers to more accurately understand the effects of source, wave propagation, and local site conditions on the ground motion.
Parallel computers, consisting of thousands of distributed processors, allow scientists and engineers to simulate larger problems than had been previously impossible. Parallel computing plays a crucial role in large scale simulations in a variety of physical problems. In earthquake
1MAC Lab, Dept. of Civil and Environmental Engineering, Carnegie Mellon University, ejk@cs.cmu.edu 2MAC Lab, Dept. of Civil and Environmental Engineering, Carnegie Mellon University, bielak@cs.cmu.edu 3MAC Lab, Dept. of Civil and Environmental Engineering, Carnegie Mellon University, oghattas@cs.cmu.edu

(a) (b)

(c)

FIG. 1. (a) Structured Grid (FDM, Pitarka 1999) (b) Unstructured Mesh (FEM, Bao et al. 1998) (c) Octree-Based Multiresolution Mesh (FEM, Kim 2003)

ground motion simulation, it enables one to model ground motion in large, highly heteroge-

neous basins, such as the Los Angeles (LA) basin with reliable resolution.

An earthquake ground motion simulation entails solving numerically the partial differential

equations (PDE) of elastodynamic wave propagation. There are several numerical methods

available for ground motion simulations. The finite difference method (FDM) (e.g., Graves

1996; Olsen and Archuleta 1996; Pitarka 1999), the boundary element method (BEM) (e.g.,

Kawase 1988) and the finite element method (FEM) (e.g., Bao et al. 1998) are commonly used.

In seismology and earthquake engineering, the FDM has been the most popular technique due

to its satisfactory accuracy and ease of implementation. However, the shortcoming of the FDM

is that in its standard form it uses a regular grid even in the presence of highly heterogeneous

materials. Considering alternative methods, the main advantage of the BEM is its unique ability

to provide a complete solution in terms of boundary values only, with substantial savings in

modeling effort. But the BEM is not an adequate technique for ground motion simulations

in large scale and highly heterogeneous domains, because the systems of algebraic equations

become dense as it requires that the domain should be divided into a large number of smaller

homogeneous subdomains within each domain. On the other hand, the FEM is a very common

numerical technique for solving PDEs in boundary value problems. FEM has the advantage of

sparseness and adaptivity, and memory use. In this

but it study,

has the

sOevcterreael-dBiassaeddvaMntualgteirse:somluetsihoinngMeefsfohrtM, ceothmopdu(taOtiBonMt£ i)mies

introduced in an attempt to make improvements in all these areas.

PARALLEL OCTREE-BASED MULTIRESOLUTION MESH METHOD The FEM based on an unstructured mesh has many advantages when modeling numerically
various physical phenomena on complex domains in terms of geometry and material properties. Since the mesh in the computational domain is created adaptively according to the geometry and material properties, the numerical methods for unstructured meshes, such as FEM, enable one to improve the resolution and/or enlarge the computational domain retaining the computer capacity. Even though the FEM with its unstructured mesh capabilities is superior to the FDM that employs uniform grid structure, there is need to store large sparse matrices in order to solve the governing PDEs. Having to handle a large sparse matrix results in reducing either the

2

resolution of the problem or the computational domain size due to the memory limitation of

computers in large To resolve this

spcraolbe lseimm,utlhateioOnsB. M ¢

is presented here for the spatial discretization of the

domain into regular hexahedral elements. An `octree' is a tree data structure in 3-D analogous

to the quadtree in 2-D. The basic characteristic of the octree-based multiresolution mesh is

to recursively subdivide an element in the domain into 8 subelements until the criterion for

refinement is satisfied while building the octree data structure. This criterion is taken to be the

shear wave velocity within the medium.

7 77 7 77

Compatibility: Constraint Equations

¦¤ ¨¥ §

¤© ¤ 

(1)

¤ ¨¥ §

¤ ©  ¤! ¤" $¤ # %

(2)

legal 5 (0)1 & (02 ' 4( 31 &
6 illegal ' slave node & master node

u31

( &7

27' 7

u7 1) 7

&

( '7

27' 7

7 7

' u91

&7

7' u7 18 & 77

777

FIG. 2. Quadtree and Octree-Based

Mesh

Since the OBM¢ is based on the octree algorithm, all the elements in OBM¢ are geometri-

uacanklsletyryusacidmtuvrialeandrt;amgeexescohef,ptOthtBehaMgte¢toh.meIynetsvrtiaecraydsiimonfitlahasresiiterymsoibzfleiO.ngBThtMhise¢

geometric similarity of each element is global stiffness matrix in FEM with an allows every element to be represented

by a single normalized element stiffness matrix
explicit time integration. @ A is a pre-calculated

@B%DA C

th r% oeulgehmeelnetmmeanttr-ibxyo-eflaemcuebnitcceallecmuleantitowniothf

3 degrees of freedom on each node. @EA factors out terms related to material properties and is

normalized with respect to the element size; it is then applicable to all elements. We empha-

size that an important characteristic of this approach is that it is unnecessary to store individual

stiffness matrices for each element within the mesh. Furthermore, frequent reference of @ A

by each element during element-by-element calculation at every time step leads @ A to reside

in cache rather than memory. This cache-friendly algorithm improves overall computational

performance considering two issues; (1) performance of cache (6ns) is 10 times faster than that

of memory (60ns) and (2) matrix-vector product operation consumes most of computation time

in solver.

Our goal in wave propagation simulation is to solve the Navier's equation,

dWwihshceerrneetOiuzeBisMtht¢ihsiesecdqoiusnapstltiaroucnceHF tmbedPGaes,nI QSetadTR pvF ooeU¤ctnetonthrt,ieaF ilO¤diicVXsstcrYW teohaene-tB`idnaeRTuFbsniestydiG ty¤iMnWdafctunhPe ldet irdQeghissopa§pnlluadi¦ctie!qoqt` mnrs a¤ervMrneu teoLschacmuMre´secathotonthdseta(inOnttsBe.rMfaW(¢c3)ee).
between two elements having different size in adjacency as shown in Figure 2. To ensure continuity, we apply an additional constraint condition via Equation 1 and 2. For parallel implementation of QUAKE, MPI is used for message passing library. Figure 3 shows the

3

framework for large scale ground motion simulations. It consists of an out-of-core mesher, a parallel partitioner and a parallel solver. It thus enables us to perform any ground motion simulations that meet total memory size of available computer systems.

Velocity Model (SCEC model)

Mesh Generation (Euclid)

Mesh Partition (ParMETIS)

Visualization

Parallel Octree-Based FEA Solver (QUAKE)
PE 1 PE 2 PE 3 PE 4

FIG. 3. Framework for large scale wave propagation simulation

FIG. 4. Element partitioning (64 PEs)

GROUND MOTION SIMULATION
As an illustration of the OBMw methodology, we next apply it to the simulation of the 1994 Northridge earthquake (Wald et al. 1996). The model size is 80 x 80 x 30 kmw cover-
ing the greater LA area (Magistrale et al. 2000). The numerical model has approximately 80
million hexahedral elements of OBMw and 100 million grid points with 1 Hz resolution and
minimum shear wave velocity of 100 m/s. The simulation has been run on 2048 processors of HP-Compaq AlphaServer Cluster at the PSC. Figure 5(a) shows the distribution of the shear wave velocity on the free surface and on a vertical cross-section of the model. Figures 5(b) and
5(c) show simulation results of OBMw compared with Archimedes (ARCHM) with target reso-
lution of 0.5 Hz. Archimedes (Shewchuk and O'Hallaron 1998), developed by QUAKE group at Carnegie Mellon University, is a parallel tool for wave propagation simulation employing unstructured mesh FEM with tetrahedral elements, that has already been verified with several FDM codes. Even though Archimedes shows good performance in ground motion simulation, difficulty of handling extremely large scale models in term of meshing, partitioning and storing
large sparse matrices in core of solver, motivated the development of OBMw . We can notice
why high resolution simulation is required through Figures 5(b) and 5(c). Figure 6 shows the rupture propagation of velocity on the fault plane. Figure 7 shows wave propagation on the top surface with time history. The red dot in Figures 6 and 7 represents the hypocenter and epicenter, respectively. Figure 8 shows the snapshot of wave propagation on the top surface at 11 sec after the initiation of the event. The black lined rectangle in Figures 7 and 8 represents the projection of the fault plane onto the top surface.
PERFORMANCE AND SCALABILITY We achieve excellent performance and scalability of QUAKE with 1 to 2048 processors on
HP-Compaq AlphaServer Cluster at PSC. Table 1 shows details about the performance for the Northridge earthquake simulation in the LA basin with highest resolved frequencies ranging from 0.1 Hz to 1 Hz, corresponding to a range of problem sizes from 134,500 to over 100
4

Distance (km): N-S

80

Free Surface Shear Wave Velocity A'

Vs (m/s)400

70 350
60 300
50

40 250

30 200

20

150 10

A
10 20 30 40 50 60 70 80 Distance (km): E-W

100

0

Section A - A'

Vs (m/s)4500

5 3500
10 15 2500

20 1500

25 500

10 20 30 40 50 60 70 80 Distance (km): E-W

velocity (m/sec) velocity (m/sec) velocity (m/sec) velocity (m/sec)

JFP (0.5 Hz) 2 QUAKE
ARCHM 0 -2 0 10 20 30 40 2
0
-20 10 20 30 40 1
0
-10 10 20 30 40 time (sec)

JFP (1 Hz) 2 QUAKE
ARCHM 0 -2 0 10 20 30 40 2
0
-20 10 20 30 40 1
0
-10 10 20 30 40 time (sec)

TAR (0.5 Hz)

TAR (1 Hz)

0.2 0.2
00
-0.2 -0.2
-0.40 10 20 30 40 -0.40 10 20 30 40 0.2 0.2

00

-0.20 10 20 30 40 -0.20 10 20 30 40 0.2 0.2

00

-0.20 10 20 30 40 -0.20 10 20 30 40

time (sec)

time (sec)

Depth (km)

(a) (b)

(c)

FIG. 5. (a) S-wave velocity distribution in the LA basin model (b) Result comparison of 0.5 Hz and 1 Hz model at JFP (c) Result comparison of 0.5 Hz and 1 Hz model at TAR

FIG. 6. Rupture Prop- FIG. 7. Wave Propagation FIG. 8. Max. Wave Propaga-

agation: Velocity

on the Top Surface

tion on the Top Surface

million grid points. A relative efficiency of 88 % is achieved with 2048 PEs. The relative efficiency is calculated based on the sequential performance of LA10S and CPU efficiency is determined based on the theoretical peak performance (2 GFLOPS/s) of a processor of HPCompaq AlphaServer Cluster.

TABLE 1. Performance Analysis and Comparison of QUAKE

Model PEs
Grid Pts MFLOPS/s Rel. Effcny CPU Effcny

LA10S 1
134,500 505
100 % 25.3 %

LA5S 16
618,672 491 97 %
24.6 %

LA2S 128 14,792,064 469 93 % 23.5 %

LA1Hy
512 47,556,096
451 89 % 22.6 %

LA1H
1024 101,939,200
450 89 % 22.5 %

LA1H
2048 101,939,200
443 88 % 22.2 %

5

CONCLUDING REMARKS

We have introduced a multiresolution finite element methodology for large-scale ground

motion Greater

sLimosuAlantigoenlsesanbdasisnucocfe4ss fuC lly4

peCrf o rmkmed¢

the Northridge earthquake simulation in the domain size with approximately 300 million

DOF, frequency range of 0 Hz - 1 Hz and minimum shear wave velocity of 100 m/s. The

framework developed in this study allows us to perform extremely large-scale ground motion

simulations and to get more accurate and reliable results with a higher resolution simulation.

Our of 0

next Hz -

target is to perform a ground motion simulation
2 Hz and bigger computational domain (e.g.  4

inC

the
 4 

LC A4b aksimn

¢w),itwhhfricehquweinlcl yinrvaonlgvee

a billion DOF problem. We believe that the simulation allows us to better understand the

earthquake-induced ground motion with higher frequency level becoming closer to the realistic

ground motion.

ACKNOWLEDGMENT This work was supported by the National Science Foundation's Knowledge and Distributed
Intelligence (KDI) programs through grant number CMS-9980063 (Dr. Clifford J. Astill is the cognizant official). Partial support was also provided by the Southern California Earthquake Center (SCEC). Computing resources on the HP-Compaq AlphaServer cluster system are provided by the Pittsburgh Supercomputing Center under award BCS020001P.

REFERENCES
Bao, H., Bielak, J., Ghattas, O., Kallivokas, L., O'Hallaron, D., Schewchuk, J., and Xu, J. (1998). "Large-scale simulation of elastic wave propagation in heterogeneous media on parallel computers." Computer methods in applied mechanics and engineering, 152, 85­102.
Graves, R. (1996). "Simulating Seismic wave propagation in 3D elastic media using staggeredgrid finite differences." Bull. Seism. Soc. Am., 86, 1091­1106.
Karypis, G., Schloegel, K., and Kumar, V. "ParMETIS 3.0. http://www-users.cs.umn.edu/ 
karypis/metis/parmetis/. Kawase, H. (1988). "Time-Domain response of a semi-circular canyon for incident SV, P, and
Rayleigh waves calculated by the discrete wavenumber boundary element method." Bull. Seism. Soc. Am., 78, 1415­1437. Kim, E. J. (2003). "Parallel Octree-Based Multiresolution Mesh Method for Large Scale Earthquake Ground Motion Simulation," PhD thesis, Dept. of Civil and Environmental Engineering, Carnegie Mellon University. Magistrale, H., Day, S., Clayton, R., and Graves, R. (2000). "The SCEC Southern California reference three-dimensional seismic velocity model version 2." Bull. Seism. Soc. Am., 90, S65­S76. Olsen, K. and Archuleta, R. (1996). "Three-Dimensional simulation of earthquakes on the Los Angeles fault system." Bull. Seism. Soc. Am., 86, 575­586. Pitarka, A. (1999). "3D Elastic Finite-Different Modeling of Seismic Motion Using Staggered Grids with Nonuniform Spacing." Bull. Seism. Soc. Am., 89, 54 ­ 68.
Shewchuk, J. R. and O'Hallaron, D. R. (1998). "Archimedes, www.cs.cmu.edu/ 
quake/archimedes.html". Tu, T., O'Hallaron, D., and Lo´pez, J. (2002). "Etree: A database-oriented method for generat-
ing large octree meshes." Proceedings of the Eleventh International Meshing Roundtable. Wald, D., Heaton, H., and Hudnut, K. (1996). "The slip history of the 1994 Northridge, Cal-
ifornia, Earthquake determined from strong-motion, teleseismic, GPS, and Leveling data." Bull. Seism. Soc. Am., 86, S49­S70.

6

