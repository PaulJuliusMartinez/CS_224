From: KDD-95 Proceedings. Copyright © 1995, AAAI (www.aaai.org). All rights reserved.

Extracting Support Data for a Given Task
Bernhard SchiilkopP Chris Burgest Vladimir Vapnik
AT&T Bell Laboratories 101 Crawfords Corner Road
Holmdel NJ 07733 USA
schoelObig.att.com cjcbObig.att.com vladObig.att.com

Abstract
We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algo rithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small (k: 4%) subsets of the data base. This finding opens up the possibiiity of compressing data bases significantly by disposing of the data which is not important for the solution of a given task.
In addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases
where the amount of available data is limited.

Introduction
Learning can be viewed as inferring regularities from a set of training examples. Much research has been devoted to the study of various learning algorithms which allow the extraction of these underlying regularities. No matter how different the outward appearance of these algorithms is, they all must rely on intrinsic regularities of the data. If the learning has been successful, these intrinsic regularities will be captured in
the values of some parameters of a learning machine;
for a polynomial classifier, these parameters will be the coefficients of a polynomial, for a neural net they will be the weights and biases,and for a radial basis function classifier they will be weights and centers. This variety of different representations of the intrinsic regIuM?la-"-lrl-litie^asC-,-"-h,-l-ol`-lwl"ellve-Tr-,"-"Acbo.ncealsthe fact that they all stem
In the present study, we explore the Support Vector Algorithm, an algorithm which gives rise to a number

*permanent address: Max-Planck-Institut

fiir biolo-

gische Kybernetik, Spemannstrafle 38, 72076 Tiibingen,

Germany

`supported by ARPA under ONR contract number

N00014-94-G-0186

of different types of pattern classifiers. We show that the algorithm allows us to construct different classifiers (polynomial classifiers, radial basis function classifiers, and neural networks) exhibiting similar performance and relying on almost identical subsets of the training set, their support vector seZs.In this sense,the support vector set is a stable characteristic of the data.
In the csse where the available training data is limited, it is important to have a means for achieving the
best possible generalization by controlling characteristics of the learning machine. We use a bound of sta-
tistical learning theory (Vapnik, 1995) to predict the degreewhich yields the best generalization for polynomial classifiers.
In the next Section, we follow Vapnik (1995), Baser, Guyon & Vapnik (1992), and Cortes & Vapnik (1995) in briefly recapitulating this algorithm and the idea of Structural Risk Minimization that it is based on. Following that, we will present experimental results obtained with support vector machines.

The Support Vector Machine
Structural Risk Minimization
For the caseof two-class pattern recognition, the task of learning from examples can be formulated in the following way: given a set of functions

{ja : a E A}, ja : RN + (-l,+l}

(the index set A not necessarily being a subset of R") and a set of examples

(x1,Yl),...,(w,w)~

xi E RN, w E (-1, +l},

each one generated from an unknown probability distribution Hr. ul. we want to find a-. f-u--n-c~t~io-n---,f".4 which
provides th&&ll&t p&ible value for the risk

R(a)= J KY(x)- YIdP(x,Y).

The problem is that R(a) is unknown, since P(x, y) is unknown. Therefore an induction principle for risk
minimization is necessary.

252 KDD-95

The straightforward approach to minimize the empirical risk

turns out not to guarantee a smaii actuai risk (i.e. a small error on the training set does not imply a small error on a test set), if the number 4 of training examples
is limited. To make the most out of a limited amount of data, novel statistical techniques have been developed
during the last 25 years. The Structural Risk Minimization principle is such
a technique. It is based on the fact that for the above learning problem, for any (YE A with a probability of at least 1 - q, the bound

`R"\Ia,\ <2 -Rc-m--jlJ\~\s,Iw-h\-eI log.to',
holds, 9 being defined as

(1)

qh lcdd=) JJI?' e

h (log * + 1) - lo&/4) e

.

The parameter h is called the VC-dimension of a set of functions. It describes the capacity of a set of functions implementable by the learning machine. For binary classification, h is the maximal number of points k which can be be separated into two classesin all possibie 2-"ways by using functions of the iearning machine; i.e. for each possible separation there exists a function which takes the value 1 on one class and -1 on the other class.
According to (l), given a fixed number J!of training examples one can control the risk by controlling two quantities: Rcmp(a) and h({j, : LYE A'}); A' denoting some subset of the index set A. The empirical risk depends on the junction chosen by the learning machine
(i.e. on a), and it can be controlled by picking the right cr. The VGdimension h depends on the set of junctions (jc : (Y E A') which the learning machine can implement. To control h, one introduces a structure of
nested subsets S,, := {fa : cyE A,,} of {fa : CYE A},

Sl c s2 c . . . c stl c . . . ,

(2)

with the corresponding VC-dimensions satisfying

hI 5 h2 5.. .I h, 5 . . .

For a given set of observations (xi, yi), .... (xc, yl) the !Structural Risk Minimization principle chooses the
function fa; in the subset {fa : cr E A,} for which
the guaranteed risk bound (the right hand side of (1)) is n&imal.

The Support Vector Algorithm

A Structure on the Set of Hyperplanes.

Each

particular choice of a structure (2) gives rise to a

learning algorithm. The support vector algorithm is

based on a structure on the set of hyperplanes. To de-

scribe it, first note that given a dot product space Z

and a set of vectors xl,. . . ,x, E Z, each hyperplane

{xEZ:(w.x)+k

0) corresponds uniquely to a

pair (w, a) E Z x R if we additionally require

i=~,~,r!(W*Xi)

+ b[ =I-

(3)

Each hyperplane corresponds to a decision function constructed in the following way: we first find the smallest ball Bx~,...,x, = {x E Z : 11x- aI1 < R} (a E Z) containing the points xl,. . . , xp. Then we define a decision function fw,b by

fW,b : Bxr I..., X, + {flh

fW,b = %n ((W - X) + a).

(4)

The possibility of introducing a structure on the set of

hyperplanes is baaed on the fact (Vapnik, 1995) that

the set {j;H,b : iiwii 5 A) ha8 a Vcdimension ir sati&

fying

h < R2A2.

(5)

Note. Dropping the condition llwll 5 A leads to a set

of functions whose VC-dimension equals N + 1, where

N is the dimensionality of Z. Due to llwll 5 A, we can

get VC-dimensions which are much smaller than N,

enabling us to work in very high dimensional spaces.

The Support Vector Algorithm.

Now suppose we

aregivenasetofexamples(xl,yl),...,(xt,yl),

XiE

RN;w E {-1: tl]: and we want to find a decision

pction fw,? with the property fW+,b(qi)= 2/i, i .=

, . . . , f?.If this function exrsts, canomcahty (3) imphes

Yi((W *Xi) + 6) 2 1, i = 1,..`, e.

(6)

In many practical applications, a separating hyperplane does not exist. To allow for the possibility of examples violating (6), Cortes & Vapnik (1995) introduce slack variables

(i>O, i=l,..., J,

(7)

to get

&((W *Xi) + 6) 1 1 - &, i = 1,...I 1. (8)

The support vector approach to minimizing the guaranteed risk bound (1) consists in the following: mini-
mize

subject to the constraints (7) and 1;;. According to (5), minimizing the first term amounts to minimizing the VC-dimension of the learning machine, thereby minimizing the second term of the bound (1). The
term cf=, &, on the other hand, is an upper bound on the number of misclassifications on the training set this controls the empirical risk term in (1). For a suitable positive constant 7, this approach therefore constitutes a practical implementation of Structural Risk Minimization on the given set of functions.
Schtilkopf 253

Introducing Lagrange multipliers cy( and using the Kuhn-Tucker theorem of optimization theory one can show that the solution has an expansion

C
w= c YiWXi, id

(10)

with nonzero coefficients oi only for the caseswhere the corresponding example (xi, vi) precisely meets the con-
straint (8). These xi are called Support Vecdors,and (10) is the Support Vector Expansion. All the remaining examples xj of the training set are irrelevant: their
constraint (8) is satisfied automatically (with <j = 0), and they do not appear in the support vector expan-
sion. Although the solution w is unique, the coefficients oi are not. They can be found by solving the following quadratic programming problem: maximize

c

W(*)=&ui-i
i=l

C W!/jWaj(Xi * Xj) (11) i,j=l

subject to

Osaisr,

i=l,.*., 1, and koigi = 0. (12)

i=l
By linearity of the dot product, the decision function (4) can thus be written as

f(x)=sgn

&/iCEi*(X*Xi)+b

.

( i=l )

So far, we have described linear decision surfaces.

These are not appropriate for all tasks. To allow

for much more general decision surfaces, one can first

nonlinearly transform the input vectors into a high-

dimensional feature space by a map 4 and then do

a linear separation there. Maximizing 11) then re-

quires the computation of dot productb I4(x) .4(Xi)) in a high-dimensional space. Under certain conditions

(Boser, Guyon & Vapnik, 1992), these expensive calcu-

lations can be reduced significantly by using a function

K such that

(4(x) *4(xi)) = K(x, pi)*

We thus get decision functions of the form

f(x) = sgn eloi
i=l

*K(x, xi) + 6 . (13)

Experimental Results

In our experiments, we used the support vector al~anVr;&thmaV"&w*.A.itah"*. Yata"nUc.-laaUrcl-.. ymIrUart"r"d`i"rY." ynm'.msbrm'"n"m"ain'tLr "b tI."w"h*-niques to construct three different types of classifiers. This was done by choosing three different functions K in the decision function (13) and in the function to be maximized under the constraint (12),

c

C W.t/jaiajK(xi,xj)*
i,j=l

(14

Homogeneous polynomial classifiers:
K(x,Xi) = (X*Xi)degree
Radial Basis Function (RBF) classifiers:
K(x,Xi)= "xp(-IIX-Xi112/U2)
Neural networks: K(x, xi) = tanh(K * (X *xi) - 0)
All the results reported in this paper were obtained with y = 10 (cf. (9)). We used a US postal database of 9300 handwritten digits (7300 for training, 2000 for testing) (cf. LeCun et al., 1990). Each digit is a 16 x 16 vector with entries between -1 and 1. Preprocessing consisted in smoothing with a Gaussian kernel of width u = 0.75.
To get the ten-class classifiers for the digit recognition problem, we constructed ten two-class classifiers, each trained to separate a given digit from the other nine, and combined them by doing the ten-class classification according to the maximal output (before applying the sgn function) among the two-class classifiers.

Performance

for Various Types of

VPla,,Qc.P:aU-IwUszI cl

The results for the three different functions K are sum-

marized in Table 1. They should be compared with

values achieved on the same database with a five-layer

neural net (LeCun et al., 1990), 5.1%, a two-layer

neural net, 5.9%, and the human performance, 2.5%

(Bromley & Szckinger, 1991).

Table 1: Performance for three different types of classifiers, constructed with the support vector algorithm by choosing different functions K in (13) and (14). Given are raw errors (i.e. no rejections allowed) on the test set. The normalization factor c = 1.04 in the sigmoid caseis chosen such that c. tanh(2) = 1. For each of the ten-class-classifiers, we also show the average number of support vectors of the ten two-clas~classifiers.

polynomial:
degree raw error % av. # of SVs

K(x, y) = ((x . y)/256) degree 123456 8.9 4.7 4.0 4.2 4.5 4.5 282 237 274 321 374 422

RBF: K(x, y) = exp (-[lx - y112/(256 u"))

CT2 1.0 0.8 0.5 0.2 0.1

raw error/%

4.7 4.3 4.4 4.4 4.5

av. &of SVn

2.14 2.15 2.51 366 722

sigmoid: K(x, y) = 1.04tanh(2(x . y)/256 - 0)

8 0.9 1.0 1.2 1.3 1.4

raw error %

4.8 4.1 4.3 4.4 4.8

av. #of SVs

242 254 278 289 296

The similar performance for the three different functions K suggests that the choice of the set of decision

254 KDD-95

Table 2: Performance of the classifiers with degree predicted by the VC-bound. Each row describes one two-class-classifier separating one di~$ (stated in the first column) from the rest. The remaining columns contain:
de ree: the degree of the best polynonual as predicted by the described procedure, parameters: the dimensionality Ao-ht- e lgh dimensional space, which is also the maximum possible V&dimension for hnear classifiers in that space, hertim.: the VC dimension estimate for the actual classifiers, which is much smaller than the number of free parameters of linear classifiers in that space, 1 - 7: the numbers of errors on the test set for polynomial classifiers of degrees 1 through 7. The table shows that the decribed procedure chooseapolynomial degrees which are optimal
or close to optimal.

chosen classifier digit degree parameters herrim.

errors on the tl 3t set for degrees 1 - 7 1 2 3 41 51 61 7

0 3 2.8 - lo6 547 36 14 1111 11

1 7 1.5 * 101z 95 17 15 14 11

q2 3 2.8 - lo6 832 53 32

26

3 3 2.8 - lo6 1130 57 25 1221

44 53 64 I7n 5

1.8.10s

977 50 32 32

2.8. lo6

1117 37 20 pJ

1.8.10"

615 23 12 12

I 9.5 - log I 526 1125 1 15 1 12

24 10

8

4

1.8 - 10s

1466 71 33 28

9 5 9.5 * log 1210 51 18 15

functions is less important than capacity control in the chosen type of structure. This phenomenon is wellknown for the Parzen window method for density estimation: there, the choice of the Kernel function is less critical than the choice of the appropriate value of the bandwidth parameter for a given amount of data.
Predicting the Optimal Decision Functions
Given a set of training data, for some choices of the nested set of decision functions (2) one can estimate the second term on the right hand side of Eq. (1). We will call the latter term the "VC-confidence". In these cases, since one can also measure the empirical risk R e,,,,,(~), one can actually compute upper bounds on the expected risk R(cr) (i.e. the expected error on the test set). We can try to use these bounds to predict, which set of decision functions will give the best performance on test data, without the use of any validation or test sets. Clearly this could be very useful in situations where the amount of available data is very limited.
To test this idea we concentrated on the polynomial
classifiers with K(x, y) = (((x . y) + 1)/256)degree. For these decision functions, the nested structure (2) is implemented by varying the degree of the polynomial. We estimated the VC dimension h by making

the assumption that the bound (5) is met, that is,
h N hertim. = R211wl12s
In this case, the right hand side of Equation (1) is dominated by the VC-confidence, which is minimized when the VC-dimension is minimized. Now llwll is determined by the support vector algorithm; so in order to estimate h, we need to compute R.
Recall that R is the radius of the smallest sphere enclosing the training data in the high dimensional space (the space in which K corresponds to a dot product). We formulate the problem as follows: Minimize R2 subject to IlXi - X*11' 5 R2 where X* is the (to be determined) position vector of the center of the sphere. This is a well known quadratic programming problem. We use the objective function t2 -xi &( R2 - (Xi -X*)2) and vary R and X* to get
* = xi &Xi and the Wolfe dual problem: Maximize
cXi'(Xi*Xi)-CXiAj*(Xi*Xj)
i i,j
subject to
i where the & are Lagrange multipliers. As in the support vector algorithm, this problem has the property
SchSlkopf 255

8Vmlgv8c-dh. 204

toulno.oft8aemnY t

degree: 2 3 4 5 6 7
Figure 1: Average VC-dimension (solid) and total number of test errors of the ten two-class-classifiers (dotted) for polynomial degrees2 through 7 (for degree 1, Rempis comparably big, so the VC-dimension alone is not sufficient for predicting R, cf. (1)). The baseline on the error scale, 174, correspondsto the total number of test errors of the ten best two-class-classifiers out of the degrees2 through 7. The graph shows that the VC-dimension allows us to predict that degree4 yields the best overall performance of the two-class-classifiers on the test set. This is not necessarily the case for the performances of the ten-class-classifiers, which are built from the two-clsss-classifier outputs before applying the sgn functions. The fact that indeed our ten-class-classifier with degree 3 so far performs better that the one with degree4 (see above) leads us to believe that the latter can be improved by using a more principled way of doing ten-class-classification.
that the Xi appear only in dot products, so as before one can compute the dot products in the high dimensional spaceby replacing (Xi *Xi) by K(xi,Xj) (where the xi live in the low dimensional space, and the Xi in the high dimensional space), provided the K functions satisfy certain positivity conditions. In this way, we computed the radius of the minimal enclosing sphere for all the training data for polynomial classifiers of degrees1 through 7.
We then trained a binary polynomial classifier for each digit, for degrees1 through 7. Using the obtained values for our approximation to h, we can predict, for each digit, which degree polynomial will give the best generalization performance. We can then compare this prediction with the actual polynomial degree which gives the best performance on the test set. The results are shown in Table 2; cf. also Fig. 1. The above method for predicting the optimal classifier functions gives good results. In four casesthe theory predicted the correct degree;in the other cases,the predicted degree gave performance close to the best possible one.
Comparison of the Support Vector Sets for Different Classifiers
In the remainder of the paper we shall use the optimal parameters according to Table 1 in studying the sup-
256 KDD-95

port vector sets for the three different types of classifiers. Table 3 shows that all three classifiers use around 250 support vectors per two-class-classifier (less than 4% of the training set). The total number of different support vectors of the ten-class-classifiers is around 1600. The reason why it is less than 2500 (ten times the above 250) is the following: one particular vector that has been used as a positive example (i.e. gi = +l in (13)) for digit 7, say, might be a good negative example (vi = -1) for digit 1.

Table 3: First row: total number of different support vectors of three different ten-class-classifiers (i.e. number of elements of the union of the ten two-classclassifier support vector sets) obtained by choosing different functions K in (13) and (14); second row: average number of support vectors per two-clsss-classifier.

Polynomial RBF Sigmoid

total # of svs

1677 1498 1611

average* of SVs 274 235 254

Tables 4-6 show how many elements the support vector sets of the different classifiers have in common. As mentioned above, the support vector expansion (10) is not unique. Depending on the way the quadratic programming problem is solved, one can get different expansions and therefore different support vector sets. We used the same quadratic programming algorithm and the same ordering of the training set in all three cases.
Table 4: Percentage of the support vector set of [column] contained in the support set of [row]; ten-class clsssifiers.

Table 5: Percentage of the support vector set of [column] contained in the support set of [row]; only one classifier (digit 7) each

Polynomial RBF
Sigmoid

Polynomial 100

RBF Sigmoid 84 93 100 92 86 100

shown that the support vector set contains all the information a given classifier needs for constructing the decision function. Due to the overlap in the support vector sets of different classifiers, one can even train
classifiers on support vector sets of another classifier. Table 7 shows that this leads to results comparable to
those after training on the whole database.

Table 6: Comparison of all three support vector sets at a time. For each of the (ten-class) classifiers, % intersection gives the fraction of its support vector set shared with both the other two classifiers. Out of a total of 1834different
support vectors, 1355 are shared by all three classifiers; an additional 242 is common to two of the classifiers.

Polynomial RBF Sigmoid intersection shared by 2 union

no. of supp. vectors % intersection

1677 1498 1611 81 90 84

1355 100

242 1834 --

Table 7: %aining classifiers on the support vector sets of other classifiers leads to performances on the test set which are as good as the results for training on the full data base (shown are numbers of errors on the 2000-clement test set, for two-class classifiers separating digit 7 from the rest). Additionally, the results for training on a random subset of the data base of size 200 are displayed.

Conclusion
We have shown that
the support vector algorithm allows the construction of various learning machines, all of which are performing similarly well,
we were able to predict the degree of polynomial classifiers which leads to high generalization ability, and
for the considered classification problem (the US postal handwritten digit database) the three constructed learning machines (polynomial classifiers, neural nets, radial basis function classifiers) construct their decision functions from highly overlapping subsets of the training set.
Considering that in our experiments these support uector sets constituted less than 4% of the whole training set (for binary classification), the latter point indicates the possibility of a substantial data compression. Whether this holds true for various other real-life problems is an open and interesting question.

References
Boser, B. E.; Guyon, I. M.; and Vapnik, V. 1992. A traming algorithm for optimal margin classifiers. Fifth Annual Workshop on Computational Learning Theory, Pittsburgh ACM 144-152.
Bromley, J.; and SHckinger, E. 1991. Neural-network and k-nearest-neighbor classifiers. Technical Report 11359-910819-16TM, AT&T.
Cortes, C.; and Vapnik, V. 1995. Support Vector Networks. To appear in Machine Learning.
Le Cun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. J. 1990. Handwritten digit recognition with a backpropagation network. Advances in Neural Information Processing Systems, Vol. 2, 396-404, Morgan Kaufman.
Vapnik, V. 1995. The Nature of Statistical Learning Theory. Springer Verlag, forthcoming.

Schilkapf

257

