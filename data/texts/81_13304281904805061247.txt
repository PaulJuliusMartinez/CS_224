Bayesian Learning via Stochastic Gradient Langevin Dynamics

Max Welling

welling@ics.uci.edu

D. Bren School of Information and Computer Science, University of California, Irvine, CA 92697-3425, USA

Yee Whye Teh

ywteh@gatsby.ucl.ac.uk

Gatsby Computational Neuroscience Unit, UCL, 17 Queen Square, London WC1N 3AR, UK

Abstract
In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.
1. Introduction
In recent years there has been an increasing amount of very large scale machine learning datasets, ranging from internet traffic and network data, computer vision, natural language processing, to bioinformatics. More and more advances in machine learning are now driven by these large scale data, which offers the opportunity to learn large and complex models for solving many useful applied problems. Recent successes in large scale machine learning have mostly been optimization based approaches. While there are sophisticated algorithms designed specifically for certain types of models, one of the most successful class of algorithms are stochastic optimization, or Robbins-Monro, algorithms. These algorithms process small (mini-
Appearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).

)batches of data at each iteration, updating model parameters by taking small gradient steps in a cost function. Often these algorithms are run in an online setting, where the data batches are discarded after processing and only one pass through the data is performed, reducing memory requirements drastically.
One class of methods "left-behind" by the recent advances in large scale machine learning are the Bayesian methods. This has partially to do with the negative results in Bayesian online parameter estimation (Andrieu et al., 1999), but also the fact that each iteration of typical Markov chain Monte Carlo (MCMC) algorithms requires computations over the whole dataset. Nevertheless, Bayesian methods are appealing in their ability to capture uncertainty in learned parameters and avoid overfitting. Arguably with large datasets there will be little overfitting. Alternatively, as we have access to larger datasets and more computational resources, we become interested in building more complex models, so that there will always be a need to quantify the amount of parameter uncertainty.
In this paper, we propose a method for Bayesian learning from large scale datasets. Our method combines Robbins-Monro type algorithms which stochastically optimize a likelihood, with Langevin dynamics which injects noise into the parameter updates in such a way that the trajectory of the parameters will converge to the full posterior distribution rather than just the maximum a posteriori mode. The resulting algorithm starts off being similar to stochastic optimization, then automatically transitions to one that simulates samples from the posterior using Langevin dynamics.
In Section 2 we introduce the two ingredients of our method: stochastic optimization and Langevin dynamics. Section 3 describes our algorithm and how it converges to the posterior distribution. Section 4 describes a practical method of estimating when our algorithm will transition from stochastic optimization to Langevin dynamics. Section 5 demonstrates our al-

Stochastic Gradient Langevin Dynamics

gorithm on a few models and Section 6 concludes.

2. Preliminaries

Let  denote a parameter vector, with p() a prior

distribution, and p(x|) the probability of data item

x given our model parameterized by . The posterior

idsi:strpi(bu|Xtio)nofp(a)set Ni=of1

N data p(xi|).

items X = {xi}Ni=1 In the optimization

literature the prior regularizes the parameters while

the likelihood terms constitute the cost function to

be optimized, and the task is to find the maximum a posteriori (MAP) parameters . A popular class

of methods called stochastic optimization (Robbins &

Monro, 1951) operates as follows. At each iteration t,

a subset of n data items Xt = {xt1, . . . , xtn} is given, and the parameters are updated as follows:

t

=

(

t 2

 log p(t)

+

N n

n )  log p(xti|t)

(1)

i=1

where t is a sequence of step sizes. The general idea is that the gradient computed on the subset is used to approximate the true gradient over the whole dataset. Over multiple iterations the whole dataset is used and the noise in the gradient caused by using subsets rather than the whole dataset averages out. For large datasets where the subset gradient approximation is accurate enough, this can give significant computational savings over using the whole dataset to compute gradients at each iteration.

To ensure convergence to a local maximum, in addition to other technical assumptions, a major requirement is for the step sizes to satisfy the property

 t = 
t=1

 2t < 
t=1

(2)

Intuitively, the first constraint ensures that parameters
will reach the high probability regions no matter how
far away it was initialized to, while the second ensures
that the parameters will converge to the mode instead
of just bouncing around it. Typically, step sizes t = a(b + t)- are decayed polynomially with   (0.5, 1].

The issue with ML or MAP estimation, as stochastic optimization aims to do, is that they do not capture parameter uncertainty and can potentially overfit data. The typical way in which Bayesian approaches capture parameter uncertainty is via Markov chain Monte Carlo (MCMC) techniques (Robert & Casella, 2004). In this paper we will consider a class of MCMC techniques called Langevin dynamics (Neal, 2010). As before, these take gradient steps, but also injects Gaussian noise into the parameter updates so that they do

not collapse to just the MAP solution:

t

=

(

 2

 log p(t)

+

N )  log p(xi|t)

+

t

i=1

t  N (0, )

(3)

The gradient step sizes and the variances of the injected noise are balanced so that the variance of the samples matches that of the posterior. Langevin dynamics is motivated and originally derived as a discretization of a stochastic differential equation whose equilibrium distribution is the posterior distribution. To correct for discretization error, one can take (3) to just be a proposal distribution and correct using Metropolis-Hastings. Interestingly, as we decrease  the discretization error decreases as well so that the rejection rate approaches zero. However typical MCMC practice is to allow an initial adaptation phase where the step sizes are adjusted, followed by fixing the step sizes to ensure a stationary Markov chain thereafter.

More sophisticated techniques use Hamiltonian dynamics with momentum variables to allow parameters to move over larger distances without the inefficient random walk behaviour of Langevin dynamics (Neal, 2010). However, to the extent of our knowledge all MCMC methods proposed thus far require computations over the whole dataset at every iteration, resulting in very high computational costs for large datasets.

3. Stochastic Gradient Langevin Dynamics

Given the similarities between stochastic gradient algorithms (1) and Langevin dynamics (3), it is natural to consider combining ideas from the two approaches. This allows efficient use of large datasets while allowing for parameter uncertainty to be captured in a Bayesian manner. The approach is straightforward: use Robbins-Monro stochastic gradients, add an amount of Gaussian noise balanced with the step size used, and allow step sizes to go to zero. The proposed update is simply:

t

=

(

t 2

 log p(t)

+

N n

n )  log p(xti|t)

+

t

i=1

t  N (0, t)

(4)

where the step sizes decrease towards zero at rates satisfying (2). This allows averaging out of the stochasticity in the gradients, as well as MH rejection rates that go to zero asymptotically, so that we can simply ignore the MH acceptance steps, which require evaluation of probabilities over the whole dataset, all together.

Stochastic Gradient Langevin Dynamics

In the rest of this section we will give an intuitive argument for why t will approach samples from the posterior distribution as t  . In particular, we will show that for large t, the updates (4) will approach Langevin dynamics (3), which converges to the posterior distribution. Let

N g() =  log p() +  log p(xi|)
i=1

(5)

be the true gradient of the log probability at  and

ht()

=

 log p()

+

N n

n  log

p(xti|)

-

g()

(6)

i=1

The stochastic gradient is then g()+ht(), with ht() a zero mean random variable (due to the stochasticity of the data items chosen at step t) with finite variance V (), and (4) is,

t

=

t 2

(g(t

)

+

ht(t))

+

t,

t  N (0, t)

(7)

There are two sources of stochasticity in (7): the in-

jected Gaussian noise with variance t, and the noise in

the

stochastic

gradient,

which

has

variance

(

t 2

)2V

(t).

The first observation is that for large t, t  0, and

the injected noise will dominate the stochastic gradient

noise, so that (7) will be effectively Langevin dynam-

ics (3). The second observation is that as t  0,

the discretization error of Langevin dynamics will be

negligible so that the MH rejection probability will ap-

proach 0 and we may simply ignore this step.

In other words, (4), (7) effectively define a nonstationary Markov chain such that the tth step transition operator, for all large t, will have as its equilibrium distribution the posterior over . The next question we address is whether the sequence of parameters 1, 2, . . . will converge to the posterior distribution. Because the Markov chain is not stationary and the step sizes reduce to 0, it is not immediately clear that this is the case. To see that this is indeed true, we will show that a subsequence t1 , t2 , . . . will converge to the posterior as intended so the whole sequence will also converge.

First fix an 0 such that 0 < 0  1. Since {t} satisfy

the step size t1 < t2 < · ·

·prsoupcherttyha(t2), wtt=se+t1sc+a1nfit nd

a subsequence 0 as s  .

Since large

etnhoeuinghjecsttehdentooitsaelaitnjeeaccthedstneopisise,indepttes=+nt1sd+e1nt,t

for 2,

between steps ts and ts+1 will be O( 0). We now

show that the total noise due to the stochasticity of

the gradients among these steps will be dominated by

the total injected noise. Since 0  1, we may take

t - ts 2  1 for t between ts and ts+1. Making the assumption that the gradient g(·) vary smoothly (e.g. they are Lipschitz continuous in the models in
Section 5), the total stochastic gradient is:

ts+1

t 2

(g(t)

+

ht

(t))

t=ts +1

=

0 2

g(ts

)

+

O(0

)

+

ts+1

t 2

ht(t

)

t=ts +1

(8)

Since the parameters did not vary much between ts and ts+1, the stochasticity in ht(t) will be dominated by the randomness in the choice of the mini-batches.

Assuming that these are chosen randomly and inde-

pendently, ht(t) for each t will be basically iid (if mini-batches were chosen by random partitioning of

the whole dataset, ht(t) will be negatively correlated

instead and will not change the results here). Thus

the

variance

of

ts+1
t=ts +1

t 2

ht

(t

)

is

 O( t

2t 4

)

and

=

0 2

g(ts

)

+

O(0)

+

O(tt=s+t1s+1

)
t2
4

=

0 2

g(ts

)

+

O(0

)

The last equation says that the total stochastic gra-
dient step is approximately the exact gradient step at
ts with a step size of 0, with a deviation dominated by O(0). Since this is in turn dominated by the total injected noise which is O( 0), this means that the sequence t1 , t2 , . . . will approach a sequence generated by Langevin dynamics with a fixed step size 0, so it will converge to the posterior distribution. Note also
that it will have infinite effective sample size.

The implication of this argument is that we can use stochastic gradient Langevin dynamics as an "anytime" and general-purpose algorithm. In the initial phase the stochastic gradient noise will dominate and the algorithm will imitate an efficient stochastic gradient ascent algorithm. In the later phase the injected noise will dominate, so the algorithm will imitate a Langevin dynamics MH algorithm, and the algorithm will transition smoothly between the two. However a disadvantage is that to guarantee the algorithm to work it is important for the step sizes to decrease to zero, so that the mixing rate of the algorithm will slow down with increasing number of iterations. To address this, we can keep the step size constant once it has decreased below a critical level where the MH rejection rate is considered negligible, or use this algorithm for burn-in, but switch to a different MCMC algorithm that makes more efficient use of the whole dataset later. These alternatives can perform better

Stochastic Gradient Langevin Dynamics

but will require further hand-tuning and are beyond the scope of this paper. The point of this paper is to demonstrate a practical algorithm that can achieve proper Bayesian learning using only mini-batch data.

4. Posterior Sampling
In this section we consider the use of our stochastic gradient Langevin dynamics algorithm as one which produces samples from the posterior distribution. We first derive an estimate of when the algorithm will transition from stochastic optimization to Langevin dynamics. The idea is that we should only start collecting samples after it has entered its posterior sampling phase, which will not happen until after it becomes Langevin dynamics. Then we discuss how the algorithm scales with the dataset size N and give a rough estimate of the number of iterations required for the algorithm to traverse the whole posterior. Finally we discuss how the obtained samples can be used to form Monte Carlo estimates of posterior expectations.

4.1. Transition into Langevin dynamics phase

We first generalize our method to allow for precon-

ditioning, which can lead to significant speed ups by

better adapting the step sizes to the local structure of

the posterior (Roberts & Stramer, 2002; Girolami &

Calderhead, 2011). For instance, certain dimensions

may have a vastly larger curvature leading to much

bigger gradients. In this case a symmetric precondi-

tioning matrix M can transform all dimensions to the

same scale. The preconditioned stochastic gradient

Langevin dynamics is simply,

()

t

=

t M 2

g(t) + ht(t)

+ t,

t  N (0, tM )

As noted previously, whether the algorithm is in the stochastic optimization phase or Langevin dynamics phase depends on the variance of the injected noise, which is simply tM , versus that of the stochastic gradient. Since the stochastic gradient is a sum over the current mini-batch, if its size n is large enough the central limit theorem will kick in and the variations ht(t) around the true gradient g(t) will become normally distributed. Its covariance matrix can then be estimated from the empirical covariance:

V (t)



V [ht(t)]



N2 n2

n (sti

- st)(sti

-

st)

(9)

i=1

where sti =  log of data item i at

p(xti|t) + iteration t

1 N



log

and st

p(t)

=

1 n

is tnhe
i=1

score sti is

the empirical mean.

Note that V (t) =

N2 n

Vs

,

where

Vs is the empirical covariance of the scores {sti}, so

scales as

N2 n

.

From this we see that the variance of

the

stochastic

gradient

step

is

2t N 4n

2

M

Vs

M

,

so

that

to

get the injected noise to dominate in all directions, we

need the condition

tN 2 4n

max(M

1 2

VsM

1 2

)

=





1

(10)

where max(A) is the largest eigenvalue of A. In other words, if we choose a stepsize such that the sample threshold   1, the algorithm will be in its Langevin dynamics phase and will be sampling approximately from the posterior.

We can now relate the step size at the sampling thresh-

old to the posterior variance via the Fisher informa-

tion, which is related to Vs as IF  N Vs, and to the posterior variance   IF-1. Using these relationships as well as (10), we see that the step size at the sam-

pling threshold is t 

4n N

min

(

).

Since Langevin

dynamics explores the posterior via a random walk,

using this step size implies that we need on the order

of N/n steps to traverse the posterior, i.e. we process

the whole dataset. So we see this method is not a

silver bullet. However, the advantage of the method

is its convenience: stochastic optimization smoothly

and automatically transitions into posterior sampling

without changing the update equation. Even without

measuring the sampling threshold one will enjoy the

benefit of protection against overfitting and the ability

to perform Bayesian learning. Measuring the sampling

threshold will only be important if one needs to faith-

fully represent the posterior distribution with a finite

collection of samples.

4.2. Estimating Posterior Expectations

Since 1, 2, . . . converges to the posterior distribution,

we can estimate the posterior expectation E[f ()] of

some erage

fu1 nctTion
T t=1

f () f (t)

by simply taking the sample av(as typically in MCMC, we may

remove the initial burn-in phase, say estimated using

the sampling threshold). Since f (t) is an asymptotically unbiased estimator for E[f ()], this sample aver-

age will be consistent. Observe however that because

the step size decreases, the mixing rate of the Markov

chain decreases as well, and the simple sample aver-

age will over-emphasize the tail end of the sequence

where there is higher correlation among the samples,

resulting in higher variance in the estimator. Instead

we propose to use the step sizes to weight the samples:

E[f ()]  Tt=1Tt=t1f(tt)

(11)

Stochastic Gradient Langevin Dynamics

noise variance
average rejection rate Log joint probability per datum Accuracy on test data

33

22

11

00

-1 -1

-2 -2

-3 -3

-1 0 1 2

-1 0 1 2

Figure 1. True and estimated posterior distribution.

100 100

00

-1-1

-2-2

-3-3

-4-4

-5-5

-6-6

-7-7

00

2 4 6 82 4 6 8
Number of iterations through whole dataset

1100

0.08.855

0.08.8

0.07.755

0.07.7 0.60.56500

Accuracy after 10 iterations Accuracy
Numbe00r..o55f iterations 1t1hrough who11le..55dataset 22

Figure 3. Average log joint probability per data item (left) and accuracy on test set (right) as functions of the number of sweeps through the whole dataset. Red dashed line represents accuracy after 10 iterations. Results are averaged over 50 runs; blue dotted lines indicate 1 standard deviation.

10-2

10-1

10-4 10-6
100

 noise
1
 noise
2
injected noise
102 104 iteration

10-2

106

10-3 10-8

10-6

10-4

step size

10-2

Figure 2. Left: variances of stochastic gradient noise and injected noise. Right: rejection probability versus step size. We report the average rejection probability per iteration in each sweep through the dataset.

Since


t=1

t

=

,

this

estimator

will

be

consistent

as well. The intuition is that the rate at which the

Markov chain mixes is proportional to the step size, so

that we expect the to be proportional

etffoectivtT=e 1satm, palnedsiztehaotf

{1, . each

.., t

T } will

contribute an effective sample size proportional to t.

5. Experiments

5.1. Simple Demonstration

We first demonstrate the workings of our stochastic gradient Langevin algorithm on a simple example involving only two parameters. To make the posterior multimodal and a little more interesting, we use a mixture of Gaussians with tied means:

1  N (0, 12) ;

2  N (0, 22)

xi



1 2

N

(1

,

x2 )

+

1 2

N

(1

+

2, x2)

where 12 = 10, 22 = 1 and x2 = 2. 100 data points are drawn from the model with 1 = 0 and 2 = 1. There is a mode at this parameter setting, but also a secondary mode at 1 = 1, 2 = -1, with strong negative correlation between the parameters. We ran the
stochastic gradient Langevin algorithm with a batch-

size of 1 and using 10000 sweeps through the whole dataset. The step sizes are t = a(b + t)- where  = .55 and a and b are set such that t decreases from .01 to .0001 over the duration of the run. We see from Figure 1 that the estimated posterior distribution is very accurate. In Figure 2 we see that there are indeed two phases to the stochastic gradient Langevin algorithm: a first phase where the stochastic gradient noise dominates the injected noise, and a second phase where the converse occurs. To explore the scaling of the rejection rate as a function of step sizes, we reran the experiment with step sizes exponentially decreasing from 10-2 to 10-8. In the original experiment the dynamic range of the step sizes is not wide enough for visual inspection. Figure 2(right) shows the rejection probability decreasing to zero as step size decreases.

5.2. Logistic Regression

We applied our stochastic gradient Langevin algorithm to a Bayesian logistic regression model. The probability of the ith output yi  {-1, +1} given the corresponding input vector xi is modelled as:

p(yi|xi) = (yixi)

(12)

where



are

the

parameters,

and

(z)

=

1 1+exp(-z)

.

The bias parameter is absorbed into  by including 1

as an entry in xi. We use a Laplace prior for  with a scale of 1. The gradient of the log likelihood is:

 

log

p(yi|xi)

=

(-yixi)yixi

(13)

while the gradient of the prior is simply -sign(), which is applied elementwise.

We applied our inference algorithm to the a9a dataset derived by (Lin et al., 2008) from the UCI adult dataset. It consists of 32561 observations and 123 features, and we used batch sizes of 10. Results from 50

Stochastic Gradient Langevin Dynamics

runs are shown in Figure 3, with the model trained on a random 80% of the dataset and tested on the other 20% in each run. We see that both the joint probability and the accuracy increase rapidly, with the joint probability converging after at most 10 iterations, while the accuracy converging after less than 1 iteration through the dataset, demonstrating the efficiency of the stochastic gradient Langevin dynamics.

5.3. Independent Components Analysis

In the following we will briefly review a popular ICA

algorithm based on stochastic (natural) gradient opti-

mization (Amari et al., 1996). We start from a proba-

bilistic model that assumes independent, heavy tailed

marginal distributions,

[] 
p(x, W ) = | det(W )| pi(wiT x) N (Wij; 0, )

i ij

(14)

where we have used a Gaussian prior over the weights.

It has been found that the efficiency of gradient descent

can be significantly improved if we use a natural gradi-

ent. This is implemented by post-multiplication of the

gradient with the term W T W (Amari et al., 1996). If

we

choose

pi(yi) =

1

4

cosh2

(

1 2

yi

)

with

yi

= wiT x,

we

get

D(W

=.

W

log[p(X, W )]

WTW )

=

N

I

-

N

tanh(

1 2

yn)ynT

W - W W T W

n=1

(15)

The term W T W acts like a preconditioning matrix (see

section 4.1), Mij,kl = ik(W T W )jl which is symmetric

under the exchange (i  k, j  l). It can be shown

that the inverse of M is given by M -1 =(W T W )-1,

and W

the TW

matrix

=

U

1 2

square root as U T if W T W =

M= U U T

 .

W T W with

The update equation for Langevin dynamics thus becomes,

Wt+1

=

Wt

+

1 2

t

DW

+

 t W T W

(16)

where every element of t is normally distributed with variance t: ij,t  N [0, t]. Our stochastic version simply approximates the part of the gradient that
sums over data-cases with a sum over a small mini-
batch of size n and multiplies the result with N/n to
bring it back to the correct scale. We also anneal the stepsizes according to t  a(b + t)-.

To assess how well our stochastic Langevin approach compares against a standard Bayesian method we implemented the "corrected Langevin" MCMC sampler.

This sampler, proposes a new state W , as in Eqn.16.

Note however that we sum over all data-cases and that

we do not anneal the stepsize. Secondly, we need to

accept or reject the proposed step based on all the

data-cases in order to guarantee detailed balance. The

proposal distribution is given by (suppressing depen-

dence on t),

[]

q(W  W ) = N

W ;

W

+

1 2 DW ;

M

(17)

where the quadratic function in the exponent is conveniently computed as,

-1 tr[(W 2

-

1 2

DW

)(W

T

W

)-1

(W

-

1 2

DW

)T

]

(18)

with W = W  - W and the normalization constant requires the quantity det M = det(W T W )D. The ac-

cept/reject step is then given by the usual Metropolis

Hastings rule:

[

p(W )q(W 



] W)

p(accept) = min 1, p(W )q(W  W )

(19)

Finally, to compute the sampling threshold of Eqn.10, we can use

M

1 2

V

(s)M

1 2

=

(20)

[( ) ]

covn

1 N  log p(W ) +  log p(xi|W )

(W

T

W

)

1 2

with covn the sample covariance over the mini-batch of n data-cases.

To show the utility of our Bayesian variant of ICA we

define the following "instability" metric for indepen-

dent components:

 Ii = var(Wij)var(xj)

(21)

j

where var(Wij) is computed over the posterior sam-

ples and var(xj) is computed over the data-cases.

The reason that we scale the variance of the weight

entry Wij with the variance of xj is that the variance of the sources yi = j Wijxj is approximately

equal for all i because they are fit to the distribution

pi(yi)

=

.1

4

cosh2

(

1 2

yi

)

5.3.1. Artificial Data
In the first experiment we generated 1000 data-cases IID in six channels. Three channels had high kurtosis distributions while three others where normally distributed. We ran stochastic Langevin dynamics with

Amari distance Amari distance Instability Metric Instability Metric

Stochastic Gradient Langevin Dynamics

Amari Distance Stoc. Lan.
10

Amari Distance Corr. Lan.
10

Instability Metric Stoc. Lan.
200

8 8 150

6 6 100

4 4 50

2 2468

2 0

0.5

1

1.5

2

0 123456

iteration

x 104

iteration

x 104

Sorted Component ID

Instability Metric Corr. Lan.
100
80
60
40
20
0 123456
Sorted Component ID

Figure 4. Left two figures: Amari distance over time for stochastic Langevin dynamics and corrected Langevin dynamics. Thick line represents the online average. First few hundred iterations were removed to show the scale of the fluctuations. Right two figures: Instability index for the 6 independent components computed in section 5.3.1 for stochastic Langevin dynamics and corrected Langevin dynamics.

W(1,2) W(1,2) W(2,1) W(2,1)

PDF W(1,1) vs W(1,2) Stoc. Lan.
6

4

2

0

-2

-4

-6
-5 0
W(1,1)

5

PDF W(1,1) vs W(1,2) Corr. Lan.

4 2 0 -2 -4 -6
-4

-2 0
W(1,1)

2

4

PDF W(1,1) vs W(2,1) Stoc. Lan.
5

0

-5
-5 0
W(1,1)

5

PDF W(1,1) vs W(2,1) Corr. Lan.
5 4 3 2 1 0

-4 -2 0 2
W(1,1)

4

Figure 5. Posterior density estimates for artificial dataset for stochastic Langevin and corrected Langevin dynamics measured across the W11 - W12 and W1,1 - W2,1 axes.

a batch-size of 100 for a total of 500,000 iterations

and

a

polynomial

annealing

schedule

t

=

4 N

t-0.55

.

After around 10,000 iterations the sampling threshold

at  = 0.1 was met. At that point we recorded the

"mixing distance" asD0 = t and collected samples only when the sum t t from the last sample time exceeded D0 (in other words, as t decreases we col-
lect fewer samples per unit time). We note that simply

collecting all samples had no noticeable impact on the

final results. The last estimate of W was used to ini-

tialize corrected Langevin dynamics (this was done to

force the samplers into the same local maximum) after

which we also collected 500, 000 samples. For corrected

Langevin

we

used

a

constant

stepsize

of



=

0.1 N

.

The two left figures of Figure 4 show the Amari distance (Amari et al., 1996) over time for stochastic and corrected Langevin dynamics respectively. The right two figures show the sorted values of our proposed instability index. Figures 5 show two dimensional marginal density estimates of the posterior distribution of W . ICA cannot determine the Gaussian components and this fact is verified by looking at the posterior distribution. In fact, the stochastic Langevin algorithm has mixed over a number of modes that presumably correspond to different linear combinations of the Gaussian components. To a lesser degree the corrected Langevin has also explored two modes. Due to the complicated structure of the posterior distri-

bution the stability index varies strongly between the two sampling algorithms for the Gaussian components (and in fact also varies across different runs). We verified that the last three components correspond to stable, high kurtosis components.

5.3.2. MEG Data

We downloaded the MEG dataset from

http://www.cis.hut.fi/projects/ica/eegmeg/MEG data.html.

There are 122 channels and 17730 time-points, from

which we extracted the first 10 channels for our

experiment. To initialize the sampling algorithms,

we first ran fastICA (Hyvarinen, 1999) to find an

initial estimate of the de-mixing matrix W . We

then ran stochastic Langevin and corrected Langevin

dynamics to sample from the posterior. The settings

were very similar to the previous experiment with a

schedule

of

t

=

0.1 N

t-0.55

for

stochastic

Langevin

and

a constant stepsize of 1/N for corrected Langevin.

We obtained 500, 000 samples for stochastic Langevin

in 800 seconds and 100,000 samples for corrected

Langevin in 9000 seconds. We visually verified

that the two dimensional marginal distributions of

stochastic Langevin and corrected Langevin dynamics

were very similar. The instability values are shown in

figure 6. Due to the absence of Gaussian components

we see that the stability indices are very similar across

the two sampling algorithms. It was verified that

Stochastic Gradient Langevin Dynamics

Instability Index Instability Index

Instability Index Stoc. Lan.
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0 1 2 3 4 5 6 7 8 9 10
Sorted Components

Intability Index Corr. Lan.
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0 1 2 3 4 5 6 7 8 9 10
Sorted Components

Figure 6. Instability indices of 10 components for MEG dataset for stochastic Langevin (left) and corrected Langevin (right) respectively.

the most stable component corresponded to a highly kurtotic source (kurtosus = 15.4), while the most unstable component was closer to Gaussian noise with a kurtosis of 3.4 (2 corresponds to Gaussian). These findings verify that the stochastic Langevin procedure produces accurate posterior distributions that are in full agreement with a well established MCMC procedure.
6. Discussion
Stochastic gradient optimization is among the most effective algorithms if we measure "predictive accuracy obtained per unit of computation" (Bottou & Bousquet, 2008). Due to subsampling noise, the parameter estimates fluctuate around their MAP values. The common wisdom is that one must anneal these stepsizes to zero to reach the fixed point. However, we argue that one should not optimize beyond the scale of the posterior distribution. The posterior represents the intrinsic statistical scale of precision and trying to determine parameter values with more precision runs the risk of overfitting at additional computational cost.
MCMC sampling from the posterior distribution does of course address the overfitting issue. However, general MCMC algorithms need to see all the data at every iteration, and thus lose the benefits of the stochastic approximation approaches. This paper offers for the first time a surprisingly simple solution that represents the best of both worlds: stick with stochastic gradients but sample from the posterior nevertheless.
But perhaps the biggest advantage of stochastic gradient Langevin dynamics is the fact that stochastic optimization seamlessly transitions into posterior sampling. By simply adding Gaussian noise with the correct variance our method performs "early stopping" automatically without ever having to worry about it. In fact, we have shown that with a polynomial annealing schedule the obtained samples will asymptotically represent the posterior distribution faithfully.

We believe that this work represents only a tentative first step to further work on efficient MCMC sampling based on stochastic gradients. Interesting directions of research include stronger theory providing a solid proof of convergence, deriving a MH rejection step based on mini-batch data, extending the algorithm to the online estimation of dynamical systems, and deriving algorithms based on more sophisticated Hamiltonian Monte Carlo approaches which do not suffer from random walk behaviour.
Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 0447903, 1018433 (MW) and the Gatsby Charitable Foundation (YWT).
References
Amari, S., Cichocki, A., and Yang, H.H. A new algorithm for blind signal separation. In Neural Information Processing Systems, volume 8, pp. 757­763, 1996.
Andrieu, C., de Freitas, N., and Doucet, A. Sequential MCMC for Bayesian model selection. In Proceedings of the IEEE Signal Processing Workshop on Higher-Order Statistics, pp. 130­134, 1999.
Bottou, L. and Bousquet, O. The tradeoffs of large scale learning. In Advances in Neural Information Processing Systems, volume 20, pp. 161­168, 2008.
Girolami, M. and Calderhead, B. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society B, 73:1­37, 2011.
Hyvarinen, A. Fast and robust fixed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks, 10(3):626­634, 1999.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. Trust region Newton method for large-scale logistic regression. Journal of Machine Learning Research, 9:627­650, 2008.
Neal, R. M. MCMC using Hamiltonian dynamics. In Brooks, S., Gelman, A., Jones, G., and Meng, X.-L. (eds.), Handbook of Markov Chain Monte Carlo. Chapman & Hall / CRC Press, 2010.
Robbins, H. and Monro, S. A stochastic approximation method. Annals of Mathematical Statistics, 22(3):400­ 407, 1951.
Robert, C. P. and Casella, G. Monte Carlo statistical methods. Springer Verlag, 2004.
Roberts, G. O. and Stramer, O. Langevin diffusions and metropolis-hastings algorithms. Methodology and Computing in Applied Probability, 4:337­357, 2002.

