From: AAAI-98 Proceedings. Copyright © 1998, AAAI (www.aaai.org). All rights reserved.

Richard Dearden
Department of Computer Science University of British Columbia
Vancouver, BC V6T 1Z4, Canada
dearden@cs.ubc.ca

Bayesian Q-learning
Nir Friedman 
Computer Science Division 387 Soda Hall
University of California Berkeley, CA 94720
nir@cs.berkeley.edu

Stuart Russell
Computer Science Division 387 Soda Hall
University of California Berkeley, CA 94720
russell@cs.berkeley.edu

Abstract
A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information--the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.
1 Introduction
Reinforcement learning is a rapidly growing area of interest in AI and control theory. In principle, reinforcement learning techniques allow an agent to become competent simply by exploring its environment and observing the resulting percepts and rewards, gradually converging on estimates of the value of actions or states that allow it to behave optimally. Particularly in control problems, reinforcement learning may have significant advantages over supervised learning: first, there is no requirement for a skilled human to provide training examples; second, the exploration process allows the agent to become competent in areas of the state space that are seldom visited by human experts and for which no training examples may be available.
In addition to ensuring more robust behavior across the state space, exploration is crucial in allowing the agent to discover the reward structure of the environment and to determine the optimal policy. Without sufficient incentive to explore, the agent may quickly settle on a policy of low utility simply because it looks better than leaping into the
Current address: Institute of Computer Science, The Hebrew University, Givat Ram, Jerusalem 91904, Israel, nir@cs.huji.ac.il.
Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.

unknown. On the other hand, the agent should not keep exploring options that it already has good reason to believe are suboptimal. Thus, a good exploration method should balance the expected gains from exploration against the cost of trying possibly suboptimal actions when better ones are available to be exploited.
Optimal solution of the exploration/exploitation tradeoff requires solving a Markov decision problem over information states--that is, the set of all possible probability distributions over environment models that can be arrived at by executing all possible action sequences and receiving any possible percept sequence and reward sequence. The aim is to find a policy for the agent that maximizes its expected reward. Although this problem is well-defined, given a prior distribution over possible environments, it is not easy to solve exactly. Solutions are known only for very restricted cases--mostly the so-called bandit problems in which the environment has a single state, several actions, and unknown rewards [3].
Section 2 discusses several existing approaches to exploration, as well as the model-free Q-learning algorithm we use as our underlying learning method. This paper presents two new approaches to exploration:
Q-value sampling: Wyatt [17] proposed Q-value sampling as a method for solving bandit problems. The idea is to represent explicitly the agent's knowledge of the available rewards as probability distributions; then, an action is selected stochastically according to the current probability that it is optimal. This probability depends monotonically not only on the current expected reward (exploitation) but also on the current level of uncertainty about the actual reward (exploration). In this work, we extend this approach to multi-state reinforcement learning problems. The primary contribution here is a Bayesian method for representing, updating, and propagating probability distributions over rewards.
Myopic-VPI: Myopic value of perfect information [8] provides an approximation to the utility of an informationgathering action in terms of the expected improvement in decision quality resulting from the new information. This provides a direct way of evaluating the exploration/exploitation tradeoff. Like Q-value sampling, myopic-VPI uses the current probability distributions over rewards to control exploratory behavior.
Section 3 describes these two algorithms in detail, along with the Bayesian approach to computing reward distributions. In Section 4 we prove convergence results for the algorithms, and in Section 5 we describe the results of a

1. Let the current state be s. 2. Select an action a to perform. 3. Let the reward received for performing a be r, and the
resulting state be t. 4. Update Qs; a to reflect the observation s; a; r; t as
,follows:
Qs; a = 1 Qs; a + r + maxa0 Qt; a0
where is the current learning rate. 5. Go to step 1.
Figure 1: The Q-learning algorithm.

number of experiments comparing them against other exploration strategies. In our experiments, myopic-VPI was uniformly the best approach.

2 Q-Learning

We assume the reader is familiar with the basic concepts

S A S A!olsoiftwiiMsoinnaDgmsPneostod(toseafelteisto,htaneat:.tgeAcs.,,anKptMaueirsDlebsaPlitsnhiesgetaepot4rfoa-atlbu.ca[tpbi9loie]ln),i.tsy,Wpo;etfwrse;iaaplcltth;upisniersgtahswettahrftaeoenrlet--

jafter we execute action a at state s, and prr s; a is a
mexoedceultitnhgataccatipotnuraesatthsetapterosb.ability of getting reward

rrewwhaerdn

In this paper, we focus on infinite-horizon MDPs with a

tddhieesncoeotxuepnsettchftaeecdrteodwris0acrodurnetceedivt1eo.dtaTalthresetweaapgreidn. tE'LseaPttiimni gisiVrtio;mswahxdeiemrneoizrteei

tshaenodptQimasl;eaxpedcetneodtdeitshceouvnaltuede

roefweaxredcauctihnigevaabaltesf,rwome

state have

the standard Bellman equations [2]:

V s Qs; a

= =

mXaxraQpr

s; a
rjs; a

+

X pts!a tV t;

rt

tahnedRaepgiren.nfotI'nrscteehxmipseenpctatepldeearrrewnwienagfrodpcwuroshceoenndtuQhree-lsaegaaetrntnetimndgopet[s1ton4o]m,t akaxnsiiommwpizpleet

and elegant model-free method that learns Q-values without
learning the model pt. In Section 6, we discuss how our

results carry over to model-based learning procedures.

QAs;Qa-lefarronmingitsaegxepnet riwenocrkess.

by It

estimating then select

the values of actions based

on their Q-values. The algorithm is shown in Figure 1. If

every action is performed in every state infinitely often, and
toiQs desc;aayedfoarpaplrlosprainadtealy,[1Q5]s. ; a will eventually converge

The strategy used to select an action to perform at each

step is crucial to the performance of the algorithm. As

with any reinforcement learning algorithm, some balance

between exploration and exploitation must be found. Two

commonly used methods are semi-uniform random explo-

ration and Boltzmann exploration. In semi-uniform random

,eaxbpilliotyrapti,onan[d16w],itthhepbreosbtaabcitliiotyn

is 1

selpe,ctaend

with some probaction is chosen

at random. In some cases, p is initially set quite low to

encourage exploration, and is slowly increased. Boltzmann

ethxeplporroabtiaobnili[t1y4o]fisexaecmuotirnegsaocpthioisntiacaitnedstaatpepsroiasc: h in which

Qs;a=T

a e ePr

= P
a0

Qs;a0=T

where T is a temperature parameter that can be decreased

slowly over time to decrease exploration. In this approach,

the probability of an action being selected increases with the

current estimate of its Q-value. This means that sub-optimal

but good actions tend to be selected more often than clearly

poor actions.

Both these exploration methods are undirected, meaning

that no exploration-specific knowledge is used. A number

of directed methods have also been proposed, of which the

best known is interval estimation [10]. Most of the directed

techniques can be thought of as selecting an action to per-

form based on the expected value of the action plus some

exploration bonus [11]. In the case of interval estimation, we

assume a normal distributionfor the observed future values of

,each action in each state, and select an action by maximizing
the upper bound of a 1001 % confidence interval (for

some confidence coefficient ) over this distribution. The

exploration bonus for interval estimation is half the width

of the confidence interval. Other exploration bonuses have

been proposed, based on the frequency or recency with which

each action has been performed, or on the difference between

predicted and observed Q-values.

The exploration-specific information in the Interval Esti-

mation algorithm is strictly local in nature. The exploration

bonus is calculated only from the future values observed

from the current state. Exploration can also be done glob-

ally, selecting actions now that we believe will lead us to

less-explored parts of the state space in the future. We can

do this by backing up exploration specific information along

with the Q-values. Meuleau and Bourgine [11], propose

IEQL+, which is closely related to interval estimation in that

it backs up Q-values and uses them to compute a local explo-

ration bonus. Unlike interval estimation, IEQL+ also backs

up an exploration bonus and combines the two to compute

the new exploration value of the action.

For a survey of directed and undirected exploration tech-

niques, see [13].

3 Bayesian Q-learning
In this work, we consider a Bayesian approach to Q-learning in which we use probability distributions to represent the uncertainty the agent has about its estimate of the Q-value of each state. As is the case with undirected exploration techniques, we select actions to perform solely on the basis of local Q-value information. However, by keeping and propagating distributions over the Q-values, rather than point estimates, we can make more informed decisions. As we shall see, this results in global exploration, but without the use of an explicit exploration bonus.

3.1 Q-Value Distributions
In the Bayesian framework, we need to consider prior distributions over Q-values, and then update these priors based
on the agent's experiences. Formally, let Rs;a be a random
variable that denotes the total discounted reward received

when action a is executed in state s and an optimal policy is

hvfoaollwluoewRQesd;atshi;seardeias=fttreiErb.uWRtedsh;;aati.nwpeaartriecuinlaitri,awllye

uncertain about is want to learn the

We start by making the following simplifying assumption:

Assumption 1: Rs;a has a normal distribution.

We claim that this assumption is fairly reasonable. The

accumulated reward is the (discounted) sum of immediate

rewards, each of which is a random event. Thus, appealing to

the central limit MDP is ergodic

theorem, when the

if is close to optimal policy

1isaanpdptlhieedu, nthdeenrlRyisn;ag

is approximately normally distributed.

tarbiboTuuhttiisothnaesosvdueimrstptrhitbieoumntieoianmnpolfiesRs;ast;ahan,adtitttohseumfpfiorcedecesilsitooonumr sou;dnaecolefratRaidsni;tasy-.

(The precision of a normal variable is the inverse of its vari-
ance, that is, s;a = 1= s2;a. As it turns out, it is simpler to

represent uncertainty over the precision than over the vari-

ance.) Of
of s; a.

course,

the

mean,

s;a,

corresponds

to

Our next assumption is that the prior beliefs
are independent of those about Rs0;a0.

the Q-value
about Rs;a

6 6Assumption 2: The prior distribution over

ifnodr esp=ends0enotr

oaf0

the prior
= a.

distribution

over

ss0;;aa0aanndd

s;a is s0;a0

This assumption is fairly innocuous, in that it restricts only

the form of prior knowledge about the system. Note that this

assumption does not imply that the posterior distribution

satisfy such independencies. (We return to this issue below.)

Next we rameters of

eaasscuhmRes;tahaatrethferopmrioa rpdaristitcriubluartifoanms iolyv:er

the

pa-

Assumption 3: The prior ps;a; s;a, is a normal-gamma

distribution.

We will now define and motivate the choice of the normal-

gamma distribution. See [7] for more details.

preAcisnioornmaol-fgaanmumnakndoiwstnrinbuotrimonalloyvderisttrhiebumteedavnariaabnledRthies

h idetermined by a tuple of hyperparameters We say that p;  NG0; ; ;  if

= 0; ; ;

.

p ; e e 

, , ,  1
2

1 2



0 2

1

Standard when we

results show how to update such receive independent samples of

a prior values

odfisRtr:ibution

Theorem 3.1: [7] Let p;   NG0; ; ;  be a

prior distribution over the unknown parameters for a nor-

mally distributed variable R, and let

j wdn1ehPpeernei drei2n.00 t ,0 = +

samples of R with M1 =

Then p;

= 0++nnM1 ,

n M M1
2



2

2 1

0r1=; :

:: 

; rn + n,

, + n2M1+n02

n1r10PN; =:Gi: :r;ir00a+n;ndb021;enM,n0;2ainn=0d-

That is, given a single normal-gamma prior, the posterior after

any sequence of independent observations is also a normal-

gamma distribution.

Assumption 3 implies that to represent the agent's prior

over the distribution of
h iple of hyperparameters

Rs;sa;a=, we 0so;an;lysn;eae;d

to maintain a tu-
s;a; s;a . Given

Alecsstiuomnpotifonhsyp2erapnadra3m, ewteerscafnorreeparcehsensttaoteursparniodr abcytiaoncoal-.
Theorem 3.1 implies that, had we had independent samples
of each Rs;a, the same compact representation could have
been used for the joint posterior. We now assume that the posterior has this form

Assumption 4: At any stage, the agent's posterior over

6 6s;a
and

and s0;a0

s;a for

sis=insd0 eoprean0d=enta.of

the

posterior

over

s0;a0

In an MDP setting, this assumption is likely to be violated;

the agent's observations about the reward-to-go at different

states and actions can be strongly correlated--in fact, they

are related by the Bellman equations. Nonetheless, we shall

assume that we can represent the posterior as though the

observations were independent, i.e., we use a collection of

hmyepaenrpaanrdampreetceirssions;apafroarmtheetenrosromfaela-cghamRms;aa.posterior for the

We exploit this compact representation in the Bayesian

Q-learning algorithm, which is similar to the standard Q-

lQeasr;an,inwgeanlgoowristhtomre,

except that instead of the hyperparameters

storing the Q-value s;a. In the follow-

ing sections, we address the two remaining issues: how to

select an action based on the current belief state about the

MDP, and how to update these beliefs after a transition.

3.2 Action Selection

In every iteration of the Q-learning algorithm we need to

select an action to execute. Assuming that we have a prob-

aabctiiloitnysdai,sthriobwutdioonwoevesrelQectsa;naac=tionst;oa

for all states s
perform in the

and cur-

rent state? We consider three different approaches, which we

call greedy, Q-value sampling, and myopic-VPI.

pGmtRorrisozs;eaaheec.sodhwyt.hTsetIhehnuelaesxtt,chpEtietishoceatnepsdgp;arrvOeoaeianldscueyhsepi,maEowpsppselirybsos;elaoaelcueah.crptUewptsrnhotoifeumaolcardahtctutesiinesoaolntetfhecaelttyhgt,ethhrieatmeetiedsamaycentaaaixospoiyn-f-
with the greatest mean, and would not attempt to perform exploration. In particular, it does not take into account any uncertainty about the Q-value.

Q-value sampling Q-value sampling was first described by Wyatt [17] for exploration in multi-armed bandit problems. The idea is to select actions stochastically, based on our
cauirsrepnetrfsourbmjeecdtiwveitbheplireofbtahbaitltihtyeygiavreenobpytimal. That is, action

8 6a Pr = arg maa0x s;a0 = Pr a0=a; s;a s;a0
= Z1 Prs;a = qa Y Prs;a0 qa dqa (1)
,1 a0=6 a

The last step in this derivation is justified by Assumption 4 that states that our posterior distribution over the values of separate actions is independent.
of Togeivveanluaatneotrhmisale-xgparmesmsiaodni,stwriebuutsieonth. e marginal density

action 1 action 2

action 1 action 2

-6 -4 -2 0 2 4 6 8
(a)

-6 -4 -2 0 2 4 6 8
(b)

Figure 2: Examples of Q-value distributions of two actions for which Q-value sampling has the same exploration policy even though the payoff of exploration in (b) is higher than in (a).

Lemma 3.2: [7] If p;   NG0; ; ; , then

p 



=

,

 2



1 2

  ,  ;



+

1 2



 

1 +2



,



+

1 2



02

(2)

,and

Pr

x = T x

  0

1 2 :2 

where T x : d is the cumulative t-distributionwith d degrees of freedom. Moreover, E  = 0, and Var  =  ,1.

weInsapmrapcleticaev, awlueecfarnomaveoaidchthpecos;map,uatnadtioenxeocfu(t1e)t.hIenasctetiaodn,

with the highest sampled value. It is straightforward to show
that this procedure selects a with probability given by (1). Of

course, sampling from a distributionof the form of (2) is non-

Ptrivial

anxd.rFeqourtiurensateevlayl,uTatixon:

dof

the can

cumulative distribution be evaluated efficiently

using standard statistical packages. In our experiments, we

used the library routines of Brown et al. [5].

Q-value sampling resembles, to some extent, Boltzmann

exploration. It is a stochastic exploration policy, where the

probability of performing an action is related to the distribu-

tion of the associated Q-values. One drawback of sampling is that it only considers the probability

tQha-tvaaluies

cbhesotosaicntgiona,

and does not consider the amount by might improve over the current policy.

which Fig-

ure 2 show examples of two cases where Q-value sampling

would generate the same exploration policy. In both cases,
Pra2 a1 = 0:6. However, in case (b) exploration
seems more useful than in case (a), since the potential for

larger rewards is higher for the second action in this case.

Myopic-VPI selection This method considers quantitatively the question of policy improvement through exploration. It is based on value of information [8]. Its application in this context is reminiscent of its use in tree search [12], which can also be seen as a form of exploration. The idea is to balance the expected gains from exploration--in the form of improved policies--against the expected cost of doing a potentially suboptimal action.
truWe veasltuaertbsy;acoonfsidse;ari.ngHwowhawt coaunldbethgiasinkendowbyleldegaerncihnagntghee
the agent's future rewards? Clearly, if this knowledge does not change the agent's policy, then rewards would not change.

Thus, the only interesting scenarios are those where the new

knowledge does change the agent's policy. This can happen

in two cases: (a) when the new knowledge shows that an

action previously considered sub-optimal is revealed as the

best choice (given the agent's beliefs about other actions),

and (b) when the new knowledge indicates that an action that

was previously considered best is actually inferior to other

actions. We now derive the value of the new information in

both cases.

, ,EpksEethhxnosaopF;Fosatueewoossicltrr;;sdlthaa,eec12caEddapatgsesstveeher;aafe(soilE(b;nunsara)dm;ee)1,a,wi1Esacsb.suana;kyuapt2dne0ppvsosiapoin;fw2oartsosth1seltuireeaes.etdatahttlTdgohahlefhteasoopuiat;tsfnaehs1ea1d,reaci1firwso1coianatreshatmcdeEnetet-siidhxbonatepecnhgwsetssabitac;etoaeataan2is0cent.ti,hxtwssieapMtotaiehcetnaatbheoc.gidnetorteetIhnontiftoeet;fhtvtreathhtteaohoiercaga.tgggsnhtiueaaoeeipniiwnssnn-tt,;

To summarize learning the value

tohfissd;iascoufssiso;an,asw: e

define

the

gain

from

8 E s;a2 , s;a

Gains;as;a =
:

s;a , E

s;a1

0

if a aifnad
and

=6=ss;;aaaa11

E s;a2 E s;a1

otherwise

where, again, a1 and a2 are the actions with the best and

second best expected values respectively. Since the agent

dos;eas,

not we

know in advance what value need to compute the expected

will gain

be revealed for given our prior

abbeloiuetfs. s;aHiesn:ce the expected value of perfect information

Z1

VPIs; a = Gains;axPrs;a = xdx

,1

Using simple manipulations we can reduce VPIs; a to a

oclfoses;da

form equation (which can be

involving computed

the cumulative efficiently).

distribution

, 6 ,EPtorco+ps;oasE1itiPorsn;a3.s3;a:E1 VPsIE;as1;asP;ar2issw;eahqeunaEla t=osa;ca11,+awndhEeintiass;e=a2quaa1l,

where

c p p E  , :=
, s;a

s;a

s;a+

1 2



1 2



s;a

1 2





s;a
s;a 2 s;a

1+

2


s;a

2 s;a

s;a+

1 2

theTmheyovpaliucevoalfupeeorffeicntfionrfmoarmtioantiofonrgeixvpesloarninugpapcetriobnouan. dTohne

expected cost incurred for this exploration is given by the

,bdeifsfteraecntcioenb,etiw.e.e,enmtahxeav0 aEluQe osf ;aaa0nd

the value of the
E Qs; a .

current This

suggests we choose the action that maximizes

VPIs; a , maa0x E Qs; a0 , E Qs; a :

Clearly, this strategy is equivalent to choosing the action that

maximizes:

E Qs; a + VPIs; a:

We see that the value of exploration estimate is used as a

twhaeyagoef nbtoiossctionngfitdheentdoesfirthabeielisttyimofatdeidffQer-evnatluacetsi,otnhse.

When VPI of

each action is close to 0, and the agent will always choose

the action with the highest expected value.1

3.3 Updating Q-values

Finally, we turn to the question of how to update the estimate

of the distribution over Q-values after executing a transition.

The analysis of the updating step is complicated by the fact

that a distribution over Q-values is a distribution over ex-

pected, total rewards, whereas the available observations are

instances of actual, local rewards. Thus, we cannot use the

Bayesian updating results in Theorem 3.1 directly.

Suppose that receives reward

rth, eanadgelnatndiss

in state s,
up in state

etx. ecWuteeswaocutlidonlikae,

tvtoaorkinanwbolawerddsteh,nebouctitonmtghpitshleeitsedinssoecqtouuaevnnatecildeabsolufem.reowLfeartredRws atrredbcseeifavroerdmanftdr.oommIf

we assume that the agent will follow the apparently optimal

policy, then Rt is distributed as
with the highest expected value

Rattt;a. t,

where

at

is

the

action

We might hope to use this distributionto substitute in some

way for the unknown future experiences. We now discuss

two ways of going about this.

Moment updating The idea of moment updating is, no-
tdriio+sntrailbRluy,1tti;ot:on:,:ra;anrnd+domthlRyetnns,awumphpdelaerteevwaPleuteRaskseR;aet1a;c:wh: i:st;haRmttnphleferotsomamhoapvuleer
weight n1 . Theorem 3.1 implies that we only need the first two moments of this sample to update our distribution. Assuming
that n tends to infinity, these two moments are:

M1 = E r + Rt = r + E Rt

M2 = E r + Rt2 = E r2 + 2 rRt + 2Rt2 = r2 + 2 rE Rt + 2E Rt2

Now, since our estimate of the distribution of Rt is a
gamma distribution over the mean and variance of

normal-
Rt, we

ctoancoumsepustteantdhearfidrsptrotwpeormtieosmoefnntsoormf Ral-tg. amma distributions

uLnekmnmowan3m.4e:anLetaRndbuenaknnoowrmnaplrleycidsiisotnribu, taenddvlaert ipabl;e

with


NG0; ;

2 0

.

;

. Then E R = 0, and E R2 = + 1

,1 +

Now we can update the hyperparameters s;a as though we had seen a collection of examples with total weight 1,
mean M1, and second moment M2. TfqouorTiscuehkpeilsdytahabtipiesnp,cgnoroomthateeceshthhtyaoreptosweucreolptnacsrfiaaindnmernoeattuesogrifhsmltfyhpoelirenvRtacesllruo;paesr.eeodUtft-nhfthofeoerpmrmatureaenamqanuteeatlteysir,o;aint.
as the confidence in our estimate of the unknown mean. The

1It is clear that the value of perfect information is an optimistic
assessment of the value of performing a; by performing a once, we
do not get perfect information about it, but only one more training
instance. Thus, we might consider weighting the VPI estimate by
some constant. We leave this for future work.

method we just described updates the unknown reward, which is just

r0+andERwti,thasthief

mean of we were

confident of this being a true sample. Our uncertainty about
euTwthsnhhetciuimecvsrh,aatlaotumeiuneaortiyofunftanlhybcReoeatuvrftatfiasretihicnarettnesympcteraehebaoesonefuenoRsttftesiRRdm;ats.abT;itayseh. entoIhonufetspttsdsheehiaercodeovt,cnaiottrdlfyilaeamnatlrldcoaetsmnhtsiooelsfnahittsRiegMdtshh;teaa2ort.,

the precision of the mean increases too fast, leading to low

exploration values and hence to premature convergence on

sub-optimal strategies.

One ad-hoc way of dealing with this problem is to use

exponential forgetting. This method reduces the impact of

previously seen examples on the priors by a constant (which

is usually close to 1) at each update. Due to space con-

siderations, we do not review the details of this forgetting

operation.

jjMssdRdcleaiii.sscpigttxttrrhiutiioItubbrflneyuurwecttodiiaoouueinfnnrpfobeudoobrenvavesectaneeirevrrtnrvotwgRaeisdida;saney;;tTda.tyhhLbseiae;syebapvtouparapusfoltitubenteslrghes;aoe;mRta;bhv;tsdeaseels;dr=uas;viacesirtnxxirbgiR,rbbeduydt+ihtsibiwecnoenoentxuihtogthnehvh.etepeetpirrdnuWeoRrgcpseeetdttwedhaircieatninaesrongddaer
distribution by the probability that Rt = x. This results in
the following mixture posterior:
Z1
prm;tixs;a; s;a = ps;a; s;a j r + xpRt = xdx
,1

Unfortunately, the posterior pmr;tixs;a; s;a does not have a
simple representation, and so updating this posterior would

lead to a more complex one, complexity by approximating

pamrn;tdixsoso;an;.

We can avoid this s;a with a normal-

gamma distribution after each update.

We compute the best normal-gamma approximation by

minimizing the KL-divergence [6] from the true distribution.

Theorem 3.5: Let q;  be some density measure over 
atdhnievdedrigsaetrnnidcbeuletKtioLnpq0;p.;Ifiws edecNfionGnesdtrab0i;ynth; eto;foblelotghwraeintamgteeirnqtiuhmaaitnzieo1sn+tsh:e,

0 = Eq  =Eq



=

,Eq 2

, Eq

2,1 0

,= max1 + ; flog Eq

Eq log 

= =Eq

,where fx is the inverse of gy = log y y, and x =

x0  x

is

the

digamma

function.

The requirement that 1+ is to ensure that 1 so that
the normal-gamma distributionis well defined. Although this theorem does not give a closed-form solution for , we can
find a numerical solution easily since gy is a monotonically
decreasing function [1].

Another complication with this approach is that it re-
quires us to compute E s;a , E s;as;a , E s;as2;a and

E log s;a with respect to pmr;tixs;a; s;a. These expecta-
tions do not have closed-form solutions, but can be approximated by numerical integration, using formulas derived fairly straightforwardly from Theorem 3.5.
To summarize, in this section we discussed two possible ways of updating the estimate of the values. The first, moment update leads to an easy closed form update, but might become overly confident. The second, mixture update, is more cautious, but requires numerical integration.

4 Convergence

We are interested in knowing whether our algorithms con-

tvheergme etoanosptims;aalcpoonlviceiregseintoththeelitmruite.

It suffices Q-values,

to show that and that the

variance of the means converges to 0. If this is the case, then

both the Q-value sampling and the myopic-VPI strategies

will, eventually, execute an optimal policy.

Without going into details, the standard convergence proof

1 1[15] for Q-learning requires that each

ofteanndinPea1nch=0statenin2

an

infinite run, where

aancdtiothnaitsPtrin1ed=0infinnite=ly
is the learning rate. If

these conditions are met, then the theorem shows that the

approximate Q-values converge to the real Q-values.

Using this theorem, we can show that when we use moment

updating, our algorithm converges to the correct mean.

Theorem 4.1: If each action a is tried infinitely often in

atehvneedrmyaecsattinaotne,sa;a.ancdonthveeraglegsotroitthhme turuseesQm-voamlueentfourpedvaetriyngs,tathteens

Moreover, for moment updating we can also prove that the
variance will eventually vanish:
Theorem 4.2: If each action a is tried infinitely often in ceuvopendrvayetresgtteahsteet,opao0nsftdoerrtiheoevreaerlysgtiosmtraiatthteemss,autnhsedesnatcthhtieeonmvaaorm.iaennctemVeathr ods;tao

Combining these two results, we see that with moment updating, the procedure will converge on an optimal policy if all actions are tried eventually often. This is the case when we select actions by Q-value sampling.
If we select actions using myopic-VPI, then we can no longer guarantee that each action is tried infinitely often. More precisely, myopic VPI might starve certain actions and hence we cannot apply the results from [15]. Of course, we can define a "noisy" version of this action selection strategy (e.g., use a Boltzmann distributionover the adjusted expected values), and this will guarantee convergence.
At this stage, we do not yet have counterparts to Theorems 4.1 and 4.2 for mixture updating. Our conjecture is that the estimated mean does converge to the true mean, and therefore similar theorems holds.

5 Experimental Results
We have examined the performance of our approach on several different domains and compared it with a number of different exploration techniques. The parameters of each algorithm were tuned as well as possible for each domain. The algorithms we have used are as follows:
Semi-Uniform Q-learning with semi-uniform random exploration.

b,2
1

a,0 2
b,2 b,2

a,0 3

a,0 4

b,2 b,2

(a) Task 1 [11].

a,10
a,0 5

b,0

6
b,0
7

a,0 a,0

b,0

5 b,0 a,0 1
a,0

0

a,b,2

a,b,1

84

(b) Task 2 [14].

a,b,0
2
a,b,0
3
a,b,0

SF

G

F

F

(c) Task 3. A agent receives

anarevwigaartdiounpopnrorbealecmhi.ngSGisbtahseedstoanrttshteanteu.mTbheer

of flags collected.

Figure 3: The three domains used in our experiments.

Boltzmann Q-learning with Boltzmann exploration.
Interval Q-learning using Kaelbling's interval-estimation algorithm [10].
IEQL+ Meuleau's IEQL+ algorithm [11].
Bayes Bayesian Q-learning as presented above, using either Q-value sampling or myopic-VPI to select actions, and either Moment updating or Mixture updating for value updates. These variants are denoted QS, VPI, Mom, Mix, respectively. Thus, there are four possible variants of the Bayesian Q-Learning algorithm, denoted, for example, as VPI+Mix.
We tested these learning algorithms on three domains:
ChFaaniigdnubrT.ehW3is(iatd)h.opmrIoat ibncaobcnoislniitssytiss0tso.2fo, fsthitxheesatcgahetaneistn"aosnlfdipststaw"taeonsdaschatocitowunnaslilany
performs the opposite action. The optimal policy for this
dttirooampnpaaeindeva(eatrstyshwuemhieninrietgi.aalHsdotaiwstceeo,vupenrr,etflfeearacrritnnoigrngotofaf0lgo.9oll9roi)wthismthtseocbda­onloagocep-t
to obtain a series of smaller rewards.
Loop This domain consists of two loops, as shown in Figure 3(b). Actions are deterministic. The problem here is that a
alefaornrisntagteal0gboeriftohrme tmheaylarhgaevrereawlraeraddyavcaoilnavbelregiendsotantea8cthioans been backed up. Here the optimal policy is to do action b
everywhere.

Domain chain
loop
maze

Method
Uniform Boltzmann Interval IEQL+ Bayes QS+Mom Bayes QS+Mix Bayes VPI+Mom Bayes VPI+Mix
Uniform Boltzmann Interval IEQL+ Bayes QS+Mom Bayes QS+Mix Bayes VPI+Mom Bayes VPI+Mix
Uniform Boltzmann Interval IEQL+ Bayes QS+Mom Bayes QS+Mix Bayes VPI+Mom Bayes VPI+Mix

1st Phase

Avg. Dev.

1519.0 37.2

1605.8 78.1

1522.8 180.2

2343.6 234.4

1480.8 206.3

1210.0 86.1

1875.4 478.7

1697.4 336.2

185.6

3.7

186.0

2.8

198.1

1.4

264.3

1.6

190.0 19.6

203.9 72.2

316.8 74.2

326.4 85.2

105.3 10.3

195.2 61.4

246.0 122.5

269.4

3.0

132.9 10.7

128.1 11.0

403.2 248.9

817.6 101.8

2nd Phase

Avg. Dev.

1611.4 34.7

1623.4 67.1

1542.6 197.5

2557.4 271.3

1894.2 364.7

1306.6 102.0

2234.0 443.9

2417.2 650.1

198.3

1.4

200.0

0.0

200.0

0.0

292.8

1.3

262.9 51.4

236.5 84.1

340.0 91.7

340.0 91.7

161.2

8.6

1024.3 87.9

506.1 315.1

253.1

7.3

176.1 12.2

121.9

9.9

660.0 487.5

1099.5 134.9

Table 1: Average and standard deviation of accumulated rewards over 10 runs. A phase consists of 1,000 steps in chain and loop, and of 20,000 steps in maze.

Maze This is a maze domain where the agent attempts to

"collect" flags and get them to the goal. In the experiments

Swemuasrekds

the the

maze start

ssthaotew, nGinmFairgkusreth3e(cg)o.alInstathteis,

afingdurFe,

rmeacrekivseldocoantiorenascohfinflgagGs

that can be collected. The reward is based on the number of flags

collected. Once the agent reaches the goal, the problem

is reset. There are a total of 264 states in this MDP. The

agent has four actions--up, down, left, and right. There is

a small probability, 0.1, that the agent will slip and actually

perform an action that goes in a perpendicular direction.

If the agent attempts to move into a wall, its position does

not change. The challenge is to do sufficient exploration

to collect all three flags before reaching the goal.

The first two domains are designed so that there are suboptimal strategies that can be exploited. Thus, if the learning algorithm converges too fast, then it will not discover the higher-scoring alternatives. The third domain is larger and less "tricky" although it also admits inferior policies. We use it to evaluate how the various exploration stratgies scale up.
There are several ways of measuring the performance of learning algorithms. For example, we might want to measure the quality of the policy they "recommend" after some number of steps. Unfortunately, this might be misleading, since the algorithm might recommend a good exploiting policy, but might still continue to explore, and thus receive much smaller rewards. We measured the performance of the learning algorithms by the total reward collected during a fixed number of time steps (Table 1). Additionally, we measured the discounted total reward-to-go at each point in the run. More
tipnrteaocribuseenlyPo,f stle0unptgprtohts0Ne tt.0h,Teth.aegnOenwftecroeduceerfisinveee, stthhreieswreeawsrtdaimsrdra-1tte;or-ig2s;or:ae:tl:it;aibrmNlee
only for points that are far enough from the end of the run. In
Figure 4, we plot the average reward-to-go as a function of t
by averaging these values over 10 runs with different random

300

250

200

150
100
0
36 34 32 30 28 26 24 22 20 18 16 14
0

Interval IEQL+ Bayes VOI+Mom Bayes VOI+Mix Bayes QS+Mom

500

1000

1500

(a) Results for the chain domain.

Interval IEQL+ Bayes VOI+Mom Bayes VOI+Mix Bayes QS+Mom

500

1000

1500

(b) Results for the loop domain.

2000 2000

70 Boltzmann Interval IEQL+
60 Bayes VOI+Mom Bayes VOI+Mix
50

40

30

20

10

0 0 5000 10000 15000 20000 25000 30000 35000 40000

(c) Results for the maze domain.

Figure 4: Plots of function of number

oacftsutaelpsdi(sxc-oauxniste)dforrewseavredra(lym-aextihso) dassina

three domains. The curves are avarege of 10 runs for each

method. The curves for chain and maze were smoothed.

seeds.2
Our results show that in all but the smallest of domains our methods are competitive with or superior to state of the art exploration techniques such as IEQL+. Our analysis suggests that this is due to our methods' more effective use of small numbers of data points. Results from the maze domain in particular show that our VPI-based methods begin directing the search towards promising states after making significantly fewer observations than IEQL+ and interval estimation. Overall, we have found that using mixture updating combined with VPI for action selection gives the best performance, and expect these to be the most valuable techniques as we expand this work to model-based learning.
One weakness of our algorithms is that they have significantly more parameters than IEQL+ or interval estimation. In the full version of the paper we analyze the dependence of these results on various parameters. The main parameters that seem to effect the performance of our method is the vari-
ance of the initial prior, that is, the ratio  ,1. Priors with
larger variances usually lead to better performance.
6 Conclusion
We have described a Bayesian approach to Q-learning in which exploration and exploitation are directly combined by representing Q-values as probability distributions and using these distributions to select actions. We proposed two methods for action selection -- Q-value sampling and myopicVPI. Experimental evidence has shown that (at least for some fairly simple problems) these approaches explore the state space more effectively than conventional model-free learning algorithms, and that their performance advantage appears to increase as the problems become larger. This is due to an action selection mechanism that takes advantage of much more information than previous approaches.
A major issue for this work is that the computational requirements are greater than for conventional Q-learning, both for action selection and for updating the Q-values. However, we note that in most applications of reinforcement learning, performing actions is more expensive tha computation time.
We are currently investigating ways to use a Bayesian approach such as this with model-based reinforcement algorithms. In this case, we explicitly represent our uncertainty about the dynamics of the system to estimate the usefulness of exploration. We are also investigating alternative action selection schemes, and approximations that could be used to reduce the computational requirements of this algorithm. Finally, it should be possible to use function approximators to extend this work to problems with large and/or continuous state spaces. There is a well-understood theory of Bayesian neural network learning [4, Ch. 10] that allows posterior means and variances to be computed for each point in the input space; these can be fed directly into our algorithm.
2We performed parameter adjustment to find the best-performing parameters for each method. Thus the results reported for each algorithm are probably somewhat optimistic. In the full version of the paper we intend to also show the sensitivity of each method to changes in the parameters.

Acknowledgments
We are grateful for useful comments from David Andre, Craig Boutilier, Daphne Koller and Ron Parr. We thank Nicolas Meuleau for help in implementing the IEQL+ algorithm. Nir Friedman and Stuart Russell were supported in part by ARO under the MURI program "Integrated Approach to Intelligent Systems", grant number DAAH04-96-1-0341, and by ONR under grant number N00014-97-1-0941.
References
[1] M. Abramowitz and I. A. Stegun, editors. Handbook of Mathematical Functions. Dover, 1964.
[2] R. E. Bellman. Dynamic Programming. Princeton Univ. Press, 1957.
[3] D. A. Berry and B. Fristedt. Bandit Problems: Sequential Allocation of Experiments. Chapman and Hall, 1985.
[4] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford Univ. Press, 1995.
[5] B. W. Brown, J. Lovato, and K. Russell. Library of routines for cumulative distribution functions, inverses, and other parameters, 1997. ftp://odin.mdacc. tmc.edu/pub/source/dcdflib.c-1.1. tar.gz.
[6] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley, 1991.
[7] M. H. Degroot. Proability and Statistics. AddisonWesley, 1986.
[8] R. A. Howard. Information value theory. IEEE Trans. Systems Science and Cybernetics, SSC-2:22­26, 1966.
[9] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. J. Artificial Intelligence Research, 4:237­285, 1996.
[10] L. P. Kaelbling. Learning in Embedded Systems. MIT Press, 1993.
[11] N. Meuleau and P. Bourgine. Exploration of multi-states environments: Local measures and back-propogation of uncertainty. Machine Learning, 1998. To appear.
[12] S. J. Russell and E. H. Wefald. Do the Right Thing: Studies in Limited Rationality. MIT Press, 1991.
[13] S. B. Thrun. The role of exploration in learning control. In D. A. White and D. A. Sofge, eds., Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. Van Nostrand Reinhold, 1992.
[14] C. J. Watkins. Models of Delayed Reinforcement Learning. PhD thesis, Psychology Department, Cambridge University, 1989.
[15] C. J. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3):279­292, 1992.
[16] S. D. Whitehead and D. H. Ballard. Learning to percieve and act by trial and error. Machine Learning, 7:45­83, 1991.
[17] J. Wyatt. Exploration and Inference in Learning from Reinforcement. PhD thesis, Department of Artificial Intelligence, University of Edinburgh, 1997.

