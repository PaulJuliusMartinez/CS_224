An Interval Classi er for Database Mining Applications

Rakesh Agrawal

Sakti Ghosh Tomasz Imielinski Bala Iyer IBM Almaden Research Center
650 Harry Road, San Jose, CA 95120

Arun Swami

Abstract

1 Introduction

We are given a large population database that contains information about population instances. The population is known to comprise of m groups, but the population instances are not labeled with the group identi cation. Also given is a population sample (much smaller than the population but representative of it) in which the group labels of the instances are known.
We present an interval classi er (IC) which generates
a classi cation function for each group that can be used to e ciently retrieve all instances of the speci-
ed group from the population database. To allow IC
to be embedded in interactive loops to answer adhoc
queries about attributes with missing values, IC has
been designed to be e cient in the generation of classi cation functions. Preliminary experimental results
indicate that IC not only has retrieval and classi er
generation e ciency advantages, but also compares favorably in the classi cation accuracy with current tree classi ers, such as ID3, which were primarily designed for minimizing classi cation errors. We also describe some new applications that arise from encapsulating the classi cation capability in database systems and
discuss extensions to IC for it to be used in these new
application domains.
Current address: Computer Science Department, Rutgers University, NJ 08903 Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment.
PVaroncceoeudvienrg,sBorfittihshe 1C8otlhumVbLiDa,BCCanoandfeare1n9c9e2

With the maturing of database technology and the successful use of commercial database products in business data processing, the market place is showing evidence of increasing desire to use database technology in new application domains. One such application domain that is likely to acquire considerable signi cance in the near future is database mining 4] 10] 12] 16] 18] 17] 20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making. One application of database mining involves the ability to do classi cation in the database systems. Target mailing is a prototypical application for classi cation, although the same paradigm extends naturally to other applications such as franchise location, credit approval, treatment-appropriateness determination, etc. In a target mailing application, a history of responses to various promotions is maintained. Based on this response history, a classi cation function is developed for identifying new candidates for future promotions. As another application of classi cation, consider the store location problem. It is assumed that the success of the store is determined by the neighborhood characteristics, and the company is interested in identifying neighborhoods that should be the prime candidates for further investigation for the location of next store. The company has access to a neighborhood database. It rst categorizes its current stores into successful, average, and unsuccessful stores. Based on the neighborhood data for these stores, it then develops a classi cation function for each category of stores, and uses the function for the successful stores to retrieve candidate neighborhoods. The problem of inferring classi cation functions from a set of examples can be formally stated as follows. Let
G be a set of m group labels fG1 G2 . . . Gmg. Let A

be a set of Let dom(Ai

n )

attributes (features) fA1 A2 . . . Ang.
refer to the set of possible values for

ajetcttrsibDuteinAwi.hicWh eeaacrhe

given object

a is

large database of oba n-tuple of the form

<nobotjevco1tnsve2ino.f.D.Aiv.anreI>nnoowtthhkeenrreowwvonir.d2s,Wtdehoemarg(eAroiau)lpsoalnagdbiveGelsn

is of a

set of example objects E in which each object is a

(n + vi 2 jects

1)-tuple of the form

dinomE (Ahaiv) eatnhde

gk 2
same

< G.

v1 v2 . . . In other

vn gk > where words, the ob-

attributes as the objects in

D, and additionally have group labels associated with

them. The problem is to obtain m classi cation func-

tEianil,osgnowfsri,jtehof:entAreht1efoocrtlhaAeesa2sceihxacg.amr.to.piouAlenpnsfGsu!ejnt,cGEtuijsoainnsfogtfrhjtjehfeot=rrian1gifnroo.irn.umg.pasmGteito.jnaWbniende-

the database D as the test data set.

This problem has been investigated in the AI and

Statistics literature under the topic of supervised learn-

ing (see, for example, 6] 11] 12] 17]) 1. We put the

following additional requirements, not considered in

the classical treatment of the problem, on the classi -

cation functions:

1. Retrieval E ciency: The classi cation function
should be able to exploit database indexes to minimize the number of redundant objects retrieved for nding the desired objects belonging to a group. Currently, database indexes can only be used for queries involving predicates of the form Ai v (point predicates), or v1 1 Ai 2 v2 (range predicates), or their conjuncts and disjuncts, where , 1, and 2 are appropriate comparison operators. Retrieval e ciency has not been of concern in current classi ers, and it di erentiates classi ers suitable for database mining applications from the classi ers used in applications such as image processing. In an image processing application (e.g. character recognition), having developed a classication function, the problem usually is to classify a given image into one of the given groups (a character in the character recognition application). It is rare that one uses the classi er to retrieve all images belonging to a group.

2. Generation E ciency: The algorithm for generat-
ing the classi cation functions should be e cient.

1The other major topic in classi cation is unsupervised learning. In unsupervised classi cation methods, clusters are rst located in the feature space, and then the user decides which clusters represent which groups. See 9] for an overview of clustering algorithms.

The emphasis in the current classi ers has been on minimizing the classi cation error and generation e ciency has not been an important design consideration. This has been the case because usually the classi er is generated once and then is used over and over again. If, however, classi ers were to be embedded in an interactive system or the training data changes frequently, generation e ciency becomes important.

Due to the requirement for retrieval e ciency, a clas-

si er requiring objects to be retrieved one at a time

into memory from the database before the classi ca-

tion function can be applied to them is not appropri-

ate for database mining applications. Neural nets (see

11] for a survey) t in this category. A neural net is a

xed sized data structure with the output of one node

feeding into one or many other nodes. The classi -

cation functions generated by neural nets are buried

in the weights on the inter-node links. Even artic-

ulating these functions is a problem, let alone using

them for e cient retrieval. Neural nets learn classi -

cation functions by multiple passes over the training

set till the net converges, and have poor generation ef-

ciency. Neural nets also do not handle non-numerical

data well.

Another important family of classi ers is based on de-

cision trees (see 7] 6] 12] for an overview). The basic
idea behind tree classi ers is as follows 13]. Let E be a

nite collection of objects. If E contains only objects

of one group, the decision tree is just a leaf labeled

with that group. Otherwise, let T be any test on an

object object

with
in E

possible will give

outcomes O1 one of these

O2 . . . Ow. outcomes for

Each T , so

Tttebhrmyeopepasatefdryoto,eribcteaiijasoelcilncohtsonsfEEthiEraeii.vnseiAtsnfomosgrfalooEEluln1iet,grcEtohat2mhse.aetr.nwe.OsEoEuiw.,lotgaIrwfnwmedoaiuotschrlihdenEEcbEieiie'icEssaoarnidsertepeacliinannscioiitnoenedng-,

this procedure will terminate.

ID3 (and its variants such as C4.5) 13] 14] and CART

2] are the best-known examples of tree classi ers.

These decision trees usually have a branch for every

value of a non-numeric attribute at a decision node.

A numeric attribute is handled by repeated binary de-

composition of its range of values. The advantage of

the binary decomposition is that it takes away the bias

in favor of attributes with large number of values at

the time of attribute selection. However, it has the dis-

advantage that it can lead to large decision trees, with

unrelated attribute values being grouped together and

with multiple tests for the same attribute 13]. Mor-

ever, binary decomposition may cause large increase

in computation, since an attribute with w values has a

computational requirement similar to 2w;1 ; 1 binary
attributes 13]. The essence of classi cation is to construct a decision tree that correctly classi es not only objects in the training set but also the unseen objects in the test data set 13]. An imperfect, smaller decision tree, rather than one that perfectly classi es all the known objects, usually is more accurate in classifying new objects because a decision tree that is perfect for the known objects may be overly sensitive to statistical idiosyncrasies of the given data set 2] 15]. To avoid over tting the data, both ID3 and CART rst obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves 2] 14] 15]. Developing the full tree and then pruning it leads to more accurate trees, but makes classi er generation expensive.
The interval classi er (IC) we propose is also a tree
classi er. It creates a branch for every value of a nonnumeric attribute, but handles a numeric attribute by decomposing its range of values into k intervals. The value of k is algorithmically determined separately for
each node. Thus, for numeric attributes, IC results in k-ary trees, and does not su er from the disadvantages of the binary trees. IC does dynamic pruning
as the tree is expanded to make the classi er generation phase e cient. By limiting tests at decision nodes
to point and range predicates, IC generates decision trees that decompose the feature space into nested n-
dimensional rectangular regions, each of which can be speci ed as a conjunction of point and range pred-
icates. IC can, therefore, generate SQL queries for
classi cation functions that can be optimized using the relational query optimizers and can exploit database indexes to realize retrieval e ciency. The organization of the rest of the paper is as follows.
In Section 2, we present the IC classi er generation
algorithm. In Section 3, we present the results of the
empirical evaluation of the performance of IC. We consider the sensitivity of IC to various algorithm pa-
rameters and the noise in the training and test data.
We also present results comparing IC to ID3. Besides
presenting a classi er suitable for database mining applications, a secondary goal of this paper is to argue that database mining is an important research topic requiring attention from database perspective. In Section 4, we describe some new problems that arise from encapsulating the classi cation capability in database systems, which have not been considered in the classication literature. We also discuss extensions that will
allow IC to be used in these new application domains.
We conclude in Section 5.

2 IC Generation Algorithm

We assume for simplicity that the population database

D consists of one relation. Such a relation can usually

be obtained by appropriate joins. Each tuple of this

relation has n attributes. Every tuple belongs to one

of m groups in the population, but the group label is not known for the tuples in D. We also have a training

sample E of tuples. Tuples in E are structurally identical to tuples in D, except that the training tuples have

an additional attribute specifying their group label.

Attributes can be categorical or non-categorical. Cat-

egorical attributes are those for which there are a nite

discrete set of possible values. The number of possible

values is usually small and have no natural ordering

to allow interpolation between two values. Examples

of categorical attributes include \make of car", \zip

code", etc. Other attributes are non-categorical. Ex-

amples of non-categorical attributes include \salary",

\age", etc.

We de ne an interval to be a range of values for a non-

categorical attribute or a single value for a categorical

attribute. Tuples having values for an attribute falling

in an interval are said to belong to that interval. Each

group can be assigned a count of the tuples belong-

ing to an interval of an attribute with that group as
the label. The function winner grp uses the group

counts to determine the winning group for an inter-
val. A function called winner strength categorizes

each winning group as a strong winner or a weak win-

ner. The corresponding interval is then called a strong

interval or a weak interval.

IC generation consists of two main steps. The function make tree creates the decision tree, leaves of which

are labeled with one group label. A tree traversal algo-

rithm then generates a classi cation function for each

group by starting from the root and nding all paths

to a particular group at the leaves. Each path gives

rise to a conjunction of terms, each term being a point

predicate or range predicate. Disjunction of these con-

junctions, one corresponding to each path for a group,

yields the classi cation function for the group. We will
only describe the function make tree here the gener-

ation of classi cation functions from the decision tree

is fairly straightforward.

The function make tree has a recursive structure. It

works on an interval (or subdomain) of an attribute.

Initially, it is given the entire domain of each attribute.

One of the attributes is selected to be the winner at-
tribute in the classi cation predicate (see next attr).

A goodness function is used for this determination. It

then uses the tuples belonging to the input subdomain

to partition the strong and weak

dinotmeravianlso(fsteheemwaikneneirnatettrrvibaulst)e.

into De-

cisions regarding the winning group are made nal for the strong intervals of the winner attribute. The function then recursively processes the weak intervals of the winner attribute. The function terminates when a stopping criteria is satis ed. For ease of exposition, in Figure 1, we present a ver-
sion of the function make tree in which several de-
tails have been omitted. In the pseudo code below, we present a more detailed version of the function
make tree.
// Determine the best attribute to use next // for classification func next_attr(H: Histograms) returns Attr {
For every attribute A do { Compute the value of the goodness function for A
}
Let winner_attr = attr with the largest value for the goodness function // Example of a goodness function is // the information gain see Remarks
Return winner_attr }
// Partition the domain of attribute into // intervals. proc make_intervals(A: Attribute,
H: Histograms) {
For each value v in histogram of A { // Determine winning group for value v // using histograms in H // Example: return the group that has // the largest frequency for the value v winner = winner_grp(H, A, v)
// Determine if winner is strong or weak // Example: return strong if the ratio of // the frequency of the winning group to // the total frequency for the value v // is greater than a specified threshold strength =
winner_strength(winner, H, A, v)

by merging adjacent values that have the same winner with the same strength } else { Each value forms an interval by itself // i.e., the left and right endpoints // of the interval are the same } }
// Procedure to build classification tree. // Called as make_tree(training_set) function make_tree(tuples: Tupleset) returns TreeNode {
If stopping criteria is satisfied // see Remarks below return NULL
Create a new tree node N For each group G and attribute A do
make_histogram(G, A, tuples)
For every non-categorical attribute do Smooth the corresponding histograms // see Remarks for smoothing procedure
Let H be the resultant set of histograms for all attributes
winner_attr = next_attr(H) make_intervals(winner_attr, H)
Save in N the winner_attr and also the strong and weak intervals corresponding to the winner_attr for all groups
For each weak interval WI of the winner_attr do { remaining_tups = training set tuples satisfying the predicate for WI child of WI = make_tree(remaining_tups)
}
return N }

Save the winner and strength information for value v of attribute A
}

If the domain of A can be ordered { Form intervals of domain values

procedure make_tree (tupleset T) Partition T according to groups  groups G  attributes A
Obtain histogram of G tuples over domain of A
Apply goodness function to select winner attribute A' Partition domain of A' into strong and weak intervals
Each strong interval is assigned the winner group  weak intervals I of A' having tupleset TI make_tree (TI )

Figure 1: Procedure make tree

REMARKS: In the above description of the IC generation algo-
rithm, we did not specify bodies of some of the functions. Our intention was to present a generic algorithm from which a whole family of algorithms may be obtained by instantiating these functions with di erent decision modules. We now discuss the speci c functions used in our implementation and also suggest some alternatives.
winner grp: The function winner grp(H, attr, v) re-
turns the group that has the largest frequency for the value v of the attribute attr in histograms H. It is possible to use weighting if it is desired to bias the selection in favor of some speci c groups.
next attr: The function next attr(H) is greedy, and
selects the next branching attribute by considering one attribute at a time. (The problem of computing optimal decision trees has been shown to be NP-complete 8].) We consider two goodness functions: one min-

imizes the resubstitution error rate, the other maximizes the information gain ratio. Other possibilities for the goodness function include the cost of evaluating a predicate. The resubstitution error rate 2] for an attribute is computed as
1 ; Xvwinner freq(v)=total freq

where winner freq(v) is the frequency of the winning

group for the attribute value v, and total freq is the

total frequency of all groups over all values of this attribute in histograms H.

The information gain ratio is an information theoretic

measure proposed in 13]. Let the example set E of

e objects contain entropy E of E is

ek objects given by

of

group

Gk.

Then the

E

=

;Xk

ek e

log2

ek e

IaftttfhhhEseeai1attstthErutherei2baiebtvb.rwuer.eta.ievetnahoEclAfhuiwAEieigniijgawawjiissiatiotthEthfhtirjeAvi,Ebaritijul.ohutoeceIetnfos, nittsifhthtaateehii1wneeeixiaxnlwpi2lpgeeep.icceg.attjih.eredttdoeaitbdeiweionjngeanttcvrrtioeosEsrppayyuoginfseffetooEdorr

Ei

=

X
j

eji e

Eij

The information gain by branching on Ai is therefore

gain(Ai) = E ; Ei

Now, the information content of the value of the attribute Ai can be expressed as

I

(Ai

)

=

;Xj

eij e

log2

eji e

The information gain ratio for attribute Ai is then de ned to be the ratio

gain(Ai)=I(Ai)

If two attributes are equally attractive, ties are currently broken by arbitrarily picking one of them. One could use additional criteria, such as the length of description, selectivity of attributes, etc. to break the ties.
winner strength(winner, H, attr, v): The func-
tion winner strength returns the strength as strong if the ratio of the frequency of the winning group winner to the total frequency for the value v of the attribute attr in H is above a certain precision threshold. Again, other criteria may be used to determine the strength. For example, besides precision threshold, one may require that there be certain minimum frequency at the value v before the winner is classi ed as strong. The precision threshold may have a xed value. For example, a xed precision threshold of 1 has the e ect that a winner is declared strong if instances of only the winning group are present. The precision threshold can also be an adaptive function of the current depth of the classi cation tree. The adaptive precision threshold we use is given by

1 ; (curr depth=max depth)2

This function is conservative in the beginning in declaring a winner strong, but loosens the criteria as the tree grows.
Smoothing: Conceptually, we handle a non-
categorical attribute by rst generating a smooth frequency distribution from the histogram of its values. This distribution is then sampled at equi-distant points in its range of values. The number of sampling points is given by

max(minimum sampling points, sampling multiplier number of distinct values in the histogram) where minimum sampling points and the sampling multiplier are algorithm parameters. The smooth frequency distribution is not generated in practice. Rather, the frequency at a sampling point is determined using the following procedure.
cGaatittvreeigbnourtiaecaflhoiarstttvoragilbruauemtev,if,w(whveierfenie)fgeidisoafthwveaafylrueetqsoueionnftceyarpoofnlotahntee-
for frequency for values not present in the histogram. Following the technique in 5], the frequency f for a value v is determined by considering the contribution of all values that occur in the histogram within an interval of length h centered at v. One way to interpret
tvvhai +ilsuhem=ve2ithaaocscdboeirsidntignogstmhtoienaakrweodefifgorhveeqtrufuethnnecctyiinoftnie.orvfIfaelwvveeirly;etshaW=m2(putloe)
be the \boxcar" weight function:

W (u) =

1 if abs(u) < 1=2 0 otherwise

then smearing can be thought of as replacing every the frequency fi at value vi by the function

fi W ((v ; vi)=h)=h

The total area under is fi. The smoothed

the curve frequency

ffiat

Wany((vva;luvei)v=hth)=ehn

is the sum of smears from all sample values:

f = 1=n Xn fi W ((v ; vi)=h)=h
i=1

Instead of the \boxcar" function, we use the raised cosine arch function:

W (u) =

1 + 2 cos(2 u) if abs(u) < 1=2 0 otherwise

This function decreases gradually from 0 to 1/2 and symmetrically from 0 to -1/2. Note that the area under the raised cosine arch is 1 and that the in uence distance h represents a trade-o between smoothing and locality in terms of how much we want to constrain the smearing of frequency.
Stopping condition: Further branching from a node
does not take place if all the intervals for the corresponding attribute are found to be strong. Similarly, if there are no tuples (or less than a speci ed number of tuples) in some range of values for the selected attribute, the corresponding interval is not further expanded. The winning group of the parent node is made the winner group in this empty interval. Branching may also be limited by specifying a maximum tree

depth, in which case all weak intervals in a leaf node

are treated as strong.

IC also provides a dynamic tree pruning criteria. For

each node, an expansion merit is computed, and a

node is expanded only if this merit is above an accept-

able level. The expansion merit is based on the ideas

in 14]. Suppose a tree T has been generated using N

cases in the training set. Let some leaf account for K

of these cases with J of them misclassi ed. The ratio

J=K does not provide a reliable estimate of the error

rate of the leaf for unseen cases, since the tree has been

tailored to the training set. A more realistic error rate

is obtained by applying continuity correction for the

binomial distribution 19] in which J is repalaced by

toJahfn+edP1ll=eeKa2tv.PeLusenJotsfeSaeSnnb.decPSaassweKusib.llbtrmeNeetioshwoceflaclTseostrcirfoEyenstPpbaoiennJidtnh+ignegLLns((uSuSmm))=bls2eeoarovvueeotrsf

number of cases from the training set that a node mis-

classi es. This node beyond one standard

eirsroerxpoafnPdeJd

only if E + L(S)=2.

+

1=2

is

To avoid pruning too aggressively, IC also supports a

lookahead procedure. The basic idea is that each node

inherits from its parent a certain number of lookahead

credits. If an expansion of a node does not result in

acceptable error reduction, the node is still expanded

if it still has credits left. Such a node passes one less

credit to its children. If an acceptable level of error

reduction takes place at a node, its credits are reset to

the maximum. Further expansion does not take place

if a node does not have any credit left and the amount

of error reduction is not acceptable.

2.1 Example
We illustrate the IC classi er generation with a simple
example. Consider a people database in which every tuple has only three attributes:
age (age) { non-categorical attribute { uniformly
distributed from 20 to 80
the zip code of the town the person lives in (zipcode) { categorical attribute { uniformly dis-
tributed between 9 available zipcodes
level of education (elevel) { categorical attribute
{ uniformly distributed from 0 to 4
Group membership depends only on age and elevel, and is independent of zipcode. There are only two
groups in the population:
Grp A: ((age < 40) ^ (elevel 2 0::1])) _ ((40 age < 60) ^ (elevel 2 0::3])) _ ((60 age) ^ ((elevel = 0)))
Grp B: otherwise

where (elevel 2 1::k]) is equivalent to ((elevel = 1) _

(elevel = 2) _ . . . _ (elevel = k)). We have a training

set of 1000 tuples that satisfy the above predicates.

IC rst generates three histograms, one each for val-

uheisstoignraamgef,orzitphecondoen,-caantedgoerliecvalela.ttrIitbustme oaogthe,s

the and

chooses 100 equi-distant sampling values (an algorithm

parameter) from the range of values for age. For each

attribute value, it nds the winning group using the

function winner grp. The winning group for an at-

tribute value is simply the group that has the largest

frequency for that attribute value. We assume that

the next attribute selection is based on the minimiza-

tion of resubstitution error rate. The resubstitution

error rate for an attribute is determined by adding for

every value of the attribute the frequency of the win-

ning group, dividing this sum by the total frequency,

and subtracting this ratio from 1. The following are

the values obtained for the resubstitution error rate

for the three attributes:

Attribute Error Rate
age .227306 zipcode .432000 elevel .254000

Therefore, the function next attr selects age, which

has the minimum resubstitution error rate, as the next

branching attribute.
IC uses the function winner strength for each sample value of age to determine whether the winning

group is strong or weak for that value. For a winner

to be strong, the ratio of the frequency of the winning

group to the total frequency at that value should be at

least equal to a precision threshold. We used adaptive

precision to dynamically adjust this threshold. For

level 0 node, of the values

othf eagveal,uteheofwtihninsetrhgreroshuopldisinso1t.

For any found to

be strong.

aTghee

function make
into intervals by

interval partitions the domain of
merging adjacent values that have

the same winner. The following three intervals are

formed:

Attribute: age

Interval Winner Strength

20.00, 39.59) Group B Weak

39.59, 59.79) Group A Weak

59.79, 80.61) Group A Weak

Since all the three intervals are weak, the tree is fur-
ther developed for them. IC partitions the original
training set into three sets corresponding to the above
three range of values for age, and the algorithm re-
peats for each set of training tuples. Let us consider the processing for the rst set.

First, histograms of attribute values are developed for the tuples belonging to the reduced set. The resubstitution error rates are computed for the selection of the next attribute and the following values are obtained:

Attribute Error Rate
age .024345 zipcode .340176 elevel 0

Therefore, elevel is selected as the next branch0in.7g5abttyribthuete.adTaphteivperepcriescioisniotnhraelsghooriltdhmis ,readnudcewdintoner strength nds the winning group to be strong for every value of elevel. Since elevel is a categori-
cal attribute, each of its values is considered to be an interval and we have the following intervals:

Attribute: elevel
Interval Winner 0] Group A 1] Group A 2] Group B 3] Group B 4] Group B

Strength Strong Strong Strong Strong Strong

Since all the intervals are strong, the tree is not grown further. If all or some of the intervals were weak, the algorithm would develop the tree further for those intervals. Figure 2 shows the decision tree generated
by IC. It is a coincindence that the next attribute

age

not be balanced | di erent branches could grow to di erent depth.
Thus, IC infers the following classi cation functions Grp A: ((20 age < 39:6) ^ (elevel 2 0::1])) _
((39:6 age < 59:8) ^ (elevel 2 0::3])) _ ((59:8 age < 80:6) ^ ((elevel = 0)))

Grp B: ((20 age < 39:6) ^ (elevel 2 2::4])) _ ((39:6 age < 59:8) ^ (elevel = 4)) _ ((59:8 age < 80:6) ^ (elevel 2 1::4]))

which is very close to the original rules. Note that the actual age range in the training set was from 20 to 80. It is easy for someone familiar with SQL to see how
IC can generate SQL queries from the decision tree it
synthesizes. Let us remark on two termination conditions that are not illustrated in the above example. Firstly, suppose we had limited the classi er depth to 1. Since the algorithm treats all weak intervals in the leaf node as strong, we would have obtained the following classi cation functions:
Grp A: ((39:6 age < 59:8))

Grp B: ((20 age < 39:6) _ (59:8 age < 80:6))

The other case arises when there are less than a certain

speci ed number of tuples in some range of values for

the selected attribute. For example, having selected

tehleevreelisasntohetunpelxet faotrtreibleuvteelas=ab0.ovTe,hwene ,mtahye

nd that winning

group of the parent node is made the winner group in

this empty interval.

[20, 39.59) [39.59, 59.79) [59.79, 80.61)

elevel

elevel

elevel

01234 01234 01234 AABBB AAAAB ABBBB
Figure 2: Example Decision Tree Generated by IC selected for each of the three intervals of age turned out to be elevel. In general, the siblings may not be
the same attribute and di erent attributes may be selected for each of the intervals. Also, the tree need

3 Performance
The goodness of a classi er has several dimensions:
1. Generation E ciency: How e cient is the classi er
generation process.
2. Retrieval E ciency: How e cient is the classi er
in retrieving all instances of a speci ed group.
3. Classi cation Accuracy: How correct is the classi-
cation of instances into groups. Compared to ID3 and CART, the generation e ciency
in IC stems from doing k-ary decomposition, instead of
binary decomposition, of the range of a non-categorical attribute, and from using dynamic pruning, rather
than back-tracking. Since IC develops k-ary trees, instead of binary trees, the trees in IC should be smaller
and shallower. This should result in smaller queries, leading to better retrieval performance.

The classi cation error, that is, the fraction of instances in the test data that are incorrectly classied, is the classical measure of the quality of a classier. We performed several experiments to empirically determine the accuracy of the classi cation functions
generated by IC. We rst describe the experimen-
tal set up and the evaluation methodology, and then present some results, including comparison with ID3.

3.1 Methodology

We developed synthetic data to empirically evaluate
the classi cation errors produced by IC. The synthetic dthaetaniins efoarttaripbeurtseosngdivaetnabinasTeaibnlew1h.icAhtetaricbhupteesrseolnevheals, car, and zipcode are categorical attributes, all other
are non-categorical attributes. Attribute values were randomly generated. There is also a derived attribute
called equity, de ned as follows:

hyears < 20 =) 0 hyears 20 =) 0:1 hvalue (hyears ; 20)

We developed a series of classi cation functions of in-

creasing complexity that used the above attributes to

classify people into di erent groups. There are 5 func-

tions, labeled 1 through 5, involving 2 groups. Func-

tions 1, 2, and 3 involve predicates with ranges on one,

two, and three attribute values respectively. Function

4 is a linear function and Function 5 is a non-linear

function of attribute values. These functions are listed

in Appendix A.

For every experiment, we generated a training set and

a test data set. Tuples in the training set were assigned

the group label by rst generating the tuple and then

applying the classi cation function on the tuple to de-

termine the group to which the tuple belongs. Labels

were also generated for tuples in the test data set as

per the classi cation function to determine whether

the classi er correctly identi ed the group for the tu-

ple or not.

It is rarely the case that the boundaries between the

groups are very sharp. To model fuzzy boundaries, the

data generation program takes a perturbation factor

p as an additional argument. After determining the

values of di erent attributes of a tuple and assigning it

a group label, the values for non-categorical attributes

are perturbed. tuple t is v and

If the value of an the range of values

attribute of Ai is a,

Ai for a then the

value of where r

Ai for t after perturbation becomes v+r p a, is a uniform random variable between -0.5 and

+0.5.

For each experimental run, the errors for all the groups

are summed to obtain the classi cation error. For each

classi cation function, 100 replications were done with

new training sets being generated. The replications were then used to calculate the mean error with 95% con dence intervals. Errors are reported as percentages of the total test data set. In cases where the test data was perturbed, the intrinsic error in the test data was subtracted from the total error to arrive at the error due to misclassi cation. We used training sets of 2500 tuples and test data sets of 10000 tuples. Before settling on these sizes, we stud-
ied the sensitivity of IC to these sizes. The training set
was reduced from 2500 tuples to 1000 tuples in steps of 500. As expected, the classi cation error increased with decreasing training set size, but the increase in mean error was small. In database mining applications involving databases in gigabytes, the training sets are likely to be fairly large, and training sets of 2500 tuples are not unreasonable. We increased the test data sizes from 10000 to 25000, 50000, and 100000 tuples. The results indicated that 10000 tuples provided almost identical error estimates as larger test data sets, and we decided to stay with 10000 tuple test data sets to conserve computing time.
3.2 Classi er Accuracy
The rst set of experimental results presented in Figure 3 show the classi cation error rates for the ve functions. The results have been shown for the follow-
ing versions of IC: 1. Error Pruning: This version used pruning based
on error reduction with lookahead described in Section 2. A lookahead of 5 was used, and no interval was declared strong unless all the tuples in that interval belonged to the winning group (i.e., precision threshold = 1.0).
2. Adaptive Precision: In this version adaptive
precision threshold is used for deciding winner strength. A maximum depth of 10 was used to limit the growth of the tree.
3. Fixed Precision: This version used a xed preci-
sion threshold of 0.9. A maximum depth of 10 was also used in this version. All the three versions used information gain ratio as the criteria for the next attribute selection. The values of all the non-categorical attributes were perturbed by 5% for all the tuples in the training set and the test data set. The minimum sampling values for the non-categorical attributes were 100 and the sampling multiplier of 0.10 was used. The rst three functions partition the attribute space into hyper-parallelopipeds (n-dimensional rectangular
regions). IC works very well for such classi cation

Attribute

Description

Value

salary

salary

uniformly distributed from 20000 to 150000

commission commission

salary >= 75000 =) commission = 0 else

uniformly distributed from 10000 to 75000

age age

uniformly distributed from 20 to 80

elevel

education level

uniformly chosen from 0 to 4

car make of the car uniformly chosen from 1 to 20

zipcode zip code of the town uniformly chosen from 9 available zipcodes

hvalue

value of the house uniformly distributed from 0:5k100000 to 1:5k100000

where k 2 f0 9g depends on zipcode

hyears

years house owned uniformly distributed from 1 to 30

loan total loan amount uniformly distributed from 0 to 500000

Table 1: Description of Attributes

Classification Error (%)

20
Fixed Precision Adaptive Precision
15 Error Pruning

10

5

0 1

234 Function Number

5

Figure 3: Comparing 3 Versions of IC

functions. Functions 4 and 5 partition the attribute
space into hyper-polyhedra. Now IC has to approximate the partitioning using n-dimensional rectangular
regions. Hence the error is expected to increase.
The performance of the three versions of IC is pretty
close. We were somewhat surprised by the superiority (albeit, only little) of xed and adaptive precision versions over the error pruning version as they are much simpler and computationally much cheaper algorithms. Although we have presented the results only for perturbation of 5%, similar results were obtained for other perturbation values.
The following is a summary of the results from other sensitivity experiments (the constraint on number of pages prohibits us from presenting data from these experiments):

The information gain ratio performed somewhat better than the resubstitution error rate as the criterion for the next attribute selection, but the di erence was not large. The resubstitution error is a computationally cheaper metric. Increasing lookahead to 10 did not improve the performance of error pruning. A lookahead of 2 did as well as 5. Increasing the maximum depth to 15 did not help adaptive precision. The error rate with maximum depth of 5 were not very di erent from that of maximum depth of 10. It supports our conjecture that we may work with shallower trees when using
IC. The performance of IC seems to be sensitive to
the smoothing parameters. We need to explore the parameter space further, and develop better understanding of it. Our conjecture is that we can
improve the performance of IC by ne tuning the
smoothing parameters.
3.3 Sensitivity to Noise
Insensitivity with respect to noise in the training and test data is an important quality of classi ers. Figure 4
shows the error rates for the Adaptive Precision IC for
di erent amounts of perturbations in the data. These experiments were performed with the maximum depth of the tree set to 10.
The results show that IC is fairly stable. Errors in-
creased as expected for Functions 1, 2, and 3, but only very moderately. We were surprised by reduction in error rates with increase in perturbation for Functions 4 and 5. But we found similar behavior with ID3. The

Classification Error (%) Classification Error (%)

20
15 f1
f2 f3 f4
10 f5
5

15 Adaptive Precision ID3
10
5

0
0 2.5 5 7.5 10 Perturbation (%)
Figure 4: Sensitivity to Noise of Adaptive Precision

0 1

234 Function Number

5

Figure 5: Comparing IC to ID3

explanation seems to be that since we subtract intrinsic error rate in the test data from the observed error rate to arrive at the error rate due to the classi er, some of the tuples that would have been reported as misclassi ed end up being in the intrinsic error pool, bringing the e ective error rate down.
3.4 Comparison with ID3
We obtained the IND tree package 3] from the NASA Ames Research Center and ported it to IBM RISC
System/6000 to compare the performance of IC with
other classi ers. Experiments have been performed by the IND designers to ensure that IND reimplements C4 reasonably well. We present in this section the results of the comparison of the classi cation errors produced
by ID3 (really C4) and IC.
Figure 5 shows the comparative error rates for the
Adaptive Precision IC and ID3 for 5% perturbation
in training and test data. For Function 5, ID3 beats
IC (12.5% vs. 17.5% average error rate), but IC beats
ID3 for Function 2 (4.4% vs. 10.5% average error rate). The error rates for other functions are quite close, with ID3 doing a little better. A maximum depth of 10 was
used for the adaptive Precision IC. The di erence in
error rates did not change much between the two algorithms for di erent perturbation values (data not shown here), except that the performance of the two became identical for perturbation = 0 for Function 1, and for perturbation = 10% for Function 3. Given that
IC does dynamic pruning to gain generation e ciency
and ID3 fully expands the tree and then backtracks to
prune it, the best we expected was that IC would come
close to ID3 in the classi cation accuracy. Hence we feel satis ed with the classi cation accuracy shown by

IC. We also think that the error accuracy of IC can
be improved by ne tuning the smoothing parameters.
4 New Applications
A secondary goal of this paper was to present the case that database mining applications lead to new interesting research problems. We now brie y summarize some of the new applications and research problems that we could identify in the context of our work on classi cation for database mining. These new applications call for integration of retrieval and classi cation components leading to a tight coupling of these functionalities. In fact, in our view, classi cation should be encapsulated as a function of database systems to meet these increasing demands. In order to do this, the generation, retrieval and classi cation times should be added as measures of quality of a classi cation method. We outline here three applications:
4.1 Best N problem
In target marketing, instead of requiring all individuals belonging to a group, very often the best N target individuals for a promotion are desired. The objective is to maximize pro t, where pro t is calculated as the di erence between the amount of money made on all cumulative sales resulting from positive responses cost of mailing - cost of retrieval of N candidates from the database. Due to the inclusion of retrieval cost, a solution with lower positive response may be selected over the one with the higher response, if the former has a much smaller retrieval cost.
The classi cation function generated by IC is a disjunction D of conjuncts C. Assuming that D selects

more than N tuples, the problem is to determine those conjuncts in C whose disjunction selects at least N tuples and the pro t is maximized at the same time. For each conjunct c in C, we can de ne two parameters: 1. The expected error of c, error(c) = 1 - return(c),
where return(c) is the expected number of positive responses from those targets that satisfy c. 2. The expected retrieval cost of c, denoted by retrieve(c). The pro t from using c is given by Pro t(c) = return(c) sale pro t ;
(mail cost(c) + retrieve(c)) where mail cost(c) is the cost of mailing to the targets satisfying conjunct c. In the worst case, the conjunct c can be retrieved by a sequential scan of the database, in which case the retrieval cost in number of I/Os will equal the number of blocks B that the database occupies. Assume that the database has indexes available on some attributes. jEuasctehdinsedleexcteidvitayt:trsib(Autie),Awihiischchisartahcetearvizeerdagbeynaunmbaderof I/O (in blocks) necessary to access all records with AoeqfiuA=ail.atIonwthchaeerseeseaAleircathniavgsiettyshooevfceAlruiastdlleirveilinedgmediennbdtyesxtohtfehtehnneumsd(obAmeir)aioinsf records per block. If any indexed attribute appears in c, we may use the index to selectively retrieve records and then apply c to them. Assuming no index ANDing for simplicity, we can calculate the retrieval cost of c as minfB minfs(Ai) : Ai occurring in cgg Calculate the pro t for each conjunct c in C. Sort conjuncts according to the value of the pro t. Take rst K conjuncts that together cover N targets, remembering that the total I/O cost for K conjuncts ceases to be additive after the retrieval cost exceeds the number of blocks in the database. In that case the retrieval cost will be constant and the nal pro t will depend only on the response rate of the rst K conjuncts. Interesting open questions involve the performance of the above method compared to a hypothetical \special purpose" classi er that makes attribute selections on the basis of the base pro t gain for the Best N problem. We are currently working on this problem.
4.2 Adhoc Queries and Missing Data
Suppose that a market researcher would like to try a hypothetical new package which is similar to some of

the packages used in the past. She would like to estimate its performance on a population which was not a target for the previous mailing. For example, she may decide to test a package which has both ski vacation and a 3-day tour of Paris. If she had some past data about customers who, in the past, took a ski package and those who took Paris vacation, she may decide to \compute the pro le" for a union of the set of those customers and use it in estimating the (missing) values of the attribute corresponding to a new package and use this pro le on the new population. After that she may change her mind and modify slightly the package, starting the next iteration. These successive iterations capture the \ad hoc", unexpected, nature of the planning process for a new marketing campaign. For this scenario to become realistic, not only the expected retrieval time should be minimized but also the classi er generation time, since it contributes to the overall run time of the query. How do we decide what part of the classi cation task will be performed at compile time and which part at query run time?
4.3 Filters
A classi er that provides a rough classi cation but generates a pro le that has \good" retrieval properties can be used as a \ lter" for another classi er with good error characteristics, but poor retrieval properties. An example of a classi er with good error but poor retrieval is a neural net. A tree classi er, on the other hand could work as a lter, if it displays good retrieval performance. This involves, in general, building a classi er that best approximates a given \black box" function and has desirable retrieval characteristics.
5 Summary
We considered the problem of synthesizing classi cation functions for retrieving all instances of speci ed groups from a large database based on a representative sample of examples, and presented a tree-based
interval classi er (IC) for this purpose. The classi-
er is designed to be interfaced e ciently with the database systems. Since such classi ers may be embedded in interactive loops to answer adhoc queries
about attributes with missing values, IC has been de-
signed to be e cient in the generation of classi ca-
tion functions. The novel aspect of IC is its treatment
of non-categorical attributes. Instead of creating binary subtrees for such attributes as is the case with
ID3 and CART, IC creates k-ary subtrees, where k
is algorithmically determined for each node. Rather
than generating a full tree and pruning it, IC does
dynamic pruning as the tree is expanded to make the

classi er generation phase e cient. IC generates SQL
queries for classi cation functions that can be optimized using relational query optimizers to realize retrieval e ciency. Preliminary empirical comparison
with ID3 indicates that IC not only has retrieval e -
ciency and classi er generation e ciency advantages, but also compares quite favorably in the classi cation accuracy. We also argued that classi cation should be encapsulated as part of future database systems. It then opens up new application areas for classi ers, not hitherto considered in the classi cation literature. We de-
scribed extensions to IC for it to be used in these new
application domains, and also presented some interesting open problems. A by-product of this work has been the development of a systematic methodology for evaluating the performance of various classi ers. Our approach and the benchmarks we are developing allows one to systematically explore various operating regions. Our hope is that these benchmarks will serve the same role in classi er performance evaluation as the Wisconsin Benchmarks 1] played in the evaluation of relational query processing strategies. The work reported in this paper has been done in the context of the Quest project at the IBM Almaden Research Center. In Quest, we are exploring the various aspects of the database mining problem. Besides classi cation, some other problems that we have looked into include the enhancement of the database capability with \what goes together" kinds of association queries and queries over large sequences such as stock tables. We believe that database mining is an important application area for databases and we hope that it will be developed into an important research topic by the database community.
6 Acknowledgments
We wish to thank Byron Dom and Wayne Niblack for technical discussions and information on current classi ers. Wolf-Ekkehard Blanz, Dragutin Petkovic, and David Steele provided useful pointers into the image processing literature. Byron Dom, Bruce Lindsay, Allen Luniewski, Eli Messinger, and Wayne Niblack provided insightful comments on an earlier version of this paper. We thank Wray Buntine for providing us
the IND tree package that allowed us to compare IC
with ID3. Finally, we wish to thank Irv Traiger for bringing the problem of database mining to our attention.

7 Appendix A

In the following, (X 2 1::k]) is equivalent to ((X = 1) _ (X = 2) _ . . . _ (X = k)). Function 1 Grp A: ((age < 40) _ ((60 age)
Grp B: otherwise
Function 2 Grp A: ((age < 40) ^ (50K salary 100K)) _
((40 age < 60) ^ (75K salary 125K)) _ ((age 60) ^ (25K salary 75K))
Grp B: otherwise
Function 3 Grp A: ((age < 40)^
(((elevel 2 0::1]) ^ (25K salary 75K)) _ ((elevel 2 2::3]) ^ (50K salary 100K)))) _ ((40 age < 60)^ (((elevel 2 1::3]) ^ (50K salary 100K)) _ (((elevel = 4)) ^ (75K salary 125K)))) _ ((age 60)^ (((elevel 2 2::4]) ^ (50K salary 100K)) _ (((elevel = 1)) ^ (25K salary 75K))))
Grp B: otherwise
Function 4 disposable = (0:67 (salary + commission) ; 0:2 loan ; 10K Grp A: disposable > 0
Grp B: otherwise
Function 5 hyears < 20 =) equity = 0 hyears 20 =) equity = 0:1 hvalue (hyears ; 20)

disposable = (0:67 (salary + commission);

0:2 loan + 0:2 equity ; 10K)

Grp A:

disposable > 0

Grp B:

otherwise

References

1] Dina Bitton, David J. DeWitt, and Carolyn Turby ll, \Benchmarking Database Systems: A Systematic Approach", VLDB 83, Florence, Italy, 1983.
2] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classi cation and Regression Trees, Wadsworth, Belmont, 1984.

3] Wray Buntine, About the IND Tree Package, NASA Ames Research Center, Mo ett Field, California, September 1991.

4] Wray Buntine and Matha Del Alto (Editors), Collected Notes on the Workshop for Pattern Discovery in Large Databases, Technical Report FIA-9107, NASA Ames Research Center, Mo ett Field, California, April 1991.

5] John M. Chambers, William S. Cleaveland, Beat Kleiner, and Paul A. Tukey, Graphical Methods of Data Analysis, Wadsworth International Group (Duxbury Press), 1983.

6] Philip Andrew Chou, \Application of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises", Ph.D. Thesis, Stanford University, California, June 1988.

7] G. R. Dattatreya and L. N. Kanal, \Decision Trees in Pattern Recognition", In Progress in Pattern Recognition 2, L. N. Kanal and A. Rosenfeld (Editors), Elsevier Science Publishers B.V. (North-Holland), 1985.

8] L. Haya l and R. L. Rivest, \Constructing Opti-

mal Binary Decision formation Processing

Trees is Letters,

N5,P1-C, 1o9m7p6l,e1te5"-1, 7I.n-

9] A. K. Jain and R. C. Dube, Algorithms for Clustering Data, Prentice Hall, 1988.

10] Ravi Krishnamurthy and Tomasz Imielinski, \Practitioner Problems in Need of Database Research: Research Directions in Knowledge Discovery", SIGMOD Record, Vol. 20, No. 3, Sept. 1991, 76-78.

11] Richard P. Lippmann, \An Introduction to Computing with Neural Nets", IEEE ASSP Magazine, April 1987, 4-22.

12] David J. Lubinsky, \Discovery from Databases: A Review of AI and Statistical Techniques", AT&T Bell Laboratories, Holmdel, New Jersey, June 1989.

13] J. Ross Quinlan, \Induction of Decision Trees",
Machine Learning, 1, 1986, 81-106.

14] JIn. t.RJo.ssMQanu-iMnlaanch, in\eSimStpuldifiyesin,g27D, e1c9is8i7o,n22T1r-e2e3s4".,

15] J. Ross Quinlan and Ronald L. Rivest, \Inferring Decision Trees Using the Minimum Description Length Principle", Information and Compu-
tation, 80, 1989, 227-248.

16] G. Piatetsky-Shapiro and W. Frawley (Editors), Proceedings of IJCAI-89 Workshop on Knowledge Discovery in Databases, Detroit, Michigan, August 1989.
17] G. Piatetsky-Shapiro (Editor), Knowledge Discovery in Databases, AAAI/MIT Press, 1991.
18] G. Piatetsky-Shapiro (Editor), Proceedings of AAAI-91 Workshop on Knowledge Discovery in Databases, Anaheim, California, July 1991.
19] G. W. Snedecor and W.G. Cochran, Statistical Methods, 7th Edition, Iowa State University Press, 1980.
20] Shalom Tsur, \Data Dredging", IEEE Data En-
gineering Bulletin, 13, 4, December 1990, 58-63.

