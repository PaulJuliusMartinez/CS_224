Bagging, Boosting, and C4.5
J. R. Quinlan
University of Sydney Sydney, Australia 2006
quinlan@cs.su.oz.au

Abstract
Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classi er learning systems. Both form a set of classi ers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater bene t. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classi ers reduces this downside and also leads to slightly better results on most of the datasets considered.
Introduction
Designers of empirical machine learning systems are concerned with such issues as the computational cost of the learning method and the accuracy and intelligibility of the theories that it constructs. Much of the research in learning has tended to focus on improved predictive accuracy, so that the performance of new systems is often reported from this perspective. It is easy to understand why this is so { accuracy is a primary concern in all applications of learning and is easily measured (as opposed to intelligibility, which is more subjective), while the rapid increase in computers' performance/cost ratio has de-emphasized computational issues in most applications.1
In the active subarea of learning decision tree classiers, examples of methods that improve accuracy are: Construction of multi-attribute tests using logical combinations (Ragavan and Rendell 1993), arithmetic combinations (Utgo and Brodley 1990
1For extremely large datasets, however, learning time can remain the dominant issue (Catlett 1991 Chan and Stolfo 1995).

Heath, Kasif, and Salzberg 1993), and counting operations (Murphy and Pazzani 1991 Zheng 1995). Use of error-correcting codes when there are more than two classes (Dietterich and Bakiri 1995). Decision trees that incorporate classi ers of other kinds (Brodley 1993 Ting 1994). Automatic methods for setting learning system parameters (Kohavi and John 1995). On typical datasets, all have been shown to lead to more accurate classi ers at the cost of additional computation that ranges from modest to substantial. There has recently been renewed interest in increasing accuracy by generating and aggregating multiple classi ers. Although the idea of growing multiple trees is not new (see, for instance, (Quinlan 1987 Buntine 1991)), the justi cation for such methods is often empirical. In contrast, two new approaches for producing and using several classi ers are applicable to a wide variety of learning systems and are based on theoretical analyses of the behavior of the composite classi er. The data for classi er learning systems consists of attribute-value vectors or instances. Both bootstrap aggregating or bagging (Breiman 1996) and boosting (Freund and Schapire 1996a) manipulate the training data in order to generate di erent classi ers. Bagging produces replicate training sets by sampling with replacement from the training instances. Boosting uses all instances at each repetition, but maintains a weight for each instance in the training set that re ects its importance adjusting the weights causes the learner to focus on di erent instances and so leads to di erent classi ers. In either case, the multiple classi ers are then combined by voting to form a composite classi er. In bagging, each component classi er has the same vote, while boosting assigns di erent voting strengths to component classi ers on the basis of their accuracy. This paper examines the application of bagging and boosting to C4.5 (Quinlan 1993), a system that learns decision tree classi ers. After a brief summary of both methods, comparative results on a substantial number of datasets are reported. Although boosting generally increases accuracy, it leads to a deterioration on

some datasets further experiments probe the reason for this. A small change to boosting in which the voting strengths of component classi ers are allowed to vary from instance to instance shows still further improvement. The nal section summarizes the (sometimes tentative) conclusions reached in this work and outlines directions for further research.
Bagging and Boosting
We assume a given set of N instances, each belonging to one of K classes, and a learning system that constructs a classi er from a training set of instances. Bagging and boosting both construct multiple classiers from the instances the number T of repetitions or trials will be treated as xed, although Freund and Schapire (1996a) note that this parameter could be determined automatically by cross-validation. The classi er learned on trial t will be denoted as Ct while C is the composite (bagged or boosted) classi er. For any instance x, Ct(x) and C (x) are the classes predicted by Ct and C respectively.
Bagging
For each trial t = 1,2,...,T , a training set of size N is sampled (with replacement) from the original instances. This training set is the same size as the original data, but some instances may not appear in it while others appear more than once. The learning system generates a classi er Ct from the sample and the nal classi er C is formed by aggregating the T classi ers from these trials. To classify an instance x, a vote for class k is recorded by every classi er for which Ct(x) = k and C (x) is then the class with the most votes (ties being resolved arbitrarily).
Using CART (Breiman, Friedman, Olshen, and Stone 1984) as the learning system, Breiman (1996) reports results of bagging on seven moderate-sized datasets. With the number of replicates T set at 50, the average error of the bagged classi er C ranges from 0.57 to 0.94 of the corresponding error when a single classi er is learned. Breiman introduces the concept of an order-correct classi er-learning system as one that, over many training sets, tends to predict the correct class of a test instance more frequently than any other class. An order-correct learner may not produce optimal classi ers, but Breiman shows that aggregating classi ers produced by an order-correct learner results in an optimal classi er. Breiman notes:
\The vital element is the instability of the prediction method. If perturbing the learning set can cause signi cant changes in the predictor constructed, then bagging can improve accuracy."
Boosting
The version of boosting investigated in this paper is AdaBoost.M1 (Freund and Schapire 1996a). Instead of drawing a succession of independent bootstrap samples from the original instances, boosting maintains a

weight for each instance { the higher the weight, the

more the instance in uences the classi er learned. At

each trial, the vector of weights is adjusted to re ect

the performance of the corresponding classi er, with

the result that the weight of misclassi ed instances

is increased. The nal classi er also aggregates the

learned classi ers by voting, but each classi er's vote

is a function of its accuracy.

w1,h2Le,.er.te.,,Twf,xtoarcdleeavnseosrityeexrth,Cetwwix1seci=ognhst1t/roNufc.itnesdAtaftrnocemeactxhheatgrtiiavtlernitailn=t-

stances under the distribution wt (i.e., as if the weight

wTxht eoferirnosrtantcoef

x re this

ects its probability of occurrence). classi er is also measured with re-

spect to the weights, and consists of the sum of the

weights of the instances that it misclassi es. If t

is greater than 0.5, the trials are terminated and T

is altered to t-1. Conversely, if Ct correctly classi-

es all instances so that t is zero, the trials termi-

nate and T becomes t. Otherwise, the weight vec-

tor wt+1 for the next trial is generated by multiply-

Ping the weights of instances that Ct classi es correctly
by the factor t = t=(1 ; t) and then renormalizCCin1g,Cisso2o,t.b.h.t,aaCtinTe,dwxbhwyextr+seu1mthemqeuivnaolgstet1hf.oerTvcohltaeessbsioofoestrtheCedtccliaslaswsssioireterhrs

log(1= t) units.

Provided that t is always less than 0.5, Freund and

Schapire prove that the error rate of C on the given

examples under the original (uniform) distribution w1

approaches zero exponentially quickly as T increases.

A succession of \weak" classi ers fCtg can thus be

boosted to a \strong" classi er C that is at least as

accurate as, and usually much more accurate than, the

best weak classi er on the training data, Of course, this

gives no guarantee of C 's generalization performance

on unseen instances Freund and Schapire suggest the

use of mechanisms such as Vapnik's (1983) structural

risk minimization to maximize accuracy on new data.

Requirements for Boosting and Bagging

These two methods for utilizing multiple classi ers

make di erent assumptions about the learning system.

As above, bagging requires that the learning system

should not be \stable", so that small changes to the

training set should lead to di erent classi ers. Breiman

also notes that \poor predictors can be transformed

into worse ones" by bagging.

Boosting, on the other hand, does not preclude the

use of learning systems that produce poor predictors,

provided that their error on the given distribution can

be kept below 50%. However, boosting implicitly re-

quires the same instability as bagging if Ct is the same

as C erty

t;1, that

the
t

weight = 0.5.

adjustment scheme has the propAlthough Freund and Schapire's

speci cation of AdaBoost.M1 does not force termina-

tion when wt and all

t = 0.5, classi ers

t=1 from

in
Ct

this case on have

so that wt+1 = votes with zero

anneal audiology auto breast-w chess colic credit-a credit-g diabetes glass heart-c heart-h hepatitis hypo iris labor letter lymphography phoneme segment sick sonar soybean splice vehicle vote waveform
average

C4.5 err (%)
7.67 22.12 17.66 5.28 8.55 14.92 14.70 28.44 25.39 32.48 22.94 21.53 20.39
.48 4.80 19.12 11.99 21.69 19.44 3.21 1.34 25.62 7.73 5.91 27.09 5.06 27.33 15.66

Bagged C4.5 vs C4.5
err (%) w-l ratio 6.25 10-0 .814 19.29 9-0 .872 19.66 2-8 1.113 4.23 9-0 .802 8.33 6-2 .975 15.19 0-6 1.018 14.13 8-2 .962 25.81 10-0 .908 23.63 9-1 .931 27.01 10-0 .832 21.52 7-2 .938 20.31 8-1 .943 18.52 9-0 .908 .45 7-2 .928 5.13 2-6 1.069 14.39 10-0 .752 7.51 10-0 .626 20.41 8-2 .941 18.73 10-0 .964 2.74 9-1 .853 1.22 7-1 .907 23.80 7-1 .929 7.58 6-3 .981 5.58 9-1 .943 25.54 10-0 .943 4.37 9-0 .864 19.77 10-0 .723 14.11 .905

Boosted C4.5 vs C4.5
err (%) w-l ratio 4.73 10-0 .617 15.71 10-0 .710 15.22 9-1 .862 4.09 9-0 .775 4.59 10-0 .537 18.83 0-10 1.262 15.64 1-9 1.064 29.14 2-8 1.025 28.18 0-10 1.110 23.55 10-0 .725 21.39 8-0 .932 21.05 5-4 .978 17.68 10-0 .867 .36 9-1 .746 6.53 0-10 1.361 13.86 9-1 .725 4.66 10-0 .389 17.43 10-0 .804 16.36 10-0 .842 1.87 10-0 .583 1.05 10-0 .781 19.62 10-0 .766 7.16 8-2 .926 5.43 9-0 .919 22.72 10-0 .839 5.29 3-6 1.046 18.53 10-0 .678 13.36 .847

Boosting vs Bagging w-l ratio 10-0 .758 10-0 .814 9-1 .774 7-2 .966 10-0 .551 0-10 1.240 0-10 1.107 0-10 1.129 0-10 1.192 9-1 .872 5-4 .994 3-6 1.037 6-1 .955 9-1 .804 0-8 1.273 5-3 .963 10-0 .621 10-0 .854 10-0 .873 10-0 .684 9-1 .861 10-0 .824 8-1 .944 6-4 .974 10-0 .889 1-9 1.211 8-2 .938
.930

Table 1: Comparison of C4.5 and its bagged and boosted versions.

weight in the nal classi cation. Similarly, an over tting learner that produces classi ers in total agreement with the training data would cause boosting to terminate at the rst trial.

Experiments

C4.5 was modi ed to produce new versions incorpo-

rating bagging and boosting as above. (C4.5's facil-

ity to deal with fractional instances, required when

some attributes have missing values, is easily adapted

tTohhesaendvleerstihone si,nrsetfaenrcreedwteoigbhetslowwxt aus sbeadggbeyd

boosting.) C4.5 and

boosted C4.5, have been evaluated on a representative

collection of datasets from the UCI Machine Learning

Repository. The 27 datasets, summarized in the Ap-

pendix, show considerable diversity in size, number of

classes, and number and type of attributes.

The parameter T governing the number of classi ers

generated was set at 10 for these experiments. Breiman

(1996) notes that most of the improvement from bag-

ging is evident within ten replications, and it is inter-

esting to see the performance improvement that can be

bought by a single order of magnitude increase in computation. All C4.5 parameters had their default values, and pruned rather than unpruned trees were used to reduce the chance that boosting would terminate prematurely with t equal to zero. Ten complete 10-fold cross-validations were carried out with each dataset.2
The results of these trials appear in Table 1. For each dataset, the rst column shows C4.5's mean error rate over the ten cross-validations. The second section contains similar results for bagging, i.e., the class of a test instance is determined by voting multiple C4.5 trees, each obtained from a bootstrap sample as above. The next gures are the number of complete cross-validations in which bagging gives better or worse results respectively than C4.5, ties being omitted. This section also shows the ratio of the error rate using bagging to the error rate using C4.5 { a value
2In a 10-fold (strati ed) cross-validation, the training instances are partitioned into 10 equal-sized blocks with similar class distributions. Each block in turn is then used as test data for the classi er generated from the remaining nine blocks.

error (%) error (%)

chess colic

12 11

bbaogogstiningg

10

9

8

7

6

5

4

3

1 10 20 30 40 50

number of trials T

20 19

bbaogogstiningg

18

17

16

15

14 1 10 20 30 40 50
number of trials T

Figure 1: Comparison of bagging and boosting on two datasets

less than 1 represents an improvement due to bagging. Similar results for boosting are compared to C4.5 in the third section and to bagging in the fourth.
It is clear that, over these 27 datasets, both bagging and boosting lead to markedly more accurate classiers. Bagging reduces C4.5's classi cation error by approximately 10% on average and is superior to C4.5 on 24 of the 27 datasets. Boosting reduces error by 15%, but improves performance on 21 datasets and degrades performance on six. Using a two-tailed sign test, both bagging and boosting are superior to C4.5 at a signi cance level better than 1%.
When bagging and boosting are compared head to head, boosting leads to greater reduction in error and is superior to bagging on 20 of the 27 datasets (signi cant at the 2% level). The e ect of boosting is more erratic, however, and leads to a 36% increase in error on the iris dataset and 26% on colic. Bagging is less risky: its worst performance is on the auto dataset, where the error rate of the bagged classi er is 11% higher than that of C4.5.
The di erence is highlighted in Figure 1, which compares bagging and boosting on two datasets, chess and colic, as a function of the number of trials T . For T =1, boosting is identical to C4.5 and both are almost always better than bagging { they use all the given instances while bagging employs a sample of them with some omissions and some repetitions. As T increases, the performance of bagging usually improves, but boosting can lead to a rapid degradation (as in the colic dataset).
Why Does Boosting Sometimes Fail?
A further experiment was carried out in order to better understand why boosting sometimes leads to a deterioration in generalization performance. Freund and Schapire (1996a) put this down to over tting { a large number of trials T allows the composite classi er C to become very complex.
As discussed earlier, the objective of boosting is to

construct a classi er C that performs well on the training data even when its constituent classi ers Ct are weak. A simple alteration attempts to avoid overtting by keeping T as small as possible without impacting this objective. AdaBoost.M1 stops when the error of any Ct drops to zero, but does not address the possibility that C might correctly classify all the training data even though no Ct does. Further trials in this situation would seem to o er no gain { they will increase the complexity of C but cannot improve its performance on the training data.
The experiments of the previous section were repeated with T =10 as before, but adding this further condition for stopping before all trials are complete. In many cases, C4.5 requires only three boosted trials to produce a classi er C that performs perfectly on the training data the average number of trials over all datasets is now 4.9. Despite using fewer trials, and thus being less prone to over tting, C4.5's generalization performance is worse. The over tting avoidance strategy results in lower cross-validation accuracy on 17 of the datasets, higher on six, and unchanged on four, a degradation signi cant at better than the 5% level. Average error over the 27 datasets is 13% higher than that reported for boosting in Table 1.
These results suggest that the undeniable bene ts of boosting are not attributable just to producing a composite classi er C that performs well on the training data. It also calls into question the hypothesis that over tting is su cient to explain boosting's failure on some datasets, since much of the bene t realized by boosting seems to be caused by over tting.
Changing the Voting Weights
Freund and Schapire (1996a) explicitly consider the use by AdaBoost.M1 of con dence estimates provided by some learning systems. When instance x is classi ed by Ct, let Ht(x) be a number between 0 and 1 that represents some informal measure of the reliability of the prediction Ct(x). Freund and Schapire suggest us-

ing this estimate to give a more exible measure of

classi er error.

An alternative use of the con dence estimate Ht is in

combining the predictions of the classi ers fCtg to give

the nal prediction C (x) of the class of instance x.

Instead of using the xed weight log(1= t) for the vote

of classi er Ct, it seems plausible to allow the voting

weight of Ct to vary in response to the con dence with

which x is classi ed.

C4.5 can be \tweaked" to yield such a con dence

estimate. If a single leaf is used by Ct to classify an

instance x as belonging to class k=Ct(x), let S denote

the set of training instances that are mapped to the

lke.af,TahnedcoSnk

the subset of dence of the

them that belong to class prediction that instance x

belongs to class k can then be estimated by the Laplace

PPratio

Ht(x) = N N

i2Sk wit i2S wit

+ +

1 2:

(When x has unknown values for some attributes, C4.5

can use several leaves in making a prediction. A similar

con dence estimate can be constructed for such situa-

tions.) Note that the con dence measure Ht(x) is still

determined relative to the boosted distribution wt, not

to the original uniform distribution of the instances.

The above experiments were repeated with a mod-

i ed form of boosting, the only change being the use

of Ht(x) rather than log(1= t) as the voting weight of

Ct when classifying instance x. Results show improve-

ment on 25 of the 27 datasets, the same error rate on

one dataset, and a higher error rate on only one of

the 27 datasets (chess). Average error rate is approx-

imately 3% less than that obtained with the original

voting weights.

This modi cation is necessarily ad-hoc, since the

con dence estimate Ht has only an intuitive meaning.

However, it will be interesting to experiment with other

voting schemes, and to see whether any of them can

be used to give error bounds similar to those proved

for the original boosting method.

Conclusion
Trials over a diverse collection of datasets have conrmed that boosted and bagged versions of C4.5 produce noticeably more accurate classi ers than the standard version. Boosting and bagging both have a sound theoretical base and also have the advantage that the extra computation they require is known in advance { if T classi ers are generated, then both require T times the computational e ort of C4.5. In these experiments, a 10-fold increase in computation buys an average reduction of between 10% and 19% of the classi cation error. In many applications, improvements of this magnitude would be well worth the computational cost. In some cases the improvement is dramatic { for the largest dataset (letter) with 20,000 instances, modi ed boosting reduces C4.5's classi cation error from 12% to 4.5%.

Boosting seems to be more e ective than bagging

when applied to C4.5, although the performance of the

bagged C4.5 is less variable that its boosted counter-

part. If the voting weights used to aggregate compo-

nent classi ers into a boosted classi er are altered to

re ect the con dence with which individual instances

are classi ed, better results are obtained on almost all

the datasets investigated. This adjustment is decid-

edly ad-hoc, however, and undermines the theoretical

foundations of boosting to some extent.

A better understanding of why boosting sometimes

fails is a clear desideratum at this point. Freund and

Schapire put this down to over tting, although the

degradation can occur at very low values of T as shown

in Figure 1. In some cases in which boosting increases

error, I have noticed that the class distributions across

the weight vectors wt become very skewed. With the

iris dataset, for example, the initial weights of the three

classes are equal, but the weight vector w5 of the fth

trial has them as setosa=2%, versicolor=75%, and vir-

ginica=23%. Such skewed weights seem likely to lead

to an undesirable bias towards or against predicting

some classes, with a concomitant increase in error on

unseen instances. This is especially damaging when,

as in this case, the classi er derived from the skewed

distribution has a high voting weight. It may be possi-

ble to modify the boosting approach and its associated

proofs so that weights are adjusted separately within

each class without changing overall class weights.

Since this paper was written, Freund and Schapire

(1996b) have also applied AdaBoost.M1 and bagging

to C4.5 on 27 datasets, 18 of which are used in this

paper. Their results con rm that the error rates of

boosted and bagged classi ers are signi cantly lower

than those of single classi ers. However, they nd bag-

ging much more competitive with boosting, being su-

perior on 11 datasets, equal on four, and inferior on 12.

Two important di erences between their experiments

and those reported here might account for this discrep-

ancy. First, Freund and Schapire use a much higher

number T =100 of boosting and bagging trials than

the T =10 of this paper. Second, they did not mod-

ify C4.5 to use weighted instances, instead resampling

the training data in a manner analogous to bagging,

batuteaucshindgrawwxt

as on

the probability of selecting instance x trial t. This resampling negates a ma-

jor advantage enjoyed by boosting over bagging, viz.

that all training instances are used to produce each

constituent classi er.

Acknowledgements
Thanks to Manfred Warmuth and Rob Schapire for a stimulating tutorial on Winnow and boosting. This research has been supported by a grant from the Australian Research Council.

Appendix: Description of Datasets

Name Cases Classes Attributes

Cont Discr

anneal

898 6 9 29

audiology 226 6 { 69

auto 205 6 15 10

breast-w 699 2 9 {

chess 551 2 { 39

colic 368 2 10 12

credit-a 690 2 6 9

credit-g 1,000 2 7 13

diabetes 768 2 8 {

glass 214 6 9 {

heart-c

303 2 8 5

heart-h 294 2 8 5

hepatitis 155 2 6 13

hypo 3,772 5 7 22

iris 150 3 4 {

labor 57 2 8 8

letter 20,000 26 16 {

lymph 148 4 { 18

phoneme 5,438 47 { 7

segment 2,310 7 19 {

sick 3,772 2 7 22

sonar 208 2 60 {

soybean 683 19 { 35

splice 3,190 3 { 62

vehicle

846 4 18 {

vote 435 2 { 16

waveform 300 3 21 {

References
Breiman, L. 1996. Bagging predictors. Machine Learning, forthcoming. Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. 1984. Classi cation and regression trees. Belmont, CA: Wadsworth. Brodley, C. E. 1993. Addressing the selective superiority problem: automatic algorithm/model class selection. In Proceedings 10th International Conference on Machine Learning, 17-24. San Francisco: Morgan Kaufmann. Buntine, W. L. 1991. Learning classi cation trees. In Hand, D. J. (ed), Arti cial Intelligence Frontiers in Statistics, 182-201. London: Chapman & Hall. Catlett, J. 1991. Megainduction: a test ight. In Proceedings 8th International Workshop on Machine Learning, 596-599. San Francisco: Morgan Kaufmann. Chan, P. K. and Stolfo, S. J. 1995. A comparative evaluation of voting and meta-learning on partitioned data. In Proceedings 12th International Conference on Machine Learning, 90-98. San Francisco: Morgan Kaufmann.

Dietterich, T. G., and Bakiri, G. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Arti cial Intelligence Research 2: 263-286. Freund, Y., and Schapire, R. E. 1996a. A decisiontheoretic generalization of on-line learning and an application to boosting.Unpublished manuscript, available from the authors' home pages (\http://www.research. att.com/orgs/ssr/people/fyoav,schapireg"). An extended abstract appears in Computational Learning Theory: Second European Conference, EuroCOLT '95, 23-27, Springer-Verlag, 1995. Freund, Y., and Schapire, R. E. 1996b. Experiments with a new boosting algorithm. Unpublished manuscript. Heath, D., Kasif, S., and Salzberg, S. 1993. Learning oblique decision trees. In Proceedings 13th International Joint Conference on Arti cial Intelligence, 10021007. San Francisco: Morgan Kaufmann. Kohavi, R., and John, G. H. 1995. Automatic parameter selection by minimizing estimated error. In Proceedings 12th International Conference on Machine Learning, 304-311. San Francisco: Morgan Kaufmann, Murphy, P. M., and Pazzani, M. J. 1991. ID2-of-3: constructive induction of M-of-N concepts for discriminators in decision trees. In Proceedings 8th International Workshop on Machine Learning, 183-187. San Francisco: Morgan Kaufmann. Quinlan, J. R. 1987. Inductive knowledge acquisition: a case study. In Quinlan, J. R. (ed), Applications of Expert Systems. Wokingham, UK: Addison Wesley. Quinlan, J. R. 1993. C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann. Ragavan, H., and Rendell, L. 1993. Lookahead feature construction for learning hard concepts. In Proceedings 10th International Conference on Machine Learning, 252-259. San Francisco: Morgan Kaufmann. Ting, K. M. 1994. The problem of small disjuncts: its remedy in decision trees. In Proceedings 10th Canadian Conference on Arti cial Intelligence, 91-97. Utgo , P. E., and Brodley, C. E. 1990. An incremental method for nding multivariate splits for decision trees. In Proceedings 7th International Conference on Machine Learning, 58-65. San Francisco: Morgan Kaufmann. Vapnik, V. 1983. Estimation of Dependences Based on Empirical Data. New York: Springer-Verlag. Zheng, Z. 1995. Constructing nominal X-of-N attributes. In Proceedings 14th International Joint Conference on Arti cial Intelligence, 1064-1070. San Francisco: Morgan Kaufmann.

