From: AAAI Technical Report WS-94-03. Compilation copyright © 1994, AAAI (www.aaai.org). All rights reserved.
Discovering Informative .Patterns and Data Cleaning
I. Guyon*, N. Matin, and V. Vapnik
AT&TBell Laboratories, Holmdel, NJ 07733 ¯ AT&T,50 Fremont street, 40th floor, San Francisco, CA94105, isabelle@neural.att.com
Abstract Wepresent a methodfor discovering informative patterns from data. With this method,large databases can be reducedto only a few representative data entries. Our frameworkencompassesalso methodsfor cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposedand experimentally checkedon databases of handwritten images. The generality of the framework makesit an attractive candidate for newapplications in knowledgediscovery.
Keywords-knowledge discovery, machine learning, informative patterns, data cleaning, informationgain.
1 INTRODUCTION
Databases often contain redundant data. It wouldbe convenient if large databases could be replaced by only a subset of informative patterns. A difficult, yet important problem, is to define what informative patterns are. Weuse the learning theoretic definition [11, 6, 8]: given a model trained on a sequence of patterns, a new pattern is informative if it is difficult to predict by a model trained on previously seen data. With that definition, we derive on-line and batch algorithms for discovering informative patterns. The techniques were developed for classification problems, but are also applicable to regression and density estimation problems.
Informative patterns are often intermixed with other "bad" outlyers which correspond to errors introduced non-intentionally in the database. For databases containing errors, our algorithms can be used to do computer-aided data cleaning, with or without supervision. Wereview several results of experiments in handwriting recognition [1, 5, 9] which demonstrate the usefulness of our data cleaning techniques.

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 145

Pattern number

"-Pattemi .. ~ i .:~~.

Fractioonf ~ Surprise" votesfor label one w ~ level-

(a) (b)
Figure 1: Small example database containing "zeros" and "ones". (a) A data entry. (b) A sequence of data entries during a training session showing the variation of the surprise level. The patterns which are most surprising are most informative.
2 DISCOVERING INFORMATIVE PATTERNS
In this section, we assumethat the data is perfectly clean. First, we give an intuition of what informative patterns ought to be. Then, we show that this intuition coincides with the information theoretic definition. Finally, we derive algorithms to discover informative patterns.

2.1 InformativePatterns are most Surprising
In figureI, we constructaedsmallexampldeatabasceontaininognlyhandwrittzeenros and.onesM.ostpatternosf a givencategorlyooksimilarA.typicazleroisa circlaend a typicaolneis a verticablar.Howevert,hereexistothershapesof zerosandones.If we wantedto keeponlya fewdatarepresentativweesw,ouldprobablkyeepat leastone examploef eachbasicshape.
To choosethebestdatarepresentativewse,runan imaginareyxperimentI.magine thatwe found100peoplewhodidnotknowat allwhattheshapeof a zeroandthatof a oneare.We teachthosepeopleto recognizzeerosandonesby lettingthemexamine thepatternsof our databasein sequenceE.verytimewe showthema new image,we firsthidethelabelandletthemmakea guess.
We showin figure1 theaveragvealueof theguessesfora particulasrequencoef data.Sincewe assumedthatour subjecthsadneverseena zeronora onebeforethe experimenta,bout50% guessed"zero"and 50% guessed"one"whentheywere shown thefirstpatternB.ut,forthesecondexampleof thesameshape,themajoritmyade the correctguess.As learningoeson,familiarshapesare guessedmoreandmore accuratelayridthepercentagoef wrongguessesraisesonlyoccasionallwyhena new shapeappears.
We represenwtiththesizeof a questiomnarktheaverageamountof surpristehat wasgenerateadmongour subjectwshenthetruelabelwasuncoveretdo them.People who guessedthe correctlabelwherenot surprisewdhilepeoplewho madethe wrong guess were surprised. Wesee on figure 1 that a large average surprise level coincides

Page 146

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

with the apparition of a newshape. Therefore, the level of surprise is a good indication of howinformative a pattern is.
Moreformally, the level of surprise varies in the opposite direction as the probability of guessing the correct label Pk(~k= Yk)= P(~k=yk[z~; (x0, Y0), (zl, Yl), ...(Xk-1, Yk-1))l.
This probability of makingthe correct guess is precisely what is involved in Shannon's information gain:

I(k)=--logP~(~k=yk)=--yklogPk(~k=

1)--(1--yk)log(1--Pk(~k=

1)) (1)

where Sk is an image, YkE {0, 1} is its associated label, k - 1 is the numberof data entries seen thus far and ~k is the label predicted for pattern Xk. The log dependency ensures additivity of information quantities. In the information theoretic sense, the data entries that are most informative are those that are most surprising.

2.2 Machine Learning Techniques to Estimate the Information Gain
It is somewhatunrealistic to hire 100 ignorant people to estimate the information gain. Let us now replace people with machines.
In the Machine Learning framework, patterns drawn from a database are presented to the learning machine which makes predictions. The prediction error is evaluated and used to improve the accuracy of further predictions by adjusting the learning machine parameters.
Assumefirst that we trained 100 different learning machines (substituting our 100 people), each one predicting its own value of ~k. This is referred to as a "Bayesian" approach [8]. Formula 1 can be readily used to determine the information gain. Although this is a perfectly valid method, we propose here a more economical one: we train a single learning machine to give an estimate Pk(Yk = 1) of the probability that the correct label is "one" (figure 2). Our prediction ~k will me the most likely category according to Pk(Yk -" 1). In formula 1, wesubstitute l=~k(Yk -" 1) to Pk(Yk "- 1).
ManyMachine Learning techniques can be applied to discover informative patterns. For example, the learning machine can be a simple K-nearest-neighbor classifier [4]. All patterns presented to the classifier are stored. Jbk(yk "- 1) is given by the fraction of the K training patterns that are nearest to Xk which have label "one".
Another example is a neural network trained with a "cross-entropy" cost function for which the information gain is the cost function itself. The meansquare error cost function (Yk -- Pk(Yk -- 1)) 2 provides an information criterion which ranks patterns in the same order as the cross-entropy cost function and can therefore be used as well.
In the following, weuse the size of the symbol "question mark", in the figures, and the notation I(k), in the text, to represent the information criteria used to measurethe "surprise level".
1Tobe perfectly correct, the probabilityshouldbe also conditionedonthe modelclass.

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 147

A P(y~1,

I(k)

Figure 2: A learning machine used to predict the information gain (or surprise level).¯ In the classical machine learning framework, the goal is to train the learning machine, either to provide a model of the data or to makepredictions. In our frameworkthe goal is to discover the informative patterns of the database (dashed line).
2.3 On-line Algorithms and Batch Algorithms
In an on-line algorithm, patterns are presented in sequence and the learning machine adjusts its parameters at each presentation of a newpattern. This is a situation similar to that of our example of section 2.1. In that case, we will say that a pattern is informative if the information gain exceeds a pre-defined threshold. The disadvantage of this methodis that informative patterns depend on the sequence in which patterns are presented. However,there may be practical situations where the data is only available on-line.
In a batch algorithm, conversely, all data entries are available at once and the information gain is independent on pattern ordering. This implies that, if there are p data entries in the database, we need to train p machines, each one on all the data but one pattern, and then try to predict that last pattern. In practice, this is feasible only if training is unexpensive, as for the K-nearest-neighbor algorithm.
For other batch algorithms, werather train the learning machine only once on all the data. Wethen approximate the information gain of each pattern with an estimate of how much the cumulative information gain would decrease if we removed that pattern from the training set.
For batch algorithms all the patterns of the database are uniquely ranked according to their information gain. The m most informative patterns can be selected to represent the entire database.
2.4 Minimax Algorithms
Minimaxalgorithms are batch algorithms that are particularly well suited to discover
informativepatterns.- Mostalgorithmstrain the learningmachineto minimizethe averageloss (e.g. the mean-square-errorM). inimaaxlgorithmsminimizethe maximum loss:
(2)

Page148

AAAI-94Workshopon KnowledgeDiscoveryin Databases

KDD-94

where w represents the parameters of the learning machine and k runs over all data entries. Minimaxalgorithms are extreme cases of some "active learning" methods which emphasize the patterns with large information gain [8]. The solution of a minimax algorithm is a function of only a small subset of the training patterns, precisely called "informative patterns" [12]. These are the patterns that have maximumloss.
In reference [1], we propose and study a minimaxalgorithm for classification problems: the OptimumMargin Classifier. The algorithm maximize the minimumdistance of the training patterns to the decision boundary. It is shownthat the solution w* is a linear combination of basis functions of the informative patterns:

ak> 0,
k--1

(3)

where yk ~ {0,1} indicates the class membershipand the ak coefficients are all zeros, except for the informative patterns. Weuse the value of the cost function at the solution as cumulative information criterion:

k--1
from which we derive and estimate of the information loss incurred by removing the informative pattern k:
I(k) (5)
One important question is: what is the rate of growth of the number of informative patterns with the size of the database. The results of experiments carried out on a database of handwritten digits using the Optimal Margin Classifier suggest a logarithmic growth, in the regime when the number of patterns is small compared to the total numberof distinct patterns (figure 3).
Other minimaxalgorithms have been proposed for classification and regression [3, 2].

3 DATA CLEANING
In thissectionwe tackletheproblemof realworlddatabasewshichmaycontaicnorrupteddataentries.We proposedatacleaningalgorithmasnd analyzeexperimental results.

3.1 Garbage Patterns are also Surprising
Theinformatitohneoretidcefinitioonf infor,,,atipvaetterdnoesnotalwayscoincide withthe commonsensedefinitionI.n figure4, we showexamplesof patternsdrawn from our database of "zeros" and "ones", which have a large information gain. Wesee two kinds of patterns:
¯ Patterns that are actually informative: Atypical shapes or ambiguous shapes. ¯ Garbage patterns: Meaningless or mislabeled patterns.

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 149

numbeorf informativpeatterns 1500'

I000

5OO

ss

0

II I
10002000

I
4000

i

I

| 80~0 sizeo| f

thedatabase

numbeorf informativpeattams. 1500.

,,,,.

I000 ',',4,,u.,'O'".,,,,-

~sOS,

500

~S

0

I

1| ooI o2o'oI o4o'0I 08obo

.ize'of
thedatabase

Figure 3: Variation of the number of informative patterns as a function of the number of patterns in the database. Experiments were carried out on a database of handwritten digits, encoded with 16 real-valued global features derived from the pen trajectory information. A polynomial classifier of degree 2 was trained with a minimax algorithm (Optimal Margin Classifier). The informative patterns are the patterns for which
I(zk) = ak#

-:,...
:~,
....
I

A¯ typical

.. INFORMATIVE
~ (a)
-: Ambiguous
I

(b) ¯ GARBAGE

Meaningless "" " Mislabeled

Figure 4: (a) Informative patterns versus (b) garbage patterns. Informative patterns are intermixed with garbage patterns which also generate a lot of surprise (i.e. have large information gain).

Page 150

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

Learning machine
¯ UNCLEAN DATABASE

Figure 5: Flow diagram of on-line cleaning. In the process of learning, patterns which information gain exceeds threshold 0 are examined by a humanoperator. Good patterns are kept in the database and sent to the recognizer for adaptation. Bad patterns are removed from the database.
Truly informative patterns should be kept in the database while garbage patterns should be eliminated.
Purely automatic cleaning could be performed by eliminating systematically all patterns with suspiciously large information gain. However,this is dangerous since valuable informative patterns mayalso be eliminated. Purely manual cleaning, by examining all patterns in the database, is tedious and impractical for very large databases. Wepropose a computer-aided cleaning method where a human operator must check only those patterns that have largest information gain and are therefore most suspicious.
3.2 On-line Algorithms and Batch Algorithms
In figure 5 we present our on-line version of data cleaning. It combines cleaning and training in one single session. The learning machineis initially trained with a few clean examples. At step k of the cleaning process, a new pattern xk is presented to the learning machine. The prediction of the learning machine and the desired value yk are used to computethe information criterion I(k). If I(k) is below a given threshold 0, the pattern is directly sent to the learning machine for adaptation. Otherwise, the pattern is sent to the humanoperator for checking. Dependingon the decision of the operator, the pattern is either trashed or sent to the learning machine for adaptation.
Whenall the data has been processed, both training and cleaning are completed, since the learning machinewas trained only on clean data. This is an advantage if further use is madeof the learning machine. But on-line cleaning has several disadvantages:
¯ One has to find find an appropriate threshold 0.

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 151

'~.." '?i/

, .

ii

train the
2) ~,
with the ~ informatiognain.

Information gain(surpriselevel)
MOST
~ ISUSPRIICEISOUS I~ ! manually removeI
I~| the baddataentries

UNCLEAN DATABASE

SORTED DATABASE

Figure 6: Block diagram of batch cleaning. The learning macl~ne is trained on unclean data. The information gain is evaluated for each pattern by estimating how much information would be lost if that pattern would be removed. The database is sorted sorted according to the information gain. Only the top ranked patterns are examined for cleaning by the human operator. After cleaning, the learning machine needs to be retrained to obtain a good model and/or make good predictions.

¯ Training the learning machine may be slower than checking the patterns which may result in wasting the time of the operator.
¯ The method depends on the order of presentation of the patterns; it is not possible to revert a decision on whether a pattern is good or bad.
When possible, batch methods are preferred for cleaning (figure 6). Training performed on all patterns, including garbage patterns. The information gain of the patterns is computed as explained in sections 2.3 and 2.4. The data entries are then sorted in decreasing order of information gain. The patterns are examined by the operator, starting from the top (most suspicious), and until the number of consecutive "good" patterns exceeds a given threshold. If the database contains correlated errors, it may be necessary to iterate this procedure several times to remove all "bad" patterns.
The combination of batch cleaning and minimax algorithms is particularly attractive. In that case, only informative patterns (with non-zero information gain) need be examined. In figure 7 we show the first few informative patterns obtained with a minimax classifier (the Optimum Margin Classifier [1]). The classifier was trained discriminate between the digit "two" and all the other digits. The patterns with largest ak (our estimate of the information gain from equation 5) is a garbage pattern.

3.3 Point of Optimal Cleaning
Wemay wonder how reliable these cleaning techniques are: Did we examine all the patterns that should be candidates for cleaning? Amongthe patterns that were candidates for cleaning, did we remove too many or too few patterns?

Page 152

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

(a) (b)
Figure 7: The informative patterns obtained by a minimax algorithm (the Optimum Margin Algorithm), for the separation of handwritten digit "two" against all other digit categories [1]. Patterns are represented by a 16x16 grey-level pixel-map. The informative patterns are shownin order of decreasing information gain.(a) Informative patterns for class 2: a garbage pattern comes first. (b) Informative patterns for all other classes: several ambiguousor atypical shapes are amongthe patterns with largest information gain.

Trainingerror(%) 11.24
mm
8.1 5.2
-'CORRUPl"EONOCLEANING
(a)

Validatioenrror(%)

8.6

9.7
I

"-18.1

l

I"
I
NOCLEANING
(b)

OPTIMUM ~CLEANING CLEANINQv

Figure 8: Data cleaning results showinga point of optimal cleaning [9]. A neural network was trained with a mean-square-error cost function to recognize handwritten lowercase letters. The representation is 630 local features of the pen trajectory. A training data set of 9513 characters and a validation data set of 2000 characters from disjoint set of writers was used for cleA~ning. Starting with the "uncleaned" database, several levels of cleaning were applied by the humanoperator; Each stage was more strict, i.e. he lowered the tolerance for marginal-quality characters. To test the powerof the cleaning technique, wealso corrupted the initial, uncleaned database, by artificially mislabeling 5%of the characters in the training set (left most column).

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 153

Wecan again use our learning machine to provide us with an answer. In the experiments of figure 8, wevaried the amountof cleaning. The data was split into a training set and a validation set. The learning machine was trained on data from the training set with various amounts of cleaning. It was tested with the unclean data of the validation set. Weobserve that, as more patterns are removedthrough the cleaning process, the error rate on the training set decreases. This is understandable since the learning task
becomeseasier and easier to the learning machine. The validation error however goes through a minimum.This is because we remove first only really bad patterns, then we start removing ~aluable informative patterns that we mistook for garbage patterns. The minimumof the validation error is the point of optimal cleaning. If our validation error
does not go through a minimum, more patterns should be examined and considered for cleaning.
It is not always possible nor desirable to split the database into a training and a validation set. Another way of obtaining the point of optimumcleaning is to use the predictions of the Vapnik-Chervonenkis (VC)theory [121. According to the VC-theory, the point of optimumcleaning is the minimumof the so-called "guaranteed risk", which is a boundon the validation set frequency of errors:

G

Et,,i,,(m)

+ Bd(In2

--~ ~ + 1) + m(ln~ + 1) p--m

(6)

where/~ is a constant which was experimentally determined with the method de-

scribed in reference [7] (/~ - 0.5), Et,.,,,,(m) is the frequency training errors whenm

suspicious patterns have been removedfrom the training set, p is the numberof train-

ing patterns before cleaning (p--9513) and d is the VC-dimension.

These predictions can be madeonly if the capacity (or VC-dimension)of the learning

machine is knownand if the training algorithm does not learn the training set with

zero error. In our experiments on data cleaning wi~h neural networks (figure 9) [9]

we "obtained good agreement between the prediction of the VC-theory and that of the

validation set, even with a very rough estimate of the VC-dimension.In fact, we checked

the robustness of the VC-prediction of optimumcleaning with respect to a change in

the estimate of the VC-dimension. Wefound no significant change in the position of

the optimum in the range 300 < d < 5000.

4 CONCLUSIONS
Wepresented a computer-aided method for detecting informative patterns and cleaning data. Weused this method on various databases of handwritten characters. The results are summarized in table 1. It is important to stress that the number of informative patterns varies sub-linearly with the number of data entries and that therefore the cleaning time varies sub-linearly with the num__berof data entries. The point of optimum cleaning Can be predictedby the VC-theory which does not require splitting the data between a training set and a validation set. Our cleaning methodclearly improves the quality of the data, as measured by the improvementin classification performance on a test set independent from the training set. Our framework is general and applies ¯ to other problems than classification problems (regression or density estimation) which makesit attractive for newapplications in knowledgediscovery.

Page 154

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

2O
~
T

:f (F.~=.p, ,m,d) idation

0 5 i0 15 20 25 30 35 40
%removpeadttem(ms/p)
Figure 9: Detection of the point of optimal cleaning with the Vapnik-Chervonenkis prediction. A neural network was trained to minimize the maximum squared error. Training and validation sets are the same as in figure 8. The VC-dimension of the neural network was estimated to d=2000 (approximately one third of the number of free. parameters). G has been rescaled to fit on the figure.

References
Boser.gf,[1] Mati~-93~lO] Guyon-9~

Database
OCtt digits on-line lower on-line ASCII

Vb,
size 7,300 16,000 100,000

Num. of info. part.
690 3,130 10,983

Cleaning time (h)
1 1 5

test error undean data
15 11 30

%test error clean data
10.5 6.5 6

Table 1: Results obtained on various databases of handwritten patterns. The cleaning time is the time spent by the operator browsing through the informative patterns (not including the learning machine training time). The test errors were computed separated test sets of more than 2000 examples from a disjoint set of writers. Without cleaning, the lastdatabase was completely worthless.

KDD-94

AAAI-94 Workshop on Knowledge Discovery in Databases

Page 155

Acknowledgements
Discussions with colleagues AT&TBell Laboratories and at UCBerkeley are gratefully acknowledged. Special thanks to Bernhard Boser who contributed many ideas.
¯ References
[1]B. Boser,I. Guyon,and V. Vapnik.A trainingalgorithmfor optimalmargin
classifiers. In Fifth Annual Workshop on Computational Learning Theory, pages 144-152, Pittsburgh, July 1992. ACM.
[2]C. Cortesand V. Vapnik.SoftmarginclassifierT.echnicarleport,AT&TBell
Labs,HolmdelN, J, Decembe1r992.
[3]V. F. Demyanov and V. N. Malozemov. Introduction to Minimax. Dover, New
York, 1972.
[4]R.O. Duda and P.E. Hart. Pattern Classification And Scene Analysis. Wiley and
Son, 1973.
[5] I. Guyon, B. Boser, and V. Vapnik. Automatic capacity tuning of very large VC ¯dimensiocnlassifiers. In Neural Information Processing Systems conference, pages 147-155, Denver, December 1992.
[6]D. Haussler, M. Kearns, and R. Shapire. Bounds on the sample complexity of
bayesian learning using information theory and the VCdimension. In Computational Learning Theory workshop, ACM,1991. Also Machine Learning, volume 14 (1), pages 83-113, January 1994.
[7]E. . Levin, Y. Le Curt, and V. Vapnik. Measuringthe capacity of a learning machine.
Technical Report 11359-920728-20TM, AT&TBell Labs, Holmdel, N J, 1992.
[8]D. MacKay.Information-based objective functions for active data selection. Neural
Computation, 4 (4):590-604, July 1992.
[9]N. Matid, I. Guyon, L. Bottou, J. Denker, and V. Vapnik. Computer aided clean-
ing of large databases for character recognition. In 11th International Conference on Pattern Recognition, volume II, pages 330-333, Amsterdam, August 1992. IAPR/IEEE.
[10N]. Matid,I. Guyon,J. Denker,and V. Vapnik.Writeradaptatiofnor on-line
handwritten character recognition. In Second International Conference on Pattern Recognition and DocumentAnalysis, pages 187-191, Tsukuba, Japan, October 1993. IAPR/IEEE.
[11] J. Rissanen. Stochastic Complezity in Statistical Inquiry. World Scientific, Singapore, 1989.
[12] V.N. Vapnik and A.Ya. Chervonenkis. The theory of pattern recognition. Nauka, Moscow, 1974.

Page 156

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

