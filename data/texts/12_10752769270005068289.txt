Gatsby Computational Neuroscience Unit University College London http://www.gatsby.ucl.ac.uk
Funded in part by the Gatsby Charitable Foundation.

17 Queen Square, London WC1N 3AR, United Kingdom
+44 20 7679 1176

July 11, 2001

GCNU TR 2001­001
Passing And Bouncing Messages For Generalized Inference

Yee Whye Teh

Max Welling

Gatsby Unit

1 Introduction
Inference on general loopy graphs is a NP hard problem. Many approximate methods, like Monte carlo sampling and variational approximations have become available over the last decades, each with its own advantages and disadvantages. However, when the graphical structure is a tree, there is an algorithm for doing inference that is only linear in the number of nodes in the graph. The algorithm, called belief propagation (BP), was independently discovered by [Pearl, 1988] and [Lauritzen and Spiegelhalter, 1988]. In this algorithm, messages are sent over the edges of the network in both directions, either in parallel or sequentially. It can be proven that if each message is sent only when all incoming messages which it requires have been received, then BP converges when all messages have been updated exactly once.
Recently, people have found that on some graphs, BP can also perform accurate (approximate) inference on graphs with cycles [Murphy et al., 1999]. Especially in the field of errorcorrecting codes, it was proven that the very successful decoding algorithms for Turbo codes and low density parity check codes can be viewed as versions of the loopy BP algorithm [McEliece et al., 1998], [MacKay and Neal, 1995].
The fact that loopy BP works at all is remarkable, since we know that evidence will be moving around in the cycles and will be overcounted as a result. However, especially when the cycles have a large circumference, and the interactions are not too strong, we may expect that the evidence from a particular observed node has "died out" when it feeds back into itself. An analysis in terms of unwrapped networks was performed by [Weiss, 2000] and revealed that in networks with a single loop, the posterior assignment will always be correct, even though the marginal probabilities are usually overly confident. However, the real breakthrough in our understanding of the nature of the approximation made when computing posterior marginals using BP on loopy graphs came with the identification of the Bethe free energy as the appropriate cost function which is being minimized1. This very fruitful marriage between machine learning and physics led to a class of generalized belief propagation algorithms which act as fixed point equations to minimize Kikuchi free energies, in which larger clusters are treated exactly and made consistent through propagation.
A seemingly unrelated problem is that of finding a probability distribution with prescribed marginals that is close to a prior distribution in the sense of the KL-divergence. It is well known that this problem can be solved with the iterative proportional fitting (IPF) procedure [Deming and Stephan, 1940]. The algorithm computes the conditional probability table of all the nodes, given the node whose marginal we are going to update and multiplies that with the desired marginal. Although, this guarantees that that particular node now has the correct marginal, it also changes all the other marginals, possibly in the wrong direction. However, if one updates the marginals one by one, the algorithm is guaranteed to decrease the KL-divergence at every step and converge to the unique solution. Generalizations of this procedure (Generalized IPF), which can deal with more general constraints than marginalization constraints, and which can perform parallel updates have been analysed [Darroch and Ratcliff, 1972] (see also [Pietra et al., 1997] for the related improved iterative scaling algorithm). The most important drawback of IPF is the fact that one deals with full joint probability tables, which become computationally infeasible, even for relatively small networks. In that respect, great advance was made by [Jirousek and Preucil, 1995], who have studied a "space saving implementation" of IPF, which basically uses Junction trees to represent the joint table. An IPF update is performed on the clique which contains the constrained marginal only, after which this information is propagated through the rest of the junction tree, before a new update is considered.
In this paper we will propose an algorithm which combines BP and IPF into one message
1More precisely, the fixed points of loopy BP are the stationary points of Bethe free energy. However, in practise it turns out that it converges to minima
1

passing algorithm on a tree. The new set of messages reduces to BP messages on the internal nodes of the tree. However, when a messages reaches a constrained marginal (other than a "hard evidence" node), it will be "bounced back", while being changed in the process. When the constrained marginal is a delta function (hard observation), the returned message is independent of the incoming message in the usual way. The algorithm requires a scheduling of messages such that the information from a bounced message has reached the node where the next bounce takes place. Unlike BP on a tree (but like IPF on a tree), the combined algorithm does not converge within a finite number of iterations.
We will argue that it is natural to view the problem being solved by this IPF-BP algorithm as one of generalized inference, where certain marginals can be constrained to marginal tables with values other than zeros and ones. Generalized inference is defined as finding the minimum divergence distribution relative to a prior distribution, when certain constraints are imposed on the marginals. When these constraints come in the form of hard evidence, this objective reduces to finding the posterior distribution. Since BP itself can be understood as minimizing divergences between marginals (and pairwise marginals) with their prior counterparts under the constraints that they have to be consistent and normalized, it may come as no surprise that one set of message updates can solve the combined problem.
Generalized inference on loopy graphs is intractible, even for a modest amount of nodes on the graph. However, also for this case we can define an extended Bethe free energy as an objective function. To minimize this objective we can run loopy BP internally until (approximate) convergence after which we perform an IPF update (a bounce) at a softly constrained node. This procedure is however not guaranteed to converge, since loopy BP is not. We therefore propose to segment the loopy graph into a set of overlapping trees, while clamping the boundaries to their current estimated marginals (or observed marginals). We can now run IPF-BP on each tree and cycle through them until convergence. If every hidden node is unclamped at least once in every cycle we can proof that the algorithm decreases the objective at every step, and moreover only terminates at a stationary point. As a special case, this induces a stable alternative for loopy BP on graphs containing only hard evidence (and hidden nodes). In previous work we have called this kind of algorithm belief optimization [Welling and Teh, 2001].
2

2 Iterative Proportional Fitting as Belief Propagation

Consider a discrete, undirected, tree-structured graphical model   with pairwise potentials.

Let £¡ ¥¢ ¤¦¨¢ § denote vertices, and ©¡¤  denote an edge connecting nodes ¡ and ¤ . Each node ¡ is

associated with a discrete nodes, or a subgraph of   ,

variable  , and let &% ')(0120 34%

let  ¢ " ! be some and 5%6')(0 2073 %

states of # . . Let 8'9A @

If $ is a subset of and & 'BC @ . Each

node ¡ has a marginal potential ED  ©"F and each edge has a pairwise potential ED H G©"  ¢ I GP . The

distribution over  is defined by

Q ©8'95 S RUTV `D H ¦G © ¢ aGbcT Dde©  F W YG X 

(1)

2.1 Belief propagation
Suppose observations 6 f'h"g  were made at evidence nodes p¡ 9i q . Belief propagation (BP) is the standard procedure for inferring the posterior distribution over the unobserved nodes r given the observations [Pearl, 1988]. The posterior distribution is fully described by the marginal and pairwise marginal distributions over nodes in r . BP is an iterative procedure where at each iteration a message is passed from one node to a neighboring node based on the incoming messages to the first node.
Let s¡ i r be an unobserved node, and ¤ iuvt ©¡ be a neighbor of ¡ (¤ can be either observed or unobserved). Then the BP message vw H¦G © a GP from ¡ to ¤ is

w WG ©  G SRyx 0 #D HG ©  ¢  G  D  ©    T V w   ©    34  X G

(2)

where vt ©¡   ¤ are all neighbors of ¡ except ¤ . If d¡ i q is an observed node, then the outgoing message of ¡ to ¤6iutv© ¡ is held fixed at the boundary conditions of w HG© I G yR D`H ¦G ©b g  ¢ aPG  D` b© " g F  . For numerical stability, the messages are often normalized after each update. After BP has converged, the marginals and pairwise marginals are computed from the messages using

Q ©"¥ 'U £ © " F yR D`e © "¥ b T

Vw

  © "¥ 

 34  X

Q ©¢  ¥'U WG©"  ¢ I G RyD`H G¦© ¢ aGP D` ©"¥ D ¦G ©aPG  T V w   ©  T V w d G ©a PG 

 43   X G

d 34 GYXe

(3) (4)

BP messages can be updated in series or in parallel. The parallel BP scheme is not practical on a normal computer, but is useful when one has a large graph and highly parallel hardware with one processor per node sending and receiving messages from its neighbors. Consider one such computer for node ¡ . The computation of message wW G from ¡ to ¤ requires the product of all incoming messages except fw ¨G  . We can make BP more efficient by first computing the product of all incoming messages, and then dividing out gw G£ to compute wgH G for each ¤ iuvt ©¡ .

3

bii(x¢ ¡i)¡ ¢ ¡¢  ¡¢  ¡ ¢ M¢ ¡ij¢¡  (x¡ ¢ j¡¢ ) ¡ ¢ ¡ ¢  ¡¢ ¢  j
ji( xi )
Figure 1: Two nodes and the messages between them.

That is, rewrite the BP update rule (2) as a sequence of three updates2 :

  ©    yR D  ©   T V w   ©  

 37  X

£ £G  ©"¥ '

¨  ©"F  D  ©   e w ¨G  ©   

£

w HG ©aPG  ' xF0 `D H¦G ©" ¢ I G D`£ © " F ¨G  ©"¥

(5)
(6) (7)

¥¤ ¦¨§The updates (5) are used only f0o r0 unobserved nodes &¡ i r . However if for each observed

node ¡ i q we clamp Y © "¥  '

to have probability 1 on the observation   ' " g  , then the

updates (6,7) can be defined for the observed nodes ¡ too, and reduces to holding w H ¦G ©I G `R

D`H ¦G b© "g e ¢ aPG  `D  ©b"g F fixed.
£The beliefs and messages relating nodes ¡ and ¤ are depicted in figure 1. The arrows for G¨

and w HG depict the direction of information flow. By a filled circle, we will mean a node whose

marginals are clamped to some fixed values, while an unfilled circle means an unclamped

node.

2.2 A minimum divergence problem

We shall show that the same BP update rules (5,6,7) can be used to solve a minimum diver-

gence problem which can be understood as a generalization of the inference problem in the

previous section.

Let ¡`i q be an observed node. Suppose instead of a "hard" observation where  s ' " g  ,

we have a "soft" observation where we observe   '8" with probability ¨g  ©" ¥ .
©  © minimum divergence problem is to find the distribution 6©5  minimizing ©

Then our Q  while

©satisfying the constraints #© 0    01  ' g  ©   . The following theorem shows that when we have

¤ ¦¨§hard evidence, i.e. g £© "F  '

, the minimum divergence problem reduces to inference.

©Theorem 1

  © © ¤ ¦¨§ ©mizes
Q

`©

! " $# %" ¤ &¦ §© F g 

Suppose we observed Q  a0 n d0 satisfies the  3 .

  '  g  at nodes constraints #© Fu '

¡

i 0 q 0 . The distribution 6© 5  which minifor each ¡ i q is given by #©5  '

© © '" (# %" ¤ ¨¦ § ©Proof. As 6© 5  has to satisfy the constraints, we have #©  S '

0  0
3 hence 6©5 has to

) 10 &2 3 ¨4685 9 7 &2 3&2 3¨4¨4 9 1 &2 3 1 4@B5 A 1 2&3 IH 3 1 4 )@10 &2 3 ¨42In

practice,

for

more 

ef ficiency ,

w e 

can

use

the

following

updates 

instead



of

(6,7) 

:





&2 3 ¨4 1 CEGD F 

) 1We used (6,7) because the

's are exponentials of Lagrange multipliers which we shall introduce in the next

section.

4

© ¥© ! " $#have the form 6©5' 6©  F g  3%" ¤ 0  ¦&0§  . Now

  © © ©Q £¡ ¥¢ ¤ ¡£¢¥¤ Q

©  ' x  6©5 5© #©  § ¦

© 5 



©  "' x  x  #© E¢  S
¨©

¡£¢¥¤
© ! " %" ¤ ¦¨§6© 

F g S x  3

¡£¥¢ ¤ 0  0 

¡£¥¢ ¤ Q

¦ ©5 

© ! " ©  "  " "¡¢¥¤

£¡ ¢¥¤ Q

£¡ ¥¢ ¤ Q

y' x  6©  F g S5 © #© F g S¦

©  S §¦

P©  g S

¨

(8)

©  "  "and now ©  " $# " ' "#©  

minimizing
Q ' ©  F g S

(8) gives 6©
3 g e© ¥ 

is

F g
a

Q S'
trivial

©  F g S . extension


of

the

posterior

distribution

Q

©

F g s

to  @ . This justifies the minimum divergence problem as an extension of inference. We shall

refer to the minimum divergence problem as generalized inference, and to the minimum di-
©vergence #©5  as the generalized posterior distribution.

Our minimum divergence problem is quite similar in flavor to using soft evidence. In soft

evidence, related to

instead of observing   , we observe another   via a directed observation model Q ©   

random variable   '   . With observations

g 
of

where   , we

 is then

infer the posterior distribution over  . Soft evidence is similar to our minimum divergence

problem in that the evidence on   in both are soft. However in soft evidence the posterior

marginal distribution for   is not given and the evidence only acts as a bias affecting &  . In

our problem, the marginal distribution over   is given and is required to be satisfied. Ordi-

nary BP can be used to find the posterior given soft evidence but unfortunately the minimum

divergence problem cannot be solved so easily. In the next section, we introduce an algorithm

to solve it which is very similar to BP.

2.3 IPF-BP
Generalized iterative proportional fitting (GIPF) is a procedure for solving general constrained minimum divergence problems [Darroch and Ratcliff, 1972]. It can be used to solve our generalized inference problem as well. Since our distributions are defined over a tree, BP is required as an inner loop to compute marginal distributions. In this section we describe the relationship between the GIPF outer loop and the BP inner loop and introduce a unifying algorithm which we call IPF-BP to solve the generalized inference problem. IPF-BP consists of the same BP updates (5,6,7) performed in a particular order to ensure convergence.

©Theorem 2 Suppose we made observations 6©  ' g e ©  F for ¡ i q . There is an ordering of the

IPF-BP updates (5,6,7), with   ©    clamped at g  ©    for each ¡ i q , which guarantees that IPF-BP

will converge. Further, the beliefs given by (3,4) are the marginal and pairwise marginal distributions
© ©  © ©of #©5 , where 6©5  minimizes © Q  while satisfying the constraints #©   p ' g  ©   for each ©¡ i q . #©   itself is given by
' & (

©#©5 S R



Q
"©5 T 3 "!#

D`©F
% $
¨g £ ©   

£
T V G¨ ©"¥)0 G 73   X

(9)

where 1  is the number of neighbors of ¡ .

©Proof. We can solve for the posterior distribution #©  using Lagrange multipliers. The La-
grangian is

'yx 

¡£¢¥¤
© © %" ©#© 5© 6© 5 §¦

¡ ¥¢ ¤ Q

© 5 2 ¦gx 3

 x 0 4 3  ©"F© #© F5 ¦ g ©   

(10)

5

©Solving for #©  , we get

¢Q ©#©5R © 5 T
%¡"  3

 V 0  X R

TV D H G ©   ¢  G  T
H YG X  3

"D  ©   T 3

£

¢ ¥¤ V 0  X

 D  ©  

(11)

where 3  © "¥  's are chosen to satisfy the constraints.

We shall first prove the result when q consists of only leaf nodes (i.e. nodes with only one

neighbor). Then we shall show the general case by applying the algorithm and proof to each

segment of the tree separated by q , and patching the segments together using the Markov property of trees. The BP algorithm remains unchanged.

First suppose that q consists of only leaf nodes. The Lagrange multipliers in (11) can be

¦¨§solved using GIPF, which at each iteration updates a subset

   with the update rule

(

q of the Lagrange multipliers

¢ © V 0 X
 

¢  V 0  X ¨g £ ©  
  © 6©" F 

(12)

©where #©5  is given by (11) with the current setting of the Lagrange multipliers. We shall in ¦particular use the serial version of GIPF where consists of a single node ¡ . The update rule

is thus

¢ © V 0 X
 

¢  V 0  X g e © ¥
  ©#©"F

(13)

©#©    can be computed in an inner loop using the usual BP procedure with no evidence nodes

©and #©  given by (11). From (3), it is given by

©6©"¥ R

 V 0

£   ¢ ¤`D  © "¥ 

X

 w G  ©"F 

(14)

where ¤0 is the unique neighbor of node ¡ . Substituting (14) into (13), the serial GIPF update

rule then reduces to

 V 0 

¢ ©X ¨g  © " F 

(15)

D` © "Few G e © 
  £ ¢
wh ich is exactly the same as the BP update for G  ©    given in (6). Identifying

 V 0  X'

£  G  ©"¥ , we see that the two loop serial IPF algorithm can be reduced to a single run of the BP

£procedure given by (5,6,7) with   ©   fixed at g  ©    for each ¡Si q . We shall call this single run
BP procedure IPF-BP. IPF-BP updates of G£e© " ¥  where ¡ is a clamped node will be called IPF

updates, other updates will be called internal BP updates. Note that our derivation of IPF-BP

assumes a particular ordering of the updates : after each IPF update, we have to perform a

batch of internal BP updates until the internal BP messages have converged. In section 2.4 we

show that more efficient update orderings are possible. Now suppose that q contains internal nodes of the tree   . The minimum divergence dis-
©tribution #©  is the unique distribution of the form (11) which satisfies the constraints. We

will construct a distribution satisfying the above properties, which by uniqueness, must be

©#© 

. First

we

partition

the

tree

 

into a number of segments as follows. Each node &¡ i)q

is

replicated 1  times and each replica of ¡ is connected to a neighbor ¤ i t© ¡  and no other nodes. A segment is defined to be a connected component of the resulting graph. Aside from

being connected to different neighbors of ¡ , there is no difference between the replicas of ¡ , and

we will in general make no distinction between the individual replicas and the node ¡ itself, and will identify each segment with the corresponding subtree of   . The same segmentation
scheme can be applied to any graph with a number of clamped nodes. This is depicted in

figure 2. Note that a single edge can form a segment (bottom right of graph), and a segment

can be singly connected even though the original subgraph is not (top right).

6

Figure 2: Segmenting a graph. On the left is the original graph, with filled circles being clamped nodes, and unfilled circles being unclamped nodes. The clamped nodes are replicated on the right and each connected component forms a segment of the original graph.

Since the clamped nodes of   appear only as leaf nodes of segments of   , we can run the previously described IPF-BP algorithm independently on each segment $ of   . The resulting

posterior distribution on $ is

©E% ©  "% R

£V T DdWG©" ¢ I GCT `D  © " ¥ T %"¡ HGX43 % 73 % 3 %

G£¢  © F

(16)

where ¤

% 

is the unique neighbor of ¡ in $

. Define

©#© 5 S '

# ©% E% ©5 %" & ( # %" 3 g  ©" ¥

(17)

©where © ©f¡ i q ,

we take ¤¦§¥ ¤
it can easily

¤
'
be

. Because   shown that

¨

is 

a tree and
#©  s' ©

$
we have 1% hence 6©5

© " ¥' g  © "¥ for each $ adjacent to is a distribution. Further, for each

© ©¡Eiq , 6©" Fd ' g  ©" F  satisfies the constraint. Now expanding each 1% in (17) using (16), we

see that

'&(

©#©5 S R



Q
"© 5 T  3 "!#

D  ©  
$
¨g £ © 

£
T V ¨G  © " ¥  )0 G 73   X

(18)

©has the same form as (11). By uniqueness, #©   must be the sought after generalized posterior
distribution. 

2.4 Scheduling of IPF-BP messages
There are a few differences between IPF-BP and ordinary BP. Firstly, ordinary BP can converge with only one update to each message on the tree. This is achieved with the following scheme : update a message wW G only if each incoming message w   for §i t © ¡ ¤ has already been updated or is a fixed message coming in from an observed node. IPF-BP, on the other hand, cannot in general converge exactly to the correct solution with a finite number of steps, since simple GIPF itself cannot do so either.
Secondly, the updates of ordinary BP can be performed in parallel, while the IPF updates in
¦IPF-BP cannot be parallelized unless one takes small steps, i.e. use )' q in the GIPF update
(12). Ordinary BP can be parallelized because the value of each w W G never directly or indirectly affect the value of fw £G  . With IPF-BP, a message going into a clamped node will "bounce" off the node and affect the message going out of the node. As a result every message is dependent on every other message which implies that IPF-BP cannot be parallelized.
Thirdly, the proof of theorem 2 assumes a particular ordering of message updates where all internal BP messages are updated before each IPF update. Since the convergence of IPF-BP is determined by the number of IPF updates performed, this is very inefficient since it requires a full sweep of internal BP updates for each IPF update. However, it turns out that we do not actually need to update all internal BP messages in between two consecutive IPF updates.

7

lk

i

u vj

w

Figure 3: The internal BP update path from ¡ to   is given by the arrowed edges.

£ £Suppose the first IPF update is for £G e©" ¥  and the second update is for d ©  d  . See figure 3.

Then we only need to update the messages along the path from ¡ to   . For each unclamped

£nuopddeat¡ etra© v¢ e©r¤ s¢ e dthonenthueppdaatthefwrom¢ ©

¡ © 

t©o


 , .

we update £ ¢ ©¤ ¢  and for edge ¡¦¨¥ § traversed we first The reason these internal BP updates are sufficient is

£because updating  d © d  requires only w  d ©  d  which in turn only requires messages pointing

£toward   to be computed. Among these messages, only those laying along the path from ¡ to  

are
w 

©af©f e© c teind

by the first IPF update of ¨G  ©   figure 3 remain unchanged after

and hence need to be the first IPF update.

recomputed.

Messages

like

The above describes the minimum set of internal BP updates required between two IPF

updates. We still have freedom in deciding how to schedule the IPF updates. For efficiency, it

is desirable to schedule the IPF updates such that the number of internal BP updates per IPF

update is minimized. This can be achieved by minimizing the length of the path traversed

from one IPF update site to the next.

8

3 Loopy Extension to IPF-BP

Recently, a number of groups have shown that BP works surprisingly well in many prac-

tical problems where the graphical model involved contains loops [Frey and MacKay, 1997,

McEliece et al., 1998, Freeman and Pasztor, 1998, Murphy et al., 1999]. Efforts to understand

how and why loopy BP works were spear headed by Weiss [Weiss, 2000], but the breakthrough

came when Yedidia et al showed that the fixed points of loopy BP correspond exactly to the

stationary points of the Bethe free energy [Yedidia et al., 2000]. The Bethe free energy is an

approximation to the true free energy, and in practice the BP fixed points are found to be min-

ima of the Bethe free energy. This motivates exploring the use of other algorithms to minimize

the Bethe free energy. In this section we shall derive an algorithm based on IPF-BP to directly

minimize the Bethe free energy. We refer to this algorithm as IPF-BO, where BO stands for

belief optimization [Welling and Teh, 2001].
Suppose we have a second order, discrete, undirected graphical model . Let ¡£¥¢ 7¤ ¢¨§ denote

nodes of the model, and © ¡I¤  denote an edge connecting nodes ¡ and ¤ . Each node ¡ is associated

with a variable   , and let   be some states of   . Let  ')(0  2 and  ')0(   2 . The distribution

over  is defined by

Q ©8'95 S UR TV `D H ¦G ©  ¢ a GbcT Dde© F 

(19)

WYG X 

where DdW G ©"e¢ IbG  denotes a pairwise potential leosctiaml patoeteonftQ ia©l fo¢ rI nPG o.dDee¡ fi. nLeet the beliefs   ©   

bbeetwoueernesntoimdeat¡ eaonfdQ

¤

, while D e©  ¥ ©    , and  W G ©

denotes the  ¢  G  be our

  ¢¡£¡ ¥¢ ¤

¤£WG©" ¢ IG  ' ¦

D`HG ©" ¢ IG  `D  © " ¥ D¦G © aGP

  ¢¡¡£¢¥¤

¤£¦¥e© FS ' ¦

dD e © F 

(20)

The Bethe free energy is defined as

©§ ¨   £   ¤£¡ ¥¢ ¤

' x V 0x    HG¦©  ¢ a PG 
¦ 1HGX

H G ©"e¢ I bG  

H G¦© ¢ a Gb

© £   ¤ x © ¦ 1  x 01   ©" ¥

¡£¥¢ ¤

 ©" F 

¨£ ©   

(21)



We obtain estimates   ¢¨H G by minimizing §¨   subject to constraints that every  H G marginal-

izes down to   and  G , and that every   sums to 1. We can enforce these constraints by adding

Lagrange multipliers to the free energy as follows3,

' §¨    ¦

x V x 0 HG X

£ £ 13 £G  ©"F x 

 H G© ¢ a GP§ ¦

¤
 © " F
¤

1¦ x V x  3 HG¦© I G x 0 H G© " ¢ I G§¦ eG¦©aGP ¦ x H GYX 

£  x 0 

¨ £ © §¦

©¤

(22)

It was shown in [Yedidia et al., 2000], that the stationary points of this cost function correspond to the fixed points of loopy BP.
In the following we will describe an iterative procedure to decrease the Bethe free energy at every iteration and show that the procedure stops only when a stationary point has been found. At each iteration a number of nodes are chosen to have their marginals clamped at the current values. These clamped nodes define a segmentation of the graph as in figure 2. The nodes are chosen such that each resulting segment forms a tree, and IPF-BP is applied to
3Notice that unlike [Yedidia et al., 2000], we do not include Lagrange multipliers which directly enforce the pairwise marginals to integrate to one, which are redundant.

9

each segment with the aforementioned nodes clamped to their current marginals. After IPF-

BP converges, a new iteration starts with a new set of nodes chosen to have their marginals

clamped. We will show that each iteration of this algorithm decreases the Bethe free energy,

hence is guaranteed to converge since the Bethe free energy is bounded below for discrete

networks. So long as no node stays clamped at every iteration, we shall show furthermore

that the algorithm will converge to a stationary point of the Bethe free energy (it can be a

saddle point or local minimum).

 Let be a set of nodes   to their marginals.   is

and consider a assumed to be

ssienggmlye-ncto nnoefctthede.gWraephwaisfthertowme icnlaimmipzeth§ e¨n o dewsiothf

respect to the beliefs corresponding to the edges and unclamped nodes of   . The relevant

terms of  are

 £   ¤@ ' V x

0 x   H G ©   ¢ a PG 

¦ 1HGYX3P@

£¡ ¥¢ ¤

W G © " e¢ IG

 HG¦© ¢ a GP

£   ¤@
© x © ¦ 1   x 0A £ © "F 

¡£¥¢ ¤

 ©" ¥ 

¨ © "F

£ b3 @

¤

¦ V x xF0 3 £G  © " ¥ x H ¦G © ¢ a PG ¦  ©"¥
£ 1 ¤W YG X 3P@

¦ V x x  3 H G© I G x 01  HG©"¢ I G§¦ eG©a GP
1W YG X P3 @  £ © ¤¦ x  x 0 1 ¨  © "¥¦

(23)

 3b@

 wunhcelraem1 p@ edisbtehleienfsuomfb  erboyfmneinigimhbizoirnsgof

node ¡ in   @ . Because

.
 

We can minimize is a tree, if each  H G

with respect to the marginalizes down

to   and eG , we can define a distribution over   by

 @ © @ '

V
# HYG X 3b@ H G © e ¢ I& Gb( 
# ¡ b3 @   © "F

(24)

The problem of the constrained minimization of @ wi$ th respect to the unclamped beliefs of
  is equivalent to the problem of minimizing

   £¡ ¥¢ ¤

©x  ¨ @ ©" @ 

¨ @ ©C @   V x

@ H G¦©  ¢ a PG   x © ¦ 1   £ ©  0)

¡ #! H GYX 3b@ P3 @

(25)

¢¤  £subject to
ize down

the to Y

constraints that Y @ has ©" ¥  for each clamped

to be node

of
¡Si

the

form given in   . Define a

d(2i4st)raibnudtitohnato v@ ehr a  s

to by

marginal-

¦¥¨§©Q @ ©C @ R

¦ Vx

  H G © " ¢ IGP5 ¦ x

©  @
© ¦ 1   e © ¥ )0

(26)

!# W GYX 3P@

b3 @

© ©By theorem 2, the the minimum divergence distribution @ from Q @ with constraints @ ©F'

¢  £ e©  F for each ¡ i

  is

Q
© ¥¨§©@ ©  @  R @ © @  T  3 4  @

©  ©   

(27)

 ©where  ©   are chosen to satisfy the constraints. Because   is a tree, a @ given by (27) can

also be expressed in the form

©`@ © "@  '

V
# ©H GX 3b@ `@ © e ¢ I & PG ( 
# © ¡ P3 @ `@ © "¥

(28)

$

10

  ©Now (25) is, up to an additive constant, ©  @ Q @  , hence d@ is the solution to the con-

strained minimization of (25).
©By theorem 2, we can find

@

using IPF-BP, and so IPF-BP can be used to decrease §

¨  

at of

e§ a¨ ch 

iteration. Now we will show that the overall algorithm if no node is left clamped in every iteration. Suppose

converges to a stationary point that the algorithm has already

converged. Let ¡ be a node and ¤ be a neighbor of ¡ , and consider an iteration where ¡ is unclamped. Let   be the segment containing ¡ and consider the IPF-BP updates of   . Since
£the overall algorithm has converged, the update rules (5,6,7) do not change   , HG or w G£ .

Consider YW G as given by (4). We have

x  HG¦©  ¢ aPG  Ryx  D`HG¦©" ¢ I G D`£ © " F  D G© abG  T V w   ©   T V w d G ©aPG 

1 1 £ £ 34  eX  G

d 34 GYeX 

1' x  `D H¦G ©"¢ IG  `D £© " F DG© aGb WG©IG  G¨e© ¥  £

'ydD e ©F G¨e ©   e w G£ ©"¥

y' Dde ©FsT V w   ©" ¥  43  X

'U © " ¥ 

by (5,6)
by (7) by (5,6)
by (3)

(29)

So the  and

beliefs  H G are

astattihsefysttahteiomnaarrygipnoaliinztaotifo§n

¨ a¤ n d

normalization constraints. if they are given by (3) and

Since a set of (4), and they

beliefs satisfy

the has

mcoanrvgeinrgaleidzattoioansatantdionnoarrmy paloizinattioofn§

c¨o n  s t.raints

[Yedidia

et

al.,

2000],

our

overall

algorithm

11

4 Comparing Loopy BP and IPF-BO

We investigated the practicality of minimizing the Bethe free energy using IPF-BO by com-

paring the accuracy of its estimated marginal and pairwise marginal distributions with loopy

BP. We ran all our experiments on a 10 by 10 square lattice with binary © ¤ ¢ ©b -valued units. This

is because it is still feasible to compute the exact marginal and pairwise marginal distributions,

against which we can compare the estimates obtained using loopy BP and IPF-BO. The exact

marginal and pairwise marginal distributions are calculated using the junction tree algorithm

where we cluster the nodes in each row into a super-node. We sampled each weight  vH G

independently from a zero mean Gaussian with stand( ard devV iation ¡  . Then we sampled each
¨bias Y  independently from a Gaussian with mean ¦ ¢ ¨  34 X     and standard deviation ¡ .
The means of the biases are offset from 0 so that networks with biases deviating only slightly

¥farpopmrotxhiemiramteelyan¤ s¤£

have complex multi-modal , while if the biases deviate

distributions where the mean value of each more from their means the distribution will



 is tend

to be peaked around a single mode with the mean value of   polarized at either 0 or 1. That is,

¨if ¡ is small the resulting distribution will be qualitatively more similar to a prior distribution

¨before observations were made, and if ¡ is large the resulting distribution will be qualitatively

¥ ¨ ¨amreorcehsoimseinlafrrotomth( e¤

posterior
© © ©4¢ 4¦¢ ¥I¦¢ I§ ¢

a¤ f2 tesreopbasreartveilny.g

some nodes. The standard deviations ¡  and ¡ For each setting of ¡  and ¡ , 20 networks are

¨generated and used to compare loopy BP and IPF-BO. For large weights ¡ ©¨ § and small
biases ¡  ¥ , we generated and used 40 networks instead, as loopy BP does not converge all

the time.

For IPF-BO, we iterate over the nodes ¡ of the network, clamping the neighboring marginals

¥e G © IPG  ¢¥f¤ igtv©¡ to their
taining ¡ . For loopy BP,

current values and running IPF-BO we used a strong damping factor of

o¤ n ¤

the star-shaped segment conso that it has a higher chance

of be

convergence. For both
changed by less than ©

¤

a&lg orithms, the convergence criteria was for all the means for twenty consecutive iterations. We stop if loopy BP

4 © b©  to
has not

¨converged by 10000 iterations. For a given setting of ¡  and ¡ , the generated networks are separated into two sets : one

in which loopy BP converged, and one in which it failed to converge. For each set separately,

we compared IPF-BO and loopy BP using the mean error in the estimated marginals e © b© 

averaged over all nodes and all networks in the set. We also compared the mean error in the

estimated covariances H G © 4© ¢ ©b ¦  £© ©b 9¦ eG¦© b©  , averaged over all neighboring pairs of nodes

and all networks in each set. Accompanying each mean we also looked at the mean absolute

deviation (MAD). The results are given in figure 4.

Each row of figure 4 corresponds to a setting of ¡  , increasing from top to bottom. Within

each row the left plot shows the errors in the estimated marginals, while the right plot shows

¨the errors in the estimated covariances. In each plot there are five groups of four bars each.
Each group corresponds to a setting of ¡ , increasing from left to right. In each group, the first

two bars show the errors using IPF-BO and loopy BP respectively, when loopy BP converged.

The next two bars show the errors for both IPF-BO and loopy BP when loopy BP failed to

converge in 10000 iterations. The number associated with each group indicates the percentage

of runs that loopy BP failed to converge.

The qualitative behaviors of the errors of the marginals and covariances are the same.

Hence we shall concentrate on the errors of the marginals. The general trends in figure 4

confirm our expectations. With increasing weights, both loopy BP and IPF-BO performed

increasingly worse, as the distribution becomes more complicated and multi-modal. With in-

creasing biases, both loopy BP and IPF-BO performed better, as the distribution tends toward

a single mode.
¨ ¨For small weights or large biases (¡   ¥ , or ¡ ¨ § , or ¡  ' § and ¡ ¨ © ), loopy BP always

12

Error

Error

Error

0.014 0.012
0.01 0.008 0.006 0.004 0.002
0

0 0.1

0.04 0.035
0.03 0.025
0.02 0.015
0.01 0.005
0

0 0.1

0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01
00 0.1

0.25

Weight size = 0.1

00 13
Bias sizes
Weight size = 1

0 6

00 13
Bias sizes
Weight size = 3

0 6

00 13
Bias sizes
Weight size = 6

0 6

0.2

0.15
52.5 0.1

0.05 0 0.1
0.5

00 13
Bias sizes
Weight size = 10

0 6

0.4

0.3 87.5 0.2 85
35 0.1

0 0.1

0 136
Bias sizes

0 10
0 10
0 10
0 10
0 10

Error

Error

Error

Error

Error

x 10-3 7 6 5 4 3 2 1 00
0.1
0.02

Weight size = 0.1

00 13
Bias sizes
Weight size = 1

0 6

0.015

0.01

0.005

00 0.1
0.06 0.05 0.04 0.03 0.02 0.01
00 0.1
0.15

00 13
Bias sizes Weight size = 3
00 13
Bias sizes Weight size = 6

0 6
0 6

0.1
52.5 0.05

0 0.1
0.25

00 13
Bias sizes
Weight size = 10

0 6

0.2

0.15 87.5
0.1 85 35
0.05

0 0.1

0 136
Bias sizes

0 10
0 10
0 10
0 10
0 10

Error

Error

Figure 4: Errors in the estimated marginals (left) and the estimated covariances between neighboring nodes (right).

13

sw<= 3, or sw=6 and sb=6,10 1

sw=6 and sb=1,3, or sw=10 and sb=6,10 1

0.8 0.8

Marginals of IPF-BP Marginals of IPF-BP

0.6 0.6

0.4 0.4

0.2 0.2

0 0 0.2 0.4 0.6 0.8 1
Marginals of loopy BP

0 0 0.2 0.4 0.6 0.8 1
Marginals of loopy BP

Figure 5: Scatter plots of the estimated marginals b£© b© 
  ¨BP on the -axis. The left plot is for networks with ¡ 
¨ ¨plot is for networks with ¡  '§ and ¡ ' ©4¢¦¥ , or ¡  '

for IPF-BO on the 


©¤

¥ or ¡£g ' and ¡ '

§
I§ ¢

a© ¤n.d

¡

-a' xisI§ ¢a©n¤ d.

for loopy The right

IPF-BO

-120

Bethe free energy

-140

-160

-180

-200

-220

-240

-260

-280

-300

-320
-320 -300 -280 -260 -240 -220 -200 -180 -160 -140 -120 Loopy BP

Figure 6: Scatter plot of Bethe free energy obtained using IPF-BO versus those obtained using loopy BP.

converged and both algorithm performed equally well. As a matter of fact, most of the time

both algorithms converged to very similar solutions. This is shown in figure 5, where we plot

the marginals obtained by IPF-BO versus those obtained by loopy BP. On the right plot, the

algorithms sometimes get stuck in local minima or plateaus, resulting in a very small number

of marginals
¡  ¥region  ¦

be in¤ g¤

different
©.

:

out

of

a

¨ ¨ ¥ ¨ ¥or

¡

The
'

s© i¤tuaantidon¡

is 

more complicated ¥ ). In the regime

¨than loopy BP, especially when ¡  '

total of 12000 points on the plot, only 372 lie outside the

wf© o¤ hr.elrIanerg¡tehew' ree¤ igg© ihmatsnedawn¡ dhfesr' me aI§¡ l¢ l©

b¤ i,aIsPeFs-(B¡ O ¨ © and ¡



' § and ¡ ' ¤ © , pe' rfo© r¤ m, leodopbyetBtePr

amazingly performed better than IPF-BO even when loopy BP did not converge.

One possible explanation for this phenomenon is that IPF-BO is stuck in local minima or

plateaus, in which case we can diagnose this by seeing if the Bethe free energy of the final

beliefs obtained using IPF-BO is larger than the Bethe free energy of the beliefs obtained using

loopy BP. This is shown in figure 6. We see that the reverse is true instead ­ IPF-BO always

converges to a point where the Bethe free energy is lower than the Bethe free energy obtained

with loopy BP. This shows that IPF-BO is not stuck in local minima and also shows that IPF-BO

does what it was advertised to do ­ to decrease the Bethe free energy.

To understand why IPF-BO gives larger errors than loopy BP we look at how the marginals

14

Loopy BP versus true marginals 1

IPF-BO versus true marginals 1

0.8 0.8

Estimated marginals Estimated marginals

0.6 0.6

0.4 0.4

0.2 0.2

0 0 0.2 0.4 0.6 0.8 1 True marginals

0 0 0.2 0.4 0.6 0.8 1 True marginals

¨Figure
with ¡ 

7:
'

Sc© a¤ t¢¡t er

plot of
' ©.

loopy

BP

and

IPF-BO

marginals

versus

the

true

marginals

for

networks

estimated by IPF-BO and loopy BP are related to the true marginals. This is shown in figure 7.

We did not separate out cases where loopy BP converged from those where it did not because

the analyses turn out to be similar
versus the true marginals. Most of This means that if a true marginal

anyway. Consider first the

the points is close to

a¤ roerco© ,nlcoeonptryatBePd

left plot near the

of
© ©4¢

lb©o oapnydB©P¤

often converges to a

¢ m¤ arcgoirnnaelrs.
limit cycle

or stationary point close to the true marginal. Otherwise the loopy BP marginal can be totally

ufrnormela© t¤ e¢ d¤



to the and ©

true marginal, as seen by
©4¢ b©  . In summary, loopy

the uniform BP often got

spread of the points on the plot away the right marginal but sometimes got

it totally wrong. Now consider the right plot of IPF-BO marginals versus the true marginals.

¥Since there are not many points in
IPF-BO marginals are often on the

stahme etospidleefot fan¤ ¤d£

bottom right quadrants, we see that as the true marginals. The problem

the lies

¥ ¥ ¥ ¥ ¥ ¥ ¥IswcoPiofdiFnpteh-toBrotiiOnbfhute¤ set¤(,£seaot)dli.nmmTteooahntsiteesth)aeiihrsshot©or¤irfgui¢tzhe¤eon¤e¦£envrctrlaeaoolnnsrrdewiadstohgonreeee¤npo¤n£tfohepr(aeetorvtiinren© u©4ntfie¢s¤tg,mhw¤u£¦oarhue)re.gg4rIihe.tniIatrsPhel gtFeihsa-IBernPdOpeFlao-epBrisnrOsteotosfee¤fnsrwoetsiarmhirta© sa©t(t¤mote¢hba¤ mesr¤£¦egtirrgiuvnhaeeantltmdlshietae© o4©rotg¢ wbn¤ ien¤ot£¦hancleelwiuassrmh,atmi¤ hpc¤hee£s

¨ ¥because they give a lower Bethe free energy, as seen in figure 6.

¡

'

Th¤ e©

same analysis shows why IPF-BO does . The results are shown in figure 8 for ¡

better
) ' ©

¤th. aTnhleoorepsyulBtsPawrehseinm¡ilar

'

I§ ¢
for ¡

© ¤ and
) ' § .

Loopy BP versus true marginals 1

IPF-BO versus true marginals 1

0.8 0.8

Estimated marginals Estimated marginals

0.6 0.6

0.4 0.4

0.2 0.2

0 0 0.2 0.4 0.6 0.8 1 True marginals

0 0 0.2 0.4 0.6 0.8 1 True marginals

Figure with ¡ 

8:
'

Sc© a¤ t¢¡t er¨

plo¤ t
'

of
¥© .

loopy

BP

and

IPF-BO

marginals

versus

the

true

marginals

for

networks

15

Again we did not distinguish between whether loopy BP converged or not as the analyses

¥ ¥ ¥wmeorsetlysimbeitlawr.eenFir¤ s¤t¥

oafndall¤

n  o. teLothopatybBePcaduisde

the not

biases are so converge in ¡

 

small, the true marginals ¤££¢ of the networks, and

are we

can see from figure 8 that the marginals it estimated are essentially random. For IPF-BO, the

¥ ¥ ¥ ¥pfrooimnts© ¤

in¤
¢

the
¤£¦

right
to © 4© ¢

¥IPF-BO marginals

¤p¤l£¦o t,

of figure 8 and a less

either close to

dc¤ ai¤sn£ tib(nhecotarpvizeporrtnoitcxaailml ssattrrteiipply)f,rsooprmliat r©i¤en¤tI£too¢ ¤ttwa ltloyosr©t¤ari¤np£Id¢so©b:ma. Th(hvoiresirzmtoicneaatlanslstsrttihrpiap)t.

This should not be much better than what loopy BP did on the left plot. The reason the IPF-BO

¥errors in figure 4 are so much smaller than
themselves are coincidentally often close to

t¤ h¤£ e.

loopy

BP

errors

is

because

the

true

marginals

The above detailed analysis shows that loopy BP always converges when the Bethe ap-

proximation is good. Both IPF-BO and loopy BP will then converge to the same solutions in

this case. If the Bethe approximation is bad, loopy BP often does not converge, but IPF-BO

does not seem to do much better either. We can view this as an advantage for loopy BP ­ if

loopy BP does not converge we can be certain that the Bethe approximation is not good to start

with and perhaps we should look into more accurate approximations. The down side is that

even if loopy BP converges we still cannot be sure if the Bethe approximation is good and the

loopy BP estimates are good. However the stable nature of IPF-BO might make it more suit-

able as part of a network parameter learning scheme with approximate inference, as it is likely

that we will encounter parameter settings for which loopy BP will not converge during the

course of learning. Our conclusions are, however, limited by our use of synthetic randomly

generated networks, and further experiments are necessary in understanding when and why

direct minimization of the Bethe free energy might be better than loopy BP.

16

5 Discussion
In this work we have presented a unified view on inference and an algorithm to solve it efficiently on trees. We are currently investigating the possibility to perform exact generalized inference on loopy graphs which can be efficiently represented as junction trees, and its relation to Jirousek's and Preucil's algorithm.
Approximate generalized inference on loopy graphs is studied through the introduction of an extended Bethe free energy as an objective function to be minimized. An iterative algorithm which cycles through tree structured subgraphs while performing IPF-BP on each segmented tree was shown to minimize this objective efficiently. We briefly mentioned the possibility of segmenting small clusters of nodes containing loops and treating them exactly. Although initial experiments show promising results, we haven't yet found an appropriate costfunction which is being minimized. It it our hope that there is a close relationship with the Kikuchi free energy and the generalized BP algorithm [Yedidia et al., 2000].
In the experiment section we have tried to map out where the Bethe free energy is a good approximation. Theoretically, we know three regimes where it should be appropriate,
  On a tree, where it is exact.
  For small weights, since the correlations are expected to be short ranged, which is the appropriate regime for mean field type approximations.
  For very large weights if the interactions at most second order, since in this regime the entropy is negligable and the energy contribution is exact.
From an algorithmic perspective we have found good performance of both loopy BP and our belief optimization procedure for small weights (and on trees of course). However, when the weights grow too large and the biases remain small both loopy BP and BO procedures become inaccurate. In this regime, the lack of external evidence (in the form of small biases) will not produce a sharply peaked posterior distribution, but a distribution with many modes with polarized marginals. Since BP (when it converges) and BO will produce identical, but inaccurate estimates of the posterior marginals, we deduce, that the Bethe approximation must have broken down. We have found that this happens even before loopy BP fails to converge, making the use of minimization procedures like belief optimization not extremely useful for this application (one could even think of using the failure of BP to converge as a diagnostic tool for the accuracy of Bethe free energy). We have not found good performance in the regime of very large weights, irrespective of the biases (third regime), which may be explained by the rugged energy landscape of the posterior distribution.
Possible applications may involve learing where BO can be used as a partial E-step in an EM algorithm. It is our hope that other applications, possibly in the field of error correcting codes, will emerge in the future.
17

References
[Darroch and Ratcliff, 1972] Darroch, J. and Ratcliff, D. (1972). Generalized iterative scaling for loglinear models. Annals of Mathematical Statistics, 43:1470­1480.
[Deming and Stephan, 1940] Deming, W. E. and Stephan, F. F. (1940). On a least square adjustment of a samplied frequency table when the expected marginal totals are known. Annals of Mathematical Statistics, 11:427­444.
[Freeman and Pasztor, 1998] Freeman, W. and Pasztor, E. (1998). Learning to estimate scenes from images. In Advances in Neural Information Processing Systems, volume 11.
[Frey and MacKay, 1997] Frey, B. and MacKay, D. (1997). A revolution: Belief propagation in graphs with cycles. In Neural Information Processing Systems, volume 10.
[Jirousek and Preucil, 1995] Jirousek, R. and Preucil, S. (1995). On the effective implementation of the iterative proportional fitting procedure. Computational Statistics And Data Analysis, 19:177­189.
[Lauritzen and Spiegelhalter, 1988] Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, 50.
[MacKay and Neal, 1995] MacKay, D. J. and Neal, R. M. (1995). Good codes based on very sparse matrices. In Boyd, C., editor, Cryptography and Coding : 5th IAM Conference, number 1025 in Lecture Notes in Computer Science, pages 100­111. Springer-Verlag.
[McEliece et al., 1998] McEliece, R., MacKay, D., and Cheng, J. (1998). Turbo decoding as an instance of pearl's belief propagation algorithm. IEEE J. Selected Areas in Communication, 1997, 16:140­152.
[Murphy et al., 1999] Murphy, K., Weiss, Y., and Jordan, M. (1999). Loopy belief propagation for approximate inference : An empirical study. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, volume 15. Morgan Kaufmann Publishers.
[Pearl, 1988] Pearl, J. (1988). Probabilistic reasoning in intelligent systems : networks of plausible inference. Morgan Kaufmann Publishers, San Mateo CA.
[Pietra et al., 1997] Pietra, S. D., Pietra, V. D., and Lafferty, J. (1997). Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380­393.
[Weiss, 2000] Weiss, Y. (2000). Correctness of local probability propagation in graphical models with loops. Neural Computation, 12:1­41.
[Welling and Teh, 2001] Welling, M. and Teh, Y. W. (2001). Belief optimization for binary networks : A stable alternative to loopy belief propagation. In Uncertainty in Artificial Intelligence.
[Yedidia et al., 2000] Yedidia, J., Freeman, W., and Weiss, Y. (2000). Generalized belief propagation. In Advances in Neural Information Processing Systems, volume 13.
18

