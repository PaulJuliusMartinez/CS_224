Global Illumination Models for Volume Rendering
A dissertation presented by
Lisa Marie Sobierajski
to The Graduate School in Partial Fulfillment of the Requirements
for the Degree of Doctor of Philosophy
in Computer Science
The State University of New York at Stony Brook Stony Brook, New York August 1994

Copyright by Lisa Marie Sobierajski
1994

Abstract of the Dissertation
Global Illumination Models for Volume Rendering
by Lisa Marie Sobierajski
Doctor of Philosophy
in
Computer Science State University of New York at Stony Brook
1994
The increasing demand for realistic images has led to the development of several global illumination models and rendering techniques. Great effort has been taken to extend these illumination models and optimize the rendering techniques to produce more realistic images in less time. Despite this effort, most of these methods are designed for scenes consisting of geometric surface descriptions, and cannot directly render volumetric data. Volumetric data sets can be rendered using volume rendering techniques that, in order to decrease rendering times and therefore increase interactivity, typically employ only a local illumination model.
The development of global illumination models and rendering techniques for volumetric data is the focus of this work. These illumination methods can be used to generate realistic images of scenes containing volumetric as well as geometric data. The volumetric global illumination methods can be employed by a visualization system in order to add intuitive cues to an image for a greater three-dimensional understanding of a scene. Also, these methods allow for amorphous effects that can not be captured using geometric illumination models, including clouds, fog, and smoke.
A volumetric ray tracing method is presented that encompasses classical ray tracing while allowing for volume rendering effects. The definition of an intersection is extended to include intersection points for surface contributions to the intensity equation, and intersection segments for volumetric contributions. A method for accelerating primary and shadow rays is developed.
A volumetric radiosity method is presented that encompasses classical radiosity while also including isotropic and diffuse volumetric interactions. Geometric surfaces and volumetric isosurfaces are approximated using standard surface patches, and voxels are used to approximate
iii

participating volumes. Diffuse volumetric interactions make it possible to give the appearance of shaded surfaces to scenes consisting of only volumetric data. Hierarchical techniques are applied to decrease the number of interactions required for a solution.
Multipass methods are developed to increase realism by combining illumination methods. A multipass method that combines volumetric ray tracing and volumetric radiosity is presented. Also, a multipass method that extends volumetric ray tracing to account for indirect specular lighting is presented.
iv

To Rick,
For celebrating with me in the good times, and comforting me in the bad times, for always encouraging me, and always listening to me, I dedicate this work to you.

Acknowledgments
I would like to express my gratitude to my advisor, Leading Professor Arie Kaufman, for his support and guidance over the past five years. The research opportunity that he gave to me as an undergraduate sparked my interest in computer graphics and gave me the desire to pursue a graduate degree in the field.
I would like to thank the members of my committees, Leading Professor Theo Pavlidis, Associate Professor Steven Skiena, and Associate Professor Issei Fujishiro, for their helpful comments during the preparation of this dissertation. I would like to thank Professor Paul Adams for the use of his lab and equipment. I would also like to acknowledge the efforts of the system and administration staff in the department. In particular, I would like to thank Brian Tria for maintaining the systems and answering many questions, and Kathy Germanai, Betty Knittweis, and Stella Mannino for all the paper work and administrative details that they took care of on my behalf.
I would like to thank the numerous individuals who have given me invaluable comments and insights during my research - Dany Cohen, Taosong He, Lichan Hong, Hanspeter Pfister, Claudio Silva, Sidney Wang, and Roni Yagel. I would also like to thank the many people who have worked on the development of the VolVis system - Richard Calmbach, Sophocles Englezos, Ben Hom, Kwan Leung, Jiang Qian, Sean Smith, Manka Sung, Yi-Chih Wei, Cynthia Yang, and Lan Yu.
This work was supported in part by the National Science Foundation under grants IRI-9008109 and CCR-9205047, by the Department of Energy under the PICS grant, by a grant from the Center for Biotechnology, SUNY at Stony Brook, which is sponsored by the New York State Science and Technology Foundation, by a grant from the Hewlett-Packard Company, and by Howard Hughes Medical Institute.
I would like to thank my parents, Joseph and Marie Sobierajski, for their love and encouragement. Through their confidence in me, I was able to have confidence in myself. I would also like to thank my sister, Linette Sobierajski, for always being proud of me. Finally, I dedicate this work to Ricardo Avila, in appreciation of his encouragement, admiration, and love.

Chapter 1 Introduction
Over the past two and a half decades generating realistic images of complex scenes has become an important aspect of computer graphics. Applications that benefit from realistic images range in discipline from art to science, and these images have become a part of everyday life for many people. Realistic image generation techniques are commonly used to add special effects to movies, and to create product advertisements for television. These techniques are also employed by manufacturers to design new machine parts, and by neurobiologists to investigate the structure and function of nerve cells.
The process of generating a realistic image typically begins with a three-dimensional description of an environment, or scene. An illumination model is then chosen in order to describe the behavior of light. Finally, a rendering technique for the given illumination model is applied to the scene to create the image. These three steps are not independent; the scene can only contain objects with properties that are described in the illumination model and can be rendered using the rendering technique. Generally, more realistic images require more complex scenes, illumination models, and rendering techniques, and therefore also require more memory and computation time.
Early rendering techniques for scenes containing geometric objects are based on local illumination models in which shading is performed using only information about the light sources in the scene, and local object properties such as the color and normal of the surface. Since only local characteristics are considered, these illumination models do not capture global shading effects such as shadows, reflections, or translucency. The first global illumination model was developed in 1968 for the purpose of adding shadows to renderings of machine parts in order to aid in structural comprehension. This technique, known as ray tracing, preceded standard raster displays, so the resulting images were viewed using a digital plotter.
Improvements in computing power and display devices over the last two and a half decades, as well as a great increase in the demand for realistic images, has led to the development of several global illumination models and associated rendering techniques. Since it is far too computationally expensive to evaluate the ``true'' illumination model found in the real world, an approximation is typically made in either the illumination model or the rendering technique. In order to increase realism and improve efficiency, these illumination models are continually extended, and the rendering techniques are refined.
Although global illumination models have been used extensively in the rendering of classical geometric objects, volume rendering techniques, which operate directly on the volumetric data, still typically employ only a local illumination model for shading. There are several justifications for the choice of a local illumination model in volume rendering. First, volume rendering algorithms are often found within visualization systems used by scientists to

Chapter 1

2

examine their data. Therefore, these volume rendering algorithms sacrifice global effects in order to attempt to achieve interactive projection rates. Also, in some cases global effects are not desirable, such as when they would decrease the amount of useful information in an image, or possibly lead to incorrect interpretations. Finally, most volume rendering techniques are designed to render scenes consisting of only a single volumetric data set, and global illumination models are not always useful in this case. It is difficult to model a complex scene in a single data set, and therefore even if global illumination models were employed, it would still be impractical to use these volume rendering techniques to generate realistic images of complex scenes.
There are advantages to employing a global illumination model in volume rendering techniques. First, despite the additional rendering time required, global effects can often be desirable in scientific visualization applications. The process of volume rendering typically takes three-dimensional data as input and produces a two-dimensional image as output. During this rendering process, a dimension of information is essentially lost. In order to give the viewer a better understanding of the three-dimensional nature of the data, depth cues can be added to the image through the use of shading. Including global shading effects can greatly increase the amount of information in the image. For example, by placing mirrors in a scene, a single image can show several views of an object in a natural, intuitive manner. Also, there are times when a scientist may wish to view the data with a volume rendering technique that does not include local shading effects. In this case, global shading effects can be essential for image comprehension. For example, the data can be placed in a virtual room with lights, walls, and a floor. Even though no shading is performed on the actual volume data, global shading effects can be applied to the walls and floor, resulting in shadows and reflections of the data.
Another advantage to developing global illumination models and rendering techniques for volumetric data is that this allows volumetric data to be directly included in realistic images. With classical global illumination models, three-dimensional volumetric data can be included in these scenes only after being converted into a two-dimensional geometric surface model. The loss of information incurred during this conversion process may not be significant when viewing volumetric isosurfaces, but is unacceptable when rendering amorphous phenomena such as fog and clouds.
In some cases, complex geometric surfaces can be rendered more efficiently after being converted to a volumetric representation. The global illumination models developed during this research are suitable for use in volume graphics, in which geometric scenes are modeled using voxelized objects and then efficiently rendered using volume rendering algorithms.
In order to understand the problem of applying global illumination models to volume rendering, we must first examine the previous work performed in these separate areas. In Chapter 2, the basic concepts of volume rendering are covered, as well as some rendering algorithms and optimization techniques. Most of these rendering techniques employ only a simple, local illumination model, but some of the efficiency considerations and optimization methods are crucial when developing volume rendering techniques for global illumination models.
Chapter 3 covers previous work performed in the area of global illumination models. These methods were developed for scenes containing geometric objects, and include ray tracing for capturing specular effects, and radiosity for capturing the diffuse interactions between objects. In order to make image generation computationally feasible, both ray tracing and radiosity only

Chapter 1

3

approximate the ``true'' global illumination model. A formulation of a more accurate global illumination model known as the Rendering Equation is given in Chapter 3, and a method for evaluating it based on a Monte Carlo simulation technique is discussed. Techniques for computing the indirect specular illumination in a scene are presented. Multipass techniques that combine ray tracing and radiosity in order to include a greater number of global effects in a single image are also briefly covered in Chapter 3.
In Chapter 4, the concept of volumetric ray tracing is described. Since complex scenes cannot be easily modeled in a single volumetric data set, this volumetric ray tracing method allows for an arbitrary number of volumetric data sets of independent size and orientation. Also, the illumination model and rendering techniques presented in this chapter encompass classical geometric ray tracing, and therefore can be used to render scenes containing geometric objects. Efficiency considerations are explored, and several acceleration techniques for geometric and volumetric objects are developed which improve the performance of this method.
In Chapter 5, a method for volumetric radiosity is described. The basic elements of this radiosity method include the standard surface patch of classical radiosity, as well as the volume element, or voxel, for isotropic and diffuse volumetric interactions. Due to the high complexity of this algorithm, optimization techniques are critical for reducing the computation time required to converge on a solution for the radiosities in a scene. Therefore, the application of a hierarchical iterative shooting technique to volumetric radiosity is explored. A view-dependent ray casting method is used to composite radiosity values to generate a final image.
The volumetric ray tracing technique described in Chapter 4 can capture specular effects in a scene including specular reflection and transmission of light. The volumetric radiosity method presented in Chapter 5 captures diffuse interactions between objects, including color bleeding. To capture both specular and diffuse interactions in a single image, an multipass technique is developed in Chapter 6, that combined these two methods. Neither volumetric ray tracing, nor volumetric radiosity account for light arriving from indirect specular sources. A method for computing the indirect specular lighting in a volumetric scene is explored in this chapter. A second multipass method is developed for specular scenes that combines the indirect specular lighting method with volumetric ray tracing.
In Chapter 7 the basic ideas of volume rendering and global illumination models, as well as the new developments presented here, are summarized. Areas of future research are identified for the volumetric global illumination methods, and related areas of research are identified. Also, some suggestions are given for research in new global illumination models and rendering techniques for volumetric data.

Chapter 2 Volume Rendering
As technology advances and methods for obtaining data are developed and improved, volume visualization of three-dimensional data is becoming an integral part of many scientific fields [Kaufman91, Kaufman94]. For example, data obtained from Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) is often visualized for diagnostic purposes, and to plan treatment or surgery [Vannier88, Mohan88, Adams90, Arridge90, Levoy90]. Confocal Microscopy produces data which is visualized to study the morphology of biological structures [Burbach93, Avila94b, Barney90, Chen89], and data in the earth sciences is analyzed using visualization techniques [Santek87, Hibbard89, Wolfe88].
Many techniques have been developed over the years to visualize three-dimensional data. Since methods for displaying geometric primitives were already well-established, most of the early methods involve approximating a surface contained within the data using geometric primitives. When volumetric data is visualized using a surface rendering technique, a dimension of information is essentially thrown away. In response to this, volume rendering techniques were developed that attempt to represent the entire three-dimensional data in a two-dimensional image. Volume rendering images convey more information than surface rendering images, but at the cost of increased algorithm complexity, and consequently increased rendering times. To improve interactivity in volume rendering, many methods of optimization have been developed.
This chapter begins with an introduction to volume rendering terminology. In Section 2.2, some surface rendering techniques for volumetric data are briefly described. Section 2.3 covers many volume rendering techniques, including image-order, object-order, and hybrid methods. Optimization methods for volume rendering are discussed in Section 2.4.
2.1. Volume Rendering Terminology
Volume rendering is a name given to the process of creating a two-dimensional image directly from three-dimensional volumetric data. The data is typically a set S of samples (x, y, z, v), representing the value v of some property of the data, at a three-dimensional location (x, y, z). If the value is simply a 0 or a 1, with a value of 0 indicating background and a value of 1 indicating the object, then the data is referred to as binary data. The data may instead be multivalued, with the value representing some measurable property of the data, including, for example, color, density, heat or pressure.
In general, the samples may be taken at purely random locations in space, but in this chapter only sets that contain samples taken at regularly spaced intervals along three orthogonal axes are considered. The spacing between samples along each axis is a constant, but there may be a different spacing constants for each of the three axes. If a set of samples does not have this property, such as in irregular grids [Speray90, Wilhelms90, Williams90, Challinger92,

Chapter 2

5

Giersten92], a preprocessing step of resampling can be used to create a new set that can be used with the algorithms described in this chapter. Since the set of samples is defined on a regular grid, a three-dimensional array, also known as a volume buffer, a cubic frame buffer, or a 3D raster, is typically used to store the values, with the element location indicating the position of the sample on the grid. For this reason, the set S will be referred to as the array of values S(x, y, z), which is defined only at grid locations.
The array S only defines the value of some measured property of the data at discrete locations in space. A function f (x, y, z) may be defined over R3 in order to describe the value at any continuous location. The function f (x, y, z) = S(x, y, z) if (x, y, z) is a grid location, otherwise f (x, y, z) approximates the sample value at a location (x, y, z) by applying some interpolation function to S. There are many possible interpolation functions. The simplest interpolation function is known as zero-order interpolation, which is actually just a nearestneighbor function. When zero-order interpolation is used, each uniform valued region represented by a grid location is known as a voxel. Higher-order interpolation functions can also be used to define f (x, y, z) between sample points. One common interpolation function is a piecewise function known as trilinear interpolation. With this interpolation function, the value is assumed to vary linearly along directions parallel to one of the major axes. When trilinear interpolation is unsed, the eight grid locations at (x + 0, 1, y + 0, 1, z + 0, 1) define the corners of a cell.

2.2. Surface Rendering Techniques
Several surface rendering techniques have been developed which approximate a surface contained within volumetric data using geometric primitives, which can be rendered using conventional graphics hardware. A surface can be defined by applying a binary segmentation function B to the volumetric data. B(v) evaluates to 1 if the value v is considered part of the object, and evaluates to 0 if the value v is part of the background. The surface is then the region where B(v) changes from 0 to 1. If a zero-order interpolation function is used, then the surface is simply the set of faces which are shared by voxels with differing values of B(v). If a higherorder interpolation function is being used, then the surface will pass between sample points according to the interpolation function.
For zero-order interpolation functions, the natural choice for a geometric primitive is the 3D rectangular cuboid, since the surface is a set of faces, and each face is a rectangle. An early algorithm for displaying human organs from computed tomograms [Herman79] uses the square as the geometric primitive.
With continuous interpolation functions, a surface, known as an iso-valued surface or an isosurface, may be defined by a single value. The Marching Cubes algorithm [Lorensen87] was developed to approximate an isosurface with a triangle mesh. The algorithm breaks down the ways in which a surface can pass through a cell into 15 cases. For each of these 15 cases, a generic set of triangles representing the surface is stored in a look-up table. Each cell through which a surface passes maps to one of the 15 cases, with the actual triangle vertex locations determined using linear interpolation on the cell vertices. A normal value is estimated for each triangle vertex, and standard graphics hardware can be utilized to project the triangles, resulting in a smooth shaded image of the iso-valued surface.

Chapter 2

6

When rendering a sufficiently large data set with the Marching Cubes algorithm, many of the triangles will map to a single pixel when projected onto the image plane. This fact led to the development of surface rendering algorithms that use 3D points as the geometric primitive. One such algorithm is Dividing Cubes [Cline88], which subdivides each cell through which a surface passes into subcells. The number of divisions is selected such that the subcells will project onto a single pixel on the image plane. Another algorithm that uses 3D points as the geometric primitive is the Trimmed Voxel Lists method [Sobierajski93]. Instead of subdividing, this method uses only one 3D point per visible surface cell, projecting that point on up to three pixels of the image plane to insure coverage in the image.

2.3. Volume Rendering Techniques
Representing a surface contained within a volumetric data set using geometric primitives can be useful in many applications, however there are several main drawbacks to this approach. First, geometric primitives can only approximate the surface contained within the original data. Adequate approximations may require an excessive number of geometric primitives. Therefore, a trade-off must be made between accuracy and space requirements. Second, since only a surface representation is used, much of the information contained within the volumetric data is lost during the rendering process. For example, in scanned medical data useful information is contained not only on the surfaces, but within the data as well. Also, amorphous phenomena, such as clouds and fog, cannot be adequately represented using surfaces, and therefore must have a volumetric representation, and must be displayed using volume rendering techniques.
In the next subsections, various volume rendering techniques are explored. Although several of the methods described in these subsections render surfaces contained within volumetric data, these methods operate on the actual data samples, without the intermediate geometric primitive representations used by the algorithms in the previous section.
Volume rendering can be achieved using an object-order technique, an image-order technique, or a hybrid technique that combines the two approaches [Levoy91]. Object-order volume rendering techniques use a forward mapping scheme to map the volume data onto the image plane. In image-order algorithms, a backward mapping scheme is used where rays are cast from each pixel in the image plane through the volume data to determine the final pixel value. Some volume rendering algorithms consist of several steps where, for example, first an object-order technique is applied, followed by an image-order technique that produces the final pixel values. These techniques are classified as hybrid volume rendering algorithms.

2.3.1. Object-Order Techniques
Object-order techniques involve mapping the data samples to the image plane. One way to accomplish a projection of a surface contained within a volume is to loop through the data samples, projecting each sample that is part of the object onto the image plane [Frieder85]. Using this method, a correct image can only be produced by traversing the data samples in a back-to-front order. A strictly back-to-front algorithm would require that if we process n voxels in the order v1,v2, . . . vn, then the distance from the image plane to vi is greater than or equal to

Chapter 2

7

the distance from the image plane to vi+1, for all 1  i  n - 1. For this algorithm, the definition of back-to-front can be relaxed to require that if vi and v j project to the same pixel on the image plane, and i < j, then vi must be farther away from the image plane than v j. This can be accomplished by traversing the data plane-by-plane, and row-by-row inside each plane.
Although the relative orientations of the data and the image plane specify whether each axis
should be traversed in an increasing or decreasing manner, the ordering of the axes in the
traversal (and therefore the nesting of the loops) is arbitrary.

An alternative to back-to-front projection is a front-to-back method in which the voxels are traversed in the order of increasing distance from the image plane. One advantage obtained by utilizing a front-to-back method is that once a voxel is projected onto a pixel, any subsequent voxels that project to the same pixel need not be processed, since they would be hidden by the first voxel. Another advantage of front-to-back projection methods is that if the axis which is most parallel to the viewing direction is chosen to be the outermost loop of the data traversal, meaningful partial image results can be displayed to the user.

For each voxel processed using either a back-to-front or front-to-back projection method, its distance to the image plane could be stored in the pixel to which it maps. At the end of the data traversal, an image is obtained where the value at each pixel on the image plane is the distance to the closest voxel. A two-dimensional discrete shading technique can then be applied to the image, resulting in a shaded image suitable for display. The simplest 2D discrete shading method is known as depth shading, or depth-only shading [Herman81, Vannier83], where the intensity value stored in each pixel of the output image is inversely proportional to the depth stored in the corresponding input pixel. This produces images where features far from the image plane appear dark, while close feature are bright. Since surface orientation is not considered in this shading method, most details such as surface discontinuities and object boundaries are lost.

A more accurately shaded image can be obtained by passing the 2D depth image to a gradient-shader [Gordon85] which can take into account the distance from the light source and the object surface orientation at each pixel to produce a shaded image. This method evaluates the gradient at each (x, y) pixel location in the input image by

z

=

 

z x

,

 

z y

,

1

(2.1)

where z = D(x, y) is the depth stored at pixel ( x, y). The estimated gradient vector at each pixel

is then used as a normal vector for shading purposes.

The value  z can be approximated using a backward difference D(x, y) - D(x - 1, y), a

forward

x difference D(x + 1, y) - D(x, y),

or

a

central

difference

1 (D(x + 1, y) - D(x - 1, y)).

z 2

Similar equations exist for approximating . In general, the central difference is a better

y

approximation of the derivative, but along object edges where, for example, pixels (x, y) and

(x + 1, y) belong to two different objects, a backward difference would provide a better

approximation. A context sensitive normal estimation method [Yagel92a] was developed to

provide more accurate normal estimations by detecting image discontinuities. In this method,

two pixels are considered to be in the same ``context'' if their depth values, and the first derivative of the depth at these locations do not greatly differ. Similar depth values indicate C0

Chapter 2

8

continuity, and similar first derivatives indicate C1 continuity. The gradient vector at some pixel p is then estimated by considering only those pixels which lie within a user-defined neighborhood, and belong to the same context as p. This ensures that sharp object edges, and slope changes will not be lost in the final image.

The previous rendering method considers only binary data samples where a value either belongs to the object or the background. Many forms of data acquisition (e.g. MRI, CT, confocal microscopy) produce data samples with 8, 12, or even more bits of data per sample. These data samples are known as a scalar field if the value between samples varies according to some convolution that can be applied to the data samples to reconstruct the original three-dimensional signal.

One way to reconstruct the original signal is, as described previously, to define a function f (x, y, z) that determines the value at any location in space based on an interpolation function applied to the nearest data samples. This is the technique typically employed by backwardmapping (image-order) algorithms. In forward mapping algorithms, the original signal is reconstructed by spreading the value at a data sample into space. Westover describes a splatting algorithm [Westover90] for object-ordered volume rendering in which the value of the data samples represents a density. Each data sample s = (xs, ys, zs, (s)), s S, has a function defining its contribution to every point (x, y, z) in the space:

contributions(x, y, z) = hv(x - xs, y - ys, z - zs)(s)

(2.2)

where hv() is the volume reconstruction kernel and (s) is the density of sample s which is located at (xs, ys, zs). The contribution of a sample s to an image plane pixel (x, y) can then be computed by integration:


contributions(x, y) = (s) hv(x - xs, y - ys, w)dw -

(2.3)

where the w coordinate axis is parallel to the view ray. Since this integral is independent of the sample density, and depends only on its (x, y) projected location, a footprint function can be defined as follows:


footprint(x, y) = hv(x, y, w)dw -

(2.4)

where (x, y) is the displacement of an image sample from the center of the sample's image plane projection. The weight at each pixel can then be expressed as:

weight(x, y)s = footprint(x - xs, y - ys)

(2.5)

where (x, y) is the pixel's location, and (xs, ys) is the image plane location of the sample s.

A footprint table can be generated by evaluating the integral in Equation 2.4 on a grid with a resolution much higher than the image plane resolution. A footprint table for a data sample s can be centered this table on the projected image plane location of s and sampled in order to determine the weight of the contribution of s to each pixel on the image plane. Multiplying this weight by (s) yields the contribution of s to each pixel.

Chapter 2

9

Computing a footprint table can be difficult due to the integration required. Discrete integration methods can be used to approximate the continuous integral, but generating a footprint table is still a costly operation. Luckily, for orthographic projections, the footprint of each sample is the same except for an image plane offset. Therefore only one footprint table must be calculated per view. Since this still would require too much computation time, only one generic footprint table is built for the kernel. For each view, a view-transformed footprint table is created from the generic footprint table. This generic footprint table can be precomputed, therefore it does not matter how long the computation takes.
Generating a view-transformed footprint table from the generic footprint table can be accomplished in three steps. First, the image plane extent of the projection of the reconstruction kernel is determined. Next a mapping is computed between this extent and the extent that surrounds the generic footprint table. Finally, the value for each entry in the view-transformed footprint table is determined by mapping the location of the entry to the generic footprint table, and sampling. The extent of the reconstruction kernel is either a sphere, or is bounded by a sphere, so the extent of the generic footprint table is always a circle. If the grid spacing is identical for all three axes, then the reconstruction kernel is a sphere and the image plane extent of the reconstruction kernel will be a circle. The mapping from this extent to the extent of the generic footprint table is simply a scaling operation. If the grid spacing differs along the three axes, then the reconstruction kernel is an ellipsoid and the image plane extent of the reconstruction kernel will be an ellipse. In this case, a mapping from this ellipse to the circular

View Transformed

Extent

Generic Extent

Figure 2.1: Extent and mapping for a spherical reconstruction kernel.

Chapter 2

10

extent of the generic footprint table must be computed. The extents and mapping for both spherical and elliptical kernels are shown in Figures 2.1 and 2.2.

2.3.2. Image-Order Techniques
Image-order volume rendering techniques are fundamentally different from object-order rendering techniques. Instead of determining for a data sample how it affects the pixels on the image plane, in an image-order technique each pixel on the image plane is considered to determine which data samples contribute to it.
One of the first image-order volume rendering techniques, which is known as binary ray casting, was developed by Tuy and Tuy [Tuy84]. It was developed to generate images of surfaces contained within binary volumetric data without the need to explicitly perform boundary detection and hidden-surface removal. For each pixel on the image plane, a ray is sent from that pixel to determine if it intersects the surface contained within the data. If an intersection does occur, shading is performed at the intersection, and the resulting color is placed in the pixel. In order to determine the first intersection along the ray a stepping technique is used where the value is determined using zero-order interpolation at regular intervals along the ray until the object is intersected.
To produce a shaded image, the distance to the closest intersection could be stored at each pixel in the image, with this information then passed to a two-dimensional discrete shader, such

View Transformed Extent Generic Extent
Figure 2.2: Extent and mapping for an elliptical reconstruction kernel.

Chapter 2

11

as those described previously. However, better results can be obtained by performing a threedimensional discrete shading operation, such as gray-level shading, at the intersection point [Barillot85, Hoehne86, Tiede87, Tiede88, Cline88]. If the intersection occurs at location ( x, y, z) in the data, then the gray-level gradient at that location can be approximated with a central difference:

Gx =

f (x + 1, y, z) - f (x - 1, y, z) ,
2Sx

Gy =

f (x, y + 1, z) - f (x, y - 1, z) ,
2Sy

Gz =

f (x, y, z + 1) - f (x, y, z - 1) ,
2Sz

(2.6)

where (Gx,Gx,Gz) is the gradient vector, and Sx, Sy, and Sz are the distances between neighboring samples in the x, y, and z directions, respectively. The gradient vector is used as a normal vector for shading calculation, and the intensity value obtained from shading is stored in the image.

The previous algorithm deals with the display of surfaces within binary data. A more general algorithm can be used to generate surface and composite projections of multivalued data. Instead of traversing a continuous ray and determining the closest data sample for each step with a zero-order interpolation function, a discrete representation of the ray generated using a Bresenham-like algorithm or a 3D scan conversion algorithm [Kaufman86], could be employed.
As in the previous algorithms, the data samples that contribute to each pixel in the image plane must be determined. This could be done by casting a ray from each pixel in the direction of the viewing ray. This ray would be discretized, and the contribution from each voxel along the path is considered when producing the final pixel value. This technique is referred to as discrete ray casting. If a surface projection is required, this path is traversed until the first voxel that is part of the object is encountered. Shading is performed at this voxel and the resulting color value is stored in the pixel.
There are three types of connected paths: 6-connected, 18-connected, and 26-connected, which are based upon three adjacency relationships between consecutive voxels along the path. Assuming a voxel is represented as a box centered at the grid point, two voxels are said to be 6-connected if they share a face, they are 18-connected if they share a face or an edge, and they are 26-connected if they share a face, an edge or a vertex. A 6-connected path is a sequence of voxels, v1, v2, ... vN , where for each pair of voxels vi, vi+1 (1  i < N ), vi and vi+1 are 6-connected. Similar definitions exist for 18- and 26-connected paths. An example of these three types of connected paths is given in Figure 2.3.
Actually, stopping at the first opaque voxel and shading is only one of many operations that can be performed on the voxels along a discrete path or continuous ray. Instead, the whole ray could be traversed, storing in the image plane pixel the minimum, maximum, or average value encountered along the ray, or even a composite value representing all values encountered all the ray.

Chapter 2

12

6-Connected

26-Connected

18-Connected

Figure 2.3: 6-, 18-, and 26-connected paths.

The previous two techniques, binary ray casting and discrete ray casting, use zero-order interpolation in order to define the scalar value at any location in R3. One advantage to using zero-order interpolation is simplicity and speed, since many of the calculations required can be done using integer arithmetic. One disadvantage though is that artifacts, caused by the discrete nature of the algorithm, are visible in the image. Higher-order interpolation functions can be used to create a more visually pleasing, and typically more accurate image, but generally at the cost of algorithm complexity and computation time. The next three algorithms described in this subsection all use higher-order interpolation functions.
When creating a composite projection of a data set, two important parameters are typically the color at a location, and the opacity at that location. An image-order volume rendering algorithm developed by Levoy [Levoy88] states that given an array of data samples S, preprocessing techniques can be used to generate two new arrays Sc and S , which define the color and opacity at each grid location. The interpolation functions f (x, y, z), fc(x, y, z), and f (x, y, z), are then used to define the sample value, color, and opacity at any location in R3.
Generating the array Sc of color values involves performing a shading operation at each data sample in the original array S. For this purpose, the Phong illumination model could be used. The normal at each data sample is the unit gradient vector at that location. The gradient vector at any location can be computed by partially differentiating the interpolation function with respect to x, y, and z to get each component of the gradient. If the interpolation function is not first derivative continuous, aliasing artifacts will occur in the image due to the discontinuous

Chapter 2

13

normal vectors. A smoother set of gradient vectors can be obtained using the central differencing method defined in Equation 2.6.
Calculating the array S is essentially a surface classification operation. There are various different methods for classifying surfaces within a scalar field, and each method requires a new mapping from S(x, y, z) to S (x, y, z). If an isosurface at some constant value v is to be viewed with an opacity  v, then S (x, y, z) could simply be assigned to  v if S(x, y, z) is v, otherwise S (x, y, z) = 0. This would produce aliasing artifacts that can be reduced by setting S (x, y, z) close to  v if S(x, y, z) is close to v. The best results are obtained when the thickness of the transition region is constant throughout the volume. This can be approximated by having the opacity fall off at a rate inversely proportional to the magnitude of the local gradient vector.
Multiple isosurfaces can be displayed in a single image by separately applying the classification mappings, then combining the opacities. For example, if N isosurfaces are to be displayed with values vn and opacities  vn, 1  n  N , N opacity mappings would be defined, and the total opacity could be computed by:

N



total (x,

y,

z)

=

1

-

 (1
n=1

-



n(x,

y,

z)).

(2.7)

Once the Sc(x, y, z) and S (x, y, z) arrays have been defined, rays are cast from the pixels, through these two arrays, with sampling occurring at evenly spaced locations. To determine the value at a location, the trilinear interpolation functions fc() and f () are used. After these point samples along the ray have been obtained, a fully opaque background is added, and the values are then composited in a back-to-front order to produce a single pixel color.
Two rendering techniques for displaying volumetric data, known as the V-Buffer method, were developed by Upson and Keeler [Upson88]. One of the methods for visualizing the scalar field is an image-order ray-casting technique. The other method is a hybrid-technique, and therefore will be described in the next section.
In the ray-casting V-Buffer method, rays are cast from each pixel on the image plane into the volume. For each cell in the volume along the path of this ray, the scalar value is determined at the point where the ray first intersects the cell. The ray is then stepped along until it traverses the entire cell, with calculations for scalar values, shading, opacity, texture mapping, and depth cueing performed at each stepping point. This process is repeated for each cell along the ray, accumulating color and opacity, until the ray exits the volume, or the accumulated opacity reaches unity. At this point, the accumulated color and opacity for that pixel are stored, and the next ray is cast.
The goal of this method is not to produce a realistic image, but instead to provide a representation of the volumetric data that can be interpreted by a scientist. For this purpose, the user is given the ability to modify certain parameters in the shading equations, leading to an informative, rather than physically accurate shaded image. A simplified shading equation is used where the perceived intensity as a function of wavelength, I () is define as:

I () = Ka()Ia

+ Kd()

j

( N



L

j)I

 j

(2.8)

In this equation, Ka is the ambient coefficient, Ia is the ambient intensity, Kd is the diffuse coefficient, N is the normal approximated by the local gradient, L j is the vector to the jth light

Chapter 2

14

source, and I j is the intensity of the jth light source. In order to highlight certain features in the final image, the diffuse coefficient can be defined as a function of not only wavelength, but also
scalar value and solid texture:

Kd (, S, M) = K () Td (, S(x, y, z) M(, x, y, z)),

(2.9)

where K is the actual diffuse coefficient, Td is the color transfer function, S is the sample array, and M is the solid texture map. The color transfer function is defined for r, g, and b, and maps scalar value to intensity. Accumulating along the ray in this method approximates the following intensity integral:

I() =

[F
w

A(d

)O( s)[ K a (

)Ia

+

Kd (, S, M) [(N  L j)I j]] + (1 - F A(d)bg())]dw

(2.10)

where F A(d) represents atmospheric attenuation as a function of distance, O is the opacity transfer function, bg is the background color, and w is a vector in the direction of the view ray. The opacity transfer function is similar to the color transfer function in that it defines opacity as a function of scalar value. Different color and opacity transfer functions can be defined to highlight different features in the volume.

In order to simulate light coming from translucent objects, volumetric data with data samples representing density values can be considered as a field of density emitters, as proposed by Sabella [Sabella88]. A density emitter is a tiny particle that both emits and scatters light. The amount of density emitters in any small region within the volume is proportional to the scalar value in that region. These density emitters are used to correctly model the occlusion of deeper parts of the volume by closer parts, but both shadowing and color variation due to differences in scattering at different wavelengths are ignored. Similar to the V-Buffer method, rays are cast from the eye point, through each pixel on the image plane, and into the volume. The intensity I of light for a given pixel is calculated according to:

t

t2 -  ()d 

 I = e t1

 (t)dt

t1

(2.11)

In this equation, the ray is traversed from t1 to t2, accumulating at each location t the density t

-  ()d 

 (t) at that location attenuated by the probability e t1

that this light will be scattered

before reaching the eye. The parameter  is modifiable, and controls the attenuation, with higher

values of  specifying a medium which darkens more rapidly. The parameter  is also

modifiable, and can be used to control the spread of density values. Low  values produce a

diffuse cloud appearance, while higher  values highlight dense portions of the data.

Krueger [Krueger91] showed that the various existing volume rendering models can be described as special cases of an underlying transport theory model of the transfer of particles in inhomogeneous media. The basic idea is that a beam of ``virtual'' particles is sent through the volume, with the user selecting the particle properties and the laws of interaction between the particles and the data. The image plane then contains the ``scattered'' virtual particles, and information about the data is obtained from the scattering pattern. If, for example, the virtual particles are chosen to have the properties of photons, and the laws of interaction are governed by

Chapter 2

15

optical laws, then this model essentially becomes a generalized ray tracer. Other virtual particles and interaction laws can be used, for example, to identify periodicities and similar hidden symmetries of the data.
Using Krueger's transport theory model, the intensity of light I at a pixel can be described as a path integral along the view ray:

S

S far -  a(S)+ sc(S)d S

I = Q(S)e Snear

dS

S near

(2.12)

The emission at each point S along the ray is scaled by the optical depth to the eye to produce the

final intensity value for a pixel. The optical depth is a function of the total extinction coefficient,
which is composed of the absorption coefficient  a, and the scattering coefficient  sc. The generalized source Q(S) is defined as:

Q(S) = q(S) +  sc(S)  sc(  )I (S, )d

(2.13)

This generalized source consists of the emission at a given point q(S), and the incoming intensity along all direction scaled by the scattering phase  sc. Typically, a low albedo approximation is used to simplify the calculations, reducing the integral in Equation 2.13 to a sum over all light sources.
The algorithms described so far in this section are concerned primarily with conveying useful information in the final image, rather than generating photo-realistic images. A threedimensional raster ray tracing method (RRT), developed by Yagel, Cohen and Kaufman [Yagel92b, Yagel91], produces realistic images of volumetric data using a global illumination model. The RRT algorithm is similar to the discrete ray casting algorithm described earlier in this section. Discrete rays are cast from the image plane, through the data to determine pixel values. Secondary rays are recursively spawned when a ray encounters a voxel belonging to an object in the data. To save time, the view-independent parts of the illumination equation can be precomputed and added to the voxel's color, thereby avoiding the calculation of this quantity during ray tracing. Also, two bits per light source per voxel can be precomputed, indicating whether the light is definitely visible, possibly visible, or definitely not visible from that voxel. Shadow rays need only be cast during ray tracing if the bits indicate that the light is possibly visible through a translucent object. Actually, all view-independent attributes (including texture) can be precomputed and stored with each voxel.

2.3.3. Hybrid Techniques
Some volume rendering techniques are not completely image-order or object-order methods, but are actually a combination of both. Some of these techniques are described in this subsection.
As mentioned in the previous subsection, Upson and Keeler developed two volume rendering techniques for displaying scalar fields, known as the V-Buffer method [Upson88]. The image-order technique was described in the previous subsection. The other method for

Chapter 2

16

visualizing the scalar field is a cell-by-cell processing technique, where within each cell an image-order ray-casting technique is used, thus making this a hybrid technique.
In this method, cells in the volume are processed in a front-to-back order. Each cell is processed by first determining for each scan line in the image plane, which pixels are affected by the cell. Then an integration volume is determined for each pixel as shown in Figure 2.4. Within the bounds of the integration volume, an intensity calculation similar to Equation 2.10 is performed according to:

   I() =

x

y

[F
z

A (d )O(s)[ K a (

)Ia

+

Kd

(

,

S,

M

)

[(N  L j)I j]] +

(1 - F A(d)bg())]dxdydz

(2.14)

This process continues in a front-to-back order, until all cells have been processed, with intensity accumulated into pixel values. Once a pixel's opacity reaches unity, a flag is set and this pixel is not processed further. Due to the front-to-back nature of this algorithm, incremental display of the image is possible.

Drebin, Carpenter, and Hanrahan [Drebin88] developed a technique for rendering volumes that contain mixtures of materials, such as CT data containing bone, muscle and flesh. In this method it is assumed that volume contains either several scalar fields, or one scalar field

Pixel

Integration Volume
Figure 2.4: Integration volume for V-Buffer volume rendering.

Chapter 2

17

representing the composition of several materials. If the latter is the case, it is assumed that material can be differentiated either by the scalar value at each point, or by additional information about the composition of each volume element.
The first step in this rendering algorithm is to create new scalar fields from the input data, known as material percentage volumes. Each material percentage volume is a scalar field representing only one material. Color and opacity are then associated with each material, with composite color and opacity obtained by linearly combining the color and opacity for each material percentage volume. A matte volume, which is a scalar field on the volume with values ranging between 0 and 1, is used to slice the volume, or perform other spatial set operations. Actual rendering of the final composite scalar field is obtained by transforming the volume so that one axis is perpendicular to the image plane. The data is then projected plane by plane in a back-to-front manner and composited to form the final image.

2.4. Volume Rendering Optimizations
Volume rendering can produce informative images that are useful in data analysis, but a major drawback of many of the techniques described in this chapter is the time required to generate a high-quality image. In this section, several volume rendering optimizations are described that decrease rendering times, and therefore increase interactivity and productivity.
Object-order volume rendering typically loops through the data, calculating the contribution of each volume sample to pixels on the image plane. This is a costly operation for moderate to large sized data sets (e.g., 128M bytes for a 5123 sample data set, with one byte per sample), leading to rendering times that are not interactive. One way to decrease rendering time is to decrease the resolution of the data. A pyramid data structure can be built, which for an original data set containing N 3 data samples, consists of a sequence of log N volumes. The first volume is the original data set, while the second volume is created by averaging each 222 group of samples of the original data set to create one sample in a volume of one-eight the resolution. The third volume is create from the second volume in a similar fashion, with this processes continuing until all log N volumes have been created. An efficient implementation of the splatting algorithm, called hierarchical splatting [Laur91], uses such a pyramid data structure. According to the desired image quality, this algorithm scans the appropriate level of the pyramid in a back-to-front order. Each element is splatted onto the image plane using the appropriate sized splat. The splats themselves are approximated by polygons which can efficiently be rendered by graphics hardware.
As mentioned previously, image-order volume rendering can be accomplished using a discrete ray casting technique. It would be quite computationally expensive to discretize every ray cast from the image plane. Fortunately, this is unnecessary for parallel projections. Since all the rays are parallel, one ray can be discretized then used as a ``template'' for all other rays. This technique, developed by Yagel and Kaufman [Yagel92c], is called template-based volume viewing. A 26-connected template v1, v2, ... vN is created such that removing any vi (1 < i < N ) would destroy the connectivity of the path. If this template were used to cast a ray from each pixel in the image plane, some voxels in the data may contribute to the image twice while others may not be considered at all. A two-dimensional example of this problem is shown in Figure 2.5a. To solve this problem, the rays are cast instead from the base plane, which is the plane of

Chapter 2

18

the volume buffer most parallel to the image plane. This ensures that each data sample can contribute at most once to the final image, and all data samples could potentially contribute. When these rays are extended back to the image plane, it is clear that these rays have not been cast exactly from the pixels. Once all the rays have been cast, a simple final step of resampling is required that employs bilinear interpolation to determine the pixel values from the calculated ray values. Figure 2.5b shows this process for a two-dimensional data set with a onedimensional image line.
An extension can be made to this template-based ray casting to allow higher-order interpolation [Yagel92d]. The template for higher-order interpolation consists of connected cells, as opposed to the connected voxel template used for zero-order interpolation. With higher-order interpolation the value varies within a cell, so it is desirable to take multiple samples along the continuous ray inside of each cell. Since these samples are taken at regular intervals, and the same template is used for every ray, there is only a finite number of three-dimensional locations

(a) (b)
Figure 2.5: Discrete rays cast from (a) the image plane and (b) the base plane.

Chapter 2

19

(relative to a cell) at which sampling occurs. This fact allows part of the interpolation function to be precomputed and stored in a table, allowing for faster rendering times.
Another extension to template-based ray casting allows for screen space supersampling to improve image quality [Yagel92d]. This is accomplished by allowing rays to originate at subpixel locations. A finite number of sub-pixel locations from which a ray can originate is selected, and a template is created for each. When a ray is cast, its sub-pixel location determines which template is used. For example, to accomplish a 22 uniform supersampling, four rays would be cast per pixel, and therefore four sub-pixel locations are possible. Stochastic supersampling can also be supported by limiting the possible ray origins to a finite number of sub-pixel locations, and precomputing a template for each.
One obvious optimization for both discrete and continuous ray casting is to limit the sampling to the segment of the ray that intersects the data. If the data itself contains many zerovalued data samples, or a segmentation function is applied to the data that evaluates to 0 for many samples, the efficiency of ray casting can be greatly enhanced by further limiting the segment of the ray along which samples are taken. One algorithm of this sort is known as polygon assisted ray casting, or PARC [Avila92]. This algorithm approximates objects contained within a volume using a crude polyhedral representation. Only the segment of the ray passing through this polyhedral representation is considered when searching for a surface intersection point, which can reduce the number of samples required, and therefore also reduce rendering times.

2.5. Summary
In this chapter, image-order, object-order, and hybrid volume rendering techniques were discussed. Except for RRT, these methods all employ either a strictly local illumination model, or account only for attenuation along the viewing ray. In general, it would be difficult to include a global illumination model in an object-order volume rendering technique, since only one data sample is considered at a time during the image generation process. The image-order and hybrid volume rendering techniques employ local illumination models for efficiency and clarity since global effects would increase rendering times, and in some cases may complicate data interpretation.

Chapter 3 Global Illumination Models

Generating an image of a three-dimensional scene composed of geometric surfaces requires a process of visible surface detection, and the evaluation of an intensity equation for visible surface points. Similar to volume rendering, visible surface detection techniques for geometric scenes include both image-order and object-order methods, also known as image-precision and object-precision methods. Ray casting [Appel68, Group68, Goldstein71] is an example of an image-order method, where rays are cast to determine the first visible surface location for each pixel. The intensity equation is evaluated at this location, and the resulting color is stored in the pixel. Object-order methods are typically employed by scan-line algorithms [Wylie67, Bouknight70a, Bouknight70b, Watkins70], and graphics hardware, where geometric primitives are projected onto the image plane. Either one intensity value for the primitive, or an intensity value at each vertex [Wylie67, Gourad71], is used to determine the final pixel color.
A general equation for the intensity of light traveling in the direction  from a surface location x is:

I (x, ) = E(x, ) +

g(x, x) sc(x, , )I (x, )d

(3.1)

=4

where E(x, ) is the light emitted at x in the direction , and the integral over all incoming directions  accounts for light arriving from all other surfaces that is redirected into  by x. The surface point x is the first visible surface at x from the direction . The geometry term

g(x, x) accounts for geometric factors such as occlusion and light attenuation. The scattering function  sc(x, , ) indicates the fraction of light arriving from the  direction that is redirected into the  direction by x. The terms I , E, and  sc are wavelength dependent, and can be evaluated at different wavelengths to determine a final pixel color.

In general, Equation 3.1 is far too complex to compute analytically for each visible surface point. Therefore, an approximation is typically made that decreases the complexity of this equation. In this chapter, several basic illumination models for geometric scenes are described, and the approximations made in each of these models are discussed.

3.1. Local Illumination
Early algorithms for rendering geometric scenes are based on local illumination models. A local illumination model calculates the intensity of light at some location using only local information such as surface normal and the distance to each light source. For example, in a scene with m light sources the intensity of light in the direction  at a surface point x is:

Chapter 3

21

I (x, )

=

I A k AOD

+

1im

A(

x

i,

x

)

E

(

x

i,

-


Li

 )k 

D

O

D

 


N




Li

 

+

k

S

 


N




Hi

 

n

 . 

(3.2)


In this equation, I A is the intensity of ambient light in the scene, E(xi,-Li) is the intensity of light 
originating from point light source i at location xi and traveling in the -Li direction, where Li is

the vector from x to xi. A(xi, x) is the light attenuation factor for point light source i, OD is the

diffuse color of the surface, and k A, kD, and kS are the ambient, diffuse, and specular coefficients

for this point.


N

is

the

normal

vector

at

the

point,

and


Hi

is

the

vector

halfway

between



and



Li. This illumination model is illustrated in Figure 3.1.

Equation 3.2 allows only for simple point light sources that are not visible in the final

image, and therefore the term E is not directly included in the equation for all surface points.

The integral in Equation 3.1 is replaced with a sum over all light sources to account for emitted

light that is visible from the view point after a diffuse interaction with the surface point x,

E

(

x

i,

-


Li

)

k

D

O

D

 


N




Li

 

,

and

light

that

travels

down

a

single

bounce

specular

highlight

path,

E

(

x

i,

-


Li

)

k

S

 


N




Hi

 

n

.

In Equation 3.2, the specular highlight path occurs not only in the

direction of perfect specular reflection, but in a cone of angles around this direction, according to

 


N




H

i

n 

in

order

to

simulate

the

effect

of

a

non-perfect

surface

reflection

function.

This is only

an approximation to produce more pleasing images, and is not a physically correct solution.

Since only direct diffuse illumination and one bounce specular highlights are considered, the

approximation of the light arriving at x is too small. The ambient term is used to approximate all

other light interactions in the scene. I A is constant throughout the scene, and is therefore

Eye Point

N Hi

 

Light Source
Li

Figure 3.1: Local illumination model.

Chapter 3

22

physically incorrect for most x. Finally, the geometric term g of Equation 3.1 is approximated in Equation 3.2 using the attenuation factor and the dot products, but occlusion is not calculated.
Using a brute-force ray casting algorithm where every object is considered for intersection at every pixel, the intersection process requires O(M2 N ) work for an MM pixel image of a scene containing N objects. Each shading calculation then requires O(L) work, where L is the number of light sources in the scene. This yields an O(M2(N + L)) algorithm for generating an image using a local illumination model.
Local illumination models give nice results for single, convex objects, but cannot correctly render a scene with multiple, concave, translucent or reflective objects. For example, there is no way to determine whether a surface location is in a shadow by looking at local characteristics; the entire scene must be considered to see if any other surface might block light traveling from the light source to this location.
Many algorithms were developed to augment the local illumination equation, producing some global effects. Shadows can be generated using a scan-line algorithm [Bouknight70a], a two-pass rendering algorithm [Atherton78], a shadow volume algorithm [Crow77], or a two-pass Z-buffer algorithm [Williams78]. Transparent objects can be rendered using a back-to-front Zbuffer based algorithm [Mammen89], or screen-door transparency [Foley90]. Also, interobject reflections can be approximated using reflection mapping [Blinn76, Hall86].
Unfortunately, every global effect requires another algorithm, many of which can only approximate the actual effect. Global illumination models were developed to incorporate global effects into the illumination model itself, eliminating the need for these special algorithms.

3.2. Ray Tracing

Some of the most realistic computer generated images have been created using an image synthesis technique known as ray tracing [Appel68, Whitted80]. This technique, based on a global illumination model that attempts to simulate the physics of light, can produce images with a variety of effects including shadows, specular reflection and specular transmission of light. The basic idea of ray tracing is similar to the pinhole camera concept, only approached from the opposite direction. Primary rays are sent from the eye point through each pixel in the image plane and into the scene to determine how much light came from this direction. When the ray intersects an object, secondary rays are recursively spawned which originate at the intersection point, and travel to each light source to determine shadowing, and in the directions of reflection and transmission to compute additional intensity contributions.

The basic ray tracing illumination equation is similar to equation 3.2:

I (x, ) = I Ak AOD

+

1im

S(xi,

x) A(xi,

x)E(xi,

-


Li

 )k 

D

O

D

 


N




Li

 

+

k

SR

 


N




Hi

 

n

  

+ kSR I (xSR,SR) + kST I (xST ,ST ).

(3.3)

Chapter 3

23

The term S(xi, x) is used to indicate the percentage of light from source i that reaches this surface unoccluded, while A(xi, x) accounts for attenuation as the light travels from xi to x. These two terms, together with the two dot products, form the geometric term g from Equation 3.1. I (xSR,SR) and I (xST,ST ) are the intensity of light arriving from the direction of specular reflection SR and specular transmission ST , where xSR and xST are the first surface locations encountered along the direction of specular reflection and transmission, respectively. In comparison to Equation 3.1, the set of directions considered here includes only the light source directions, and the directions of specular reflection and transmission.
Using a brute-force ray casting technique for primary ray intersection calculations requires O(M2 N ) for an MM pixel image of a scene containing N objects. Direct illumination calculations, including shadowing computations, then require O(LN ) work since a ray must be cast to each of the L light sources to determine occlusion. Specular reflection and specular transmission contributions each require a recursive call to the intensity equation, yielding a final image generation complexity of O(M22l+1(N + NL)), where l is the maximum depth of recursion.
Many improvements have been made to the original ray tracing algorithm in order to increase efficiency. Since ray-object intersection calculations account for as much as 95 percent of the computation time of the algorithm [Whitted80], reducing the complexity and number of these calculations is critical. One of the first techniques used to speed up ray tracing is to enclose each object within a simpler bounding object. A fast test is developed to determine if a ray intersects the bounding object. This test is applied, and the more complicated (and time consuming) intersection calculation for the object is performed only if the ray intersects the bounding object. Different bounding objects have been used including spheres [Whitted80], boxes [Roth82], and parallelopipeds [Kay86].
Space subdivision techniques have been used to reduce the number of ray-object intersection calculations. These techniques involve subdiving the scene into regions, or cells, where each cell contains a list of objects whose surface intersects the cell. Uniform [Fujimoto86], and octree [Glassner84] space subdivision techniques have both been considered, and significant efficiency improvements have been realized. A technique based on fivedimensional subdivision [Arvo87], with three spatial and two orientation dimensions, has also been used to accelerate ray tracing. In this technique, the position along the ray as well the direction of the ray are used to limit the objects considered for intersection.
Another important aspect of ray tracing that has been investigated by many researchers is accuracy. Ray tracing is essentially a form of point sampling, so artifacts can arise. A pixel represents an area on the screen, not just a point, so a ray should represent not just a line, but a volume. This can be achieved by redefining a ray as, for example, a cone [Amanatides84], a beam [Heckbert84], or a pencil [Shinya87]. These new ray types can be used to reduce spatial aliasing that is found on the silhouettes of objects, and temporal aliasing that occurs during animation. Also, new effects such as fuzzy shadows, dull reflections, and dull translucency can be incorporated into an image since a ray-object intersection is no longer binary decision. These fuzzy effects can significantly increase the realistic appearance of an image, since real world objects are generally not perfect specular reflectors and transmitters of light. The disadvantage to these methods is that the new ray definition typically results in more complex ray-object intersection calculations, and therefore increases rendering times.

Chapter 3

24

Another method for adding realistic effects to an image is to simplify the intensity evaluation technique instead of simplifying the intensity equation. This approach is used in distributed ray tracing [Cook84], where a more complex intensity equation similar to Equation 3.1 is point sampled to approximate the final intensity value. This leads to dull reflections, that can be produced by sampling the scattering function, while fuzzy shadows can be included by sampling area light sources to estimate partial occlusion in g. The increased realism comes at the cost of increased rendering times due to the larger number of rays required for this method. Also, the scattering function must be close to a perfect specular function for light arriving from other surface locations. If it is instead, for example, a diffuse scattering function, then the variance in sample intensities will be high, and a prohibitively high number of rays must be cast at each surface point to converge on a solution. Similarly, only small area light sources with simple shapes can be used for illumination in order to produce realistic shadowing effects.

3.3. The Rendering Equation

Even with these extensions to the basic ray tracing method, many global effects are lost due to simplifying assumptions made to reduce computational costs. The desire for a more complete illumination model led to the development of the Rendering Equation [Kajiya86]. This illumination equation describes the intensity of photon transport as:

I (x1, x2) = g(x1, x2) (x1, x2) + x3S (x1, x2, x3)I (x2, x3)dx3.

(3.4)

I (x1, x2) is known as the transport intensity, and measures the energy of radiation passing from point x2 to point x1. g(x1, x2) is a geometry term similar to the one in Equation 3.1.  (x1, x2) is an emittance term that measures the amount of energy emitted by a surface at point x2 which reaches point x1, known as the transport emittance. (x1, x2, x3) is a scattering term that is the intensity of energy that originates at point x3, travels to point x2, and is reflected to point x1. This is known as the transport reflectance. S is the union of all surfaces in the environment, including

a global backdrop surface. Equation 3.4 basically states that the transport intensity of light

arriving at point x1 from point x2 is the sum of the light emitted by point x2 reaching point x1 and the total intensity of light reflected by point x2 to point x1 from all other surface points. This equation is essentially equivalent to Equation 3.1, except that it is phrased as an integral over all

surface locations instead of all directions.

The method of solving the Rendering Equation is based on a Monte Carlo solution. A brief description of this method is given here, while more details on Monte Carlo techniques can be found in one of the classic texts [Hammersley67, Screider66, Halton70, Yakowitz77, Kuo72].

To solve the rendering equation using a Monte Carlo technique, it is necessary to note that Equation 3.4 is of the form:

 (x) =  (x) +  (x, x) (x)d (x) x  S

(3.5)

and is known as a Fredholm Equation of the Second Kind. An unbiased primary estimator for

such an equation is:

Chapter 3

25

 (x0) =  (x0) +
 (x0, x1) (x1) + P1(x0, x1)
 (x0, x1) (x1, x2) (x2) + P2(x0, x1, x2)
 (x0, x1) (x1, x2) (x2, x3) (x3) + P3(x0, x1, x2, x3)
...

(3.6)

 (x0,

x1) (x1, x2) . . .  (xn-1, Pn(x0, x1, . . . , xn)

xn) (xn)

.

Pi(x0, x1, . . . , xi) is a probability density function indicating the probability that light will travel the specified path from xn to x0.

Simplifying this equation by making the probability density function proportional to  (),

and rewriting using the terms from Equation 3.4, results in:

I (x0, x1) = g(x0, x1) (x0, x1) + g(x0, x1)g(x1, x2)(x0, x1, x2) (x1, x2) + g(x0, x1)g(x1, x2)g(x2, x3)(x0, x1, x2)(x1, x2, x3) (x2, x3) + ... .

(3.7)

This series can be expanded until a g or  term becomes 0, or can be terminated probabilistically [Arvo90].

An algorithm for solving the rendering equation using Monte Carlo Markov chains is
straight-forward from Equation 3.7. Starting at the eye, x0, send a ray out into the scene and find the first intersection point, x1. Calculate the light emitted by this point that reaches x0 and set I (x0, x1) equal to this value. Select another surface point by casting a ray from x1 in a chosen direction and finding the closest intersection point, x2. Calculate the amount of light emitted by x2 that reaches x1, scale this by the probability that this light will then travel to x0, and add this to I (x0, x1). Continue on in this manner, essentially computing one term of Equation 3.7 at each step, for the length of the Markov chain [Shirley90 ].

The obvious advantage to this method over classical ray tracing is that many global effects that are lost in the simplifications of ray tracing can be captured using the Rendering Equation. The main disadvantage is that these global effects require the computation of many possible light paths. This would result in prohibitively high rendering times or restrictions on the scene if implemented as described above. For example, consider a scene where the area of the light sources is relatively small compared to the area of all surfaces in the scene. This results in a high variance for the light path intensities, since most sample paths will not encounter a light source. To reduce this problem, direct lighting instead of emitted light can be computed at each surface point [Kajiya86]. This method reduces but does not entirely eliminate the problem since light arriving from alight source after encountering one or more specular surfaces still results in a high variance.

Chapter 3

26

If a brute-force ray casting technique is employed to evaluate each g term, the amount of
work required to compute the intensity along a path from x0 to xn is O(nN ). If n is the average path length, and c is the average number of paths per pixel, generating an MM pixel image using this method would require O(M2ncN ) work. If direct illumination is computed at each step instead of emitted light, this complexity becomes O(M2nc(N + NL)). The difficulty with
this method is that in order to produce an image with no visible artifacts, the constant c must be
very large.

3.4. Radiosity

Most objects in a real environment reflect light diffusely. Conventional ray tracing does not account for the interaction between diffuse surfaces, and therefore does not correctly calculate some global illumination effects. In response to this, the radiosity method was created [Goral84, Nishita85]. In this method, all surfaces are assumed to be Lambertian reflectors, and calculations are performed similar to those used for radiative heat exchange in enclosures. Unlike the shiny, reflective images produced by ray tracing, images generated by the conventional radiosity method typically contain Lambertian surfaces showing global diffuse interactions.

In radiosity, a scene is represented by a finite number of discrete patches. Each patch has finite size, and is assumed to emit and reflect light uniformly over its entire area. For each patch i in a scene consisting of N patches:

Bi

=

Ei + i

B j F j,i

1 jN

Aj Ai

,

(3.8)

or equivalently:

Bi = Ei + i

B j Fi, j,

1 jN

(3.9)

due to the relationship:

A j F j,i = Ai Fi, j .

(3.10)

Bi and B j are the radiosities of patches i and j, and Ei is the rate at which light is emitted from patch i. These quantities are measured in energy per unit time per unit area. The reflectivity of patch i is i, and F j,i is a form factor specifying the fraction of energy leaving patch j that arrives at patch i, respectively. This form factor is based on the shape and orientation of the two patches, and the presence of any patches between them in the scene. Ai and A j are the areas of patches i and j. This equation is similar to Equation 3.1 with the arbitrary scattering function  sc replaced by pure Lambertian reflection.
Equation 3.8 can be expressed as a set of simultaneous equations:

Chapter 3

27

 1 - 1 F1,1 -1 F1,2 . .

 

- 2 F2,1

1 - 2 F2,2 . .

-1 F1,N   B1   E1 

- 2 F2,N

 

 

B2

 

 

E2

 

.

.

.  . = . 

.

 

- N F N,1

. - N F N,2

.  . 

.

.

1

-

N

FN,N

 

 

BN

 

.

 

EN

 

(3.11)

and solved with any standard matrix solver. Since i and Ei are wavelength dependent, this equation must be solved for each band of wavelengths considered in the lighting model. The entire scene can then be rendered from any viewing direction using only a conventional visiblesurface algorithm. Since an entire patch has the same radiosity, this may produce sharp edges and Mach bands on a smooth surface. To reduce this effect, radiosities can be assigned to vertices, and radiosity interpolation can be performed over each surface.

Figure 3.1 illustrates that the form factor from the differential area dAi to the differential area dA j is:

dF di,dj

=

cos  i cos  j  r2

Hij dA j.

(3.12)

dA j

Ni
i

r

j
Nj

Aj

Ai dA i

Figure 3.2: The form factor from dAi to dA j.

Chapter 3

28

A line segment with length r connects the two differential areas, where  i is the angle between that line segment and the normal of patch i, and  j is the angle between the line segment and the normal of patch j. Hij is a binary term that is 1 if dA j is visible from dAi, and is 0 otherwise.

Integration is required to determine the form factor from the differential area dAi to the finite area A j of patch j:

Fdi, j =

cos  i cos  j  r2

Hij dA j.

Aj

(3.13)

An average of Equation 3.13 must be taken over the entire patch i. This yields the form factor from patch i to patch j:

 Fi, j

=

1 Ai

Ai A j

cos  i cos  j  r2

Hij dA j dAi.

(3.14)

Similar to ray-object intersection calculations in ray tracing, determining the form factors in

radiosity is typically one of the most expensive, time consuming parts of the algorithm. In the

general case, Equation 3.7 is difficult to solve analytically due to the double integral it contains.

If

the

distance

between patch

i

and

patch

j

is large compared to

their

sizes,

then

cos  i cos  j  r2

will remain relatively constant. Therefore a good approximation of Fi, j can be made by

calculating Fdi, j for the differential area dAi at the center of patch i. There is now only one

integral in the form factor equation, yet it is still too costly to compute directly.

A geometric method for calculating form factors in thermal heat transfer [Siegel72] is
illustrated in Figure 3.3a. A hemisphere is constructed, with the base being a circle with unit
radius, centered at differential area dAi, and perpendicular to the normal of patch i. The form factor Fi, j is then equivalent to the fraction of the base circle that is covered by projecting patch j first onto the hemisphere, then orthographically down onto the circle.

A version of the hemisphere technique, known as the hemicube method [Cohen85], was developed for use in computer graphics. In Figure 3.3b, a hemicube is shown to be one half of a cube centered at the differential area dAi, with the Z axis of the cube coinciding with the normal of patch i. Each patch in the scene is then projected onto this hemicube, with a depth buffer hidden surface algorithm used to determine the closest patch projecting onto each element. Equation 3.13 is then used to determine the delta form factor for each element. This allows an entire row or column of N form factors from Equation 3.11 to be computed with O(N ) work.

The form factor between two patches in a scene can also be approximated using a ray casting technique [Malley88, Ward88, Wallace89, Sillion89]. As mentioned above, the form factor between two patches can be approximated by the differential form factor only if the distance between the two patches is large compared to the areas of the patches. This condition can not always be met, as in the case of two polygons joined at an edge. To solve this, the form factor could be solved analytically [Baum89], the form factor integral can be adaptively supersampled using a ray casting technique [Wallace89].

To solve for the radiosity values in a scene using the basic radiosity algorithm with a hemicube technique for computing form factors required O(N 2) work, since each of the N
patches can potentially interact with all N patches in the scene. After the radiosity equations

Chapter 3

29

Aj Ni

Aj Ni

dAi 1

dAi

(a) (b)

Figure 3.3: (a) The hemisphere method and (b) the hemicube method for form factor calculation.

have been solved, an image can be generated with O(M2 N ) work for an MM pixel image. Since the radiosity solution is view-independent, multiple images can be generated from one radiosity solution.
Even with the form factor acceleration techniques, radiosity calculations are still slow, and meaningful images of a partial solution cannot be created using the ``gathering'' method in Equation 3.11. This equation can be revised to a ``shooting'' method [Cohen88] where instead of considering how light is gathered by some patch i from all other patches, the radiosity of patch i is contributed, or shot, to all other patches in the scene. This technique is known as progressive refinement radiosity. Using this shooting algorithm, images of partial results can be computed during the radiosity calculation. Since early images may be dark, a constant ambient term representing the radiosity that has not been shot can be added when computing the partial image.

Chapter 3

30

Another method for reducing the over-all time required to compute a radiosity solution is to reduce the number of patch-to-patch interactions. The first technique for reducing the number of interactions, known as substructuring [Cohen86, Campbell90, Heckbert90], is based on the similarities between neighboring patches. Basically, the form factor between patch si and patch s j is similar to the form factor between si and sk if sk is a neighbor of s j with similar surface characteristics, and the distance between si and s j is large compared to their areas. In substructuring, several small patches can be grouped together to form one larger patch before interacting with the rest of the patches in the scene. The basic substructuring idea has been extended to employ hierarchical models of the patches in a scene [Hanrahan90, Hanrahan91, Gortler93]. These techniques, known as hierarchical radiosity, can be used to reduce the complexity of the radiosity method for a scene with N patches from O(N 2) to O(N ).
The high number of patches required to compute a smooth solution to the radiosity equation is a direct result of the assumption that light behaves uniformly over the entire area of each patch. If higher-order functions are allowed to represent the interaction of light [Troutman93, Zatz93], then fewer patches can be used to represent the scene. The drawback to this method is that although the number of interactions decreases, the cost of each individual interaction increases.
One problem with reducing the number of patches using either hierarchical or higher-order methods is that it is harder to capture the discontinuities that occur at surface intersections and shadow boundaries with large patches. Discontinuity meshing [Baum91, Campbell91, Heckbert92, Chin92] reduces this problem by locating these discontinuities, and using them as boundaries for the scene patches. This method can be combined with hierarchical [Lischinski93] and higher-order methods [Heckbert91] to improve the accuracy of the final image.
Extensions have been made to the classical radiosity algorithm in order to remove the restriction of having only Lambertian surfaces. Directional effects cannot be captured by storing only one radiosity value for each patch, since the incomining direction of radiosity at a patch determines not only the intensity, but the direction of the outgoing radiosity. One way to extend this method to include directional effects is to use a discrete ordinates method [Cohen86, Buckalew89, Saec90]. In this method, a set of radiosity values are stored at each patch, representing a set of directions. A reflectance function is defined for each patch indicating the fraction of light arriving from each incident direction that is reflected into each outgoing direction. The problem of including directional effects in radiosity can also be solved using spherical harmonics [Sillion91], where a finite number of terms are used to approximate relatively smooth surface reflectance functions.

3.5. Hybrid Illumination Techniques
A comparison of the local illumination model in Equation 3.2, the ray tracing model in Equation 3.3, the radiosity model in Equation 3.8, and the Rendering Equation model in Equation 3.4 with the general illumination model described in Equation 3.1 is given in Table 3.1. The first four rows of this table indicate whether the given model includes emission, the geometry term, a scattering function, and an integral over all directions. The fifth row indicates whether the associated rendering technique computes an exact answer to the intensity equation. The possible paths that light may take from a light source to the eye point [Heckbert90] for each illumination model are illustrated in Figure 3.4, where D represents Lambertian reflectors, S

Chapter 3

31

represents pure specular reflectors and transmitters, and A represents a surface of arbitrary reflectance. In the local illumination model and ray tracing, surfaces are actually a linear combination of ideal diffuse reflectors, pure specular reflectors, and pure specular transmitters, and multiple paths contributing to the final intensity value may pass through the same surface point.
From this table it is clear that only the Rendering Equation includes all the terms of the general illumination model, but the rendering method associated with this illumination model

Term

Table 3.1: A comparison of basic global illumination models.

Local Illumination

Ray Tracing

Radiosity

Rendering Equation

Part Part

Part

E

Not directly included Not directly included Only diffuse emission

Yes

Part Yes Yes Yes g
No occlusion

Part Part Part

Linear combination of: Linear combination of:

Only pure

 - Lambertian

- Lambertian

sc

- Specular Reflection - Pure specular reflection

Lambertian scattering

- Pure specular

transmission

Yes


=

Part
Only directions of: - light sources

Part
Only directions of: - light sources - specular reflection - specular transmission

Part Sum over all patches

Yes

Exact Solution?

Yes

Yes

Yes No Approximates solution with sample paths

Chapter 3

32

approximates the intensity by taking many sample paths. The ray tracing method has been extended using either a new ray definition, or distributed ray tracing, in order to capture some dull specular effects and limited area light sources, but these methods cannot truly capture diffuse effects. The radiosity method has been extended using direction bins or spherical harmonics to account for directional effects, but cannot truly capture pure specular effects.
A solution to these problems can be found by combining ray tracing and radiosity in order to generate images with both specular and diffuse effects. A multipass solution typically involves a radiosity pass to compute view-independent diffuse effects, followed by a ray tracing pass to compute view-dependent specular effects and the final image [Wallace87, Sillion89]. The ray tracing pass can be replaced by a Monte Carlo ray tracing solution, similar to the one used to solve the Rendering Equation, in order to capture strongly directional, but not necessarily pure specular surfaces [Shirley90].

3.6. Indirect Specular Illumination

Neither the ray tracing illumination model nor the radiosity illumination model can account for indirect specular lighting in a scene. Indirect specular lighting occurs when light from a light source interacts with one or more specular elements before arriving at a diffuse element. Indirect specular lighting paths, also known as caustic paths, are illustrated in Figure 3.5. To account for indirect specular lighting in a scene, a method for computing the indirect illumination can be added to the hybrid method [Shirley90, Chen91]. The indirect specular lighting pass typically computes a view-independent caustic map for each object in the scene. During the image generation pass of the hybrid method, the intensity values stored in the caustic maps are consulted to determine indirect specular lighting at surface locations. The remainder of this section is dedicated to methods for computing indirect specular illumumination.
The intensity of light at a diffuse surface location x arriving from indirect specular paths can be expressed as:

where

IIS(x) =

g1(x, x)IS(x, ) d

=4

I (x, ) = kSR g2(x, x)[E(x, SR) + I (x, SR)] + kST g3(x, x)[E(x, ST ) + IS(x, ST )]

(3.15) (3.16)

These equations state that the indirect specular intensity IIS(x) at x is the integral over all possible directions  of the intensity of light IS(x, ) traveling along a specular path. The specular path intensity includes all light arriving at x from the directions of specular reflection SR and specular transmission ST . The main difficulty in computing indirect specular intensity is the double integral in Equation 3.15, since IS has a non-zero value for only a small number of locations and directions. Another difficulty is the complex geometric factors involved in these
equations that must keep track of the illumination wavefront that contains the path being
considered, and account for the change in shape of this illumination wavefront as it travels along
the specular lighting path. Basically, the intensity of light at a location x is proportional to the

Chapter 3

33

Local Illumination L
E
L(D|S)E
D|S

L D|S

Ray Tracing E
S*

L ( D | S ) S* E

Radiosity
L E
L D* E
D*
The Rendering Equation L
E
L A* E
A*

Key: D - Diffuse Surface S - Specular Surface A - Arbitrary Surface L - Light Source E - Eye Point * - zero or more

Figure 3.4: Possible light paths for local illumination, ray tracing, radiosity, and the Rendering Equation.

Chapter 3

34

S+ E

L D

L S+ D E

Key:

D - Diffuse Surface
S - Specular Surface L - Light Source E - Eye Point
* - zero or more + - One or more

Figure 3.5: Caustic paths.

Gaussian curvature of the wavefront associated with that point. For each indirect specular lighting path, the original wavefront shape from the light source must be modified to account for the distance traveled, specular reflections, and specular transmissions, until it reaches the final diffuse location. More details on this can be found in texts on classical differential geometry [Struik61], and classical geometric optics [Stavroudis72].
One method for including indirect specular lighting in a scene is to cast rays into the scene using a ray tracing method, then evaluate the indirect specular lighting contribution at each point directly. The integral in Equation 3.15 could be approximated with a sampling technique that involves spawning rays to all specular surfaces in the scene in search of paths that encounter a light source. Due to the point model used for light sources, and the high variance of indirect lighting contributions, a prohibitively high number of rays are required for a good approximation in the final image. If the scene consists of geometric surfaces, then an analytical method can be used to determine the exact caustic contributions [Mitchell92]. This computationally expensive method produces accurate results, but cannot easily be extended for use with volumetric data.

Chapter 3

35

Approaching the problem from the other direction, several methods have been explored that shoot rays from the light source into the scene. If the scene consists of polyhedral specular surfaces, backward beam tracing methods [Shinya87, Shinya89, Watt90] can be employed where, for each visible polygonal, the solid angle of reflected light is recursively traced through the scene until it arrives at a diffuse surface. If the scene contains curved specular surfaces, then backward ray tracing, also known as light ray tracing, can be used to trace light rays through the scene until they encounter a diffuse surface [Arvo86, Heckbert90, Shirley90, Chen91]. These light ray tracing methods typically do not account for wavefront shape changes in the geometric factors. Instead, a large number of rays are cast from each light source such that all rays carry the same amount of energy. The final diffuse intersection locations give samples of radiant power, with the distribution of radiant power, or intensity, estimated through a process of reconstruction.
3.7. Summary
A general equation for the intensity of light at a surface location is too complex to compute analytically. To solve this problem, several approaches have been taken. In ray tracing, only direct illumination from simple point light sources, shadowing, and specular reflection and transmission of light are allowed, thereby restricting the general intensity equation to a simpler form. In radiosity, only direct illumination, shadowing, and diffuse interactions between surface patches are allowed, again restricting the general equation to a less complex form. The opposite approach was taken in the Rendering Equation, where the general equation is sampled to estimate an intensity solution. In hybrid methods, the basic illumination methods are combined to form a multipass technique where separte components of the illumination equation are computed in each pass. Indirect lighting methods, including analytical, beam tracing, and light ray tracing techniques, have been developed to compute the indirect specular lighting in a scene. These methods have been employed in multipass solutions to produce more accurate images of specular scenes.

Chapter 4 Volumetric Ray Tracing
The volume rendering techniques presented in Chapter 2 almost all employ only a local illumination model, while the global illumination models presented in Chapter 3 are almost exclusively for classical geometric primitives. In this chapter, the illumination model and rendering techniques for volumetric ray tracing are developed, and some acceleration techniques are described.
The goal of the research presented in this chapter is to develop a useful global illumination model and an efficient rendering technique for scenes containing volumetric as well as geometric data. For this purpose, the standard ray tracing illumination equation [Whitted80, Glassner89] has been extended to allow for volume rendering effects such as compositing and maximumvalue projections. There are several motivations for this work. First, global effects can often be desirable in scientific applications. For example, by placing mirrors in the scene, a single image can show several views of an object in a natural, intuitive manner, leading to a better understanding of the three-dimensional nature of the scene. Most volume rendering techniques currently employed in visualization systems allow for only local illumination of one volumetric data set. In order to produce intuitive global effects, multiple volumetric and even geometric data sets should be supported in one global illumination model.
Another advantage to developing a global illumination model and rendering techniques for volumetric data is computational efficiency. Complex geometric models are typically created using constructive solid geometry (CSG) operations on geometric primitives and high-order functions. In volumetric ray tracing, complex surfaces are easier to render when represented volumetrically for several reasons. First, all CSG operations are precomputed, resulting in only one volumetric object, instead of multiple geometric primitives. Also, only one surface intersection routine is necessary for volumetric objects, while a different routine is typically created for each geometric primitive in classical ray tracing. Finally, a solid texture map can be precomputed for each volumetric object, thereby avoiding expensive texture mapping operations during ray tracing.
4.1. Discrete Ray Tracing
There has been some previous work in the area of developing a global illumination model based on the ray tracing technique for volumetric data. As mentioned in Chapter 2, a 3D raster ray tracing method (RRT) [Yagel92b] has been developed for isosurfaces contained within volumetric data. This algorithm has the advantage of speed, since most calculations can be performed with integer arithmetic. Also, since the scene being rendered consists of a single three-dimensional array of volume elements, or voxels, parts of the illumination equation such as surface normal, texture color, and light source visibility can be precomputed and stored with each voxel.

Chapter 4

37

Unfortunately, the speed of the RRT method obtained by precomputing parts of the illumination equation is offset by the high memory costs. If a geometric scene is converted into a 2563 grid of voxels, with 1 bit indicating the presense or absence of an object, 3 bytes for the normal, 3 bytes for the texture, and 4 bits to indicate light source visibility in a scene with two light sources, the grid would occupy over 110 megabytes! Even with this great amount of storage, accuracy is sacrificed in the final image due to the discrete nature of the normal and the ray-object intersection calculation. If the volumetric data was obtained by process of scan conversion of a geometric object, then the original geometric description of the object can be used to calculate a more accurate ray-object intersection. For other volumetric data though, the intersection is forced to occur at grid locations in the data. This produces aliasing effects that increase in severity as the level of the ray (in the recursive ray tracing tree of rays) increases.
The volumetric ray tracing technique presented in this chapter is developed to achieve several goals. First, the illumination model and rendering techniques developed should be of general use in the area of volume visualization. The RRT method supports the rendering of surfaces contained within volumetric data, but does not support volume rendering techniques such as compositing, or maximum-value projections. Also, accurate images are essential for scientific visualization, therefore continuous, rather than discrete, intersection techniques are employed in the volumetric ray tracing method.
Another goal of this volumetric ray tracing illumination model and rendering technique is practical use in a wide variety of realistic image generation applications. The RRT method works exclusively on volumetric data, and therefore is not a practical method to use when rendering a scene consisting of only a small number of simple geometric primitives. In this case, geometric ray-object intersection calculations are much more efficient in terms of memory requirements, and using simple, standard optimization techniques, are faster than the corresponding volumetric intersection calculations. For this reason, the illumination model presented in the next section encompasses the classical geometric ray tracing illumination model, and the intersection calculations presented in Section 4.3 allow for both geometric primitives and volumetric data.
4.2. The Illumination Model

The classical ray tracing illumination equation defines the intensity of light, I (x, ), at a position x, in the direction , as:

I (x, ) = Is(x, )

(4.1)

where x is the first surface intersection point visible at x from the direction . Is(x, ) is the intensity of light at this surface location. This intensity can be evaluated for a scene with M light

sources by the equation:

Is(x, ) = I AkD +

(4.2)

M
i=1

S(xi, x) A(xi, x)E(xi,

-


Li

 )k 

D

 


N




Li

 

+

k

SR

 


N




Hi

 

n

  

+

Chapter 4

38

kSR Is(xSR, SR) + kST Is(xST , ST ).



In this equation, I A is the intensity of ambient light in the scene, E(xi, -Li) is the intensity of



light emitted at xi traveling in the -Li direction, where Li is the vector from xi to x. kD, kSR, and



kST are the diffuse, specular reflection, and transmission coefficients, respectively. N is the

normal vector at x,


Hi

is

the

vector

halfway

between



and


Li,

A(xi, x)

is

the

attenuation

that

occurs due to distance along the path from xi to x, S(xi, x) is a shadowing term indicating the

fraction of light leaving the light source that arrives at x unblocked by other surface locations in

the scene, and n is the specular highlight power. Is(xSR, SR) and Is(xST , ST ) are the amount of

light arriving at x from the first intersections found in the directions of specular reflection and

specular transmission, respectively.

A standard ray tracing algorithm using Equation 4.1 can produce realistic images for a scene containing geometric objects, and can be easily extended to include objects defined by an isovalue surface within volumetric data. Since the illumination equation is evaluated only at surface locations, volumetric data cannot easily be rendered using this equation. This problem can be solved by extending Equation 4.1 to include volumetric effects:

I (x, ) = Iv(x, x, ) +  (x, x)Is(x, ).

(4.3)

In this equation, Iv(x, x, ) is the volumetric contribution to the intensity along the ray from x to x, and  (x, x) is the attenuation of Is(x, ) by any intervening volumes, along the ray from x to x. This attenuation can be expressed as:

x V

 -
 (x, x) = e

 a(xi(t))+ sc (xi(t))dt
x i=1

(4.4)

where V is the number of volumes in the scene, and  a(xi(t)) and  sc(xi(t)) are the amounts of light absorbed and scattered, respectively, in volume i at location xi(t). The summation is necessary to account for the fact that multiple volumes may overlap. In this case, the absorption
and scattering coefficients are summed up for all volumes contributing to a location. Equation 4.4 can also be used to calculate S(x, x), where  a +  sc  is replaced by kD + kSR for geometric or isosurface objects.

The volumetric component of Equation 4.3 can be based on a transport theory model of light propagation [Krueger91]:

where

 Iv(x, x, ) =

x x

V i=1

V

(

x

i

(t

),



)dt

(4.5)

V (x, ) = E(x, )

 + =4


t=to

g(x,

x(t)) sc(x, , )V (x(t), )dt

d.

(4.6)

E(x, ) is the intensity of light emitted at location x in the direction ,  sc(x, , ) is a scattering function, indicating the percentage of light arriving from direction  that is redirected into the direction . The geometric term g includes the attenuation due to distance traveled, and

any intervening volumes. Since this general equation is typically too complex and time

consuming to calculate, a low albedo approximation can be used, where the integral over

Chapter 4

39

scattering directions becomes a sum over light sources. Also, the scattering function  sc can be simplified by allowing only ideal diffuse and specular reflection.

For some applications, even this approximated equation may contain too many shading effects. For example, many volume rendering techniques use a simplified version of this equation:

x  V



 Iv(x, x) =

x

  sc(xi(t))  (xi(t), x)dt  j=1 

(4.7)

where each location in the volume is considered to be illuminated by a constant amount of

isotropic light, with no shadowing or shading occurring except along the viewing ray. For photo-

realistic rendering, the user typically wants to include the maximum amount of shading effects

that can be calculated within a given time limit, while for visualization, the user may sometimes

want to view volumetric data with no shading effects. Therefore, in a visualization system the

user should be given control over the illumination equations for both volumetric and geometric

objects, and should be allowed to specify, for each object in the scene, which shading effects

should be computed. For example, the user may place a mirror behind a volumetric object in a

scene in order to capture two views in one image, but may not want the volumetric object to cast

a shadow on the mirror.

4.3. Intersection Calculations
In classical ray tracing, the basic algorithm consists of casting a ray from the eye through a pixel, determining the first intersection along that ray, and evaluating the intensity equation at that location. For this algorithm, an intersection is simply a point where the geometric object and the line meet. In volumetric ray tracing, intersection points are still necessary for geometric and isosurface objects, but intersection segments are also necessary for volumetric data. Figure 4.1 illustrates these two different intersection types. The shaded rectangle represents volumetric data that is to be rendered using a compositing technique, while the unshaded rectangle contains an isosurface defined within the volume, which is to be shaded as a surface.
The type of data, and the way in which the data is to be viewed, determines whether the rayobject intersection calculation produces a point or a segment. In general, if the illumination is computed using Is, then an intersection point is produced, and if Iv is used, then an intersection segment is required. Is is used for geometric data and volumetric data that is viewed as, for example, an isosurface or with a maximum-value projection technique. Iv is used for volumetric data when, for example, a compositing or average value rendering method is used.
In volumetric ray tracing, there are five classes of ray-object intersections, as shown in Table 4.1. These classes are based on the type of intersection, the data type, and the information that must be passed to the shader. For intersection points, surface information such as the intersection location and the normal at that location are passed to the shader. For intersection segments, information for an entire segment of the ray, not just a single location, must be passed to the shader.

Chapter 4

40

Intersection Segment
Intersection Point View Ray
Figure 4.1: The two intersection types. The first volume encountered along the ray is rendered with a compositing technique and therefore requires an intersection segment, while the second volume, viewed as a volumetric isosurface, requires an intersection point.

Table 4.1: Ray-object intersection classes.

Class 1 2
3
4
5

Description
Geometric data
Volumetric data Early termination possible Volumetric data No early termination Volumetric data One value for segment Volumetric data Samples along segment

Type Point Point
Point
Segment
Segment

Example
Sphere Isosurface
Maximum Value Average Value Composite Value

The first class of ray-object intersections contains classical geometric data. The intersection calculations in this class require an intersection point that can be determined by analytically

Chapter 4

41

computing the closest location at which the parametric ray and the geometric object intersect. As opposed to geometric data, where the entire shape and structure of the object is typically defined in a single equation, volumetric data may contain millions of sample points to define the object. Therefore, instead of a single, analytical calculation, ray-object intersection calculations for volumetric data are computed by examining the values along the segment of the ray that passes through the volumetric data.
The second class of ray-object intersections is for volumetric data where the intersection occurs at a point, and early termination of the intersection calculation is possible. The location of the intersection point is dependent on the form of interpolation used to define values between sample points. If zero-order (nearest-neighbor) interpolation is used, then the intersection can be located by stepping voxel by voxel along the ray, where a voxel is the region of uniform value centered around a data sample. This traversal continues until the first voxel above the isosurface value is encountered, with the intersection given as the point where the ray first enters this voxel.
When higher-order interpolation methods are used, there are two techniques for determining the intersection point. First, samples could be taken along the ray at regular intervals, until the sampled value is above the isosurface value. To improve the accuracy of the intersection point, a binary search can be performed between the last sample below the isovalue and the first sample above the isovalue, until the maximum error in the calculation is below some  . Alternatively, the ray could be traversed cell by cell, where a cell is a rectangular cuboid with eight neighboring data samples as vertices. In each cell, an analytical computation can be performed to determine if the parametric ray intersects with the isosurface, as defined by the interpolation function. The advantage to using this method is that an accurate intersection point, according to the interpolation function, is computed. Unfortunately, the computational cost of this analytical calculation can become prohibitive for complex interpolation functions. This problem can be reduced by performing the full analytical solution only if some simpler test is passed. For example, if trilinear interpolation is used, the cubic equation necessary for determining the intersection point would be evaluated only if at least one of the eight vertices is above the isovalue, and at least one is below the isovalue.
In the third class, the intersection calculation is performed on volumetric data and occurs at a point, but generally cannot terminate early. For example, if a maximum-value projection is required, then the traversal cannot terminate early, since all values along the ray must be considered when determining the maximum. The result of this intersection calculation is also dependent on the interpolation function. Zero-order interpolation requires simply examining all voxels along the ray, with the intersection occurring when the ray enters the maximum-valued voxel. For higher-order interpolation, techniques similar to the two described above for isosurfaces can be used. To improve the efficiency of the analytical solution, all cells along the ray are examined, and sorted by maximum vertex value. The cells are then considered in order of decreasing maximum vertex value, with the analytical solution performed until the maximum value encounted so far is above the maximum vertex value of the next cell.
The fourth and fifth classes of ray-object intersections are for volumetric data, where the intersection does not occur at a single point, but occurs instead along a segment of the ray. For the fourth class, only one value is necessary to represent the segment, as in an average-value projection. For zero-order interpolation, determining the average value along the ray is a straight-forward calculation. Although analytical methods could be used for higher-order

Chapter 4

42

interpolation, they are almost always prohibitively expensive, since the analytical solution must be performed for every cell along the ray. Therefore, a sampling technique is used instead to determine the average value.
The fifth class is used when all values along the ray must be passed to the shader, as occurs in compositing. For the this class of intersection calculations, a sampling technique is always used. All the sampled values along the ray are passed back for the intersection segment, and the actual compositing operation is performed in the shader.
4.4. Intersection Ordering
If several intersection calculations must be performed along a single ray, these calculations should be ordered in such a way as to decrease the total amount of computation required for that ray. The first step in this ordering is to place a bounding box around each object, and compute the points at which the ray enters and exits each box. This is used to quickly determine which objects could be intersected by this ray, and, for each object, the segment of the ray on which the intersection could take place.
According to Equation 4.3, only the closest intersection point, and all intersection segments occurring closer than the closest intersection point, must be passed to the shader in order to evaluate the intensity equation. Since the closest intersection point may affect the evaluation of an intersection segment, this point must be determined before any intersection segments are processed. An example of this is shown in Figure 4.2, where an average value projection is

S1
Intersection Point
S2
Figure 4.2: Intersection ordering. The intersection point of the surface object occurs along the intersection segment S1 for the volumetric object. To obtain a correct intersection segment result, the calculation must be terminated at the intersection point, resulting in the segment S2.

Chapter 4

43

being used for the first object along the ray. If this calculation is performed first, then the average value for the entire segment of the ray, S1, would be computed. Since the closest intersection point occurs on S1, only the average value for the segment S2 should be computed. Intersection point calculations are generally much faster than intersection segment calculations, so this ordering reduces the overall computation time for a ray.
For computational efficiency, the intersection point classes should also be ordered. Consider the situation depicted in Figure 4.3a, where the first object along the ray, as determined by the bounding boxes, requires a class two intersection calculation, and the second object is class one. Even though the ray enters the bounding box of the class two object first, it is better to perform the class one calculation first, since it is generally a much faster calculation, and will reduce the segment of the ray that must be considered for the class two intersection. Similarly, if the first object requires a class three intersection calculation as shown in Figure 4.3b, early termination of this calculation may result from performing the class one calculation first. For example, consider the case where the first object along the ray is to be rendered using a maximum-value projection. The closest intersection is the maximum value only if this value occurs at a closer location than P1, otherwise the class one intersection is the closest. If the maximum value encountered along the ray up until the point P1 is v1, then the class three calculation could terminate after P1 as soon as a value v2 is encountered such that v2 > v1. According to these observations, ordering of the intersection point calculations could be obtained by sorting the objects by distance, where the distance for class one is where the ray enters the

Class Two Intersection

Class Three Intersection

v2 v1

Class One Intersection

P1
Class One Intersection

(a) (b)

Figure 4.3: Intersection ordering for two points. The ray enters the bounding box of the class one object after entering the bounding box of (a) the class two object, and (b) the class three object.

Chapter 4

44

bounding box, and the distance for classes two and three are where the ray exits the bounding box.
Pseudocode for casting a ray, R, and computing a final color, C, is given in Figure 4.4. The compute_bounding_boxes(R,P,S) routine determines all objects requiring intersection points, P, and segments, S, whose bounding boxes are intersected by this ray. The lists P and S are returned in arbitrary order, so P is sorted by the order_points(R,P) routine according to the above observations. The variable Dmin is used to keep track of the closest intersection point, and is initially set to infinity. The intersection points are processed first, followed by the intersections segments, with Dmin used to avoid, or terminate the calculation early, whenever possible. Finally, R, P, and S are passed to the shade routine to determine the final pixel color, as is described in Section 4.6.

4.5. Acceleration Techniques
This volumetric ray tracing method is general in that it supports multiple data sets of arbitrary size and orientation. Each volumetric and geometric data set in the scene contains a local coordinate system that defines the position, orientation and scaling of the object in

cast_ray ( R , C )
compute_bounding_box ( R , P , S ) order_points (R , P )
Dmin =
while ( P  Null ) D = point_intersect ( P , Dmin ) if ( D < D min) D min = D P = P->next
while ( S  Null ) segment_intersect ( S , Dmin ) S = S->next
C = shade ( R , P , S )
Figure 4.4: Intersection pseudocode for the volumetric ray tracing algorithm.

Chapter 4

45

reference to a standard world coordinate system. For volumetric data, this local coordinate system is aligned with the grid axes and is scaled so that the sample spacing is 1 along all three axes. Matrices, defined in a homogeneous coordinate system, are employed to transform points, vectors, and normals from the world coordinate system to the local coordinate system and vice versa. The time required to compute an intersection point or segment calculation for volumetric data can be reduced by performing the computation in the local coordinate system. Consider the trilinear interpolation equation that is computed, possibly many times, at each step along the ray for an intersection segment. This interpolation method requires three distance measurements computed along the three axes of the data. These measurements are simple to obtain in the local coordinate system, and computationally expensive to compute in the world coordinate system.
An acceleration technique that works well for primary rays in volumetric ray tracing is known as polygon assisted ray casting, or PARC [Avila92]. In this method, the graphics hardware present on most graphics workstations is used to speed up the intersection process. A polygonal representation of all objects in the scene is created and projected into two Z-buffers one capturing the standard ``nearest distance'' information, and the other obtaining the ``farthest distance'' for each pixel in the image. If the polygonal models are created in such a way as to completely contain the actual object, then these distances can be used to bound the segment of the ray along which an intersection can occur. Also, if no Z-buffer information was written for a pixel, then no intersection could occur for this ray.

Far Approximation

Sample

View Near Approximation

(a) (b)

Figure 4.5: (a) Brute-force ray casting and (b) the PARC algorithm for primary rays.

Chapter 4

46

An illustration of the PARC acceleration method is given in Figure 4.5, with the part of the polygonal representation captured in the near Z-buffer shown in a solid line, and the part captured in the far Z-buffer shown in a dashed line. It is obvious from this figure that many voxels in the data are skipped during the intersection calculation due to the information obtained from these Z-buffers.
The PARC algorithm can be adapted to various workstation configurations by varying the complexity, and therefore the ``fit'' of the polygonal representation. If a polygonal model could be created that exactly matched the actual surface contained in the volumetric data, then the near Z-buffer would contain the exact intersection location. Since the polygonal model is only an approximation of the surface, there are two sources of extra computation. First, the silhouette of the model will not match the silhouette of the surface, and therefore rays will be cast that will not intersect the surface. Second, the distance stored in the near Z-buffer is less than the actual intersection distance, and so several cells will have to be considered before the cell containing the intersection point is encountered. The closer the polygonal model approximates the actual surface, the less ``extra work'' needs to be done. Unfortunately, more accurate polygonal models typically contain more polygons, and therefore requires more time to render into the two Z-buffers. This technique can be adapted to a particular workstation by taking into account the relative floating point performance versus the polygonal rendering rate, and adjusting the fit of the polygonal model accordingly.
The PARC acceleration method can be extended to efficiently compute shadow ray intersections. For primary rays the eye point is fixed for perspective projection and the viewing ray direction is fixed for parallel projection, whereas for shadow rays, the end point of the rays is fixed at the light source for point light sources, and the direction of the rays is fixed for directional light sources. Near and far Z-buffers are created for each object as viewed from each light source. These buffers can be created at any resolution as long as each polygon is guaranteed to project onto at least one pixel so that no polygon is ``missed'' because it fell

Figure 4.6: (a) The PARC polygons and (b) the ray traced image for a bullfrog sympathetic ganglion.

Chapter 4

47

between pixels. Generally, higher resolution buffers should be used for objects with more surface complexity, while low resolution buffers can be used for simple objects. These buffers are view-independent and need only be recomputed if a light source or an object in the scene is modified. When a shadow ray is cast, the four pixels in each near and far Z-buffer that bound this ray are considered. For each object, intersections must be considered on the segment of the ray bounded by the minimum value found in the four pixels of the near Z-buffer, and the maximum value found in the four pixels of the far Z-buffer for that object.
The PARC acceleration method for primary rays was tested for two sample images, as shown in Figures 4.6 and 4.7, with statistical results given in tables 4.2 and 4.3. These images were each generated at a resolution of 500500 pixels on a Silicon Graphics Indigo Elan with an R3000 processor and 80 megabytes of RAM. The polygonal models used to approximate the data were created using a uniform subdivision technique where a 646464 element grid was created for each object. The polygonal model consists of the outer faces of all box-shaped elements through which an object surface passes. Figure 4.6 contains the PARC polygon approximation and the ray traced image of a volumetric isosurface of a bullfrog sympathetic ganglion data set obtained using confocal microscopy. The polygonal aprroximation is shaded in

Table 4.2: Statistical results for Figure 4.6.

Time (in seconds)
Steps (in cells)
Shaded pixels

Without PARC 123.46
16,562,055 9,617

With PARC 21.70
357,231 9,617

Rays Cast
Average steps per ray
Rays cast per shaded pixel

173,000 95.73 17.99

22,625 15.79 2.35

Chapter 4

48

this image for the purpose of illustration only; no shading of the polygonal model occurs during the PARC acceleration method.
The results of casting primary rays for the image with and without the PARC acceleration technique are given in Table 4.2. The first row of this Table indicates the total time, in seconds, required for image generation. The second row indicates the total number of steps required, where each step involves considering the eight voxel values that define a cell, with a cell-by-cell stepping technique employed to traverse the ray. The third row indicates the number of shaded pixels in the final image, which is identical for the two methods since identical images are produced. The fourth row in the table contains the number of rays cast to create the image for each method. When the PARC method is not used, a simple bounding-box test is employed to avoid casting rays that do not intersect any part of the data set. The final two rows of this table indicate the average number of steps along each ray, and the number of rays cast per shaded pixel.
Figure 4.7 contains the PARC polygon approximation and the ray traced image for a quantum mechanics simulation of a high-potential iron protein. This simulation data actually consists of two objects, with the positive wave function isosurface shown in purple, and the negative wave function value surface shown in green. The statistics for this image are given in Table 4.3.
When the Z-buffer sizes are chosen to be the same as the image size in the PARC acceleration method for shadow rays, similar speedups are obtained. Since the shadow ray originates with a polygonal model, only the far Z-buffer projection is required for this object. The shadow ray is tranversed until the distance along the ray is greater than the distances stored in the four elements of the far Z-buffer bounding this ray, or until an intersection occurs. Therefore a small number of steps are always required for each shadow ray.

Figure 4.7: (a) The PARC polygons and (b) the ray traced image for a high-potential iron protein.

Chapter 4

49

Table 4.3: Statistical results for Figure 4.7.

Time (in seconds)
Steps (in cells)
Shaded pixels

Without PARC 145.47
17,110,260 31,307

With PARC 46.80
1,055,700 31,307

Rays Cast
Average steps per ray
Rays cast per shaded pixel

334,846 51.10 10.69

71,225 14.82 2.27

4.6. Illumination Evaluation
Once the intersection process is complete, the resulting information is passed to the shade routine to determine the final pixel color. Figure 4.8 shows a pictorial representation of the information passed to the shader after the intersection calculations are complete. The first object along the ray, A, requires a class five intersection calculation, therefore, samples along the ray are passed to the shader for the segment of the ray passing through the object. The second object, B, requires a class four intersection calculation, so one value is passed to the shader for the segment. The third object, C, requires a class two intersection calculation, so information for a single intersection point is passed to the shader.
The sampling along a ray for a class five intersection segment occurs during the segment_intersect routine, while shading of these samples occurs in the shade routine. If samples are obtained at uniform locations defined by a stepping distance along the ray, then samples are obtained at corresponding locations for overlapping volumetric objects. This simplifies the shading process, but may result in artifacts in the final image. A small offset can be randomly selected for each ray, and added to the sample locations along the ray. This technique will hide the artifacts and will produce an image that is more visually pleasing, but not

Chapter 4

50

A
(Composite)

B
(Average)

C
(Isosurface)

A B C

Figure 4.8: An example of the information passed to the shader by the intersection routine.

more accurate than the original uniform sampling method. More accuracy can be obtained by varying the stepping size according to the local gradient magnitude, allowing more samples in regions of high magnitude. The shader is then required to resample the ray for segments passing through overlapping volumes. This produces more accurate images at the cost of higher intersection and shading times.
Since ray tracing is a recursive algorithm, this may result in further calls to the cast_ray routine. The standard ray tracing model has been extended to include many user-controlled, nonrealistic effects, therefore care must be taken when casting secondary rays from class three intersection points. For example, if all shading is eliminated for an object rendered with a maximum-value projection, it would not be possible to see other objects through this object. Typically, for maximum-value projections all shading effects except transmission are turned off, with the index of refraction set to 1.0, and the transmission coefficient set to 1. 0 -  , where  is the opacity of the maximum value. If a transmission ray is naively cast from the maximum-value intersection, another maximum-value intersection will occur within the same object. To avoid this problem, the object rendered with the maximum-value projection is turned ``off'', so that the transmission ray cannot perform any intersection calculations with it. Once the ray intersects another object, the first object must be turned ``on'' again, so that shadowing, and reflection calculations are correctly performed.

Chapter 4

51

4.7. Results
The volumetric ray tracer was implemented in C within the VolVis volume visualization system [Sobierajski94, Avila94a, Kaufman93, Avila92]. For a complete description of the VolVis system, see Appendix A. In this section, a description is given for several ray traced images that illustrate the motivations behind this work. All times are given in seconds for a Silicon Graphics Indigo Elan workstation, with an R3000 processor and 80Mb of RAM.
Figure 4.9 shows a quantum mechanics simulation of a high-potential iron protein. The positive and negative wave function values were separated into two data sets, each containing 646464 voxels. In this image, an isosurface in the positive values, volume rendering of the negative values, and two light sources were used to generate the final image. A transport theory model was employed to render the negative wave function values. The isosurface in the positive values is a specular surface, and therefore reflections of both the isosurface itself, and the volume rendered negative values can be seen on the isosurface. This image was computed at a resolution of 500500 pixels in 359.23 seconds.

Figure 4.9: A volumetric ray traced image of a quantum mechanics simulation of a highpotential iron protein.

Chapter 4

52

Figure 4.10: A volumetric ray traced image of a hippocampal pyramidal cell using a maximumvalue projection method.
Figure 4.10 shown a volumetric ray traced image of a hippocampal pyramidal cell that was rendered with a maximum-value projection method. The cell was obtained using confocal microscopy, and contains 384256200 voxels, while the polygon floor was obtained through a volume-sampling method [Wang94] and contains 595913 voxels. A light source was placed above the cell in order to cast a shadow on the polygonal floor. This image illustrates the benefits of including global shading effects in visualization images, since depth information for the cell that was lost in the maximum-value projection is added back into the image through the use of the shadow. This image was generated at a resolution of 500500 pixels in 318.69 seconds.
Figure 4.11 shows two ray traced images of the surface: x4 + y4 + z4 - x2 - y2 - z2 = 0
with the image on the left computed using an analytical intersection method, and the image on the right computed using a voxelized version of this function. A simple voxelization technique was employed where the scalar value at each location is proportional to the function value. Although this voxelization method is fast, better results can be obtained using volume-sampling techniques. Also, the voxelized model was computed at a resolution of 100100100 voxels. Smoother results can be obtained using a larger grid of voxels. Two difference images for the images in Figure 4.11 are given in Figure 4.12. The image on the left in Figure 4.12 indicates intersection location differences while the image on the right indicates final pixel color differences. Grey scale values are used to indicate the magnitude of the difference where dark values indicate low differences and light values indicate high differences. Although many pixels

Chapter 4

53

Figure 4.11: A volumetric ray traced image of a fourth order (a) geometric and (b) voxelized function.

Figure 4.12: The difference in (a) the intersection location and (b) the final color, obtained when comparing an analytical solution to a volumetric solution.
have slight distance differences, most pixels are identical in final color value. The differences in these images are mostly due to the volumetric approximation of the data, although the analytical intersection routine, implemented by constructing an upper Hessenberg matrix whose eigenvalues are the roots of the intersection equation [Press92], also contains some error. Statistics for these two difference images are given in Tables 4.4 and 4.5, where the pixel percentages are computed by considering only those pixels that are shaded in at least on of the images in Figure 4.12. The distance differences are given in units, where the object occupies a region of approximately 100100100 units. The color differences are computed using the

Chapter 4

54

Table 4.4: Statistical results for the intersection difference image.

Distance Difference Pixels with this difference

(in units)

(including only shaded pixels)

< 0.01

73.7%

0.01 - 0.02

13.1%

0.02 - 0.03

7.3%

0.03 - 0.04

3.8%

0.04 - 0.05

1.5%

> 0.05

0.6%

absolute value of R1 - R2 + G1 - G2 + B1 - B2 where each (R, G, B) triple represents the pixel color with each component ranging between 0 and 255.
The efficiency of rendering voxelized geometric surfaces compared to employing an analytical solution is illustrated in Table 4.6. All voxelized objects were computed on a 100100100 grid of voxels, an a uniform subdivision of this grid to 323232 elements was performed to obtain the polygonal aprroximations employed by the PARC acceleration method. Each image was generated at a resultion of 500500 pixels. Note that the time to compute the analytical solution increases as the order of the polynomial function increases, while both the accelerated and unaccelerated voxelized solutions remain relatively constant. Although more efficient analytical solutions may be employed, the complexity of the solution will still grow with the order of the polynomial.
In Figure 4.13 geometric descriptions were used to voxelize three objects using a volume sampling method [Wang94, Wang93]. The polygon was created from a geometric polygon description, while the nut and bolt were generated by voxelizing several simple geometric primitives, then applying voxel-based CSG operations to create the final models. The polygon has a resolution of 595913 voxels, the nut contains 684159 voxels, and the bolt contains 7012366 voxels. Two light sources were included in the scene, and the final image required 133.89 seconds to compute at a resolution of 400400 pixels. The texture for the bolt was precomputed and stored in a solid texture map with a resolution of 100100100 elements. This solid texture map was consulted to determine the color at intersection points for the bolt.

Chapter 4

55

Table 4.5: Statistical results for the pixel color differences obtained when comparing an analytical solution to a volumetric solution.

Color Difference
0 1 2 3 4 >4

Pixels with this difference (including only shaded pixels)
80.3% 15.0% 3.4% 0.7% 0.1% 0.5%

Table 4.6: Statistical results for rendering polynomial functions.

Function

Analytical Representation

Voxel Representation
Without PARC With PARC

x4 + y4 + z4 x2 - y2 - z2

410.75

93.48

44.21

x6 + y6 + z6 x4 - y4 - z4

649.90

93.21

45.81

x8 + y8 + z8 x6 - y6 - z6

953.49

95.96

44.34

x10+ y10 + z10 x8 - y8 - z8

1239.00

94.81

47.73

Chapter 4

56

Figure 4.13: A volumetric ray traced image of three volume sampled geometric primitives.
4.8. Summary
The volumetric ray tracing method presented in this chapter can be used to generate realistic images of scenes containing volumetric as well as geometric objects. Global effects such as shadowing, specular reflection and transmission of light, and also volumetric effects such as compositing can be captured using this method. This is accomplished by extending the classical ray tracing illumination equation to account for volumetric contributions along ray segments. This volumetric ray tracing method has been employed by a visualization system called VolVis in order to provide intuitive cues in an image that lead to a greater understanding of the threedimensional nature of a scene. This method has also been used to efficiently render complex geometric models through the use of voxelization.

Chapter 5 Volumetric Radiosity
The radiosity method, based on radiative heat transfer in an enclosure, allows for efficient generation of images where the scene consists of surface patches, and each patch is a uniform diffuse emitter and reflector of light. As presented in Chapter 3, many extensions have been made to the classical radiosity algorithm to improve realism or increase computational efficiency. The first methods for reducing computational costs were based on a technique known as substructuring [Cohen86, Campbell90, Heckbert90], where adaptive techniques are employed to subdivide large patches into smaller elements. Hierarchical techniques have been developed [Hanrahan90, Hanrahan91, Gortler93] in order to reduce the complexity of the radiosity of N patches from O(N 2) to O(N ) by building a hierarchy of patch resolutions, and allowing patches to interact at the highest level possible while still maintaining a certain error bound.
In this chapter the concept of volumetric radiosity is introduced. This method contains not only the basic surface patch elements of classical radiosity, but also volume elements, or voxels, used to capture volumetric effects in a scene. Absorption, and diffuse emission and reflection of light by a patch are allowed, as in classical radiosity. Absorption, and diffuse and isotropic emission and scattering of light by a voxel are also allowed in this method.
In the zonal method [Rushmeier87, Hottel67], radiatively participating media (called volumes) are used to augment the geometric scene, and isotropic emission and scattering of light by the media are allowed. Such media might include flames, clouds, dust, smoke and fog as light emitters, scatterers, and absorbers embedded in a classical geometric scene of opaque surfaces.
In reality, light passing through a volume is actually scattered, reflected and absorbed according to a complex bidirectional reflectance/scattering function defined over 4 steradians. However, in order to utilize the radiosity algorithm, a simplification must be made to this bidirectional function. In the zonal method, the bidirectional function is essentially eliminated by only allowing light to be scattered and absorbed uniformly in all directions. This results in a set of simultaneous equations for a scene with one radiosity value for each surface or volume element. The volumetric radiosity method presented in this chapter is designed with the ability to render scenes that may consist solely of volumetric data, therefore diffuse emission and reflection of light by the volume is essential for creating the appearance of surfaces within the volumetric data. The bidirectional function becomes a linear combination of uniform absorption and scattering, and Lambertian reflection of light. This results in two radiosity equations for each volume element in the scene.
This radiosity approach is suitable for rendering volumetric data, such as that obtained by scanning, simulation, or voxelization. The data can be given a translucent volumetric appearance by allowing the uniform absorption and scattering of light to dominate the bidirectional function, or surfaces within the data can be highlighted by increasing the contribution of the Lambertian reflection of light.

Chapter 5

58

Since the number of voxels in a volume is typically large, the memory requirements and complexity of brute-force radiosity are prohibitive in volumetric radiosity. Consequently, a hierarchical iterative shooting method is developed for this algorithm to reduce the memory requirements and improve the computational efficiency by reducing the number of interactions required to compute the radiosity solution. An analysis of this method is given showing that theoretically the hierarchical technique could reduce the complexity of the volumetric radiosity algorithm, and the difficulties in obtaining this theoretical reduction in complexity are described.
The development of the simultaneous radiosity equations used in the volumetric radiosity method is given in the next three sections of this chapter. It is not possible to extensively cover all of the concepts introduced in these sections. More information on these topics can be found in radiative transfer texts, such as [Hottel67, Love68, Siegel72, Sparrow78].

5.1. Intensity Variation

Consider a beam of infinitesimal cross-sectional area with intensity I traveling an infinitesimal distance dl in the direction  through an emitting, absorbing, and scattering medium. After this beam has traveled the distance dl, it has lost some intensity in the amount of
K a I dl due to absorption. K a is the absorption coefficient, measured in units of inverse length,
and indicates the fraction of intensity absorbed per unit distance along a beam. The beam also
has lost intensity in the amount of K s I dl due to out-scattering. K s is the scattering coefficient,
and indicates the fraction of intensity scattered per unit distance along a beam.

As the beam travels the distance dl, it gains intensity due to emission in the amount of
K a E dl, where E is the surface emission rate of the medium. Also, the beam gains intensity due
to in-scattering. To determine how much the intensity is augmented due to in-scattering, all
intensity arriving at this location along the beam must be considered. The phase function, or scattering function (, ) gives the amount of intensity coming from the  direction that is scattered into the direction , divided by the amount that would be scattered into  if the scattering were isotropic. For isotropic scattering, the phase function (, ) = 1. By conservation of energy:

1 (, )d = 1
4
=4

(5.1)

for any phase function. The amount by which the intensity along the beam is augmented due to in-scattering is:

K s I ()(, )d dl
4
=4

(5.2)

where I () is the intensity of light arriving from the  direction. An equation for the change in the intensity of the beam as it travels in the  direction is:

dI = - K a I - K s I + K a E + K s
dl 4

I ()(, )d

=4

(5.3)

Chapter 5

59

As noted, the intensity is diminished by absorption and out-scattering, and is augmented by emission and in-scattering. The amount of intensity lost due to absorption and out-scattered is called attenuation, and can be expressed as:

 x2
- [K a(x)+K s(x)]dx
I = I0 e x1

(5.4)

= I0  (x1, x2)

where  (x1, x2) is the transmittance term, indicating the fraction of the initial intensity remaining after a beam travels from x1 to x2. The augmentation of the intensity due to emission and inscattering is captured in the radiosity equation for the volume:

 4Kt

V

Bv

=

4Ka V

E

+

4V K s 4

I ()(, )d d

=4 =4

(5.5)

where K t = K a + K s, and V is the volume of the region. The coefficient K t is known as the

extinction coefficient. This equation represents the total amount of energy emitted or scattered by

this volume, and includes the energy emitted directly by this volume, and the energy arriving

from all other directions that is scattered by this volume.

5.2. Radiosity Equations

For this algorithm, a scene consists of a set of volumes, where each volume is a three dimensional array of data samples. Each data sample represents some property or properties of the volume at that location. Data samples occur at regularly spaced intervals along each of the three axes, with each axis having an independent sample spacing constant. Sources of this sort of volumetric data include sampled data (e.g., MRI, CT, Confocal Microscopy), simulation results, and voxelized geometric data. A volume can be partitioned into uniform subvolumes by using a nearest data sample or interpolating function. These subvolumes are called ``voxels'', and are one of the basic elements of the volumetric radiosity algorithm.

Each voxel can emit, absorb, scatter, and transmit light. Both isotropic and diffuse emission of light are allowed, where ``isotropic'' implies directional independence, and ``diffuse'' implies Lambertian (i.e., dependent on normal, or gradient) emission over 2 steradians. Light can also be scattered isotropically or diffusely by a voxel. Light entering a voxel that is not absorbed or scattered by the voxel is transmitted unchanged. The values for emission, absorption, and scattering are constant within each voxel.

Consider Equation 5.5 for a scene in which all emission, absorption, and scattering of light
are isotropic. Since the scene has been discretized into a finite number of voxels, the integral of  over 4 steradians can be replaced by a sum over all voxels. The inner integral of  over 4 steradians can be replaced by the fraction of radiosity leaving each voxel that arrives at this
location. The radiosity of voxel i in a scene with Nv voxels can be expressed as:

Bvi

=

K ai K ti

Ei +

K si K ti

Nv
Bv j Fij
j=0

(5.6)

Since all emission, absorption, and scattering are isotropic, the phase function  can be replaced

Chapter 5

60

by a simple form factor Fij. This form factor is based on simple geometric properties such as the distance between the two voxels, and is discussed in detail in Section 5.3.

If all emission, absorption, and scattering of light are diffuse instead of isotropic Equation
5.6 is still valid, with the form factor Fij depending also on the orientation of the two voxels. If both diffuse and isotropic emission, absorption, and scattering are allowed, then Equation 5.6 is
not sufficient, since the radiosity of a voxel can not be captured in a single value. Instead, each
voxel vi has two radiosity values: BDi is the diffuse radiosity of voxel vi, and BIi is the isotropic radiosity of vi. These radiosities can be expressed as:

BDi =

K ai K ti

E Di +

K sdi K ti

Nv j=0

 

F

DDij

BD j + F DIij

B

I

j

 

(5.7a)

BIi =

K ai K ti

EIi +

K sii K ti

Nv j=0

 

F

IDij

BD j

+

F IIij

B

I

j

 

(5.7b)

where K sdi + K sii = K si.
The diffuse radiosity of vi is the amount of light emitted diffusely from this voxel, plus the amount of light arriving at vi from all Nv voxels in the scene scaled by the diffuse scattering coefficient of vi, K sdi. Light can arrive at vi either diffusely, BD, or isotropically, BI, so two form factors are necessary. The form factor F DDij indicates the fraction of light diffusely emitted or scattered by v j that arrives at vi while the form factor F DIij indicates the fraction of light isotropically emitted or scattered by v j that arrives at vi. These form factors are described in more detail in the next section.

Equation 5.7b is similar to Equation 5.7a. The isotropic radiosity of a voxel vi is the amount of light emitted isotropically from this voxel, plus the amount of light arriving at vi from all Nv voxels in the scene scaled by the isotropic scattering coefficient of vi, K sii. Again, light can arrive at vi either diffusely or isotropically, with the form factor F IDij indicating the fraction of diffuse radiosity from v j arriving at vi, and F IIij indicates the amount of isotropic radiosity from v j that arrives at vi.
According to the above equations, E D, E I, K a, K sd, and K si must be defined for each voxel. For voxelized data, this information can be added directly to each voxel during the voxelization process. For sampled data and simulation results, transfer functions similar to those used for standard volume rendering can be employed to map scalar value, or scalar value and magnitude of the gradient, to E D, E I, K a, K sd, and K si.

5.3. Form Factors
The form factors in this radiosity method indicate the fraction of the radiosity leaving voxel v j that arrives at voxel vi. As shown in Figure 5.1, both the diffuse and isotropic radiosities of voxel vi, and the diffuse and isotropic scattering of voxel v j must be considered when examining the interaction between the two voxels vi and v j. In this section, these form factors are derived by examining the energy exchange between these two voxels. Form factors are dimensionless, and can never have a negative value.

Chapter 5

61

dvi

r

dvi

Nj j r

dvi

Nj j r

i Ni

dvj dvj dvj
(a) (b) (c)
Figure 5.1: Light traveling between two voxels. (a) Isotropic emission and isotropic scattering. (b) Diffuse emission and isotropic scattering. (c) Diffuse emission and diffuse reflection.

The amount of light isotropically emitted or scattered by a differential voxel dv j that is incident on another differential voxel dvi for isotropic scattering, as shown in Figure 5.2, is:

d2 Pij

=

 (xi, x j)

K ti

Ktj  r2

BI j

dVi dA j dx j

=

 (xi, x j)

Kti Kt j  r2

BI j

dVi dV j

(5.8)

where r is the distance between vi and v j, xi and x j are the locations of vi and v j, and Vi and V j are the volumes of vi, and v j, respectively. Since light is arriving from an isotropic source, and is scattered uniformly over 4 steradians, Equation 5.8 is independent of angular direction.

The amount of isotropic radiosity leaving some voxel v j that is incident on a differential voxel dvi can be found by integrating Equation 5.8 over v j:

dPij =

 (xi, x j) K ti K t j BI j dVi dV j Vj  r2

(5.9)

Integrating over Vi gives the amount of isotropic radiosity leaving v j that arrives at vi:

 Pij =

Vi

 (xi, x j) K ti K t j BI j dVi dV j Vj  r2

(5.10)

Chapter 5

62

xj dVj
dA dx
r Vj

xi dVi

Vi

Figure 5.2: Isotropic radiosity from v j arriving at vi for isotropic scattering.

The form factor F IIij is the fraction of the isotropic radiosity leaving v j that arrives at vi per unit volume for isotropic scattering. Since the isotropic radiosity of v j and the extinction coefficient of vi have already been included in Equation 5.7b, the form factor F IIij is:

 F IIij

=

Pij = Vi Kti BI j

1 Vi

Vi

 (xi, x j) K t j dVi dV j Vj  r2

(5.11)

Next, consider the amount of light diffusely emitted or scattered by dv j that is incident on dvi for isotropic scattering. In this case:

 Pij =

Vi

H ( j)  (xi, x j) K ti K t j BD j cos  j dVi dV j Vj  r2

(5.12)

where  j is the angle between the normal N j and the line segment connecting xi and x j, and H( j) is a binary term that evaluates to 1 if cos  j  0, otherwise it evaluates to 0. This term is used to account for the fact that vi only emits or scatters light over the 2 steradians around the normal N j. Again, the form factor F IDij is for a unit volume of vi, and the coefficient K ti and the radiosity BI j have already been included in Equation 5.7b. The form factor F IDij is:

 F IDij

=

Pij = Vi Kti BD j

1 Vi

Vi

H ( j)  (xi, x j) K t j cos  j dVi dV j Vj  r2

(5.13)

Similarly, the form factor F DI is:

Chapter 5

63

 F DIij

=

Pij = Vi Kti BI j

1 Vi

Vi

H ( i)  (xi, x j) K t j cos  i dVi dV j Vj  r2

(5.14)

Finally, consider the amount of light diffusely emitted or scattered by dv j that is incident on dvi for diffuse scattering. In this case:

 Pij =

Vi

H ( i) H ( j)  (xi, x j) K ti K t j BD j cos  i cos  j dVi dV j Vj  r2

(5.15)

and the form factor F DDij is:

F DDij

=

Pij Vi Kti BD j

 = 1

H ( i) H ( j)  (xi, x j) K t j cos  i cos  j dVi dV j

Vi Vi V j

 r2

(5.16)

The double integral found in these four form factors makes it prohibitively expensive to analytically compute a solution. Instead, the form factors could be approximated using a ray casting technique [Malley88, Ward88, Wallace89, Sillion89]. Also, if the voxels are small enough, then the form factor between vi and v j can be approximated by the differential form factor between the centers of these voxels. If this method is used, then a modified hemicube technique can be employed to accelerate the form factor calculations [Rushmeier87].

5.4. Surface Elements

So far this volumetric radiosity method has been based only on the voxel primitive. In this section the radiosity equations are extended to incorporate surface elements, called surface patches. These patches, which are the basic element of classical radiosity, can be used to represent geometric primitives, as well as isosurfaces contained within volumetric data. Each patch can emit and reflect light diffusely, with ESi being the rate of emission, and i being the dimensionless diffuse reflectance coefficient of patch si.
With this extension, there are now three sources of incident radiosity for any voxel or patch in the scene: the isotropic radiosity of a voxel, the diffuse radiosity of a voxel, and the diffuse radiosity of a patch. The radiosity equations for a scene with Nv voxels and Ns surface patches then becomes:

 BDi =

K ai K ti

E Di +

K sdi K ti

  

Nv j=0

 

F

DDij

BD j

+

F DIij

BI

 j

+

Ns k=0

 

F

DSik

BSk 

(5.17a)

 BIi =

K ai K ti

EIi +

K sii K ti

  

Nv j=0

 

F

IDij

BD j

+

F IIij

BI

j

 

+

Ns k=0

 

F

ISik

BSk 

(5.17b)

 BSi

=

E Si

+

 i


Nv j=0

F

SDij

BD j

+

F SIij

B

I

j

 

+

Ns k=0

 

F

SS

ik

BSk 

(5.17c)

Chapter 5

64

Five new form factors have been introduced in Equations 5.17a-c: F SS, F DS, F SD, F IS, and
F SI. The exchange of light between two surface patches, si with area Ai, and s j with area A j, as found in classical radiosity, is:

 Pij =

Ai

H ( i) H ( j)  (xi, x j) BS j cos  i cos  j dAi dA j Aj  r2

(5.18)

and the form factor is:

 F SSij =

Pij Ai BS j

=

1 Ai

Ai

H ( i) H ( j)  (xi, x j) cos  i cos  j dAi dA j Aj  r2

(5.19)

If surface patches are considered opaque, then if the line segment from xi to x j intersects any other surface patches,  (xi, x j) = 0. Alternatively, each surface patch can be given a transmittance coefficient,  , that indicates the fraction of light transmitted unchanged by this patch, where  +   1. If the intensity of light is I0 before intersection, the intensity of light is  I0 after passing through the patch.
Now consider the light arriving from patch s j incident on voxel vi for diffuse scattering, as shown in Figure 5.3. The energy exchange can be written as:

xj dAj

i r j

Aj

xi dVi

Vi

Nj

Figure 5.3: Diffuse radiosity from a surface patch s j arriving at a voxel vi for diffuse scattering.

Chapter 5

65

 Pij =

Vi

H ( i) H ( j)  (xi, x j) K ti BS j cos  i cos  j dVi dA j Aj  r2

and the form factor is:

(5.20)

 F DSij

=

Pij Kti Vi

= BS j

1 Vi

Vi

H ( i) H ( j)  (xi, x j) cos  i cos  j dVi dA j Aj  r2

Similarly, the form factor F SDij is:

F SDij

=

Pij Ai BD j

 = 1 H ( i) H ( j)  (xi, x j) K t j cos  i cos  j dAi dV j

Ai Ai V j

 r2

(5.21) (5.22)

Finally, consider the light arriving from patch s j incident on voxel vi for isotropic scattering. The energy exchange can be written as:

and the form factor is:

 Pij =

Vi

H ( j)  (xi, x j) K ti BS j cos  j dVi dA j Aj  r2

(5.23)

 F ISij

=

Pij Kti Vi

= BS j

1 Vi

Vi

H ( j)  (xi, x j) cos  j dVi dA j Aj  r2

Similarly, the form factor F SIij is

 F SIij

=

Pij Ai BI j

=

1 Ai

H ( i)  (xi, x j) cos  i dAi dV j

Ai V j

 r2

(5.24) (5.25)

5.5. Radiosity Evaluation
Now that the radiosity equations and form factor equations have been defined, a standard matrix solver could be used to determine the final voxel radiosities. One major drawback to this straightforward solution is the high number of interactions required, and hence the high storage requirements and long computation times. Even for a ``small'' scene with Nv = 643 voxels and Ns = 642 patches there are 4Nv2 + 2Nv Ns + Ns2 = 277, 042, 167, 808 form factors, requiring over a terabyte of storage at 4 bytes per form factor! Even if such storage requirements weren't prohibitive, the amount of time necessary to calculate that many form factors would be.
Alternatively, an iterative shooting technique [Cohen88], can be used to solve for the radiosities. For each voxel vi four values BDi, BIi , BDi, BIi are stored, and for each patch s j two values BS j and BS j are stored. BDi and BIi are the current estimates of the total diffuse and isotropic radiosity values, respectively, for vi. BDi and BIi indicate how much diffuse and isotropic radiosity, respectively, has been accumulated by vi since it was last processed. Similarly, BS j indicates the current estimate of the radiosity of patch s j, and BS j indicates the accumulated radiosity since this patch was last processed. Initially, BDi and BDi are set to E Di, BIi and BIi are set to E Ii, and BS j and BS j are set to E S j. An element is processed by shooting its accumulated radiosity to all other elements in the scene, then setting the accumulated

Chapter 5

66

radiosity to 0. This process of shooting continues until all accumulated radiosities are below some tolerance. This iterative algorithm is illustrated in Figure 5.4.
Unfortunately, although this iterative process reduces the amount of storage required, it actually increases the number of form factors calculated, since many passes (typically over 2Nv + Ns) through the shooting iteration are required to converge on a solution. There are two methods for reducing the time required to solve for the scene radiosities. First, the amount of time required to calculate one interaction between two elements in a scene can be reduced. Essentially this requires reducing the time spent computing the form factor. This can be done using the modified hemicube technique employed in the zonal method for participating media [Rushmeier87].

Voxel Element

Initialization

Set all BD and BD to ED

Patch Element

Set all BI and BI to EI

Set all BS and BS to ES

Shooting Iteration

Examine all elements to
efind MAX containing BMAX
No BMAX <  ? Yes
For all elements ei e eshoot( MAX, i )
Set BMAX to 0

END Figure 5.4: The basic iterative shooting algorithm for volumetric radiosity.

Chapter 5

67

level i (top level)

level i-1

level i-2

Figure 5.5: A hierarchical model for a geometric object.

The other method for decreasing the total computation time is to reduce the number of interactions. One reason for the high number of elements (voxels and patches), and hence the high number of interactions, is that in the basic algorithm each element is assumed to emits, absorb, reflect, and scatter light uniformly over its volume or area. This can be extended to allow higher-order functions to represent the interaction of light [Troutman93, Zatz93], but at the cost of higher computation time per interaction. Another solution is to note that the form factor between patch si and patch s j is similar to the form factor between si and sk if sk is a neighbor of s j with similar surface characteristics, and the distance between si and s j is large compared to their areas. In this case, one interaction between si and a combined patch s jk can be used to approximate the two interactions. Such hierarchical techniques have been employed for classical radiosity [Hanrahan91, Lischinski93], and have been combined with higher-order element approaches to form the theory of wavelet radiosity [Gortler93].
Hierarchical methods can be employed to reduce the complexity of this volumetric algorithm as they did for the classical version. In a preprocessing step, a hierarchical model is built for each object in the scene. In general, there are three types of objects in the scene: geometric objects approximated with patches, volumetric objects composed of voxels, and volumetric isosurfaces approximated with patches. For geometric objects, the hierarchy can be built by approximating the geometric object with large patches at the top level of the hierarchy, and building lower levels by subdividing to form child patches. Alternatively, if the model is composed of many small patches, these can be combined to form parent patches at higher levels in the hierarchy. An example of a geometric object hierarchy is given in Figure 5.5. Each patch in the lowest level of the hierarchy stores a value for BS, BS, and , as well as NEW _BS, which contains the radiosity accumulated by this patch during one iteration. For each higher level the

Chapter 5
Volume

Surface

68

(a) (b)
Figure 5.6: Volume and surface elements. (a) One data sample for a participating volume represents a voxel that emits, absorbs, and scatters light uniformly, and (b) for a surface, eight data samples represent a patch that emits, absorbs, and reflects light uniformly.

level i (top level)

level i-1

level i-2

Figure 5.7: A hierarchical model for a volumetric object.

values of BS and  of the child patches are averaged to form the parent patch value, and a NEW _BS value is also stored. These values are updated during the shooting iteration that is described in detail later in this section.

Chapter 5

69

For volumetric data the original data resolution is typically used as the resolution of the lowest level in the hierarchy, although lower levels can be created using subdivision techniques. For volumetric data that is treated as a volume, each voxel in the level of the hierarchy corresponding to the original data resolution represents the small, uniform volume surrounding each data sample, as shown in Figure 5.6a. Each voxel on the lowest level of the hierarchy contains values for the current radiosity estimates BD and BI, the radiosity accumulated since this voxel was processed BD and BI, the radiosity accumulated during one iteration NEW _BD and NEW _BI, the coefficients K sd, K si, and K a, and the emission values E D and E I. For volumetric data that is treated as a volume, it is not necessary to store the actual scalar value, since the coefficients and emission values completely describe the behavior of the volume. All values stored at the lowest level except the current radiosity estimates are stored at the higher levels in the hierarchy, and are initialized to be the average of the child values. This is basically a standard pyramid representation, and is illustrated in Figure 5.7.
The hierarchical model for volumetric data that contains an isosurface is similar to the one for a participating volume. For an isosurface, it is necessary to store the scalar value for each element on all levels of the hierarchy, since these values define the patches, as shown in Figure 5.6b. Also, the values for BS at the lowest level, and BS, NEW _BS, ES and  at all levels, are associated with each patch defined by eight scalar values, not with the scalar value itself.
The modified iterative algorithm for computing scene radiosities using these hierarchical models is presented in Figure 5.8. In this algorithm, the object hierarchies are built during the initialization phase, and the iterative shooting process attempts to interact elements at the top level of the hierarchies using the shoot routine as shown in pseudocode in Figure 5.9. The shoot routine performs the interaction between the elements ei and e j if this interaction is at the appropriate level according to good_interaction. Otherwise, one or both of the interactions is pushed to a lower level, as determined by the routines good_level and better_level. These routines are shown in pseudocode in Figure 5.10.
Two elements are allowed to interact if they are both at the lowest level of an object hierarchy, or if they are independently at good levels, and the amount of radiosity shot from e j to ei is less than some 1. An element is considered to be at a good level if it is at the lowest level in an object hierarchy, or the approximation error associated with this element is less than 2. This approximation error calculation depends on the type of the object to which this element belongs. For a volume object, the approximation error is based upon the error in the averaged normal, radiosity coefficients, emission values, and B values. Ideally, each voxel at level l of the hierarchy would closely approximate the values of its eight children at level l - 1. Since there may be discontinuities in the volumetric data (i.e., sharp edges or high-gradient areas), a voxel at level l may not be a good approximation of its children at level l - 1, and therefore all interactions involving this voxel should be forced down to level l - 1.
Similar criterion are used to determine the error associated with a surface patch. For surface patches belonging to a volumetric isosurface, surface shape information is also used to determine approximation error. For example, Figure 5.11 shows an example of low surface shape error, while in Figure 5.12 the approximation at level i has a high surface shape error value since two disjoint patches at level i - 1 become one connected patch at level i. An estimate of this error, as well as an estimate of the patch area necessary for form factor calculations, can be obtained using a method based on the Marching Cubes technique [Lorensen87]. In this method,

Chapter 5

70

Voxel Element

Initialization

Set all BD and BD to ED

Patch Element

Set all BI and BI to EI
Build all volume object hierarchies

Set all BS and BS to ES
Build all surface object hierarchies

Shooting Iteration

Examine all elements on the top level of each hierarchy to
efind MAX containing BMAX

No BMAX <  ? Yes

For all elements ei on
the top level of a hierarchy
e eshoot( MAX, i )
Set BMAX to 0
For all hierarchies hi fix_hierarchy( hi)
END
Figure 5.8: The hierarchical iterative shooting algorithm for volumetric radiosity.

Chapter 5
shoot( e j, ei ) {
if ( good_interaction( e j, ei ) ) interact( e j, ei )
else if ( ( NOT good_level( ei ) ) AND ( NOT good_level( e j ) ) ) for all children eq of e j for all children e p of ei shoot( eq, e p )
else if ( better_level( e j, ei ) ) for all children e p of ei shoot( e j, e p );
else if ( better_level( ei, e j ) ) for all children eq of e j shoot( eq, ei )
}
Figure 5.9: The shoot routine.

71

Chapter 5

72

good_interaction( e j, ei ) {
if ( level( e j) = 0 and level( ei) = 0 ) return TRUE else if ( total_radiosity( e j, ei ) <  1 and
good_level( ei ) and good_level( e j ) ) return TRUE else return FALSE; }
good_level( ei ) {
if ( level( ei) = 0 ) return TRUE else if ( approximation_error( ei) <  sub 2 ) return TRUE else return FALSE }
better_level( ei, e j ) {
if ( level( ei) = 0 ) return TRUE else if ( level( e j) = 0 ) return FALSE else if ( approximation_error( ei) < approximation_error( e j) ) return TRUE else return FALSE }
Figure 5.10: Three routines for determining interaction level. These are the good_interaction,
good_level and better_level routines.

each vertex in a cell is considered to be a binary value indicating if this vertex is above or below the isosurface value, resulting in 256 possible surface configurations. The surface configuration of a cell at level l and its eight children at level l - 1 are examined, and the approximation error is set high if there is a discrepancy in, for example, the number of distinct surfaces represented.
The total amount of radiosity accumulated at an element ei at level 0 in the hierarchy is the amount of radiosity accumulated directly by this element, plus all the radiosity accumulated by the ancestors of ei in the hierarchy. The fix_hierarchy routine updates the radiosity estimates and B values at the lowest level in each hierarchy by pushing all newly accumulated radiosity down to this level. The averaged B values at all levels above the lowest in the hierarchy are then updated using these new values.

Chapter 5

73

level l-1

level l

Vertex value is below isosurface value Vertex value is above isosurface value
Figure 5.11: An example of a good surface approximation at level l.

This algorithm is presented here using a shooting technique, but could also be written using a gathering method. The shooting technique has the advantage that voxels with the most radiosity to contribute to the scene are processed first, causing the solution to converge more quickly. This is useful when displaying images of partial radiosity results, since early images closely approximate the final result, with progressive refinement occurring as computation progresses.
To determine the complexity of this method, consider a scene where each volume is composed of O(N 3) voxels, and each geometric surface or volumetric isosurface is composed of O(N 2) patches. For the brute-force volumetric radiosity algorithm, there are O(N 6) interactions between elements, and each interaction requires O(N ) work due to the attenuation calculation in the form factor. This O(N ) work results from the O(N ) voxels that must be considered along the line segment connecting two elements. Uniform space subdivision techniques can be used to reduce the number of patches considered during attenuation to a constant, with O(N ) spatial subdivision elements visited. The complexity of this brute-force volumetric radiosity algorithm is then O(N 7).
An analysis of this hierarchical algorithm shows that in the best possible cast the N7
complexity of volumetric radiosity can be reduced to: O( ) where l is the highest level of the 27l
object hierarchies. This best case occurs only when the averaged coefficients and normals are a close approximation of the children values for all ei at all levels of the hierarchy, and 1 is greater than any radiosity shot during the iterative process. In general, the complexity of this method will fall somewhere between the best and worst cases, and depends on how smooth the original data model is, and the value of  1.

Chapter 5

74

level l-1

level l

Vertex value is below isosurface value Vertex value is above isosurface value

Figure 5.12: An example of a bad surface approximation at level l.

5.6. Image Generation

As in the classical method, the radiosity values determined by our volumetric radiosity
technique are view-independent, and can be computed in a preprocessing step. In classical
radiosity, image generation is simply a visible surface detection process. In the radiosity method presented in this paper, ( BDi + BIi ) represents the amount of radiosity emitted directly, scattered or reflected by vi, but does not include the radiosity from other elements that is transmitted unchanged through vi. Similarly, BSi is the radiosity emitted directly, or reflected by pi, but does not include the radiosity that passes through this surface element from other elements in the
scene. For this reason, an accumulation method, such as ray casting [Blinn82, Sabella88], must
be used to generate the final image. Each pixel value is determined by accumulating radiosity
along the ray cast through that pixel.

The volumetric contribution along a segment of the ray can be computed according to:

 I =


0

H

t

BDt

+

BIt e-

t
0 K T x dxdt

(5.26)

Ht is a binary term that evaluates to 1 if the angle between the view ray and the normal at that voxel is greater than 90 degrees, otherwise it evaluates to 0. This term is used to account for the
fact that BDt is defined only over 2 steradians, as opposed to BIt, which is defined over the full 4 steradians. For a volumetric element, the case where the normal and the view ray are less than 90 degrees apart is similar to looking at the back face of a surface patch in classical
radiosity. For example, in Figure 5.13 a ray is cast from the eye and passes through voxels vi and v j. Both the isotropic and diffuse radiosities of vi contribute to the pixel value, but for v j only the isotropic radiosity is considered, since the view ray and N j are less than 90 degrees apart.

Chapter 5

75

Nj Vj
Vi
Ni
View Ray
Eye Point
Figure 5.13: BDt and BIt contributions to the final pixel intensity.
To compute a final pixel intensity, a ray is cast into the scene, and all surface patch intersections are determined. Then for the segment of the ray between the eye point and the first surface patch, each segment of the ray between two surface patches, and the remaining portion of the ray after the last surface patch intersection, the volumetric contribution is calculated. These volumetric contributions, and the contributions of the surface patches, are then composited in a front-to-back order to determine the final pixel value.
The process of computing the volumetric contribution along a ray segment is illustrated in Figure 5.14. As the view ray passes through a voxel, the value of Ht, BDt, BIt, and K T remain constant, so the view ray can be considered in four segments, as shown in Figure 5.14a. The outer integral in Equation 5.26 is then simply a summation, with the attenuation integral either approximated at the center of each ray segment, or computed at several sample locations along the view ray. The intensity value for this ray segment can then be determined by stepping along a discretized ray, considering the length of the ray segment contained in each voxel, the normal of each voxel, and the radiosity values and coefficients of each voxel.
In volumetric radiosity, each surface patch emits and reflects light uniformly over its area, which can lead to aliasing in the final image. Similarly, voxel emits, reflects, scatters, and absorbs light uniformly over its volume, which can also lead to a ``blocky'' appearance in the final image. In classical radiosity, the problem is reduced by computing radiosity values at the vertices of each patch, and using bilinear interpolation to determine the radiosity at any continuous location on the patch. To diminish the artifacts for volumetric contributions in volumetric radiosity, the final radiosity value determined for a voxel can be considered as a point sample of the continuous radiosity function. Trilinear interpolation can then be used to determine the radiosity value at any location in a volume by considering the eight voxels

Chapter 5

76

Data Sample

Data Sample

View Ray

Ray Segment (with constant radiosity values and coefficients)

Eye Point

View Ray

Radiosity Sample (for image generation)

Eye Point

(a) (b)
Figure 5.14: Calculating the final pixel intensity values (a) using piece-wise constant coefficients, and (b) using trilinear interpolation.

bounding this location. Samples along the view ray are taken at regularly spaced intervals as shown in Figure 5.14b, where a compositing technique similar to the one described above is used to combine radiosity values. As in volume rendering, care must be taken when selecting a step size - if the step size is too large, artifacts will still appear in the image, but if the step size is too small, image generation will require too much time.
5.7. Results
Volumetric data rendered with the volumetric radiosity method can scatter and absorb light isotropically giving a ``cloud'' appearance, can reflect light diffusely giving a ``surface'' appearance, or can both scatter and reflect light. An example of diffuse interactions is given in Figures 5.15 - 5.17. This scene contains a volumetric light source, a volumetric polygon slab, and a volumetric sphere. The light source is visible in Figure 5.17, which contains another view of the same scene. The light source emits light isotropically while both the polygonal slab and the sphere diffusely scatter light. Note the color bleeding effect on the lower half of the sphere.

Chapter 5

77

Figure 5.15: A volumetric radiosity image of a scene containing a diffuse sphere.

Figure 5.16: A low resolution volumetric radiosity image of a scene containing a diffuse sphere.
The image in Figure 5.16 is a low resolution version of Figure 5.15 created by subsampling both in the image and along each ray during ray casting. Low resolution images are used to quickly display the state of the radiosity solution during the iterative process. Figure 5.18 was generated

Chapter 5

78

with the same scene used for the previous three images, except that the volumetric sphere scatters light isotropically in this image.
Figure 5.19 contains a voxelized spherical light source, and simulation data representing the wave function values around a high potential iron protein. Transfer functions based on the scalar data values were used to define the radiosity coefficients for each of the volumetric data sets. The light source is an isotropic emitter placed to the left and above the eye point. The positive wave function values are diffusely reflecting purple light, while the negative wave function values are isotropically scattering green light.
Statistics for the radiosity preprocessing required for the images shown in this section are given in Tables 5.1 - 5.4, while the image generation statistics are given in Table 5.5. All times are given for a Silicon Graphics Indy with an R4000 PC processor and 80 megabytes of RAM. No effort was made to optimize the code for the iterative or rendering process.
Table 5.1 indicates, for each of the three scenes, the total number of elements in the scene, the number of interactions required by the hierarchical iterative method, the number of interactions that would be required in a non-hierarchical iterative method and in a matrix solving method, and the time, in seconds, required to solve for the radiosity values. For the hierarchical and non-hierarchical iterative methods, an interaction is only counted if some radiosity contribution could occur between two interacting elements, and an attenuation calculation is performed. Basically this eliminates interactions that do not result in any energy exchange due to orientation considerations. In Table 5.2, statistics are given for levels at which elements

Figure 5.17: A distant volumetric radiosity image of a scene containing a diffuse sphere. The volumetric light source is visible at the top of the image.

Chapter 5

79

Figure 5.18: A volumetric radiosity image of a scene containing an isotropic sphere.

Figure 5.19: A volumetric radiosity image of a scene containing a high potential iron protein.

Chapter 5

80

Scene
Diffuse Sphere

Table 5.1: Radiosity preprocessing statistics for the three scenes.

Elements

Interactions with hierarchical
iterative method

Estimated interactions with non-hierarchical iterative method

Interactions with matrix solver

Time (seconds)

46,426

2,548,978

229,979,475 2,155,373,467

4378

Isotropic Sphere

46,426

3,220,461

492,957,821 2,155,373,467

4917

Iron Protein 54,352

3,109,995

984,022,837 2,954,139,904

5237

interacted during the iterative shooting process for the scene containing the diffuse sphere. Table 5.3 indicates the interaction statistics for the scene containing the isotropic sphere, while Table 5.4 contains the interaction statistics for the iron protein scene. For these three scenes, the object hierarchies were constructed to a maximum level of 2. Table 5.5 contains the image generation statistics for the five images shown in this section. Although the image resolution of Figure 5.16 is 500500 pixels, this image was generated by sampling only 125125 pixels, and using a large stepping distance along each ray. The image is a reasonable approximation of Figure 5.15, and contains sufficient detail to show the progress of the iterative shooting technique without the high computation time associated with the generation of a high resolution image.

Chapter 5

81

Table 5.2: Interaction levels for the scene containing the diffuse sphere.

Sending level

Receiving level 012

0

1,744,443

133,721

38,537

1

27,146

518,004

29,191

2

1,483

15,330

41,123

Table 5.3: Interaction levels for the scene containing the isotropic sphere.

Sending level

Receiving level 012

0

1,938,277

341,104

43,007

1

87,339

509,920

62,837

2

1,826

152,723

83,428

Chapter 5

82

Table 5.4: Interaction levels for the scene containing the high-potential iron protein.

Sending level

Receiving level 012

0

2,470,525

15,241

1,513

1

13,278

318,397

42,964

2

1488

18,398

226,703

Table 5.5: Image generation statistics for the five images.

Image

Image generation time Image resolution

(in seconds)

(in pixels)

Figure 5.15 Figure 5.16 Figure 5.17

972.93 21.00 241.03

500x500
500x500 Subsampled at
125x125
500x500

Figure 5.18

1012.15

500x500

Figure 5.19

1226.63

500x500

Chapter 5

83

5.8. Summary
Volumetric radiosity encompasses classical surface radiosity while also allowing for isotropic and diffuse volumetric interactions. This results in one radiosity equation for each surface patch in the scene, and two radiosity equations for each volume element in the scene. Solving these radiosity equations using a matrix solver would be a computationally expensive operation requiring large amounts of memory. Instead, a hierarchical iterative shooting method can be employed to solve for the scene radiosities. A view-dependent compositing step can be used to create an image after the radiosity solution is complete. Low resolution images can be computed during the radiosity calculations for progressive refinement.

Chapter 6 Multipass Illumination Methods

In Chapter 4, a volumetric ray tracing method was introduced that can account for direct lighting from simple light sources, and perfect specular effects such as specular reflection and transmission of light. In this method, a single ambient term is used to approximate light arriving from other directions, and therefore diffuse interactions between objects, such as color bleeding, are lost. In Chapter 5, a volumetric radiosity method was developed that allows for more complex light sources and diffuse interactions between objects in a scene, but does not account for any specular effects. To increase realism it is necessary to include both diffuse interactions and specular effects in the same image, but the simplicity of each of these methods is a direct result of underlying approximations that prohibit either specular or diffuse effects. This problem was also encountered in Chapter 3 with classical ray tracing and radiosity algorithms, and the same basic multipass concept used to improve realism in the classical methods can be applied to the volumetric methods.
In this chapter, multipass techniques for global illumination, and their application to volumetric data sets are discussed. In particular, two multipass methods for global illumination that utilize the concepts presented in the previous two chapters are discussed. The first method uses volumetric radiosity as a view-independent pass to calculate direct lighting and diffuse interactions, while a view-dependent ray tracing pass is used to incorporate specular effects and to create the image. For the second multipass technique a view-independent light tracing method is developed in order to account for light arriving at diffuse surfaces after one or more interactions with specular surfaces. A ray tracing technique is employed to compute direct lighting and specular interactions, and to create the final image.

6.1. Combining Volumetric Ray Tracing and Radiosity

The relative advantages and disadvantages of ray tracing and radiosity have led to the

development of several multipass methods for global illumination which attempt to capture both

specular and diffuse interactions in a geometric scene [Wallace87, Sillion89]. With similar

motivations, a multipass method combining volumetric radiosity and volumetric ray tracing can

be developed.

A general equation for the intensity of light traveling in the direction  at the location x in a

volumetric scene is:

 I(x, ) = E(x, ) + =4


t=t0

g(x,

x(t))

sc ( x ,

 ,

)I

( x (t ),

)dtd

(6.1)

Chapter 6

85

This intensity equation is similar to Equation 3.1 for geometric scenes, except that instead of a single possible surface location, many locations in the  direction may contribute to the
intensity at x. Integration is required to account for all locations along the ray originating at x in the the  direction. Also, the geometric term g(x, x(t)) must now account for attenuation along the ray segment from x to x(t).

An intensity equation for the volumetric ray tracing method presented in Chapter 4 can be

expressed as:

 I(x, ) = A + =4


t=t0

k D g1( x,

x(t))E(x(t), )dt

d  

+


t=t0

g2(x,

x (t )) k SR

I

( x (t ),

 SR ) dt

+


t=t0

g2(x,

x (t )) k ST

I

( x (t ),

 ST

)dt

(6.2)

Since light sources are typically modeled as point or directional light sources and are not

rendered as objects in the final image, the emission term for x is not necessary in this equation.

All directions are considered for direct illumination from a light source, yet since only a small

number of point or directional light sources are allowed, the double integral for direct

illumination can be replaced by a single sum over all light source directions and locations. In this equation, light can also arrive from the directions of specular reflection SR, and specular transmission ST . The coefficients of diffuse reflection, specular reflection and specular transmission are kD, kSR and kST , respectively. All other light arriving at x is modeled with a single ambient term, A. Two geometric terms, g1() and g2(), appear in this equation since different geometric factors are considered when accounting for light arriving at a diffuse element

or a specular element. Actually, most elements are partly diffuse and partly specular, resulting in

the three coefficients, kD, kSR and kST , that govern the behavior of an element.

An intensity equation for the volumetric radiosity method presented in Chapter 5 can be

expressed as:

I (x, ) = IISC (x, ) + IDSC (x, )

(6.3)

where

 IISC (x, ) = EI (x, ) + =4


t=t0

kISC [g3(x,

x(t))IISC (x(t),

 )

+ g4(x, x(t))IDSC (x(t), )]dt d,

(6.4)

 IDSC (x, ) = ED(x, ) + =2


t=t0

kDSC [g5(x,

x(t))IDSC (x(t),

 )

+ g6(x, x(t))IDSC (x(t), )]dt d,

(6.5)

and

E(x, ) = ED(x, ) + EI (x, ).

(6.6)

The area and volume light sources of volumetric radiosity can be visible in the final image, so the emission at x must be considered in the intensity equation. Light can arrive at x from all

Chapter 6

86

directions but only pure isotropic and diffuse scattering is allowed, so  sc() from Equation 6.1 can be replaced by the coefficients for isotropic kISC , and diffuse kDSC scattering. The four geometry terms are used to account for attenuation due to distance and intervening volumes, and possible orientation considerations. These four geometry terms are necessary to account for the four possible interaction combinations. For example, the relative orientations of two interacting diffuse elements must be considered, while orientation is not a factor when two isotropic elements interact.

A multipass method designed to capture the effects of both volumetric ray tracing and
volumetric radiosity requires a hybrid version of the two intensity equations. For this multipass method, the intensity of light at a location x in the direction  can be written as:

I (x, ) = IISC (x, ) + IDSC (x, )

+ kSR


t=t0

g2(x,

x ( x )) I

( x (t ),

 SR )

+ kST


t=t0

g2(x,

x(x))I (x(t),

 ST

)

(6.7)

The view-independent components of the illumination equation, IISC () and IDSC (), are determined during the first pass of this multipass illumination method using the volumetric radiosity method as described in Chapter 5. Due to conservation of energy, care must be taken in volumetric radiosity to ensure that the total amount of light diffusely reflected, scattered, absorbed, or transmitted by an element is not more than the amount of light arriving at this element. Similarly, for this method the amount of light reflected, scattered, absorbed, or transmitted by an element during the radiosity pass plus the amount of light specularly reflected or transmitted by an element during the ray tracing pass must not be more than the amount of light arriving at this element.
A modified form of volumetric ray tracing is employed to evaluate Equation 6.7, and is the second pass of this multipass technique. The values for IISC () and IDSC () are obtained from the radiosity solution, while the intensity of light arriving from the directions of specular reflection and transmission is determined by recursively spawning rays. As in volumetric ray tracing, the integral along the ray is approximated using a sampling technique. For an MM image with an average of N samples required per ray, and a maximum recursion level of l, the complexity of this pass is O(M22l+1 N l), therefore care must be taken when setting the sampling rate.
Figure 6.1 illustrates the possible light paths for this method, and indicates which pass of this multipass technique is responsible for computing each part of the path. In this Figure, D represents not only a diffuse surface element, but also a diffuse or isotropic volume element. Similarly, S represents a specularly reflecting or transmitting surface element or voxel. Some elements are partly diffuse and partly specular, and are therefore considered during radiosity, and cause reflection rays to be spawned during ray tracing.
As in volumetric radiosity, this multipass method can benefit from an implementation based on progressive refinement. The iterative shooting technique employed to solve the scene radiosities can be interrupted to generate a partial image where the remaining unprocessed energy can be used as an ambient term for image generation. Subsampling in both the image and along the ray, as well as a low maximum level of recursion for reflection rays can be used to quickly produce a partial image using the modified ray tracing technique.

Chapter 6

87

L D

E L D* S* E
S*

Key: D - Diffuse or Isotropic Element S - Specular Element L - Light Source E - Eye Point * - zero or more

Ray Tracing Path Radiosity Path

Figure 6.1: Illumination paths captured in the volumetric radiosity and ray tracing multipass method.

This multipass method was tested on a scene containing a volumetrc sphere, a volumetric plane, and a volumetric light source. Diffuse scattering and isotropic scattering of light by the sphere and the plane are allowed, while the light source emits and scatters light isotropically. The plane also specularly reflects light, with the specular reflection coefficient for most voxels in the plane greater than both the isotropic and diffuse coeffients.
An image of the full solution of the multipass intensity equation is given in Figure 6.2. The color bleeding effects, such as the purple tint of the bottom of the sphere, were captured during the radiosity pass, while the specular reflections, such as the reflection of the sphere on the plane, were captured during the ray tracing pass. Images of partial solutions are given in Figures 6.3 and 6.4. The brightness and contrast of these partial images have been enhanced to ensure photographic reproduction. Figure 6.3a represents the state of the radiosity solution after only the direct lighting contributions have been processed. Figure 6.3b represents the additional contibutions made by indirect diffuse and isotropic lighting. Figure 6.4a is the ray tracing solution, with radiosity contributions considered only after an interaction with a specular surface. Note that not only the radiosity values of the sphere, but also those of the plane itself contribute to the specular reflection seen in the volumetric plane.
One difficulty that was faced by the volumetric ray tracing method is that volumetric data does not necessarily represent solid objects, and therefore overlapping volumetric objects must

Chapter 6

88

Figure 6.2: The full radiosity and ray tracing solution for a scene containing a diffuse sphere and a specular polygonal slab.
Figure 6.3: The results of (a) direct illumination and (b) indirect diffuse and isotropic illumination. be allowed. Since sampling and shading occur in two seperate phases of the algorithm, either resampling must be performed for segments of the ray where volumes overlap, or samples must

Chapter 6

89

be taken at regular intervals along the ray. To avoid resampling, a uniform stepping distance is employed in this method. Unfortunately, a very high sampling rate is required to eliminate sampling artifacts in high gradient areas. Since all primary rays originate at a single point, a low, uniform sampling rate will produce ``rings'' in the image that are especially noticable in highly specular areas, as shown in Figure 6.4b. By adding a random number between 0 and the sampling distance to the sample locations along the ray, these artifacts can be reduced as shown in Figure 6.4a. Although this image is not more physically accurate than Figure 6.4b, it is more visually pleasing.
The timing results for this example are given in Table 6.1. These results were obtained using a Silicon Graphics Indigo Elan with an R3000 processor and 80 megabytes of RAM. The times for the radiosity pass are highly dependent on the number of elements in the scene, while the times for the ray tracing pass are highly dependent on the image resolution and sampling rate.

6.2. Indirect Specular Lighting Extension

Neither the volumetric ray tracing illumination model nor the volumetric radiosity illumination model can account for indirect specular lighting in a scene. In this section, methods for computing indirect specular illumination are considered, and a multipass illumination technique based on an indirect specular method and volumetric ray tracing is developed.

The intensity of light at a diffuse element location x arriving from indirect specular paths

can be expressed as:

 IIS(x) = =4


t=t0

g1(x,

x(t))IS(x(t), )dt

d  

(6.8)

Figure 6.4: The specular reflection part of the solution using uniform sampling (a) with and (b) without a random offset of the first sample.

Chapter 6

Table 6.1: Timing results for the multipass example.

Pass

Solution Part

Time (seconds)

Radiosity (186,659 elements)

Direct Illumination
Indirect Diffuse and Isotropic Illumination

Ray Tracing (400x400 pixels)

Image Generation (Primary Rays Only)
Specular Effects (Secondary Rays Only)

1207 2783 620 2358

Both Full Solution 6968

90

where

I(x, ) =


t=t0 [kSR

g2(x,

x (t ))[ E ( x (t ),

 SR )

+

I

( x (t ),

 SR )]

+

kST g3(x, x(t))[E(x(t), ST ) + IS(x(t), ST )]]dt

(6.9)

These equations state that the indirect specular intensity IIS(x) at x is the integral over all possible directions , and locations x(t), of the intensity of light IS(x(t), ) traveling along a specular path. The specular path intensity includes all light arriving at x(t) from the directions of specular reflection SR and specular transmission ST . As in the geometric case, the main difficulties in computing indirect specular intensity is the double integral and the complex geometric terms in Equation 6.8.
The concept of light ray tracing [Arvo86, Heckbert90, Shirley90, Chen91] can be employed in volumetric scenes to capture indirect specular lighting effects. In this method, a threedimensional caustic volume is created for each diffuse, or partially diffuse, object in the scene. A caustic volume is a three-dimensional grid that will be used to store the estimates of indirect specular intensity for each grid location. For each light source in the scene, rays are cast into the scene such that each ray carries the same amount of energy. This can be accomplished using a Monte Carlo sampling technique or a hemi-cube method. These rays are traversed through the scene until a diffuse location is encountered, then the radiant power is filtered and accumulated into the caustic volume for that object. A uniform reconstruction kernel may be employed,

Chapter 6

91

although this may result in artifacts for regions of the caustic volume with sparse radiant power samples. Instead, the radiant power samples can be store in the caustic volume until all light rays have been processed, then the caustic volume can be filtered with the kernel size dependent on the local density of radiant power samples. A large kernel size is used in areas of sparse samples to avoid artifacts, while dense regions employ a small kernel size to avoid blurring the indirect specular lighting effect.

This indirect lighting method can be combined with volumetric ray tracing to produce an image. For this multipass method, the intensity of light at a location x in the direction  is:

 I(x, ) = A + =4


t=t0

k D g1( x,

x(t))E(x(t), )dt

d  

+

kD IIS (x)

+


t=t0

g2(x,

x (t )) k SR

I (x(t),

 SR ) dt

+


t=t0

g2(x,

x (t )) k ST

I (x(t),

 ST

)dt.

(6.10)

The multipass method begins with light ray tracing to compute the caustic volumes. Ray tracing

is then used to compute direct illumination and specular interactions, with the indirect specular

contributions obtained from the caustic volumes.

Figure 6.5: An example image generated using a combined light ray tracing and volumetric ray tracing method.

Chapter 6

92

Figure 6.5 shows an example image generated using this multipass method. The scene contains a highly specular brown slab with a cylinder cut out of it defined on a grid of 10010020 voxels, a diffuse white slab define on a 1001004 voxel grid, and a single point light source. The caustic volumes were generated at the same resolution as the input data, and a uniform convolution kernel size was employed for the caustic volume reconstruction. The image was generated at a resolution of 500500 pixels on a Silicon Graphics Indy, with an R4000 PC processor and 96 megabytes of RAM. The view-independent light ray tracing pass required 98. 0 seconds, while the view-dependent ray tracing pass required 148. 21 seconds. The indirect specular effects can be seen clearly in this image on the white slab.
One possible extension to the multipass method is to combine it with volumetric radiosity to form a three-pass method. This method would involve volumetric radiosity pass to compute diffuse interactions, a light ray tracing pass to capture indirect specular lighting effects with each element containing sufficient intensity treated as a light source, and a volumetric ray tracing pass to compute specular interactions and to generate the final image. An illustration of the possible light paths captured by this three-pass method is given in Figure 6.6. The two possible paths of the ray tracing pass account for the fact that indirect specular lighting may occur, requiring at least one specular and one diffuse surface, or may not occur at a location. The difficulty faced by this method is that a high number of elements may need to be considered as light sources for the light ray tracing pass, leading to high rendering times.

S+ S* L

D D

E

L D* (( S+D* S*) | S* ) E

Key:

D - Diffuse or Isotropic Element S - Specular Element L - Light Source E - Eye Point * - zero or more + - one or more

Ray Tracing Path Radiosity Path Caustic Path

Figure 6.6: Possible light paths for a combined volumetric radiosity, light ray tracing, and volumetric ray tracing illumination method.

Chapter 6

93

6.3. Summary
Assumptions are made in the volumetric ray tracing and volumetric radiosity methods in order to simplify the intensity calculations. These assumptions make it difficult to include diffuse effects in the volumetric ray tracing method, and specular effects in the volumetric radiosity method. As in the classical geometric methods, more global effects can be included in a single image by combining these two methods to form a multipass method. Since neither of these two methods can capture indirect specular illumination, a separate technique is developed for this effect. This method can be combined with volumetric ray tracing to render specular scenes, or can be added to the volumetric ray tracing and radiosity multipass technique to capture indirect specular lighting in diffuse and specular scenes.

Chapter 7 Conclusions
Generating realistic images of complex scenes has been an important part of computer graphics for many years now. As the desire for more realistic images of scenes with greater complexity grows, global illumination models and rendering techniques are developed, extended, and improved. Volumetric data can be included in realistic images using these techniques typically only after a process of conversion to a geometric model. Since volumetric data is threedimensional, and geometric surface models are two-dimensional, a loss of information occurs during this conversion process. Alternatively, volumetric data can be viewed using a volume rendering method. These methods produce images representing all three dimensions of information, but typically employ only a local illumination.
In this work I have attempted to unite the areas of global illumination models and volume rendering. To accomplish this, I have brought global illumination models and rendering techniques into volumetric space, rather than convert volumetric data into geometric space. This has the advantage of allowing volumetric data to be directly included in a realistic scene, without the need for an intermediate representation or loss of information.
In Chapters 2 and 3, the separate areas of volume rendering and global illumination models were investigated. In Chapter 2, volume rendering terminology was introduced, and standard methods and optimization techniques were discussed. The rendering methods include objectorder techniques such as splatting, image-order techniques such as ray casting, and hybrid techniques that combine these two methods.
In Chapter 3, a general global illumination model for geometric scenes was developed. Several standard illumination models for geometric scenes, including ray tracing, radiosity, and the Rendering Equation, were described, and compared with the general model. The concept of multipass illumination was discussed, which combines several global illumination techniques in order to capture more global effects in a single image. Also, methods for computing indirect specular lighting contributions were investigated.
In Chapter 4, volumetric ray tracing was presented for realistically rendering volumetric data. This method is an extension of the illumination model and rendering techniques of classical ray tracing, and can be used to capture specular effects in a scene, such as specular reflection and transmission of light. The illumination model of classical ray tracing is extended to include volumetric contributions to the intensity along a ray in addition to the standard surface contributions. The notion of an intersection is revised to include both an intersection point, which occurs at a surface location, and an intersection segment, which occurs when a ray passes through a contributing volume. Since ray-object intersection calculations account for most of the computation time in a ray tracing method, several efficiency considerations and optimizations were made. In particular, the PARC acceleration technique was applied to both primary and shadow rays in order to reduce computation time.

Chapter 7

95

Volumetric ray tracing was shown to be applicable to scientific visualization, where the cues obtained by adding global effects to an image can greatly enhance the viewer's three-dimensional understanding of the scene. This method was also shown to be suitable for volume graphics, since complex geometric models can be volume sampled, then efficiently rendered using this method.
In Chapter 5, a volumetric radiosity method was presented for realistically rendering volumetric data. This method allows for diffuse and isotropic emission and scattering by a volume element, or voxel, and diffuse emission and scattering by a surface element, or patch. Due to the high complexity of this algorithm, a hierarchical approach to an iterative shooting technique is used to reduce the storage and time required to converge on a view-independent solution to the simultaneous radiosity equations of a scene. A view-dependent ray casting technique is employed to composite volume and surface radiosity values to produce an image.
Using this volumetric radiosity method, isotropic volumetric interactions can be used to augment a scene with effects such as fog, smoke, and haze. This method was shown to have an advantage over previous radiosity methods that include only isotropic volumetric interaction, since diffuse voxel interactions allow participating volumes to be given the appearance of shaded objects.
Two multipass methods for generating realistic images were presented in Chapter 6. The first method was developed to use volumetric radiosity as a first pass in order to calculate the view-independent diffuse interactions between surfaces and volumes in the scene, while a second pass similar to volumetric ray tracing is used to capture view-dependent specular effects, and to create the image. In the second multipass method, a light ray tracing technique is developed to account for indirect specular lighting in a scene. This technique is then combined with volumetric ray tracing to generate an image.

7.1. Future Directions
The need for realistic images of scenes containing volumetric data is growing as the sources of this data, and the areas in which this data is found, continues to increase. Scanning devices and simulation techniques are providing volumetric data for research in fields such as medicine, biology, geology, physics, and meteorology. Volume sampling methods are attracting traditional surface-based applications into the volumetric domain. To meet these needs, the accuracy of the global illumination models, and the efficiency of the volume rendering techniques must both be increased.
Many improvements can be made to the volumetric ray tracing algorithm by examining advances made in the area of classical geometric ray tracing. Since volumetric ray tracing encompasses an illumination model and rendering techniques for geometric data, many of the advances for classical ray tracing can be directly encorporated into volumetric ray tracing. For example, distributed ray tracing, or a modified definition of a ray, can be encorporated into this volumetric ray tracing model in order to provide dull reflections and fuzzy shadows.
Improvements for volumetric ray tracing can be taken also from the area of volume visualization. The computation of the intensity value associated with a ray-object intersection segment is similar to many standard volume rendering techniques, so new optimization and

Chapter 7

96

acceleration methods developed for standard volume rendering can be employed in volumetric ray tracing as well.
Improvements to the volumetric radiosity algorithm can be found by examining the techniques used in geometric radiosity. Non-uniform methods of generating object hierarchies, such as the techniques used in discontinuity meshing, can be employed by volumetric radiosity to increase accuracy while maintaining or possibly even decreasing the number of interacting elements. Also, higher-order functions can be used to represent the interaction of light across a surface patch or within a voxel, resulting in a smoother radiosity solution with fewer elements.
One problem that volumetric radiosity shares with classical radiosity is the difficulty of determining coefficients and emission values that will produce the desired intensity and color of light in the resulting image. This problem has been addressed in classical radiosity, and the solutions found there can be extended for use in volumetric radiosity.
The time required for the volumetric ray tracing, volumetric radiosity, and indirect lighting algorithms can be reduced with parallel implementations. On shared-memory architectures, these parallel implementations are relatively straight-forward. For volumetric ray tracing, computing the final value for a pixel in the image is independent of all other pixel calculations. Therefore, the computation of an image can be divided among processors at the pixel level. Similarly, the each ray traced from a light source in the indirect lighting calculations is independent, although unlike ray tracing, processing two separate rays can lead to memory writes in the same location. For volumetric radiosity, separate passes through the shooting iteration can be sent to separate processors with only minor additional memory requirements. Parallel implementations of these volumetric global illumination models on distributed-memory systems are not as straight-forward. For example, in volumetric ray tracing the problem is to determine how to break the scene up between processors so that computing the intersection calculations does not require an prohibitive amount of process communication.
To make these volumetric illumination methods practical, additional tools must be developed. For example, modeling tools such as those employed to create geometric scenes must be developed for scenes containing volumetric as well as geometric data. Since it is much too time consuming to individually set all object parameters, these tools should include the ability to create objects with higher-level properties, such as a metallic surface or a foggy volume.
Finally, new global illumination methods capable of capturing the effects caused by surfaces and volumes with arbitrary reflectance/scattering functions should be developed. This may be achieved with an extended multipass method where different global effects are captured in different passes of the illumination solution. Alternatively, sampling techniques may be employed to estimate the intensities defined in a complete illumination equation.

Appendix A The VolVis System
The VolVis system for volume visualization is a comprehensive system that is currently in use for scientific research. Due to the flexible and extensible nature of this system, it is also employed as a testbed for new graphics and visualization algorithms. The volumetric ray tracing, volumetric radiosity, and multipass illumination techniques described in Chapters 4-6 were implemented as part of the VolVis system. This system and the volumetric global illumination methods were developed simultaneously, and each had a great impact on the development of the other. Therefore, a better understanding of these methods, including the motivation and implementation details, can be gained by examining the VolVis system. For this reason, a description of this system is provided, including an introduction to the basic concepts of a visualization system, the motivation behind the development of the VolVis system, an overview of VolVis, a detailed description of the major functional components, and some implementation decisions. A.1. Introduction
The visualization of volumetric data has aided many scientific disciplines ranging from geophysics to the biomedical sciences. The diversity of these fields coupled with a growing reliance on visualization has spawned the creation of a number of specialized visualization systems. These systems are usually limited by machine and data dependencies and are typically not flexible or extensible. A few visualization systems have attempted to overcome these dependencies and have chosen to take a data-flow approach. However, the added computational costs associated with data-flow systems results in poor performance. In addition, these systems require that the scientist or engineer invest a large amount of time understanding the capabilities of each of the computational components and how to effectively link them together.
VolVis is a volume visualization system that unites numerous visualization methods within a comprehensive system, providing a flexible tool for the scientist and engineer as well as the visualization developer and researcher. The VolVis system has been designed to meet several key objectives. First, it is essential that a general-purpose visualization system supply the user with a wide range of functionality by providing numerous methods within each functional component. For example, in one scientific investigation the ability to interactively render surfaces may be critical, while in another experiment highly accurate volume rendering of of multiple, overlapping data sets may be essential. For this reason, the rendering component of VolVis provides various projection methods, including Marching Cubes, ray casting, volumetric ray tracing, and volumetric radiosity.
Another important objective is for the system to be useful to a scientist with only a limited background in computer science. The VolVis user interface is organized into functional components, providing an easy to use turn-key visualization system. One clear advantage of this approach over data-flow systems is that the user does not have to learn the details of how to link numerous modules in order to perform a task. Instead, the entire functionality of the system is

Appendix A

98

always available to the user by simply navigating through the windows of the VolVis functional components.
The VolVis system has been designed for use by scientist as well as visualization developers, therefore extensibility was a key objective in the design of the system. The structure of the VolVis system is designed to allow a visualization programmer to easily add new representations and algorithms into the system. For this purpose, an extensible and hierarchical abstract model was developed that contains definitions for every object used in the system. Also, the highly diverse set of algorithms based on this abstract model provides many functional building blocks that can be used in the development of new algorithms. For example, the basic routine for locating intersections along a ray segment was initially developed for the volumetric ray tracer, and is also employed in volumetric radiosity, the indirect lighting calculations of the multipass method, and as part of the Navigator for object picking.
Due to the wide range of workstations available, a general-purpose visualization system must be portable. The VolVis system, written in C, is highly portable, running on most Unix workstations supporting X/Motif. The system has been tested on Silicon Graphics, Sun, HewlettPackard, Digital, and IBM workstations and PCs.
A.2. Abstract Model
A few of the major concerns during the implementation of VolVis were to ensure that the system was flexible, could be expanded to include new functionality and techniques, and would be relatively easy to port to new platforms. Therefore, the development of the VolVis system required the creation of a comprehensive, flexible, and extensible abstract model. This model is organized hierarchically, beginning with low-level building blocks which are then used to construct higher-level structures. These basic building blocks include a Position that defines a point pR3, and a Vector that defines a 3D orientation or direction. A Matrix is a 4  4 array of floating point values that represents a transformation in a three-dimensional homogeneous coordinate system. A Plane is a plane equation defining an infinite plane in R3, and a Color is a (red,green,blue) triple corresponding to a 24-bit color. Many of these building blocks have multiple types, with an initial B indicating unsigned character, or byte, I indicating integer, and F indicating floating point. For example, an IPosition represents an integer position that is useful for indicating discrete grid locations, while an FPosition contains a floating point position in R3.
Built upon these basic building blocks are some higher level constructs such as a CoordSys, which is a Position indicating the origin, and three Vectors representing the x, y, and z axes of a cartesian coordinate system in R3. Some of the building blocks in the VolVis abstract model can assume one of multiple representations. For example, a Segmentation can be a simple threshold, a binary segmentation function, or a transfer function defining opacity based on scalar value and gradient. Likewise, a LocalShade defines a shading model for a volume that can contain constant coefficients, or complex transfer functions defining shading coefficients based on scalar value, gradient, data color, or texture color. A DataColor can define a single color for the volume, or a transfer function mapping scalar value and gradient to color. A Texture may contain a 2D texture, a 3D solid texture, or a function that is used to generate the texture values. A DataCut contains a cut geometry which is to be applied to a volume. For example, a DataCut may be a plane, a hexahedron, or a sphere. The VolumeData can be either geometric, scalar data, or vector data,

Appendix A

99

and a LightSource can be a point light source, a directional light source, a geometric light source or a volumetric light source. These basic abstract model building blocks are illustrated in Figure A.1.
The top-level component of the VolVis abstract model is the World, as shown in the iconic representation of Figure A.2. The World contains several Views, that define the current specification of a view plane. This is done by storing an original CoordSys, a current CoordSys, and a Matrix defining a transformation between the two. The x and y axes of the current CoordSys lie on the view plane, while the z axis defines the general viewing direction. Also stored in the View are the field of view, and the width and height of the View in both pixels and units, where a unit is considered to be a distance of 1.0 in the World coordinate system. The multiple Views contained in the World are used in the Navigator component to allow the user to view the scene from several positions simultaneously.
The CoordSys for the World is a standard left-handed cartesian coordinate system. All other coordinate systems defined in the abstract model are relative to this standard coordinate system. The World also contains the WorldShade shading model that includes shading information for the entire scene, such as ambient light and background color. The World may contain multiple DataCuts defining cut geometries that are to be applied to all Volumes.
Multiple Lights are defined within the World, where each Light includes a LightSource containing, for example, the color position, and intensity of a point light source, or the color, direction and intensity of a directional light source. Multiple Volumes are also supported, with each Volume containing an original CoordSys, a current CoordSys, and a transformation Matrix,

Position

Vector

00 10 20 30 01 11 21 31 02 12 22 32 03 13 23 33

Matrix

Color Plane CoordSys

L W
S
T

LocalShade

WorldShade Segmentation

D

DataColor

Texture

V

DataCut VolumeData View LightSource Volume

Figure A.1: Some basic building blocks of the abstract model.

Appendix A

100

orig_c_sys

view[]

W world_shade

light[]

world_cut[]

V volume[]

Figure A.2: Iconic representation of the World.

similar to the View. Also contained in the Volume are the Plane equations defining the faces of the bounding hexahedron of the Volume, as well as the x, y, and z resolution of this hexahedron in units. Each Volume contains a unit type that indicates what one unit in this volume represents. For example, one unit in a volume may represent a micron, an inch, or a kilometer. This information is used during measurement calculations, and is also used to convert multiple volumes into a common unit type, thereby showing the relative sizes of these volumes. A Volume may contain multiple DataCuts that are applied to the data in addition to the ones specified in the World. Each Volume also contains a LocalShade, a DataColor, a Texture, and the actual VolumeData. For geometric data, the VolumeData contains a geometric object description. If the VolumeData is scalar data defined on a rectilinear grid, it contains a Segmentation, the data samples, and the dimensions of this data set in samples, or voxels. An iconic representation of a Volume is shown in Figure A.3.
In order to facilitate transformations between the World coordinate system and the Volume coordinate system, several conversion Matrices are stored within each Volume. These Matrices are used to transform a Position or a Vector from the World coordinate system to the Volume coordinate system, and vice-versa. For many VolumeData types it is useful to have a scaled version of these transformation matrices. This allows, for example, the transformation of a location in units according to the World coordinate system into a voxel location within the grid of data sample in the local coordinate system of the volume data.
One of the major advantages of this abstract model is the ease with which Volumes can be manipulated. Translations and rotations are performed by simply concatenating the new transformation matrix to the one stored in the Volume, thereby defining a new current CoordSys, new Plane equations, and new conversion Matrices. Unlike many other systems that require data interpolation to be performed during the data input/reconstruction phase, the VolVis abstract

Appendix A

101

orig_c_sys

00 10 20 30 01 11 21 31 02 12 22 32 03 13 23 33

c_sys_convert[]

c_sys
x_units float y_units
z_units
UnitType unit_type

00 10 20 30 01 11 21 31 02 12 22 32 03 13 23 33

transform

Tplane_equation[]

L local_shade

D

data_cuts data_color Texture volume_data

Figure A.3: Iconic representation of a Volume.

model allows for arbitrary scaling of the data with no additional memory expense. This is accomplished by modifying the x, y, and z size in units of the data, which results in modified Plane equations and conversion Matrices.
Another advantage of the VolVis abstract model is its expandability. The model is designed so that, for example, the addition of a new Texture type, Segmentation type, or geometric object requires a minimal amount of work, and does not compromise the flexibility of the model. Incorporating a new projection algorithm or measurement tool is also made simple by this model.
A.3. System Overview
Visualization systems typically take input, and through a sequence of user-specified operations, produce output visualization. The input generally comes from data files and physical input devices such as a keyboard and mouse. The output visualization is the image, or sequence of images, that is displayed to the user on the screen. Figure A.4 shows the VolVis pipeline, indicating some paths that input data could take through the VolVis system in order to produce visualization output.
Two of the basic input data classes of VolVis are volumetric data and three-dimensional geometric data. The input data is processed by the Modeling and Filtering components of the system to produce either a 3D volume model, or a 3D geometric surface model of the data. For example, geometric data can be converted into a volume model by the Modeling component of the system [Wang93, Wang94] to allow for volumetric graphic operations. A geometric surface model can be created from a volume model by the process of surface extraction [Avila92, Lorensen87].
The Measurement component can be used to obtain quantitative information from the data models. Surface area, volume, histogram and distance measurement information can be extracted

Appendix A

102

3D Scalar Field 3D Vector Field

Modeling Filte&ring

3D Geometric Objects

Volumetric Model

Surface Extraction

Geometric Surface Model

Key:

Input Data
Internal Data Output Visualization Action

Measurement Manipulation

Virtual Input Device

Environment

Input Device Abstraction
Physical Input Device

Rendering

Image

Animation

Navigation Preview

Figure A.4: The VolVis pipeline.

from volumes using one of several methods. Isosurface volume and surface area measurements can be taken either on an entire volume or on a surface-tracked section. Additionally, two methods are available for computing the surface area or volume of an isosurface. A simple noninterpolated voxel box counting method and a Marching Cubes [Lorensen87] based measurement method are provided. For geometric surface models, surface area, volume, and distance measurements can be performed.
Most of the interaction in VolVis occurs within the Manipulation component of the system. This part of the system allows the user to modify object parameters, such as color, texture, and segmentation, and viewing parameters such as image size and field of view. Within the Navigation portion of the Manipulation component, known as the Navigator, the user can interactively modify the position and orientation of the volumes, the light sources, and the view.

Appendix A

103

This is closely connected to the Animation section of the Manipulation component, known as the Animator, which allows the user to specify animation sequences either interactively, or with a set of transformations to be applied to objects in the scene. The Manipulation component is described in more detail in Section A.4.
The Rendering component handles all rendering within the VolVis system. Several different rendering algorithms are supported including geometric primitive techniques such as marching cubes, global illumination methods such as ray tracing and radiosity, and a compression domain volume rendering technique [Avila94a]. The Rendering component is described in Section A.5.
The Input Device component of the system is responsible for mapping physical input device data into a device independent representation that is used by various algorithms requiring user interaction [He93]. For example, the navigator acquires position and orientation information from the Input Device component instead of directly querying the physical device. As a result of this indirection, the VolVis system is input device independent. New input devices can be easily added to the system by modifying only the Input Device component, while all other components that access input device information remain unchanged. This indirection also allows for the interactive specification of ``virtual'' input devices. For example, the user may wish to supply position information using a spaceball, while orientation information is obtained from an isotrak. A.4. Manipulation
The Manipulation component of VolVis actually consists of three sections: the Object Control section, the Navigator, and the Animator. Both the Navigator and the Animator produce output visualization, shown in Figure A.4 as Navigation and Animation, respectively.
The Object Control section of the system is extensive, allowing the user to manipulate the parameters that define the objects in the scene. This includes modifications to the color, texture, and shading parameters of each volume, as well as more complex operations such as positioning of cut planes, and data segmentation. The color and position of all light sources within the scene can be interactively manipulated by the user. Also, viewing parameters, such as the size of the final image, and global parameters, such as ambient lighting and the background color, can be modified within this section of the system.
The VolVis Navigator allows the user to interactively manipulate objects within the system. The user can translate, scale and rotate all volumes and light sources, as well as the view itself. The Navigator can also be used to interactively manipulate the view in a manner similar to a flight simulator. To provide interactive speed within the Navigator, a fast rendering algorithm was developed that involves projecting reduced resolution representations of all objects in the scene. This task is relatively simple for geometric objects, where calculating, storing, and projecting a polygonal approximation requires little overhead. However, when considering a volumetric isosurface the cost of an additional representation increases considerably. A simple and memory efficient method available within the Navigation section creates a reduced resolution representation of an isosurface by uniformly subdividing the volume into boxes and projecting the outer polygons of all the boxes that contain a portion of the isosurface. These subvolumes serve a dual purpose in that they are also used by the PARC acceleration method [Avila92] during ray casting and ray tracing.
Although the PARC representation can be stored as a list of subvolume indices, that occupies a small amount of space, the resulting navigator images are boxy and uninformative for many datasets. To overcome this problem, another method is provided within the Navigator that

Appendix A

104

utilizes a reduced resolution Marching Cubes representation of an isosurface. In order to reduce the amount of data required for this representation, edge intersections used to compute triangle vertices are quantized down to one of four possible locations. This results in much smoother images that are more informative in most cases than the uniform subdivision method. The Navigator also supports the other VolVis rendering techniques that are described in Section A.5, although interactive projection rates with these methods can be achieved only on high-end workstations.
The Animation section of the VolVis system is tightly connected to the Navigation section. It also allows the user to specify transformations to be applied to objects within the scene, but as opposed to the Navigator which is used to apply a single transformation at a time, the Animator is used to specify a sequence of transformations producing an animation. The user can see a preview of the animation using one of the fast rendering techniques of the Navigator. The user can then select a more accurate and time consuming rendering technique, such as volumetric ray tracing, to create a high quality animation. In addition to simple rotation, translation and scaling animations, the Navigator can be used to interactively specify a ``flight path'', which can then be passed to the Animator, and rendered to create an animation.
A.5. Rendering
Rendering is one of the most important and extensive components of the VolVis system. For the user, speed and accuracy are both important, yet often conflicting aspects of the rendering process. For this reason, a variety of rendering techniques have been implemented within the VolVis system, ranging from the fast, rough approximation of the final image, to the comparatively slow, accurate rendering within a global illumination model. Also, each rendering algorithm itself supports several levels of accuracy, giving the user an even greater amount of control. These multiple levels of accuracy also lead to a progressive-refinement technique employed by the Navigator. For example, with the volumetric ray tracing algorithm, the image could be subsampled, with an interpolation method used to fill in missing pixel values, Also, when sampling along the ray is required, the step size can be varied depending on the image accuracy level. The Navigator can use a low accuracy version of the volumetric ray tracing algorithm while the user is flying, then fill in the detail after the user has stopped.
Due to the large number of shading and projection options given to the user, computational efficiency was often sacrificed for coding efficiency in the rendering section of VolVis. For example, the color of an object can be defined in many ways, including a single color, a transfer function mapping scalar value to color, or a three-dimensional solid texture. For volumetric ray tracing, multiple versions of the intersection and shading code could have been created, with each optimized for a specific color type. Instead, the intersection and shading routines were developed to handle all possible shading and projection options. As a result, the ray tracing code is less than 4,000 lines, and can be easily extended to include new shading and projection options.
A.6. Implementation
As mentioned in the introduction, the VolVis system requires only X/Motif on a Unix workstation to run. Unfortunately, only simple two-dimensional graphics operations are supported in X, therefore all viewing transformations, shading, and hidden surface removal must

Appendix A

105

be done in software. This greatly reduces the rendering speed for the geometry-based projection routines used in the Navigator, and therefore also reduces the overall interactivity of the system. Since many Unix workstations now include graphics hardware, interactivity can be maintained by utilizing the graphics language of the workstation. To avoid rewriting large sections of the code, we have developed a library of basic graphics functions that are used throughout the VolVis code. This simplifies the process of porting the system to a new workstation that has a different graphics language, since only the graphics function library must be rewritten.
A.7. Future Directions
The VolVis system for volume visualization has been used for many tasks in diverse applications and situations. First, VolVis has been used to test new algorithms for rendering, modeling, animation generation, and computer-human interaction. Due to the flexible nature of the abstract model, testing new ideas within the system is much easier and less time consuming than writing a new application for each new algorithm. VolVis has also been used by scientists and researchers in many different areas. For example, neurobiologists have used VolVis to navigate through the complex dendritic paths of nerve cells, which is extremely useful since the function of nerve cells is closely tied to their structure.
VolVis is a rapidly growing system, with new plans for future development continually being considered. Since the system is currently being used by many research labs and visualization developers, feedback from these sources is used to make future versions of the system easier to use and extend. To increase portability, a user-interface library, similar to the graphics function library described in the previous section, is being developed to allow VolVis to be easily ported to new windowing systems.
There are many possible areas of future research within the VolVis system. One interesting possibility is to extend the system to handle four-dimensional (time-dependent) data. This involves modifications to both the Navigator and Animator, since object manipulations would include time transformations, as well as the standard spatial transformations.
Another interesting area of future research is to extend the types of data that are handled by the system. Additional data types may include irregularly gridded data, and vector fields. These changes require modifications to the Rendering component in order to generate images of these new data types.

References
Adams90. L. Adams, J. M. Gilsback, W. Krybus, D. Meyer-Ebrecht, R. Mosges, and G. Schlondorff, "CAS - A Navigation Support for Surgery," pp. 411-423 in 3D Imaging in Medice: Algorithms, Systems, Applications, ed. K. H. Hoehne, H. Fuchs, and S. M. Pizer, SpringerVerlag, Berlin, West Germany (1990).
Amanatides84. J. Amanatides, "Ray Tracing with Cones," Computer Graphics (Proc. SIGGRAPH) 18(3) pp. 129-135 (July 1984).
Appel68. A. Appel, "Some Techniques for Shading Machine Renderings of Solids," AFIPS Conference Proceedings 32 pp. 37-45 (1968).
Arridge90. S. R. Arridge, "Manipulation of Volume Data for Surgical Simulation," pp. 289-300 in 3D Imaging in Medice: Algorithms, Systems, Applications, ed. K. H. Hoehne, H. Fuchs, and S. M. Pizer, Springer-Verlag, Berlin, West Germany (1990).
Arvo86. J. Arvo, "Developments in Ray Tracing," SIGGRAPH Course Notes 12(1986).
Arvo87. J. Arvo and D. Kirk, "Fast Ray Tracing by Ray Classification," Computer Graphics (Proc. SIGGRAPH) 21(4) pp. 55-64 (July 1987).
Arvo90. J. Arvo and D. Kirk, "Particle Transport and Image Synthesis," Computer Graphics (Proc. SIGGRAPH) 24(3) pp. 63-66 (August 1990).
Atherton78. P. R. Atherton, K. Weiler, and D. Greenberg, "Polygon Shadow Generation," Computer Graphics (Proc. SIGGRAPH), pp. 275-281 (1978).
Avila94a. R. Avila, T. He, L. Hong, A. Kaufman, H. Pfister, C. Silva, L. Sobierajski, and S. Wang, "VolVis: A Diversified System for Volume Visualization," Visualization '94 Proceedings, IEEE CS Press, (October 1994).
Avila92. R.S. Avila, L.M. Sobierajski, and A.E. Kaufman, "Towards a Comprehensive Volume Visualization System," IEEE Visualization '92 Proceedings, pp. 13-20 (October 1992).
Avila94b. R.S. Avila, L.M. Sobierajski, and A.E. Kaufman, "Visualizing Nerve Cells," IEEE Computer Graphics and Applications, (September 1994).
Barillot85. C. Barillot, B. Gibaud, L.M. Luo, and I.M. Scarabin, "3D Representation of Anatomic Structures From CT Examinations," Proceedings SPIE 602 pp. 307-314 (1985).
Barney90. S. A. Barney, J. M. Nyce, M. J. Ackerman, W. X. Graves, J. C. King, E. T. Koh, and J. W.

References

107

Sundsten, "Visualization in the Neurosciences: Addressing Problems in Research, Teaching and Clinical Practices," Proceeding of the First Conference on Visualization in Biomedical Computing, pp. 322-327 IEEE Computer Society Press, (1990).
Baum89. D. R. Baum, H. E. Rushmeier, and J. M. Winget, "Improving Radiosity Solutions Through The Use of Analytically Determined Form-factors," Computer Graphics (Proc. SIGGRAPH) 23(3) pp. 325-334 (July 1989).
Baum91. D. R. Baum, S. Mann, K. P. Smith, and J. M. Winget, "Making Radiosity Usable: Automatic Preprocessing and Meshing Techniques for the Generation of Accurate Radiosity Solutions," Computer Graphics (Proc. SIGGRAPH) 25(4) pp. 51-60 (July 1991).
Blinn76. J. F. Blinn and M. E. Newell, "Texture and Reflection in Computer Generated Images," CACM 19(10) pp. 542-547 (October 1976).
Blinn82. J. F. Blinn, "Light Reflection Functions For Simulation of Clouds and Dusty Surfaces," Computer Graphics (Proc. SIGGRAPH) 16(3) pp. 21-29 (July 1982).
Bouknight70a. W. J. Bouknight and K. C. Kelly, "An Algorithm for Producing Half-Toned Computer Graphics Presentations with Shadows and Movable Light Sources," SJCC, pp. 1-10 AFIPS Press, (1970).
Bouknight70b. W. J. Bouknight, "A Procedure for Generations of Three-Dimensional Half-Toned Computer Graphics Presentations," CACM 13(9) pp. 527-536 (September 1970).
Buckalew89. C. Buckalew and D. Fussel, "Illumination Networks: Fast Realistic Rendering with General Reflectance Functions," Computer Graphics (Proc. SIGGRAPH) 23(3) pp. 89-98 (July 1989).
Burbach93. B. J. Burbach, R. S. Avila, L. M. Sobierajski, A. E. Kaufman, and P. R. Adams, "The Living and the Dead: Interactive Visualization of Nerve Cells," Society for Neuroscience Abstracts 19(November 1993). Abstract 458.9
Campbell90. A. T. Campbell and D. S. Fussel, "Adaptive Mesh Generation for Global Diffuse Illumination," Computer Graphics (Proc. SIGGRAPH) 24(4) pp. 155-164 (July 1990).
Campbell91. A. T. Campbell, "Modeling Global Diffuse Illumination for Image Synthesis," PhD Dissertation, University of Texas at Austin, Texas (December 1991).
Challinger92. J. Challinger, "Parallel Volume Rendering for Curvilinear Volumes," IEEE Computer Society Proceedings Scalable High Performance Computing Conference, pp. 14-21 (April 1992).

References

108

Chen89. H. Chen, J. W. Sedat, and D. A. Agard, "Manipulation, Display, and Analysis of ThreeDimensional Biological Images," pp. 127-135 in The Handbook of Biological Confocal Microscopcy, The Confocal Microscopy Workshop, The Electron Microscopy Society of America, San Antonio, Texas (August 1989).
Chen91. S. E. Chen, H. E. Rushmeier, G. Miller, and D. Turner, "A Progressive Multi-Pass Method for Global Illumination," Computer Graphics (Proc. SIGGRAPH) 25(4) pp. 165-174 (July 1991).
Chin92. N. Chin and S. Feiner, "Fast Object-Precision Shadow Generation for Area Light Sources Using BSP Trees," Proceedings of 1992 Symposium on Interactive 3D Graphics, pp. ??-?? (March 1992).
Cline88. H. E. Cline, W. E. Lorensen, S. Ludke, C. R. Crawford, and B. C. Teeter, "Two Algorithms for the Three-Dimensional Construction of Tomograms," Medical Physics 15(3) pp. 320-327 (May/June 1988).
Cohen85. M. F. Cohen and D. P. Greenberg, "Hemi-Cube: Radiosity Solution for Complex Environments," Computer Graphics (Proc. SIGGRAPH) 19(3) pp. 31-40 (July 1985).
Cohen86. M. F. Cohen, D. P. Greenberg, D. S. Immel, and P. J. Brock, "An Efficient Radiosity Approach For Complex Environments," IEEE Computer Graphics and Application 6(2) pp. 26-30 (1986).
Cohen88. M. F. Cohen, S. E. Chen, J. R. Wallace, and D. P. Greenberg, "A Progressive Refinement Approach to Fast Radiosity Image Generation," Computer Graphics (Proc. SIGGRAPH) 22(4) pp. 75-84 (July 1988).
Cook84. R. L. Cook, T. Porter, and L. Carpenter, "Distributed Ray Tracing," Computer Graphics (Proc. SIGGRAPH) 18(3) pp. 137-145 (July 1984).
Crow77. F. C. Crow, "Shadow Algorithms for Computer Graphics," Computer Graphics (Proc. SIGGRAPH), pp. 442-448 (1977).
Drebin88. R. A. Drebin, L. Carpenter, and P. Hanrahan, "Volume Rendering," Computer Graphics (Proc. SIGGRAPH) 22(4) pp. 64-75 (August 1988).
Foley90. J. D. Foley, A. vanDam, S. K. Feiner, and J. F. Hughes, Computer Graphics PRINCIPLE AND PRACTICE, ADDISION-WESLEY Publishing Company (1990).
Frieder85. G. Frieder, D. Gordon, and R. A. Reynolds, "Back-to-Front Display of Voxel-Based

References

109

Objects," IEEE Computer Graphics and Applications 5(1) pp. 52-59 (January 1985).
Fujimoto86. A. Fujimoto, T. Tanaka, and K. Iwata, "ARTS: Accelerated Ray-Tracing System," IEEE Computer Graphics and Applications 6(4) pp. 16-26 (April 1986).
Giersten92. C. Giersten, "Volume Visualization of Sparse Irregular Meshes," IEEE Computer Graphics and Applications 12(2) pp. 40-48 (March 1992).
Glassner84. A. S. Glassner, "Space Subdivision for Fast Ray Tracing," IEEE Computer Graphics and Applications 4(10) pp. 15-22 (October 1984).
Glassner89. A. S. Glassner, An Introduction to Ray Tracing, Academic Press, London (1989).
Goldstein71. R. A. Goldstein and R. Nagel, "3-D Visual Simulation," Simulation 16(1) pp. 25-31 (January 1971).
Goral84. C. M. Goral, K. E. Torrance, D. P. Greenberg, and B. Battaile, "Modeling the Interaction of Light Between Diffuse Surfaces," Computer Graphics (Proc. SIGGRAPH) 18(3) pp. 213-222 (July 1984).
Gordon85. D. Gordon and R. A. Reynolds, "Image Space Shading of 3-Dimensional Objects," Computer Graphics and Image Processing 29(3) pp. 361-376 (March 1985).
Gortler93. S. J. Gortler, P. Schroeder, M. F. Cohen, and P. Hanrahan, "Wavelet Radiosity," Computer Graphics (Proc. SIGGRAPH), pp. 221-230 (August 1993).
Gourad71. H. Gourad, "Continuous Shading of Curved Surfaces," IEEE Transactions on Computers C-20(6) pp. 623-629 (June 1971).
Group68. Mathematical Applications Group, Inc., "3-D Simulated Graphics Offered by Service Bureau," Datamation 13(1) p. 69 (February 1968).
Hall86. R. Hall, "Hybrid Techniques for Rapid Image Synthese," Image Rendering Tricks, Course Notes 16 for SIGGRAPH 86, (August 1986).
Halton70. J. H. Halton, "A Retrospective and Prospective of the Monte Carlo Method," SIAM Review 12(1) pp. 1-63 (January 1970).
Hammersley67. J. M. Hammersley and D. C. Handscomb, Monte Carlo Methods, Methuen & Co LTD, London (1967).

References

110

Hanrahan91. P. Hanrahan, D. Salzman, and L. Aupperle, "A Rapid Hierarchical Radiosity Algorithm," Computer Graphics (Proc. SIGGRAPH) 25(4) pp. 197-206 (July 1991).
Hanrahan90. P. S. Hanrahan and D. B. Salzman, "A Rapid Hierarchical Radiosity Algorithm for Unoccluded Environments," in Photosimulation, Realism and Physics in Computer Graphics, ed. K. Bouatouch, Springer-Verlag (1990).
He93. T. He and A. Kaufman, "Virtual Input Devices for 3D Systems," Visualization '93, pp. 142-148 (October 1993).
Heckbert84. P. S. Heckbert and P. Hanrahan, "Beam Tracing Polygonal Objects," Computer Graphics (Proc. SIGGRAPH) 18(3) pp. 119-127 (July 1984).
Heckbert90. P. S. Heckbert, "Adaptive Radiosity Textures for Bidirectional Ray Tracing," Computer Graphics (Proc. SIGGRAPH) 24(4) pp. 145-154 (July 1990).
Heckbert91. P. S. Heckbert, "Simulating Global Illumination Using Adaptive Meshing," PhD Dissertation, University of California at Berkley, California (June 1991).
Heckbert92. P. S. Heckbert, "Discontinuity Meshing for Radiosity," Proceedings of the Third Eurographics Workshop on Rendering, pp. 203-216 (May 1992).
Herman79. G. T. Herman and H. K. Liu, "Three-Dimensional Display of Human Organs from Computed Tomograms," Computer Graphics and Image Processing 9(1) pp. 1 -21 (January 1979).
Herman81. G. T. Herman and J. K. Udupa, "Display of Thee Dimensional Discrete Surfaces," Proceedings SPIE, pp. 90-97 (1981).
Hibbard89. W. Hibbard and D. Santek, "Visualizing Large Data Sets in the Earth Sciences," IEEE Computer 22(8) pp. 53-57 (August 1989).
Hoehne86. K. H. Hoehne and R. Bernstein, "Shading 3D-Images from CT Using Gray-Level Gradients," IEEE Transactions on Medical Imaging MI-5 pp. 45-47 (March 1986).
Hottel67. H. C. Hottel and A. D. Sarofim, Radiative Transfer, McGraw-Hill, New York, New York (1967).
Kajiya86. J. T. Kajiya, "The Rendering Equation," Computer Graphics (Proc. SIGGRAPH) 20(4) pp. 143-150 (August 1986).

References

111

Kaufman86. A. Kaufman and E. Shimony, "3D Scan-Conversion Algorithms for Voxel-Based Graphics," Proceedings ACM Workshop Interactive 3D Graphics, pp. 45-76 (1986).
Kaufman91. A. Kaufman, Volume Visualization, IEEE Computer Society Press Tutorial, Los Alamitos, CA (1991).
Kaufman93. A. Kaufman, L.M. Sobierajski, R.S. Avila, and T. He, "Navigation and Animation in a Volume Visualization System (invited paper)," pp. 64-74 in Models and Techniques in Computer Animation, ed. N.M. Thalmann and D. Thalmann, Springer-Verlag (1993).
Kaufman94. A.E. Kaufman and L.M. Sobierajski, "Continuum Volume Display," in Computer Visualization, ed. R. Gallagher, CRC Press (1994).
Kay86. T. L. Kay and J. T. Kajiya, "Ray Tracing Complex Scenes," Computer Graphics (Proc. SIGGRAPH) 20(4) pp. 268-278 (August 1986).
Krueger91. W. Krueger, "The Application of Transport Theory to Visualization of 3D Scalar Data Fields," Computers in Physics, pp. 397-406 (July/August 1991).
Kuo72. S. S. Kuo, Computer Applications Of Numerical Methods, Addison-Wesley (1972).
Laur91. D. Laur and P. Hanrahan, "Hierarchical Splatting: A Progressive Refinement Algorithm for Volume Rendering," Computer Graphics (Proc. SIGGRAPH) 25(4) pp. 285-288 (July 1991).
Levoy88. M. Levoy, "Display of Surfaces from Volume Data," IEEE Computer Graphics and Applications 8(5) pp. 29-37 (May 1988).
Levoy90. M. Levoy, H. Fuchs, S. M. Pizer, J. Rosenman, E. L. Chaney, G. W. Sherouse, V. Interrante, and J. Kiel, "Volume Rendering in Radiation Treatment Planning," Proceeding of the First Conference on Visualization in Biomedical Computing, pp. 4-10 IEEE Computer Society Press, (1990).
Levoy91. M. Levoy, "Viewing Algorithms," pp. 89-92 in Volume Visualization, ed. A. Kaufman, IEEE Computer Society Press Tutorial, Los Alamitos, CA (1991).
Lischinski93. D. Lischinski, F. Tampieri, and D. P. Greenberg, "Combining Hierarchical Radiosity and Discontinuity Meshing," Computer Graphics (Proc. SIGGRAPH), pp. 199-208 (August 1993).
Lorensen87. W. E. Lorensen and H. E. Cline, "Marching Cubes: A High Resolution 3D Surface

References

112

Construction Algorithm," Computer Graphics (Proc. SIGGRAPH) 21(4) pp. 163-169 (July 1987).
Love68. T. J. Love, Radiative Heat Transfer, Charles E. Merrilll Publishing Company, Columbus, Ohio (1968).
Malley88. T. J. V. Malley, "A Shading Method for Computer Generated Images," Master's Thesis, (1988).
Mammen89. A. Mammen, "Transparency and Antialiasing Algorithms Implemented with the Virtual Pixel Maps Technique," IEEE Computer Graphics and Applications 9(4) pp. 43-55 (July 1989).
Mitchell92. D. Mitchell and P. Hanrahan, "Illumination from Curved Reflectors," Computer Graphics (Proc. SIGGRAPH) 26(2) pp. 283-291 (July 1992).
Mohan88. R. Mohan, G. Barset, L. J. Brewster, C. S. Chui, G. J. Kutcher, J. S. Laughlin, and Z. Fuks, "A Comprehensive Three-Dimensional Radiation Treatment Planning System," International Journal of Radiation Oncology Biological Physics 15 pp. 481-495 (August 1988).
Nishita85. T. Nishita and E. Nakamae, "Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflection," Computer Graphics (Proc. SIGGRAPH) 19(3) pp. 23-30 (July 1985).
Press92. W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes in C, Cambridge University Press (1992).
Roth82. S. D. Roth, "Ray Casting for Modeling Solids," Computer Graphics and Image Processing 18(2) pp. 109-144 (February 1982).
Rushmeier87. H. E. Rushmeier and K. E. Torrance, "The Zonal Method For Calculating Light Intensities in the Presence of a Participating Medium," Computer Graphics (Proc. SIGGRAPH) 21(4) pp. 293-306 (July 1987).
Sabella88. P. Sabella, "A Rendering Algorithm for Visualizing 3D Scalar Fields," Computer Graphics (Proc. SIGGRAPH) 22(4) pp. 160-165 (August 1988).
Saec90. B. Le Saec and C. Schlick, "A Progressive Ray-Tracing based Radiosity with General Reflectance Functions," Proceedings of the Eurographics Workshop on Photosimulation, Realism and Physics in Computer Graphics, (June 1990).

References

113

Santek87. D. Santek, L. Leslie, B. Goodman, G. Diak, and G. Callan, "4-D Techniques for Evaluation of Atmospheric Model Forecasts," Proceeding Digital Image Processing and Visual Communications Technologies in Meteorology 846 pp. 75-77 (October 1987).
Screider66. Y. A. Screider, The Monte Carlo Method, Pergamon Press, New York (1966).
Shinya87. M. Shinya, T. Takahashi, and S. Naito, "Principles and Applications of Pencil Tracing," Computer Graphics (Proc. SIGGRAPH) 21(4) pp. 45-54 (July 1987).
Shinya89. M. Shinya, S. Naito, and T. Takahashi, "Rendering Techniques for Transparent Objects," Proceedings of Graphics Interface '89, pp. 173-182 (1989).
Shirley90. P. S. Shirley, "Physically Based Lighting Calculations For Computer Graphics," Ph.D. Thesis, University of Illinois at Urbana-Champaign (1990).
Siegel72. R. Siegel and J. R. Howell, Thermal Radiation Heat Transfer, McGraw-Hill, New York, New York (1972).
Sillion89. F. Sillion and C. Puech, "A General Two-Pass Method Integrating Specular and Diffuse Reflection," Computer Graphics (Proc. SIGGRAPH) 23(3) pp. 335-344 (July 1989).
Sillion91. F. Sillion, J. R. Arvo, S. H. Westin, and D. P. Greenberg, "A Global Illumination Solution for General Reflectance Distributions," Computer Graphics (Proc. SIGGRAPH) 25(4) pp. 187-196 (July 1991).
Sobierajski93. L. Sobierajski, D. Cohen, A. Kaufman, R. Yagel, and D. Acker, "Fast Display Method for Volumetric Data," The Visual Computer, (1993).
Sobierajski94. L.M. Sobierajski and A.E. Kaufman, "Volumetric Ray Tracing," 1994 Symposium on Volume Visualization, ACM Press, (October 1994).
Sparrow78. E. M. Sparrow and R. D. Cess, Radiation Heat Transfer, McGraw-Hill, New York, New York (1978).
Speray90. D. Speray and S. Kennon, "Volume Probes: Interactive Data Exploration on Arbitrary Grids," San Diego Workshop on Volume Visualization, Computer Graphics 24(5) pp. 5-12 (December 1990).
Stavroudis72. O. N. Stavroudis, The Optics of Rays, Wavefronts, and Caustics, Academic (1972).
Struik61. D. J. Struik, Lectures on Classical Differential Geometry, Dover Publications, New York

References

114

(1961).
Tiede87. U. Tiede, K.H. Hoehne, and M. Riemer, "Comparison of Surface Rendering Techniques for 3D Tomographics Objects," pp. 599-610 in Computer Assisted Radiology, ed. U. Lemke, Springer, Berlin Heidelberg New York (1987).
Tiede88. U. Tiede, M. Riemer, M. Bomans, and K.H. Hoehne, "Display Techniques for 3-D Tomographic Volume Data," Proceedings NCGA '88 Conference 3 pp. 188-197 (March 1988).
Troutman93. R. Troutman and N. Max, "Radiosity Algorithms Using Higher Order Finite Elements," Computer Graphics (Proc. SIGGRAPH), pp. 209-212 (August 1993).
Tuy84. H. K. Tuy and L. T. Tuy, "Direct 2-D Display of 3-D Objects," IEEE Computer Graphics and Applications 4(10) pp. 29-33 (October 1984).
Upson88. C. Upson and M. Keeler, "V-BUFFER: Visible Volume Rendering," Computer Graphics (Proc. SIGGRAPH) 22(4) pp. 59-64 (August 1988).
Vannier83. M. W. Vannier, J. L. Marsh, and J. O. Warren, "Three Dimensional Computer Graphics for Craniofacial Surgical Planning and Evaluation," Computer Graphics (Proc. SIGGRAPH) 17(3) pp. 263-273 (July 1983).
Vannier88. M. W. Vannier, F. R. Guiterrez, J. C. Laschinger, S. Gronemeyer, C. E. Canter, and R. H. Knapp, "Three-Dimensional Magnetic Resonance Imaging of Cogenital Heart Disease," Radiographics 8(5) pp. 857-873 (September 1988).
Wallace87. J. R. Wallace, M. F. Cohen, and D. P. Greenberg, "A Two-Pass Solution to the Rendering Equation: A Synthesis of Ray Tracing and Radiosity Methods," Computer Graphics (Proc. SIGGRAPH) 21(4) pp. 311-320 (July 1987).
Wallace89. J. R. Wallace, K. A. Elmquist, and E. A. Haines, "A Ray Tracing Algorithm for Progressive Radiosity," Computer Graphics (Proc. SIGGRAPH) 23(3) pp. 315-324 (July 1989).
Wang93. S.W. Wang and A.E. Kaufman, "Volume Sampled Voxelization of Geometric Primitives," Proceedings Visualization '93, pp. 78-84 (October 1993).
Wang94. S.W. Wang and A.E. Kaufman, "Volume-Sampled 3D Modeling," IEEE Computer Graphics and Applications, (September 1994).
Ward88. G. J. Ward, F. M. Rubinstein, and R. D. Clear, A Ray Tracing Solution for Diffuse Interreflection. August 1988.

References

115

Watkins70. G. S. Watkins, "A Real Time Visible Surface Algorithm," UTEC-CSc-70-101, NTIS AD-762 004, Computer Science Department, University of Utah, Salt Lake City, UT (June 1970).
Watt90. M. Watt, "Light-water Interaction Using Backward Beam Tracing," Computer Graphics (Proc. SIGGRAPH) 24(4) pp. 377-385 (August 1990).
Westover90. L. Westover, "Footprint Evaluation for Volume Rendering," Computer Graphics (Proc. SIGGRAPH) 24(4) pp. 367-376 (August 1990).
Whitted80. T. Whitted, "An Improved Illumination Model for Shaded Display," Communications of the ACM 23(6) pp. 343-349 (June 1980).
Wilhelms90. J. Wilhelms, J. Challinger, N. Alper, S. Ramamoorthy, and A. Vaziri, "Direct Volume Rendering of Curvilinear Volumes," San Diego Workshop on Volume Visualization, Computer Graphics 24(5) pp. 41-47 (December 1990).
Williams78. L. Williams, "Casting Curved Shadows on Curved Surfaces," Computer Graphics (Proc. SIGGRAPH), pp. 270-274 (1978).
Williams90. P.L. Williams, "Issues in Interactive Direct Projection Volume Rendering of Nonrectilinear Meshed Data Sets. Work in Progress Report,," San Diego Workshop on Volume Visualization, (December 1990).
Wolfe88. R. H. Wolfe and C. N. Liu, "Interactive Visualization of 3D Seismic Data: A Volumetric Method," IEEE Computer Graphics and Applications 8(7) pp. 24-30 (July 1988).
Wylie67. C. Wylie, G. W. Romney, D. C. Evans, and A. C. Erdahl, "Halftone Perspective Drawings by Computer," FJCC 67, pp. 49-58 Thompson Books, (1967).
Yagel91. R. Yagel, A. Kaufman, and Q. Zhang, "Realistic Volume Imaging," IEEE Visualization '90 Proceedings, pp. 226-231 IEEE Computer Society Press, (October 1991).
Yagel92a. R. Yagel, D. Cohen, and A. Kaufman, "Normal estimation in 3D discrete Space," The Visual Computer 8 pp. 278-291 (1992).
Yagel92b. R. Yagel, D. Cohen, and A. Kaufman, "Discrete Ray Tracing," IEEE Computer Graphics and Applications 12(5) pp. 19-28 (September 1992).
Yagel92c. R. Yagel and A. Kaufman, "Template-Based Volume Viewing," Proceedings Eurographics '92, pp. 153-167 (September 1992).

References

116

Yagel92d. R. Yagel, "High Quality Template-Based Volume Viewing," OSU-CISRC-10/92-TR28, (October 1992).
Yakowitz77. S. J. Yakowitz, "," in Computational Probability and Simulation, Addison-Wesley, New York (1977).
Zatz93. H. R. Zatz, "Galerkin Radiosity: A Higher Order Solution Method for Global Illumination," Computer Graphics (Proc. SIGGRAPH), pp. 213-220 (August 1993).

List of Figures

Figure 2.1: Spherical reconstruction kernel ................................................................. Figure 2.2: Elliptical reconstruction kernel ................................................................. Figure 2.3: 6-, 18-, and 26-connected paths ................................................................ Figure 2.4: Integration volume for V-Buffer volume rendering .................................. Figure 2.5: Discrete ray casting ................................................................................... Figure 3.1: Local illumination model .......................................................................... Figure 3.2: The form factor between two patches ....................................................... Figure 3.3: The hemisphere and hemicube methods ................................................... Figure 3.4: Possible light paths for illumination methods ........................................... Figure 3.5: Caustic paths ............................................................................................. Figure 4.1: The two intersection types ........................................................................ Figure 4.2: Intersection ordering for a point and a segment ........................................ Figure 4.3: Intersection ordering for two points .......................................................... Figure 4.4: Intersection pseudocode ............................................................................ Figure 4.5: The PARC acceleration technique ............................................................ Figure 4.6: A bullfrog ganglion rendered with PARC ................................................. Figure 4.7: A high-potential iron protein rendered with PARC .................................. Figure 4.8: Information passed to the shader .............................................................. Figure 4.9: A ray traced image of a high-potential iron protein .................................. Figure 4.10: A ray traced image of a hippocampal cell ............................................... Figure 4.11: A ray traced image of a fourth order function ........................................ Figure 4.12: A difference image for location and color .............................................. Figure 4.13: Volume sampled geometric primitives .................................................... Figure 5.1: Light traveling between two voxels .......................................................... Figure 5.2: Isotropic radiosity between two voxels ..................................................... Figure 5.3: Diffuse radiosity between a patch and a voxel .......................................... Figure 5.4: The basic iterative shooting algorithm ...................................................... Figure 5.5: A hierarchical model for a geometric object ............................................. Figure 5.6: Volume and surface elements .................................................................... Figure 5.7: A hierarchical model for a volumetric object ........................................... Figure 5.8: The hierarchical iterative shooting algorithm ........................................... Figure 5.9: The shoot routine ...................................................................................... Figure 5.10: Three routines for determining interaction level ..................................... Figure 5.11: A good surface approximation ................................................................ Figure 5.12: A bad surface approximation .................................................................. Figure 5.13: Radiosity contributions to the pixel value ...............................................

9 10 12 16 18 21 27 29 33 34 40 42 43 44 45 46 48 50 51 52 53 53 56 61 62 64 66 67 68 68 70 71 72 73 74 75

viii

Figure 5.14: Calculating the final pixel intensity values ............................................. Figure 5.15: An image of a diffuse sphere .................................................................. Figure 5.16: A low resolution image of a diffuse sphere ............................................ Figure 5.17: A distant image of a diffuse sphere ......................................................... Figure 5.18: An image of an isotropic sphere ............................................................. Figure 5.19: An image of a high potential iron protein ............................................... Figure 6.1: Illumination paths for the multipass method ............................................ Figure 6.2: The full radiosity and ray tracing solution ................................................ Figure 6.3: The results of diffuse and isotropic illumination ...................................... Figure 6.4: The specular reflection part of the solution ............................................... Figure 6.5: A light ray tracing image .......................................................................... Figure 6.6: Possible light paths for a combined method ............................................. Figure A.1: Basic building blocks of the abstract model ............................................ Figure A.2: Iconic representation of the World ........................................................... Figure A.3: Iconic representation of a Volume ............................................................ Figure A.4: The VolVis pipeline ..................................................................................

76 77 77 78 79 79 87 88 88 89 91 92 99 100 101 102

ix

List of Tables

Table 3.1: A comparison of basic global illumination models .................................... Table 4.1: Ray-object intersection classes ................................................................... Table 4.2: Statistical results for Figure 4.6 .................................................................. Table 4.3: Statistical results for Figure 4.7 .................................................................. Table 4.4: Statistical results for the intersection difference ......................................... Table 4.5: Statistical results for the color difference ................................................... Table 4.6: Statistical results for the rendering times ................................................... Table 5.1: Preprocessing statistics for the three scenes ............................................... Table 5.2: Interaction levels for the diffuse sphere scene ............................................ Table 5.3: Interaction levels for the isotropic sphere scene ......................................... Table 5.4: Interaction levels for the iron protein scene ................................................ Table 5.5: Image generation statistics .......................................................................... Table 6.1: Timing results for the multipass example ...................................................

31 40 47 49 54 55 55 80 81 81 82 82 90

x

Table of Contents

ABSTRACT ..............................................................................................................
Table of Contents .....................................................................................................
List of Figures ..........................................................................................................
List of Tables ............................................................................................................
Acknowledgments ....................................................................................................
Chapter 1: Introduction ..........................................................................................
Chapter 2: Volume Rendering ................................................................................ 2.1 Volume Rendering Terminology ...................................................................... 2.2 Surface Rendering Techniques ........................................................................ 2.3 Volume Rendering Techniques ........................................................................ 2.3.1 Object-Order Techniques ........................................................................... 2.3.2 Image-Order Techniques ........................................................................... 2.3.3 Hybrid Techniques ..................................................................................... 2.4 Volume Rendering Optimizations ................................................................... 2.5 Summary ..........................................................................................................
Chapter 3: Global Illumination Models ................................................................ 3.1 Local Illumination ........................................................................................... 3.2 Ray Tracing ..................................................................................................... 3.3 The Rendering Equation .................................................................................. 3.4 Radiosity .......................................................................................................... 3.5 Hybrid Illumination Techniques ...................................................................... 3.6 Indirect Specular Illumination ......................................................................... 3.7 Summary ..........................................................................................................
Chapter 4: Volumetric Ray Tracing ....................................................................... 4.1 Discrete Ray Tracing ....................................................................................... 4.2 The Illumination Model ................................................................................... 4.3 Intersection Calculations ................................................................................. 4.4 Intersection Ordering ....................................................................................... 4.5 Acceleration Techniques .................................................................................. 4.6 Illumination Evaluation ................................................................................... 4.7 Results ............................................................................................................. 4.8 Summary ..........................................................................................................

iii
vi
viii
x
xi
1
4 4 5 6 6 10 15 17 19
20 20 22 24 26 30 32 35
36 36 37 39 42 44 49 51 56

vi

Chapter 5: Volumetric Radiosity ........................................................................... 5.1 Intensity Variation ............................................................................................ 5.2 Radiosity Equations ......................................................................................... 5.3 Form Factors .................................................................................................... 5.4 Surface Elements ............................................................................................. 5.5 Radiosity Evaluation ........................................................................................ 5.6 Image Generation ............................................................................................ 5.7 Results ............................................................................................................. 5.8 Summary ..........................................................................................................
Chapter 6: Multipass Illumination Methods ........................................................ 6.1 Combining Volumetric Ray Tracing and Radiosity ......................................... 6.2 Indirect Specular Lighting Extension .............................................................. 6.3 Summary ..........................................................................................................
Chapter 7: Conclusion ............................................................................................ 7.1 Future Directions .............................................................................................
Appendix A: The VolVis System ............................................................................ A.1 Introduction ..................................................................................................... A.2 Abstract Model ............................................................................................... A.3 System Overview ............................................................................................ A.4 Manipulation ................................................................................................... A.5 Rendering ........................................................................................................ A.6 Implementation ............................................................................................... A.7 Future Directions ............................................................................................
References .................................................................................................................

57 58 59 60 63 65 74 76 83
84 84 89 93
94 95
97 97 98 101 103 104 104 105
106

vii

