Stochastic Attribute Value Grammars
Rob Malouf and Miles Osborne
ESSLLI'01 August, 2001 Helsinki, Finland

ËØÓ ×Ø ØØÖ ÙØ Î ĞÙ Ö ÑÑ Ö×
ÊÓ Å ĞÓÙ Ò Å Ğ × Ç× ÓÖÒ ÙÖÓÔ Ò ËÙÑÑ Ö Ë ÓÓĞ Ò ÄÓ ¸ Ä Ò Ù Ò ÁÒ ÓÖÑ Ø ÓÒ
¾¼ß¾ Ù Ù×Ø ¾¼¼½

ÍÒ ¬ Ø ÓÒ¹ × ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö ÓÖÑ Ğ ×Ñ× ×Ù × Ä Ü Ğ¹ ÙÒ Ø ÓÒ Ğ Ö Ñ¹

Ñ Ö Ò À ¹ Ö Ú Ò È Ö × ËØÖÙ ØÙÖ Ö ÑÑ Ö Ú ÔÖÓÚ Ò ØÓ

Ğİ ×Ù ×× ÙĞ ÓÖ

ÔÖ Ø Ğ Ğ Ö ¹× Ğ Ö ÑÑ Ö Ú ĞÓÔÑ ÒØº ÀÓÛ Ú Ö¸ Ö Ğ ×Ø ÔÔĞ Ø ÓÒ× Ó ØØÖ ÙØ ¹

Ú ĞÙ Ö ÑÑ Ö× ÓÖ Ò ØÙÖ Ğ Ğ Ò Ù Ô Ö× Ò ÓÖ Ò Ö Ø ÓÒ Ö ÕÙ Ö Ø Ù× Ó ×ÓÔ ×Ø Ø

×Ø Ø ×Ø Ğ Ø Ò ÕÙ × ÓÖ Ö ×ÓĞÚ Ò Ñ Ù Ø ×º Ì × ÓÒ ¹Û ÓÙÖ× Û ĞĞ ÔÖÓÚ Ò Ò¹

ØÖÓ Ù Ø ÓÒ ØÓ Ø Ñ Ü ÑÙÑ ÒØÖÓÔİ ÔÖ Ò ÔĞ Ò Ø ÓÒ×ØÖÙ Ø ÓÒ Ó Ñ Ü ÑÙÑ ÒØÖÓÔİ

ÑÓ Ğ× ÓÖ Ò ØÙÖ Ğ Ğ Ò Ù ÔÖÓ ×× Ò º Ì ÖÓÙ ÓÑ Ò Ø ÓÒ Ó Ğ ØÙÖ × Ò ¸ × ĞÓ Ğ

ÓÑÔÙØ Ò Ğ Ø × Ô ÖÑ Ø¸ Ò ×¹ÓÒ Ğ Ü Ö × ×¸ ×ØÙ ÒØ× Û ĞĞ ÒÚ ×Ø Ø Ø ÑÔĞ Ñ Ò¹

Ø Ø ÓÒ Ó Ñ Ü ÑÙÑ ÒØÖÓÔİ ÑÓ Ğ× ÓÖ ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö×¸ Ò ĞÙ Ò ×Ù ØÓÔ × ×

Ñ Ù Øİ ÒØ ¬ Ø ÓÒ¸ ØÙÖ × Ğ Ø ÓÒ¸ Ò ÑÓ Ğ ØÖ Ò Ò Ò Ú ĞÙ Ø ÓÒº

Ì × ÓÙÖ× Û ĞĞ ××ÙÑ × ÒÓÛĞ Ó ÔÖÓ Ğ Øİ Ø ÓÖİ¸ Ò ×ÓÑ ÜÔ Ö Ò

Ò Ö ÑÑ Ö Ú ĞÓÔÑ ÒØ ÓÖ ÔÖÓ Ö ÑÑ Ò Ò

Ğ Ú Ğ Ğ Ò Ù ÛÓÙĞ

ĞÔ ÙĞº

Ê Ò×

Ò İ¸ ËØ Ú Òº ½ º ËØ Ø ×Ø Ğ Ñ Ø Ó × Ò Ğ Ò Ù ×Ø ×º ÁÒ ÂÙ Ø ÃĞ Ú Ò× Ò È Ğ Ô Ê ×Ò ´ ×ºµ¸ Ì Ğ Ò Ò Øº Ì ÅÁÌ ÈÖ ××¸ Ñ Ö ¸ Å º ÔÔº ½ß¾ º

Ò İ¸ ËØ Ú Òº ½ º ËØÓ ×Ø ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö×º ÓÑÔÙØ Ø ÓÒ Ğ Ä Ò Ù ×Ø × ¾¿ ß ½ º

Ö Ö¸ Ñ¸ ËØ Ô Ò ĞĞ È ØÖ ¸ Ò Î Ò ÒØ ĞĞ È ØÖ º ½ º Ñ Ü ÑÙÑ ÒØÖÓÔİ ÔÔÖÓ ØÓ Ò ØÙÖ Ğ Ğ Ò Ù ÔÖÓ ×× Ò º ÓÑÔÙØ Ø ÓÒ Ğ Ä Ò Ù ×Ø × ¾¾ ¿ ß ½º

ÓÙÑ ¸ Ó×× ¸ ÖØ Ò Ú Ò ÆÓÓÖ ¸ Ò ÊÓ ÖØ Å ĞÓÙ º ÁÒ ÔÖ ××º ĞÔ ÒÓ Ï ÓÚ Ö ÓÑÔÙØ Ø ÓÒ Ğ Ò Ğİ× × Ó ÙØ º ÁÒ ÓÑÔÙØ Ø ÓÒ Ğ Ä Ò Ù ×Ø × Ò Ø Æ Ø ÖĞ Ò ×º

ĞĞ È ØÖ ¸ ËØ Ô Ò¸ Î Ò ÒØ ĞĞ È ØÖ ¸ Ò ÂÓ Ò Ä « ÖØİº ½ º ÁÒ Ù Ò ØÙÖ ×

Ó Ö Ò ÓÑ ¬ Ğ ×º Á

ÌÖ Ò× Ø ÓÒ× ÓÒ È ØØ ÖÒ Ò Ğİ× × Ò Å Ò ÁÒØ ĞĞ Ò

½ ¿ ¼ß¿ ¿º

ÂÓ Ò×ÓÒ¸ Å Ö ¸ ËØÙ ÖØ ×Ø Ñ ØÓÖ× ÓÖ ×ØÓ
ÒÒÙ Ğ Å Ø Ò Ó Ø
½º

Ñ Ò¸ ËØ Ô Ò ÒÓÒ¸ İ ¸ Ò ËØ ×Ø ÙÒ ¬ Ø ÓÒ¹ × Ö ÑÑ Ö×º ÁÒ ÈÖÓ
××Ó Ø ÓÒ ÓÖ ÓÑÔÙØ Ø ÓÒ Ğ Ä Ò Ù ×Ø × ´

Ò Ê ŞĞ Öº ½ º
Ò×Ó Ø ¿ Ø
Ä ½ µº ÔÔº ¿ ß

Ç× ÓÖÒ ¸ Å Ğ ×º ¾¼¼¼º ×Ø Ñ Ø ÓÒ Ó ×ØÓ ×Ø ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö× Ù× Ò Ò Ò ÓÖ¹ Ñ Ø Ú × ÑÔĞ º ÁÒ ÈÖÓ Ò × Ó Ø ½ Ø ÁÒØ ÖÒ Ø ÓÒ Ğ ÓÒ Ö Ò ÓÒ ÓÑÔÙØ Ø ÓÒ Ä Ò Ù ×Ø × ´ ÇÄÁÆ ¾¼¼¼µº

ÈÓĞĞ Ö ¸ ÖĞº ½ º Ä ØÙÖ × ÓÒ Ø ÓÙÒ Ø ÓÒ× Ó ÀÈË º ÍÒÔÙ Ğ × Ñ ÒÙ× Ö ÔØ Ç Ó ËØ Ø ÍÒ Ú Ö× Øİº

Statistical Methods and Linguistics
Steven Abney University of Tubingen
In: Judith Klavans and Philip Resnik, eds., The Balancing Act. MIT Press, Cambridge, MA. 1996.
abney@sfs.nphil.uni-tuebingen.de http: www.sfs.nphil.uni-tuebingen.de abney
Wilhelmstr. 113, 72074 Tubingen, Germany

Statistical Methods and Linguistics
Steven Abney University of Tubingen
In the space of the last ten years, statistical methods have gone from being virtually unknown in computational linguistics to being a fundamental given. In 1996, no one can profess to be a computational linguist without a passing knowledge of statistical methods. HMM's are as de rigeur as LR tables, and anyone who cannot at least use the terminology persuasively risks being mistaken for kitchen help at the ACL banquet.
More seriously, statistical techniques have brought signi cant advances in broad-coverage language processing. Statistical methods have made real progress possible on a number of issues that had previously stymied attempts to liberate systems from toy domains; issues that include disambiguation, error correction, and the induction of the sheer volume of information requisite for handling unrestricted text. And the sense of progress has generated a great deal of enthusiasm for statistical methods in computational linguistics.
However, this enthusiasm has not been catching in linguistics proper. It is always dangerous to generalize about linguists, but I think it is fair to say that most linguists are either unaware and unconcerned about trends in computational linguistics, or hostile to current developments. The gulf in basic assumptions is simply too wide, with the result that research on the other side can only seem naive, ill-conceived, and a complete waste of time and money.
In part the di erence is a di erence of goals. A large part of computational linguistics focuses on practical applications, and is little concerned with human language processing. Nonetheless, at least some computational linguists aim to advance our scienti c understanding of the human language faculty by better understanding the computational properties of language. One of the most interesting and challenging questions about human language computation is just how people are able to deal so e ortlessly with the very issues that make processing unrestricted text so di cult. Statistical methods provide the most promising current answers, and as a result the excitement about statistical methods is also shared by those in the cognitive reaches of computational linguistics.
In this paper, I would like to communicate some of that excitement to fellow linguists, or at least, perhaps, to make it comprehensible. There is no denying that there is a culture clash between theoretical and computational linguistics that serves to reinforce mutual prejudices. In charicature, computational linguists believe that by throwing more cycles and more raw text into their statistical black box, they can dispense with linguists altogether, along with their fanciful Rube Goldberg theories about exotic linguistic phenomena. The
I would like to thank Tilman Hoehle, Graham Katz, Marc Light, and Wolfgang Sternefeld for their comments on an earlier draft of this paper. All errors and outrageous opinions are of course my own.
1

linguist objects that, even if those black boxes make you oodles of money on speech recognizers and machine-translation programs which they don't, they fail to advance our understanding. I will try to explain how statistical methods just might contribute to understanding of the sort that linguists are after.
This paper, then, is essentially an apology, in the old sense of apology. I wish to explain why we would do such a thing as to use statistical methods, and why they are not really such a bad thing, maybe not even for linguistics proper. 1 Language Acquisition, Language Variation, and Language Change I think the most compelling, though least well-developed, arguments for statistical methods in linguistics come from the areas of language acquisition, language variation, and language change. Language acquisition. Under standard assumptions about the grammar, we would expect the course of language development to be characterized by abrupt changes, each time the child learns or alters a rule or parameter of the grammar. If, as seems to be the case, changes in child grammar are actually re ected in changes in relative frequencies of structures that extend over months or more, it is hard to avoid the conclusion that the child has a probabilistic or weighted grammar in some form. The form that would perhaps be least o ensive to mainstream sensibilities is a grammar in which the child tries out" rules for a time. During the trial period, both the new and old versions of a rule co-exist, and the probability of using one or the other changes with time, until the probability of using the old rule nally drops to zero. At any given point, in this picture, a child's grammar is a stochastic i.e., probabilistic grammar.
An aspect of this little illustration that bears emphasizing is that the probabilities are added to a grammar of the usual sort. A large part of what is meant by statistical methods" in computational linguistics is the study of stochastic grammars of this form: grammars obtained by adding probabilities in a fairly transparent way to algebraic" i.e., non-probabilistic grammars. Stochastic grammars of this sort do not constitute a rejection of the underlying algebraic grammars, but a supplementation. This is quite di erent from some uses to which statistical models most prominently, neural networks are put, in which attempts are made to model some approximation of linguistic behavior with an undi erentiated network, with the result that it is di cult or impossible to relate the network's behavior to a linguistic understanding of the sort embodied in an algebraic grammar. It should, however, be pointed out that the problem with such applications does not lie with neural nets, but with the unenlightening way they are put to use. Language change. Similarcomments apply, on a larger scale, to language change. If the units of change are as algebraic grammars lead us to expect|rules or pa-
2

rameters or the like|we would expect abrupt changes. We might expect some

poor bloke to go down an eel instead, because

to the local pub one evening, order the Great Vowel Shift happened to

Ale!", and be him a day too

esearrvlye.d1

In fact, linguistic changes that are attributed to rule changes or changes of pa-

rameter settings take place gradually, over considerable stretches of time, mea-

sured in decades or centuries. It is more realistic to assume that the language of

a speech community is a stochastic composite of the languages of the individual

speakers, described by a stochastic grammar. In the stochastic community"

grammar, the probability of a given construction re ects the relative proportion

of speakers who use the construction in question. Language change consists in

shifts in relative frequency of constructions rules, parameter settings, etc. in

the community. If we think of speech communities as populations of grammars

that vary within certain bounds, and if we think of language change as involv-

ing gradual shifts in the center of balance of the grammar population, then

statistical models are of immediate applicability 25 .

In this picture, we might still continue to assume that an adult monolingual

speaker possesses a particular algebraic grammar, and that stochastic gram-

mars are only relevant for the description of communities of varying grammars.

However, we must at least make allowance for the fact that individuals rou-

tinely comprehend the language of their community, with all its variance. This

rather suggests that at least the grammar used in language comprehension is

stochastic. I return to this issue below.

Language variation. There are two senses of language variation I have in mind here: dialectology, on the one hand, and typology, on the other. It is clear that some languages consist of a collection of dialects that blend smoothly one into the other, to the point that the dialects are more or less arbitrary points in a continuum. For example, Tait describes Inuit as a fairly unbroken chain of dialects, with mutual intelligibility limited to proximity of contact, the furthest extremes of the continuum being unintelligible to each other" 26, p.3 . To describe the distribution of Latin American native languages, Kaufman de nes a language complex as a geographically continuous zone that contains linguistic diversity greater than that found wthin a single language : : :, but where internal linguistic boundaries similar to those that separate clearly discrete languages are lacking" 14, p.31 . The continuousness of changes with geographic distance is consistent with the picture of a speech community with grammatical variance, as sketched above. With geographic distance, the mix of frequency of usage of various constructions changes, and a stochastic grammar of some sort is an appropriate model 15 .
Similar comments apply in the area of typology, with a twist. Many of the universals of language that have been identi ed are statistical rather than abso-
1I have read this anecdote somewhere before, but have been unable to nd the citation. My apologies to the unknown author.

3

lute, including rough statements about the probability distribution of language features  head-initial and head- nal languages are about equally frequent" or conditional probability distributions  postpositions in verb-initial languages are more common than prepositions in verb- nal languages" 11, 12 . There is as yet no model of how this probability distribution comes about, that is, how it arises from the statistical properties of language change. Which aspects of the distribution are stable, and which would be di erent if we took a sample of the world's languages 10,000 years ago or 10,000 years hence? There is now a vast body of mathematical work on stochastic processes and the dynamics of complex systems which includes, but is not exhausted by, work on neural nets, much of which is of immediate relevance to these questions.
In short, it is plausible to think of all of these issues|language acquisition, language change, and language variation|in terms of populations of grammars, whether those populations consist of grammars of di erent speakers or sets of hypotheses a language learner entertains. When we examine populations of grammars varying within bounds, it is natural to expect statistical models to provide useful tools. 2 Adult Monolingual Speakers But what about an adult monolingual speaker? Ever since Chomsky, linguistics has been rmly committed to the idealization to an adult monolingual speaker in a homogeneous speech community. Do statistical models have anything to say about language under that idealization?
In a narrow sense, I think the answer is probably not. Statistical methods bear mostly on all the issues that are outside the scope of interest of current mainstream linguistics. In a broader sense, though, I think that says more about the narrowness of the current scope of interest than about the linguistic importance of statistical methods. Statistical methods are of great linguistic interest because the issues they bear on are linguistic issues, and essential to an understanding of what human language is and what makes it tick. We must not forget that the idealizations that Chomsky made were an expedient, a way of managing the vastness of our ignorance. One aspect of language is its algebraic properties, but that is only one aspect of language, and certainly not the only important aspect. Also important are the statistical properties of language communities. And stochastic models are also essential for understanding language production and comprehension, particularly in the presence of variation and noise. I will focus here on comprehension, though considerations of language production have also provided an important impetus for statistical methods in computational linguistics 22, 23 .
To a signi cant degree, I think linguistics has lost sight of its original goal, and turned Chomsky's expedient into an end in itself. Current theoretical syntax gives a systematic account of a very narrow class of data, judgments about
4

the well-formedness of sentences for which the intended structure is speci ed, where the judgments are adjusted to eliminate gradations of goodness and other complications. Linguistic data other than structure judgments are classi ed as performance" data, and the adjustments that are performed on structurejudgment data are deemed to be corrections for performance e ects". Performance is considered the domain of psychologists, or at least, not of concern to linguistics.
The term performance suggests that the things that the standard theory abstracts away from or ignores are a natural class; they are data that bear on language processing but not language structure. But in fact a good deal that is labelled performance" is not computational in any essential way. It is more accurate to consider performance to be negatively de ned: it is whatever the grammar does not account for. It includes genuinely computational issues, but a good deal more that is not. One issue I would like to discuss in some detail is the issue of grammaticality and ambiguity judgments about sentences as opposed to structures. These judgments are no more or less computational than judgments about structures, but it is di cult to give a good account of them with grammars of the usual sort; they seem to call for stochastic, or at least weighted, grammars.
2.1 Grammaticality and ambiguity Consider the following: 1 a. the a are of I
b. the cows are grazing in the meadow c. John saw Mary The question is the status of these examples with respect to grammaticality and ambiguity. The judgments here, I think, are crystal clear: 1a is word salad, and 1b and c are unambiguous sentences. In point of fact, 1a is a grammatical noun phrase, and 1b and c are at least two ways ambiguous, the non-obvious reading being as a noun phrase. Consider: an are is a measure of area, as in a hectare is a hundred ares, and letters of the alphabet may be used as nouns in English  Written on the sheet was a single lowercase a," As described in section 2 paragraph b : : :". Thus 1a has a structure in which are and I are head nouns, and a is a modi er of are. This analysis even becomes perfectly natural in the following scenario. Imagine we are surveyors, and that we have mapped out a piece of land into large segments, designated with capital letters, and subdivided into one-are subsegments, designated with lower-case letters. Then the a are of I is a perfectly natural description for a particular parcel on our map. As for 1b, are is again the head noun, cows is a premodi er, and grazing in the meadow is a postmodi er. It might be objected that plural nouns cannot
5

be nominal premodi ers, but in fact they often are: consider the bonds mar-

ket, a securities exchange, he is vice president and media director, an in-home

health care services provider, Hartford's claims division, the nancial-services

industry, its line of systems management software. Several of these examples

are extracted from the Wall Street Journal.

It may seem that examples 1a and b are illustrative only of a trivial and

arti cial problem that arises because of a rare usage of a common word. But

the problem is not trivial: without an account of `rare usage', we have no way

of distinguishing between genuine ambiguities and these spurious ambiguities.

Alternatively, one might object that if one does not know that are has a reading

as a noun, then are is actually unambiguous in one's idiolect, and 1a is gen-

uinely ungrammatical. But in that case the question becomes why a hectare is

a hundred ares is not judged equally ungrammatical by speakers of the idiolect

in question.

Further, 1c illustrates that the rare usage is not an essential feature of

examples a and b. Saw has a reading as a noun, which may be less frequent

than the verb reading, but is hardly a rare usage. Proper nouns can modify

Gatling gun and be modi ed by Typhoid Mary common nouns. Hence,

John saw Mary has a reading as a noun phrase, referring to the Mary who is

associated with a kind of saw called a John saw.

It may be objected that constructions like Gatling gun and Typhoid Mary

belong to the lexicon, not the grammar, but however that may be, they are

completely productive. I may not know what Cohen equations, the Russia house,

or Abney sentences Russian house, or

are, but if not, those sentences

tohfenAtbhneeyd'esnaorteatsauroeflyCoehqeuna'lslyequunaftaiomnisl,iatrh.e2

Likewise I may not know who pegleg Pete refers to, or riverboat Sally, but that

does not make the constructions any less grammatical or productive.

The problem is epidemic, and it snowballs as sentences grow longer. One

often hears in computational linguistics about completely unremarkable sen-

tences with hundreds of parses, and that is in fact no exaggeration. Nor is it

merely a consequence of having a poor grammar. If one examines the undesired

analyses, one generally nds that they are extremely implausible, and often

do considerable violence to `soft' constraints like heaviness constraints or the

number and sequence of modi ers, but no one piece of the structure is outright

ungrammatical.

To illustrate, consider this sentence, drawn more or less at random from a

book Quine's Word and Object drawn more or less at random from my shelf:

2 In a general way such speculation is epistemologically relevant, as suggesting how organisms maturing and evolving in the physical environ-

2There are also syntactic grounds for doubt about the assumption that noun-noun modi cation belongs to the lexicon. Namely, adjectives can intervene between the modifying noun and the head noun. Examples are given later in this section. If adjective modi cation belongs to the syntax, and if there are no discontinuous words or movement of pieces of lexical items, then at least some modi cation of nouns by nouns must take place in the syntax.

6

ment we know might conceivably end up discoursing of abstract objects as we do. 28, p. 123 One of the many spurious structures this sentence might receive is the following: 3 S

PP AdjP

Absolute

In a general way RC epistemologically relevant PP organisms maturing and evolving ...

such speculation is

as suggesting how

NP VP

we know

NP

S VP

might AP

Ptcpl

objects as we do

conceivably end up discoursing of abstract

There are any number of criticisms one can direct at this structure, but I believe none of them are fatal. It might be objected that the PP-AdjP-Absolute sequence of sentential premodi ers is illegitimate, but each is individually ne, and there is no hard limit on stacking them. One can even come up with relatively good examples with all three modi ers, e.g.: PP on the beach AdjP naked as jaybirds Absolute waves lapping against the shore the wild boys carried out their bizarre rituals.
Another point of potential criticism is the question of licensing the elided sentence after how. In fact its content could either be provided from preceding context or from the rest of the sentence, as in though as yet unable to explain how, astronomers now know that stars develop from specks of grit in giant oysters.
Might is taken here as a noun, as in might and right. The AP conceivably end up may be a bit mysterious: end up is here an adjectival, as in we turned the box end up. Abstract is unusual as a mass noun, but can in fact be used as one, as for example in the article consisted of three pages of abstract and only two pages of actual text.
One might object that the NP headed by might is bad because of the multiple postmodi ers, but in fact there is no absolute constraint against stacking nominal postmodi ers, and good examples can be constructed with the same structure: marlinespikes, business end up, sprinkled with tabasco sauce, can be a powerful deterrent against pigeons. Even the commas are not absolutely required. The strength of preference for them depends on how heavy the modi ers are: cf. strength judicially applied increases the e ectiveness of diplomacy, a cup

7

of peanuts unshelled in the stock adds character.3

In short, the structure 3 seems to be best characterized as grammatical,

though it violates any number of parsing preferences and is completely absurd.

One might think that one could eliminate ambiguities by turning some of the

dispreferences into absolute constraints. But attempting to eliminate unwanted

readings that way is like squeezing a balloon: every dispreference that is turned

into an absolute constraint to eliminate undesired structures has the unfortunate

side e ect of eliminating the desired structure for some other sentence. No

matter how di cult it is to think up a plausible example that violates the

constraint, some writer has probably already thought one up by accident, and we

will improperly reject his sentence as ungrammatical if we turn the dispreference

into an absolute constraint. To illustrate: if a noun is premodi ed by both an

adjective and another noun, standard grammars require the adjective to come

rst, inasmuch as the noun adjoins to N0 but the adjective adjoins to N. It is

not easy to think reader would care

up to

tgryoobdefeoxraemrepaledsintghatthevieoxlaamteptlhesisincotnhsetrfaoionttn. otPee.r4haps

the

Not only is my absurd analysis 3 arguably grammatical, there are many,

many equally absurd analyses to be found. For example, general could be a

noun the army o cer instead of an adjective, or evolving in could be analyzed

as a particle verb, or the physical could be a noun phrase a physical exam|

not to mention various attachment ambiguities for coordination and modi ers,

giving a multiplicative e ect. The consequence is considerable ambiguity for a

sentence that is perceived to be completely unambiguous.

Now perhaps it seems I am being perverse, and I suppose I am. But it

is a perversity that is implicit in grammatical descriptions of the usual sort,

and it emerges unavoidably as soon as we systematically examine the structures

that the grammar assigns to sentences. Either the grammar assigns too many

structures to sentences like 2, or it incorrectly predicts that examples like three

pages of abstract or a cup of peanuts unshelled in the stock have no well-formed

structure.

To sum up, there is a problem with grammars of the usual sort: their predic-

tions about grammaticality and ambiguity are simply not in accord with human

perceptions. The problem of how to identify the correct structure from among

the in-principle possible structures provides one of the central motivations for

the use of weighted grammars in computational linguistics. A weight is as-

signed to each aspect of structure permitted by the grammar, and the weight of

a particular analysis is the combined weight of the structural features that make

it up. The analysis with the greatest weight is predicted to be the perceived

analysis for a given sentence.

3Cf. this passage from Tolkien: Their clothes were mended as well as their bruises their tempers and their hopes. Their bags were lled with food and provisions light to carry but stro4nMgatuonbderirncglitmheamticocvyecrletsh,eimceo-cuonrteaicnlimpaastsaelso.g"ica2l7,repco.6r1ds, a Kleene-star transitive closure, Precambrian era solar activity, highland igneous formations.

8

Before describing in more detail how weighted grammars contribute to a solution to the problem, though, let me address an even more urgent issue: is this even a linguistic problem? 2.2 Is this linguistics? Under usual assumptions, the fact that the grammar predicts grammaticality and ambiguity where none is perceived is not a linguistic problem. The usual opinion is that perception is a matter of performance, and that grammaticality alone does not predict performance; we must also include non-linguistic factors like plausibility and parsing preferences and maybe even probabilities. Grammaticality and acceptability. The implication is that perceptions of grammaticality and ambiguity are not linguistic data, but performance data. This stance is a bit odd|aren't grammaticality judgments perceptions? And what do we mean by performance data"? It would be one thing if we were talking about data that clearly has to do with the course of linguistic computation, data like response times and reading times, or regressive eye movement frequencies, or even more outlandish things like PET scans or ERP traces. But human perceptions judgments, intuitions about grammaticality and ambiguity are classic linguistic data. What makes the judgments concerning examples 1a-c performance data? All linguistic data is the result of little informal psycholinguistic experiments that linguists perform on themselves, and the experimental materials are questions of the form Can you say this?" Does this mean this?" Is this ambiguous?" Are these synonymous?"
Part of the answer is that the judgments about examples 1a-c are judgments about sentences alone rather than about sentences with speci ed structures. The usual sort of linguistic judgment is a judgment about the goodness of a particular structure, and example sentences are only signi cant as bearers of the structure in question. If any choice of words and any choice of context can be found that makes for a good sentence, the structure is deemed to be good. The basic data are judgments about structured sentences in context|that is, sentences plus a speci cation of the intended structure and intended context| but this basic data is used only grouped in sets of structured contextualized sentences having the same possibly partial structure. Such a set is de ned to be good just in case any structured contextualized sentence it contains is judged to be good. Hence a great deal of linguists' time is spent in trying to nd some choice of words and some context to get a clear positive judgment, in order to show that a structure of interest is good.
As a result, there is actually no intent that the grammar predict|that is, generate|individual structured sentence judgments. For a given structured sentence, the grammar only predicts whether there is some sentence with the same structure that is judged to be good.
For the examples 1, then, we should say that the structure
9

NP the N a N are PP of N I is indeed grammatical in the technical sense, since it is acceptable in at least one context, and since every piece of the structure is attested in acceptable sentences.
The grouping of data by structure is not the only way that standard grammars fail to predict acceptability and ambiguity judgments. Judgments are rather smoothly graded, but goodness according to the grammar is all or nothing. Discrepancies between grammar and data are ignored if they involve senteeneccetss,coornitnaigneinnegracleinfttehreeirmbbaeddndeisnsgc,apnabrseinagscprirbeefedretonceprvoiocelastsiionngsc,ogmarpdleexnitpya"t.h5 Grammar and computation. The di erence between structure judgments and string judgments is not that the former is competence data" in some sense and the latter is performance data". Rather, the distinction rests on a working assumption about how the data are to be explained, namely, that the data is a result of the interaction of grammatical constraints with computational constraints. Certain aspects of the data are assumed to be re ections of grammatical constraints, and everything else is ascribed to failures of the processor to translate grammatical constraints transparently into behavior, whether because of memory limits or heuristic parsing strategies or whatever obscure mechanisms create gradedness of judgments. We are justi ed in ignoring those aspects of the data that we ascribe to the idiosyncracies of the processor.
But this distinction does not hold up under scrutiny. Dividing the human language capacity into grammar and processor is only a manner of speaking, a way of dividing things up for theoretical convenience. It is naive to expect the logical grammar processor division to correspond to any meaningful physiological division|say, two physically separate neuronal assemblies, one functioning as a store of grammar rules and the other as an active device that accesses the grammar-rule store in the course of its operation. And even if we did believe in a physiological division between grammar and processor, we have no evidence at all to support that belief; it is not a distinction with any empirical content.
A couple of examples might clarify why I say that the grammar processor distinction is only for theoretical convenience. Grammars and syntactic structures are used to describe computer languages as well as human languages, but typical compilers do not access grammar-rules or construct parse-trees. At
5In addition, there are properties of grammaticality judgments of a di erent sort that are not being modelled, properties that are poorly understood and somewhat worrisome. Disagreements arise not infrequently among judges|it is more often the case than not that I disagree with at least some of the judgments reported in syntax papers, and I think my experience is not unusual. Judgments seem to change with changing theoretical assumptions: a sentence that sounds not too good" when one expects it to be bad may sound not too bad" if a change in the grammar changes one's expectations. And judgments change with exposure. Some constructions that sound terrible on a rst exposure improve considerably with time.
10

the level of description of the operation of the compiler, grammar-rules and

parse-trees exist only virtually" as abstract descriptions of the course of the

computation being performed. What is separately characterized as, say, gram-

mar versus parsing strategy at the logical level is completely intermingled at

the level of compiler operation.

At the other extreme, the constraints that probably have the strongest

computational avor are the parsing strategies that are considered to underly

garden-path e ects. But it is equally possible to characterize parsing prefer-

bpenreocecphsoarirtnaiocgntreaarlmizteomdtahbteiycadalesptseitgrhmniosnf.gthaFeocrsousebtxtatrmoeepstYleru,. ctTthuhereelsoowpoftiamtthtaealcfhsotmrrmuecnttuXrsiet+r1aistXetghiyeY

can
Z
one

with the least cost. Nothing depends on how trees are actually computed: the

characterization is only in terms of the shapes of trees.

If we wish to make a distinction between competence and computation, an

appropriate distinction is between what is computed and how it is computed.

By this measure, most performance" issues are not computational issues at

all. Characterizing the perceptions of grammaticality and ambiguity described

in the previous section does not necessarily involve any assumptions about the

computations done during sentence perception. It only involves characterizing

the set of structures that are perceived as belonging to a given sentence. That

can be done, for example, by de ning a weighted grammar that assigns costs

to trees, and specifying a constant C such that only structures whose cost is

within distance C of the best structure are predicted to be perceived. How the

set thus de ned is actually computed during perception is left completely open.

We may think of competence versus performance in terms of knowledge

versus computation, but that is merely a manner of speaking. What is really at

issue is an idealization of linguistic data for the sake of simplicity.

The frictionless plane, autonomy and isolation. Appeal is often made to an analogy between competence and frictionless planes in mechanics. Syntacticians focus on the data that they believe to contain the fewest complicating factors, and clean up" the data to remove what they believe to be remaining complications that obscure simple, general principles of language.
That is proper and laudable, but it is important not to lose sight of the original problem, and not to mistake complexity for irrelevancy. The test of whether the simple principles we think we have found actually have explanatory power is how well they fare in making sense of the larger picture. There is always the danger that the simple principles we arrive at are artifacts of our data selection and data adjustment. For example, it is sometimes remarked how marvelous it is that a biological system like language should be so discrete and clean, but in fact there is abundant gradedness and variability in the original data; the evidence for the discreteness and cleanness of language seems to be mostly evidence we ourselves have planted.

11

It has long been emphasized that syntax is autonomous. The doctrine is
older than Chomsky; for example, Tesniere writes : : :la syntaxe. Il est autonome" emphasis in the original. To illustrate that structure cannot be
equated with meaning, he presents the sentence pair: le signal vert indique le voie libre le symbole veritable impose le vitesse lissant
The similarity to Chomsky's later but more famous pair revolutionary new ideas appear infrequently colorless green ideas sleep furiously
is striking. But autonomy is not the same as isolation. Syntax is autonomous in the
sense that it cannot be reduced to semantics; well-formedness is not identical to meaningfulness. But syntax in the sense of an algebraic grammar is only one piece in an account of language, and it stands or falls on how well it ts into the larger picture.
The holy grail. The larger picture, and the ultimate goal of linguistics, is to describe language in the sense of that which is produced in language production, comprehended in language comprehension, acquired in language acquisition, and, in aggregate, that which varies in language variation and changes in language change.
I have always taken the holy grail of generative linguistics to be to characterize a class of models, each of which represents a particular potential or actual human language L, and characterizes a speaker of L by de ning the class of sentences a speaker of L produces, the structures that a speaker of L perceives for sentences; in short, by predicting the linguistic data that characterizes a speaker of L.
A Turing test" for a generative model would be something like the following. If we use the model to generate sentences at random, the sentences that are produced are judged by humans to be clearly sentences of the language|to sound natural". And in the other direction, if humans judge a sentence or nonsentence to have a particular structure, the model should also assign precisely that structure to the sentence.
Natural languages are such that these tests cannot be passed by an unweighted grammar. An unweighted grammar distinguishes only between grammatical and ungrammatical structures, and that is not enough. Sounding natural" is a matter of degree. What we must mean by randomly generating natural-sounding sentences" is that sentences are weighted by the degree to which they sound natural, and we sample sentences with a probability that accords with their weight. Moreover, the structure that people assign to a sentence is the structure they judge to have been intended by the speaker, and that
12

judgment is also a matter of degree. It is not enough for the grammar to de ne the set of structures that could possibly belong to the sentence; the grammar should predict which structures humans actually perceive, and what the relative weights are in cases where humans are uncertain about which structure the speaker intended.
The long and little of it is, weighted grammars and other species of statistical methods characterize language in such a way as to make sense of language production, comprehension, acquisition, variation, and change. These are linguistic, and not computational issues, a fact that is obscured by labelling everything performance" that is not accounted for by algebraic grammars. What is really at stake with competence" is a provisional simplifying assumption, or an expression of interest in certain subproblems of linguistics. There is certainly no indicting an expression of interest, but it is important not to lose sight of the larger picture.
3 How Statistics Helps Accepting that there are divergences between theory and data|for example, the divergence between predicted and perceived ambiguity|and accepting that this is a linguistic problem, and that it is symptomatic of the incompleteness of standard grammars, how does adding weights or probabilities help make up the di erence? Disambiguation. As already mentioned, the problem of identifying the correct parse|the parse that humans perceive|among the possible parses is a central application of stochastic grammars in computational linguistics. The problem of de ning which analysis is correct is not a computational problem, however; the computational problem is describing an algorithm to compute the correct parse. There are a variety of approaches to the problem of de ning the correct parse. A stochastic context-free grammar provides a simple illustration. Consider the sentence John walks, and the grammar 4 1. S ! NP V .7
2. S ! NP .3 3. NP ! N .8 4. NP ! N N .2 5. N ! John .6 6. N ! walks .4 7. V ! walks 1.0 According to grammar 4, John walks has two analyses, one as a sentence and one as a noun phrase. The rule S ! NP represents an utterance consisting of a single noun phrase. The numbers in the rightmost column represent the weights of rules. The weight of an analysis is the product of the weights of the rules used
13

in its derivation. In the sentential analysis of John walks, the derivation consists of rules 1, 3, 5, 7, so the weight is :7:8:61:0 = :336. In the noun-phrase analysis, the rules 2, 4, 5, 6 are used, so the weight is :3:2:6:4 = :0144. The weight for the sentential analysis is much greater, predicting that it is the one perceived. More re ned predictions can be obtained by hypothesizing that an utterance is perceived as ambiguous if the next-best case is not too much worse than the best. If not too much worse" is interpreted as a ratio of, say, not more than 2:1, we predict that John walks is perceived as unambiguous, as the ratio between the weights of the parses is 23:1.6 Degrees of grammaticality. Gradations of acceptability are not accommodated in algebraic grammars: a structure is either grammatical or not. The idea of degrees of grammaticality has been entertained from time to time, and some classes of ungrammatical structures are informally considered to be worse" than others most notably, ECP violations versus subjacency violations. But such degrees of grammaticality as have been considered have not been accorded a formal place in the theory. Empirically, acceptability judgments vary widely across sentences with a given structure, depending on lexical choices and other factors. Factors that cannot be reduced to a binary grammaticality distinction are either poorly modelled or ignored in standard syntactic accounts.
Degrees of grammaticality arise as uncertainty in answering the question Can you say X?" or perhaps more accurately, If you said X, would you feel you had made an error?" As such, they re ect degrees of error in speech production. The null hypothesis is that the same measure of goodness is used in both speech production and speech comprehension, though it is actually an open question. At any rate, the measure of goodness that is important for speech comprehension is not degree of grammaticality alone, but a global measure that combines degrees of grammaticality with at least naturalness and structural preference i.e., parsing strategies".
We must also distinguish degrees of grammaticality, and indeed, global goodness, from the probability of producing a sentence. Measures of goodness and probability are mathematically similar enhancements to algebraic grammars, but goodness alone does not determine probability. For example, for an in nite language, probability must ultimately decrease with length, though arbitrarily long sentences may be perfectly good.
Perhaps one reason that degrees of grammaticality have not found a place in standard theory is the question of where the numbers come from, if we permit continuous degrees of grammaticality. The answer to where the numbers come from is parameter estimation. Parameter estimation is well-understood for a
6The hypothesis that only the best structure or possibly, structures are perceptible is somewhat similar to current approaches to syntax in which grammaticality is de ned as optimal satisfaction of constraints or maximal economy of derivation. But I will not hazard a guess here about whether that similarity is signi cant or mere happenstance.
14

number of models of interest, and can be seen psychologically as part of what goes on during language acquisition. Naturalness. It is a bit di cult to say precisely what I mean by naturalness. A large component is plausibility, but not plausibility in the sense of world knowledge, but rather plausiblity in the sense of selectional preferences, that is, semantic sortal preferences that predicates place on their arguments.
Another important component of naturalness is not semantic, though, but simply how you say it". This is what has been called collocational knowledge, like the fact that one says strong tea and powerful car, but not vice versa 23 , or that you say thick accent in English, but starker Akzent  strong accent" in German.
Though it is di cult to de ne just what naturalness is, it is not di cult to recognize it. If one generates text at random from an explicit grammar plus lexicon, the shortcomings of the grammar are immediately obvious in the unnatural|even if not ungrammatical|sentences that are produced. It is also clear that naturalness is not at all the same thing as meaningfulness. For example, I think it is clear that di erential structure is more natural than di erential child, even though I could not say what a di erential structure might be. Or consider the following examples, that were in fact generated at random from a grammar: 5 a. matter-like, complete, alleged strips
a stratigraphic, dubious scattering a far alternative shallow model b. indirect photographic-drill sources earlier stratigraphically precise minimums Europe's cyclic existence All these examples are about on a par as concerns meaningfulness, but I think the b examples are rather more natural than the a examples. Collocations and selectional restrictions have been two important areas of application of statistical methods in computational linguistics. Questions of interest have been both how to include them in a global measure of goodness, and how to induce them distributionally 19 , both as a tool for investigations, and as a model of human learning. Structural preferences. Structural preferences, or parsing strategies, have already been mentioned. A longest-match" preference is one example. The example 6 the emergency crews hate most is domestic violence
15

is a garden-path because of a strong preference for the longest initial NP, the emergency crews, rather than the correct alternative, the emergency. The correct interpretation is: the emergency that crews hate most is domestic violence. The longest-match preference plays an important role in the dispreference for the structure 3 that we examined earlier.
As already mentioned, these preferences can be seen as structural preferences, rather than parsing preferences. They interact with the other factors we have been examining in a global measure of goodness. For example, in 6, an even longer match, the emergency crews hate, is actually possible, but it violates the dispreference for having plural nouns as nominal modi ers. Error tolerance. A remarkable property of human language comprehension is its error tolerance. Many sentences that an algebraic grammar would simply classify as ungrammatical are actually perceived to have a particular structure. A simple example is we sleeps, a sentence whose intended structure is obvious, albeit ungrammatical. In fact, an erroneous structure may actually be preferred to a grammatical analysis; consider 7 Thanks for all you help. which I believe is preferentially interpreted as an erroneous version of Thanks for all your help. However, there is a perfectly grammatical analysis: thanks for all those who you help.
We can make sense of this phenomenon by supposing that a range of errorcorrection operations are available, though their application imposes a certain cost. This cost is combined with the other factors we have discussed, to determine a global goodness, and the best analysis is chosen. In 7, the cost of error correction is apparently less than the cost of the alternative in unnaturalness or structural dispreference. Generally, error detection and correction are a major selling point for statistical methods. They were primary motivations for Shannon's noisy channel model 21 , which provides the foundation for many computational linguistic techniques. Learning on the y. Not only is the language that one is exposed to full of errors, it is produced by others whose grammars and lexica vary from one's own. Frequently, sentences that one encounters can only be analysed by adding new constructions or lexical entries. For example, when the average person hears a hectare is a hundred ares, they deduce that are is a noun, and succeed in parsing the sentence. But there are limits to learning on the y, just as there are limits to error correction. Learning on the y does not help one parse the a are of I.
Learning on the y can be treated much like error correction. The simplest approach is to admit a space of learning operations|e.g., assigning a new part of speech to a word, adding a new subcategorization frame to a verb, etc.|and
16

assign a cost to applications of the learning operations. In this way it is conceptually straightforward to include learning on the y in a global optimization.
People are clearly capable of error correction and learning on the y; they are highly desirable abilities given the noise and variance in the typical linguistic environment. They greatly exacerbate the problem of picking out the intended parse for a sentence, because they explode the candidate space even beyond the already large set of candidates that the grammar provides. To explain how it is nonetheless possible to identify the intended parse, there is no serious alternative to the use of weighted grammars. Lexical acquisition. A nal factor that exacerbates the problem of identifying the correct parse is the sheer richness of natural language grammars and lexica. A goal of earlier linguistic work, and one that is still a central goal of the linguistic work that goes on in computational linguistics, is to develop grammars that assign a reasonable syntactic structure to every sentence of English, or as nearly every sentence as possible. This is not a goal that is currently much in fashion in theoretical linguistics. Especially in GB, the development of large fragments has long since been abandoned in favor of the pursuit of deep principles of grammar.
The scope of the problem of identifying the correct parse cannot be appreciated by examining behavior on small fragments, however deeply analyzed. Large fragments are not just small fragments several times over|there is a qualitative change when one begins studying large fragments. As the range of constructions that the grammar accommodates increases, the number of undesired parses for sentences increases dramatically.
In-breadth studies also give a di erent perspective on the problem of language acquisition. When one attempts to give a systematic account of phrase structure, it becomes clear just how many little facts there are that do not fall out from grand principles, but just have to be learned. The simple, general principles in these cases are not principles of syntax, but principles of acquisition. Examples are the complex constraints on sequencing of prenominal elements in English, or the syntax of date expressions Monday June the 4th, Monday June 4, *Monday June the 4, *June 4 Monday or the syntax of proper names Greene County Sheri 's Deputy Jim Thurmond, or the syntax of numeral expressions.
The largest piece of what must be learned is the lexicon. If parametersetting views of syntax acquisition are correct, then learning the syntax which in this case does not include the low-level messy bits discussed in the previous paragraph is actually almost trivial. The really hard job is learning the lexicon.
Acquisition of the lexicon is a primary area of application for distributional and statistical approaches to acquisition. Methods have been developed for the acquisition of parts of speech 4, 20 , terminological noun compounds 1 , collocations 23 , support verbs 10 , subcategorization frames 2, 16 , selectional
17

restrictions 19 , and low-level phrase structure rules 7, 24 . These distributional techniques do not so much compete with parameter setting as a model of acquisition, as much as complement it, by addressing issues that parametersetting accounts pass over in silence. Distributional techniques are also not adequate alone as models of human acquisition|whatever the outcome of the syntactic versus semantic bootstrapping debate, children clearly do make use of situations and meaning to learn language|but the e ectiveness of distributional techniques indicates at least that they might account for a component of human language learning.
4 Objections There are a couple of general objections to statistical methods that may be lurking in the backs of readers minds, that I would like to address. First is the sentiment that, however relevant and e ective statistical methods may be, they are no more than an engineer's approximation, not part of a proper scienti c theory. Second is the nagging doubt: didn't Chomsky debunk all this ages ago?
4.1 Stochastic models are for engineers? One might admit that one can account for parsing preferences by a probabilistic model, but insist that a probabilistic model is at best an approximation, suitable for engineering but not for science. On this view, we do not need to talk about degrees of grammaticality, or preferences, or degrees of plausibility. Granted, humans perceive only one of the many legal structures for a given sentence, but the perception is completely deterministic. We need only give a proper account of all the factors a ecting the judgment.
Consider the example: Yesterday three shots were red at Humberto Calvados, personal assistant to the famous tenor Enrique Felicidad, who was in Paris attending to unspeci ed personal matters.
Suppose for argument's sake that 60 of readers take the tenor to be in Paris, and 40 take the assistant to be in Paris. Or more to the point, suppose a particular informant, John Smith, chooses the low attachment 60 of the time when encountering sentences with precisely this structure in the absence of an informative context, and low attachment 40 of the time. One could still insist that no probabilistic decision is being made, but rather that there are lexical and semantic di erences that we have inappropriately con ated across sentences with `precisely this structure', and if we take account of these other e ects, we end up with a deterministic model after all. A probabilistic model is only a stopgap in absence of an account of the missing factors: semantics, pragmatics, what topics I've been talking to other people about lately, how tired I am, whether I ate breakfast this morning.
18

By this species of argument, stochastic models are practically always a stopgap approximation. Take stochastic queue theory, for example, by which one can give a probabilistic model of how many trucks will be arriving at given depots in a transportation system. One could argue that if we could just model everything about the state of the trucks and the conditions of the roads, the location of every nail that might cause a at and every drunk driver that might cause an accident, then we could in principle predict deterministically how many trucks will be arriving at any depot at any time, and there is no need of stochastic queue theory. Stochastic queue theory is only an approximation in lieue of information that it is impractical to collect.
But this argument is awed. If we have a complex deterministic system, and if we have access to the initial conditions in complete detail, so that we can compute the state of the system unerringly at every point in time, a simpler stochastic description may still be more insightful. To use a dirty word, some properties of the system are genuinely emergent, and a stochastic account is not just an approximation, it provides more insight than identifying every deterministic factor. Or to use a di erent dirty word, it is a reductionist error to reject a successful stochastic account and insist that only a more complex, lower-level, deterministic model advances scienti c understanding. 4.2 Chomsky v. Shannon In one's introductory linguistics course, one learns that Chomsky disabused the eld once and for all of the notion that there was anything of interest to statistical models of language. But one usually comes away a little fuzzy on the question of what, precisely, he proved.
The arguments of Chomsky's that I know are from Three Models for the Description of Language" 5 and Syntactic Structures 6 essentially the same argument repeated in both places, and from the Handbook of Mathematical Psychology, chapter 13 17 . I think the rst argument in Syntactic Structures is the best known. It goes like this.
Neither a `colorless green ideas sleep furiously' nor b `furiously sleep ideas green colorless', nor any of their parts, has ever occured in the past linguistic experience of an English speaker. But a is grammatical, while b is not. This argument only goes through if we assume that if the frequency of a sentence or `part' is zero in a training sample, its probability is zero. But in fact, there is quite a literature on how to estimate the probabilities of events that do not occur in the sample, and in particular how to distinguish real zeros from zeros that just re ect something that is missing by chance. Chomsky also gives a more general argument:
19

If we rank the sequences of a given length in order of statistical approximation to English, we will nd both grammatical and ungrammatical sequences scattered throughout the list; there appears to be no particular relation between order of approximation and grammaticalness. Because for any n, there are sentences with grammatical dependencies spanning more than n words, so that no nth-order statistical approximation can sort out the grammatical from the ungrammatical examples. In a word, you cannot de ne grammaticality in terms of probability. It is clear from context that `statistical approximation to English' is a reference to nth-order Markov models, as discussed by Shannon. Chomsky is saying that there is no way to choose n and such that for all sentences s, grammaticals $ Pns where Pns is the probability of s according to the `best' nth-order approximation to English. But Shannon himself was careful to call attention to precisely this point: that for any n, there will be some dependencies a ecting the well-formedness of a sentence that an nth-order model does not capture. The point of Shannon's approximations is that, as n increases, the total mass of ungrammatical sentences that are erroneously assigned nonzero probability decreases. That is, we can in fact de ne grammaticality in terms of probability, as follows:
grammaticals $ limn!1 Pns 0
A third variant of the argument appears in the Handbook. There Chomsky states that parameter estimation is impractical for an nth-order Markov model where n is large enough to give a reasonable t to ordinary usage". He emphasizes that the problem is not just an inconvenience for statisticians, but renders the model untenable as a model of human language acquisition: we cannot seriously propose that a child learns the values of 109 parameters in a childhood lasting only 108 seconds."
This argument is also only partially valid. If it takes at least a second to estimate each parameter, and parameters are estimated sequentially, the argument is correct. But if parameters are estimated in parallel, say, by a highdimensional iterative or gradient-pursuit method, all bets are o . Nonetheless, I think even the most hardcore statistical types are willing to admit that Markov models represent a brute force approach, and are not an adequate basis for psychological models of language processing.
However, the inadequacy of Markov models is not that they are statistical, but that they are statistical versions of nite-state automata! Each of Chomsky's arguments turns on the fact that Markov models are nite-state, not on the fact that they are stochastic. None of his criticisms are applicable
20

to stochastic models generally. More sophisticated stochastic models do exist: stochastic context-free grammars are well understood, and stochastic versions of Tree-Adjoining Grammar 18 , GB 8 , and HPSG 3 have been proposed.
In fact, probabilities make Markov models more adequate than their nonprobabilistic counterparts, not less adequate. Markov models are surprisingly e ective, given their nite-state substrate. For example, they are the workhorse of speech recognition technology. Stochastic grammars can also be easier to learn than their non-stochastic counterparts. For example, though Gold 9 showed that the class of context-free grammars is not learnable, Horning 13 showed that the class of stochastic context-free grammars is learnable.
In short, Chomsky's arguments do not bear at all on the probabilistic nature of Markov models, only on the fact that they are nite-state. His arguments are not by any stretch of the imagination a sweeping condemnation of statistical methods. 5 Conclusion In closing, let me repeat the main line of argument as concisely as I can. Statistical methods|by which I mean primarily weighted grammars and distributional induction methods|are clearly relevant to language acquisition, language change, language variation, language generation, and language comprehension. Understanding language in this broad sense is the ultimate goal of linguistics.
The issues to which weighted grammars apply, particularly as concerns perception of grammaticality and ambiguity, one may be tempted to dismiss as performance issues. However, the set of issues labelled performance" are not essentially computational, as one is often led to believe. Rather, competence" represents a provisional narrowing and simpli cation of data in order to understand the algebraic properties of language. Performance" is a misleading term for everything else". Algebraic methods are inadequate for understanding many important properties of human language, such as the measure of goodness that permits one to identify the correct parse out of a large candidate set in the face of considerable noise.
Many other properties of language, as well, that are mysterious given unweighted grammars, properties such as the gradualness of rule learning, the gradualness of language change, dialect continua, and statistical universals, make a great deal more sense if we assume weighted or stochastic grammars. There is a huge body of mathematical techniques that computational linguists have begun to tap, yielding tremendous progress on previously intransigent problems. The focus in computational linguistics has admittedly been on technology. But the same techniques promise progress at long last on questions about the nature of language that have been mysterious for so long. The time is ripe to apply them.
21

References 1 Didier Bourigault. Surface grammatical analysis for the extraction of ter-
minological noun phrases. In COLING-92, Vol. III, pages 977 981, 1992. 2 Michael R. Brent. Automatic acquisition of subcategorization frames from
untagged, free-text corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 209 214, 1991. 3 Chris Brew. Stochastic HPSG. In Proceedings of EACL-95, 1995. 4 Eric Brill. Transformation-Based Learning. PhD thesis, Univ. of Pennsylvania, 1993. 5 Noam Chomsky. Three models for the description of language. IRE Transactions on Information Theory, IT-23:113 124, 1956. Institute of Radio Engineers, New York. 6 Noam Chomsky. Syntactic Structures. Mouton, 1957. 7 Steven Paul Finch. Finding Structure in Language. PhD thesis, University of Edinburgh, 1993. 8 Andrew Fordham and Matthew Crocker. Parsing with principles and probabilities. In The Balancing Act: Combining Symbolic and Statistical Approaches to Language, 1994. 9 E. Mark Gold. Language identi cation in the limit. Information and Control, 105:447 474, 1967. 10 Gregory Grefenstette. Corpus-based method for automatic identi cation of support verbs for nominalizations. In EACL-95, 1995. 11 John A. Hawkins. Word Order Universals. Academic Press, New York, 1983. 12 John A. Hawkins. A parsing theory of word order universals. Linguistic Inquiry, 212:223 262, 1990. 13 James Jay Horning. A Study of Grammatical Inference. PhD thesis, Stanford Computer Science, 1969. 14 Terrence Kaufman. The native languages of Latin America: general remarks. In Christopher Moseley and R.E. Asher, editors, Atlas of the World's Languages, pages 31 33. Routledge, London and New York, 1994. 15 Brett Kessler. Computational dialectology in Irish Gaelic. In EACL-95, 1995.
22

16 Christopher D. Manning. Automatic acquisition of a large subcategorization dictionary from corpora. In 31st Annual Meeting of the Association for Computational Linguistics, pages 235 242, 1993.
17 George A. Miller and Noam Chomsky. Finitary models of language users. In R.D. Luce, R. Bush, and E. Galanter, editors, Handbook of Mathematical Psychology, chapter 13. Wiley, New York, 1963.
18 Philip Resnik. Probabilistic Tree-Adjoining Grammar as a framework for statistical natural language processing. In COLING-92, pages 418 424, 1992.
19 Philip Resnik. Selection and Information. PhD thesis, University of Pennsylvania, Philadelphia, PA, 1993.
20 Hinrich Schutze. Part-of-speech induction from scratch. In 31st Annual Meeting of the Association for Computational Linguistics, pages 251 258, 1993.
21 Claude E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 273 4:379 423, 623 656, 1948.
22 Frank Smadja. Microcoding the lexicon for language generation. In Uri Zernik, editor, Lexical Acquisition: Using on-line resources to build a lexicon. MIT Press, 1989.
23 Frank Smadja. Extracting Collocations from Text. An Application: Language Generation. PhD thesis, Columbia University, New York, NY, 1991.
24 Tony C. Smith and Ian H. Witten. Language inference from function words. Manuscript, University of Calgary and University of Waikato, January 1993.
25 Whitney Tabor. The gradualness of syntactic change: A corpus proximity model. Ms. for Berkeley Colloquium talk, CSLI, Stanford University, November 1993.
26 Mary Tait. North America. In Christopher Moseley and R.E. Asher, editors, Atlas of the World's Languages, pages 3 30. Routledge, London and New York, 1994.
27 J.R.R. Tolkien. The Hobbit. Houghton Mi in Co., Boston, 1966. 28 Willard van Orman Quine. Word and Object. The MIT Press, Cambridge,
MA, 1960.
23

Stochastic Attribute-Value Grammars
Steven P. Abney
AT&T Laboratories
Probabilistic analogues of regular and context-free grammars are well-known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to de ne an adequate parameter-estimation algorithm.
In the present paper, I de ne stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and La erty, 1995. To estimate model parameters, it is necessary to compute the expectations of certain functions under random elds. In the application discussed by Della Pietra, Della Pietra, and La erty representing English orthographic constraints, Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm.
1. Introduction
Stochastic versions of regular grammars and context-free grammars have received a great deal of attention in computational linguistics for the last several years, and basic techniques of stochastic parsing and parameter estimation have been known for decades. However, regular and context-free grammars are widely deemed linguistically inadequate; standard grammars in computational linguistics are attribute-value AV grammars of some variety. Before the advent of statistical methods, regular and context-free grammars were considered too inexpressive for serious consideration, and even now the reliance on stochastic versions of the less-expressive grammars is often seen as an expedient necessitated by the lack of an adequate stochastic version of attribute-value grammars.
Proposals have been made for extending stochastic models developed for the regular
 AT&T Laboratories, Rm. A216, 180 Park Avenue, Florham Park, NJ 07932

Computational Linguistics

Volume 0, Number 0

and context-free cases to grammars with constraints.1 Brew, 1995 sketches a probabilis-

tic version of Head-Driven Phrase Structure Grammar HPSG. He proposes a stochastic

process for generating attribute-value structures, that is, directed acyclic graphs dags.

A dag is generated starting from a single node labelled with the unique most general

type. Each type S has a set of maximal subtypes T1; : : :; Tm. To expand a node labelled S,

one chooses a maximal subtype T stochastically. One then considers equating the current

node with other nodes of type T , making a stochastic yes no decision for each. Equating

two nodes creates a re-entrancy. If the current node is equated with no other node, one

proceeds to expand it. Each maximal type introduces types U1; : : :; Un, corresponding to

values of attributes; one creates a child node for each introduced type, and then expands

each child in turn. A limitation of this approach is that it permits one to specify only

the average rate of re-entrancies; it does not permit one to specify more complex context

dependencies.

Eisele, 1994 takes a logic-programming approach to constraint grammars. He as-

signs probabilities to proof trees by attaching parameters to logic program clauses. He

presents the following logic program as an example:

1. 2. 3. 4. 5.

pqqrraXXbX,,,,,Ybcde,Z 0000::::46551

qX,Y, . . . .

rY,Z.

The probability of a proof tree is de ned to be proportional to the product of the proba-

bilities of clauses used in the proof. Normalization is necessary, because some derivations

lead to invalid proof trees: for example, the derivation

1 I con ne my discussion here to Brew and Eisele because they aim to describe parametric models of probability distributions over the languages of constraint-based grammars, and to estimate the gpraarmammeatresrsboufttnhootsedimscoudseselsd. Opatrhaemr eatuetrheosrtsimhaavteioans.sOignneedapwperigohacths oorf pthreefleartetnecressotrot ctohnasttIrainntd-boafsed cpoanrtsitcrualianrt-ibnatseerdesgtriasmthmaatrosftShtaetfachnaRraiecztleerrizeRsitehzelelra,n1g9u9a6g,eswohfothdeesgcrraibmems aarwseaisghfutezdzylosgeitcs.foTrhis interpretation avoids the need for normalization that Brew and Eisele face, though parameter estimation still remains to be addressed.

2

Abney

Stochastic Attribute-Value Grammars

pX,Y,Z by,1 qX,Y rY,Z by,3 rc,Z : Y=c by,4 : Y=c b=c Z=d

is invalid because of the illegal assignment b = c. Both Brew and Eisele associate weights with analogues of rewrite rules. In Brew's
case, we can view type expansion as a stochastic choice from a nite set of rules of form X ! i, where X is the type to expand and each i is a sequence of introduced child types. A re-entrancy decision is a stochastic choice between two rules, X ! yes and X ! no, where X is the type of the node being considered for re-entrancy. In Eisele's case, expanding a goal term can be viewed as a stochastic choice among a nite set of rules X ! i, where X is the predicate of the goal term and each i is a program clause whose head has predicate X. The parameters of the models are essentially weights on such rules, representing the probability of choosing i when making a choice of type X.
In these terms, Brew and Eisele propose estimating parameters as the empirical relative frequency of the corresponding rules. That is, the weight of the rule X ! i is obtained by counting the number of times X rewrites as i in the training corpus, divided by the total number of times X is rewritten in the training corpus. For want of a standard term, let us call these estimates Empirical Relative Frequency ERF estimates. To deal with incomplete data, both Brew and Eisele appeal to the Expectation-Maximization EM algorithm, applied however to ERF rather than maximum likelihood estimates.
Under certain independence conditions, ERF estimates are maximum likelihood estimates. Unfortunately, these conditions are violated when there are context dependencies of the sort found in attribute-value grammars, as will be shown below. As a consequence, applying the ERF method to attribute-value grammars does not generally yield maximum likelihood estimates. This is true whether one uses EM or not|a method that yields the wrong" estimates on complete data does not improve when EM is used to extend the method to incomplete data.
3

Computational Linguistics

Volume 0, Number 0

Eisele identi es an important symptom that something is amiss with ERF estimates: the probability distribution over proof trees that one obtains does not agree with the frequency of proof trees in the training corpus. Eisele recognizes that this problem arises only where there are context dependencies.
Fortunately, solutions to the context-dependency problem have been described and indeed are currently enjoying a surge of interest in statistics, machine learning, and statistical pattern recognition, particularly image processing. The models of interest are known as random elds. Random elds can be seen as a generalization of Markov chains and stochastic branching processes. Markov chains are stochastic processes corresponding to regular grammars and random branching processes are stochastic processes corresponding to context-free grammars. The evolution of a Markov chain describes a line, in which each stochastic choice depends only on the state at the immediately preceding time-point. The evolution of a random branching process describes a tree in which a nitestate process may spawn multiple child processes at the next time-step, but the number of processes and their states depend only on the state of the unique parent process at the preceding time-step. In particular, stochastic choices are independent of other choices at the same time-step: each process evolves independently. If we permit re-entrancies, that is, if we permit processes to re-merge, we generally introduce context-sensitivity. In order to re-merge, processes must be in synch," which is to say, they cannot evolve in complete independence of one another. Random elds are a particular class of multi-dimensional random processes, that is, processes corresponding to probability distributions over an arbitrary graph. The theory of random elds can be traced back to Gibbs, 1902; indeed, the probability distributions involved are known as Gibbs distributions.
To my knowledge, the rst application of random elds to natural language was Mark et al., 1992. The problem of interest was how to combine a stochastic contextfree grammar with n-gram language models. In the resulting structures, the probability

4

Abney

Stochastic Attribute-Value Grammars

of choosing a particular word is constrained simultaneously by the syntactic tree in which it appears and the choices of words at the n preceding positions. The context-sensitive constraints introduced by the n-gram model are re ected in re-entrancies in the structure of statistical dependencies, e.g.:

S
NP VP
there was NP no response
In this diagram, the choice of label on a node z with parent x and preceding word y is dependent on the label of x and y, but conditionally independent of the label on any other node.
Della Pietra, Della Pietra, and La erty, 1995, henceforth, DD&L also apply random elds to natural language processing. The application they consider is the induction of English orthographic constraints|inducing a grammar of possible English words. DD&L describe an algorithm called Improved Iterative Scaling IIS for selecting informative features of words to construct a random eld, and for setting the parameters of the eld optimally for a given set of features, to model an empirical word distribution.
It is not immediately obvious how to use the IIS algorithm to equip attribute-value grammars with probabilities. In brief, the di culty is the following. The IIS algorithm requires the computation of the expectations, under random elds, of certain functions. In general, computing these expectations involves summing over all con gurations all possible character sequences, in the orthography application, which is not possible when the con guration space is large. Instead, DD&L use Gibbs sampling to estimate the needed expectations.
Gibbs sampling is possible for the application that DD&L consider. A prerequisite for Gibbs sampling is that the con guration space be closed under relabelling of graph

5

Computational Linguistics

Volume 0, Number 0

nodes. In the orthography application, the con guration space is the set of possible English words, represented as nite linear graphs labelled with ASCII characters. Every way of changing a label, that is, every substitution of one ASCII character for a di erent one, yields a possible English word.
By contrast, the set of graphs admitted by an attribute-value grammar G is highly constrained. If one changes an arbitrary node label in a dag admitted by G, one does not necessarily obtain a new dag admitted by G. Hence, Gibbs sampling is not applicable. However, I will show that a more general sampling method, the Metropolis-Hastings algorithm, can be used to compute the maximum-likelihood estimate of the parameters of AV grammars.

2. Stochastic Context-Free Grammars
Let us begin by examining stochastic context-free grammars SCFGs and asking why the natural extension of SCFG parameter estimation to attribute-value grammars fails. A point of terminology: I will use the term grammar to refer to an unweighted grammar, be it a context-free grammar or attribute-value grammar. A grammar equipped with weights and other periphenalia as necessary I will refer to as a model. Occasionally I will also use model to refer to the weights themselves, or the probability distribution they de ne.
Throughout we will use the following stochastic context-free grammar for illustrative purposes. Let us call the underlying grammar G1 and the grammar equipped with weights as shown, M1:
6

Abney

Stochastic Attribute-Value Grammars

1. S ! A A 2. S ! B

1 2

= =

1=2 1=2

3. A ! a 4. A ! b

3 4

= =

2=3 1=3

1

5. B ! a a 6. B ! b b

5 6

= =

1=2 1=2

The probability of a given tree is computed as the product of probabilities of rules used

in it. For example:

1
S

AA

3 a

a 3

2

Let x be tree 2 and let q1 be the probability distribution over trees de ned by model M1. Then:

q1x =

1

3

3

=

1 2



2 3



2 3

=

2 9

In parsing, we use the probability distribution q1x de ned by model M1 to disambiguate: the grammar assigns some set of trees fx1; : : :; xng to a sentence , and we choose that tree xi that has greatest probability q1xi. The issue of e ciently computing the most-probable parse for a given sentence has been thoroughly addressed in the literature. The standard parsing techniques can be readily adapted to the random- eld models to be discussed below, so I simply refer the reader to the literature. Instead, I concentrate on parameter estimation, which for attribute-value grammars cannot be accomplished by standard techniques.
By parameter estimation we mean determining values for the weights . In order for a stochastic grammar to be useful, we must be able to compute the correct weights, where by correct weights we mean the weights that best account for a training corpus.

7

Computational Linguistics

Volume 0, Number 0

The degree to which a given set of weights accounts for a training corpus is measured by the similarity between the distribution qx determined by the weights and the distribution of trees x in the training corpus.

2.1 The Goodness of a Model
The distribution determined by the training corpus is known as the empirical distribution. For example, suppose we have a training corpus containing twelve trees of the following four types from LG1:

x1 S
AA
aa c = 4x ~p = 4/12

x2 S AA
bb 2x 2/12

x3 x4 SS

B
aa
3x 3/12

B
bb
3x = 12 3/12

3

where cx is the count of how often the tree type x appears in the corpus, and p~ is the empirical distribution, de ned as:

p~x

=

cx N

N = X cx
x

In comparing a distribution q to the empirical distribution p~, we shall actually mea-

sure dissimilarity rather than similarity. Our measure for dissimilarity of distributions is

the Kullback-Leibler KL divergence, de ned as:

Dp~j

q

=

X x

p~x

ln

p~x qx

The divergence between p~ and q at point x is the log of the ratio of p~x to qx. The

overall divergence between p~ and q is the average divergence, where the averaging is over

tree tokens in the corpus; i.e., point divergences ln p~x=qx are weighted by p~x and

summed.

8

Abney

Stochastic Attribute-Value Grammars

For example, let q1 be, as before, the distribution determined by model M1. The following table shows q1, p~, the ratio q1x=p~x, and the weighted point divergence p~x lnp~x=q1x. The sum of the fourth column is the KL divergence Dp~jq1 between p~ and q1. The third column contains q1x=p~x rather than p~x=q1x so that one can see at a glance whether q1x is too large  1 or too small  1.

q1 p~ q1=p~ p~lnp~=q1

xx21

29 1 18

13 16

0.67 0.33

0.14 0.18

x3 1 4 1 4 1.00 0.00

x4 1 4

1 4 1.00 0.00
0.32

The total divergence Dp~jq1 = 0:32.

4

One set of weights is better than another if its divergence from the empirical distribu-

tion is less. For example, let us consider a di erent set of weights for grammar G1. Let M0

be G1 with weights 1=2; 1=2; 1=2; 1=2; 1=2; 1=2, and let q0 be the probability distribution

determined by M0. Then the computation of the KL divergence is as follows:

q0 p~ q0=p~ p~lnp~=q0

x1 1 8 1 3 0.38 0.33

x2 1 8 1 6 0.75 0.05

xx43

14 14

14 14

1.00 1.00

0.00 0.00
0.38

The t for x2 improves, but that is more than o set by a poorer t for x1. The distribution

q1 is a better distribution than q0, in the sense that q1 is more similar less dissimilar to

the empirical distribution than q0 is.

One reason for adopting minimal KL divergence as a measure of goodness is that

minimizing KL divergence maximizes likelihood. The likelihood of distribution q is the

probability of the training corpus according to q:

Lq

= =

Q Qx
x

qinxtracinxing

qx

Since log is monotone increasing, maximizing likelihood is equivalent to maximizing log

likelihood:

9

Computational Linguistics

Volume 0, Number 0

ln Lq

= =

PN xPcxxp~lxnqlnxqx

The expression on the right hand side is -1 N times the cross entropy of q with respect to p~, hence maximizing log likelihood is equivalent to minimizing cross entropy. Finally, Dp~jq is equal to the cross entropy of q less the entropy of p~, and the entropy of p~ is constant with respect to q; hence minimizing cross entropy maximizing likelihood is equivalent to minimizing divergence.

2.2 The ERF Method
For stochastic context-free grammars, it can be shown that the ERF method yields the best model for a given training corpus. First, let us introduce some terminology and notation. With each rule i in a stochastic context-free grammar is associated a weight i and a function fix that returns the number of times rule i is used in the derivation of tree x. For example, consider tree 2, repeated here for convenience:

1
S

AA

3 a

a 3

Rule 1 is used once and rule 3 is used twice; accordingly f1x = 1, f3x = 2, and

fix = 0 for i 2 f2; 4; 5; 6g.

We use the notation p f to represent the expectation of f under probability dis-

tribution p; that is, p f

=

P x

pxfx.

The

ERF

method

instructs

us

to

choose

the

weight i for rule i proportional to its empirical expectation p~ fi . Algorithmically, we

compute the expectation of each rule's frequency, and normalize among rules with the

same lefthand side.

10

Abney

Stochastic Attribute-Value Grammars

To illustrate, let us consider corpus 3 again. The expectation of each rule frequency fi is a sum of terms p~xfix. These terms are shown for each tree, in the following table.

S!AA S!B A!a A!b B!aa B!bb

p~ p~f1 p~f2 p~f3 p~f4 p~f5 p~f6

x1 S A a A a 1 3 1 3

23

x2 S A b A b 1 6 1 6

26

xx34

S S

B B

a b

a b

14 14

14 14

14 14

p~ f = 1 2 1 2 2 3 1 3 1 4 1 4

= 12 12 23 13 12 12

For example, in tree x1, rule 1 is used once and rule 3 is used twice. The empirical probability of x1 is 1 3, so x1's contribution to p~ f1 is 1=3  1, and its contribution to p~ f3 is 1=3  2. The weight i is obtained from p~ fi by normalizing among rules with the same lefthand side. For example, the expected rule frequencies p~ f1 and p~ f2 of rules with lefthand side S already sum to 1, so they are adopted without change as 1 and 2. On the other hand, the expected rule frequencies p~ f5 and p~ f6 for rules with lefthand side B sum to 1 2, not 1, so they are doubled to yield weights 5 and 6. It should be observed that the resulting weights are precisely the weights of model M1.
It can be proven that the ERF weights are the best weights for a given context-

free grammar, in the sense that they de ne the distribution that is most similar to the empirical distribution. That is, if are the ERF weights for a given grammar, de ning distribution q, and 0 de ning q0 is any set of weights such that q =6 q0, then Dp~jq Dp~jq0.
One might expect the best weights to yield Dp~jq = 0, but such is not the case.

We have just seen, for example, that the best weights for grammar G1 yield distribution q1, yet Dp~jq1 = 0:32 0. A closer inspection of the divergence calculation 4 reveals that q1 is sometimes less than p~, but never greater than p~. Could we improve the t by increasing q1? For that matter, how can it be that q1 is never greater than p~? As

11

Computational Linguistics

Volume 0, Number 0

probability distributions, q1 and p~ should have the same total mass, namely, one. Where is the missing mass for q1?
The answer is of course that q1 and p~ are probability distributions over LG, but not all of LG appears in the corpus. Two trees are missing, and they account for the missing mass. These two trees are:

S AA ab

S AA ba

5

Each of these trees has probability 0 according to p~ hence they can be ignored in the divergence calculation, but probability 1=9 according to q1.
Intuitively, the problem is this. The distribution q1 assigns too little weight to trees x1 and x2, and too much weight to the missing" trees 5; call them x5 and x6. Yet exactly the same rules are used in x5 and x6 as are used in x1 and x2. Hence there is no way to increase the weight for trees x1 and x2, improving their t to p~, without simultaneously increasing the weight for x5 and x6, making their t to p~ worse. The distribution q1 is the best compromise possible.
To say it another way, our assumption that the corpus was generated by a contextfree grammar means that any context dependencies in the corpus must be accidental, the result of sampling noise. There is indeed a dependency in corpus 3: in the trees where there are two A's, the A's always rewrite the same way. If corpus 3 was generated by a stochastic context-free grammar, then this dependency is accidental.
This does not mean that the context-free assumption is wrong. If we generate twelve trees at random from q1, it would not be too surprising if we got corpus 3. More extremely, if we generate a random corpus of size 1 from q1, it is quite impossible for the resulting empirical distribution to match the distribution q1. But as the corpus size

12

Abney

Stochastic Attribute-Value Grammars

increases, the t between p~ and q1 becomes ever better.

3. Attribute-Value Grammars

But what if the dependency in corpus 3 is not accidental? What if we wish to adopt

a grammar that imposes the constraint that both A's rewrite the same way? We can

impose such a constraint by means of an attribute-value grammar.

We may formalize an attribute-value grammar as a context-free grammar with at-

tribute labels and path equations. An example is the following grammar; let us call it

G2:

1. S ! 1:A 2:A 2. S ! 1:B 3. A ! 1:a 4. A ! 1:b 5. B ! 1:a 6. B ! 1:b

11 = 21

The following illustrates how a dag is generated from G2.

G2

SS

S

12

12

12

S 1A A 3A A 3 A A

11

11 a

11 a

(a) (b)

(c)

(d)

We begin in a with a single node labelled with the start category of G2, namely, S. A

node x is expanded by choosing a rule that rewrites the category of x. In this case, we

choose rule 1 to expand the root node. Rule 1 instructs us to create two children, both

labelled A. The edge to the rst child is labelled 1" and the edge to the second child

is labelled 2". The constraint 1 1 = 2 1 " indicates that the 1" child of the

1" child of x is identical to the 1" child of the 2" child of x. We create an unlabelled

node to represent this grandchild of x and direct appropriately labelled edges from the

children, yielding b.

We proceed to expand the newly introduced nodes. We choose rule 3 to expand the

13

Computational Linguistics

Volume 0, Number 0

rst A" node. In this case, a child with edge labelled 1" already exists, so we use it rather than creating a new one. Rule 3 instructs us to label this child a", yielding c. Now we expand the second A" node. Again we choose rule 3. We are instructed to label the 1" child a", but it already has that label, so we do not need to do anything. Finally, in d, the only remaining node is the bottommost node, labelled a". Since its label is a terminal category, it does not need to be expanded, and we are done.
Let us back up to c again. Here we were free to choose rule 4 instead of rule 3 to expand the righthand A" node. Rule 4 instructs us to label the 1" child b", but we cannot, inasmuch as it is already labelled a". The derivation fails, and no dag is generated.
The language LG2 is the set of dags produced by successful derivations, namely:

x1 S AA
a

x2 S AA
b

x3 S
B a

x4 S
B b

6

The edges of the dags should actually be labelled with 1's and 2's, but I have suppressed the edge labels for the sake of perspicuity.

3.1 AV Grammars and the ERF Method
Now we face the question of how to attach probabilities to grammar G2. The natural extension of the method we used for context-free grammars is the following. Associate a weight with each of the six rules of grammar G2. For example, let M2 be the model consisting of G2 plus weights  1; : : :; 6 = 1=2; 1=2; 2=3; 1=3; 1=2; 1=2. Let 2x be the weight that M2 assigns to dag x; it is de ned to be the product of the weights of the rules used to generate x. For example, the weight 2x1 assigned to tree x1 of 6 is 2=9, computed as follows:

14

Abney

Stochastic Attribute-Value Grammars

1
S x1= A A
3 a 3
Rule 1 is used once and rule 3 is used twice; hence 2x1 = 1 3 3 = 1=22=32=3 = 2=9. Observe that 2x1 = 1 32, which is to say, 1f1x1 3f3x1. Moreover, since 0 = 1,
it does not hurt to include additional factors ifix1 for those i where fix1 = 0. That is, we can de ne the dag weight corresponding to rule weights =  1; : : :; n generally as:
x = Yn ifix
i=1
The next question is how to estimate weights. Let us consider what happens when
we use the ERF method. Let us assume a corpus distribution for the dags 6 analogous
to the distribution in 3:

x1 x2 x3 x4 p~ = 1=3 1=6 1=4 1=4 Using the ERF method, we estimate rule weights as follows:

7

p~ p~f1 p~f2 p~f3 p~f4 p~f5 p~f6

xxxx2431

13 16 14 14

13 16

14 14

23

26

14

14

p~ f = 1 2 1 2 2 3 1 3 1 4 1 4

= 12 12 23 13 12 12

8

This table is identical to the one given earlier in the context-free case. We arrive at the

same weights M2 we considered above, de ning dag weights 2x.

3.2 Why the ERF Method Fails
But at this point a problem arises: 2 is not a probability distribution. Unlike in the context-free case, the four dags in 6 constitute the entirety of LG. This time, there

15

Computational Linguistics

Volume 0, Number 0

are no missing dags to account for the missing probability mass. There is an obvious x" for this problem: we can simply normalize 2. We might
de ne the distribution q for an AV grammar with weight function as:

qx

=

1 Z

x

where Z is the normalizing constant:

Z = X x
x2LG
In particular, for 2, we have Z = 2=9 + 1=18 + 1=4 + 1=4 = 7=9. Dividing 2 by 7 9 yields the ERF distribution:

q2x = 2x=17 1x=124 9x=238 9x=248
On the face of it, then, we can transplant the methods we used in the context-free case to the AV case and nothing goes wrong. The only problem that arises  not summing to one has an obvious x normalization.
However, something has actually gone very wrong. The ERF method yields the best weights only under certain conditions that we inadvertently violated by changing LG and re-apportioning probability via normalization. In point of fact, we can easily see that the ERF weights 8 are not the best weights for our example grammar. Consider the alternative model M given in 9, de ning probability distribution q:

S 36!++22ppA22A

S!B
6+23p2

A1+p!p2 2a

A!b
1+1p2

B!a
21

B!b
21

9

These weights are proper, in the sense that weights for rules with the same lefthand side sum to one. The reader can verify that  sums to Z = 3+3p2 and that q is:

qx = 1x=13 1x=26 1x=34 1x=44

16

Abney

Stochastic Attribute-Value Grammars

That is, q = p~. Comparing q2 the ERF distribution and q to p~, we observe that Dp~jq2 = 0:07 but Dp~jq = 0.
In short, in the AV case, the ERF weights do not yield the best weights. This means that the ERF method does not converge to the correct weights as the corpus size increases. If there are genuine dependencies in the grammar, the ERF method converges systematically to the wrong weights. Fortunately, there are methods that do converge to the right weights. These are methods that have been developed for random elds.

4. Random Fields

A random eld de nes a probability distribution over a set of labelled graphs called con gurations. In our case, the con gurations are the dags generated by the grammar, i.e., = LG. The weight assigned to a con guration is the product of the weights assigned to selected features of the con guration. We use the notation:
x = Y ifix
i
where i is the weight for feature i and fi is its frequency function, that is, fix is the number of times that feature i occurs in con guration x. For most purposes, a feature can be identi ed with its frequency function; I will not always make a careful distinction between them.
I use the term feature here as it is used in the machine learning and statistical pattern recognition literature, not as in the constraint grammar literature, where feature is synonymous with attribute. In my usage, dag edges are labelled with attributes, not features. Features are rather like geographic features of dags: a feature is some larger or smaller piece of structure that occurs|possibly at more than one place|in a dag.
The probability of a con guration that is, a dag is proportional to its weight, and is obtained by normalizing the weight distribution.

17

Computational Linguistics

Volume 0, Number 0

qx = Z1

x

Z

=

P x2

x

If we identify the features of a con guration with local trees|equivalently, with applications of rewrite rules|the random eld model is almost identical to the model we considered in the previous section. There are two important di erences. First, we no longer require weights to sum to one for rules with the same lefthand side. Second, the model does not require features to be identi ed with rewrite rules. We use the grammar to de ne the set of con gurations = LG, but in de ning a probability distribution over LG, we can choose features of dags however we wish.
Let us consider an example. Let us continue to assume grammar G2 generating language 6, and let us continue to assume the empirical distribution 7. But now rather than taking rule applications to be features, let us adopt the following two features:

A

1. 1

2. B

a

For purpose of illustration, take feature 1 to have weight 1 = p2 and feature 2 to have

weight 2 = 3=2. The functions f1 and f2 represent the frequencies of features 1 and 2,

respectively:

S

1

1A

A

S AA

S 2B

S 2B

a

f1 = 2

f2 = =

p20 p2

q= 1 3

b ab
0 00 0 11 1 3 2 3 2 Z=6 16 14 14

We are able to exactly recreate the empirical distribution using fewer features than before.

Intuitively, we need only use as many features as are necessary to distinguish among trees

that have di erent empirical probabilities.

18

Abney

Stochastic Attribute-Value Grammars

This added exibility is welcome, but it does make parameter estimation more involved. Now we must not only choose values for weights, we must also choose the features that weights are to be associated with. We would like to do both in a way that permits us to nd the best model, in the sense of the model that minimizes the Kullback-Leibler distance with respect to the empirical distribution. The IIS algorithm Della Pietra, Della Pietra, and La erty, 1995 provides a method to do precisely that.

5. Field Induction

In outline, the IIS algorithm is as follows:
1.Start t = 0 with the null eld, containing no features.
2.Feature Selection. Consider every feature that might be added to eld Mt
and choose the best one.
3.Weight Adjustment. Readjust weights for all features. The result is eld
Mt+1.
4.Iterate until the eld cannot be improved.
For the sake of concreteness, let us take features to be labelled subdags. In step 2 of the algorithm we do not consider every conceivable labelled subdag, but only the atomic i.e., single-node subdags and those complex subdags that can be constructed by combining features already in the eld or by combining a feature in the eld with some atomic feature. We also limit our attention to features that actually occur in the training corpus.
In our running example, the atomic features are:

SA B a b
Features can be combined by adding connecting arcs. For example:

19

Computational Linguistics

Volume 0, Number 0

A A+ a =
a

S S+A =
A

SS +A=
A AA

5.1 The Null Field

Field induction begins with the null eld. With the corpus we have been assuming, the

null eld takes the following form.

S S SS

AA

AA

BB

a
x = 1 qx = 1 4

1b 1a 1b Z = 4 14 14 14

No dag x has any features, so

x

=

Q i

ifix is a product of zero terms, and hence has

value 1. As a result, q is the uniform distribution. The Kullback-Leibler divergence Dp~jq

is 0.03. The aim of feature selection is to choose a feature that reduces this divergence

as much as possible.

The astute reader will note that there is a problem with the null eld if LG is

in nite. Namely, it is not possible to have a uniform probability mass distribution over

an in nite set. If each dag in an in nite set of dags is assigned a constant nonzero

probability , then the total probability is in nite, no matter how small is. There are

a couple of ways of dealing with the problem. The approach that DD&L adopt is to

assume a consistent prior distribution pk over graph sizes k, and a family of random

elds qk representing the conditional probability qxjk; the probability of a tree is then

pkqxjk. All the random elds have the same features and weights, di ering only in

their normalizing constants.

I will take a somewhat di erent approach here. As sketched at the beginning of section

3, we can generate dags from an AV grammar much as proposed by Brew and Eisele.

If we ignore failed derivations, the process of dag generation is completely analogous to

the process of tree generation from a stochastic CFG|indeed, in the limiting case in

which none of the rules contain constraints, the grammar is a CFG. To obtain an initial

20

Abney

Stochastic Attribute-Value Grammars

distribution, we associate a weight with each rule, the weights for rules with a common

lefthand side summing to one. The probability of a dag is proportional to the product of

weights of rules used to generate it. Renormalization is necessary because of the failed

derivations. We estimate weights using the ERF method: we estimate the weight of a

rule as the relative frequency of the rule in the training corpus, among rules with the

same lefthand side.

The resulting initial distribution the ERF distribution is not the maximum like-

lihood distribution, as we know. But it can be taken as a useful rst approximation.

Intuitively, we begin with the ERF distribution and construct a random eld to take ac-

count of context-dependencies that the ERF distribution fails to capture, incrementally

improving the t to the empirical distribution.

In this framework, a model consists of: 1 An AV grammar G whose purpose is to

de ne a set of dags LG. 2 A set of initial weights  attached to the rules of G. The

weight of a dag is the product of weights of rules used in generating it. Discarding failed

derivations and renormalizing yields the initial distribution p0x. 3 A set of features

f1; : : :; fn with weights

1; : : : ;

n to de ne the

eld

distribution

qx

=

Z1

p0x

Q i

ifix.

5.2 Feature Selection
At each iteration, we select a new feature f by considering all atomic features, and all complex features that can be constructed from features already in the eld. Holding the weights constant for all old features in the eld, we choose the best weight for f how
is chosen will be discussed shortly, yielding a new distribution q ;f. The score for feature f is the reduction it permits in Dp~jqold, where qold is the old eld. That is, the score for f is Dp~jqold , Dp~jq ;f. We compute the score for each candidate feature and add to the eld that feature with the highest score.
To illustrate, consider the two atomic features `a' and `B'. Given the null eld as old
21

Computational Linguistics

Volume 0, Number 0

eld, the best weight for `a' is = 7=5, and the best weight for `B' is = 1. This yields q and Dp~jf as follows:

S S SS

AA

AA

B

B

p~

a
13

1 b6 1 a4 1 b4

a 75

qa p~ln

qp~a

7 24 0.04

1 7 5 1 Z = 24=5 5 24 7 24 5 24 ,0:04 ,0:04 0:05 D = 0:01

B

qB p~ln

qp~B

1 14 0.10

1 1 1 Z=4 14 14 14 ,0:07 0 0 D = 0:03

The better feature is `a', and `a' would be added to the eld if these were the only two

choices.

Intuitively, `a' is better than `B' because `a' permits us to distinguish the set fx1; x3g

from the set fx2; x4g; the empirical probability of the former is 1=3+1=4 = 7=12 whereas

the empirical probability of the latter is 5=12. Distinguishing these sets permits us to

model the empirical distribution better since the old eld assigns them equal probability,

counter to the empirical distribution. By contrast, the feature `B' distinguishes the set

fx1; x2g from fx3; x4g. The empirical probability of the former is 1=3 + 1=6 = 1=2 and

the empirical probability of the latter is also 1=2. The old eld models these probabilities

exactly correctly, so making the distinction does not permit us to improve on the old

eld. As a result, the best weight we can choose for `B' is 1, which is equivalent to not

having the feature `B' at all.

5.3 Selecting the Initial Weight
DD&L show that there is a unique weight ^ that maximizes the score for a new feature f provided that the score for f is not constant for all weights. Writing q for the distribution that results from assigning weight to feature f, ^ is the solution to the equation

22

Abney

Stochastic Attribute-Value Grammars

q f = p~ f

10

Intuitively, we choose the weight such that the expectation of f under the resulting new eld is equal to its empirical expectation.
Solving equation 10 for is easy if LG is small enough to enumerate. Then the sum over LG that is implicit in q f can be expanded out, and solving for is simply a matter of arithmetic. Things are a bit trickier if LG is too large to enumerate. DD&L show that we can solve equation 10 if we can estimate qold f = k for k from 0 to the maximum value of f in the training corpus. See appendix 1 for details.
We can estimate qold f = k by means of random sampling. The idea is actually rather simple: to estimate how often the feature appears in the average dag", we generate a representative mini-corpus from the distribution qold and count. That is, we generate dags at random in such a way that the relative frequency of dag x is qoldx in the limit, and we count how often the feature of interest appears in dags in our generated mini-corpus.
The application that DD&L consider is the induction of English orthographic constraints, that is, inducing a eld that assigns high probability to English-sounding" words and low probability to non-English-sounding words. For this application, Gibbs sampling is appropriate. Gibbs sampling does not work for the application to AV grammars, however. Fortunately, there is an alternative random sampling method we can use: Metropolis-Hastings sampling. We will discuss the issue in some detail shortly.

5.4 Readjusting Weights
When a new feature is added to the eld, the best value for its initial weight is chosen, but the weights for the old features are held constant. In general, however, adding the new feature may make it necessary to readjust weights for all features. The second half

23

Computational Linguistics

Volume 0, Number 0

of the IIS algorithm involves nding the best weights for a given set of features. The method is very similar to the method for selecting the initial weight for a new
feature. Let  1; : : :; n be the old weights for the features. We wish to compute increments"  1; : : :; n to determine a new eld with weights  1 1; : : :; n n. Consider the equation

qold iffi = p~ fi

11

where

fx

=

P i

fix

is

the

total

number

of

features

of

dag

x.

The

reason

for

the

factor if is a bit involved. Very roughly, we would like to choose weights so that the

expectation of fi under the new eld is equal to p~ fi . Now qnewx is:

qnewx

= =

ZZ11pq0olxdxQ Qj jj jfjjxfjx

where we factor Z as Z Z , for Z the normalization constant in qold. Hence, qnew fi =

qold

Z1

fi

Q j

jfjx .

Now

there

are

two

problems

with

this

expression:

it

requires

us

to

compute Z , which we are not able to do, and it requires us to determine weights j for

all the features simultaneously, not just the weight i for feature i. We might consider ap-

proximating qnew fi by ignoring the normalization factor and assuming that all features

have

the

same

weight

as

feature

i.

Since

Q j

ifjx =

ifx, we arrive at the expression

on the lefthand side of equation 11.

One might expect the approximation just described to be rather poor, but it is proven

in Della Pietra, Della Pietra, and La erty, 1995 that solving equation 11 for i for

each i and setting the new weight for feature i to i i is guaranteed to improve the

model. This is the real justi cation for equation 11, and the reader is referred to Della

Pietra, Della Pietra, and La erty, 1995 for details.

Solving 11 yields improved weights, but it does not necessarily immediately yield

the globally best weights. We can obtain the globally best weights by iterating. Set

24

Abney

Stochastic Attribute-Value Grammars

i  i i, for all i, and solve equation 11 again. Repeat until the weights no longer change.
As with equation 10, solving equation 11 is straightforward if LG is small enough to enumerate, but not if LG is large. In that case, we must use random sampling. We generate a representative mini-corpus and estimate expectations by counting in the mini-corpus. See appendix 2.

5.5 Random Sampling
We have seen that random sampling is necessary both to set the initial weight for features under consideration and to adjust all weights after a new feature is adopted. Random sampling involves creating a corpus that is representative of a given model distribution qx. To take a very simple example, a fair coin can be seen as a method for sampling from the distribution q in which qH = 1=2, qT  = 1=2. Saying that a corpus is representative is actually not a comment about the corpus itself but the method by which it was generated: a corpus representative of distribution q is one generated by a process that samples from q. Saying that a process M samples from q is to say that the empirical distributions of corpora generated by M converge to q in the limit. For example, if we ip a fair coin once, the resulting empirical distribution over H; T is either 1; 0 or 0; 1, not the fair-coin distribution 1=2; 1=2. But as we take larger and larger corpora, the resulting empirical distributions converge to 1=2; 1=2.
An advantage of SCFGs that random elds lack is the transparent relationship between an SCFG de ning a distribution q and a sampler for q. We can sample from q by performing stochastic derivations: each time we have a choice among rules expanding a category X, we choose rule X ! i with probability i, where i is the weight of rule X ! i.
Now we can sample from the initial distribution p0 by performing stochastic deriva-
25

Computational Linguistics

Volume 0, Number 0

tions. At the beginning of section 3, we sketched how to generate dags from an AV grammar G via nondeterministic derivations. We de ned the initial distribution in terms of weights  attached to the rules of G. We can convert the nondeterministic derivations discussed at the beginning of section 3 into stochastic derivations by choosing rule X ! i with probability i when expanding a node labelled X. Some derivations fail, but throwing away failed derivations has the e ect of renormalizing the weight function, so that we generate a dag x with probability p0x, as desired.
The Metropolis-Hastings algorithm provides us with a means of converting the sampler for the initial distribution p0x into a sampler for the eld distribution qx. Generally, let p be a distribution for which we have a sampler. We wish to construct a sample x1; : : :; xN from a di erent distribution q. Assume that items x1; : : :; xn are already in the sample, and we wish to choose xn+1. The sampler for p proposes a new item y. We do not simply add y to the sample|that would give us a sample from p|but rather we make a stochastic decision whether to accept the proposal y or reject it. If we accept y, it is added to the sample xn+1 = y, and if we reject y, then xn is repeated xn+1 = xn.
The acceptance decision is made as follows. If py qy, then y is overrepresented among the proposals. We can quantify the degree of overrepresentation as py=qy. The idea is to reject y with a probability corresponding to its degree of overrepresentation. However, we do not consider the absolute degree of overrepresentation, but rather the degree of overrepresentation relative to xn. If y and xn are equally overrepresented, there is no reason to reject y in favor of xn. That is, we consider the value

r

=

py=qy pxn=qxn

=

pyqxn pxnqy

If r  1, then y is underrepresented relative to xn, and we accept y with probability one. If r 1, then we accept y with a probability that diminishes as r increases: speci cally,

26

Abney

Stochastic Attribute-Value Grammars

with probability 1=r. In brief, the acceptance probability of y is Ayjxn = min1; 1=r.

It can be shown that proposing items with probability p and accepting them with

probability Ajxn yields a sampler for q. See e.g. Winkler, 1995.2

The acceptance probability Ayjxn reduces in our case to a particularly simple form.

If r

1 then Ayjx = 1. Otherwise, writing x for the

eld

weight"

Q i

ifix, we

have:

6. Final Remarks

Ayjxn

= =

ZZ,,y11=xynxpp0n0yxnp0px0ny

12

In summary, we cannot simply transplant CF methods to the AV grammar case. In particular, the ERF method yields correct weights only for SCFGs, not for AV grammars. We can de ne a probabilistic version of AV grammars with a correct weight-selection method by going to random elds. Feature selection and weight adjustment can be accomplished using the IIS algorithm. In feature selection, we need to use random sampling to nd the initial weight for a candidate feature, and in weight adjustment we need to use random sampling to solve the weight equation. The random sampling method that DD&L used is not appropriate for sets of dags, but we can solve that problem by using the Metropolis-Hastings method instead.

2 The Metropolis-Hastings acceptance probability is usually given in the form

Ayjx

=

min


 1;

ygy; xgx;

x y



in which  is the distribution we wish to sample from q, in our notation and gx; y is the

cpornopogsuarlaptiroonbwabaislixty.:Tthhee

probability that the input sampler will propose y if the case we consider is a special case in which the proposal

previous probability

is

independent of x: the proposal probability gx;y is, in our notation, py.

The which

original Metropolis algorithm is also a special the proposal probability is symmetric, that is,

gcaxse; yof =thgeyM; extr.oTpholeisa-cHcaespttianngsceaflguonrcittihomn ,thinen

reduces to min1;y=x, which is min1;qy=qx in our notation. I mention this only to

point out that it is a di erent special case. Our proposal probability is not symmetric, but rather

independent of the previous con guration, and though our acceptance function reduces to a form

12 that is similar to the original Metropolis acceptance function, it is not the same: in general,

y= x =6 qy=qx.

27

Computational Linguistics

Volume 0, Number 0

Open questions remain. First, random sampling is notorious for being slow, and it

remains to be shown whether the approach proposed here will be practicable. I expect

practicability to be quite sensitive to the choice of grammar|the more the grammar's

distribution diverges from the initial context-free approximation, the more features will

be necessary to correct" it, and the more random sampling will be called on.

A second issue is incomplete data. The approach described here assumes complete

data a parsed training corpus. Fortunately, an extension of the method to handle in-

complete data unparsed training corpora is described in Riezler, 1997, and I refer

readers to that paper.

As a closing note, it should be pointed out explicitly that the random eld techniques

described here can be pro tably applied to context-free grammars, as well. As Stanley

Peters nicely put it, there is a distinction between possibilistic and probabilistic context-

sensitivity. Even if the language described by the grammar of interest|that is, the set

of possible trees|is context-free, there may well be context-sensitive statistical depen-

dencies. Random elds can be readily applied to capture such statistical dependencies

whether or not LG is context-sensitive.

Acknowledgments
This work has greatly pro ted from the comments, criticism, and suggestions of a number of people, including Yoav Freund, John La erty, Stanley Peters, Hans Uszkoreit, and members of the audience at talks I gave at Saarbrucken and Tubingen. Michael Miller and Kevin Mark introduced me to random elds as a way of dealing with context-sensitivities in language, planting the idea that led much later to this paper. Finally, I would especially like to thank Marc Light and Stefan Riezler for extended discussions of the issues addressed here and helpful criticism of my rst attempts to present this material. All responsibility for aws and errors of course remains with me.
References
Brew, Chris. 1995. Stochastic HPSG. In

Proceedings of EACL-95.
Della Pietra, Stephen, Vincent Della Pietra, and John La erty. 1995. Inducing features of random elds. tech report CMU-CS-95-144, CMU.
Eisele, Andreas. 1994. Towards probabilistic extensions of constraint-based grammars. Technical Report Deliverable R1.2.B, DYANA-2.
Gibbs, W. 1902. Elementary principles of statistical mechanics. Yale University Press, New Haven, CT.
Mark, Kevin, Michael Miller, Ulf Grenander, and Steve Abney. 1992. Parameter estimation for constrained context-free language models. In Proceedings of the Fifth Darpa Workshop on Speech and Natural Language, San Mateo, CA. Morgan Kaufman.
Riezler, Stefan. 1996. Quantitative constraint logic programming for

28

Abney
weighted grammar applications. Talk given at LACL, September. Riezler, Stefan. 1997. Probabilistic Constraint Logic Programming. Arbeitspapiere des Sonderforschungsbereichs 340, Bericht Nr. 117, Universitat Tubingen. Winkler, Gerhard. 1995. Image Analysis, Random Fields and Dynamic Monte Carlo Methods. Springer.
A. Initial Weight Estimation
In the feature selection step, we choose an initial weight for each candidate feature f so as to maximize the gain G = Dp~jqold , Dp~jqf;  of adding f to the eld. It is actually more convenient to consider log weights = ln . For a given feature f, the log weight ^ that maximizes gain is the solution to the equation:
q f = p~ f
where q is the distribution that results from adding f to the eld with log weight . This equation can be solved using Newton's method. De ne
F  = p~ f , q f 13 To nd the value of for which F   = 0, we begin at a convenient point 0 the null" weight 0 = 0 recommends itself

Stochastic Attribute-Value Grammars
and iteratively compute:

t+1 =

t,

F F 0

t t

14

Della Pietra, Della Pietra, and La erty,

1995 show that F 0 t is equal to the

negative of the variance of f under the

new eld, which I will write ,V f .

To compute the iteration 14 we need

to be able to compute F  t and F 0 t.

For F  t we require p~ f and q f , and F 0 t can be expressed as q f 2,q f2 .

p~ f is simply the average value of f in the

training corpus. The remaining terms are

all of the form q fr . We can re-express

this expectation in terms of the old eld

qold: q fr

= =
=

PqPoqloxdPxldfffxrerreexfxffeqxfqxoxlqdolxdx

The expectations qold fre f can be ob-

tained by generating a random sample z1; : : :; zN

of size N from qold and computing the average value of fre f. That is, qold fre f 

1=Nsr , where:

sr 

= =

P Pk
u

frzke fzk countk fzk

=

u

ure

u

This yields:

29

Computational Linguistics

q

fr

=

sr  s0

 

and the Newton iteration 14 reduces to:

t+1 =

t

+

s20s0ttp~sf2,ts0,

ts1 t s1 t2

To compare candidates, we also need

to know the gain Dp~jqold , Dp~jq^ for

each candidate. This can be expressed as

follows Della Pietra, Della Pietra, and

La erty, 1995:

Gf; ^

= 

p~ f p~ f

ln ln

^ ^

, ,

ln ln

qold e^f s0^ +

ln

N

Putting everything together, the algorithm for feature selection has the following form. The array E f is assumed to have been initialized with the empirical expectations p~ f .

procedure SelectFeature  begin
Fill array C f; u = countk fzk = u by sampling from old eld G^  0, g  none
for each f in candidates do 0
until is accurate enough do s0  s1  s2  0

Volume 0, Number 0

for u from 0 to umax do

x  C f;u e u

s0  s0 + x

s1  s0 + xu

s2  s0 + xu2

end 
end

+ s02Es0sf2,,ss012s1

G  E f , ln s0 + ln N if G G^ then G^  G; g  f; ^ 

end

return g; ^; G^

end

B. Adjusting Field Weights
The procedure for adjusting eld weights has much the same structure as the procedure for choosing initial weights. In terms of log weights, we wish to compute increments  1; : : :; n such that the new eld, with log weights  1 + 1; : : :; n + n has a lower divergence than the old eld  1; : : :; n. We choose each i as the solution to the equation:

p~ fi = qold fie if

30

Abney

Stochastic Attribute-Value Grammars

Again, we use Newton's method. We wish

by sampling from q

to nd such that Fi  = 0, where:

for i from 1 to n

Fi  = p~ fi , qold fie f

0 until is su ciently accurate do

As Della Pietra, Della Pietra, and Laf-

s0  s1  0

ferty, 1995 show, the rst derivative is:

for m from 0 to mmax do

Fi0  = ,qold fife f

x  C i; m e m s0  s0 + x

We see that the expectations we need to

s1  s1 + xm

compute by sampling from qold are of form

end

qold fifr e f . We generate a random sam-

 + NE fsi1 ,s0

ple z1; : : :; zN  and de ne:

end

sri; 

= = =

PPPkmmfPmirzuekcomfuPntzkkkjffriezzfkk==zmkufi^zkf

zk



=

m umr
end

ie

m

i+

As we generate the sample we update the

array C i; m

=

P kjfzk=m

fizk.

We

estimate qold fifr e f as the average value

end return  1; : : :; n end

of fifr e f in the sample, namely, 1=Nsri; .

This permits us to compute Fi  and Fi0 .

The resulting Newton iteration is:

t+1 =

t

+

N

p~

fi , s0i; s1i; 

t

The estimation procedure is:

procedure AdjustWeights  1; : : :; n begin until the eld converges do
Fill array C i; m

31

A Maximum Entropy Approach to Natural Language Processing
Adam L. Berger Stephen A. Della Pietray Vincent J. Della Pietray
IBM T.J. Watson Research Center1 P.O. Box 704
Yorktown Heights, NY 10598 The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach e ciently, using as examples several problems in natural language processing.
1. Introduction
Statistical modeling addresses the problem of constructing a stochastic model to predict the behavior of a random process. In constructing this model, we typically have at our disposal a sample of output from the process. Given this sample, representing an incomplete state of knowledge about the process, the modeling problem is to parlay this knowledge into a representation of the process. We can then use this representation to make predictions of the future behavior of the process.
Baseball managers who rank among the better paid statistical modelers employ batting averages, compiled from a history of at-bats, to gauge the likelihood that a player will succeed in his next appearance at the plate. Thus informed, they manipulate their lineups accordingly. Wall Street speculators who rank among the best paid statistical modelers build models based on past stock price movements to predict tomorrow's uctuations and alter their portfolios to capitalize on the predicted future. At the other end of the pay scale reside natural language researchers, who design language and acoustic models for use in speech recognition systems and related applications.
The past few decades have witnessed signi cant progress toward increasing the predictive capacity of statistical models of natural language. In language modeling, for instance, Bahl et al 1989 have used decision tree models and Della Pietra et al 1994 have used automatically inferred link grammars to model long range correlations in language. In parsing, Black et al 1992 has described how to extract grammatical rules from
 Now at Columbia University computer science department y Now at Renaissance Technologies, Stony Brook, NY 1 Research supported in part by ARPA under grant ONR N00014-91-C-0135 c 1996 Association for Computational Linguistics

Computational Linguistics

Volume 22, Number 1

annotated text automatically and incorporate these rules into statistical models of grammar. In speech recognition, Lucassen and Mercer 1984 have introduced a technique for automatically discovering relevant features for the translation of word spelling to word pronunciation.
These e orts, while varied in speci cs, all confront two essential tasks of statistical modeling. The rst task is to determine a set of statistics which capture the behavior of a random process. Given a set of statistics, the second task is to corral these facts into an accurate model of the process|a model capable of predicting the future output of the process. The rst task is one of feature selection; the second is one of model selection. In the following pages we present a uni ed approach to these two tasks based on the maximum entropy philosophy.
Our discussion will proceed as follows. In Section 2 we give an overview of the maximum entropy philosophy and work through a motivating example. In Section 3 we describe the mathematical structure of maximum entropy models and give an e cient algorithm for estimating the parameters of such models. In Section 4 we discuss feature selection, and present an automatic method for discovering facts about a process from a sample of output from the process. We then present a series of re nements to the method to make it practical to implement. Finally, in Section 5 we describe the application of maximumentropy ideas to several tasks in stochastic language processing: bilingual sense disambiguation, word reordering, and sentence segmentation.
2. A Maximum Entropy Overview
We introduce the concept of maximum entropy through a simple example. Suppose we wish to model an expert translator's decisions concerning the proper French rendering of the English word in. Our model p of the expert's decisions assigns to each French word or phrase f an estimate, pf, of the probability that the expert would choose f as a translation of in. To guide us in developing p, we collect a large sample of instances of the expert's decisions. Our goal is to extract a set of facts about the decision-making process from the sample the rst task of modeling that will aid us in constructing a model of this process the second task.
One obvious clue we might glean from the sample is the list of allowed translations. For example, we might discover that the expert translator always chooses among the following ve French phrases: fdans, en, a, au cours de, pendantg. With this information in hand, we can impose our rst constraint on our model p:

pdans + pen + pa + pau cours de + ppendant = 1 This equation represents our rst statistic of the process; we can now proceed to search for a suitable model which obeys this equation. Of course, there are an in nite number of models p for which this identity holds. One model which satis es the above equation is pdans = 1; in other words, the model always predicts dans. Another model which obeys this constraint predicts pendant with a probability of 1=2, and a with a probability of 1=2. But both of these models o end our sensibilities: knowing only that the expert always chose from among these ve French phrases, how can we justify either of these probability distributions? Each seems to be making rather bold assumptions, with no empirical justi cation. Put another way, these two models assume more than we actually know about the expert's decision-making process. All we know is that the expert chose exclusively from among these ve French phrases; given this, the most intuitively appealing model is the following:

2

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

pdans = 1=5 pen = 1=5 pa = 1=5
pau cours de = 1=5 ppendant = 1=5
This model, which allocates the total probability evenly among the ve possible phrases, is the most uniform model subject to our knowledge. It is not, however, the most uniform overall; that model would grant an equal probability to every possible French phrase.
We might hope to glean more clues about the expert's decisions from our sample. Suppose we notice that the expert chose either dans or en 30 of the time. We could apply this knowledge to update our model of the translation process by requiring that p satisfy two constraints:
pdans + pen = 3=10 pdans + pen + pa + pau cours de + ppendant = 1
Once again there are many probability distributions consistent with these two constraints. In the absence of any other knowledge, a reasonable choice for p is again the most uniform|that is, the distribution which allocates its probability as evenly as possible, subject to the constraints:
pdans = 3=20 pen = 3=20 pa = 7=30
pau cours de = 7=30 ppendant = 7=30
Say we inspect the data once more, and this time notice another interesting fact: in half the cases, the expert chose either dans or a. We can incorporate this information into our model as a third constraint:
pdans + pen = 3=10 pdans + pen + pa + pau cours de + ppendant = 1
pdans + pa = 1=2
We can once again look for the most uniform p satisfying these constraints, but now the choice is not as obvious. As we have added complexity, we have encountered two di culties at once. First, what exactly is meant by uniform," and how can one measure the uniformity of a model? Second, having determined a suitable answer to these questions, how does one go about nding the most uniform model subject to a set of constraints like those we have described?
The maximum entropy method answers both these questions, as we will demonstrate in the next few pages. Intuitively, the principle is simple: model all that is known and
3

Computational Linguistics

Volume 22, Number 1

assume nothing about that which is unknown. In other words, given a collection of facts, choose a model which is consistent with all the facts, but otherwise as uniform as possible. This is precisely the approach we took in selecting our model p at each step in the above example.
The maximum entropy concept has a long history. Adopting the least complex hypothesis possible is embodied in Occam's Razor  Nunquam ponenda est pluralitas sine necesitate" and even appears earlier, in the Bible and the writings of Herotodus Jaynes 1990. Laplace might justly be considered the father of maximum entropy, having enunciated the underlying theme 200 years ago in his Principle of Insu cient Reason": when one has no information to distinguish between the probability of two events, the best strategy is to consider them equally likely Guiasu and Shenitzer 1994. As E.T. Jaynes, a more recent pioneer of maximum entropy, put it Jaynes 1990:

...the fact that a certain probability distribution maximizes entropy subject to certain constraints representing our incomplete information, is the fundamental property which justi es use of that distribution for inference; it agrees with everything that is known, but carefully avoids assuming anything that is not known. It is a transcription into mathematics of an ancient principle of wisdom...

3. Maximum Entropy Modeling

We consider a random process which produces an output value y, a member of a nite set Y. For the translation example just considered, the process generates a translation of the word in, and the output y can be any word in the set fdans, en, a, au cours de, pendantg. In generating y, the process may be in uenced by some contextual information x, a member of a nite set X. In the present example, this information could include the words in the English sentence surrounding in.
Our task is to construct a stochastic model that accurately represents the behavior of the random process. Such a model is a method of estimating the conditional probability that, given a context x, the process will output y. We will denote by pyjx the probability that the model assigns to y in context x. With a slight abuse of notation, we will also use pyjx to denote the entire conditional probability distribution provided by the model, with the interpretation that y and x are placeholders rather than speci c instantiations. The proper interpretation should be clear from the context. We will denote by P the set of all conditional probability distributions. Thus a model pyjx is, by de nition, just an element of P.

3.1 Training Data

To study the process, we observe the behavior of the random process for some time,

collecting have been

a large number of considering, each

samples x1; y1; x2; y2; sample would consist of

:: a

:; xN; phrase

yxN

. In the example we containing the words

surrounding in, together with the translation y of in which the process produced. For

now we can imagine that these training samples have been generated by a human expert

who was presented with a number of random phrases containing in and asked to choose

a good translation for each. When we discuss real-world applications in Section 5, we

will show how such samples can be automatically extracted from a bilingual corpus.

We can summarize the training sample in terms of its empirical probability distri-

4

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

bution p~, de ned by

p~x; y



1 N



number

of

times

that

x; y

occurs

in

the

sample

Typically, a particular pair x; y will either not occur at all in the sample, or will occur at most a few times.

3.2 Statistics, Features and Constraints
Our goal is to construct a statistical model of the process which generated the training sample p~x; y. The building blocks of this model will be a set of statistics of the training sample. In the current example we have employed several such statistics: the frequency that in translated to either dans or en was 3=10; the frequency that it translated to either dans or au cours de was 1=2; and so on. These particular statistics were independent of the context, but we could also consider statistics which depend on the conditioning information x. For instance, we might notice that, in the training sample, if April is the word following in, then the translation of in is en with frequency 9=10.
To express the event that in translates as en when April is the following word, we can introduce the indicator function

fx; y =

1 if y = en and April follows in 0 otherwise

The expected value of f with respect to the empirical distribution p~x; y is exactly the statistic we are interested in. We denote this expected value by

p~f  X p~x; yfx; y
x;y

1

We can express any statistic of the sample as the expected value of an appropriate binaryvalued indicator function f. We call such function a feature function or feature for short. As with probability distributions, we will sometimes abuse notation and use fx; y to denote both the value of f at a particular pair x; y as well as the entire function f.
When we discover a statistic that we feel is useful, we can acknowledge its importance by requiring that our model accord with it. We do this by constraining the expected value that the model assigns to the corresponding feature function f. The expected value of f with respect to the model pyjx is

pf  X p~xpyjxfx; y
x;y

2

where p~x is the empirical distribution of x in the training sample. We constrain this expected value to be the same as the expected value of f in the training sample. That is, we require

pf = p~f

3

Combining 1, 2 and 3 yields the more explicit equation X p~xpyjxfx; y = X p~x; yfx; y
x;y x;y

5

Computational Linguistics

Volume 22, Number 1

We call the requirement 3 a constraint equation or simply a constraint. By restricting attention to those models pyjx for which 3 holds, we are eliminating from consideration those models which do not agree with the training sample on how often the output of the process should exhibit the feature f.
To sum up so far, we now have a means of representing statistical phenomena inherent in a sample of data namely, p~f, and also a means of requiring that our model of the process exhibit these phenomena namely, pf = p~f.
One nal note about features and constraints bears repeating: though the words feature" and constraint" are often used interchangeably in discussions of maximum entropy, we will be vigilant to distinguish the two and urge the reader to do likewise: a feature is a binary-valued function of x; y; a constraint is an equation between the expected value of the feature function in the model and its expected value in the training data.

3.3 The Maximum Entropy Principle

Suppose that are important

we in

are given n modeling the

fpearotuceress.fuWncetiwonosuldfi,likwehoicuhr

determine model to

statistics we feel accord with these

statistics. That is, we would like p to lie in the subset C of P de ned by

C  f p 2 P j pfi = p~fi for i 2 f1; 2; : : :; ngg

4

Figure 1 provides a geometric interpretation of this setup. Here P is the space of all

unconditional probability distributions on 3 points, sometimes called a simplex. If we

impose no constraints depicted in a, then all probability models are allowable. Im-

pbcoyonsCisnt1rg,aaoinsntesshlioanwreenarsianctoisnbsat.rbaAlien;stetChco1isnrdeisstlirtnihceetasrcuacssoentositntrhaoicnste,

p 2 P which lie on the region de ned could determine p exactly, if the two where the intersection of C1 and C2

is non-empty. Alternatively, a second linear constraint could be inconsistent with the

rst|for instance, the rst might require that the probability of the rst point is 1=3

and the second that the probability of the third point is 3=4|this is shown in d. In the

present setting, however, the linear constraints are extracted from the training sample

and cannot, by construction, be inconsistent. Furthermore, the linear constraints in our

applications will not even come close to determining p 2 P uniquely as they do in c;

instead, the Among

set the

C = C1 models

p

C22

::: C, the

Cn of allowable models will be in nite. maximum entropy philosophy dictates that

we

select

the distribution which is most uniform. But now we face a question left open in Section

2: what does uniform" mean?

A mathematical measure of the uniformity of a conditional distribution pyjx is

provided by the conditional entropy2

Hp  , X p~xpyjx log pyjx
x;y

5

The entropy is bounded from below by zero, the entropy of a model with no uncertainty at all, and from above by logjYj, the entropy of the uniform distribution over all possible

2 A more common notation for the conditional entropy is HY j X, where Y and X are random variables with joint distribution p~xpyjx. To emphasize the dependence of the entropy on the probability distribution p, we have adopted the alternate notation Hp.

6

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

P
(a)
P C1
C2 (c)

P C1
(b)

C1 C3

P

(d)

FdFlIcinonioisnugetsarrudtirbrrd,aecuiitotnh1nietoersnetCtnsr1w.ationInsnactcr.eornnIaonaswr,tisrnocastoihn,icetntoswsnceoaostrtnceroosaftininrnasactiloislsnotneewasndriastebtoeclapenopttnipmmsltioieir.zdedaa.ei,tnlCsiato1snntdo.C1tCaPhl3alorn=psedep2;rCwe2P;shenidcnaoehtrsepnlitaee2hllaeoPonwsspicatnaahbgcnelleee.slioamnIftneioasddflylbeelpt,rnphoteeh2bdmeaCbbb1yiloitttyhhC.e2.

7

Computational Linguistics

Volume 22, Number 1

jYj values of y. With this de nition in hand, we are ready to present the principle of maximum entropy.

To select a model from a set C of allowed probability distributions, choose the model p? 2 C with maximum entropy Hp:

p? = argp2mCaxHp

6

It p?

can be shown that p? is with maximum entropy

always well-de ned; that in any constrained set C.

is,

there

is

always

a

unique

model

3.4 Parametric Form
The maximum entropy principle presents us with a problem in constrained optimization: nd the p? 2 C which maximizes Hp. In simple cases, we can nd the solution to this problem analytically. This was true for the example presented in Section 2 when we imposed the rst two constraints on p. Unfortunately, the solution of the general maximum entropy cannot be written explicitly, and we need a more indirect approach. The reader is invited to try to calculate the solution for the same example when the third constraint is imposed.
To address the general problem, we apply the method of Lagrange multipliers from the theory of constrained optimization. The relevant steps are outlined here; the reader is referred to Della Pietra et al 1995 for a more thorough discussion of constrained optimization as applied to maximum entropy.

We will refer to the original constrained optimization problem,

nd p? = argp2mCaxHp

as the primal problem.

For We

deaechnefetahteurLeafgirawnegiianntrodpu;ceabpyarameter

i

a

Lagrange

multiplier.

p;   Hp + X i pfi , p~fi
i

7

Holding  xed, we compute the unconstrained maximum of the

Lagrangian achieves its

p;  over all maximum and

p 2 P. We denote by  the value

by p the p where at this maximum:

p;



p  arpg2mPaxp;    p; 

8 9

We call  the dual function. The functions calculated explicitly using simple calculus. We

pndand



may

be

pyjx 

= =

,ZX1xp~exxploXgiZixfi+x;Xy!ip~fi

xi

10 11

8

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

PwhyepreZyjxx =is1afonroarmll axl:izing constant determined by the requirement that

Zx

=

X

exp

X

ifix;

! y

12

yi

Finally, we pose the unconstrained dual optimization problem

Find ? = argm ax

At rst glance it is not clear what these machinations achieve. However, a fundamen-

tal principle in the theory of Lagrange multipliers, called generically the Kuhn-Tucker

theorem, asserts that under suitable assumptions, the primal and dual problems are, in

fact, closely related. This is the case in the present situation. Although a detailed account

of this relationship is beyond the scope of this paper, it is easy to state the nal result:

Suppose that ? is the primal problem; that is

solution of the dual problem. p? = p?. In other words,

Then

p?

is

the

solution

of

the

The maximum entropy model subject to the constraints C has the para-

metric mined

form3 p? of 10, by maximizing the

where the parameter dual function .

values

?

can

be

deter-

The most important practical consequence of this result is that any algorithm for nding the maximum ? of  can be used to nd the maximum p? of Hp for p 2 C.

3.5 Relation to Maximum Likelihood

The log-likelihood de ned by4

Lp~p

of

the

empirical

distribution

p~

as

predicted

by

a

model

p

is

Lp~p  logY pyjxp~x; y = X p~x; y log pyjx
x;y x;y

13

It is easy to check that the dual function  of the previous section is, in fact, just the log-likelihood for the exponential model p; that is

 = Lp~p

14

With this interpretation, the result of the previous section can be rephrased as:

fTahmeilmyopdelypjx?2thCawt imthamximaxiizmesutmheenlitkreolpihyoiosdthoef

model in the the training

parametric sample p~.

3 It might be that the dual function  does not achieve its maximum at any nite ?. In this case,

the maximum models of this

efonrtmro,paysminoddieclatweidllbnyotthheavfoelltohweinfogrmrespultfworhaonsey

. However, it proof we omit:

will

be

the

limit

of

cSounpvpeorsgeestno

is any p?.

sequence

such

that

n

converges

to

the

maximum

of

.

Then

pn

4 We will henceforth abbreviate Lp~p by Lp when the empirical distribution p~ is clear from context.

9

Computational Linguistics

Volume 22, Number 1

Primal

Dual

problem description

maarxgmimauxmp2eCnHtroppy

maxairmgmumaxlikelihood

type of search search domain
solution

constrained p

2opCtimization

p?

reuanl-cvoanlusterdaivneecdt?oorpstfim1iz;at2io:n: :g

Table 1

Kuhn-Tucker theorem: p? = p?

The duality of maximum entropy and maximum likelihood is an example of the more general

phenomenon of duality in constrained optimization.

This result provides an added justi cation for the maximum entropy principle: if the snitaomstoioenhpaaopfrpasmeenleescttrtiihncagftoartmhmiso1ds0aelm,pec?apon?nbitsehsaetlbsaoacsctiohsueonfmtmfooadrxetlimhweuhmtircahei,nntfirnroogpmsyaaimsmnp'oltenc.gomallpemlloindgelesnoofutghhe,
Table 1 summarizes the primal-dual framework we have established.

3.6
For

Computing the Parameters
all but the most simple problems,

the

?

that

maximize



cannot

be

found

analyt-

ically. Instead, we must resort to numerical methods. From the perspective of numerical

moCpoetntihmseoiqdzuaiestnicotonlyo,,rtdahinevaafturein-ewctytiisooenfanscuemnte,riiisncawwlehmllicebhtehho?advissedccoa,mnsipnbuceeteuidstebdiys tisotmercoaaotlticvhuellaayntemd ac?xo.inmOveinzxei-nsgiminple.

one coordinate at a time. When applied to the maximumentropy problem, this technique

yields the popular Brown algorithm Brown 1959. Other general purpose methods that

can be used to maximize  include gradient ascent and conjugate gradient.

An optimization method speci cally tailored to the maximumentropy problem is the

iterative scaling algorithm of Darroch and Ratcli Darroch and Ratli 1972. We present

here a version of this algorithm speci cally designed for the problem at hand; a proof of

the monotonicity and convergence of the algorithm is given in Della Pietra et al 1995.

The algorithm is applicable whenever the feature functions fix; y are non-negative:

fix; y  0

for all i, x, and y

15

This is, of course, true for the binary-valued feature functions we are considering here.

TthheenAaollnggo-onrrietightahmtimvgiet1ny,e:rtaIhmlaiztpertsohtvehefedeaDIttauerrrraeotficvuhen-RcStacioatcnlilsni gsaptriosfcyedPurief,iwxh;iych

requires, in addition = 1 for all x; y.

to

Input: Output:

Feature functions f1; f2; : : Optimal parameter values

:f?ni;;

empirical distribution optimal model p?

p~x;

y

1. Start with i = 0 for all i 2 f1; 2; : : :; ng 2. Do for each i 2 f1; 2; : : :; ng:
a. Let i be the solution to X p~xpyjxfix; yexpifx; y = p~fi
x;y

16

10

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

where fx; y  Xn fix; y
i=1
b. Update the value of i according to: i  i + i 3. Go to step 2 if not all the i have converged

17

The key step that solve 16. If

ifnthxe;

yalgiosrcitohnmstainststefp

2a, x; y

the computation = M for all x; y,

osafythethienncremienistsgiveni

explicitly as

i

=

1 M

log

p~fi pfi

If fx; y is not constant, then i must be computed numerically. A simple and e ective way of doing this is by Newton's method. This method computes the solution ? of an equation g ? = 0 iteratively by the recurrence

n+1 =

n,

g g0

n n

18

with an appropriate choice for 0 and suitable attention paid to the domain of g.

4. Feature Selection

Earlier we divided the statistical modeling problem into two steps: nding appropriate facts about the data; the second is to incorporate these facts into the model. Up to this point we have proceeded by assuming that the rst task was somehow performed for us. Even in the simple example of Section 2, we did not explicitly state how we selected those particular constraints. That is, why is the fact that dans or a was chosen by the expert translator 50 of the time any more important than countless other facts contained in the data? In fact, the principle of maximum entropy does not directly concern itself with the issue of feature selection: it merely provides a recipe for combining constraints into a model. But the feature selection problem is critical, since the universe of possible constraints is typically in the thousands or even millions. In this section we introduce a method for automatically selecting the features to be included in a maximum entropy model, and then o er a series of re nements to ease the computational burden.

4.1 Motivation We begin by specifying a large collection F of candidate features. We do not require
a priori that these features are actually relevant or useful. Instead, we let the pool be as large as practically possible. Only a small subset of this collection of features will eventually be employed in our nal model.
If we had a training sample of in nite size, we could determine the true" expected value for a candidate feature f 2 F simply by computing the fraction of events in the sample for which fx; y = 1. In real-life applications, however, we are provided with only a small sample of N events, which cannot be trusted to represent the process fully and accurately. Speci cally, we cannot expect that for every feature f 2 F, the estimate of p~f we derive from this sample will be close to its value in the limit as n grows large. Employing a larger or even just a di erent sample of data from the same process might result in di erent estimates of p~f for many candidate features.

11

Computational Linguistics

Volume 22, Number 1

In short, we would like to include in the model only a subset S of the full set of candidate features F. We will call S the set of active features. The choice of S must capture as much information about the random process as possible, yet only include features whose expected values can be reliably estimated.
To nd S, we adopt an incremental approach to feature selection, similar to the strategy used for growing decision trees Bahl et al 1989. The idea is to build up S by successively adding features. The choice of feature to add at each step is determined by the training data. Let us denote the set of models determined by the feature set S as CS. Adding" a feature f is shorthand for requiring that the set of allowable models all satisfy the equality p~f = pf. Only some members of CS will satisfy this equality; the ones that do we denote by CS f.
Thus, each time a candidate feature is adjoined to S, another linear constraint is imposed on the space CS of models allowed by the features in S. As a result, CS shrinks; the model p? in C with the greatest entropy re ects ever-increasing knowledge and thus, hopefully, becomes a more accurate representation of the process. This narrowing of the space of permissible models was represented in gure 1 by a series of intersecting lines hyperplanes, in general in a probability simplex. Perhaps more intuitively, we could represent it by a series of nested subsets of P, as in gure 2.

P

C(S ) 1

C(S ) 2 C(S ) 3

Figure 2

A nested large sets

sequence of of features

Ssu1 bseSts2

CSS13:

:

:

CS2



CS3 : : : of P corresponding to increasingly

4.2 Basic Feature Selection
The basic incremental growth procedure may be outlined as follows. Every stage of the process is characterized by a set of active features S. These determine a space of models

CS  fp 2 P j pf = p~f for all f 2 Sg

19

The optimal model in this space, denoted by pS, is the model with the greatest entropy:

pS  apr2gCmSaxHp

20

By adding feature f^ to S, we obtain a new set of active features S f^. Following 19, this set of features determines a set of models

CS f^  fp 2 P j pf = p~f for all f 2 S f^g

21

12

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

The optimal model in this space of models is

pS f^  pa2rCgmS afx^ Hp

22

Adding the this results

ifneaatugraeinf^aLlloSw;sf^thienmthoedelol gp-Slikf^eltihooboedttoefrtahcectoruanintifnogr

the training data

sample;

LS; f^  LpS f^ , LpS

23

Af^t2eaFchwshtiacghemofatxhime mizeosdtehl-ecognasintrucLtioSn;

pf^ro; ctehsast,

our goal is to select the candidate feature is, we select the candidate feature which,

when adjoined to the set of active features S, produces the greatest increase in likelihood

of the training sample. This strategy is implemented in

Algorithm 2: Basic Feature Selection

Input: Collection F of candidate features; empirical distribution p~x; y Output: Set S of active features; model pS incorporating these features

1. Start with S = ;; thus pS is uniform 2. Do for each candidate feature f 2 F:

Compute Compute

the the

gmaoindeilnptShef

using Algorithm 1 log-likelihood from

adding

this

feature

using 23

3. Check the termination condition 4. Select the feature f^ with maximal gain LS; f^

5. Adjoin f^ to S

6. Compute pS using Algorithm 1 7. Go to step 2

One issue left unaddressed by this algorithm is the termination condition. Obviously, we would like a condition which applies exactly when all the useful" features have been selected. One reasonable stopping criterion is to subject each proposed feature to crossvalidation on a held-out sample of data. If the feature does not lead to an increase in likelihood of the held-out sample of data, the feature is discarded. We will have more to say about the stopping criterion in Section 5.3.

4.3 Approximate Gains

Algorithm 2 is not a practical method for incremental feature selection. For each candi-

date feature f 2 F considered in step 2, we must compute the maximum entropy model

rpiSthfm,

a task that introduced

is computationally costly even earlier. We therefore introduce

with the e cient iterative scaling algoa modi cation to the algorithm, making

it greedy but much more feasible. We replace the computation of the gain LS; f of a

feature f with an approximation, which we will denote by LS; f.

pS

fRceocnatllatinhsatthaismsoedt eolfppSahraams aetseerts,opf lpuasraamsientgelres

, one for each new parameter

feature in S. The , corresponding

model to f.5

5

Another but =

way 0 for

to think pS.

of

this

is

that

the

models

pS

f

and pS

have the same number of parameters,

13

Computational Linguistics

Volume 22, Number 1

Given this structure, we might hope that the optimal values for  do not change as the

feature f is adjoined to S. Were this the case, imposing an additional constraint would

require only optimizing the single parameter to maximize the likelihood. Unfortunately,

when a new constraint is imposed, the optimal values of all parameters change.

However, to make the feature-ranking computation tractable, we make the approx-

imation that the addition of a feature f a ects only , leaving the -values associated

with other features unchanged. That is, when determining the gain of f over the model

pS, we pretend that the best model containing features S f has the form

pS;f

=

Z

1 x

pS

yjxe

fx;y;

for some real valued

24

where

Z

x

=

X
y

pS

yjxe

fx;y

25

The only parameter which distinguishes models of the form 24 is . Among these models, we are interested in the one which maximizes the approximate gain

GS;f  

 =

,LXpS;fp~x,LlogpZS x +

p~f

x

We will denote the gain of this model by

26

LS; f  maxGS;f  

27

and the optimal model by

pS f  arpgSm;faxGS;f  

28

Despite the rather unwieldy notation, the idea is simple. Computing the approxi-

mate gain in likelihood from adding feature f to dimensional optimization problem over the single

ppSarhaams ebteeren

reduced , which

to a simple onecan be solved by

any popular line-search technique such as Newton's method. This yields a great savings

in computational complexity over computing the exact gain, an n-dimensional optimiza-

tion problem requiring more sophisticated methods such as conjugate gradient. But the

savings comes at a price: for any particular feature f, we are probably underestimating its

gain, and there gain LS; f

is a reasonable chance  was highest and pass

that over

wtheewfeilaltsuerleecft^

awiftehatmuraexfimwahl ogsaeinappLroxSi;mf^a.te

A graphical representation of this approximation is provided in gure 3. Here the

log-likelihood is represented as an arbitrary convex function over two parameters: 

corresponds to the old" parameter, and to the new" parameter. Holding  xed

and adjusting to maximize the log-likelihood involves a search over the darkened line,

rather than a search over the entire space of ; .

The actual algorithms, along with the appropriate mathematical framework, are

presented in the appendix.

5. Case Studies

In the next few pages we discuss several applications of maximum entropy modeling within Candide, a fully automatic French-to-English machine translation system under

14

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

(a) L(p)

L(p)




(b)



Figure 3

The likelihood Lp is a convex function of its parameters. If we start from a one-constraint

model whose optimal parameter value adjoining a second constraint with the

ipsara=met0erand,

tchoenseidxaerctthaensiwncerreraesqeuiinreLs p~apseafrrcohmover

; the

. We can simplify this task by holding possible values of the new parameter .

In=a0,

constant and the darkened

performing a line search over line represents the search

space we restrict attention to. In b we show the reduced problem: a line search over .

15

Computational Linguistics

Volume 22, Number 1

development at IBM. Over the past few years, we have used Candide as a test bed for exploring the e cacy of various techniques in modeling problems arising in machine translation.
We begin in Section 5.1 with a review of the general theory of statistical translation, describing in some detail the models employed in Candide. In Section 5.2 we describe how we have applied maximum entropy modeling to predict the French translation of an English word in context. In Section 5.3 we describe maximum entropy models that predict di erences between French word order and English word order. In Section 5.4 we describe a maximum entropy model that predicts how to divide a French sentence into short segments that can be translated sequentially.

5.1 Review of Statistical Translation

WE^ hwehnicphreissemntoesdt

with a French likely given F

sentence :

F

,

Candide's

task

is

to

nd the English sentence

E^ = argmaxpEjF
E
By Bayes' theorem, this is equivalent to nding E^ such that

29

E^ = argEmaxpFjEpE

30

Candide estimates pE|the probability that a string E of English words is a well-

formed English sentence|using a parametric model of the English language, commonly

referred to as a language model. The system estimates pF jE|the probability that a

French sentence F is a translation of E|using a parametric model of the process of

English-to-French translation search strategy for nding the

kE^nowwhnicahsmaatxriamniszleastio3n0mfoodr eslo. mTeheFse,

two models, comprise the

plus a engine

of the translation system.

We now brie y describe the translation model for the probability PFjE; a more

thorough account is provided in Brown et al 1991. We imagine that an English sentence

E generates a French sentence F in two steps. First each word in E independently

generates zero or more French words. These words are then ordered to give a French

sentence F. We denote the ith word of E

nemotpaltoiyony.j

rather than We denote

the the

mnuomrebienrtuoiftiwvoerfdjs

tboyaveoiidancdontfuhseiojnthwiwthortdheoffeFatubrye in the sentence E by jEj and the

fyujn.ctWione number

of words in the sentence F by jFj. The generative process yields not only the French

sentence F but also an association of the words of F with the words of E. We call this

association an alignment, and denote it by A. An alignment A is parametrized by a

sequence of jFj numbers the word position in E of alignment.

aj, the

wEinthgli1shwaoirdthjEatj.gFenoerreavteesryyjw. oFridguproesi4tidoenpijctisn

F , aj is a typical

The probability pFjE that F is the translation of E is expressed as the sum over all possible alignments A between E and F of the probability of F and A given E:

pFjE = X pF; AjE

31

A

The sum in equation 31 is computationally unwieldy; it involves a sum over all jEjjFj possible alignments between the words in the two sentences. For this reason we sometimes

16

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

The1 dog2 ateH3HHHHmyH4HHhHomHewHHorHk5HH
Le1 chien2 a3 mange4 mes5 devoirs6
Figure 4
Alignment of a French English sentence pair. The subscripts give the position of each word in its sentence. Here a1 = 1, a2 = 2, a3 = a4 = 3, a5 = 4, and a6 = 5.

make the simplifying assumption that there exists one extremely probable alignment A^, called the Viterbi alignment," for which

pFjE  pF; A^jE

32

Given some alignment A Viterbi or otherwise between E and F, the probability pF; AjE is given by

pF;

AjE

=

YjEj
i=1

pneijei





YjF j
j=1

pyj

jeaj





dAjE

;

F



33

where nei denotes the number of French words aligned with ei. In this expression

pnje is the probability that the English word e generates n French words, pyje is the probability that the English word e generates the French word y; and dAjE; F is the probability of the particular order of French words.

We call the model described by equations 31 and 33 the basic translation model. We take the probabilities pnje and pyje as the fundamental parameters of the
model, and parametrize the distortion probability in terms of simpler distributions. Brown et al 1991 describe a method of estimating these parameters to maximize the likelihood of a large bilingual corpus of English and French sentences. Their method is based on the Estimation-Maximization EM algorithm, a well-known iterative technique for maximum likelihood training of a model involving hidden statistics. For the basic translation model, the hidden information is the alignment A between E and F.
We employed the EM algorithm to estimate the parameters of the basic translation model so as to maximize the likelihood of a bilingual corpus obtained from the proceedings of the Canadian parliament. For historical reasons, these proceedings are sometimes called Hansards." Our Hansard corpus contains 3:6 million English-French sentence pairs for a total of a little under 100 million words in each language. Table 2 shows our parameter estimates for the translation probabilities pyjin. The basic translation model has worked admirably: given only the bilingual corpus, with no additional knowledge of the languages or any relation between them, it has uncovered some highly plausible translations.
Nevertheless, the basic translation model has one major shortcoming: it does not take the English context into account. That is, the model does not account for surrounding English words when predicting the appropriate French rendering of an English word. As we pointed out in Section 3, this is not how successful translation works. The best French translation of in is a function of the surrounding English words: if a month's time are

17

Computational Linguistics

Volume 22, Number 1

Translation Probability

dans 0.3004

a 0.2275

de 0.1428

en 0.1361

pour 0.0349

OTHER 0.0290

au cours de 0.0233

, 0.0154

sur 0.0123

par 0.0101

Table 2

pendant 0.0044

Most frequent French translations of in as estimated using EM-training. OTHER represents

a catch-all classi er for any French phrase not listed, none of which had a probability

exceeding 0:0043.

18

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

||||||||||||||| Je dirais m^eme que les chances sont superieures a 50.

I would even say that the odds are superior to 50.
Il semble que Bank of Boston ait pratiquement acheve son reexamen de Shawmut.
He appears that Bank of Boston has almost completed its review of Shawmut. |||||||||||||||
Figure 5
Typical errors encountered in using EM-based model of Brown et. al. in a French-to-English translation system

the subsequent words, pendant might be more likely, but if the scal year 1992 are what follows, then dans is more likely. The basic model is blind to context, always assigning a probability of 0:3004 to dans and 0:0044 to pendant.
This can yield errors when Candide is called upon to translate a French sentence. Examples of two such errors are shown in Figure 5. In the rst example, the system has chosen an English sentence in which the French word superieures has been rendered as superior when greater or higher is a preferable translation. With no knowledge of context, an expert translator is also quite likely to select superior as the English word which generates superieures. But if the expert were privy to the fact that 50 was among the next few words, he might be more inclined to select greater or higher. Similarly, in the second example, the incorrect rendering of Il as He might have been avoided had the translation model used the fact that the word following it is appears.

5.2 Context-Dependent Word Models

In the hope of rectifying these errors, we consider the problem of context-sensitive mod-

eling of word translation. We envision, in practice, a separate maximum entropy model,

peyjx, for each English translator would choose

ywoarsdteh,ewFhreernechperyenjxderrienpgreosfenet,sgtihveenprtohbeabsuilrirtoyutnhdaitnganEenxgpleisrht

context x. This is just a slightly recast version of a longstanding problem in compu-

tational linguistics, namely sense disambiguation|the determination of a word's sense

from its context.

We begin with a training sample of English-French sentence pairs E; F randomly

esexntrteanctceedpfariorm, wteheuHseanthsaerbdacsoicrpturas,nssulacthiotnhamtoEdeclotnotacionms pthueteEtnhgeliVshitwerobridailnig. nFmoreneatcAh^

between E and F. Using this alignment, we then construct an x; y training event. The

event consists y equal to the

of a context French word

x containing the six words in E surrounding which is according to the Viterbi alignment

Ai^n

and a future aligned with

in. A few actual examples of such events for in are depicted in Table 3.

Next we de ne the set of candidate features. For this application, we employ features

that are indicator functions of simply described sets. Speci cally, we consider functions

fx; y which are one if y is some particular French word and the context x contains a

given English word, and are zero otherwise. We employ the following notation to represent

these features:

19

Computational Linguistics

Volume 22, Number 1

translation dans a au cours de dans a de

e,3 the work by of not

e,2 committee was the diphtheria given

e,1 stated required government reported notice



e+1 a respect the the Canada the

e+2 letter of scal same , ordinary

e+3 to the year postal by way

Table 3
Several actual training events for the maximum entropy translation model for in, extracted from the transcribed proceedings of the Canadian parliament.

20

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

Number of

Template 1

actual features
jVF j

fx;y = 1 if and only if : : : y=3

2 jVFj jVEj y = 3 and 2 2

3 jVFj jVEj y = 3 and 2 2

4 jVFj jVEj y = 3 and 2 2

5 jVFj jVEj y = 3 and 2 2

Table 4

jFVeFajtuthree

templates for word-translation size of the French vocabulary.

modeling.

jVEj

is

the

size

of

the

English

vocabulary;

8

f1x; y

=

:

1 0

y =en and April 2 otherwise

8

f2x;

y

=

:

1 0

y =pendant and weeks 2 otherwise

Here f1 = 1 when April follows in and en is the translation of in; f2 = 1 when weeks is one of the three words following in and pendant is the translation.
The set of features under consideration is vast, but may be expressed in abbreviated form in Table 4. In the table, the symbol 3 is a placeholder for a possible French word aaafrnbnoddomvjteVhtFieesmjtsthypoumltasatbldeoFel5rr2ievwneciidsthhfpwrl3oaomcr=edhtspeo,emltndhpdeealrranefttoeara2renawdjpVito2Fhsjs3=tibewml=eeepeEnklansa.tgnelI-idf1sht2fhewea=rtoeAurdrpae.rrseiT,l;ahjVtnehEdefjejtfVaeotEatujtarulejrVEefFn1fjg2fmleiisasehtndutewriroeoisvnreedoddsf templates 2,3,4 and 5.
Template 1 features give rise to constraints that enforce equality between the probability of any French translation y of in according to the model and the probability of that translation in the empirical distribution. Examples of such constraints are

py = dans = p~y = dans py = a = p~y = a py = de = p~y = de py = en = p~y = en ...

A maximum entropy model that uses only template 1 features predicts each French

21

Computational Linguistics

Volume 22, Number 1

translation y with the probability p~y determined by the empirical data. This is exactly the distribution employed by the basic translation model.
Since template 1 features are independent of x, the maximum entropy model which employs only constraints derived from template 1 features takes no account of contextual information in assigning a probability to y. When we include constraints derived from template 2 features, we take our rst step towards a context-dependent model. Rather than simply constraining the expected probability of a French word y to equal its empirical probability, these constraints require that the expected joint probability of the English word immediately following in and the French rendering of in be equal to its empirical probability. An example of a template 2 constraint is

py = pendant; e+1 = several = p~y = pendant; e+1 = several

A maximum entropy model that incorporates this constraint will predict the translations

of in in a manner consistent with whether or not the following word is several. In par-

ticular, if in the empirical sample, the presence of several led to a greater probability for

pendant, this will be re ected in a maximum entropy model incorporating this constraint.

We have thus taken our rst step toward context-sensitive translation modeling.

Templates 3, 4 and 5 consider, each in a di erent way, various parts of the context.

For instance, template 5 constraints allow us to model how an expert translator is biased

by the appearance of a word somewhere in the three words following the word he is

translating. If house appears within the next three words e.g. the phrases in the house

and in the red house, then dans might be a more likely translation. On the other hand,

if year appears within the same window of words as in in the year 1941 or in that fateful

year, then au cours de might be more likely. Together, the ve constraint templates

allow the model to condition its assignment of probabilities on a window of six words

around We

e0, the word constructed

in question. a maximum

entropy

model

pinyjx

by

the

iterative

model-growing

method described in Section 4. The automatic feature selection algorithm rst selected

a template 1 constraint for each of the translations of in seen in the sample 12 in all,

thus constraining the model's expected probability of each of these translations to their

empirical probabilities. The next few constraints selected by the algorithm are shown

in Table 5. The rst column gives the identity of the feature whose expected value is

constrained; the second column gives LS; f, the approximate increase in the model's

log-likelihood on the data as a result of imposing this constraint; the third column gives

Lp, the log-likelihood after adjoining the feature and recomputing the model.

Let us consider the fth row in the table. This constraint requires that the model's

expected probability of dans, if one of the three words to the right of in is the word

speech, is equal to that in the empirical sample. Before imposing this constraint on the

model during the iterative model-growing process, the log-likelihood of the current model

on the empirical sample was ,2:8703 bits. The feature selection algorithm described in

Section 4 calculated that if this constraint were imposed on the model, the log-likelihood

would rise by approximately 0:019059 bits; since this value was higher than for any other

constraint considered, the constraint was selected. After applying iterative scaling to

recompute the parameters of the new model, the likelihood of the empirical sample rose

to ,2:8525 bits, an increase of 0:0178 bits.

Table 6 lists the rst few selected features for the model for translating the English

word run. The Hansard avor"|the rather speci c domain of parliamentary discourse

related to Canadian a airs|is easy to detect in many of the features in this Table 5.

It is not hard to incorporate the maximum entropy word translation models into a

translation model pFjE for a French sentence given an English sentence. We merely

22

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

Feature fx; y
y=a and Canada 2

LS; f Lp 0:0415 ,2:9674

y=a and House 2

0:0361 ,2:9281

y=en and the 2

0:0221 ,2:8944

y=pour and order 2

0:0224 ,2:8703

y=dans and speech 2

0:0190 ,2:8525

y=dans and area 2

0:0153 ,2:8377

y=de and increase 2

0:0151 ,2:8209

y= verb marker and my 2

0:0141 ,2:8034

y=dans and case 2

0:0116 ,2:7918

y=au cours de and year 2

0:0104 ,2:7792

Table 5
Maximum entropy model to predict French translation of in. Features shown here were the rst non template 1 features selected. verb marker denotes a morphological marker inserted to indicate the presence of a verb as the next word.

23

Computational Linguistics

Volume 22, Number 1

Feature fx; y
y=epuiser and out 2

LS;f Lp 0:0252 ,4:8499

y=manquer and out 2

0:0221 ,4:8201

y=ecouler and time 2

0:0157 ,4:7969

y=accumuler and up 2

0:0149 ,4:7771

y=nous and we 2

0:0140 ,4:7582

y=aller and counter 2

0:0131 ,4:7445

y=candidat and for 2

0:0124 ,4:7295

y=diriger and the 2

0:0123 ,4:7146

Table 6
Maximum entropy model to predict French translation of to run: top-ranked features not from template 1

24

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

||||||||||||||| Je dirais m^eme que les chances sont superieures a 50. I would even say that the odds are greater than 50.

Il semble que Bank of Boston ait pratiquement acheve son reexamen de Shawmut.

It appears that Bank of Boston has almost completed its review of Shawmut .

Figure 6

|||||||||||||||

Improved French-to-English translations resulting from maximum entropy-based system

replace the simple context-independent models pyje used in the basic translation model 33 with the more general context-dependent models peyjx:

pF;

AjE

=

YjEj
i=1

pnei

jei



YjF j
j=1

peaj

yj

jxaj





dAjE

;

F



wherFeigxuarjed6enilolutsetsrathtees

context of how using

tthheisEimngplrisohvewdotrrdanesalja.tion

model

in

the

Candide

system

led to improved translations for the two sample sentences given earlier.

5.3 Segmentation

Though an ideal machine translation system could devour input sentences of unrestricted

length, a typical stochastic system must cut the French sentence into polite lengths before

digesting them. If the processing time is exponential in the length of the input passage

as is the case with the Candide system, then not splitting the French sentence into

reasonably-sized segments would result in an exponential slowdown in translation.

Thus, a common task in machine translation is to nd safe positions at which to

split input sentences in order to speed the translation process. Safe" is a vague term;

one might, for instance, reasonably de ne a safe segmentation as one which results in

cthoheeVreitnetrbbiloaclkigsnomf wenotrdA^s.bFeotwr eoeunr

purposes, the input

however, a safe segmentation is dependent on French sentence F and its English translation

E.

pbwaykoosrwidtWoisoaretndjod.ssetjIthno=neotelh1teahe;fte2rrr;ioif4gfwt;htao5thsrieodnafsE,tpenhaotgejhsl.ieFitsIirhnwoennowtrchjodherisdnsaetlFenoigatjtensh,numecaceehnl.ndettOfhttoanhfoteeffvwogtihruoseurraedalFlsl4rkm,teonfeoctthrhhj,oeewxdaroakiorgmdfhpdtyaleeojjt,feaatryrhnmjeediargnefreoiennarggeraerewlanlrthekiefredtatshtbeaejdyrt,

a rift up to lines,

occurs after the French

the last position

letter f is a

oriffte.aj

;

if

word j the line

is to try to trace a line from the last letter of yj can be drawn without intersecting any alignment

Using our de nition of rifts, we can rede ne a safe" segmentation as one in which the

segment boundaries are located only at rifts. Figure 7 illustrates an unsafe segmentation,

in which a segment boundary denoted by the k symbol lies between a and mange, where

there is no rift. Figure 8, on the other hand, illustrates a safe segmentation.

The reader will notice that a safe segmentation does not necessarily result in semantically coherent segments: mes and devoirs are certainly part of one logical unit,

25

Computational Linguistics

Volume 22, Number 1

The1 dog2 ateH3HHHHmyH4HHhHomHewHHorHk5HH

Le1 chien2 a3 k mange4 mes5 k devoirs6

Figure 7

Example of y3 and y4

an unsafe in two di

segmentation. A word in the translated erent segments of the input sentence.

sentence

e3

is

aligned

to

words

yet are separated in this safe segmentation. Once such a safe segmentation has been applied to the French sentence, we can make the assumption while searching for the appropriate English translation that no word in the translated English sentence will have to account for French words located in multiple segments. Disallowing intersegment alignments dramatically reduces the scale of the computation involved in generating a translation, particularly for large sentences. We can consider each segment sequentially while generating the translation, working from left to right in the French sentence.

The1 dog2
Le1 k chien2 Figure 8
Example of a safe segmentation

ateH3HHHHmyH4HHhHomHewHHorHk5 HH
a3 mange4 mes5 k devoirs6

We now describe a maximum entropy model which assigns to each location in a

French sentence a score which is a measure of the safety in cutting the sentence at

that location. We begin as in the word translation problem, with a training sample of

English-French sentence pairs E; F randomly extracted from the Hansard corpus. For

Ae^acbhetsweneteenncEe

pair and

we use the basic F. We also use

translation model a stochastic part

to of

compute the Viterbi alignment speech tagger as described in

Merialdo 1990 to label each word in F with its part of speech. For each position j in F

we then construct a x; y training event. The value y is rift if a rift belongs at position

j and is no-rift otherwise. The context information x is reminiscent of that employed

in the word translation application described earlier. It includes a six-word window of

French words: three to the left of yj and three to the right of yj. It also includes the

part-of-speech tags for these words, and the classes of these words as derived from a

mutual-information clustering scheme described in Brown et al 1990. The complete

x; y pair is illustrated in Figure 9.

In creating priftjx, we are at least in principle modeling the decisions of an

expert French segmenter. We have a sample of his work in the training sample p~x; y,

rift? eai,3 : : : eai+3 tageai,3 : : : tageai+3 classeai,3 : : : classeai+3

Figurey9
x;y for sentence segmentation

x

26

Berger, Della Pietra, Della Pietra
-0.75
-0.8

A Maximum Entropy Approach to NLP
Training Held-out

-0.85

log-likelihood

-0.9

-0.95

-1 0 20 40 60 80 100 120 Number of Features
Figure 10
Change in log-likelihood during segmenting model-growing. Overtraining begins to occur at about 40 features

and we measure the worth of a model by the log-likelihood Lp~p. During the iterative

model-growing procedure, the algorithm selects constraints on the basis of how much they

increase this objective function. As the algorithm proceeds, more and more constraints

are imposed on the model p, bringing it into ever-stricter compliance with the empirical

data p~x; y. This is useful to a point; insofar as the empirical data embodies the expert

knowledge of the French segmenter, we would like to incorporate this knowledge into

a model. But the data contains only so much expert knowledge; the algorithm should

terminate when it has extracted this knowledge. Otherwise, the model pyjx will begin

to t itself to quirks in the empirical data.

A standard approach in statistical modeling to avoid the problem of over tting the

training data is employ cross-validation techniques. Separate the training data p~x; y

ipAcnorstonocstteharseast;iranatltgihnoaiimrtnitgpihsop,msoseredptleiraocolntgl,orfewep~sarsst,eupsar,netLods p~babraehspteetedldrtohoauunctschopionuowcnrrttemioafnouser,cshp~thhmt.ehoUenryasoentinodonconrimlecyaapslp~leryro.tichAneesstshllioewknemhlgiihcoahodsoegled-eagnLcrehop~rwrantipeenwdg.

hboowthevp~err,anthdep~nh,ewthecoqnusatrnatiintytsLnp~hoplonaglesro

increases. At help p model

the the

point when over tting random process, but

begins, instead

rrdsietasoqetpua,FpibrLeieugdp~tuhpirLneptcp~o1hr.0emHapisolaliddunnesgotlt,hrlttaeohhtneaeegslngetroonhriadesilteochemhimsna.ondIttgteheerleimsipnsiaawntlmoaotgtuphe-llilddesikwppc~eorlohiniheinttnoatsoeittndlhhf.eoaslfAtliotgttrghha-tteilhlniykaiisenllglegpisohosdroiiatntohthtda,amonLLfspp~~4htrr0hopepufelhadacetnolutdednr-retomhisnu.eiutlndeda-sotaeutt.oat

We have employed this segmenting model as a component in a French-English ma-

chine translation system in the following manner. The model assigns to each position in

the French sentence a score, prift j x, which is a measure of how appropriate a split

would be at that location. A dynamic programming algorithm then selects, given the

appropriateness" score at each position and the requirement that no segment may con-

tain more than 10 words, an optimal or, at least, reasonable splitting of the sentence.

27

Computational Linguistics

Volume 22, Number 1

Monsieur l'Orateur ,
j'aimerais poser une question au Ministre des Transports. | A quelle date le
nouveau reglement devrait il entrer en vigeur? |
Quels furent les criteres utilises pour l'evaluation de ces biens. | Nous
savons que si nous pouvions contro^ler la folle avoine dans l'ouest du Canada, en un an nous
augmenterions notre rendement en cereales de 1 milliard de dollars.
Figure 11
Maximum entropy segmenter behavior on four sentences selected at random from the Hansard data
Figure 11 shows the system's segmentation of four sentences selected at random from the Hansard data. We remind the reader to keep in mind when evaluating Figure 11 that the segmenter's task is not to produce logically coherent blocks of words, but to divide the sentence into blocks which can be translated sequentially from left to right.
5.4 Word Reordering
Translating a French sentence into English involves not only selecting appropriate English renderings of the words in the French sentence, but also selecting an ordering for the English words. This order is often very di erent from the French word order. One way Candide captures word-order di erences in the two languages is to allow for alignments with crossing lines. In addition, Candide performs, during a pre-processing stage, a reordering step which shu es the words in the input French sentence into an order more closely resembling English word order.
One component of this word reordering step deals with French phrases which have the noun de noun form. For some noun de noun phrases, the best English translation is nearly word for word: con it d'inter^et, for example, is almost always rendered as con ict of interest. For other phrases, however, the best translation is obtained by interchanging the two nouns and dropping the de. The French phrase taux d'inter^et, for example, is best rendered as interest rate. Table 7 gives several examples of noun de noun phrases together with their most appropriate English translations.
In this section we describe a maximumentropy model which, given a French noun de noun phrase, estimates the probability that the best English translation involves an interchange of the two nouns. We begin with a sample of English-French sentence pairs E; F ransdbeoentmtwelenyecneexttphraaeicrtwewoderdfrusosmeintthEheeabHnaadsnicFsat.rrdUanscsionlragptiuAo^sn, wsmuecohcdoetnlhsattortuFccotmcaopnnutatxein;tsyhaetVrdaeiitneiprnbhgiraaesvleieg.nnFtmoaresneatfocAlh^lows. We let the context x be the pair of French nouns nounL; nounR. We let y be
28

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

word-for-word phrases

somme d'argent

sum of money

pays d'origin

country of origin

question de privilege question of privilege

con it d'initnert^eetrchangedcponhraicsteosf interest

bureau de poste

post o ce

taux d'inter^et

interest rate

compagnie d'assurance insurance company

gardien de prison

prison guard

Table 7
noun de noun phrases and their English equivalents

29

Computational Linguistics

Volume 22, Number 1

Template 1 2 3

Number of actual features
222jVjjVVFFFj2jj

y=3 y=3 y=3

fx;y = 1 if and only if : : :

and and

nounL nounR

= =

2 2

and nounL = 21 and nounR = 22

Table 8
Template features for noun de noun model

no-interchange if the English translation is a word-for-word translation of the French

phrase and y = interchange if the order of the nouns in the English and French phrases

are interchanged.

We de ne candidate features based upon the template features shown in Table 8. In

this table, the symbol 3 is a placeholder for either interchange or no-interchange and

the symbols 21 and French words, there

22 are placeholders for possible French words. are 2jVFj possible features of templates 1 and

If 2

atnhdere2jVarFej2jVfFeajttuorteasl

of template 3.

Template 1 features consider only the left noun. We expect these features to be

relevant when the decision of whether to interchange the nouns is in uenced by the

identity of the left noun. For example, including the template 1 feature

fx; y =

1 y=interchange and nounL= systeme 0 otherwise

gives the model sensitivity to the fact that the nouns in French noun de noun phrases which begin with systeme such as systeme de surveillance and systeme de quota are more likely to be interchanged in the English translation. Similarly, including the template 1 feature

fx; y =

1 y=no-interchange and nounL= mois 0 otherwise

gives the model sensitivity to the fact that French noun de noun phrases which begin with mois, such as mois de mai month of May are more likely to be translated word for word.
Template 3 features are useful in dealing with translating noun de noun phrases in which the interchange decision is in uenced by both nouns. For example, noun de noun phrases ending in inter^et are sometimes translated word for word, as in con it d'inter^et con ict of interest and are sometimes interchanged, as in taux d'inter^et interest rate.

We used the feature-selection algorithm of section 4 to construct a maximumentropy model from candidate features derived from templates 1,2 and 3. The model was grown on 10,000 training events randomly selected from the Hansard corpus. The nal model contained 358 constraints.
To test the model, we constructed a noun de noun word-reordering module which interchanges the order of the nouns if pinterchange j x 0:5 and keeps the order the same otherwise. Table 9 compares performance on a suite of test data against a baseline noun de noun reordering module which never the swaps the word order.
Table 12 shows some randomly-chosen noun de noun phrases extracted from this test suite along with pinterchangejx, the probability which the model assigned to inversion. On the right are phrases such as saison d'hiver for which the model strongly

30

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

Test data 50,229 not interchanged 21,326 interchanged 71,555 total

Simple Model Accuracy 100 0 70.2

Maximum Entropy Model Accuracy
93.5 49.2 80.4

Table 9
noun de noun model performance: simple approach vs. maximum entropy

cshomaomimsmbedreed'dameragiceontmmerce canoicnvqseutiarsuiutcidt'oiionnfnldaedt'ithoearnrbietation taux d'inflation cout de transport systeme de quota tpblaauurnxeadd'euurbdgaeespnecoeste saison d'hiver

.006 .018 .043 .195 .206 .224

.440 .555

.723

.845 .911 .922 .997

smaller...

p(interchange)

...larger

Figure 12

Predictions of the noun de noun interchange model on phrases selected from a corpus

unseen during the training process

31

Computational Linguistics

Volume 22, Number 1

predicted an inversion. On the left are phrases which the model strongly prefers not to interchange, such as somme d'argent, abus de privilege and chambre de commerce. Perhaps most intriguing are those phrases which lie in the middle, such as taux d'in ation, which can translate either to in ation rate or rate of in ation.
6. Conclusion
We began by introducing the building blocks of maximumentropy modeling|real-valued features and constraints built from these features. We then discussed the maximum entropy principle. This principle instructs us to choose, among all the models consistent with the constraints, the model with the greatest entropy. We observed that this model was a member of an exponential familywith one adjustable parameter for each constraint. The optimal values of these parameters are obtained by maximizing the likelihood of the training data. Thus two di erent philosophical approaches|maximum entropy and maximum likelihood|yield the same result: the model with the greatest entropy consistent with the constraints is the same as the exponential model which best predicts the sample of data.
We next discussed algorithms for constructing maximumentropy models, concentrating our attention on the two main problems facing would-be modelers: selecting a set of features to include in a model, and computing the parameters of a model which contains these features. The general feature-selection is too slow in practice, and we presented several techniques for making the algorithm feasible.
In the second part of this paper we described several applications of our algorithms, concerning modeling tasks arising in Candide, an automatic machine-translation system under development at IBM. These applications demonstrate the e cacy of maximum entropy techniques for performing context-sensitive modeling.
Acknowledgments
The authors wish to thank Harry Printz and John La erty for suggestions and comments on a preliminary draft of this paper, and Jerome Bellegarda for providing expert French knowledge.
References
Bahl, L., Brown, P., de Souza, P., Mercer, R. 1989 A tree-based statistical language model for natural language speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 7.
Berger, A., Brown, P., Della Pietra, S., Della Pietra, V., Gillett, J., La erty, J., Printz, H., Ures, L. 1994 The Candide system for machine translation. Proceedings of the ARPA Conference on Human Language Technology, Plainsborough, New Jersey.
Black, E., Jelinek, F., La erty, J., Magerman, D., Mercer, R. and Roukos, S. 1992 Towards history-based grammars: using richer models for probabilistic parsing. Proceedings of the DARPA Speech and Natural Language Workshop, Arden House, New York.
Brown, D. 1959 A note on approximations to discrete probability distributions. Information and Control, vol. 2, 386 392.
Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. 1993 The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, vol. 19, no. 2, 263 311.
Brown, P., Cocke, J. Della Pietra, S., Della Pietra, V., Jelinek, F., La erty, J., Mercer, R., and Roossin, P. 1990 A statistical approach to machine translation. Computational Linguistics, vol. 16, 79 85.
Brown, P., Della Pietra, V, de Souza, P., Mercer, R. 1990 Class-based n-gram models of natural language. Proceedings of the IBM Natural Language ITL, 283 298.

32

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

Brown, P., Della Pietra, S., Della Pietra, V., Mercer, R. 1991 A statistical approach to sense disambiguation in machine translation. DARPA Workshop on Speech and Natural Language, 146 151.
Cover, T. and Thomas, J. 1991 Elements of Information Theory. John Wiley & Sons. Csiszar, I. 1975 I-Divergence geometry of probability distributions and minimization
problems, The Annals of Probability, vol. 3, No. 1, 146 158. ibid. 1989 A geometric interpretation of Darroch and Ratcli 's generalized iterative scaling.
The Annals of Statistics, vol. 17, No. 3, 1409 1413. Csiszar, L. and Tusnady, G. 1984 Information geometry and alternating minimization
procedures. Statistics & Decisions, Supplemental Issue, no. 1, 205 237. Darroch, J.N. and Ratcli , D. 1972 Generalized iterative scaling for log-linear models.
Annals of Mathematical Statistics, no. 43, 1470 1480. Della Pietra, S., Della Pietra, V., Gillett, J., La erty, J., Printz, H., Ures, L. 1994 Inference
and estimation of a long-range trigram model. Second International Symposium on Grammatical Inference, Alicante, Spain. Della Pietra, S., Della Pietra, V., La erty, J. Inducing features of random elds, 1995 CMU Technical Report CMU-CS-95-144. Dempster, A.P., Laird, N.M., and Rubin, D.B. 1977 Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, vol 39, no. B, 1 38. Guiasu, S. and Shenitzer, A. 1985 The principle of maximum entropy. The Mathematical Intelligencer, vol. 7, no. 1. Jaynes, E.T. 1990 Notes on present status and future prospects. In Grandy, W.T., and Schick, L.H. Maximum Entropy and Bayesian Methods. Kluwer. 1 13. Jelinek, F. and Mercer, R. L. 1980 Interpolated estimation of Markov source parameters from sparse data. In Proceedings, Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands. Lucassen, J. and Mercer, R. 1984 An information theoretic approach to automatic determination of phonemic baseforms. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, . San Diego, CA, 42.5.1 42.5.4. Merialdo, B. 1990 Tagging text with a probabilistic model. Proceedings of the IBM Natural Language ITL, Paris, France, 161 172. Nadas, A. Mercer, R., Bahl, L., Bakis, R., Cohen, P., Cole, A., Jelinek, F., and Lewis, B. 1981 Continuous speech recognition with automatically selected acoustic prototypes obtained by either bootstrapping or clustering. Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Atlanta, GA, 1153 1155. Sokolniko , I. S., Redhe er, R. M. 1966 Mathematics of Physics and Modern Engineering, Second Edition, McGraw-Hill Book Company.

Appendix: E cient Algorithms for Feature Selection

Computing the Approximate Gain of One Feature

This section picks up where section 4 left o , describing in some detail a set of algorithms

which implement the feature selection process e ciently.

fGGoS0rS;f;aWfcea?nordsciitcdsuadzrteesesrcofre.eixabTcteeoupraetnnifidnt.etrrThaahitrsieevzecaealragsoleogsrowirteihattmhapmtphislefyobruNancseioeqwmdutepoounnvt'asitnlhuigteeerfaa?tcitLvaettShrw;aofthoitct-hhenmtdhmiaenxagdxiemGmriSevut;amfhtoivodef.

An important twist is that we do not use the updates obtained by applying Newton's

method directly in the variable . This is because there is no guarantee that GS;f  n

increases Newton's

monotonically for such method in the variables

eupdoratee,s.

Instead, we use updates derived by applying . A convexity argument shows that using these

ugMapoidnraTetohevseertvL,haeilSfuse;efqnu?iestnhacGanetSyom;ffseGaqxSu?i;mefniaczneendsfGotcrhSoa;wnftvheircnghecisaGnncm0rSbeo;aefnsfoeotsunonmndicocobannlyolvytseootrnlogviectisnahlgemlytmohtnaeooxetiqom?un.uaicmtaiolalnypGptor0So0x;f,imtha?etne= 0.

33

Computational Linguistics

Volume 22, Number 1

GS;f  n will increase monotonically. This is a consequence of the convexity of GS;f   in .
We can solve an equation g  = 0 by Newton's method, which produces a sequence n by the recurrence given in 18, repeated here for convenience:

n+1 =

n,

g g0

n n

34

If we start with 0 su ciently close to ?, then the sequence n will converge to ? and g n will converge to zero. In general, though, the g n will not be monotonic. However, it can be shown that the sequence is monotonic in the following important ceiammztaepoers,pcotenalhas.TTyon:noIthhinNddofeebneieeifiunwsiuc0tpsatnheihdoln,celonacytrwti'rosecet?ntnsoaamossarGeizneoenb,etsdb0gyrhSuGot;golaa.fSttdiain;nSnfkdiignimancifisgsienorsleonaeqdnmivriutteelnehoyriexctN,ienrhv-oriceeafebewadtrtsieiatvencc?oiseonrnnseenm'fa,avsoseot0rimsxhnn.ew-asogqeTottthuahohieotGcnnounhrhdids0cacSc,eGatcoa;iflonpof0leSnyfvp0;velftie?xoenx-di-isentfio,snad0romsedrt?wcseaha,oricheenxfwriauticecvmhesrahnaieasncucraetGtigamssainioe0bnSma0nGlg;anefoSopdane;fpfnroclendyto.o,nc??NnHfiov,oninee.ocrwcwxvawrre-eletelxaoyv=c-sneatei'rnno1ss., or r = ,1 are easily computed:

n+1 =

n+

1 r

log

 1,

1 r

GG000SS;;ff

n  n

35

In order to solve the recurrence in 35, zeroth, rst and second derivatives of G are

we

need

to

compute

G0S;f

and

G00S;f .

The

GS;f   = , X p~x log Z x + p~f

G0S;f   G00S;f  

= =

p,~fXxx ,p~Xxxpp~S;xfpfS;,f fpSjx;ffjx2jx

36 37 38

where pS;f hjx  X pS;f yjxhx; y
y

39

With these in place, we are ready to enumerate
Algorithm 3: Computing the Gain of a Single Feature

Input: Output:

Empirical distribution p~x; y; Approximate gain LS; f

initial model of feature f

pS

;

candidate

feature

f

1. Let 2. Set 0  0

r=

1 ,1

iofthp~efrw isepSf

40

34

Berger, Della Pietra, Della Pietra

A Maximum Entropy Approach to NLP

3. Repeat the following until GS;f  n has converged:

Compute Compute

GnS+;f1frno+m1

n using 35 using 26

4. Set LS; f  GS;f  n

Computing Approximate Gains in Parallel
For the purpose of incremental model growing as outlined in Algorithm 2, we need to compute the maximum approximate gain LS; f for each candidate feature f 2 F. One obvious approach is to cycle through all candidate features and apply Algorithm 3 for each one sequentially. Since Algorithm 3 requires one pass through every event in the training sample per iteration, this could entail millions of passes through the training sample. Because a signi cant cost often exists for reading the training data|if the data cannot be stored in memory but must be accessed from disk, for example|an algorithm which passes a minimal number of times through the data may be of some utility. We now give a parallel algorithm speci cally tailored to this scenario.
Algorithm 4: Computing Approximate Gains for A Collection of Features

Input: Collection F of candidate features; empirical distribution p~x; y;

Output:

initial model Approximate

pgSain

LS

;

f

for

each

candidate

feature

f

2

F

1. For each f 2 F, calculate p~f, the expected value of f in the training data 2. For each x, determine the set Fx  F of f that are active for x:

Fx  ff 2 F j fx; ypSyjxp~x 0 for some yg 3. For each f, let

41

rf =

1 ,1

if p~f  pSf otherwise

42

4. For each f 2 F, initialize f  0 5. Repeat the following until f converges for each f 2 F:
a For each f 2 F, set

G0f  p~f G00f  0

b For each x, do the following: For each f 2 Fx, update G0f and G00f by

G0f  G0f , p~xpS;f fjx

43

G00f  G00f , p~xpS;f f , pS;f fjx2jx44

where

pS;f

f

jx



P
y

pS;f

yjxfx;

y

35

Computational Linguistics

Volume 22, Number 1

c For each f 2 F, update f by

f 

f

+

1 rf

log

 1,

1 rf

G0f  G00f

45

6. For each f 2 F, substitute f into 26 to determine LS; f.

Convergence for this algorithm is guaranteed just as it was for algorithm 3 after

each iteration optimal value LS; f.

o?fsftepan5d,,

the value of f for each candidate feature more importantly, the gain GS;f is closer to

f is closer to its the maximal gain

36

Alpino: Wide-coverage Computational Analysis of Dutch
Gosse Bouma, Gertjan van Noord, and Robert Malouf
Alfa-informatica Rijksuniversiteit Groningen
Abstract
Alpino is a wide-coverage computational analyzer of Dutch which aims at accurate, full, parsing of unrestricted text. We describe the head-driven lexicalized grammar and the lexical component, which has been derived from existing resources. The grammar produces dependency structures, thus providing a reasonably abstract and theory-neutral level of linguistic representation. An important aspect of wide-coverage parsing is robustness and disambiguation. The dependency relations encoded in the dependency structures have been used to develop and evaluate both hand-coded and statistical disambiguation methods.
1 Introduction
For English, tremendous progress has been made in the area of wide-coverage parsing of unrestricted text. Many of the proposed systems are statistical parsers, but systems based on a hand-written grammar exist as well. The aim of Alpino1 is to provide computational analysis of Dutch with coverage and accuracy comparable to state-of-the-art parsers for English.
The Alpino grammar (described in more detail below) is a lexicalized grammar in the tradition of constructionalist Head-driven Phrase Structure Grammar (Pollard and Sag 1994, Sag 1997). The grammar consists of hand-written, linguistically motivated rules and lexical types. To evaluate the coverage and disambiguation component of the system, a testbench of syntactically annotated material is absolutely crucial. Given the current lack of such material for Dutch, we have started to annotate corpora with dependency structures. Dependency structures provide a convenient level of representation for annotation, and a fairly neutral representation for further processing. The annotation format is taken from the project Corpus Gesproken Nederlands (Corpus of Spoken Dutch) (Oostdijk 2000). The construction of dependency structures in the grammar and our treebanking efforts are described in section 4. Both the lexicalist nature of the Alpino grammar and the use of dependency structures imply that lexical items must be associated with detailed valency information. For the Alpino lexicon we have extracted this information from the Celex and Parole lexical databases (section 3).
In section 5 we describe Alpino's parsing architecture. Section 6 describes a variety of disambiguation strategies which have been integrated in Alpino. In addition, we report on a number of preliminary disambiguation experiments. We conclude with some remarks on future work.
1Alpino is being developed as part of the NWO PIONIER project Algorithms for Linguistic Processing, www.let.rug.nl/~vannoord/alp

2 Gosse Bouma, Gertjan van Noord, and Robert Malouf
2 Grammar
The Alpino grammar is an extension of the successful OVIS grammar (van Noord, Bouma, Koeling and Nederhof 1999, Veldhuijzen van Zanten, Bouma, Sima'an, van Noord and Bonnema 1999), a lexicalized grammar in the tradition of Headdriven Phrase Structure Grammar (Pollard and Sag 1994). The grammar formalism is carefully designed to allow linguistically sophisticated analyses as well as efficient and robust processing.
In contrast to earlier work on HPSG grammar rules in Alpino are relativey detailed. However, as pointed out in Sag (1997), by organizing rules in an inheritance hierarchy, the relevant linguistic generalizations can still be captured. The Alpino grammar currently contains over 100 rules, defined in terms of a few general rule structures and principles. The grammar covers the basic constructions of Dutch (including main and subordinate clauses, (indirect) questions, imperatives, (free) relative clauses, a wide range of verbal and nominal complementation and modification patterns, and coordination) as well as a wide variety of more idiosyncratic constructions (appositions, verb-particle constructions, PP's including a particle, NP's modified by an adverb, punctuation, etc.). The lexicon contains definitions for various nominal types (nouns with various complementation patterns, proper names, pronouns, temporal nouns, deverbalized nouns), various complementizer, determiner, and adverb types, adjectives, and 36 verbal subcategorization types.
The formalism supports the use of recursive constraints over feature-structures (using delayed evaluation, van Noord and Bouma (1994)). This allowed us to incorporate an analysis of cross-serial dependencies based on argument-inheritance (Bouma and van Noord 1998) and a trace-less account of extraction along the lines of Bouma, Malouf and Sag (2001).
3 Lexical Resources
Accurate, wide-coverage parsing of unrestricted text requires a lexical component with detailed subcategorization frames. For lexicalist grammar formalisms, the availability of lexical resources which specify subcategorization frames is even more crucial. In HPSG, for instance, phrase structure rules rely on the fact that each head contains a specification of the elements it subcategorizes for. If such specifications are missing, the grammar will wildly overgenerate.
We have used two existing lexical databases (Celex and Parole) to create a wide-coverage lexicon with detailed subcategorization frames enriched with dependency relations. Celex (Baayen, Piepenbrock and van Rijn 1993) is a large lexical database for Dutch, with rich phonological and morphological information. For use within the CGN project, this database has been extended with dependency frames (Groot 2000). This version of the lexicon contains 11,800 verbal stems, with a total of 21,800 dependency frames. By far the most frequent frames are those for intransitive (4,100) and transitive (6,500) verbs. A fair number of frames occurs more than 100 times, but 300 of the 650 different dependency frame types in the database occur only once.

Alpino: Wide-coverage Computational Analysis of Dutch

3

Dependency Frame
SU:NP][OBJ1:NP] SU:NP] SU:NP][PC:PP pform ] SU:NP][OBJ1:NP][PC:PP pform ] SU:NP][VC:S subordinate ] SUP:NP het ][OBJ1:NP][SU:CP] SU:NP][OBJ2:NP][OBJ1:NP] SU:NP][SE:NP][PC:PP pform ] SU:NP][SE:NP] SU:NP][VC:VP]

Overlap
1810 257 337 129 103 7 65 65 49 10

Celex only 1211 1697 541 375 136 247 171 62 137 16

Parole only 240 42 273 308 103 5 28 102 65 37

Total
3261 1996 1151
812 342 259 264 229 251 63

Table 1: Dependency Frames and the number of stems occurring with this frame in both resources, in CGN/Celex only, in Parole only, and the total number of stems with this dependency frame in the Alpino Lexicon.

The Dutch Parole lexicon2 comes with detailed subcategorization information, including dependency relations. The Parole lexicon is smaller than Celex, with 3,200 verbal stems and a total of 5000 dependency frames. There are 320 different dependency frame types, 190 of which occur only once.
Dependency frames for the Alpino lexicon have been constructed using the dependency information provided by CGN/Celex, Parole, and by entering definitions by hand. The latter has been done mostly for auxiliary and modal verbs: a small class of high-frequent elements which are exceptional in a number of ways. The CGN/Celex dictionary is very large. As the Celex database comes with frequency information, we currently only include those lexical items whose frequency is above a certain threshold. For verbal stems, this means that roughly 50% of the stems in Celex is included in the Alpino lexicon. All verbal stems from the Parole lexicon with a dependency frame covered by the grammar are included.
Currently, for 28 different CGN/Celex dependency frames a definition in the grammar has been provided. This covers over 80% of the verbal dependency frames in the CGN/Celex database, 10,400 of which are sufficiently frequent to be included in the Alpino lexicon. For 15 different dependency frames in the Parole lexicon a definition in Alpino is present. Using these, we extract over 4,100 dependency frames (82% of the total number of dependency frames in the Parole database). An overview of overlap and non-overlap for the most frequent frames extractable from both sources is given in table 1. For transitive and intransitive verbs, we see that over 85% of the stems in Parole are present in CGN/Celex as well. For most other dependency frames, however, the overlap is generally much smaller, and a significant portion of the stems present in Parole is not present in
2http://www.inl.nl/corp/parole.htm

4 Gosse Bouma, Gertjan van Noord, and Robert Malouf
Celex. This suggests that, for more specific subcategorization frames, both resources are only partially complete, and that not even the union of both provides exhaustive coverage.3
4 Dependency Structures
Within the CGN-project (Oostdijk 2000), guidelines have been developed for syntactic annotation of spoken Dutch (Moortgat, Schuurman and van der Wouden 2000), using dependency structures similar to those used for the German Negra corpus (Skut, Krenn and Uszkoreit 1997).
Dependency structures make explicit the dependency relations between constituents in a sentence. Each non-terminal node in a dependency structure consists of a head-daughter and a list of non-head daughters, whose dependency relation to the head is marked. A dependency structure for (1) is given in figure 1. Control relations are encoded by means of co-indexing (i.e. the subject of hebben is the dependent with index 1). Note that a dependency structure does not necessarily reflect (surface) syntactic constituency. The dependent haar nieuwe model gisteren aangekondigd, for instance, does not correspond to a (surface) syntactic constituent in (1).
(1) Mercedes zou haar nieuwe model gisteren hebben aangekondigd Mercedes should her new model yesterday have announced Mercedes should have announced her new model yesterday
The Alpino grammar produces dependency structures compatible with the CGN-guidelines. We believe this is a useful output format for a number of reasons. First of all, annotating a text with dependency structures is relatively straightforward and independent of the particular grammatical framework assumed. Thus, a dependency treebank can be used to debug and test various versions of the Alpino grammar. Second, as we adopt the CGN-guidelines, a considerable amount of annotated material will be available within the near future which can be used for development and testing. Third, it has been suggested that dependency relations provide a convenient level of representation for evaluation of computational grammar based on radically different grammatical theories (Carroll, Briscoe and Sanfilippo 1998). Finally, statistics for dependency relations between head words can be used to develop accurate models for parse-selection (Collins 1999); preliminary experiments are described in section 6.
Grammatical Construction of Dependency Structures. To produce dependency structures with the Alpino grammar, a new level of representation has been added to the grammar. The attribute DT dominates a dependency structure, with attributes for the lexical head (HD) and the various dependents. The value of a dependent attribute can be a dependency structure or a leaf node consisting of a
3The less frequent verb stems in Celex (currently not included in Alpino) are almost exclusively assigned the intransitive or transitive dependency frame.

Alpino: Wide-coverage Computational Analysis of Dutch

5

s

su 1 noun mercedes

hd verb zou

vc vp

su 1

hd verb hebben

vc vp

su 1

obj1 np

mod adv gisteren

det mod hd det adj noun haar nieuwe model

hd verb aangekondigd

Figure 1: Dependency structure for example (1).

¾ verb phon subcat
dt

¿¾ verb

¶1¾np

¿¾ np

¿· phon

case nom , case acc subcat

¾ dt
hd
su

2
pos word

dt

3
¿

verb

1

2

dt

obj1 3

¿

¶1¾np

¿¾ np

¿·

case nom , case dat

¾ dt
hd

2
pos word

dt

3
¿

verb

1

su 2 obj2 3

Figure 2: Schematic lexical entry for transitive verbs taking a direct object (OBJ1), and for transitive verbs taking an indirect object (OBJ2).

6 Gosse Bouma, Gertjan van Noord, and Robert Malouf
POS-tag and word only. The construction of dependency structures is driven by the lexicon. For each
subcategorization type recognized in the lexical hierarchy a mapping between elements on the list-valued feature which specifies basic subcategorization properties (SUBCAT) and attributes of DT is defined. Two examples are given in figure 2. The leftmost feature structure exemplifies a finite, transitive verb. The value of DT of the nominative NP on subcat is identical to the value of the SU dependent. Similarly, the value of DT of the accusative NP on subcat is identical to the value of the OBJ1 dependent. The rightmost feature structure exemplifies a finite, transitive verb for which the object is assigned to the OBJ2 (secondary object) dependency relation. In some cases, the addition of dependency structures leads to more finegrained distinctions. For instance, PP-arguments can be linked to PC (prepositional complement) or LD (locative or directional complement), where the distinction between these two is primarily semantic in nature. Therefore, verbs taking a prepositional complement are assigned a subcategorization frame that differs from the frame assigned to verbs taking such a LD complement.
In HEAD-COMPLEMENT structures, the DT attribute can simply be shared between head daughter and mother. In HEAD-MODIFIER structures, the dependency structure of the modifier is added to the list-valued MOD dependent of the head.
Dependency Treebanks. For development and evaluation purposes, we have started to annotate various sample text fragments with dependency structures.
The annotation process typically starts by parsing a sentence with the Alpino grammar. This produces a (often large) number of possible analyses. The annotator picks the analysis which best matches the correct analysis. To facilitate selection of the best parse among a large number of possibilities, the HDRUG environment has been extended with a graphical tool based on the SRI TreeBanker (Carter 1997) which displays all fragments of the input which are a source of ambiguity. By disambiguating these items (usually a much smaller number than the number of readings), the annotator can quickly pick the most accurate parse.
For example, the sentence Jan zag het meisje `Jan saw the girl' has (in principle) two readings corresponding to the dependency structures in figure 3. The readings of a sentence are represented as a set of sets of dependency paths, as in figure 4. From these sets of paths, the parse selection tool computes a set of maximal discriminants which can be used to select among different analyses. In this case, the path `s:hd = v zag' is shared by all the analyses and so is not a useful discriminant. On the other hand, the path `s:obj1:hd = n meisje' does distinguish between the readings but it is not maximal, since it is subsumed by the path `s:obj1 = np het meisje' which is shorter and makes exactly the same distinctions. The maximal discriminants are presented to the annotator, who may mark any of them as either good (the correct parse must include it) or bad (the correct parse may not include it). In this simple example, marking any one of the maximal discriminants as good or bad is sufficient to uniquely identify the correct parse. For more complex sentences, several choices will have to be made to select a single best parse. To help the annotator, when a discriminant is marked as bad or good, the following

Alpino: Wide-coverage Computational Analysis of Dutch ss

hd su verb noun zag jan

obj1 np

det hd det noun het meisje

hd verb zag

su np

obj1 noun jan

det hd det noun het meisje

7

Figure 3: Dependency structures for two readings of Jan zag het meisje.

s:hd = v zag *s:su = np jan *s:obj1 = np het meisje s:obj1:det = det het s:obj1:hd = n meisje

s:hd = v zag *s:su = np het meisje
s:su:det = det het s:su:hd = n meisje *s:obj1 = np jan

Figure 4: Dependency paths for Jan zag het meisje (* indicates a maximal discriminant).

inference rules are applied to further narrow the possibilities (Carter 1997):
¯ If a discriminant is bad, any parse which includes it is bad. ¯ If a discriminant is good, any parse which does not include it is bad. ¯ If a discriminant is only included in bad parses, it must be bad. ¯ If a discriminant is included in all the undecided parses, it must be good.
This allows users to focus their attention on discriminants about which they have clear intuitions. Their decisions about these discriminants combined with the rules of inference can then be used to automatically make decisions about less obvious discriminants.
If the parse selected by the annotator is fully correct, the dependency structure for that parse is stored as XML in the treebank. If the best parse produced by the grammar is not the correct parse as it should be included in the treebank, the dependency structure for this parse is sent to the Thistle editor.4 The annotator can now produce the correct parse manually.
We have started to annotate various smaller fragments using the annotation tools described above. The largest fragments consist of two sets of sentences ex-
4LT Thistle (Calder 2000), www.ltg.ed.ac.uk/software/thistle/, is an editor and display engine for linguistic data-structures which supports XML.

8 Gosse Bouma, Gertjan van Noord, and Robert Malouf
tracted from the Eindhoven corpus (Uit den Boogaart 1975). The CDBL10 treebank currently consists of the first 519 sentences of ten words or less from section CDBL (newspaper text). The CDBL20 treebank consists of the first 252 sentences with more than 10 but no more than 20 words.

Evaluation. Evaluation of coverage and accuracy of a computational grammar usually is based on some metric which compares tree structures (such as recall and precision of (labelled) brackets or bracketing inconsistencies (crossing brackets) between test item and parser output). As is well-known, such metrics have a number of drawbacks. Therefore, Carroll et al. (1998) propose to annotate sentences with triples of the form head-word, dependency relation, dependent head-word . For instance, for the example in (1) we might obtain:

zou, su, mercedes hebben, su, mercedes aangekondigd, su, mercedes aangekondigd, mod, gisteren

aangekondigd, obj1, model model, det, haar model, mod, nieuwe

Dependency relations between head-words can be extracted easily from the dependency structures in our treebank, as well as from the dependency structures constructed by the parser. It is thus straightforward to compute precision, recall, and f-score on the set of dependency triples.

5 Robust Parsing
The initial design and implementation of the Alpino parser is inherited from the system described in van Noord (1997), van Noord et al. (1999) and van Noord (2001). However, a number of improvements have been implemented which are described below.
The construction of a dependency structure on the basis of some input proceeds in a number of steps, described below. The first step consists of lexical analysis. In the second step a parse forest is constructed. The third step consists of the selection of the best parse from the parse forest.

Lexical Analysis. The lexicon associates a word or a sequence of words with one or more tags. Such tags contain information such as part-of-speech, inflection as well as a subcategorization frame. For verbs, the lexicon typically hypothesizes many different tags, differing mainly in the subcategorization frame. For sentence (1), the lexicon produces 83 tags. Some of those tags are obviously wrong. For example, one of the tags for the word hebben is verb(hebben,pl,part sbar transitive(door)). The tag indicates a finite plural verb which requires a separable prefix door, and which subcategorizes for an SBAR complement. Since door does not occur anywhere in sentence (1), this tag will not be useful for this sentence. A filter containing a number of hand-written rules has been implemented which checks that such simple condi-

Alpino: Wide-coverage Computational Analysis of Dutch

9

tions hold. For sentence (1), the filter removes 56 tags. After the filter has applied, feature structures are associated with each of these tags. Often, a single tag is mapped to multiple feature structures. The remaining 27 filtered tags give rise to 89 feature structures.
An important aspect of lexical analysis is the treatment of unknown words. The system applies a number of heuristics for unknown words. Currently, these heuristics attempt to deal with numbers and number-like expressions, capitalized words, words with missing diacritics, words with `too many' diacritics, compounds, and proper names.
If such heuristics still fail to provide an analysis, then the system guesses a tag by inspecting the suffix of the word. A list of suffixes is maintained which predict the tag of a given word. If this still does not provide an analysis, then it is assumed that the word is a noun.
In addition to the treatment of unknown words, the robustness of the system is enhanced by the possibility to skip tokens of the input. Currently this possibility is employed only for certain punctuation marks. Even though punctuation is treated both in the lexicon and the grammar, the syntax of punctuation is irregular enough to warrant the possibility to ignore punctuation. For instance, quotation marks may appear almost anywhere in the input. The corpus contains:
(2) De z.g. " speelstraat , die hier en daar al bestaat ? The so-called " play-street , that here and there already exists ?
Apparently, the author intended to place speelstraat within quotes, but the second quote is not present. During lexical analysis, tags are optionally extended to include neighbouring words which are classified as `skipable'.

Creating Parse Forests. The Alpino parser takes the result of lexical analysis as its input, and produces a parse forest: a compact representation of all parse trees. The Alpino parser is a left-corner parser with selective memoization and goal-weaking. It is a variant of the parsers described in van Noord (1997). We generalized some of the techniques described there to take into account relational constraints, which are delayed until sufficiently instantiated (van Noord and Bouma 1994).
As described in van Noord et al. (1999) and van Noord (2001), the parser can be instructed to find all occurrences of the start category anywhere in the input. This feature is added to enhance robustness as well. In case the parser cannot find an instance of the start category from the beginning of the sentence to the end, then the parser produces parse trees for large chunks of the input. A best-first search procedure then picks out the best sequence of such chunks. Depending on the application, such chunks might be very useful. In the past, we successfully employed this strategy in a spoken dialogue system (Veldhuijzen van Zanten et al. 1999).

10 Gosse Bouma, Gertjan van Noord, and Robert Malouf

beam
1 2 4 8 16 32 

cdbl10

accuracy (%) speed (msec)

79.99

190

80.66

270

81.11

350

81.22

530

81.36

590

81.36

790

81.36

640

cdbl20

accuracy (%) speed (msec)

73.63

740

74.59

1470

75.07

2350

75.35

3630

75.31

5460

74.98

7880

--

Table 2: Effect of beam-size on accuracy and efficiency of parse selection

Unpacking and Parse Selection. The motivation to construct a parse forest is efficiency: the number of parse trees for a given sentence can be enormous. In addition to this, in most applications the objective will not be to obtain all parse trees, but rather the best parse tree. Thus, the final component of the parser consists of a procedure to select these best parse trees from the parse forest.
In order to select the best parse tree from a parse forest, we assume a parse evaluation function which assigns a score to each parse. In section 6 we describe some initial experiments with a variety of parse evaluation functions. A naive algorithm constructs all possible parse trees, assigns each one a score, and then selects the best one. Since it is too inefficient to construct all parse trees, we have implemented the algorithm which computes parse trees from the parse forest as a best-first search. This requires that the parse evaluation function is extended to partial parse trees. In order to be able to guarantee that this search procedure indeed finds the best parse tree, a certain monotonicity requirement should apply to this evaluation function: if a (partial) tree s is better than s¼, then a tree t which contains s should be better than t¼ which is just like t except it has s¼ instead of s. However, instead of relying on such a requirement, we implemented a variant of a best-first search algorithm in such a way that for each state in the search space, we maintain the b best candidates, where b is a small integer (the beam). If the beam is decreased, then we run a larger risc of missing the best parse (but the result will typically still be a relatively `good' parse); if the beam is increased, then the amount of computation increases too. Currently, we find that a value of b 4 is a good compromise between accuracy and efficiency. In table 2 the effect of various values for b is presented for two development treebanks. The grammar assigns on average about 33 parse trees per sentence for the cdbl10 corpus. This number increases rapidly for longer sentences: for the cdbl20 corpus it is at least 340.5
5This is the average number after creating all parse trees for each sentence with a maximum of 1000 parse trees per sentence.

Alpino: Wide-coverage Computational Analysis of Dutch

11

6 Disambiguation
The best-first unpack strategy described in section 5 depends on a parse evaluation function which assigns scores to (partial) parse trees. We have experimented with a number of disambiguation techniques on the cdbl10 and cdbl20 development treebanks described earlier.

Penalty rules. The simplest disambiguation method consists of hand-written `penalty' rules which implement a variety of preferences. Each such penalty rule describes a partial parse tree. For a given parse tree, the system computes how often a sub-tree matches with a penalty rule, giving rise to the total penalty of that parse. The following lists characterizes some of the penalty rules:
¯ complementation is preferred over modification ¯ subject topicalization is preferred over object topicalization ¯ long distance dependencies are dis-preferred ¯ certain rules are dis-preferred (e.g. rules which coordinate categories with-
out an explicit coordinator)
¯ certain lexical entries are dis-preferred (e.g. the preposition readings for
the words aan, bij, in, naar, op, uit, voor, tussen are preferred over the adjectival, noun and/or verb readings).
¯ certain guesses for unknown words are preferred over others
As can be concluded from the preliminary results presented in table 3, it appears to be the case that about 60% of the disambiguation problem can be solved using this very simple technique.

Dependency relations We also experimented with statistical models based on dependency relations encoded in the dependency structure. The model assigns a probality to a parse by considering each dependency relation. For this purpose, dependency relations d are 5-tuples d wh ph r wa pa where wh is the head word, ph is the corresponding part-of-speech tag taken from a small set of part-ofspeechs v n a adv p , r is the name of the relation taken from a small set of relation names su,obj1,obj2,vc,mod,det ; wa is the argument word, and pa is its part of speech.
The probability of a parse y given a sentence x might then be defined as:

p´y xµ

1 Z´xµ d¾y p´r wa pa wh phµ

For disambiguation, the normalizing factor Z´xµ is the same for every parse of a given sentence and can be ignored.

12 Gosse Bouma, Gertjan van Noord, and Robert Malouf

Due to the occurrence of reentrancies, dependency structures are generally not trees but graphs. Therefore, the product above gives poor results because it will have an unjustified bias against such reentrancies (a reentrancy gives rise to an additional dependency relation). For this reason, we have chosen to score parse
 trees by determining the mean value of log p for each tuple; this improved results
considerably. The probability of a dependency is calculated as follows:
£ £p´r wa pa wh phµ p´r wh phµ p´pa wh ph rµ p´wa wh wp r paµ

The three components are each calculated using a linear back-off strategy, where

the weights are determined by frequency and diversity (formula 2.66 of (Collins

1999)). The quantities we use for backing off are given in the following table:

back-off level p´r wh phµ p´pa wh ph rµ p´wa wh wp r paµ

1

p´r phµ

p´pa ph rµ

p´wa wp r paµ

2 p´rµ p´pa rµ p´wa r paµ

3

p´ pa µ

p´wa paµ

4 p´waµ

Because the size of the treebanks we have currently available is much too small to estimate these quantities accurately, we have chosen to do our estimation using unsupervised learning. We have parsed a large corpus (`de Volkskrant' newspaper text: first four months of 1997) using the penalty rules described in the previous section as our disambiguator. This corpus contains about 350,000 sentences and 6,200,000 words. We only used those sentences that the system could analyse as a single constituent, and within a reasonable amount of time. This meant that we could use the results of about 225,000 sentences. We estimated the quantity p using the best parse (according to the penalty rules) for each of these sentences. Collecting the 225,000 dependency structures took about one month of CPU-time (using the high-performance computing cluster of the University of Groningen).
As can be concluded from table 3, such a model performs much better than the baseline. Moreover, a combined model in which we simply add the rule penalties to the quantity p performs better than either model in isolation.

Log-linear models. While the model described in the previous section offers good performance and conceptual simplicity, it is not without problems. In particular, the strategies for dealing with reentrancies in the dependency structures and for combining scores derived from penalty rules and from dependency relation statistics are ad hoc. Log-linear models, introduced to natural language processing by Berger, Della Pietra and Della Pietra (1996) and Della Pietra, Della Pietra and Lafferty (1997), and applied to stochastic constraint-based grammars by Abney (1997) and Johnson, Geman, Canon, Chi and Riezler (1999), offer the potential to solve both of these problems. Given a conditional log-linear model, the probability of a sentence x having the parse y is:

p´y xµ

1 exp
Z´xµ

i fi´x yµ
i

Alpino: Wide-coverage Computational Analysis of Dutch

13

technique baseline log linear penalties dependency rel's heur. + dep-rel's maximum

cdbl10 precision recall
62.3 63.3 76.0 76.6 78.6 79.3 78.9 79.7 80.9 81.7 89.1 90.0

f-score 62.8 76.3 78.9 79.3 81.3 89.6

cdbl20 precision recall
58.5 59.6 66.3 67.6 73.1 73.3 69.7 71.1 74.6 75.4 83.2 84.1

f-score 59.0 66.0 73.2 70.4 75.0 83.7

Table 3: Preliminary results on the cdbl10 and cdbl20 development treebanks for a number of disambiguation techniques. The baseline row lists the percentages obtained if we select for each sentence a random parse tree from the parse forest. The maximum row lists the percentages obtained if we take for each sentence the best parse tree. These two numbers thus indicate the lower and upper bounds for parse selection.

As before, the partition function Z´xµ will be the same for every parse of a given sentence and can be ignored, so the score for a parse is simply the weighted sum of the property functions fi´x yµ. What makes log-linear models particularly well suited for this application is that the property functions may be sensitive to any information which might be useful for disambiguation. Possible property functions include syntactic heuristics, lexicalized and backed-off dependency relations, structural configurations, and lexical semantic classes. Using log-linear models, all of these disparate types of information may be combined into a single model for disambiguation. Furthermore, since standard techniques for estimating the weights i from training data make no assumptions about the independence of properties, one need not take special precautions when information sources overlap.
The drawback to using log-linear models is that accurate estimation of the parameters i requires a large amount of annotated training data. Since such training data is not yet available, we instead attempted unsupervized training from unannotated data. We used the Alpino parser to find all parses of the 82,000 sentences with ten or fewer words in the `de Volkskrant' newpaper corpus. Using the resulting collection of 2,200,000 unranked parses, we then applied Riezler et al.'s (2000) `Iterative Maximization' algorithm to estimate the parameters of a log-linear model with dependency tuples as described in the previous section as property functions. The results, given in table 3, show some promise, but the performance of the log-linear model does not yet match that of the other disambiguation strategies. Current work in this area is focused on expanding the set of properties and on using supervised training from what annotated data is available to bootstrap the unsupervised training from large quantities of newspaper text.

14 Gosse Bouma, Gertjan van Noord, and Robert Malouf
7 Conclusions
Alpino aims at providing a wide-coverage, accurate, computational grammar for Dutch. The linguistic component of the system consists of a lexicalist featurebased grammar for Dutch, a wide-coverage and detailed lexicon, and a method for constructing dependency treebanks. The parser contains a lexical analysis module and a method for reconstructing parses from a parse forest using beam search, which allows the linguistic knowledge to be applied efficiently and robustly to unrestricted text. Finally, we have presented preliminary experiments aimed at providing accurate disambiguation.
In the near future, we hope to address a number of additional issues. The valency information in the lexicon is in many ways incomplete. We hope to obtain a more complete lexicon by acquiring dependency frames from corpora. Lexical analysis currently uses hand-written filter rules to reduce the number of tags for lexical items. An obvious alternative is to use a corpus-based part-of-speech tagger to arrive at the relevant filters. Finally, the work on disambiguation can profit from the availability of more annotated material. This suggests that our efforts at creating a dependency treebank may lead to improved results in the future.
References
Abney, S. P.(1997), Stochastic attribute-value grammars, Computational Linguistics 23, 597­618.
Baayen, R. H., Piepenbrock, R. and van Rijn, H.(1993), The CELEX Lexical Database (CD-ROM), Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.
Berger, A., Della Pietra, S. and Della Pietra, V.(1996), A maximum entropy approach to natural language processing, Computational Linguistics 22(1), 39­72.
Bouma, G. and van Noord, G.(1998), Word order constraints on verb clusters in German and Dutch, in E. Hinrichs, T. Nakazawa and A. Kathol (eds), Complex Predicates in Nonderivational Syntax, Academic Press, New York, pp. 43­72.
Bouma, G., Malouf, R. and Sag, I.(2001), Satisfying constraints on adjunction and extraction, Natural Language and Linguistic Theory 19, 1­65.
Calder, J.(2000), Thistle and interarbora, Proceedings of the 18th International Conference on Computational Linguistics (COLING), Saarbru¨cken, pp. 992­996.
Carroll, J., Briscoe, T. and Sanfilippo, A.(1998), Parser evaluation: A survey and a new proposal, Proceedings of the first International Conference on Language Resources and Evaluation (LREC), Granada, Spain, pp. 447­454.
Carter, D.(1997), The TreeBanker: A tool for supervised training of parsed corpora, Proceedings of the ACL Workshop on Computational Environments For Grammar Development And Linguistic Engineering, Madrid.

Alpino: Wide-coverage Computational Analysis of Dutch

15

Collins, M.(1999), Head-Driven Statistical Models for Natural Language Parsing, PhD thesis, University Of Pennsylvania.
Della Pietra, S., Della Pietra, V. and Lafferty, J.(1997), Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 380­393.
Groot, M.(2000), Lexiconopbouw: microstructuur. Internal report Corpus Gesproken Nederlands.
Johnson, M., Geman, S., Canon, S., Chi, Z. and Riezler, S.(1999), Estimators for stochastic "unification-based" grammars, Proceedings of the 37th Annual Meeting of the ACL, College Park, Maryland, pp. 535­541.
Moortgat, M., Schuurman, I. and van der Wouden, T.(2000), CGN syntactische annotatie. Internal report Corpus Gesproken Nederlands.
Oostdijk, N.(2000), The Spoken Dutch Corpus: Overview and first evaluation, Proceedings of Second International Conference on Language Resources and Evaluation (LREC), pp. 887­894.
Pollard, C. and Sag, I.(1994), Head-driven Phrase Structure Grammar, University of Chicago / CSLI.
Riezler, S., Prescher, D., Kuhn, J. and Johnson, M.(2000), Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and em, Proceedings of the 38th Annual Meeting of the ACL, Hong Kong, pp. 480­ 487.
Sag, I.(1997), English relative clause constructions, Journal of Linguistics 33(2), 431­484.
Skut, W., Krenn, B. and Uszkoreit, H.(1997), An annotation scheme for free word order languages, Proceedings of the Fifth Conference on Applied Natural Language Processing, Washington, DC.
Uit den Boogaart, P. C.(1975), Woordfrequenties in geschreven en gesproken Nederlands, Oosthoek, Scheltema & Holkema, Utrecht. Werkgroep Frequentieonderzoek van het Nederlands.
van Noord, G.(1997), An efficient implementation of the head corner parser, Computational Linguistics 23(3), 425­456. cmp-lg/9701004.
van Noord, G.(2001), Robust parsing of word graphs, in J.-C. Junqua and G. van Noord (eds), Robustness in Language and Speech Technology, Kluwer Academic Publishers, Dordrecht.
van Noord, G. and Bouma, G.(1994), Adjuncts and the processing of lexical rules, Proceedings of the 15th International Conference on Computational Linguistics (COLING), Kyoto, pp. 250­256. cmp-lg/9404011.
van Noord, G., Bouma, G., Koeling, R. and Nederhof, M.-J.(1999), Robust grammatical analysis for spoken dialogue systems, Journal of Natural Language Engineering 5(1), 45­93.
Veldhuijzen van Zanten, G., Bouma, G., Sima'an, K., van Noord, G. and Bonnema, R.(1999), Evaluation of the NLP components of the OVIS2 spoken dialogue system, in F. van Eynde, I. Schuurman and N. Schelkens (eds), Computational Linguistics in the Netherlands 1998, Rodopi Amsterdam, pp. 213­229.

IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

1

Inducing Features of Random Fields
Stephen Della Pietra, Vincent Della Pietra, and John Lafferty, Member, IEEE

Abstract--We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights.
The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.
Keywords-- Random field, Kullback-Leibler divergence, iterative scaling, maximum entropy, EM algorithm, statistical learning, clustering, word morphology, natural language processing.

I. INTRODUCTION

IN this paper we present a method for incrementally constructing random fields. Our method builds increasingly complex

fields to approximate the empirical distribution of a set of train-

ing examples by allowing potential functions, or features, that

are supported by increasingly large subgraphs. Each feature is

assigned a weight, and the weights are trained to minimize the

Kullback-Leibler divergence between the field and the empiri-

cal distribution of the training data. Features are incrementally

added to the field using a top-down greedy algorithm, with the

intent of capturing the salient properties of the empirical sam-

ple while allowing generalization to new configurations. The

general problem that the methods we propose address is that of

discovering the structure inherent in a set of sample patterns. As

one of the fundamental aims of statistical inference and learn-

ing, this problem is central to a wide range of tasks including

classification, compression, and prediction.

To illustrate the nature of our approach, suppose we wish

to automatically characterize spellings of words according to a

statistical model; this is the application we develop in Section

5. A field with no features is simply a uniform distribution on

ASCII strings (where we take the distribution of string lengths as

given). The most conspicuous feature of English spellings is that

they are most commonly comprised of lower-case letters. The

induction algorithm makes this observation by first constructing

the field

p ! Z e1  =

P i



,a z

,a z !i

where is an indicator function and the weight  a,z associated

with the feature that a character is lower-case is chosen to be

approximately 1:944. This means
letter in some position is about 7

thea1t:9a44sttirminegswmiothrealliokweleyrtchaasne

Stephen and Vincent Della Pietra are with Renaissance Technologies, Stony Brook, NY, 11790. E-mail: [sdella,vdella]@rentec.com
John Lafferty is with the Computer Science Department of the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 15213. E-mail: lafferty@cs.cmu.edu

the same string without a lowercase letter in that position. The following collection of strings was generated from the resulting field by Gibbs sampling. (As for all of the examples that will be shown, this sample was generated with annealing, to concentrate the distribution on the more probable strings.)

m, r, xevo, ijjiir, b, to, jz, gsr, wq, vf, x, ga, msmGh, pcp, d, oziVlal, hzagh, yzop, io, advzmxnv, ijv_bolft, x, emx, kayerf, mlj, rawzyb, jp, ag, ctdnnnbg, wgdw, t, kguv, cy, spxcq, uzflbbf, dxtkkn, cxwx, jpd, ztzh, lv, zhpkvnu, l^, r, qee, nynrx, atze4n, ik, se, w, lrh, hp+, yrqyka'h, zcngotcnx, igcump, zjcjs, lqpWiqu, cefmfhc, o, lb, fdcY, tzby, yopxmvk, by, fz,, t, govyccm, ijyiduwfzo, 6xr, duh, ejv, pk, pjw, l, fl, w

The second most important feature, according to the algorithm, is

that two adjacent lower-case characters are extremely common.

The second-order field now becomes

p ! Z e

=

1

P
ij

 ,a z

,a z

,a z

,a z

!ij

P + i



,a z

,a z !i

where letters

the weight  a,z
is approximately

a1,:8z 0a.ssociated

with

adjacent

lower-case

The first 1000 features that the algorithm induces include the

strings s>, <re, ly>, and ing>, where the character "<" de-

notes beginning-of-string and the character ">" denotes end-of-

string. In addition, the first 1000 features include the regular ex-
pressions [0-9][0-9] (with weight 9:15) and [a-z][A-Z]
,(with weight 5:81) in addition to the first two features [a-z]

and [a-z][a-z]. A set of strings obtained by Gibbs sampling

from the resulting field is shown here:

was, reaser, in, there, to, will, ,, was, by, homes, thing, be, reloverated, ther, which, conists, at, fores, anditing, with, Mr., proveral, the, ,, ***, on't, prolling, prothere, ,, mento, at, yaou, 1, chestraing, for, have, to, intrally, of, qut, ., best, compers, ***, cluseliment, uster, of, is, deveral, this, thise, of, offect, inatever, thifer, constranded, stater, vill, in, thase, in, youse, menttering, and, ., of, in, verate, of, to

These examples are discussed in detail in Section 5. The induction algorithm that we present has two parts: fea-
ture selection and parameter estimation. The greediness of the algorithm arises in feature selection. In this step each feature in a pool of candidate features is evaluated by estimating the reduction in the Kullback-Leibler divergence that would result from adding the feature to the field. This reduction is approximated as a function of a single parameter, and the largest value of this function is called the gain of the candidate. This approximation is one of the key elements of our approach, making it practical to evaluate a large number of candidate features at each stage of the induction algorithm. The candidate with the largest gain is added to the field. In the parameter estimation step, the parameters of the field are estimated using an iterative scaling algorithm. The algorithm we use is a new statistical estimation algorithm

2 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

that we call Improved Iterative Scaling. It is an improvement of the Generalized Iterative Scaling algorithm of Darroch and Ratcliff [12] in that it does not require that the features sum to a constant. The improved algorithm is easier to implement than the Darroch and Ratcliff algorithm, and can lead to an increase in the rate of convergence by increasing the size of the step taken toward the maximum at each iteration. In Section 4 we give a simple, self-contained proof of the convergence of the improved algorithm that does not make use of the Kuhn-Tucker theorem or other machinery of constrained optimization. Moreover, our proof does not rely on the convergence of alternating I-projection as in Csisza´r's proof [10] of the Darroch-Ratcliff procedure.
Both the feature selection step and the parameter estimation step require the solution of certain algebraic equations whose coefficients are determined as expectation values with respect to the field. In many applications these expectations cannot be computed exactly because they involve a sum over an exponentially large number of configurations. This is true of the application that we develop in Section 5. In such cases it is possible to approximate the equations that must be solved using Monte Carlo techniques to compute expectations of random variables. The application that we present uses Gibbs sampling to compute expectations, and the resulting equations are then solved using Newton's method.
Our method can be viewed in terms of the principle of maximum entropy [19], which instructs us to assume an exponential form for our distributions, with the parameters viewed as Lagrange multipliers. The techniques that we develop in this paper apply to exponential models in general. We formulate our approach in terms of random fields because this provides a convenient framework within which to work, and because our main application is naturally cast in these terms.
Our method differs from the most common applications of statistical techniques in computer vision and natural language processing. In contrast to many applications in computer vision, which involve only a few free parameters, the typical application of our method involves the estimation of thousands of free parameters. In addition, our methods apply to general exponential models and random fields­there is no underlying Markov assumption made. In contrast to the statistical techniques common to natural language processing, in typical applications of our method there is no probabilistic finite-state or push-down automaton on which the statistical model is built.
In the following section we describe the form of the random field models considered in this paper and the general learning algorithm. In Section 3 we discuss the feature selection step of the algorithm and briefly address cases when the equations need to be estimated using Monte Carlo methods. In Section 4 we present the Improved Iterative Scaling algorithm for estimating the parameters, and prove the convergence of this algorithm. In Section 5 we present the application of inducing features of spellings, and finally in Section 6 we discuss the relation between our methods and other learning approaches, as well as possible extensions of our method.
II. THE LEARNING PARADIGM
In this section we present the basic algorithm for building up a random field from elementary features. The basic idea

is to incrementally construct an increasingly detailed field to
approximate a reference distribution p~. Typically the distribution p~ is obtained as the empirical distribution of a set of training
examples. After establishing our notation and defining the form of the random field models we consider, we present the training problem as a statement of two equivalent optimization problems. We then discuss the notions of a candidate feature and the gain of a candidate. Finally, we give a statement of the induction algorithm.

A. Form of the random field models

ALet G = E; V  be a finite graph with vertex set V and edge
set E, and let be a finite alphabet. The configuration space  is the set of all labelings of the vertices in V by letters in
A. If C V and ! 2  is a configuration, then !C denotes
the configuration restricted to C. A random field on G is a

probability distribution on . The set of all random fields is

! Rnothing more than the simplex  of all probability distributions

on . If f : 

then the support of f, written suppf,

wishtehneevsmera!ll;e!st0

v2ertexwsituhb!seCt

C
=

!C0 Vthehnavfin!gt=hefpr!o0pe.rty

that

We consider random fields that are given by Gibbs distribu-

tions of the form

p!

=

1
Z

e

P C

VC!

2 ! Rfor ! , where VC : 

are functions with suppVC =

6C. The field is Markov if whenever VC = 0 then C is a clique,

or totally connected subset of V . This property is expressed in

terms of conditional probabilities as

p!u j !v; v 6= u = p!u j !v; u; v 2 E

where u and v are arbitrary vertices. We assume that each C is a path-connected subset of V and that

VC! = X iCfiC ! = C  fC !
1inC

Rwhere iC 2 and fiC ! 2 f0; 1g. We say that the
are the parameters of the field and that the functions

fviCaluaeres

Ci
the

features of the field. In the following, it will often be convenient

to use notation that disregards the dependence of the features
and parameters on a vertex subset C, expressing the field in the

form

p!

=

1
Z

e

P i

i

fi!

=

1
Z

e

f !

:

For is a

efiveelrdyrEan0;dVo;mffiie;ldfigE ;thVa;tfisiM; faigrkoovfiathne,

above form, obtained by

there com-

pleting the edge set E to ensure that for each i, the subgraph

generated by the vertex subset C = suppfi is totally con-

nected.
If we impose the constraint i = j on two parameters i and j, then we say that these parameters are tied. If i and j are

tied, then we can write

ifi! + jfj! = g!

where g = fi + fj is a non-binary feature. In general, we can
collapse any number of tied parameters onto a single parameter

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS

3

associated with a non-binary feature. Having tied parameters is

often natural for a particular problem, but the presence of non-

binary features generally makes the estimation of parameters

more difficult.

An automorphism of a graph is a permutation of the vertices

AthartatnakdeosmedfigeelsdtoEed;gVe;s:fui;;fvig2iEs

if and only if said to have

 u; v 2 E.
homogeneous

features if for each feature fi and automorphism of the graph

2G = E; V , there is a feature fj such that fj ! = fi!
for ! . If in addition j = i, then the field is said to

be homogeneous. Roughly speaking, a homogeneous feature

contributes the same weight to the distribution no matter where

in the graph it appears. Homogeneous features arise naturally in

the application of Section 5.

The methods that we describe in this paper apply to expo-

nential models in general; that is, it is not essential that there is

an underlying graph structure. However, it will be convenient

to express our approach in terms of the random field models

described above.

B. Two optimization problems

2Suppose that we
distribution p~, and

are given a set of

faenaitnuirteisalfmo=delfq00;

f1

,
;:

a
::

reference
; fn. In

practice, it is often the case that p~ is the empirical distribution of

a set of training samples ! ; !1 2 : : : !N, and is thus given

by

p~!

=

c! N

wcohnefiregucra!tion=!Pap1peiarsNam!on; g!tihe

is the number of training samples.

times

that

2We wish to construct a probability distribution q?  that

accounts for these data, in the sense that it approximates p~ but

pdrooebsanboiltidtyevdiiasttreibtouotiofanrsfproamndqq0.inWeumsienagsuthree

distance between Kullback-Leibler

divergence

Dp

k

q

=

X
!2

p!

log

p! q!

:

(1)

Throughout this paper we use the notation

p g = X g! p!
!2

! Rfor the expectation of a function g : 

with respect to

! Rthe probability distribution p. For a function h : 

and a

distribution q, we use both the notation h q and qh to denote the

generalized Gibbs distribution given by

qh! = h

q!

=

1
Zq h

e

h!

q!

:

Note that Zqh
ization constant

is not the usual partition function. It is a normal-
determined by the requirement that h q!

sums to 1 over !, and can be written as an expectation:

Zqh = q e h :

There are two natural sets of probability distributions deter-
Pmined by the data p~, q0, and f. The first is the set f; p~ of

all distributions that agree with p~ as to the expected value of the feature function f:

Pf; p~ = fp 2  : p f = p~ f g :

QThe second
based on q0

iws itthhefseeattureff;uqn0ctioofngfen: eralized

Gibbs

distributions

Qf; q0 = f  f q0 :  2 Rn g :

Q QWe let ¯ f; q0 denote the closure of f; q0 in  (with respect

to the topology it inherits as a subset of Euclidean space).
There are two natural criteria for choosing an element q? from

these sets:

QMaximum Likelihood Gibbs Distribution. Choose q? to

be a distribution in
respect to p~:

¯ f; q0 with maximum likelihood with

kq?ML

=

arg min
q2Q¯ f;q0

Dp~

q

PMaximum Entropy Constrained Distribution. Choose q?
to be a distribution in f; p~ that has maximum entropy relative to q0:

kq?ME = pa2rgPmfi;pn~ Dp q0

Although these criteria are different, they determine the same

P Qdistribution: q? =
the unique element

oqf?MtLhe=inqte?MrEs.ecMtioonreovfe;r,p~this

distribution is
¯ f; q0, as we

discuss in detail in Section 4.1 and Appendix A.
When p~ is the empirical distribution of a set of training ex-
kamples !1; !2 : : : !N, minimizing Dp~ p is equivalent to
maximizing the probability that the field p assigns to the training

data, given by

Y p!i = Y p! c!

1iN

!2

e,NDp~ k p :

kWith sufficiently many parameters it is a simple matter to con-
struct a field for which Dp~ p is arbitrarily small. This is the
classic problem of over training. The idea behind the method proposed in this paper is to incrementally construct a field that
captures the salient properties of p~ by incorporating an increas-
ingly detailed collection of features, allowing generalization to new configurations; the resulting distributions are not absolutely continuous with respect to the empirical distribution of the training sample. The maximum entropy framework for parameter estimation tempers the over training problem; however, the basic problem remains, and is out of the scope of the present paper. We now present the random field induction paradigm.

C. Inducing field interactions We begin by supposing that we have a set of atomic features
Fatomic fg :  ,! f0; 1g; suppg = vg 2 V g

each of which is supported by a single vertex. We use atomic features to incrementally build up more complicated features. The following definition specifies how we shall allow a field to be incrementally constructed, or induced.

4 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

Definition 1: Suppose that the field q is given by q = 
f q0. The features fi are called the active features of q. A
2 Ffeature g is a candidate for q if either g atomic, or if g is of the
form g! = a!fi! for an atomic feature a and an active
 2feature fi with suppg suppfi E. The set of candidate Cfeatures of q is denoted q.
In other words, candidate features are obtained by conjoining
atomic features with existing features. The condition on supports ensures that each feature is supported by a path-connected subset
of G.
2 CIf g q is a candidate feature of q, then we call the 1-
parameter family of random fields q g =  g q the induction of q by g. We also define

Gq ; g = Dp~ k q , Dp~ k q g :

(2)

We think of Gq ; g as the improvement that feature g brings
to the model when it has weight . As we show in the following
section, Gq ; g is -convex in . (We use the suggestive
notation -convex and -convex in place of the less mnemonic
concave and convex terminology.) We define Gqg to be the greatest improvement that feature g can give to the model while
keeping all of the other features' parameters fixed:

Gqg = sup Gq ; g :

We refer to Gqg as the gain of the candidate g.
D. Incremental construction of random fields

III. FEATURE SELECTION

The feature selection step of our induction algorithm is based

upon an approximation. We approximate the improvement due

to adding a single candidate feature, measured by the reduction

in Kullback-Leibler divergence, by adjusting only the weight

of the candidate and keeping all of the other parameters of the

field fixed. In general this is only an estimate, since it may well

be that adding a feature will require significant adjustments to

all of the parameters in the new model. From a computational

perspective, approximating the improvement in this way can

enable the simultaneous evaluation of thousands of candidate

features, and makes the algorithm practical. In this section we

present explain the feature selection step in detail.
Proposition 1: Let Gq ; g, defined in (2), be the approximate improvement obtained by adding feature g with parameter
to the field q. Then if g is not constant, Gq ; g is strictly

-convex in and attains its maximum at the unique point ^

satisfying

p~ g = q ^ g g :

(3)

Proof: Using the definition (1) of the Kullback-Leibler divergence we can write

Gq ; g

= =

X
!X2

p~! p~!

log Zq,1
, g! ,

g e g!q! q!
log q e g

=

!2
p~ g

, log q e

g :

We can now describe our algorithm for incrementally constructing fields.

Field Induction Algorithm.

Initial Data:
A reference distribution p~ and an initial model q0.

Output:

Aargfimeldinq?Dwpi~thk

active
q.

features

f0; : : :; fN

such

that

q?

=

q2Q¯ f;q0

Algorithm:

2 C(0)
(1)

q qSet 0 = 0.
For each candidate

g

q n compute the gain

(2G) qLnetgfn.

=

arg max
g2Cqn 

G gqn  

be

the

feature

with

the

largest gain.
(3) Compute q?

f0
(4)

;Sfe1t;q::n:+; f1n=.

= arg min Dp~ k q,
q2Q¯ f;q0
q? and n  n + 1, and go

where to step

f
(1).

=

This induction algorithm has two parts: feature selection and parameter estimation. Feature selection is carried out in steps (1) and (2), where the feature yielding the largest gain is incorporated into the model. Parameter estimation is carried out in step (3), where the parameters are adjusted to best represent the reference distribution. These two computations are discussed in more detail in the following two sections.

Thus

@ @

Gq

; g

=

p~

g

,

q ge qe

g g

= p~ g , q g g :

Moreover,

@ @

2 2

Gq



; g

=

q ge qe

g g

2 2

,

q g2e g qe g

= ,q g ,g , q g g 2

Hence, G@@22 q ; g
g is not constant, then of g with respect to q

0, so that Gq ; g is -convex in . If

G@@

2 2

q

; g, which is

g, is strictly negative,

minus the variance
so that Gq ; g is

strictly convex.
When g is binary-valued, its gain can be expressed in a par-

ticularly nice form. This is stated in the following proposition,

whose proof is a simple calculation.
Proposition 2: Suppose that the candidate g is binary-valued. Then Gq ; g is maximized at

 p~

g

1 , q

g




^ = log q g 1 , p~ g 

and at this value,
Gqg = Gq ^ ; g = DBp k Bq

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS

5

where Bp and Bq are Bernoulli random variables given by

Bp1 = p~ g Bq1 = q g

Bp0 = 1 , p~ g Bq0 = 1 , q g :

For features that are not binary-valued, but instead take values

in the non-negative integers, the parameter ^ that solves (3) and
thus maximizes Gq ; g cannot, in general, be determined in

closed form. This is the case for tied binary features, and it

applies to the application we describe in Section 5. For these
cases it is convenient to rewrite (3) slightly. Let = e so that @=@ = @=@ . Let

gk = X q! k; g!
!

be the total probability assigned to the event that the feature g takes the value k. Then (3) becomes

@ @

Gqlog

; g = p~ g

,

PPkNNk==00kggkk

k k

=

0

(4)

This equation lends itself well to numerical solution. The gen-
eral shape of the curve !7 @=@ Gqlog ; g is shown in
Figure 1.

Fig. 1. Derivative of the gain
The limiting value of @Gqlog ; g=@ as ! 1 is p~ g ,
N . The solution to equation (4) can be found using Newton's
method, which in practice converges rapidly for such functions. When the configuration space  is large, so that the coeffi-
cients gk cannot be calculated by summing over all configura-
tions, Monte Carlo techniques may be used to estimate them. It is important to emphasize that the same set of random con-
ficagnudriadtiaotensgcsainmbueltaunseedoutsolye.stRimatahteerththeacnodefifisccuiesnsttshgekdfeotarilesacohf
Monte Carlo techniques for this problem we refer to the extensive literature on this topic. We have obtained good results using the standard technique of Gibbs sampling [17] for the problem we describe in Section 5.

IV. PARAMETER ESTIMATION

In this section we present an algorithm for selecting the pa-

rameters associated with the features of a random field. The

algorithm is a generalization of the Generalized Iterative Scal-

ing algorithm of Darroch and Ratcliff [12]. It reduces to the

Darroch-Ratcliff algorithm when the features sum to a constant;

however, the new algorithm does not make this restriction.

Throughout this section we hold the set of features f =

f0; f1
bution

;: p~

: :; fn, the
fixed, and

initial model q0
we simplify the

and the notation

reference distriaccordingly. In

R 2particular, we write q instead of  f q for

n. We

k 1assume that p~! = 0 whenever q0! = 0. This condition is
commonly written p~ q0, and it is equivalent to Dp~ q0 .

A description of the algorithm requires an additional piece of

notation. Let

f#! = Xn fi! :
i=0

If the features are binary, then f#! is
features that are "on" for the configuration

the
!.

total

number

of

Improved Iterative Scaling.

Initial Data:

A
p~

reference
q0, and

distribution p~ and an initial model non-negative features f0; f1; : : : ; fn.

q0,

with

kOutput:

The

distribution

q?

=

arg min
q2Q¯ f;q0

Dp~

q

Algorithm:

2 ,1 1(0)
(1)

q qSet 0 = 0. For each i let

ik

;  be the unique solution of

qk fi e ik f# = p~ fi :

(5)

(2)
(3)

qSet k +1 If qk has

q= k k and converged, set q?

k
=

k qk

+ 1. and

terminate.

Oth-

erwise go to step (1).

In other
limm!1

words,
m q0

twhhiseraelgomri=thmPcmko=n0strukctasnad

diisktriibs udteitoenrmqi?ne=d

as the solution to the equation

X qk! fi! e ik f#! = X p~! fi! :
!!

When used in the n-th iteration of the field induction algorithm, where a candidate feature g = fn is added to the field q = qn, we cpharoaomseettehrethinaittmiaal xdiimstrizibeusttihoengqa0intoofbge.qI0n=praqc^tgic, ew, htheirsepr^oivsidthees

a good starting point from which to begin iterative scaling. In

fact, we can view this distribution as the result of applying one

iteration of an Iterative Proportional Fitting Procedure [5], [9]
to project q g onto the linear family of distributions with gmarginals constrained to p~ g .

Our main result in this section is

k kbdyecPtrhreoeapsIeomssipmtirooonvneo3dt:oInStieucrpaalptlioyvseteoSqDcaklpi~nisgqta?hlegaosnerdiqtuhqemkn.cecToihnnevnerDdgeetspe~tromqqi?nke=d

k karg min Dp~
q2Q¯

q = argp2mPin Dp

q0.

In the remainder of this section we present a self-contained

proof of the convergence of the algorithm. The key idea of

the proof is to express the incremental step of the algorithm in

terms of an auxiliary function which bounds from below the

log-likelihood objective function. This technique is the standard

means of analyzing the EM algorithm [13], but it has not previ-

ously been applied to iterative scaling. Our analysis of iterative

scaling is different and simpler than previous treatments. In

particular, in contrast to Csisza´r's proof of the Darroch-Ratcliff

6 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

procedure [10], our proof does not rely upon the convergence of alternating I-projection [9].
We begin by formulating the basic duality theorem which states that the maximum likelihood problem for a Gibbs distribution and the maximum entropy problem subject to linear constraints have the same solution. We then turn to the task of computing this solution. After introducing auxiliary functions in a general setting, we apply this method to prove convergence of the Improved Iterative Scaling algorithm. We finish the section by discussing Monte Carlo methods for estimating the equations when the size of the configuration space prevents the explicit calculation of feature expectations.

A. Duality

The duality between the maximum likelihood and maximum

entropy problems is expressed in the following Proposition.

Proposition 4: Suppose that p~
unique q? 2  satisfying

q0. Then there exists a

(1) (2)
q

q? 2 2DQ¯p

P k q

Q¯
=

Dp k

q?

+

Dq?

k q

for

any

p

2

P

and

(3) (4)

q? q?

= arg min
q2Q¯ = argp2mPin

Dp~ k q Dp k q0.

Moreover, any of these four properties determines q? uniquely.

This result is well known, although perhaps not quite in this

packaging. In the language of constrained optimization, it ex-

presses the fact that the maximum likelihood problem for Gibbs

distributions is the convex dual to the maximum entropy prob-

lem for linear constraints. Property (2) is called the Pythagorean

kproperty since it resembles the Pythagorean theorem if we imag-
ine that Dp q is the square of Euclidean distance and p; q?; q

are the vertices of a right triangle.

We include a proof of this result in Appendix A to make this

paper self-contained and also to carefully address the technical
Qissues arising from the fact that is not closed. The proposition Q Q P Qwould not be true if we replaced ¯ with ; in fact,

might be empty. Our proof is elementary and does not rely

on the Kuhn-Tucker theorem or other machinery of constrained

optimization.

B. Auxiliary functions

! RWe now turn to the task of computing q?. Fix p~ and let

L:

be the log-likelihood objective function

Lq = ,Dp~ k q :

R  !Definition 2: A function A : n  R is an auxiliary
function for L if
R(1) For all q 2  and 2 n

L q  Lq + A ; q

R(2) A ; q is continuous in q 2  and C1 in 2 n with
A0; q = 0 and

d dt

j

t=0

At

; q

=

d dt

j

t=0

Lt



q :

We can use an auxiliary function
algorithm for maximizing L. We recursively define q k +1 by

A to
start

cwointhstrquckt

an iterative
= q0 and

q q A ; q :k k +1 =   k with k = arg max  k

It is clear from property (1) of the definition that each step of
this procedure increases L. The following proposition implies that in fact the sequence qk will reach the maximum of L.
Proposition 5: Suppose qk is any sequence in  with

q q0 = 0 and
2 Rwhere k n satisfies

q k +1 =

qk k

A k; qk = sup A ; qk :

(6)

Then Lqk increases monotonically to mq2aQx¯ Lq and qk

converges

to q?

=

arg max
q2Q¯

Lq.

Equation (6) assumes that the supremum sup A ; qk is

achieved at finite . In Appendix B, under slightly stronger

assumptions, we present an extension that allows some compo-
,1nents of k to take the value .

To use the proposition to construct a practical algorithm we
must determine an auxiliary function A ; q for which k

satisfying the required condition can be determined efficiently.

In Section 4.3 we present a choice of auxiliary function which

yields the Improved Iterative Scaling updates.

To prove Proposition 5 we first prove three lemmas.

RLemma 1:
0 for all 2

2If m
n.

 is a cluster point of qk, then A

; m 

Proof: Let qkl be a sub-sequence converging to m. Then

for any

  ,A ; qkl

A ; q L q L q kl kl

 kl+1

 kl

 ,L q L q :k  l+1

 kl

The first inequality follows from property (6) of nk. The sec-

ond and third inequalities are a consequence of the monotonicity
of Lqk. The lemma follows by taking limits and using the

fact that L and A are continuous.

j 2 2 Rddt

Lemma 2:
t=0 Lt

If


m m

=

0

 for

is a any

clustner.

point

of

qk,

then

Proof: By the previous lemma, A ; m 0 for all . Since

A0; m = 0, this means that = 0 is a maximum of A ; m

so that

0

=

d dt

j

t=0

At

;

m

=

d dt

j

t=0

Lt



m :

f gLemma 3:
ter point q.

TShuepnpqoske

qk is any sequence converges to q.

with

only

one

clus-

f g 26 26hinaqgsPkqarocoalhfun:asdsteSaarusuppunopbiiqonseustqeequn0ceolnut.csBteTeq.hr epTnnohkitinhstec.roeBnet.xriaSsditnisccatesn

opiesncsoemt pBaccto,nqtanink-
the assumption that

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS

7

2 P Q j0q,kaPn.rdoToshfoeonmf Pitrfooploloswitis¯ofnbryo5m:LeLmSeummpampo2aso2efttAhhaapttpmdedtnidsti=xa0AcLl.uBsttuetrqp?omiinsttho=ef P Qothnaltyqpokinctoinnverges¯tobyq?P.roposition 4. It follows from Lemma 3
In Appendix B we prove an extension of Proposition 5 that
,1allows the components of to equal . For this extension,
we assume that all the components of the feature function f are
non-negative:

fi!  0 for all i and all !.

(7)

,This is not a practical restriction since we can replace fi by
fi min! fi!.

C. Improved Iterative Scaling

We now prove the monotonicity and convergence of the Im-

proved Iterative Scaling algorithm by applying Proposition 5 to

a particular choice of auxiliary function. We now assume that

2 2 Reach component of the feature function f is non-negative.

For q  and

n, define

A ; q = 1 +  p~ f , X q! X fi j ! e if#!
!i

jwhere fi ,1 continuous

! = ff#i!! . It Rfunction on 

is

easy to
n

check .

that

A

extends

to

a

Lemma 4: A ; q is an extended auxiliary function for Lq.

The key ingredient in the proof of the lemma is the -convexity of the logarithm and the -convexity of the exponential, as expressed in the inequalities

e

P i

ti

i

 X ti e

i

if

ti



0,

P
i

ti

=

1

i

log x  x , 1 for all x 0:

(8) (9)

Proof of Lemma 4: Because A extends to a continuous func-

,1 tion on R

n , it suffices to prove that it satisfies

properties (1) and (2) of Definition 2. To prove property (1) note

that

L q , Lq =  p~ f , log X q! e f!



 p~ f

+ 1 , X q! e

!
f !

  p~ f + 1 , X! q! X fi j ! e if#!

!i

= A ; q :

(10) (11) (12) (13)

Equality (10) is a simple calculation. Inequality (11) follows

from inequality (9). Inequality (12) follows from the definition
of f# and Jensen's inequality (8). Property (2) of Definition 2 is
straightforward to verify.

Proposition 3 follows immediately from the the extended Proposition 5. Indeed, it is easy

above lemma to check that

ankd

defined in Proposition 3 achieves the maximum of A ; qk,

so that it satisfies the condition of Proposition 5 in Appendix B.

D. Monte Carlo methods

The Improved Iterative Scaling algorithm described in the previous section is well-suited to numerical techniques since all of the features take non-negative values. In each iteration of this algorithm it is necessary to solve a polynomial equation for each
feature fi. That is, we can express equation 5 in the form

XM amk;i im = 0
m=0

where

M

is

the

largest

value

of

f#!

=

P
i

fi!

and

8
amk;i = :

P
!

qk!

fi

!

,p~ fi

m; f# ! 

m0 m=0

(14)

where qk is
equation has

the field for no solution

the k-th iteration and precisely when amk;i

i
=

e= ik . 0 for m

This 0.

Otherwise, it can be efficiently solved using Newton's method
since all of the coefficients amk;i, m 0, are non-negative. When

Monte Carlo methods are to be used because the configuration

space  is large,
estimated for all i

the and

cmoebffiycgieenntesraatmikn;gi

can be simultaneously a single set of samples

from the distribution qk.

V. APPLICATION: WORD MORPHOLOGY
Word clustering algorithms are useful for many natural language processing tasks. One such algorithm [6], called mutual information clustering, is based upon the construction of simple bigram language models using the maximum likelihood criterion. The algorithm gives a hierarchical binary classification of words that has been used for a variety of purposes, including the construction of decision tree language and parsing models, and sense disambiguation for machine translation [7].
A fundamental shortcoming of the mutual information word clustering algorithm given in [6] is that it takes as fundamental the word spellings themselves. This increases the severity of the problem of small counts that is present in virtually every statistical learning algorithm. For example, the word "Hamiltonianism" appears only once in the 365,893,263-word corpus used to collect bigrams for the clustering experiments described in [6]. Clearly this is insufficient evidence on which to base a statistical clustering decision. The basic motivation behind the feature-based approach is that by querying features of spellings, a clustering algorithm could notice that such a word begins with a capital letter, ends in "ism" or contains "ian," and profit from how these features are used for other words in similar contexts.
In this section we describe how we applied the random field induction algorithm to discover morphological features of words, and we present sample results. This application demonstrates how our technique gradually sharpens the probability mass from the enormous set of all possible configurations, in this case ASCII strings, onto a set of configurations that is increasingly similar to those in the training sample. It achieves this by introducing both "positive" features which many of the training samples exhibit, as well as "negative" features which do not appear in the sample, or appear only rarely. A description of how the resulting features

8 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

were used to improve mutual information clustering is given in [20], and is beyond the scope of the present paper; we refer the reader to [6], [20] for a more detailed treatment of this topic.
In Section 5.1 we formulate the problem in terms of the notation and results of Sections 2, 3, and 4. In Section 5.2 we describe how the field induction algorithm is actually carried out in this application. In Section 5.3 we explain the results of the induction algorithm by presenting a series of examples.

A. Problem formulation

To
A Aspace

discover the set of

faelaltsutrreinsgosfsp=ellingisn

we the

take as configuration ASCII alphabet . We

construct a probability distribution p! on  by first predicting

j jthe length ! , and then predicting the actual spelling; thus,

p! = plj ! jps! j j ! j where pl is the length distribution

and ps is the spelling distribution. We take the length distribution
 jas given. We model the spelling distribution ps l over strings

j jof
of
!i

length l as a random field. Let l all ASCII strings of length l. Then
is an ASCII character.

be the configuration space
l = O102l since each

To reduce the number of parameters, we tie features, as de-

scribed in Section 2.1, so that a feature has the same weight

independent of where it appears in the string. Because of this it
is natural to view the graph underlying l as a regular l-gon. The

group of automorphisms of this graph is the set of all rotations,

and the resulting field is homogeneous as defined in Section 2.
Not only is each field ps homogeneous, but in addition, we tie features across fields for different values of l. Thus, the weight f of a feature is independent of l. To introduce a dependence

on the length, as well as on whether or not a feature applies at

the beginning or end of a string, we adopt the following artificial
construction. We take as the graph of l an l + 1-gon rather than an l-gon, and label a distinguished vertex by the length,

keeping this label held fixed.

To complete the description of the fields that are induced, we

need to specify the set of atomic features. The atomic features

that we allow fall into three types. The first type is the class of

features of the form

fv;c!

=

n

1 0

if !v = c
otherwise.

where c is any ASCII character, and v denotes an arbitrary char-

acter position in the string. The second type of atomic features
involve the special vertex <l> that carries the length of the string.

These are the features

fv;l! fv;<>!

= =

n1

n

0 1

0

if !v = <l>
otherwise
if !v = <l> for some l
otherwise

The atomic feature fv;<> introduces a dependence on whether a
string of characters lies at the beginning or end of the string, and
the atomic features fv;l introduce a dependence on the length of
the string. To tie together the length dependence for long strings,
we also introduce an atomic feature fv;7+ for strings of length 7
or greater.
The final type of atomic feature asks whether a character lies
in one of four sets, [a-z], [A-Z], [0-9], [@-&], denoting

arbitrary lowercase letters, uppercase letters, digits, and punctuation. For example, the atomic feature

2fv;[a-z]!

=

n

1 0

if !v [a-z]
otherwise

tests whether or not a character is lowercase. To illustrate the notation that we use, let us suppose that the the
following features are active for a field: "ends in ism," "a string of at least 7 characters beginning with a capital letter" and "contains ian." Then the probability of the word "Hamiltonianism" would be given as

jpl14 psHamiltonianism j ! j = 14 =

p Z e :1
l14

7+<[A-Z]+ian+ism>

14

Here the 's are the parameters of the appropriate features, and

we use the characters < and > to denote the beginning and ending

of a string (more common regular expression notation would be

^ and $). The notation 7+<[A-Z] thus means "a string of at

least 7 characters that begins with a capital letter," corresponding

to the feature

fu;7+ fv;[A-Z] ;

where u and v are adjacent positions in the string, recalling

from Definition 2.1 that we require the support of a feature to be

a connected subgraph. Similarly, ism> means "ends in -ism"

and corresponds to the feature

fu;i fv;s fw;m fx;<>

where u; v; w; x are adjacent positions in the string and ian
means "contains ian," corresponding to the feature

fu;i fv;a fw;n :

B. Description of the algorithm

We begin the random field induction algorithm with a model

that assigns uniform probabilityto all strings. We then incrementally add features to a random field model in order to minimize

the Kullback-Leibler divergence between the field and the unigram distribution of the vocabulary obtained from a training

corpus. The length distribution is taken according to the lengths of words in the empirical distribution of the training data. The

improvement to the model made by a candidate feature is eval-

uated by the reduction in relative entropy, with respect to the

unigram distribution, that adding the new feature yields, keeping the other parameters of the model fixed. Our learning algo-

rithm incrementally constructs a random field to describe those features of spellings that are most informative.

At each stage in the induction algorithm, a set of candidate features is constructed. Because the fields are homogeneous, the

set of candidate features can be viewed as follows. Each active

feature can be expressed in the form

fs!

=

n

1 0

substring s appears in !
otherwise

Awhere s is a string in the extended alphabet of ASCII characters
together with the macros [a-z], [A-Z], [0-9], [@-&], and

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS

9

f gthe length labels <l> and <>. If fs s2S is the set of active
features, (including s = , the empty string) using this repre-

sentation, then the set of candidate features is precisely the set
ffas; fsaga2A;s2S, where as denotes concatenation of strings.
As required by Definition 2, each such candidate increases the

support of an active feature by a single adjacent vertex.

Since the model assigns probability to arbitrary word strings,

sthmeapllaersttitsiotrninfgunlecntigotnhsZll.

can We

be computed exactly for only the therefore compute feature expec-

tations using a random sampling algorithm. Specifically, we

use the Gibbs sampler to generate 10,000 spellings of random
lengths. When computing the gain Gqg of a candidate feature, we use these spellings to estimate the probability gk that the candidate feature g occurs k times in a spelling (see equation (4)­for example, the feature fv;[a-z] occurs two times in
the string The), and then solve for the corresponding using

Newton's method for each candidate feature. It should be em-

phasized that only a single set of random spellings needs to be
generated; the same set can be used to estimate gk for each candidate g. After adding the best candidate to the field, all of

the feature weights are readjusted using the Improved Iterative

Scaling algorithm. To carry out this algorithm, random spellings

are again generated, this time incorporating the new feature,

yielding Monte
that amk;i is the

Carlo estimates of the coefficients amk;i. Recall expected number of times that feature i appears

(under the substring representation for homogeneous features)

in a string for which there is a total of m active features (see

equation 14)). Given estimates for these coefficients, Newton's

method is again used to solve equation (14), to complete a single

iteration of the iterative scaling algorithm. After convergence of

the Kullback-Leibler divergence, the inductive step is complete,

and a new set of candidate features is considered.

C. Sample results
We began with a uniform field, that is, a field with no features at all. For this field, all ASCII strings of a given length are equally likely, and the lengths are drawn from a fixed distribution. Here is a sample of strings drawn from this distribution:
~, mo, !ZP*@, m/TLL, ks;cm 3, *LQdR, D, aWf, 5&TL|4, tc, ?!@, sNeiO+, wHo8zBr", pQlV, m, H!&, h9, #Os, :, KygFM?, LW, ",8g, 89Lj, -P, A, !, H, `, Y^:Du:, 1xCl, 1!'J#F*u., w=idHnM), ~, 2, 2leW2, I,bw~tk1, 3", ], ], b, +JEmj6, +E*, nqjqe"-7f, |al2, T, ~(sOc1+2ADe, &, np9oH, i;, $6, qgO+[, xEv, #U, O)[83COF, =|B|7%cR, Mqq, ?!mv, n=7G, $i9GAJ D, 5, ,=, +u6@I9:, +, =D, 2E#vz@3-, ~nu;.+s, 3xJ, GDWeqL, R,3R, !7v, FX,@y, 4p cY2hU, ~

m, r, xevo, ijjiir, b, to, jz, gsr, wq, vf, x, ga, msmGh, pcp, d, oziVlal, hzagh, yzop, io, advzmxnv, ijv bolft, x, emx, kayerf, mlj, rawzyb, jp, ag, ctdnnnbg, wgdw, t, kguv, cy, spxcq, uzflbbf, dxtkkn, cxwx, jpd, ztzh, lv, zhpkvnu, l,^ r, qee, nynrx, atze4n, ik, se, w, lrh, hp+, yrqyka'h, zcngotcnx, igcump, zjcjs, lqpWiqu, cefmfhc, o, lb, fdcY, tzby, yopxmvk, by, fz,, t, govyccm, ijyiduwfzo, 6xr, duh, ejv, pk, pjw, l, fl, w
In the following table we show the first 10 features that the algorithm induced, together with their associated parameters. Several things are worth noticing. The second feature chosen was [a-z][a-z], which denotes adjacent lowercase characters. The third feature added was the letter e, which is the most
common letter. The weight for this feature is = e = 3:47.
The next feature introduces the first dependence on the length of the string: [a-z]>1 denotes the feature "a one character word ending with a lowercase letter." Notice that this feature has a small weight of 0.04, corresponding to our intuition that such words are uncommon. Similarly, the features z, q, j, and x are uncommon, and thus receive small weights. The appearance of the feature * is explained by the fact that the vocabulary for our corpus is restricted to the most frequent 100,000 spellings, and all other words receive the "unknown word" spelling ***, which is rather frequent. (The "end-of-sentence" marker, which makes its appearance later, is given the spelling |.)

feature [a-z] [a-z][a-z] e [a-z]>1 t

6.64 6.07 3.47 0.04 2.75

feature *

z q jx

17.25

0.02 0.03 0.02 0.06

Shown below are spellings obtained by Gibbs sampling from the resulting collection of fields.
frk, et, egeit, edet, eutdmeeet, ppge, A, dtgd, falawe, etci, eese, ye, epemtbn, tegoeed, ee, *mp, temou, enrteunt, ore, erveelew, heyu, rht, *, lkaeu, lutoee, tee, mmo, eobwtit, weethtw, 7, ee, teet, gre, /, *, eeeteetue, hgtte, om, he, *, stmenu, ec, ter, eedgtue, iu, ec, reett, *, ivtcmeee, vt, eets, tidpt, lttv, *, etttvti, ecte, X, see, *, pi, rlet, tt, *, eot, leef, ke, *, *, tet, iwteeiwbeie, yeee, et, etf, *, ov
After inducing 100 features, the model finally begins to be concentrated on spellings that resemble actual words to some extent, particularly for short strings. At this point the algorithm has discovered, for example, that the is a very common 3-letter word, that many words end in ed, and that long words often end in ion. A sample of 10 of the first 100 features induced, with their appropriate weights is shown in the table below.

It comes as no surprise that the first feature the induction al-

gorithm chooses is [a-z]; it simply observes that characters

should
tropy)

be lowercase. weight for this

The maximum feature is =

liekeliho6o:d99(m. aTxhimisummeaenns-

that a string with a lowercase letter in some position is about 7

times more likely than the same string without a lowercase letter

in that position.

When we now draw strings from the new distribution (using

annealing to concentrate the distribution on the more probable

strings), we obtain spellings that are primarily made up of low-

ercase letters, but that certainly do not resemble English words:

. ,>1 3<the tion 4<th y> ed> ion>7+ ent 7+<c 22.36 31.30 11.05 5.89 4.78 5.35 4.20 4.83 5.17 5.37
thed, and, thed, toftion, |, ieention, cention, |, ceetion, ant, is, seieeet, cinention, and, ., tloned, uointe, feredten, iined, sonention, inathed, other, the, id, and, ,, of, is, of, of, ,, lcers, ,, ceeecion, ,, roferented, |, ioner, ,, |, the, the, the, centention, ionent, asers, ,, ctention, |, of, thed, of, uentie, of, and, ttentt, in, rerey, and, |, sotth, cheent, is, and, of, thed, rontion, that, seoftr

10 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

A sample of the first 1000 features induced is shown in the table below, together with randomly generated spellings. Notice, for example, that the feature [0-9][0-9] appears with a surprisingly high weight of 9382.93. This is due to the fact that if a string contains one digit, then it's very likely to contain two digits. But since digits are relatively rare in general, the feature [0-9] is assigned a small weight of 0.038. Also, according to the model, a lowercase letter followed by an uppercase letter is rare.

s> <re ght> 3<[A-Z] ly> al>7+ ing>

7.25

4.05 3.71 2.27 5.30 94.19

16.18

[a-z][A-Z] 't> ed>7+ er>7+ ity ent>7+ [0-9][0-9]

0.003 138.56 12.58 8.11 4.34 6.12

9382.93

qu

ex ae ment ies <wh

ate

526.42 5.265 0.001 10.83 4.37 5.26

9.79

was, reaser, in, there, to, will, ,, was, by, homes, thing, be, reloverated, ther, which, conists, at, fores, anditing, with, Mr., proveral, the, ,, ***, on't, prolling, prothere, ,, mento, at, yaou, 1, chestraing, for, have, to, intrally, of, qut, ., best, compers, ***, cluseliment, uster, of, is, deveral, this, thise, of, offect, inatever, thifer, constranded, stater, vill, in, thase, in, youse, menttering, and, ., of, in, verate, of, to
Finally, we visit the state of the model after inducing 1500 features to describe words. At this point the model is making more refined judgements regarding what is to be considered a word and what is not. The appearance of the features {}> and \[@-&]{, is explained by the fact that in preparing our corpus, certain characters were assigned special "macro" strings. For example, the punctuation characters $, _, %, and & are represented in our corpus as \${}, \_{}, \%{}, and \&{}. As the following sampled spellings demonstrate, the model has at this point recognized the existence of macros, but has not yet discerned their proper use.

7+<inte prov <der <wh 19 ons>7+ ugh ic>

4.23 5.08 0.03 2.05 2.59 4.49 5.84

7.76

sys ally 7+<con ide nal {}> qui \[@-&]{

4.78 6.10 5.25 4.39 2.91 120.56 18.18 913.22

iz IB <inc <im iong $ ive>7+ <un

10.73 10.85 4.91 5.01 0.001 16.49 2.83

9.08

the, you, to, by, conthing, the, ., not, have, devened, been, of, |, F., ., in, have, -, ,, intering, ***, ation, said, prouned, ***, suparthere, in, mentter, prement, intever, you, ., and, B., gover, producits, alase, not, conting, comment, but, |, that, of, is, are, by, from, here, incements, contive, ., evined, agents, and, be, , thent, distements, all, --, has, will, said, resting, had, this, was, intevent, IBM, whree, acalinate, herned, are, ***, O., |, 1980, but, will, ***, is, ., to, becoment, ., with, recall, has, |, nother, ments, was, the, to, of, stounicallity, with, camanfined, in, this, intations, it, conanament, out, they, you
While clearly the model still has much to learn, it has at this point compiled a significant collection of morphological observations, and has traveled a long way toward its goal of statistically characterizing English spellings.

VI. EXTENSIONS AND RELATIONS TO OTHER APPROACHES
In this section we briefly discuss some relations between our incremental feature induction algorithm for random fields and other statistical learning paradigms. We also present some possible extensions and improvements of our method.

A. Conditional exponential models
Almost all of what we have presented here carries over to the more general setting of conditional exponential models, including the Improved Iterative Scaling algorithm. For general
jconditional distributionspy x there may be no underlying ran-
dom field, but with features defined as binary functions fx; y,
the same general approach is applicable. The feature induction method for conditional exponential models is demonstrated for several problems in statistical machine translation in [3], where it is presented in terms of the principle of maximum entropy.

B. Decision trees

Our feature induction paradigm also bears some resemblence to various methods for growing classification and regression
trees. Like decision trees, our method builds a top-down classification that refines features. However, decision trees correspond to constructing features that have disjoint support.
To explain, recall that a decision tree determines a partition 
2 Xof a context random variable X in order to predict the actual 2 Yclass of the context, represented by a random variable Y .
Each leaf in the tree corresponds to a sequence of binary features

fl; fl"; fl""; : : :; froot
"where n denotes the parent of node n, each feature fn is a quesX :tion which splits , and where each fn is the negation fn of the
question asked at its sibling node. The distribution assigned to
Ya leaf l is simply the empirical distribution on determined by the training samples x; y 2 X  Y for which x = l. Each
leaf l is characterized by the conjunction of these features, and
different leaves correspond to conjunctions with disjoint support. In contrast, our feature induction algorithm generally results in features that have overlapping support. The criterion of evaluating questions in terms of the amount by which they reduce the
conditional entropy of Y corresponds to our criterion of maximizing the reduction in Kullback-Leibler divergence, Gqg, over all candidate features g for a field q.
By modifying our induction algorithm in the following way, we obtain an algorithm closely related to standard methods for growing binary decision trees. Instead of considering the 1-
^parameter family of fields q;g to determine the best candidate
g = a f, we consider the 2-parameter family of fields given by

q;0;f y

j x

=

1
Z;0;f

x

eax;y^f

x;y+0

:ax;y^f

x;y

:

Since the features a ^ f and :a ^ f have disjoint support, the ^improvement obtained by adding both of them is given by Gqa : ^f + Gq a f. In general, the resulting distribution is not

absolutely continuous with respect to the empirical distribution.

If the random variable Y
the standard decision tree

can take on algorithm is

oMbtavianleudesifya1t;th: :e:ny-Mth,stthaegne

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS

11

^we add the 2M (disjoint) features fnx yi; y, :fnx ^
yi; y, for i = 1; : : : ; M. Maximum likelihood training of the
parameters of these features recovers the empirical distribution
of the data at node n.

C. Extensions
As mentioned in Section 1, our approach differs from the most common applications of statistical techniques in computer vision, since a typical application of our method involves the estimation of thousands of free parameters. Yet the induction technique may not scale well to large 2-dimensional image problems. One potential difficulty is that the degree of the polynomials in the Improved Iterative Scaling algorithm could be quite large, and it could be difficult to obtain reliable estimates of the coefficients since Monte Carlo sampling might not exhibit sufficiently many instances of the desired features. The extent to which this is a significant problem is primarily an empirical issue, dependent on the particular domain to which the method is applied.
The random field induction method presented in this paper is not definitive; there are many possible variations on the basic theme, which is to incrementally construct an increasingly detailed exponential model to approximate the reference distri-
bution p~. Because the basic technique is based on a greedy algo-
rithm, there are of course many ways for improving the search for a good set of features. The algorithm presented in Section 2 is in some respects the most simple possible within the general framework. But it is also computationally intensive. A natural modification would be to add several of the top candidates at each stage. While this should increase the overall speed of the induction algorithm, it would also potentially result in more redundancy among the features, since the top candidates could be correlated. Another modification of the algorithm would be to add only the best candidate at each step, but then to carry out parameter estimation only after several new features had been added to the field. It would also be natural to establish a more Bayesian framework in which a prior distribution on features and parameters is incorporated. This could enable a principled approach for deciding when the feature induction is complete. While there is a natural class of conjugate priors for the class of exponential models that we use [14], the problem of incorporating prior knowledge about the set of candiate features is more challenging.

APPENDIX

I. DUALITY

In this Appendix we prove Proposition 4 restated here.

2Proposition 4: Suppose that p~
unique q?  satisfying

q0. Then there exists a

(1) (2)
q

q? 2 2DQ¯p

P k q

Q¯
=

Dp k

q?

+

Dq?

k q

for

any

p

2

P

and

(3)

q?

=

arg min
q2Q¯

Dp~ k q

(4) q? = argp2mPin Dp k q0.

Moreover, any of these four properties determines q? uniquely.

Our proof of the proposition will use a few lemmas. The first

two lemmas we state without proof.

kLemma 1:
(1) Dp q is a non-negative, extended real-valued func-

tion on
(2) Dp (3) Dp

k k


q q

.
= 0 if and only if p
is strictly convex in

= q. p and

q

separately.

(4) Dp k q is C1 in q.

RLemma 2:

(1) (2)

The The

map  ; p 7!
derivative of Dp

pk

is


smooth in  ; p 2
q with respect to 

n
is

 .

d dt

j

t=0

Dp

k

t

q =   p f

,q f :

Lemma 3: If p~ q0 then P Q¯ is nonempty.

Proof: Define q? by property (3) of Proposition 4; that is,

kq? = arg min q2Q¯ Dp~ q. To see that this makes sense, note

k 1 Qthat kDp

since p~ q is

q0, Dp~ q is not
continuous and strictly

identically convex as

a

on ¯ . function

Also,
of q.

Q kThus, since ¯ is closed, Dp~ q attains its minimum at a unique

Rpoint q? 2 Q¯ . We will
closed under the action

oshf ownt,hat

q? q?

is is

Palso in . Since in Q¯ for any .

Q¯ is
Thus

buysi!nthgeLDdemep~fimknaitiAonq.2?ow.f eqTc?ao,knicnlg=udde0eqrii?vsafativm=esinp~wimfitu.hmTrheosufpsetqch?te2tfouPnc. tainodn

Lemma 4: If q? 2 P Q¯ then for any p 2 P and q 2 Q¯

Dp k q = Dp k q? + Dq? k q :

Proof: A straightforward calculation shows that

Dp1 k q1 , Dp1 k q2 , Dp2 k q1 + Dp2 k q2 =   p1 f , p2 f 

2for any
identity

pa1n;dpt2h;eq1c;oqn2tinuitywoifthDq2th=at



q1.

It follows from this

Dp1 k q1 , Dp1 k q2 , Dp2 k q1 + Dp2 k q2 = 0

if p1; p2 2 P and q1; q2 2 Q¯ . The lemma follows by taking
p1 = q1 = q?.

P QProof of
Such a q?

Proposition 4: Choose exists by Lemma A.3.

q?
It

to be any point in satisfies property

(1)

¯. by

definition, and it satisfies property (2) by Lemma A.4. As a

consequence of property (2), it also satisfies properties (3) and

Q k(4). To check property
in ¯ , then Dp~ q =

D(3),p~fkorqi?ns+tanDce,qn?okteqthat

Dif qp~iskaqn?y.point

It remains to prove that each of the four properties (1)­(4)
determines q? uniquely. In other words, we need to show that if m is any point in  satisfying any of the four properties
TD(1h)e­mn(4kb),yq?thper+onpmDertyq=?(k2q)m?.fo.Sr uSqpi?npcwoesietDhthpmat=kmmqsa=t=ismfi0e,,siDtpfroomlploekwrtmys t(h1=a)t.
Dm; q? = 0 so m = q?. If m satisfies property (2), then the same argument with q? and m reversed again proves that m = q?. Suppose that m satisfies property (3). Then

Dp~ k q?  Dp~ k m = Dp~ k q? + Dq? k m

12 IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO. 4, APRIL 1997

k where the second equality follows from property (2) for q?. Thus
Dq? m 0 so m = q?. If m satisfies property (4), then a similar proof shows that once again m = q?.
1II. DEALING WITH

In this Appendix we prove an extension of Proposition 5 that

,1allows the components of to equal . For this extension,

we assume that all of the components of the feature function f

are non-negative: fi! 0 for all i and all !. This can be

assumed with no loss of generality since we can replace fi by

,fi min! fi! if necessary.

,1Let R

denote the partially extended real numbers with

the usual topology. The operations of addition and exponentia-

,1 tion extend
of R

conntinuoduesfilynetod

R
by

,1. Let S be the open subset

S f=  ; q : q!e f! 0 for some ! g

 SObserve that Rn  is a dense subset of . The map  ; q 7!

p, which up to this point we defined only for finite , extends

Suniquely to a continuous map from all of to . (The condition 2 Son  ; q ensures that the normalization in the definition of

p is well defined, even if is not finite.)

S ! ,1Definition 3: We call a function A tended auxiliary function for L if when

:R
restricted to

Rn

an ex it is

an ordinary auxiliary function in the sense of Definition 2, and

if, in addition, it satisfies property (1) of Definition 2 for any
q;  2 S, even if is not finite.

Note that if an ordinary auxiliary function extends to a contin-
Suous function on , then the extension is an extended auxiliary

function.

We have the following extension of Proposition 5:

Proposition 5: Suppose the feature function f satisfies the

non-negativity condition 7 and suppose A is an extended auxil-

iary function for L. Then the conclusion of Proposition 5 contin-

2ues to S  2 Sand

hold if the condition
A ; qk k A

o;nqkkfiosrraenpylaced; qbyk:

; qk k
.

kProsionfc:e

Lemma 1 is valid under the altered condition
A ; q satisfies property (1) of Definition 2 for

on all

2 S ; q . As a consequence, Lemma 2 also is valid, and the

proof of Proposition 5 goes through without change.

[3] A. Berger, V. Della Pietra, and S. Della Pietra, "A maximum entropy approach to natural language processing," Computational Linguistics, 22, No. 1, 39­71, 1996.
[4] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classification and Regression Trees, Wadsworth, Belmont, 1984.
[5] D. Brown, "A note on approximations to discrete probability distributions," Information and Control 2, 386­392 (1959).
n[6] P. Brown, V. Della Pietra, P. de Souza, J. Lai, and R. Mercer, "Class-based -gram models of natural language," Computational Linguistics 18, No. 4, 467­479, 1992.
[7] P. F. Brown, J. Cocke, V. Della-Pietra, S. Della-Pietra, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. "A statistical approach to machine translation," Computational Linguistics, 16(2):79­85, 1990.
m[8] B. Chalmond, "An iterative Gibbsian technique for reconstruction of -ary
images," Pattern Recognition, 22 No. 6, 747­761, 1989.
[9] I. Csisza´r, "I-Divergence geometry of probability distributions and minimization problems," The Annals of Probability, 3, No. 1, 146­158, 1975.
[10] I. Csisza´r, "A geometric interpretation of Darroch and Ratcliff's generalized iterative scaling," The Annals of Statistics, 17, No. 3, 1409­1413, 1989.
[11] I. Csisza´r and G. Tusna´dy, "Information geometry and alternating minimization procedures,"Statistics & Decisions, Supplement Issue, 1, 205­237, 1984.
[12] J. Darroch and D. Ratcliff, "Generalized iterative scaling for log-linear models," Ann. Math. Statist. 43, 1470­1480, 1972.
[13] A.P. Dempster, N.M. Laird, and D.B. Rubin, "Maximum likelihood from incomplete data via the EM algorithm," Journal of the Royal Statistical Society 39, No. B, 1­38, 1977.
[14] P. Diaconis and D. Ylvisaker, "Conjugate priors for exponential families," Ann. Statist. 7, 269­281, 1979.
[15] P. Ferrari, A. Frigessi and R. Schonmann, "Convergence of some partially parallel Gibbs samplers with annealing," The Annals of Applied Probability, 3 No. 1, 137­152, 1993.
[16] A. Frigessi, C. Hwang, and L. Younes, "Optimal spectral structure of reversible stochastic matrices, Monte Carlo methods and the simulation of Markov random fields," The Annals of Applied Probability, 2, No. 3, 610­628, 1992.
[17] S. Geman and D. Geman, "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images," IEEE Trans. Pattern Anal. Machine Intell. 6, 721­741, 1984.
[18] C. Geyer and E. Thomson, "Constrained Monte Carlo maximum likelihood for dependent data (with discussion)", J. Royal Stat. Soc. B-54, 657­699, 1992.
[19] E. T. Jaynes, Papers on Probability, Statistics, and Statistical Physics, R. Rosenkrantz, ed., D. Reidel Publishing Co., Dordrecht­Holland, 1983.
[20] J. Lafferty and R. Mercer, "Automatic word classification using features of spellings," Proceedings of the 9th Annual Conference of the University of Waterloo Centre for the New OED and Text Research, Oxford University Press, Oxford, England, 1993.
[21] G. Potamianos and J. Goutsias, "Partition function estimation of Gibbs random field images using Monte Carlo simulations," IEEE Transactions on Information Theory 39, No. 4, 1322­1331, July, 1993.
[22] L. Younes, "Estimation and annealing for Gibbsian fields," Ann. Inst. H. Poincare´ Probab. Statist. 24 No. 2, 269­294, 1988.

III. ACKNOWLEDGEMENTS
Part of the research presented in this paper was carried out while the authors were with the IBM Thomas J. Watson Research Center in Yorktown Heights, New York. Stephen Della Pietra and Vincent Della Pietra's work was partially supported by ARPA under grant N00014-91-C-0135. John Lafferty's work was partially supported by NSF and ARPA under grants IRI9314969 and N00014-92-C-0189.
REFERENCES
[1] M. Almeida and B. Gidas, "A variational method for estimating the parameters of MRF from complete or incomplete data," The Annals of Applied Probability, 3, No. 1, 103­136, 1993.
[2] N. Balram and J. Moura, "Noncausal Gauss Markov random fields: Parameter structure and estimation," IEEE Transactions on Information Theory 39, No. 4, 1333­1343, July, 1993.

Stephen A. Della Pietra received his A.B. in Physics from Princeton University in 1981, and Ph.D. in Physics from Harvard University in 1987. From 1987 until 1988 he was a Post-Doctoral Fellow at the University of Texas at Austin. From 1988 until 1989, he was a Member of the Mathematics Department of the Institute for Advanced Study in Princeton, NJ. From 1989 until 1995 he was a Research Staff Member at the IBM Thomas J. Watson Research Center in Yorktown Heights and Hawthorne, NY, where he was project leader of the natural language understanding group. His primary research at IBM was in machine translation and natural language understanding. Since 1995 he has been working on statistical methods for modeling the stock market at Renaissance Technologies in Stony Brook, NY.

DELLA PIETRA, DELLA PIETRA, AND LAFFERTY: INDUCING FEATURES OF RANDOM FIELDS
Vincent J. Della Pietra received his A.B. in Mathematics from Princeton University in 1982, and Ph.D. in Mathematical Physics from Harvard University in 1988. From 1987 until 1988, he was a Member of the Mathematics Department of the Institute for Advanced Study in Princeton, NJ. From 1988 until 1995 he was a Research Staff Member at the IBM Thomas J. Watson Research Center in Yorktown Heights and Hawthorne, NY, where he was project leader of the machine translation group. His primary research at IBM was in machine translation and natural language understanding. Since 1995 he has been working on statistical methods for modeling the stock market at Renaissance Technologies in Stony Brook, NY.
John D. Lafferty studied mathematics as an undergraduate at Middlebury College and the Massachusetts Institute of Technology, and received the Ph.D. degree in Mathematics from Princeton University in 1986, where he was a member of the Program in Applied and Computational Mathematics. From 1986 until 1987 he was an Assistant Professor and Benjamin Pierce Lecturer on Mathematics at Harvard University, and from 1987 until 1988 he was Visiting Assistant Professor at the Nankai Institute of Mathematics in Tianjin, China. In 1989 he joined the Computer Sciences Department of the IBM Thomas J. Watson Research Center in Yorktown Heights, NY, as a Research Staff Member. Since 1994 he has been a member of the faculty of the Computer Science Department at Carnegie Mellon University, where his primary research interests include speech, language, information theory and statistical learning algorithms.

13

Estimators for Stochastic Uni cation-Based" Grammars

Mark Johnson

Stuart Geman

Stephen Canon

Cognitive and Linguistic Sciences Applied Mathematics Cognitive and Linguistic Sciences

Brown University

Brown University

Brown University

Zhiyi Chi

Stefan Riezler

Dept. of Statistics

Institut fur Maschinelle Sprachverarbeitung

The University of Chicago

Universitat Stuttgart

Abstract
Log-linear models provide a statistically sound framework for Stochastic Uni cation-Based" Grammars SUBGs and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of LexicalFunctional Grammar.
1 Introduction
Probabilistic methods have revolutionized computational linguistics. They can provide a systematic treatment of preferences in parsing. Given a suitable estimation procedure, stochastic models can be tuned" to re ect the properties of a corpus. On the other hand, Uni cation-Based" Grammars UBGs can express a variety of linguistically-important syntactic and semantic constraints. However, developing Stochastic Uni cation-based" Grammars SUBGs has not proved as straightforward as might be hoped.
The simple relative frequency" estimator for PCFGs yields the maximum likelihood parameter estimate, which is to say that it minimizes the Kulback-Liebler divergence between the training and estimated distributions. On the other hand, as Abney 1997 points out, the context-sensitive dependencies that uni cation-based" constraints introduce render the relative frequency estimator suboptimal: in general it does not maximize the likelihood and it is inconsistent.
 This research was supported by the National Science Foundation SBR-9720368, the US Army Research Ofce DAAH04-96-BAA5, and O ce of Naval Research N00014-97-1-0249.

Abney 1997 proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework. However, the Monte-Carlo parameter estimation procedure that Abney proposes seems to be computationally impractical for reasonable-sized grammars. Sections 3 and 4 describe two new estimation procedures which are computationally tractable. Section 5 describes an experiment with a small LFG corpus provided to us by Xerox Parc. The log linear framework and the estimation procedures are extremely general, and they apply directly to stochastic versions of HPSG and other theories of grammar.
2 Features in SUBGs
We follow the statistical literature in using the term feature to refer to the properties that parameters are associated with we use the word attribute" to refer to the attributes or features of a UBG's feature structure. Let be the set of all possible grammatical or well-formed analyses. Each feature f maps a syntactic anal-
ysis ! 2 to a real value f!. The form of
a syntactic analysis depends on the underlying linguistic theory. For example, for a PCFG ! would be parse tree, for a LFG ! would be a tuple consisting of at least a c-structure, an fstructure and a mapping from c-structure nodes to f-structure elements, and for a Chomskyian transformational grammar ! would be a derivation.
Log-linear models are models in which the log probability is a linear combination of feature values plus a constant. PCFGs, Gibbs distributions, Maximum-Entropy distributions and Markov Random Fields are all examples of log-linear models. A log-linear model associates each feature fj with a real-valued parameter j.

A log-linear model with m features is one in

which the likelihood P! of an analysis ! is:

PP!

=

1 Z

e

j=1;:::;m jfj!

X PZ =

e j=1;:::;m jfj!0

!02

While the estimators described below make

no the

assumptions about models considered

the range here the

voafluteheoffie,acihn

lfaeartsutrreucftiu!ral

is the number of times a particuarrangement or con guration oc-

curs in the analysis !, so fi! ranges over the

natural numbers.

For example, the features of a PCFG are

iiontfhdefxepaertdoudrbueyctifpoirnoidsius cttuhisoeendsn, uinim.eb.t,ehrethoedfevraitvilmauteeiosfnit!h!e.

This set of features induces a tree-structured

dependency graph on the productions which

is characteristic of Markov Branching Pro-

cesses Pearl, 1988; Frey, 1998. This tree

structure has the important consequence that

simple relative-frequencies" yield maximum-

likelihood estimates for Extending a PCFG

mthoedeli.

by

adding

addi-

tional features not associated with productions

will in general add additional dependencies, de-

stroy the tree structure, and substantially com-

plicate maximum likelihood estimation.

This is the situation for a SUBG, even if the

features are production occurences. The uni-

cation constraints create non-local dependen-

cies among the productions and the dependency

graph of a SUBG is usually not a tree. Conse-

quently, maximum likelihood estimation is no

longer a simple matter of computing relative

frequencies. But the resulting estimation proce-

dures discussed in detail, shortly, albeit more

complicated, have the virtue of applying to es-

sentially arbitrary features|of the production

or non-production type. That is, since estima-

tors capable of nding maximum-likelihood pa-

rameter estimates for production features in a

SUBG will also nd maximum-likelihood esti-

mates for non-production features, there is no

motivation for restricting features to be of the

production type.

Linguistically there is no particular reason

for assuming that productions are the best fea-

tures to use in a stochastic language model.

For example, the adjunct attachment ambiguity in 1 results in alternative syntactic structures which use the same productions the same number of times in each derivation, so a model with only production features would necessarily assign them the same likelihood. Thus models that use production features alone predict that there should not be a systematic preference for one of these analyses over the other, contrary to standard psycholinguistic results.

1.a VP VPBill thought Hillary

left yesterday

VP VP1.b Bill

thought Hillary left yesterday

There are many di erent ways of choosing features for a SUBG, and each of these choices makes an empirical claim about possible distributions of sentences. Specifying the features of a SUBG is as much an empirical matter as specifying the grammar itself. For any given UBG there are a large usually in nite number of SUBGs that can be constructed from it, di ering only in the features that each SUBG uses.
In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear 1995. Adjunct and argument features indicate adjunct and argument attachment respectively, and permit the model to capture a general argument attachment preference. In addition, there are specialized adjunct and argument features corresponding to each grammatical function used in LFG e.g., SUBJ, OBJ, COMP, XCOMP, ADJUNCT, etc.. There are features indicating both high and low attachment determined by the complexity of the phrase being attached to. Another feature indicates nonright-branching nonterminal nodes. There is a feature for non-parallel coordinate structures where parallelism is measured in constituent structure terms. Each f-structure attributeatomic value pair which appears in any feature structure is also used as a feature. We also use a number of features identifying syntactic structures that seem particularly important in these corpora, such as a feature identifying NPs that are dates it seems that date interpretations of NPs are preferred. We would have liked to have included features concerning speci c lexical items to capture head-to-head dependencies, but we felt that our corpora were so small

that the associated parameters could not be accurately estimated.

3 Alogplsienuedaor-mlikoedliehlsood estimator for

P!Speuuiasp=nlpo1od;og:f:s:iL;etnnsf!sed!jeyen!r=itiva=a,cttt!iihvc1ee;sa:lXon:ag:ar;elly!:iksneejslfi.ihsjoL!eoaedt,ttoirfnangtinhloifengjcgZo!ercpou2=rs-

@ log L!e
@j

=

j=1;:::;m
fj!e , nEfj

3

twhheerdeisEtribfujtioisn

the expected value determined by the

of fj under parameters

iwn.hTicnhhdeimnmgaaxtxihmimeiuzmemal-xoliigmkLeulimh!eo-loi.kdeTelishhteoimocdahtieeessftidamrieattcehuseltiys

tchalecusplaatciengofEwefllj-f,owrmheicdhsiynnvtoalvcteiscssutmrumctiunrgeosver.

There seems to be no analytic or e cient nu-

merical way of doing this for a realistic SUBG.

Abney 1997 proposes a gradient ascent,

based upon a Monte Carlo procedure for esti-

smeatiasnotamdnitmiphnPlaeget^nseEc,!oefa,tffnehjdwaet.htugoeTrrraeheudessit^eeirndutiteshcateotusfihersteehtstocoefurgleroieskrmnteeienlmitrthhaaotepteoeaddrriEa.asmtn^Srdieafbotmjuemr--,

ples are generated as follows: Given a SUBG,

Abney constructs a covering PCFG based upon

the SUBG and ^, the current estimate of . The

derivation trees of the PCFG can be mapped

onto a set containing all of the SUBG's syn-

tactic analyses. Monte Carlo samples from the

PCFG are comparatively easy to generate, and

sample syntactic analyses that do not map to

well-formed SUBG syntactic structures are then

simply discarded. This generates a stream of

syntactic structures, but not distributed accord-

ing the

rteostPri^ct!iondofistthriebPutCeFdGintsotetahde

according to SUBG. Ab-

ney proposes using a Metropolis acceptance-

rejection method to adjust the distribution of

this stream of feature structures to achieve de-

tailed balance, which then produces a stream

of feature structures distributed according to

P^W!h.ile this scheme is theoretically sound, it

would appear to be computationally impracti-

cal for realistic SUBGs. Every step of the proposed procedure corresponding to a single step of gradient ascent requires a very large number of PCFG samples: samples must be found that correspond to well-formed SUBGs; many such samples are required to bring the Metropolis algorithm to near equilibrium; many samples are needed at equilibrium to properly estimate E^Tfhje.idea of a gradient ascent of the likelihood 2 is appealing|a simple calculation reveals that the likelihood is concave and therefore free of local maxima. But the gradient in particular, Efj is intractable. This motivates an alternative strategy involving a data-based estimate of Efj:

Efj

=


En1i=EX1;:::f;njE!jfyj!!jy!

=

4 yi5

where y! is the yield belonging to the syn-

tbaecltoincgianngaltyostishe!i,'tahnsdamyipl=e inyt!hie

is the yield training cor-

pus.

The point is that erally computable.

IEnfafcjt,!ifjy!y=is

ytiheissegteno-f

well-formed syntactic structures that have yield

y i.e., the set of possible parses of the string y,

then

Efj!jy! = yi =
P P!02 yi fj!0e k=1;:::;m kfk!0 P P!02 yi e k=1;:::;m kfk!0

Hence the calculation of the conditional expec-

tations only involves summing over the possible

isnyntthaectticraainnianlgysceosroprups.arWseshileyiit

of is

the strings possible to

construct UBGs for which the number of pos-

sible parses is unmanageably high, for many

grammars it is quite manageable to enumerate

the set of possible parses and thereby directly

evaTluhaerteefoEre,fwje!pjryop!ose=reypil.acing the gradient,

3, by
fj!e ,

X

Efj!jy! = yi

6

i=1;:::;n

and performing a gradient ascent. Of course 6 is no longer the gradient of the likelihood func-

tion, but fortunately it is exactly the gradient of the log of another criterion:
PL!e = Y P! = !ijy! = yi 7 i=1;:::;n

Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we

maximize the conditional likelihood of these analyses given the observed yields. In our exper-

iments, we have used a conjugate-gradient optimization program adapted from the one pre-

sented in Press et al. 1992.

Regardless of the pragmatic computational

motivation, one could perhaps argue that the

conditional ful if not

probabilities more useful

aPs  t!hejyfulalreproabs aubsilei--

ties the

uPltim!a,teagtoalelaisst

in those syntactic

cases for analysis.

which Berger

et al. 1996 and Jelinek 1997 make this same

point and arrive at the same estimator, albeit through a maximum entropy argument.

The problem of estimating parameters for log-linear models is not new. It is especially dif-

cult in cases, such as ours, where a large sample space makes the direct computation of ex-

pectations infeasible. Many applications in spatial statistics, involving Markov random elds

MRF, are of this nature as well. In his seminal development of the MRF approach to

spatial statistics, Besag introduced a pseudolikelihood" estimator to address these di cul-

ties Besag, 1974; Besag, 1975, and in fact our proposal here is an instance of his method. In

general, the likelihood function is replaced by a more manageable product of conditional likeli-

hoods a pseudo-likelihood|hence the designa-

rtiaomnePteLrv,ecwthoric,hinissttehaedn

optimized over the paof the likelihood itself.

In many cases, as in our case here, this sub-

stitution side steps much of the computational burden without sacri cing consistency more on this shortly.

What are the asymptotics of optimizing a

pseudo-likelihood function? Look rst at the likelihood itself. For large n:

1 n

log

L!e

= =

Y1
n

log

i=1;:::;n

P!i

X1
n

i=1;:::;n

log

P!i

Z
 Po! log P!d! 8

wtehrevreectoor.is

the true and unknown parameUp to a constant, 8 is the nega-

tive of the Kullback-Leibler divergence between

the true and estimated distributions of syntac-

tic analyses. As sample size grows, maximizing

likelihood amounts to minimizing divergence.

As for pseudo-likelihood:

1 n

log

PL!e

= =

1 n 1 n

log Y Xi=1;:::;n
log
Zi=1;:::;n

P! P!

= =

!ijy! !ijy!

= =

yi yi

 Eo Po!jy log P!jyd!

So that maximizing pseudo-likelihood at large samples amounts to minimizing the average over yields divergence between the true and estimated conditional distributions of analyses given yields.
Maximum likelihood estimation is consistent: under broad conditions the sequence of distli!sirk1ia;bel:lusi:hot:io!oconondsn, scePioss^ttnneivm,netraa,gtsboesosructtifoaointrPetdhooe.wgpiPitrvhesesenetuhndtteohi-emmlikapsexallemiimmhpouelonemdstation it is consistent for the conditional dis-
ttnhroietbufhutailorldndsitsoPtrsiobeeu!ttjihyoan!tPpsoeaunsddeoe-nlCiokthelinihe1oc9eos9ds8awri.illylIntfooirts
faisliwa!ayfes=actuofrirreyecf!tilyw.ehsitcLihmatadeterepwePnedows. iolSlnurlyepfpeoronsteoyiteshuledcrshe:
fdcfPveaoaeLacnnrttidtu,vi!aretitneotiosivttncheahaosielnostfpdpcasirPaiseonstLubersdilbeaonm!enu-ocytoioionwvnfnasifcttloPhauhrnoemroto.e!assfitjnpiyoIgeinnc!ggttioavhtob;oeidossuitpcitihaasiesrsiesirs,ez.raseetm.rlhIoenee-;
Despite the assurance of consistency, pseudolikelihood estimation is prone to over tting when a large number of features is matched against a modest-sized training corpus. One particularly troublesome manifestation of over tting results from the existence of features which, relative to the training set, we might term pseudo-maximal": Let us say that a feature f is pseudo-maximal for a yield y i

8!0 2 y f!  f!0 where ! is any cor-

rect parse of y, i.e., the feature's value on every

correct parse ! of y is greater than or equal

to its value on any other parse of y. Pseudo-

minimal features are de ned similarly. It is easy

to see that if fj is pseudo-maximal on each sen-

tence of the training corpus then the param-

eter assignment j = 1 maximizes the cor-

pus pseudo-likelihood. Similarly, the assign-

mfjeinstpsjeu=do,-m1inmimaaxlimoviezresthpeseturdaion-liinkgelcihoropoudsif.

Such in nite parameter values indicate that the

model treats pseudo-maximal features categori-

cally; i.e., any parse with a non-maximal feature

value is assigned a zero conditional probability.

Of course, a feature which is pseudo-maximal

over the training corpus is not necessarily

pseudo-maximal for all yields. This is an in-

stance of over tting, and it can be addressed,

as is customary, by adding a regularization term

that promotes small values of  to the objec-

tive function. A common choice is to add a

quadratic to the log-likelihood, which corre-

sponds to multiplying the likelihood itself by

a normal distribution. In our experiments, we

multiplied the pseudo-likelihood by a zero-mean

an7notdrimmwaeilsthitnhseta1mn; :da:xa:irmdmud,mewvviiatahtliuodeniaogfjofnfjoafrloucojnvdeaqriuinaanalcnteyo,

parse in the training corpus. We experimented

with other values for j, but the choice seems to

have little e ect. Thus instead of maximizing

the log pseudo-likelihood, we choose ^ to maxi-

mize

log

PL!e

,

X
j=1;:::;m

j2 2 j2

9

4 lAogmlianxeiamrummocdoerlsrect estimator for

The pseudo-likelihood estimator described in the last section nds parameter values which maximize the conditional probabilities of the observed parses syntactic analyses given the observed sentences yields in the training corpus. One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of
shathilobleolpednouopsmbsaijrbbeselceertipCvueanrdfs!eueerns.cotfTioithsinmii:snecssfhuatocghtogeseteshmtes^acxatoionrmromeutchmatexrpliiampkreoislzseie--,

iannCdthme!eotrsatisicnoainnhvgiegcnhotlriyopnudasis.lcoopnttiimnuizoautsiofnunacltgioornitohfms,
perform poorly on it. We had the most success with a slightly modi ed version of the simulated annealing optimizer described in Press et al. 1992. This procedure is much more computationally intensive than the gradient-based pseudo-likelihood procedure. Its computational di culty grows and the quality of solutions degrade rapidly with the number of features.
5 Empirical evaluation
Ron Kaplan and Hadar Shemtov at Xerox Parc provided us with two LFG parsed corpora. The Verbmobil corpus contains appointment planning dialogs, while the Homecentre corpus is drawn from Xerox printer documentation. Table 1 summarizes the basic properties of these corpora. These corpora contain packed c fstructure representations Maxwell III and Kaplan, 1995 of the grammatical parses of each sentence with respect to Lexical-Functional grammars. The corpora also indicate which of these parses is in fact the correct parse this information was manually entered. Because slightly di erent grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below. Table 2 describes the properties of the features used for each corpus.
In addition to the two estimators described above we also present results from a baseline estimator in which all parses are treated as equally likely this corresponds to setting all the parameters j to zero.
tmeseWtaseucoreervspa.ulusaInt!eetdaensto.uarcteWusatelimpuaastreosdirnsgtuwasoipnpgelvichaaleutldiao-tnoiounat
SUBG might be used to identify the correct parse from the set of grammatical parses, so
wCotiumh^roa!esttereessdmttaemvxooafimdluseuealnmti^toelnniiskcmeealsceihtaiuosnuaolrdtlehypecatohrtuesenesttcusocnrtordhreeepcrtuntusphmae!erbteseesesrt-.
If a sentence has l most likely parses i.e., all l parses have the same conditional probability and one of these parses is the correct parse, then we score 1=l for this sentence.
The second evaluation measure is the pseudo-

Verbmobil corpus Homecentre corpus

Number of sentences

540 980

Number of ambiguous sentences 314 481

Number of parses of ambiguous sentences

3245

3169

Table 1: Properties of the two corpora used to evaluate the estimators.

Verbmobil corpus Homecentre corpus

Number of features

191 227

Number of rule features

59 57

Number of pseudo-constant features

19

41

Number of pseudo-maximal features

12

4

Number of pseudo-minimal features

8

5

Table 2: Properties of the features used in the stochastic LFG models. The numbers of pseudomaximal and pseudo-minimal features do not include pseudo-constant features.

likelihood likelihood

oiftstehlef,tesPtLc^or!eptuessti.s

The pseudothe likelihood of

the correct parses given their yields, so pseudo-

likelihood measures how much of the probability mass the model puts onto the correct anal-

yses. This metric seems more relevant to applications where the system needs to estimate

how likely it is that the correct analysis lies in a certain set of possible parses; e.g., ambiguity-

preserving translation and human-assisted disambiguation. To make the numbers more man-

ageable, we actually present the negative logarithm of the pseudo-likelihood rather than the

pseudo-likelihood itself|so smaller is better.

Because of the small size of our corpora we

evaluated our estimators using a 10-way crossvalidation paradigm. We randomly assigned sentences of each corpus into 10 approximately equal-sized subcorpora, each of which was used

in turn as the test corpus. We evaluated on each subcorpus the parameters that were estimated

from the 9 remaining subcorpora that served as the training corpus for this run. The evalua-

tion scores from each subcorpus were summed in order to provide the scores presented here.

Table 3 presents the results of the empirical evaluation. The superior performance of both estimators on the Verbmobil corpus prob-

ably re ects the fact that the non-rule features were designed to match both the gram-

mar and content of that corpus. The pseudolikelihood estimator performed better than the

correct-parses estimator on both corpora un-

der both evaluation metrics. There seems to be substantial over learning in all these models; we routinely improved performance by discarding features. With a small number of features the correct-parses estimator typically scores better than the pseudo-likelihood estimator on the correct-parses evaluation metric, but the pseudo-likelihood estimator always scores better on the pseudo-likelihood evaluation metric.
6 Conclusion
This paper described a log-linear model for SUBGs and evaluated two estimators for such models. Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG. Indeed, the number and choice of features strongly in uences the performance of the model. The estimated models are able to identify the correct parse from the set of all possible parses approximately 50 of the time.
We would have liked to introduce features corresponding to dependencies between lexical items. Log-linear models are well-suited for lexical dependencies, but because of the large number of such dependencies substantially larger corpora will probably be needed to estimate such models.1
1Alternatively, it may be possible to use a simpler non-SUBG model of lexical dependencies estimated from a much larger corpus as the reference distribution with

Baseline estimator

C9V!.e7teerstbmo,bliolgcP5o3Lr3p!eutsest C1H5!e.o2temstece,ntlroeg P6c5oL5rp!eutesst

Pseudo-likelihood estimator 58.7 396 58.8 583

Correct-parses estimator 53.7 469 53.2 604

Table 3: parses of

An the

etmespticriocrapluesvathluaattwioenreofthtehecoersrteimctaptoarrss.esC, a!nedtes,t

of the pseudo-likelihood of the test corpus.

liosgtPhLe n!eutmestbeirs

of maximum the negative

likelihood logarithm

However, there may be applications which can bene t from a model that performs even at this level. For example, in a machine-assisted translation system a model like ours could be used to order possible translations so that more likely alternatives are presented before less likely ones. In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.
References
Steven P. Abney. 1997. Stochastic AttributeValue Grammars. Computational Linguistics, 234:597 617.
Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 221:39 71.
J. Besag. 1974. Spatial interaction and the statistical analysis of lattice systems with discussion. Journal of the Royal Statistical Society, Series D, 36:192 236.
J. Besag. 1975. Statistical analysis of nonlattice data. The Statistician, 24:179 195.
Zhiyi Chi. 1998. Probability Models for Complex Systems. Ph.D. thesis, Brown University.
Brendan J. Frey. 1998. Graphical Models for Machine Learning and Digital Communication. The MIT Press, Cambridge, Massachusetts.
Jerry R. Hobbs and John Bear. 1995. Two principles of parse preference. In Antonio Zampolli, Nicoletta Calzolari, and Martha Palmer, editors, Linguistica Computazionale: Current Issues in Computational Linguistics:
respect to which the SUBG model is de ned, as described in Jelinek 1997.

In Honour of Don Walker, pages 503 512. Kluwer. Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts. John T. Maxwell III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in LexicalFunctional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14, pages 381 481. CSLI Publications. Judea Pearl. 1988. Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California. William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipies in C: The Art of Scienti c Computing. Cambridge University Press, Cambridge, England, 2nd edition.

×Ø Ñ Ø ÓÒ Ó ËØÓ

×Ø ØØÖ ÙØ ¹Î ĞÙ ÁÒ ÓÖÑ Ø Ú Ë ÑÔĞ

Ö ÑÑ Ö× Ù× Ò

Å Ğ × Ç× ÓÖÒ

Ó× ÓÖÒ Ğ ØºÖÙ ºÒĞ

Ê ×ÙÒ Ú Ö× Ø Ø ÖÓÒ Ò Ò¸ Ì

Æ Ø ÖĞ Ò ×£

Ò

×ØÖ Ø Ï Ö Ù Ø Ø ×ÓÑ Ó Ø ÓÑÔÙØ Ø ÓÒ Ğ ÓÑÔĞ Ü Øİ ××Ó Ø Û Ø ×Ø Ñ Ø ÓÒ Ó ×ØÓ ×Ø ØØÖ ÙØ ¹ Ú ĞÙ Ö ÑÑ Ö× Ò Ö Ù İ ØÖ Ò Ò ÙÔÓÒ Ò Ò ÓÖÑ Ø Ú ×Ù × Ø Ó Ø ÙĞĞ ØÖ Ò Ò × Øº Ê ×ÙĞØ× Ù× Ò Ø Ô Ö× Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ ÓÖÔÙ× × ÓÛ Ø Ø Ò ×ÓÑ Ö ÙÑ×Ø Ò ×¸ Ø × ÔÓ×× Ğ ØÓ Ó Ø Ò
ØØ Ö ×Ø Ñ Ø ÓÒ Ö ×ÙĞØ× Ù× Ò Ò Ò ÓÖÑ Ø Ú × Ñ¹ ÔĞ Ø Ò Û Ò ØÖ Ò Ò ÙÔÓÒ ĞĞ Ø Ú Ğ Ğ Ñ ¹ Ø Ö Ğº ÙÖØ Ö ÜÔ Ö Ñ ÒØ Ø ÓÒ ÑÓÒ×ØÖ Ø × Ø Ø Û Ø ÙÒĞ Ü Ğ × ÑÓ Ğ×¸ Ù×× Ò ÔÖ ÓÖ Ò Ö ¹ Ù ÓÚ Ö¬ØØ Ò º ÀÓÛ Ú Ö¸ Û Ò ÑÓ Ğ× Ö Ğ Ü ¹
Ğ × Ò ÓÒØ Ò ÓÚ ÖĞ ÔÔ Ò ØÙÖ ×¸ ÓÚ Ö¬ØØ Ò Ó × ÒÓØ × Ñ ØÓ ÔÖÓ Ğ Ñ¸ Ò Ù×× Ò ÔÖ ÓÖ Ñ × Ñ Ò Ñ Ğ « Ö Ò ØÓ Ô Ö ÓÖÑ Ò º ÇÙÖ Ô¹ ÔÖÓ × ÔÔĞ Ğ ÓÖ × ØÙ Ø ÓÒ× Û Ò Ø Ö Ö Ò Ò × Ğİ Ğ Ö ÒÙÑ Ö Ó Ô Ö× × Ò Ø ØÖ Ò Ò × Ø¸ ÓÖ Ğ× ÓÖ Û Ò Ö ÓÚ Öİ Ó Ø × Ô Ö× × ÖÓÑ Ô Ö ÔÖ × ÒØ Ø ÓÒ × Ø× Ğ ÓÑÔÙØ Ø ÓÒ ĞĞİ Ü¹ Ô Ò× Ú º

½ ÁÒØÖÓ Ù Ø ÓÒ Ò İ × ÓÛ Ø Ø ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö× Ò¹
ÒÓØ ÑÓ ĞĞ ÕÙ Ø Ğİ Ù× Ò ×Ø Ø ×Ø Ğ Ø ¹ Ò ÕÙ × Û ××ÙÑ Ø Ø ×Ø Ø ×Ø Ğ Ô Ò Ò × Ö ÒØ Ğ ´ Ò İ¸ ½ µº ÁÒ×Ø Ó Ù× Ò ÑÓ Ğ Ğ ×× Ø Ø ××ÙÑ Ò Ô Ò Ò ¸ Ò İ ×Ù ¹
×Ø Ù× Ò Ê Ò ÓÑ Ğ × ÅÓ Ğ× ´Ê Å×µ ÓÖ ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö×º Ê Å× Ğ Û Ø Ø Ö Ô Ğ ×ØÖÙ ØÙÖ Ó Ô Ö× º Ù× Ø İ Ó ÒÓØ Ñ Ò Ô Ò Ò ××ÙÑÔØ ÓÒ× ÓÙØ Ø ×ØÓ ×¹ Ø Ò Ö Ø ÓÒ ÔÖÓ ×× Ø Ø Ñ Ø Ú ÔÖÓ Ù ×ÓÑ Ô Ö× ¸ Ø İ Ö Ğ ØÓ ÑÓ Ğ ÓÖÖ ØĞİ Ô Ò¹ Ò × Ø Ø Ü ×Ø Û Ø Ò Ô Ö× ×º Ï Ò ×Ø Ñ Ø Ò ×Ø Ò Ö Ğİ¹ ÓÖÑÙĞ Ø Ê Å×¸ Ø × Ò ×× Öİ ØÓ ×ÙÑ ÓÚ Ö ĞĞ Ô Ö× × Ğ Ò× İ Ø Ö ÑÑ Öº ÓÖ Ñ Òİ ÖÓ ÓÚ Ö Ò ØÙÖ Ğ Ğ Ò¹ Ù Ö ÑÑ Ö×¸ Ø × Ñ Ø ÒÚÓĞÚ ×ÙÑÑ Ò ÓÚ Ö Ò ÜÔÓÒ ÒØ Ğ ÒÙÑ Ö Ó Ô Ö× ×º Ì × ÛÓÙĞ Ñ Ø Ø × ÓÑÔÙØ Ø ÓÒ ĞĞİ ÒØÖ Ø Ğ º Ò İ¸ ÓĞ¹ ĞÓÛ Ò Ø Ğ Ó Ä « ÖØİ Ø Ğ¸ ×Ù ×Ø ÅÓÒØ

£ ÙÖÖ ÒØ

Ö ×× Ó× ÓÖÒ Ó × º

Ò ÙÖ ¸ Ú × ÓÒ Ó ÁÒ ÓÖÑ Ø ×¸ ¾

ÄÏ¸ Ë ÓØĞ Ò º

º ºÙ ¸ ÍÒ Ú Ö× Øİ Ó Ù Ğ Ù ÈĞ ¸ À

ÖĞÓ × ÑÙĞ Ø ÓÒ × Û İ Ó Ö Ù Ò Ø ÓÑÔÙØ ¹ Ø ÓÒ Ğ ÙÖ Ò ××Ó Ø Û Ø Ê Å ×Ø Ñ Ø ÓÒ ´Ä ¹
ÖØİ Ø Ğº¸ ½ µº ÀÓÛ Ú Ö¸ ÂÓ Ò×ÓÒ Ø Ğ ÓÒ× ¹ Ö Ø ÓÖÑ Ó × ÑÔĞ Ò Ù× Ò Ø × × ÑÙĞ Ø ÓÒ ´Å ØÖÓÔÓĞ ×¹À ×Ø Ò ×µ ÒØÖ Ø Ğ ´ÂÓ Ò×ÓÒ Ø Ğº¸ ½ µº ÁÒ×Ø ¸ Ø İ ÔÖÓÔÓ× Ò ĞØ ÖÒ Ø Ú ×ØÖ Ø¹ İ Ø Ø Ö ¬Ò Ø ×Ø Ñ Ø ÓÒ Ø × º ÁØ Û × Ö Ù Ø Ø Ø × Ö ¬Ò Ø ÓÒ Ñ ×Ø Ñ Ø ÓÒ ÓÑÔÙØ Ø ÓÒ¹ ĞĞİ × ÑÔĞ ÒÓÙ Ø Ø ÅÓÒØ ÖĞÓ × ÑÙĞ Ø ÓÒ Û × ÙÒÒ ×× Öİº Ì İ ÔÖ × ÒØ Ö ×ÙĞØ× Ó Ø Ò Ù× Ò ×Ñ ĞĞ ÙÒĞ Ü Ğ × ÑÓ Ğ ØÖ Ò ÓÒ ÑÓ ¹ ×Ø ÓÖÔÙ×º ÍÒ ÓÖØÙÒ Ø Ğİ¸ ÂÓ Ò×ÓÒ Ø Ğ ××ÙÑ Ø Û × ÔÓ×× ¹ Ğ ØÓ Ö ØÖ Ú ÔĞĞ Ö× × Ğ Ò× İ Ö ÑÑ Ö Û Ò Ô Ö× Ò Ú Ò ØÖ Ò Ò × Øº ÓÖ Ù×¸ Ø × Û × ÒÓØ Ø × º ÁÒ ÓÙÖ ÜÔ Ö Ñ ÒØ× Û Ø Ñ ÒÙ ĞĞİ ÛÖ Ø¹ Ø Ò ÖÓ ÓÚ Ö ¬Ò Ø Ğ Ù× Ö ÑÑ Ö ´ µ ´ Ö × Ó Ò ÖÖÓĞĞ¸ ½ µ¸ Û Û Ö ÓÒĞİ Ğ ØÓ Ö ¹ ÓÚ Ö ĞĞ Ô Ö× × ÓÖ Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ × ÒØ Ò × Ø Ø Û Ö Ø ÑÓ×Ø ½¿ ØÓ Ò× ĞÓÒ Û Ø Ò ÔØ Ğ Ø Ñ Ò ×Ô ÓÙÒ × ÓÒ ÓÑÔÙØ Ø ÓÒº Ï Ò Û Ù× Ò Ò Ö Ñ ÒØ Ğ Å Ò ÑÙÑ × Ö ÔØ ÓÒ Ä Ò Ø ´Å Äµ × Ğ ÖÒ Ö ØÓ ÜØ Ò Ø ÓÚ Ö Ó ÓÙÖ Ñ ÒÙ ĞĞİ ÛÖ ØØ Ò Ö ÑÑ Ö ´ ÖÓÑ ÖÓÙ Ğİ ¼± ØÓ ÖÓÙÒ ¼± Ó Ø Ô Ö× Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğµ¸ Ø × ØÙ Ø ÓÒ Ñ ÛÓÖ× º Ë ÒØ Ò Ñ Ù Øİ ÓÒ× ¹ Ö Ğİ Ò Ö × º Ï Û Ö Ø Ò ÓÒĞİ Ğ ØÓ Ö ÓÚ Ö ĞĞ Ô Ö× × ÓÖ Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ × ÒØ Ò × Ø Ø Û Ö Ø ÑÓ×Ø ØÓ Ò× ĞÓÒ ´Ç× ÓÖÒ ¸ ½ µº Ï Ò ÓÛ Ú Ö¸ Ò Ù×Ù ĞĞİ Ò ÔÓĞİÒÓÑ Ğ Ø Ñ ¸ Ö ÓÚ Ö ÙÔ ØÓ ¿¼ Ô Ö× × ÓÖ × ÒØ Ò × ÙÔ ØÓ ¿¼ ØÓ Ò× ĞÓÒ Û Ò Û Ù× ÔÖÓ Ğ ×Ø ÙÒÔ Ò Ñ ¹ Ò ×Ñ ´ ÖÖÓĞĞ Ò Ö × Ó ¸ ½ ¾µº ´ÄÓÒ Ö × ÒØ Ò × Ø Ò ¿¼ ØÓ Ò× Ò Ô Ö× ¸ ÙØ Ø ÒÙÑ Ö Ó Ô Ö× × Û Ò Ö ÓÚ Ö ÓÖ Ø Ñ ÖÓÔ× Ó« Ö Ô Ğİµº½ ÀÓÛ Ú Ö¸ ¿¼ × Ö Ğ ×× Ø Ò Ø Ñ Ü ÑÙÑ ÒÙÑ Ö

½Ï Ñ

Ò ØØ ÑÔØ ØÓ Ø ÖÑ Ò Ø Ñ Ü ÑÙÑ ÒÙÑ¹

Ö Ó Ô Ö× × ÓÙÖ Ö ÑÑ Ö Ñ Ø ×× Ò ØÓ × ÒØ Ò ×º ÇÒ

¼ÅÀŞ ÍĞØÖ ËÔ Ö ¼ Û Ø ¾

Ó Ö Ğ Ñ ÑÓÖİ¸ Û Ø

Ğ Ñ Ø Ó Ø ÑÓ×Ø ½¼¼¼ Ô Ö× × Ô Ö × ÒØ Ò ¸ Ò ĞĞÓÛ Ò

ÒÓ ÑÓÖ Ø Ò ½¼¼ ÈÍ × ÓÒ × Ô Ö × ÒØ Ò ¸ Û ÓÙÒ Ø Ø

× ÒØ Ò Ñ Ù Øİ Ò Ö × ÜÔÓÒ ÒØ ĞĞİ Û Ø Ö ×Ô Ø ØÓ

× ÒØ Ò Ğ Ò Ø º Ë ÒØ Ò × Û Ø ¿¼ ØÓ Ò×

Ò ×Ø Ñ Ø

ÚÖ Ó

Ô Ö× × ´×Ø Ò Ö

Ú Ø ÓÒ ¾ ¼ µº Ï Ø ÓÙØ

Ø Ğ Ñ Ø Ó ½¼¼¼ Ô Ö× × Ô Ö × ÒØ Ò ¸ Ø × Ñ× Ğ Ğİ Ø Ø Ø ×

Ú Ö ÛÓÙĞ Ò Ö × º

Ó Ô Ö× × Ô Ö × ÒØ Ò ÓÙÖ Ö ÑÑ Ö Ñ Ø ×× Ò ØÓ Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ × ÒØ Ò ×º Òİ ØÖ Ò Ò × Ø Û
Ú ×× ØÓ Û ĞĞ Ø Ö ÓÖ Ò ×× Ö Ğİ Ğ Ñ Ø Ò×Ş º
Ï Ø Ö ÓÖ Ò Ò ×Ø Ñ Ø ÓÒ ×ØÖ Ø İ Ø Ø Ø × × Ö ÓÙ×Ğİ Ø ××Ù Ó ÜØÖ Ø Ò Ø ×Ø Ô Ö¹ ÓÖÑ Ò ÖÓÑ Ğ Ñ Ø × Ş ØÖ Ò Ò × Øº Ğ Ñ Ø × Ş ØÖ Ò Ò × Ø Ñ Ò× ÓÒ Ö Ø İ Ö ØÖ Ú Ò Ø ÑÓ×Ø Ò Ô Ö× × Ô Ö × ÒØ Ò º ĞØ ÓÙ Û ÒÒÓØ Ö ¹ ÓÚ Ö ĞĞ ÔÓ×× Ğ Ô Ö× ×¸ Û Ó Ú Ó × ØÓ Û Ô Ö× × ×Ø Ñ Ø ÓÒ × ÓÙĞ × ÙÔÓÒº
ÇÙÖ ÔÔÖÓ ØÓ Ø ÔÖÓ Ğ Ñ Ó Ñ Ò Ê Å ×¹ Ø Ñ Ø ÓÒ × Ğ ÓÖ ÓÙÖ Ğİ Ñ ÙÓÙ× × ØÓ × ÓÙØ Ò Ò ÓÖÑ Ø Ú × ÑÔĞ Ò ØÖ Ò ÙÔÓÒ Ø Øº Ï Ó ÒÓØ Ö ¬Ò Ø ×Ø Ñ Ø ÓÒ Ø × Ò ÒÓÒ¹×Ø Ò Ö Û İ¸ ÒÓÖ Ó Û Ù× ÅÓÒØ ÖĞÓ × Ñ¹ ÙĞ Ø ÓÒº
Ï ĞĞ × ÑÔĞ Ò ÓÖÑ Ø Ú Ø ÓØ Ğ × ØÓ Ø × Ğ Ø ÓÒ Ó ÑÓ Ğ Ø Ø Ó × ÒÓØ ÙÒ Ö¬Ø ÓÖ ÓÚ Ö¬Ø¸ Ò Ğ×Ó × ØİÔ Ğ Ó ÙØÙÖ × ÑÔĞ ×º ×Ô Ø ÓÒ ³× ÒØÙ Ø ÓÒ×¸ Ò Ò ÓÖÑ Ø Ú × ÑÔĞ Ñ Ø ÔÖÓÔ Ö ×Ù × Ø Ó Ø ÙĞĞ ØÖ Ò Ò × Øº Ì × Ñ Ò× Ø Ø ×Ø Ñ Ø ÓÒ Ù× Ò Ø Ò ÓÖÑ Ø Ú × ÑÔĞ Ñ Ø İ Ğ ØØ Ö Ö ×ÙĞØ× Ø Ò ×Ø Ñ Ø ÓÒ Ù× Ò ĞĞ Ó Ø ØÖ Ò Ò × Øº
Ì Ö ×Ø Ó Ø × Ô Ô Ö × × ÓĞĞÓÛ×º Ö×ØĞİ Û ÒØÖÓ Ù Ê Å×º Ì Ò Û × ÓÛ ÓÛ Ø İ Ñ İ ×Ø Ñ Ø Ò ÓÛ Ò Ò ÓÖÑ Ø Ú × ÑÔĞ Ñ Ø
ÒØ ¬ º Æ ÜØ¸ Û Ú Ø Ğ× Ó Ø ØØÖ ÙØ ¹ Ú ĞÙ Ö ÑÑ Ö Û Ù× ¸ Ò × ÓÛ ÓÛ Û Ó ÓÙØ ÑÓ ĞĞ Ò Øº Ï Ø Ò ÔÖ × ÒØ ØÛÓ × Ø× Ó ÜÔ Ö ¹ Ñ ÒØ×º Ì ¬Ö×Ø × Ø × ×Ñ ĞĞ × Ğ ¸ Ò Ö × Ò ØÓ × ÓÛ Ø Ü ×Ø Ò Ó Ò Ò ÓÖÑ Ø Ú × ÑÔĞ º Ì × ÓÒ × Ø Ó ÜÔ Ö Ñ ÒØ× Ö Ğ Ö Ö Ò × Ğ ¸ Ò Ù Ğ ÙÔÓÒ Ø ÓÑÔÙØ Ø ÓÒ Ğ × Ú Ò × Û Ö Ğ ØÓ Ú Ù× Ò ÔÖÓ Ğ ×Ø ÙÒÔ Ò ×ØÖ Ø İº Ì İ × ÓÛ ÓÛ Ğ Ö ÑÓ Ğ× ´ØÛÓ ÓÖ Ö× Ó Ñ Ò ¹ ØÙ Ğ Ö Ö Ø Ò Ø Ó× Ö ÔÓÖØ İ ÂÓ Ò×ÓÒ Ø Ğµ
Ò ×Ø Ñ Ø Ù× Ò Ø Ô Ö× Ï ĞĞ ËØÖ Ø ÂÓÙÖ¹ Ò Ğ ÓÖÔÙ×º ÇÚ Ö¬ØØ Ò × × ÓÛÒ ØÓ Ø ÔĞ º Ì İ Ğ×Ó × ÓÛ ÓÛ Ø × ÓÚ Ö¬ØØ Ò Ò ´Ô ÖØ ĞĞİµ Ö ¹ Ù İ Ù× Ò Ù×× Ò ÔÖ ÓÖº Ò ĞĞİ¸ Û Ò Û Ø ×ÓÑ ÓÑÑ ÒØ× ÓÒ ÓÙÖ ÛÓÖ º
¾ Ê Ò ÓÑ Ğ ÅÓ Ğ× À Ö Û × ÓÛ ÓÛ ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö× Ñ İ ÑÓ ĞĞ Ù× Ò Ê Å×º ĞØ ÓÙ ÓÙÖ ÓÑÑ ÒØ Öİ × Ò Ø ÖÑ× Ó Ê Å× Ò Ö ÑÑ Ö×¸ Ø × ÓÙĞ Ó ¹ Ú ÓÙ× Ø Ø Ê Å Ø ÒÓĞÓ İ Ò ÔÔĞ ØÓ ÓØ Ö ×Ø Ñ Ø ÓÒ × Ò Ö Ó×º
Ä Ø Ò ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö¸ Ø × Ø Ó × ÒØ Ò × Û Ø Ò Ø ×ØÖ Ò ¹× Ø ¬Ò İ Ä´ µ Ò ª Ø ÙÒ ÓÒ Ó Ø × Ø Ó Ô Ö× × ×× Ò ØÓ
× ÒØ Ò Ò İ Ø Ö ÑÑ Ö º Ê Ò ÓÑ Ğ ÅÓ Ğ¸ Å¸ ÓÒ× ×Ø Ó ØÛÓ ÓÑÔÓÒ ÒØ× × Ø Ó ¸ØÙÖ × Ò × Ø Ó Û ¸Ø× £º
ØÙÖ × Ö Ø × Ù Ğ Ò ĞÓ × Ó Ê Å×º

Ì İ Ò Ğ Ø ×İ×Ø Ñ × Ò Ö ØÓ ×Ô İ Ø İ

×Ô Ø× Ó Û Ø Ø Ø × ØÓ « Ö ÒØ Ø ÓÒ Ô Ö×

ÖÓÑ ÒÓØ Ö Ô Ö× º

ØÙÖ × ÙÒ Ø ÓÒ ÖÓÑ

Ô Ö× ØÓ Ò ÒØ Öº À Ö ¸ Ø ÒØ Ö Ú ĞÙ ×¹

×Ó Ø Û Ø ØÙÖ × ÒØ ÖÔÖ Ø × Ø ÒÙÑ¹

Ö Ó Ø Ñ × ØÙÖ Ñ Ø ×³ ´ × Ø Ú ³µ Û Ø

Ô Ö× º ÆÓØ ØÙÖ × × ÓÙĞ ÒÓØ ÓÒ Ù× Û Ø

ØÙÖ × × ÓÙÒ Ò ØÙÖ ¹Ú ĞÙ ÙÒ Ğ × ´Ø × Û ĞĞ

ĞĞ ØØÖ ÙØ × Ò×Ø µº ØÙÖ × Ö Ù×Ù ĞĞİ

Ñ ÒÙ ĞĞİ × Ğ Ø İ Ø ×İ×Ø Ñ × Ò Öº

Ì ÓØ Ö ÓÑÔÓÒ ÒØ Ó Ê Å¸ £¸ × × Ø Ó

Û Ø×º ÁÒ ÓÖÑ ĞĞİ¸ Û Ø× Ø ĞĞ Ù× ÓÛ ØÙÖ × Ö

ØÓ Ù× Û Ò ÑÓ ĞĞ Ò Ô Ö× ×º ÓÖ Ü ÑÔĞ ¸ Ò

Ø Ú ØÙÖ Û Ø Ğ Ö Û Ø Ñ Ø Ò Ø Ø Ø

×ÓÑ Ô Ö×

ÔÖÓ Ğ Øİº Û Ø ×

××Ó Ø Û Ø ØÙÖ º Ï Ø× Ö Ö Ğ¹Ú ĞÙ

ÒÙÑ Ö× Ò Ö ÙØÓÑ Ø ĞĞİ Ø ÖÑ Ò İ Ò ×¹

Ø Ñ Ø ÓÒ ÔÖÓ ×× ´ ÓÖ Ü ÑÔĞ Ù× Ò ÁÑÔÖÓÚ ÁØ Ö ¹

Ø Ú Ë Ğ Ò ´Ä « ÖØİ Ø Ğº¸ ½ µµº ÇÒ Ó Ø Ò

ÔÖÓÔ ÖØ × Ó Ê Å× × Ø Ø Ø Ğ Ğ ÓÓ ÙÒ Ø ÓÒ

Ó Ê Å × ×ØÖ ØĞİ ÓÒ Ú º Ì × Ñ Ò× Ø Ø Ø Ö

Ö ÒÓ ĞÓ Ğ Ñ Ò Ñ ¸ Ò ×Ó Û Ò ×ÙÖ Ø Ø

× Ğ Ò Û ĞĞ Ö ×ÙĞØ Ò ×Ø Ñ Ø ÓÒ Ó Ê Å Ø Ø ×

ĞÓ ĞĞİ ÓÔØ Ñ Ğº

Ì ´ÙÒÒÓÖÑ Ğ × µ ØÓØ Ğ Û Ø Ó Ô Ö× Ü¸

´Üµ¸ × ÙÒ Ø ÓÒ Ó Ø ØÙÖ × Ø Ø Ö Ø Ú ³

ÓÒ Ô Ö×

´Üµ ÜÔ´ ´Üµµ
½

´½µ

Ì ÔÖÓ Ğ Øİ Ó Ô Ö× ¸ È ´Ü Åµ¸ × × ÑÔĞİ Ø Ö ×ÙĞØ Ó ÒÓÖÑ Ğ × Ò Ø ØÓØ Ğ Û Ø ××Ó Ø Û Ø Ø Ø Ô Ö×

È ´Ü Åµ ½ ´Üµ

´¾µ

´İµ
İ¾ª

´¿µ

Ì ÒØ ÖÔÖ Ø Ø ÓÒ Ó Ø × ÔÖÓ Ğ Øİ Ô Ò × ÙÔÓÒ

Ø ÔÔĞ Ø ÓÒ Ó Ø Ê Åº À Ö ¸ Û Ù× Ô Ö× ÔÖÓ ¹

Ğ Ø × ØÓ Ö  Ø ÔÖ Ö Ò × ÓÖ Ô Ö× ×º

Ï Ò Ù× Ò Ê Å× ÓÖ Ô Ö× × Ğ Ø ÓÒ¸ Û × Ñ¹

ÔĞİ × Ğ Ø Ø Ô Ö× Ø Ø Ñ Ü Ñ × × ´Üµº ÁÒ Ø ×

Ö ÙÑ×Ø Ò ×¸ Ø Ö × ÒÓ Ò ØÓ ÒÓÖÑ Ğ × ´ ÓÑ¹

ÔÙØ µº Ğ×Ó¸ Û Ò ÓÑÔÙØ Ò ´Üµ ÓÖ ÓÑÔ Ø Ò

Ô Ö× ×¸ Ø Ö × ÒÓ Ù ĞØ¹ Ò × ØÓÛ Ö × × ÓÖØ Ö ´ÓÖ

ĞÓÒ Ö ×Ô

Öµ Ø

Ö ØÓ

Ú Ø ÓÒ×¸ Ò Ö Ú Ø ÓÒ Ğ

Ò×ÓØÒºÓ¾ Ò

ØÓ ÒÓÖÑ Ğ × Û Ø

¾Ì Ö ×ÓÒ Ø Ö × ÒÓ Ò ØÓ ÒÓÖÑ Ğ × Û Ø Ö ×Ô Ø ØÓ Ö Ú Ø ÓÒ Ğ Ò Ø × Ø Ø ØÙÖ × Ò Ú ÔÓ× Ø Ú ÓÖ Ò ¹ Ø Ú Û Ø×º Ì Û Ø Ó Ô Ö× Û ĞĞ Ø Ö ÓÖ ÒÓØ ĞÛ İ× ÑÓÒÓØÓÒ ĞĞİ Ò Ö × Û Ø Ö ×Ô Ø ØÓ Ø ÒÙÑ Ö Ó Ø Ú ØÙÖ ×º

¿ Ê Å ×Ø Ñ Ø ÓÒ Ò Ë Ğ Ø ÓÒ Ó Ø ÁÒ ÓÖÑ Ø Ú Ë ÑÔĞ
Ï ÒÓÛ × Ø ÓÛ Ê Å× Ñ İ ×Ø Ñ Ø Ò Ø Ò ÓÙØĞ Ò ÓÛ Û × ÓÙØ Ò Ò ÓÖÑ Ø Ú × ÑÔĞ º
Ï Ù× ÁÑÔÖÓÚ ÁØ Ö Ø Ú Ë Ğ Ò ´ÁÁËµ ØÓ ×Ø ¹ Ñ Ø Ê Å×º ÁÒ ÓÙØĞ Ò ¸ Ø ÁÁË Ğ ÓÖ Ø Ñ × × ÓĞ¹ ĞÓÛ×

½º ËØ ÖØ Û Ø Ö Ö Ò ×ØÖ ÙØ ÓÒ Ê¸ × Ø Ó ØÙÖ × Ò × Ø Ó Û Ø× £º Ä Ø Å
Ø Ê Å ¬Ò Ù× Ò Ò £º

¾º ÁÒ Ø Ğ × ĞĞ Û Ø× ØÓ Ş ÖÓº Ì × Ñ × Ø Ò Ø Ğ ÑÓ Ğ ÙÒ ÓÖÑº

¿º ÓÑÔÙØ Ø ÜÔ Ø Ø ÓÒ Ó Êº

ØÙÖ ÛºÖºØ

º ÓÖ ØÙÖ

´ µ Ò Û Ø Ø Ø ÕÙ Ø × Ø ÜÔ Ø ¹ Ø ÓÒ Ó ÛºÖºØ Ê Ò Ø ÜÔ Ø Ø ÓÒ Ó ÛºÖºØ Åº

´ µ Ê ÔĞ Ø ÓĞ Ú ĞÙ Ó Û Ø º

º Á Ø ÑÓ Ğ × ÓÒÚ Ö ØÓ Ê¸ ÓÙØÔÙØ Åº

º ÇØ ÖÛ × ¸ Ó ØÓ ×Ø Ô

Ì İ ×Ø Ô Ö × ¸ ÓÑÔÙØ Ò Ø ÜÔ Ø Ø ÓÒ×

Ó ØÙÖ × ÛºÖºØ Ø Ê Åº Ì × ÒÚÓĞÚ × Ğ ÙĞ Ø Ò

Ø ÔÖÓ Ğ Øİ Ó Ô Ö× ¸ Û ¸ × Û × Û ÖÓÑ

ÕÙ Ø ÓÒ ¾¸ Ö ÕÙ Ö × ×ÙÑÑ Ø ÓÒ ÓÚ Ö ĞĞ Ô Ö× × Ò

ªº

Ï× × ÓĞĞÓÛ×

ÓÙØ

Ò

Ò ÓÖÑ Ø Ú

× ÑÔĞ

ªØ ´ªØ

ªµ

½º È ÓÙØ ÖÓÑ ª × ÑÔĞ Ó × Ş Òº

¾º ×Ø Ñ Ø ÑÓ Ğ Ù× Ò Ø Ø × ÑÔĞ Ò Ú ĞÙ¹ Ø Øº

¿º Á Ø ÑÓ Ğ Ù×Ø ×Ø Ñ Ø × ÓÛ× × Ò× Ó ÓÚ Ö¹ ¬ØØ Ò ´Û Ø Ö ×Ô Ø ØÓ Ò ÙÒ× Ò Ğ ¹ÓÙØ Ø × Øµ¸ ĞØ Ò ÓÙØÔÙØ Ø ÑÓ Ğº

º ÇØ ÖÛ × ¸ Ò Ö × Ò Ò Ó ØÓ ×Ø Ô ½º

ÇÙÖ ÔÔÖÓ × ÑÓØ Ú Ø İ Ø ÓĞĞÓÛ Ò ´Ô Ö¹ Ø ĞĞİ Ö Ğ Ø µ Ó × ÖÚ Ø ÓÒ×

¯ Ù× Û Ù× ÒÓÒ¹Ô Ö Ñ ØÖ ÑÓ Ğ Ğ ×× Ò × Ğ Ø Ò Ò×Ø Ò Ó Ø Ò Ø ÖÑ× Ó ×ÓÑ × ÑÔĞ ´× Ø ÓÒ Ú × Ø Ğ×µ¸ ×ØÓ ×Ø ÓÑÔĞ Ü Øİ Ö ÙÑ ÒØ Ø ĞĞ× Ù× Ø Ø Ò ÓÚ ÖĞİ × Ñ¹ ÔĞ ÑÓ Ğ ´Ö ×ÙĞØ Ò ÖÓÑ ×Ñ ĞĞ × ÑÔĞ µ × Ğ Ğİ ØÓ ÙÒ Ö¬Øº Ä Û × ¸ Ò ÓÚ ÖĞİ ÓÑÔĞ Ü ÑÓ Ğ ´Ö ×ÙĞØ Ò ÖÓÑ Ğ Ö × ÑÔĞ µ × Ğ Ğİ ØÓ ÓÚ Ö¬Øº Ò Ò ÓÖÑ Ø Ú × ÑÔĞ Û ĞĞ Ø Ö ÓÖ Ö Ğ Ø ØÓ ÑÓ Ğ Ø Ø Ó × ÒÓØ ÙÒ Ö ÓÖ ÓÚ Ö¬Øº

¯ ÇÒ Ú Ö ¸ Ò Ò ÓÖÑ Ø Ú × ÑÔĞ Û ĞĞ ØİÔ¹ Ğ³ Ó ÙØÙÖ × ÑÔĞ ×º ÓÖ Ñ Òİ Ö Ğ¹Ğ × ØÙ¹
Ø ÓÒ×¸ Ø × × Ø × Ğ Ğİ ØÓ ×Ñ ĞĞ Ö Ğ Ø Ú ØÓ Ø × Ş Ó Ø ÙĞĞ ØÖ Ò Ò × Øº

Ï Ò ÓÖÔÓÖ Ø Ø ¬Ö×Ø Ó × ÖÚ Ø ÓÒ Ø ÖÓÙ ÓÙÖ

× Ö Ñ Ò ×Ñº Ù× Û ×Ø ÖØ Û Ø ×Ñ ĞĞ × Ñ¹

ÔĞ × Ò Ö Ù ĞĞİ Ò Ö × Ø Ö × Ş ¸ Û Ö Ñ Ò

Û Ø Ò Ø ÓÑ Ò Ó Æ ÒØĞİ Ö ÓÚ Ö Ğ × ÑÔĞ ×º

Ì × ÓÒ Ó × ÖÚ Ø ÓÒ × ´Ğ Ö Ğİµ Ò ÓÖÔÓÖ Ø

Ò Ø Û İ Û Ô × ÑÔĞ ×º Ì ÜÔ Ö Ñ ÒØ Ğ × ¹

Ø ÓÒ Ó Ø × Ô Ô Ö Ó × ÒØÓ Ø Ö Ğ Ú ÒØ Ø Ğ×º

ÆÓØ ÓÙÖ ØÓ Ú ĞÙ Ø

ĞĞÔ¾ÔÖªÓ

× ÔÓ××

ÙÖ ×Ø Û ÒÒÓØ Ğ ØÖ Ò Ò × Ø×º Ì

«ÓÖ ØÙ Ğ

×Ş Ø

Ó Ø Ò ÓÖÑ ÙÔÓÒ Ø ÑÓ

Ø Ú × ÑÔĞ Ğ Ğ ×× Ù×

ªØ

Û Ò

ĞĞ Ø

Ô Ò ÓØ Ñ Ü ÑÙÑ

× ÒØ Ò Ğ Ò Ø Û Ò Ğ Û Ø º Ï ÛÓÙĞ Ü¹

Ô Ø Ö Ö¸ Ğ Ü Ğ × ÑÓ Ğ× ØÓ Ü Ø ÓÚ Ö¬ØØ Ò

Û Ø ×Ñ ĞĞ Ö × ÑÔĞ × Ø Ò ÛÓÙĞ Ø × Û Ø

ÙÒĞ Ü Ğ × ÑÓ Ğ×º Ï ÛÓÙĞ ÜÔ Ø Ø × Ş Ó

Ò Ò ÓÖÑ Ø Ú × ÑÔĞ ØÓ Ò Ö × × Ø Ñ Ü ÑÙÑ

× ÒØ Ò Ğ Ò Ø Ò Ö × º

Ì Ö Ö × Ñ Ğ Ö Ø × ØÛ Ò ÓÙÖ ÔÔÖÓ Ò

Û Ø ×Ø Ñ Ø ÓÒ Ù× Ò Å Ä ´Ê ×× Ò Ò¸ ½ µº ÀÓÛ¹

Ú Ö¸ ÓÙÖ ÑÔĞ Ñ ÒØ Ø ÓÒ Ó × ÒÓØ ÜÔĞ ØĞİ ØØ ÑÔØ

ØÓ Ñ Ò Ñ × Ó Ğ Ò Ø ×º Ğ×Ó¸ Ø Ö Ö × Ñ Ğ Ö ¹

Ø × Û Ø ÑÔÓÖØ Ò × ÑÔĞ Ò ÔÔÖÓ × ØÓ Ê Å

×Ø Ñ Ø ÓÒ ´×Ù × ´ Ò Ò ÊÓ× Ò Ğ ¸ ½ µµº

ÀÓÛ Ú Ö¸ ×Ù ØØ ÑÔØ× Ó ÒÓØ Ñ Ò Ñ × ÙÒ Ö ÓÖ

ÓÚ Ö¬ØØ Ò º

Ì Ö ÑÑ Ö

Ì Ö ÑÑ Ö Û ÑÓ Ğ Û Ø Ê Ò ÓÑ Ğ ×¸ ´ ĞĞ

Ø Ì Ë ÕÙ Ò ´Ö ÑÑ Ö Ö × Ó Ò ÖÖÓĞĞ¸

½ µ¸ ÓÖ ÌË ÓÖ × ÓÖØµ Û × Ú ĞÓÔ Û Ø Ö Ö

ØÓ ÓÚ Ö ¸ Ò Û Ò ÓÑÔ Ğ ÓÒ× ×Ø× Ó

¹

Ò Ø Ğ Ù× Ö ÑÑ Ö ´ µ ÖÙĞ ×º ÁØ Ó × ÒÓØ

Ô Ö× × ÕÙ Ò × Ó ÛÓÖ × Ö ØĞİ¸ ÙØ Ò×Ø ×¹

× Ò× Ö Ú Ø ÓÒ× ØÓ × ÕÙ Ò × Ó Ô ÖØ¹Ó ¹×Ô Ø ×

´Ù× Ò Ø Ä ÏË¾ Ø × Øº Ì Ö ÑÑ Ö × Ö Ğ ¹

Ø Ú Ğİ × ĞĞÓÛ¸ ´ ÓÖ Ü ÑÔĞ ¸ Ø Ó × ÒÓØ ÙĞĞİ Ò Ğ¹

İ× ÙÒ ÓÙÒ Ô Ò Ò ×µ ÙØ Ø Ó × Ñ Ò

ØØ ÑÔØ ØÓ Ğ Û Ø ÓÑÑÓÒ ÓÒ×ØÖÙ Ø ÓÒ×¸ ×Ù ×

Ø × ÓÖ Ò Ñ ×¸ ÓÑÑÓÒĞİ ÓÙÒ Ò ÓÖÔÓÖ ¸ ÙØ Ó

Ğ ØØĞ Ø ÓÖ Ø Ğ ÒØ Ö ×Øº ÙÖØ ÖÑÓÖ ¸ Ø ÒØ Ö Ø ×

ÒØÓ Ø ×İÒØ Ü Ø ÜØ Ö ÑÑ Ö¸ ÖÓÙÔ Ò ÙØØ Ö Ò ×

ÒØÓ ÙÒ Ø× Ø Ø Ö Ù Ø ÓÚ Ö ĞĞ Ñ Ù Øİº

ÅÓ ĞĞ Ò Ø Ö ÑÑ Ö

ÅÓ ĞĞ Ò Ø ÌË Û Ø Ö ×Ô Ø ØÓ Ø Ô Ö× Ï ĞĞ

ËØÖ Ø ÂÓÙÖÒ Ğ ÓÒ× ×Ø× Ó ØÛÓ ×Ø Ô× Ö Ø ÓÒ Ó

ØÙÖ × Ø Ò ¬Ò Ø ÓÒ Ó Ø Ö Ö Ò ×ØÖ Ù¹

Ø ÓÒº

ÇÙÖ ØÙÖ × Ø × Ö Ø İ Ô Ö× Ò × ÒØ Ò × Ò

Ø ×Ø

ØÖ ÒØ

Ò Ø

Ò
Ø

×Ø
ÑÔĞ

Ø´ª×ºÌ

µ¸

Ò Ù× Ò Ø ÑÔĞ Ø

Ô Ö× ØÓ Ò¹ ¬Ò × Ñ Ğİ

Ó ØÙÖ ×º Ø ÔÖ × ÒØ¸ Ø Ø ÑÔĞ Ø × Û Ù× Ö

×ÓÑ Û Ø ¹ Ó º ÀÓÛ Ú Ö¸ Ø İ Ö ÑÓØ Ú Ø İ

Ø Ó × ÖÚ Ø ÓÒ× Ø Ø Ğ Ò Ù ×Ø ĞĞİ¹×Ø ÔÙĞ Ø ÙÒ Ø×

´ ÖÙĞ ×µ Ö Ò ÓÖÑ Ø Ú ¸ Ò Ø Ø Ñ Òİ

ÔÔĞ Ø ÓÒ× Ò ÔÖ ÖÖ Ô Ö× × Ò ÔÖ Ø Ù×¹

Ò Ğ Ü Ğ Ò ÓÖÑ Ø ÓÒº

È» ½ ÙÒ ÑÔ

½» ÔÔ½ ÙÒ ÑÔ

ÙÒ ÑÔ

ÈÈ»Ô½ İ

È½»ÔÒ½ İ

İ Æ½»Ò ØÖ Æ

ØÖ Æ

ÙÖ ½ ÌË È Ö× Ö Ñ ÒØ

Ì ¬Ö×Ø Ø ÑÔĞ Ø Ö Ø × ØÙÖ × Ø Ø ÓÙÒØ

Ø Û

Ø

ÒÙÑ Ò

ÖÓ Ô Ö×

ºØ¿ Ñ

× ÓÖ

Ò×Ø ÒØ Ø ÓÒ × ÔÖ × ÒØ Ü ÑÔĞ ¸ ×ÙÔÔÓ× Û Ô Ö×

Ø Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ È

½ ÙÒ ÑÔ İ ØÖ Æ

Ô Ö× ØÖ Ò Ö Ø İ ÌË Ñ Ø × × ÓÛÒ

Ò ¬ ÙÖ ½º À Ö ¸ ØÓ × Ú ÓÒ ×Ô ¸ Û Ú Ğ ĞĞ

ÒØ Ö ÓÖ ÒÓ Ò Ø Ô Ö× ØÖ Û Ø ÌË ÖÙĞ

Ò Ñ ×¸ Ò ÒÓØ ØØÖ ÙØ ¹Ú ĞÙ ÙÒ Ğ ×º ÙÖØ Ö¹

ÑÓÖ ¸ Û Ú ÒÒÓØ Ø

ÒÓ Û Ø Ø

ÛÓÖ Ó Ø Ô Ö × Ò ÕÙ ×Ø ÓÒº Ï Ø Ò ÓÙÖ Ö Ñ¹

Ñ Ö¸ × Ö ´Ù×Ù ĞĞİµ ÜÔĞ ØĞİ Ñ Ö º Ì ×

Ñ Ò× Û Ó ÒÓØ Ú ØÓ Ñ Òİ Ù ×× × Û Ò

ÒØ İ Ò Ø

Ó ĞÓ Ğ ØÖ º Ï Ø

Ò¹

ÓÖÑ Ø ÓÒ¸ Û Ö Ğ ØÓ Ğ Ü Ğ × ÑÓ Ğ×º Ï Ú

×ÙÔÔÖ ×× Ø Ò Ò ÓÖÑ Ø ÓÒº

ÓÖ Ü ÑÔĞ ¸ ØÙÖ ¬Ò Ù× Ò Ø × Ø ÑÔĞ Ø

Ñ Ø ÓÙÒØ Ø ÒÙÑ Ö Ó Ø Ñ × Ø Û × Û

È» ½

½» ÔÔ½ Ò Ô Ö× º ËÙ ØÙÖ × Ö ÓÖ ×ÓÑ Ó Ø ÓÒØ ÜØ Ó Ø ÖÙĞ ÔÔĞ Ø ÓÒ¸ Ò Ø Ø ÖÙĞ ÔÔĞ Ø ÓÒ× Ø Ø « Ö Ò Ø ÖÑ× Ó ÓÛ ØØÖ ÙØ × Ö ÓÙÒ Û ĞĞ ÑÓ ĞĞ İ « Ö ÒØ ØÙÖ ×º
ÇÙÖ × ÓÒ Ø ÑÔĞ Ø Ö Ø × ØÙÖ × Ø Ø Ö Ô Ö¹ Ø ĞĞİ Ğ Ü Ğ × º ÓÖ ĞÓ Ğ ØÖ ´Ó ÔØ ÓÒ µ Ø Ø × ÈÈ Ù Ø Ö¸ Û Ö Ø ØÙÖ Ø Ø ÓÙÒØ× Ø ÒÙÑ Ö Ó Ø Ñ × Ø Ø ĞÓ Ğ ØÖ ¸ ÓÖ Ø Û Ø Ø ¹ÛÓÖ Ó Ø ÈÈ¸ Û × × Ò Ò Ô Ö× º
Ò Ü ÑÔĞ Ó ×Ù Ğ Ü Ğ × ØÙÖ ÛÓÙĞ
½» ÔÔ½

ÈÈ»Ô½ İ

¿ÆÓØ ¸ ĞĞ ÓÙÖ ØÙÖ × ×ÙÔÔÖ ×× Òİ Ø ÖÑ Ò Ğ× Ø Ø ÔÔ Ö

Ò ĞÓ Ğ ØÖ º Ä Ü Ğ Ò ÓÖÑ Ø ÓÒ × Ò ĞÙ Û Ò Û

ØÓ Ğ Ü Ğ ×

ØÙÖ ×º

Ì × ØÙÖ × Ö × Ò ØÓ ÑÓ Ğ ÈÈ ØØ ¹

Ñ ÒØ× Ø Ø Ò Ö ×ÓĞÚ Ù× Ò Ø

ÓØ

ÈÈº

Ì Ø Ö Ò ¬Ò Ğ Ø ÑÔĞ Ø Ö Ø × ØÙÖ × Ø Ø

Ö Ò Ô ÖØ ĞĞİ Ğ Ü Ğ × º Ì × Ø Ñ ¸ Û Ö Ø

ĞÓ Ğ ØÖ × Ó ÔØ ÓÒ Ø Ø Ö ÓÖ Ø Û Ø Ø

ÛÓÖ º ÓÖ Ü ÑÔĞ ¸ Ö × ÓÒ ×Ù ØÙÖ

È» ½ ÙÒ ÑÔ

½» ÔÔ½

ÆÓØ Ø × ÓÒ Ò Ø Ö Ø ÑÔĞ Ø × Ö ×ÙĞØ Ò ¹ ØÙÖ × Ø Ø ÓÚ ÖĞ Ô Û Ø ØÙÖ × Ö ×ÙĞØ Ò ÖÓÑ Ô¹ ÔĞ Ø ÓÒ× Ó Ø ¬Ö×Ø Ø ÑÔĞ Ø º
Ï Ö Ø Ø Ö Ö Ò ×ØÖ ÙØ ÓÒ Ê ´ Ò ××Ó ¹ Ø ÓÒ Ó ÔÖÓ Ğ Ø × Û Ø ÌË Ô Ö× × Ó × ÒØ Ò ×¸ ×Ù Ø Ø Ø ÔÖÓ Ğ Ø × Ö  Ø Ô Ö× ÔÖ Ö Ò ×µ Ù× Ò Ø ÓĞĞÓÛ Ò ÔÖÓ ××

½º

ÜØÖ Ø ×ÓÑ × Ñ ÒØ ÓÒ Ò ×

ÑÔĞ Ø ÓÒ

¿ªµºÌ

´Ù× Ò

Ø

ÔÔÖÓ

¾º ÓÖ × ÒØ Ò Ò Ø × ÑÔĞ ¸ ÓÖ Ô Ö× Ó Ø Ø × ÒØ Ò ¸ ÓÑÔÙØ Ø ×Ø Ò ³ ¹ ØÛ Ò Ø ÌË Ô Ö× Ò Ø ÏËÂ Ö Ö Ò Ô Ö× º ÁÒ ÓÙÖ ÔÔÖÓ ¸ ×Ø Ò × Ğ ÙĞ Ø Ò Ø ÖÑ× Ó Û Ø ×ÙÑ Ó ÖÓ×× Ò Ö Ø ×¸ Ö ¹ ĞĞ Ò ÔÖ × ÓÒº Å Ò Ñ × Ò Ø Ñ Ü Ñ × × ÓÙÖ ¬Ò Ø ÓÒ Ó Ô Ö× ÔĞ Ù× Ğ Øİº ÀÓÛ Ú Ö¸ Ø Ö × ÒÓØ Ò Ò Ö ÒØĞİ ÖÙ Ğ ÓÙØ Ø × × ÓÒº Òİ ÓØ Ö Ó Ø Ú ÙÒ Ø ÓÒ ´Ø Ø Ò Ö Ô¹ Ö × ÒØ × Ò ÜÔÓÒ ÒØ Ğ ×ØÖ ÙØ ÓÒµ ÓÙĞ Ù× Ò×Ø º

¿º ÆÓÖÑ Ğ × Ø ×Ø Ò ×¸ ×Ù Ø Ø ÓÖ ×ÓÑ × Ò¹ Ø Ò ¸ Ø ×ÙÑ Ó Ø ×Ø Ò × Ó ĞĞ Ö ÓÚ¹ Ö ÌË Ô Ö× × ÓÖ Ø Ø × ÒØ Ò × ÓÒ×Ø ÒØ ÖÓ×× ĞĞ × ÒØ Ò ×º ÆÓÖÑ Ğ × Ò Ò Ø × Ñ Ò¹ Ò Ö Ò×ÙÖ × Ø Ø × ÒØ Ò × ÕÙ ÔÖÓ Ğ ´Ö Ñ Ñ Ö Ø Ø Ê Å ÔÖÓ Ğ Ø × Ö Ò Ø ÖÑ× Ó Ô Ö× ÔÖ Ö Ò ×¸ Ò ÒÓØ ÔÖÓ Ğ Øİ Ó Ó ¹ ÙÖÖ Ò Ò ×ÓÑ ÓÖÔÙ×µº

º Å Ô Ø ÒÓÖÑ Ğ × ×Ø Ò × ÒØÓ ÔÖÓ Ğ ¹ Ø ×º Á ´Ôµ × Ø ÒÓÖÑ Ğ × ×Ø Ò Ó ÌË Ô Ö× Ô¸ Ø Ò ××Ó Ø Û Ø Ô Ö× Ô Ø Ö Ö¹ Ò ÔÖÓ Ğ Øİ Ú Ò İ Ø Ñ Ü ÑÙÑ Ğ Ğ ¹ ÓÓ ×Ø Ñ ØÓÖ

È ´Ôµ Ü¾ªØ ´Üµ

´µ

ÇÙÖ ÔÔÖÓ Ø Ö ÓÖ Ú × Ô ÖØ Ğ Ö Ø ´ ÒÓÒ¹

Ş ÖÓ Ö Ö Ò Ø Ö ÓÖ ÒÓØ

ÔÖÓ Ğ Øİµ ØÓ × × ÓÒØ ÒÙÓÙ×

ĞĞ Ô ×Ø

Ö×

× ÕÙ

Ò Ú

ªĞ ØÒºØ

Ê

× ×¹

ØÖ ÙØ ÓÒ Ù× İ ÂÓ Ò×ÓÒ Ø Ğº Ï Ø Ö ÓÖ Ó ÒÓØ

Ò ØÓ Ù× × ÑÙĞ Ø ÒÒ Ğ Ò ÓÖ ÓØ Ö ÒÙÑ Ö ĞĞİ

ÒØ Ò× Ú Ø Ò ÕÙ × ØÓ ×Ø Ñ Ø ÑÓ Ğ×º

ÇÙÖ ×Ø Ò Ñ ØÖ × Ø × Ñ ÓÒ Ù× ´À ØÓ Ò¸ ½ µ

İ À ØÓ Ò

ÜÔ Ö Ñ ÒØ× À Ö Û ÔÖ × ÒØ ØÛÓ × Ø× Ó ÜÔ Ö Ñ ÒØ×º Ì ¬Ö×Ø × Ø ÑÓÒ×ØÖ Ø Ø Ü ×Ø Ò Ó Ò Ò ÓÖÑ Ø Ú × Ñ¹ ÔĞ º ÁØ Ğ×Ó × ÓÛ× ×ÓÑ Ó Ø Ö Ø Ö ×Ø × Ó Ø Ö × ÑÔĞ Ò ×ØÖ Ø ×º Ì × ÓÒ × Ø Ó ÜÔ Ö Ñ ÒØ× × Ğ Ö Ö Ò × Ğ ¸ Ò × ÓÛ Ê Å× ´ ÓØ Ğ Ü Ğ × Ò ÙÒĞ Ü Ğ × µ ×Ø Ñ Ø Ù× Ò × ÒØ Ò × ÙÔ ØÓ ¿¼ ØÓ Ò× ĞÓÒ º Ğ×Ó¸ Ø « Ø× Ó Ù×× Ò ÔÖ ÓÖ Ö ÑÓÒ×ØÖ Ø × Û İ Ó ´Ô ÖØ ĞĞİµ Ğ Ò Û Ø ÓÚ Ö¬ØØ Ò º

º½ Ì ×Ø Ò Ø Î Ö ÓÙ× Ë ÑÔĞ Ò

ËØÖ Ø

×

ÁÒ ÓÖ Ö ØÓ × ÓÛ Ú Ö ÓÙ× × Ş × Ó × ÑÔĞ Ö Ğ Ø ØÓ

×Ø Ñ Ø ÓÒ ÙÖ İ Ò Û Ø Ö Û ÓÙĞ Ú

× Ñ Ğ Ö Ğ Ú Ğ× Ó Ô Ö ÓÖÑ Ò Û Ø ÓÙØ Ö ÓÚ Ö Ò ĞĞ

ÔÓ×× Ğ Ô Ö× ×¸ Û Ö Ò Ø ÓĞĞÓÛ Ò ÜÔ Ö Ñ ÒØ×º

Ï Ù× ÑÓ Ğ ÓÒ× ×Ø Ò Ó ØÙÖ × Ø Ø Û Ö

¬Ò Ù× Ò ĞĞ Ø Ö Ø ÑÔĞ Ø ×º Ï Ğ×Ó Ø Ö Û

Û İ ĞĞ ØÙÖ × Ø Ø Ó ÙÖÖ Ğ ×× Ø Ò ØÛÓ Ø Ñ × Ò

Ø ØÖ Ò Ò × Øº Ï Ö Ò ÓÑĞİ ×ÔĞ Ø Ø Ï ĞĞ ËØÖ Ø

ÂÓÙÖÒ Ğ ÒØÓ × Ó ÒØ ØÖ Ò Ò ¸ Ğ ¹ÓÙØ Ò Ø ×Ø Ò

× Ø×º ĞĞ × ÒØ Ò × Ò Ø ØÖ Ò Ò Ò Ğ ¹ÓÙØ × Ø×

Û Ö Ø ÑÓ×Ø ½ ØÓ Ò× ĞÓÒ º Ë ÒØ Ò × Ò Ø Ø ×Ø¹

Ò × Ø Û Ö Ø ÑÓ×Ø ¿¼ ØÓ Ò× ĞÓÒ º Ì Ö Û Ö

¾ × ÒØ Ò × Ò Ø ØÖ Ò Ò × Ø¸ × ÒØ Ò × Ò

Ø Ğ ¹ÓÙØ × Ø Ò ½ × ÒØ Ò × Ò Ø Ø ×Ø Ò × Øº

Ë ÒØ Ò × Ò Ø Ğ ¹ÓÙØ × Ø ÓÒ Ú Ö ½¾

Ô Ö× ×¸ Û Ğ×Ø × ÒØ Ò × Ò Ø Ø ×Ø Ò ¹× Ø ÓÒ Ú¹

Ö ¼ Ô Ö× × Ô Ö × ÒØ Ò º

Ì Ğ ¹ÓÙØ × Ø Û × Ù× ØÓ Û ÑÓ Ğ

Ô Ö ÓÖÑ ×Øº ØÙ Ğ Ô Ö ÓÖÑ Ò Ó Ø ÑÓ Ğ×

× ÓÙĞ Ù Û Ø Ö ×Ô Ø ØÓ Ø Ø ×Ø Ò × Øº

Ú ĞÙ Ø ÓÒ Û × Ò Ø ÖÑ× Ó Ü Ø Ñ Ø ÓÖ

× ÒØ Ò Ò Ø Ø ×Ø × Ø¸ Û Û Ö ÓÙÖ× ĞÚ ×

ÔÓ ÒØ Ø Ê Å Ö Ò

×Ø Ø × Ñ Ô Ö× Ø Ø

Û ×Ö Ò

×Ø Ù× Ò Ø Ö Ö Ò ÔÖÓ Ğ Ø ×º

Ï Ò Ú ĞÙ Ø Ò Û Ø Ö ×Ô Ø ØÓ Ø Ğ ¹ÓÙØ × Ø¸

Û Ö ÓÚ Ö ĞĞ Ô Ö× × ÓÖ × ÒØ Ò × Ò Ø Ğ ¹ÓÙØ

× Øº Ï Ò Ú ĞÙ Ø Ò Û Ø Ö ×Ô Ø ØÓ Ø Ø ×Ø Ò ¹× Ø¸

Û Ö ÓÚ Ö Ø ÑÓ×Ø ½¼¼ Ô Ö× × Ô Ö × ÒØ Ò º

ÓÖ ÖÙÒ¸ Û Ö Ò ÁÁË ÓÖ Ø × Ñ ÒÙÑ Ö

Ó Ø Ö Ø ÓÒ× ´¾¼µº ÁÒ

× ¸ Û Ú ĞÙ Ø Ø

Ê Å Ø Ö ÓØ Ö Ø Ö Ø ÓÒ Ò Ö ÓÖ Ø ×Ø

Ğ ×× ¬ Ø ÓÒ Ô Ö ÓÖÑ Ò º Ì × ×Ø Ô Û × × Ò

ØÓ ÚÓ ÓÚ Ö¬ØØ Ò ×ØÓÖØ Ò ÓÙÖ Ö ×ÙĞØ×º

ÙÖ ¾ × ÓÛ× Ø Ö ×ÙĞØ× Û Ó Ø Ò Û Ø ÔÓ×¹

× Ğ Û İ× Ó Ô Ò ØİÔ Ğ³ × ÑÔĞ ×º Ì ¬Ö×Ø

ÓĞÙÑÒ × ÓÛ× Ø Ñ Ü ÑÙÑ ÒÙÑ Ö Ó Ô Ö× × Ô Ö

× ÒØ Ò × Ø Ø Û Ö ØÖ Ú Ò × ÑÔĞ º

Ì × ÓÒ ÓĞÙÑÒ × ÓÛ× Ø × Ş Ó Ø × ÑÔĞ

´ Ò Ô Ö× ×µº

Ì ÓØ Ö ÓĞÙÑÒ× Ú Ğ ×× ¬ Ø ÓÒ ÙÖ İ Ö ¹

×ÙĞØ× ´ Ô Ö ÒØ µ Û Ø Ö ×Ô Ø ØÓ Ø Ø ×Ø Ò × Øº

ÁÒ Ô Ö ÒØ × ×¸ Û Ú Ô Ö ÓÖÑ Ò Û Ø Ö ×Ô Ø ØÓ

Ø Ğ ¹ÓÙØ × Øº

Ì ÓĞÙÑÒ Ñ Ö Ê Ò × ÓÛ× Ø Ô Ö ÓÖÑ Ò

Å½¾¿ Ü Ô Ö× × Ë¾½½¾Ş¼¿¾¿¾½

Ê¿¾¿¿ººººÒ¾¾ ´´´´ ¼¿½ºººº¼¾µµµµ

¾Ë¿¼¿ºººº¿ ´´´´ ¼¿¼ºººº¼¿µµµµ

Ê¾¿¿¼ºººº ´´´´ ¼¿¼ºººº¼¼µµµµ

½½½½¼¼¼¼¼¼

¾½¿½ ½ ººº¼ ´´´ ¾ºº ºµµ¼µ ¿ººº¼¼¼´´´ ¼ºº¼¼ºµµ¼µ ¿ººº¼¼ ´´´ ¼ºº¼¼ºµµ¼µ ¾ ¼¼ ¿º¼ ´ º¼µ ¿º¼ ´ º¼µ ¿º¼ ´ º¼µ

ÙÖ ¾ Ê ×ÙĞØ× Û Ø Ú Ö ÓÙ× × ÑÔĞ Ò ×ØÖ Ø ×

Ó ÖÙÒ× Ø Ø Ù× × ÑÔĞ Ø Ø ÓÒØ Ò Ô Ö× ×

Û Û Ö Ö Ò ÓÑĞİ Ò ÙÒ ÓÖÑĞİ × Ğ Ø ÓÙØ Ó

Ø × Ø Ó ĞĞ ÔÓ×× Ğ Ô Ö× ×º Ì Ğ ×× ¬ Ø ÓÒ ¹

ÙÖ İ Ö ×ÙĞØ× ÓÖ Ø × × ÑÔĞ Ö Ö Ú Ö ÓÚ Ö ½¼

ÖÙÒ×º

Ì ÓĞÙÑÒ Ñ Ö Ë × ÓÛ× Ø Ö ×ÙĞØ× Ó ¹

Ø Ò Û Ò Ù× Ò × ÑÔĞ Ø Ø ÓÒØ Ò Ô Ö× ×

Ø Ø Û Ö Ö ØÖ Ú Ù× Ò Ø ÔÖÓ Ğ ×Ø ÙÒÔ Ò

×ØÖ Ø İº Ì × ÒÓØ ÒÚÓĞÚ Ö ØÖ Ú Ò ĞĞ ÔÓ×× Ğ

Ô Ö× × ÓÖ × ÒØ Ò Ò Ø ØÖ Ò Ò × Øº Ë Ò

Ø Ö × ÒÓ Ö Ò ÓÑ ÓÑÔÓÒ ÒØ¸ Ø Ö ×ÙĞØ× Ö ÖÓÑ

× Ò Ğ ÖÙÒº À Ö ¸ Ô Ö× × Û Ö Ö Ò Ù× Ò ×ØÓ ×¹

Ø ÓÒØ ÜØ Ö

ÓÒ ÔÔÖÓÜ Ñ Ø ÓÒ Ó ÌË º È ¹

Ö Ñ Ø Ö× Û Ö ×Ø Ñ Ø Ù× Ò × ÑÔĞ ÓÙÒØ Ò º

Ò ĞĞİ¸ Ø ÓĞÙÑÒ Ñ Ö Ê × ÓÛ× Ø Ö ¹

×ÙĞØ× Ó Ø Ò Û Ò Ù× Ò × ÑÔĞ Ø Ø ÓÒØ Ò

Ø ÓÚ Ö ĞĞ Ò¹ ×Ø Ô Ö× × Ô Ö × ÒØ Ò ¸ × ¬Ò Ò

Ø ÖÑ× Ó Ø Ö Ö Ò ×ØÖ ÙØ ÓÒº

× × Ğ Ò ¸ ÑÓ Ğ ÓÒØ Ò Ò Ö Ò ÓÑĞİ ×¹

× Ò Û Ø× ÔÖÓ Ù Ğ ×× ¬ Ø ÓÒ ÙÖ İ Ó

± ÓÒ Ø Ğ ¹ÓÙØ × ÒØ Ò ×º Ì × Ö ×ÙĞØ× Û Ö

Ú Ö ÓÚ Ö ½¼ ÖÙÒ×º

× Ò × Ò¸ Ò Ö × Ò Ø × ÑÔĞ × Ş ÔÖÓ¹

Ù × ØØ Ö Ö ×ÙĞØ× ´ ÓÖ × ÑÔĞ Ò ×ØÖ Ø İµº

ÖÓÙÒ × ÑÔĞ × Ş Ó ¼ Ô Ö× ×¸ ÓÚ Ö¬ØØ Ò ×Ø ÖØ×

ØÓ Ñ Ò ×Ø¸ Ò Ô Ö ÓÖÑ Ò ÓØØÓÑ×¹ÓÙØº ÇÒ Ó

Ø × × Ø Ö ÓÖ ÓÙÖ Ò ÓÖÑ Ø Ú × ÑÔĞ º ÆÓØ Ø Ø

Ø ×Ø × ÑÔĞ ´ ¼ Ô Ö× ×µ × Ğ ×× Ø Ò ¾¼± Ó Ø

ØÓØ Ğ ÔÓ×× Ğ ØÖ Ò Ò × Øº

Ì « Ö Ò ØÛ Ò Ø Ú Ö ÓÙ× × ÑÔĞ Ö× ×

Ñ Ö Ò Ğ¸ Û Ø ×Ğ Ø ÔÖ Ö Ò ÓÖ Ê Ò º ÀÓÛ¹

Ú Ö Ø Ø Ø Ø Ë × ÑÔĞ Ò × Ñ× ØÓ Ó ĞÑÓ×Ø

× Û ĞĞ × Ê Ò × ÑÔĞ Ò ¸ Ò ÙÖØ ÖÑÓÖ Ó × ÒÓØ

Ö ÕÙ Ö ÙÒÔ Ò ĞĞ Ô Ö× ×¸ Ñ × Ø Ø × ÑÔĞ Ò

×ØÖ Ø İ Ó Ó º

Ë × ÑÔĞ Ò × × Ò Ø × Ò× Ø Ø Ø

× ÑÔĞ ÔÖÓ Ù Ù× Ò Ø Û ĞĞ Ø Ò ØÓ ÓÒ ÒØÖ Ø

ÖÓÙÒ Ø Ó× Ô Ö× × Ø Ø Ö ĞĞ ĞÓ× ØÓ Ø ×Ø

Ô Ö× ×º Ê Ò × ÑÔĞ Ò × ÙÒ × ¸ Ò ¸ Ô ÖØ

ÖÓÑ Ø ÔÖ Ø Ğ ÔÖÓ Ğ Ñ× Ó Ú Ò ØÓ Ö ÓÚ Ö ĞĞ

Ô Ö× ×¸ Ñ Ø Ò ×ÓÑ Ö ÙÑ×Ø Ò × ØØ Ö Ø Ò

Ë × ÑÔĞ Ò º Ø Ø Ø Ñ Ó ÛÖ Ø Ò Ø × Ô Ô Ö¸

Ø Û × ÙÒ Ğ Ö Û Ø Ö Û ÓÙĞ ÓÑ Ò Ë Û Ø

Ê Ò × ÑÔĞ Ò ¹× ÑÔĞ Ô Ö× × ÖÓÑ Ø ÙĞĞ ×ØÖ Ù¹

Ø ÓÒ Û Ø ÓÙØ ÙÒÔ Ò ĞĞ Ô Ö× ×º Ï ×Ù×Ô Ø Ø Ø ÓÖ ÔÖÓ Ğ ×Ø ÙÒÔ Ò ØÓ Æ ÒØ¸ Ø ÑÙ×Ø Ö Ğİ ÙÔÓÒ ×ÓÑ ÒÓÒ¹ÙÒ ÓÖÑ ×ØÖ ÙØ ÓÒº ÍÒÔ ¹ Ò Ö Ò ÓÑĞİ Ò ÙÒ ÓÖÑĞİ ÛÓÙĞ ÔÖÓ Ğİ Ö ×ÙĞØ Ò Ğ Ö ĞÓ×× Ò ÓÑÔÙØ Ø ÓÒ Ğ Æ Ò İº

º¾ Ä Ö Ö Ë Ğ Ú ĞÙ Ø ÓÒ

À Ö Û × ÓÛ Ö ×ÙĞØ× Ù× Ò Ğ Ö Ö × ÑÔĞ Ò Ø ×Ø¹

Ò × Øº Ï Ğ×Ó × ÓÛ Ø « Ø× Ó Ğ Ü Ğ × Ø ÓÒ¸

ÓÚ Ö¬ØØ Ò ¸ Ò ÓÚ Ö¬ØØ Ò ÚÓ Ò Ù× Ò Ù×¹

× Ò ÔÖ ÓÖº ËØÖ ØĞİ ×Ô Ò Ø × × Ø ÓÒ ÓÙĞ Ú

Ò ÓÑ ØØ ÖÓÑ Ø Ô Ô Öº ÀÓÛ Ú Ö¸ ÓÒ Ú Û×

×Ø Ñ Ø ÓÒ Ù× Ò Ò Ò ÓÖÑ Ø Ú × ÑÔĞ × ÓÚ Ö¬Ø¹

Ø Ò ÚÓ Ò ¸ Ø Ò ×Ø Ñ Ø ÓÒ Ù× Ò Ù×× Ò

ÔÖ ÓÖ Ò × Ò × ÒÓØ Ö¸ ÓÑÔĞ Ñ ÒØ Öİ Ø

ÓÒ Ø ÔÖÓ Ğ Ñº

Ì ÜÔ Ö Ñ ÒØ Ğ × ØÙÔ Û × × ÓĞĞÓÛ×º Ï Ö Ò¹

ÓÑĞİ ×ÔĞ Ø Ø Ï ĞĞ ËØÖ Ø ÂÓÙÖÒ Ğ ÓÖÔÙ× ÒØÓ

ØÖ Ò Ò × Ø Ò Ø ×Ø Ò × Øº ÓØ × Ø× ÓÒØ Ò

× ÒØ Ò × Ø Ø Û Ö Ø ÑÓ×Ø ¿¼ ØÓ Ò× ĞÓÒ º Ï Ò

Ö Ø Ò Ø × Ø Ó Ô Ö× × Ù× ØÓ ×Ø Ñ Ø Ê Å×¸ Û

Ù× Ø Ë ÔÔÖÓ ¸ Ò Ö Ø Ò Ø ØÓÔ ¾

Ô Ö× × Ô Ö × ÒØ Ò º Ï Ø Ò Ø ØÖ Ò Ò × Ø ´ Ö × Ò

ÖÓÑ ½ ¾¼¼ × ÒØ Ò ×µ¸ Ø Ö Û Ö ¼ ¼¾¼ Ô Ö× ×º

Ì Ø ×Ø Ò × Ø ÓÒ× ×Ø Ó × ÒØ Ò ×¸ Û Ø Ò

Ú Ö Ó ¼ Ô Ö× × Ô Ö × ÒØ Ò º

Ï Ò Ú ĞÙ Ø Ò ¸ Û Ö ØÖ Ú Ø ÑÓ×Ø ½¼¼ Ô Ö× ×

Ô Ö × ÒØ Ò Ò Ø Ø ×Ø Ò × Ø Ò × ÓÖ Ø Ñ Ù× Ò

ÓÙÖ Ö Ö Ò ×ØÖ ÙØ ÓÒº × ÓÖ ¸ Û Û Ö

ÓÙÖ× ĞÚ × ÔÓ ÒØ Ø ÑÓ×Ø ÔÖÓ Ğ Ø ×Ø Ò Ô Ö×

´ Ò Ø ÖÑ× Ó Ø ÊÅ µ Ó Ò Û Ø Ø ÑÓ×Ø ÔÖÓ ¹

Ğ Ô Ö× ´ Ò Ø ÖÑ× Ó Ø Ö Ö Ò ×ØÖ ÙØ ÓÒµº ÁÒ

ĞĞ × ×¸ Û Ö Ò ÁÁË ÓÖ ½¼¼ Ø Ö Ø ÓÒ×º

ÓÖ Ø ¬Ö×Ø ÜÔ Ö Ñ ÒØ¸ Û Ù× Ù×Ø Ø ¬Ö×Ø

Ø ÑÔĞ Ø ´ ØÙÖ × Ø Ø Ö Ğ Ø ØÓ Ò×Ø ÒØ ¹

Ø ÓÒ×µ ØÓ Ö Ø ÑÓ Ğ ½ Ø × ÓÒ ÜÔ Ö Ñ ÒØ Ù×

Ø ¬Ö×Ø Ò × ÓÒ Ø ÑÔĞ Ø × ´ Ø ÓÒ Ğ ØÙÖ ×

Ö Ğ Ø Ò ØÓ ÈÈ ØØ Ñ ÒØµ ØÓ Ö Ø ÑÓ Ğ ¾º Ì

¬Ò Ğ ÜÔ Ö Ñ ÒØ Ù× ĞĞ Ø Ö Ø ÑÔĞ Ø × ´ Ø ÓÒ Ğ

ØÙÖ × Ø Ø Û Ö ¹Ğ Ü Ğ × µ ØÓ Ö Ø ÑÓ Ğ

¿º

Ì Ø Ö ÑÓ Ğ× ÓÒØ Ò ¿ ¾¿¼¸

Ò

¾ ½¾ ØÙÖ × Ö ×Ô Ø Ú Ğİ¸

× × Ğ Ò ¸ ÑÓ Ğ ÓÒØ Ò Ò Ö Ò ÓÑĞİ ×¹

× Ò Û Ø× Ú ¾¾± Ğ ×× ¬ Ø ÓÒ Ù¹

Ö İº Ì × Ö ×ÙĞØ× Û Ö Ú Ö ÓÚ Ö ½¼ ÖÙÒ×º ¹

ÙÖ ¿ × ÓÛ× Ø Ğ ×× ¬ Ø ÓÒ ÙÖ İ Ù× Ò ÑÓ Ğ×

½¸ ¾ Ò ¿º

× Ò × Ò¸ Ø Ğ Ö Ö × Ğ ÜÔ Ö Ñ ÒØ Ğ Ö ×ÙĞØ× Û Ö ØØ Ö Ø Ò Ø Ó× Ú Ù× Ò Ø ×Ñ ĞĞ Ö × ÑÔĞ × ´Ñ ÒØ ÓÒ Ò × Ø ÓÒ º½µº Ì Ö ¹ ×ÓÒ ÓÖ Ø × Û × Ù× Û Ù× ĞÓÒ Ö × ÒØ Ò ×º Ì Ò ÓÖÑ Ø Ú × ÑÔĞ Ö Ú Ğ ÖÓÑ ×Ù ØÖ Ò¹ Ò × Ø Û × Ğ Ğİ ØÓ Ğ Ö Ö ´ÑÓÖ Ö ÔÖ × ÒØ Ø Ú Ó

54 model1
model2
52 model3

50

Accuracy

48

46

44

42 0

10 20 30 40 50 60 70 80 90 100 Iterations

ÙÖ ¿ Ğ ×× ¬ Ø ÓÒ ÙÖ İ ÓÖ Ì Ö ÅÓ Ğ× ×Ø Ñ Ø Ù× Ò × ÁÁË

Accuracy

56 model1
model2 54 model3

52

50

48

46

44

42 0

10 20 30 40 50 60 70 80 90 100 Iterations

ÙÖ Ğ ×× ¬ Ø ÓÒ ÙÖ İ ÓÖ Ì Ö ÅÓ Ğ× ×Ø Ñ Ø Ù× Ò Ù×× Ò ÈÖ ÓÖ Ò ÁÁË

Ø ÔÓÔÙĞ Ø ÓÒµ Ø Ò Ø Ò ÓÖÑ Ø Ú × ÑÔĞ Ö Ú¹ Ğ ÖÓÑ ØÖ Ò Ò × Ø Ù× Ò × ÓÖØ Ö¸ Ğ ×× ×İÒØ ¹
Ø ĞĞİ ÓÑÔĞ Ü × ÒØ Ò ×º Ï Ø Ø ÙÒĞ Ü Ğ × ÑÓ Ğ¸ Û × Ğ Ö × Ò× Ó ÓÚ Ö¬ØØ Ò º ÅÓ Ğ ¾ ÓÚ Ö¬Ø× Ú Ò ÑÓÖ ×Óº ÓÖ Ö ×ÓÒ× Ø Ø Ö ÙÒ Ğ Ö¸ Û × Ø Ø Ø Ğ Ö Ö ÑÓ Ğ ¿ Ó × ÒÓØ ÔÔ Ö ØÓ Ü Ø ÓÚ Ö¬ØØ Ò º
Ï Ò ÜØ Ù× Ø Ù×× Ò ÈÖ ÓÖ Ñ Ø Ó Ó Ò Ò ÊÓ× Ò Ğ ØÓ Ö Ù ÓÚ Ö¬ØØ Ò ´ Ò
Ò ÊÓ× Ò Ğ ¸ ½ µº Ì × ÒÚÓĞÚ ÒØ Ö Ø Ò Ù×× Ò ÔÖ ÓÖ ´Û Ø Ş ÖÓ Ñ Òµ ÒØÓ ÁÁË Ò
× Ö Ò ÓÖ Ø ÑÓ Ğ Ø Ø Ñ Ü Ñ × Ø ÔÖÓ ¹ Ù Ø Ó Ø Ğ Ğ ÓÓ Ò ÔÖ ÓÖ ÔÖÓ Ğ Ø ×º ÓÖ Ø ÜÔ Ö Ñ ÒØ× Ö ÔÓÖØ Ö ¸ Û Ù× × Ò Ğ Ú Ö ¹ Ò ÓÚ Ö Ø ÒØ Ö ÑÓ Ğ ´ ØØ Ö Ö ×ÙĞØ× Ñ Ø
Ú Ğ ÑÙĞØ ÔĞ Ú Ö Ò × Û Ö Ù× ¸ Ô Ö Ô× Û Ø ÓÒ Ú Ö Ò Ô Ö Ø ÑÔĞ Ø ØİÔ µº Ì ØÙ Ğ Ú ĞÙ Ó Ø Ú Ö Ò Û × ÓÙÒ İ ØÖ Ğ¹ Ò ¹ ÖÖÓÖº ÀÓÛ Ú Ö¸ ÓÔØ Ñ × Ø ÓÒ Ù× Ò Ğ ¹ÓÙØ × Ø × ×İ ØÓ Ú º

Ï Ö Ô Ø Ø Ğ Ö ¹× Ğ ÜÔ Ö Ñ ÒØ¸ ÙØ Ø × Ø Ñ Ù× Ò Ù×× Ò ÔÖ ÓÖº ÙÖ × ÓÛ× Ø Ğ ×× ¬ Ø ÓÒ ÙÖ İ Ó Ø ÑÓ Ğ× Û Ò Ù× Ò
Ù×× Ò ÈÖ ÓÖº Ï Ò Û Ù× Ù×× Ò ÔÖ ÓÖ¸ Û ÓÙÒ Ø Ø ĞĞ ÑÓ Ğ× × ÓÛ × Ò× Ó ÑÔÖÓÚ Ñ ÒØ ´ ĞĞ Ø Û Ø Ú Öİ Ò Ö ×µ Ô Ö ÓÖÑ Ò Ø Ö Ò Ö × ¸ ÓÖ Ğ× ÒÓØ Ö × Û Ø Ö ×Ô Ø ØÓ Ø ÒÙÑ Ö Ó Ø Ö Ø ÓÒ×º ËØ ĞĞ¸ ÑÓ Ğ ¾ ÓÒØ ÒÙ ØÓ ÙÒ ÖÔ Ö¹ ÓÖÑº ÅÓ Ğ ¿ × Ñ ÑÓ×Ø Ö × ×Ø ÒØ ØÓ Ø ÔÖ ÓÖº ÁØ Ø Ö ÓÖ ÔÔ Ö× Ø Ø Ù×× Ò ÔÖ ÓÖ × ÑÓ×Ø Ù× ÙĞ ÓÖ ÙÒĞ Ü Ğ × ÑÓ Ğ×¸ Ò Ø Ø ÓÖ ÑÓ ¹ Ğ× Ù ĞØ ÖÓÑ ÓÑÔĞ Ü¸ ÓÚ ÖĞ ÔÔ Ò ØÙÖ ×¸ ÓØ Ö ÓÖÑ× Ó ×ÑÓÓØ Ò ÑÙ×Ø Ù× Ò×Ø º
ÓÑÑ ÒØ× Ï Ö Ù Ø Ø Ê Å ×Ø Ñ Ø ÓÒ ÓÖ ÖÓ ¹ ÓÚ Ö ØØÖ ÙØ ¹Ú ĞÙ Ö ÑÑ Ö× ÓÙĞ Ñ ÓÑÔÙ¹ Ø Ø ÓÒ ĞĞİ ØÖ Ø Ğ İ ØÖ Ò Ò ÙÔÓÒ Ò Ò ÓÖÑ ¹ Ø Ú × ÑÔĞ º ÇÙÖ ×Ñ ĞĞ¹× Ğ ÜÔ Ö Ñ ÒØ× ×Ù ×Ø Ø Ø Ù× Ò Ø Ó× Ô Ö× × Ø Ø ÓÙĞ Æ ÒØĞİ ÙÒ¹ Ô ´Ë × ÑÔĞ Ò µ Û × ĞÑÓ×Ø × « Ø Ú × × ÑÔĞ Ò ÖÓÑ ĞĞ ÔÓ×× Ğ Ô Ö× × ´Ê Ò × ÑÔĞ Ò µº
Ğ×Ó¸ Û × Û Ø Ø ÑÓ Ğ× × ÓÙĞ ÒÓØ ÓØ Ù ĞØ Ò Ğ×Ó ×Ø Ñ Ø Ù× Ò ĞĞ ÔÓ×× Ğ Ô Ö× ×º ØØ Ö Ö ×ÙĞØ× Ò Ó Ø Ò Û Ò ÑÓ Ğ× Ö Ù ĞØ Ò ØÖ Ò Ù× Ò Ò Ò ÓÖÑ Ø Ú × ÑÔĞ º
Ú Ò Ø Ö Ğ Ø ÓÒ× Ô ØÛ Ò × ÑÔĞ × Ş Ò ÑÓ Ğ ÓÑÔĞ Ü Øİ¸ Û × Ø Ø Û Ò Ø Ö × Ò¹
Ö Ó ÓÚ Ö¬ØØ Ò ¸ ÓÒ × ÓÙĞ Ù Ğ ÑÓ Ğ× ÓÒ Ø ¹ × × Ó Ò Ò ÓÖÑ Ø Ú × Øº ÀÓÛ Ú Ö¸ Ø × Ğ Ú × ÓÔ Ò Ø ÔÓ×× Ğ Øİ Ó ØÖ Ò Ò ×Ù ÑÓ Ğ ÙÔÓÒ ×Ù¹ Ô Ö× Ø Ó Ø Ò ÓÖÑ Ø Ú × Øº ĞØ ÓÙ Û Ú ÒÓØ Ø ×Ø Ø × × Ò Ö Ó¸ Û Ğ Ú Ø Ø Ø × ÛÓÙĞ Ğ ØÓ ØØ Ö Ö ×ÙĞØ× Ø Ò Ø Ó× Ú Ö º
Ì Ğ Ö Ö × Ğ ÜÔ Ö Ñ ÒØ× × ÓÛ Ø Ø Ê Å× Ò ×Ø Ñ Ø Ù× Ò Ö Ğ Ø Ú Ğİ ĞÓÒ × ÒØ Ò ×º Ì İ Ğ×Ó × ÓÛ Ø Ø × ÑÔĞ Ù×× Ò ÔÖ ÓÖ ÓÙĞ Ö Ù Ø « Ø× Ó ÓÚ Ö¬ØØ Ò º ÀÓÛ Ú Ö¸ Ø İ Ğ×Ó × ÓÛ Ø Ø Ü ×× Ú ÓÚ Ö¬ØØ Ò ÔÖÓ Ğİ Ö ÕÙ Ö Ò ĞØ ÖÒ Ø Ú ×ÑÓÓØ Ò ÔÔÖÓ º Ì ×Ñ ĞĞ Ö Ò Ğ Ö Ö ÜÔ Ö Ñ ÒØ× Ò ÓØ Ú Û × ´ ÓÑÔĞ Ñ ÒØ Öİµ Û İ× Ó Ğ Ò Û Ø ÓÚ Ö¬ØØ Ò º Ï ÓÒ ØÙÖ Ø Ø Ó Ø ØÛÓ Ô¹ ÔÖÓ ×¸ Ø Ò ÓÖÑ Ø Ú × ÑÔĞ ÔÔÖÓ × ÔÖ Ö¹ Ğ × Ø Ğ× Û Ø ÓÚ Ö¬ØØ Ò Ö ØĞİ ÓÚ Ö¬ØØ Ò Ö ×ÙĞØ× ÖÓÑ ¬ØØ Ò ØÓ ÓÑÔĞ Ü ÑÓ Ğ Û Ø ØÓÓ Ğ Ø¹ ØĞ Ø º ÇÙÖ ÓÒ Ó Ò Ö × Ö Û ĞĞ ÓÒ ÒØÖ Ø ÙÔÓÒ ×ØÖÓÒ Ö Û İ× Ó Ğ Ò Û Ø ÓÚ Ö¬ØØ Ò Ò Ğ Ü ¹ Ğ × Ê Å×º ÇÒ Ğ Ò Û Ö ÔÙÖ×Ù Ò × ØÓ ÓÑ¹ Ò ÓÑÔÖ ×× ÓÒ¹ × ÔÖ ÓÖ Û Ø Ò ÜÔÓÒ ÒØ Ğ ÑÓ Ğº Ì × Ğ Ò × Å Ä Û Ø Å Ü ÑÙÑ ÒØÖÓÔİº Ï Ö Ğ×Ó ĞÓÓ Ò Ø ĞØ ÖÒ Ø Ú Ø ÑÔĞ Ø × Ø×º ÓÖ Ü ÑÔĞ ¸ Û ÛÓÙĞ ÔÖÓ Ğİ Ò ¬Ø ÖÓÑ Ù× Ò Ø ÑÔĞ Ø × Ø Ø ÔØÙÖ ÑÓÖ Ó Ø ×İÒØ Ø ÓÒØ ÜØ Ó ÖÙĞ Ò×Ø ÒØ Ø ÓÒº

ÒÓÛĞ Ñ ÒØ× Ï ÛÓÙĞ Ğ ØÓ Ø Ò ÊÓ Å ĞÓÙ ¸ ÓÒÒĞ Æ
Ö ĞØ Ò Ø ÒÓÒİÑÓÙ× Ö Ú Û Ö× ÓÖ ÓÑ¹ Ñ ÒØ×º Ì × ÛÓÖ Û × ×ÙÔÔÓÖØ İ Ø ÌÅÊ ÈÖÓ Ø ºÄ ÖÒ Ò ÓÑÔÙØ Ø ÓÒ Ğ Ö ÑÑ Ö×

Ê ÖÒ ×

ËØ Ú Ò Èº Ò İº ½ º ËØÓ ×Ø ØØÖ ÙØ ¹

Î ĞÙ Ö ÑÑ Ö×º ¸ÓÑÔÙØ Ø ÓÒ Ğ Ä Ò Ù ×Ø ×

¾¿´ µ ß ½ ¸ Ñ Öº

Å Ğ × Ç× ÓÖÒ ½ º ÁÒ Ù Ø ÓÒ Ù× Ò Å Ä

Ò È Ö× ÓÖÔÓÖ º ÁÒ Â Ñ × Ù×× Ò×¸ ØÓÖ¸

Ä ÖÒ Ò Ä Ò Ù ¸ ÔÒ ÄÓ × ¿ß ½¸ Ğ ¸

ËĞÓÚ Ò ¸ ÂÙÒ º

Ì Ö × Ó Ò ÂÓ Ò ÖÖÓĞĞº ½ º ÙØÓÑ Ø

ÜØÖ Ø ÓÒ Ó ËÙ Ø ÓÖ Ş Ø ÓÒ ÖÓÑ ÓÖÔÓÖ º

ÁÒ ÈÖÓ

Ò×Ó Ø

Ø ÓÒ Ö Ò ÓÒ ÔÔĞ

¸ ÔÆÄÈ × ¿ ß¿ ¿¸ Ï × Ò ØÓÒ¸ º

ÂÓ Ò ÖÖÓĞĞ Ò Ì Ö × Ó º ½ ¾º ÈÖÓ Ğ ×¹

Ø ÆÓÖÑ Ğ × Ø ÓÒ Ò ÍÒÔ Ò Ó È È Ö×

ÓÖ ×Ø× ÓÖ ÍÒ ¬ Ø ÓÒ¹ × Ö ÑÑ Ö×º ÁÒ ÈÖÓ¹

Ò×Ó Ø

Á ĞĞ ËİÑÔÓ× ÙÑ ÓÒ ÈÖÓ ¹

¸ Ô ×Ğ ×Ø

ÔÔÖÓ

× ØÓ Æ ØÙÖ Ğ Ä Ò Ù

¿¿ß¿ ¸ Ñ Ö ¸ Å º

ËØ ÒĞ İ Ò Ò ÊÓÒ Ğ ÊÓ× Ò Ğ º ½ º Æ¹

ÒØ Ë ÑÔĞ Ò Ò ØÙÖ Ë Ğ Ø ÓÒ Ò Ï ÓĞ

Ë ÒØ Ò Å Ü ÑÙÑ ÒØÖÓÔİ Ä Ò Ù ÅÓ Ğ×º ÁÒ

ºÁ ËËÈ³

ËØ ÒĞ İ º Ò Ò ÊÓÒ Ğ ÊÓ× Ò Ğ º ½ º

Ù×× Ò ÈÖ ÓÖ ÓÖ ËÑÓÓØ Ò Å Ü ÑÙÑ Ò¹

ØÖÓÔİ ÅÓ Ğ×º Ì Ò Ğ Ê ÔÓÖØ ÅÍ¹ Ë¹ ¹½¼ ¸

ÖÒ Å ĞĞÓÒ ÍÒ Ú Ö× Øİº

Ö À ØÓ Òº ½ º ÈÖÓ Ğ ×Ø È Ö× Ë Ğ Ø ÓÒ

× ÓÒ Ë Ñ ÒØ ÓÓ ÙÖÖ Ò ×º ÁÒ ÈÖÓ ¹

Ò ×Ó Ø

Ø ÁÒØ ÖÒ Ø ÓÒ Ğ ÏÓÖ× ÓÔ ÓÒ È Ö× Ò

Ì ÒÓĞÓ ×¸

ÑÖ

¸ Ô ×¸ Å ×× Ù× ØØ×

½½¿ß½¾¾º

Å Ö ÂÓ Ò×ÓÒ¸ ËØÙ ÖØ Ñ Ò¸ ËØ Ô Ò ÒÒÓÒ¸

İ ¸ Ò ËØ Ô Ò Ê ŞĞ Öº ½ º ×Ø Ñ ØÓÖ×

ÓÖ ËØÓ ×Ø ÍÒ ¬ Ø ÓÒ¹ × Ö ÑÑ Ö×º ÁÒ

¿ Ø ÒÒÙ Ğ Å Ø Ò Ó Ø

Äº

Âº Ä « ÖØİ¸ Ëº ĞĞ È ØÖ ¸ Ò Îº ĞĞ È ØÖ º

½ º ÁÒ Ù Ò ØÙÖ × Ó Ê Ò ÓÑ Ğ ×º Á

ÌÖ Ò× Ø ÓÒ× ÓÒ È ØØ ÖÒ Ò Ğİ× × Ò Å

Ò

ÁÒØ ĞĞ Ò ¸ ½ ´ µ ¿ ¼ß¿ ¿¸ ÔÖ Ğº

ÂÓÖÑ Ê ×× Ò Òº ½ º ËØÓ ×Ø ÓÑÔĞ Ü Øİ Ò

ËØ Ø ×Ø ºĞ ÁÒÕÙ Öİ Ë Ö × Ò ÓÑÔÙØ Ö Ë Ò ¹

ÎÓĞÙÑ ½ º ÏÓÖĞ Ë ÒØ ¬ º

Lectures on the Foundations of HPSG
Carl Pollard Ohio State University

1

1 General character of HPSG
1.1 Similarities to Chomsky's EST GB Framework
1.1.1 Same principal goal: To characterize human linguistic competence
i.e., to construct a scienti c theory of the system of knowledge that is embodied in the human mind brain which makes language possible. This is distinct from constructing a psycholinguistic theory a theory of how the mind brain uses that knowledge to produce and interpret utterances. Knowledge of language is like a database or a system of knowledge representation; it is not a set of algorithms or procedures for processing language though such processes must have access to the system of linguistic knowledge: see below on psycholinguistic responsibility.". Part of the goal of characterizing competence is determining what all languages have in common so-called Universal Grammar".1
1.1.2 Same empirical base: acceptability judgements of speakers
However, care is taken to consider judgements only relative to speci ed classes of contexts. It is important to control for di ering abilities of native speakers to imagine contexts in which a given string becomes acceptable.
1.1.3 Multiple representations
more or less analogous to levels" of representation in EST GB. But these are simultaneous or coexisting, or parallel structures that are mutually constrained by the grammar; all are parts of a single larger structures and none is sequentially derived from another.
1.1.4 Grammaticality is determined by the interaction between the lexicon and general well-formedness principles
roughly as in GB rather than by large numbers of construction-speci c rules as in Chomsky's standard theory", classical Montague grammar, or GPSG.
1.1.5 Many EST GB concepts have HPSG analogs.
Examples inlcude -roles, indices, agreement features, and traces. But in HPSG these are all carefully formalized so that empirically vulnerable predictions can be made.
1Transformational grammar has always made a point of being concerned with limiting the range of possible variation across languages and explaining how language can be acquired. HPSG researchers consider these to be interesting long-term goals, but don't think enough is known yet to warrant positing any empirical hypotheses.

2
1.1.6 Likewise many EST GB principles have rough HPSG analogs.
Examples include binding principles A, B, and C, and constraints on extraction" such as subjacency and ECP.
1.2 Sociological di erences
1.2.1 No fearless leader"
There is no Chomsky-like gure who is always assumed to be basically on the right track no matter what s he proposes. HPSG research is normal science: the testing of hypotheses that appear plausible given accepted assumptions. The goal is not to ll in the details of a vague theory which is assumed to be basically right, but to successively replace empirical hypotheses with ones that make better predictions.
1.2.2 Receptivity to adjacent technology
HPSG was developed in an environment where some familiarity with such technical bodies of knowledge as logic, set theory, algebra, graph theory and or theoretical computer science could be assumed. Practitioners are willing to acquire a wide range of technical tools, and apply them in the interest of making their analyses and theories clearer and more precise. Consequently, they can have more con dence in their proposals.
1.3 Methodological di erences
1.3.1 Freedom from typological bias
It is not assumed that all languages are basically like English. Languages are not assumed to vary without limit, but the starting assumption is that we do not know what the range of variation is, and we can take very little for granted when we begin to try to uncover the structure of a language.
1.3.2 Nonprimacy of syntax
HPSG is not syntactocentric. There is no assumption that syntax is somehow primary, and that morphology is done in the syntax" cf. a x hopping, head movement. Phonology and semantics are not interpretations" of syntactic structures as in transformational models from Syntactic Structures to the Minimalist Program. Instead, it is assumed that we need to understand several di erent systems of linguistic knowledge, including syntax, semantics, morphology, phonology prosody, and pragmatics discourse. In HPSG theories, most grammatical principles don't involve just one of these, but instead constrain the relationship between two or more.
1.3.3 Empirical adequacy
HPSG is done bottom-up or inductively, generalizing from speci cs instead of starting with grand generalities and looking for particulars that con rm them. Thus it employs the fragment methodology: make a precise, falsi able hypothesis that accounts for a wide range of facts i.e. get the details right for a subpart of the language, then revise the hypothsis to expand coverage. Precise, empirically vulnerable generalizations with broad, determinable consequences which might not follow from very deep principles are valued over deep principles that are so vague that their empirical consequences cannot be deduced.

3
1.3.4 Psycholinguistic responsibility
Grammars, as models of competence, should nonetheless be capable in principle of being exploited by plausible psycholinguistic models. Thus, they should not be based irreducibly on computations that a language user would be incapable of carrying out. And thus there is a somewhat di erent emphasis in the subject matter: instead of a largely rhetorical concern with how linguistic knowledge is acquired, rather one is concerned with how it is organized so as to make language use possible. The language user is viewed more as an information-processing agent than as languageacquisition device. Ultimately, grammatical knowledge must be organized in such a way that it can be employed e ciently in a wide variety of tasks, including production, understanding, translation, language games, making acceptability judgements, and more. But the linguistic framework itself should be task-neutral.
1.3.5 Generativity
HPSG research takes it for granted as Chomsky originally did that the goal of a grammatical theory of language X is minimally to tell what the well-formed structures of language X are  descriptive adequacy". Thus, formal precision is necessary the current Chomskyan view is that it is premature" and therefore undesirable. In an adequate formalization, according to Pullum 1989:
1. It must be made clear what mathematical structures are used to model di erent kinds of linguistic entities. In HPSG, the mathematical structures are graphs of a certain kind called feature structures.
2. It must be determinate what the actual assertions of the theory are. They don't have to be framed in formal logic or Prolog or Lisp or C++ though sometimes it is helpful to do this. Careful English or Korean, etc. will do, as long as it is clear, speci c, and unambiguous.
3. Given a grammar G and a mathematical object O used as a model of a candidate linguistic entity a representation, there has to be a way to tell whether or not O satis es the constraints imposed by G.
1.4 Architectural di erences
There are large-scale di erences in how theories are formulated or organized.
1.4.1 Structures employed as mathematical idealizations of linguistic entities
HPSG employs di erent mathematical structures from EST GB. The mathematical structures of transformational theories are sequences of phrase-markers trees, i.e., rooted, directed, graphs satisfying the single mother condition, with nodes labelled by category symbols.

4

FIRST q nelist

q she REST
q elist

CASE q noun

q nom

HEAD

PHON

SYNSEM
q
word

LOCAL

q
synsem

q
local

BACKGROUND

CATEGORY qcat

SUBCAT

q elist

CONTENT

q
ppro

INDEX

CONTEXT

RESTR

q
ref
q
eset

GEND PERS NUM

q
fem
q
sing

q
3rd



q
neset RELN
psoa q

q
context
female
q

INSTANCE

S

NP VP

eV

S

seem NP

VP

sh to sleep

,

S

NP VP

sh V

S

seem NP

VP

t to sleep

In HPSG, the mathematical structures are feature structures: rooted, connected, directed graphs, with each arc labelled by a feature name, and each node labelled by a species name the name of a kind of linguistic object.
A rooted graph is one that has a distinguished node called the root. In a connected graph, every node is reachable from the root by a nite sequence of edges. In a directed graph, the edges are directed i.e. they are ordered pairs of nodes, not just
doubleton sets of nodes.
1.4.2 Grammars are formalized
Fully formalized, an HPSG grammar is formulated as a set of well-formedness constraints on feature structures, of which each constraint is a nonlogical axiom in a certain kind of formal language called a feature logic. In these notes, we use a slight notational variant of King's 1989 SRL. The familiar attribute-value matrices AVMs are an informal substitute for feature logic constraints. Feature structures and feature logic satisfy the criteria of formal precision stated above in subsection 1.3.5. The constraints tell which feature structures are well-formed representations of linguistic

5
entities. Technically, a grammar is a logical theory, and a well-formed structure is a model of the theory.
The feature logic is itself unconstrained in terms of what constraints it can express. It is like predicate calculus or LISP in this respect: the constraints don't come from limits on what theories the logic can express; they are a theory that the logic expresses. Thus HPSG employs an expressive formalism, not a constrained formalism.
1.4.3 Nonderivationality
HPSG is nonderivational. It employs parallel representations which are mutually constrained by the grammar. This is also true of other nontransformational frameworks, such as LFG, APG, autolexical syntax, construction grammar, and Jackendo 's 1996 programmatic Representational Modularity framework.
HPSG employs no transformations or other destructive operations to sequentially derive one structure or representation from another. Instead, the di erent representations or levels, in Ladusaw's 1988 terminology are just subparts features, actually of a single larger structure, and they are related not by destructive operations, but rather by declarative asserted constraints of the grammar. 2 Thus, the substructure reached from the root by following the PHON path is a
rough correspondent of the GB level PF; and the one reached by the path SYNSEMjLOCjCAT in a
verb corresponds roughly to the DS of the sentence headed by that verb; the substructure reached
by the path SYNSEMjLOCj CONT corresponds roughly to the GB level LF. Understood this way,
HPSG has considerably more levels than GB.
1.4.4 Structural uniformity
HPSG is fractal" structurally uniform as the parts get smaller. Every sign down to the word level not just the root clause has features corresponding inter alia to phonetic, syntactic, and semantic aspects of linguistic structures
1.4.5 No lexical insertion
Words are just the same kind of thing namely, signs as phrases; they are just small-scale constrained parallel structures. Thus, there is no distinction between terminal nodes and preterminals, and no issue as in recent Minimalist Program work of whether lexical insertion is early" or late". This is a consequence of the theory being nonderivational and fractal.
1.4.6 Locality
HPSG employs only local constraints. There are no global constraints constraining structures relative to other structures. Well-formedness is determined completely with reference to a given structure, and not via comparison with any other competing" structure as it might be in MP and OT.
2Irreducible use of transformations is hard to reconcile with psycholinguistic responsibility: to determine PF from LF, an SS must rst be determined, from which one could work back to a DS source, and ahead to derive a PF. The rst two steps require inverting transformations," which is a computational nightmare. Similarly for determining LF from PF. Although some GBists e.g. Koster 1987, Brody 1995 have argued that GB theory could just as well be reformulated without transformations, Chomsky has never been sympathetic to this position, and with the Minimalist Program, it is not really an option.

6
1.5 Technical Di erences
There are also smaller-scale di erences in mechanisms employed by the framework. Some representative examples:
Tree-con gurational notions like c-command and government have no role in HPSG, which
employs instead the traditional relation of obliqueness of grammatical relations, where
obliqueness increases from Subject to Primary Object to Secondary Object to Oblique PPs and VP, AP, and S Complements. Thus, e.g. Principle A of the binding theory requires not that an r-pronoun anaphor be coindexed with a c-commanding NP in some local domain, but rather that it be coindexed with a less oblique argument of the same head if there is one.
Noncon gurational de nitions of grammatical relations. Grammatical relations are
de ned noncon gurationally, in terms of their relative order on the lists that are the values of the VALENCE features. Thus, the subject is the unique item on the SUBJ list; the primary object is the rst item on the COMPS list, etc., rather than being sister-of-VP-and-daughterof-S, or daughter-of-VP-and-closest-sister-of-V, as in transformational theories from Aspects through GB.
Selection of subjects and speci ers. Subjects and speci ers and their detailed properties,
such as number and de niteness can be selected  subcategorized for" by lexical heads in HPSG, just as complements are. Thus subjectless sentences are just as possible as intransitive verbs, since a verb can select for no subject".
Semantic roles  -roles" are assigned to subjects directly, in the same way as to comple-
ments, not indirectly.
Likewise, either subject or object can fail to be assigned a semantic role. Among
other things, this means that raising-to-object" analyses are not ruled out. In GB, by contrast, nonthematic objects are impossible.
There is no requirement that every VP have a subject; thus, verbs that appear to take
in nitive VP complements can be treated as actually doing so, with no need for a phonologically null subject PRO or NP-trace. Control is treated as coindexing of the controller phrase with the value of the SUBJ valence feature of the VP complement.
The following di erences are directly related to the nonderivationality of HPSG.
No NP-movement and, more generally, no movement at all. Passive: Passive verbs are in the lexicon perhaps mostly derived by lexical rule. They
project a phrase according to the same constraints as active verbs do.
Raising: Structure-sharing between the NP subject or object and the member of the
SUBJ valence feature list of the VP complement takes the place of NP-movement. Thus, the analysis of raising structures di ers from the analysis of equi-structures only in that 1 raising predicates systematically have one semantic role fewer than equi predicates with the same number of syntactic arguments, and 2 raising verb select as subject or object for the same structure that the VP complement selects for its subject, while in the case of an equi verb, the subject or object is only coindexed with the complement subject. Moreover, raising to subject and raising to object are treated on a par.

7
No WH-movement: The value of the SLASH feature throughout the path from the verb
whose argument is missing" to the ller WH-element is structure-shared with 1 the LOCAL value of the ller element, and 2 the LOCAL value of the missing argument.
No head movement: Verb in ection: nite verbs are speci ed in the lexicon to select subjects bearing speci c
agreement features. Thus there is no movement between V and In in fact there is no need for a distinct In node.
Inversion: Inverted structures are at" structures, not derived from uninverted structures.
Thus there is no movement from In to Comp.
More generally, there are no null functional heads T, AgrO, AgrS, etc.. Instead the
corresponding work in HPSG is done by features.
Likewise, there are no null complementizers.

8

LEFT

LEFT q np

q det

qs RIGHT

RIGHT LEFT

qn qv

q vp RIGHT
HIGH

LEFT q np
RIGHT q+

q det qn

LOW

_
q

FRONT ROUNDED CAUSER CAUSEE
q cause

q+ q+ q index q index
BROKEN

EFFECT

q
break

2 Foundations of the HPSG Formalism
2.1 Feature structures
Feature structures are mathematical objects|graphs of a certain kind|that are used as theoretical models of structured entities of all sorts in the real world, not just linguistic entities. Thus, the diagrams above represent a high front rounded object, a constituent structure diagram, and the relation of breaking. Note: these are for illustrative purposes only. They are not being posited as serious linguistic analyses.
3

2.2 Formal Machinery
Assume that there are given, disjoint, nite sets F of feature names, and S of species names.
De nition: A feature graph is an ordered triple G = U, S, F , where: a. U is a set called the nodes of G;
b. F is a function which associates with each feature name f a partial function Ff
also written fG from U to U; and c. S is a function which associates with each species name s a subset of U; Ss is also
written sG.

9

Mathematically, a feature graph is just a sorted unary partial algebra, where S interprets the sorts species names and F interprets the operations feature names.
There is also a more primitive notion of directed graph, where all we have is nodes and directed edges with no species or features. This is de ned as follows:

De nition: A directed graph is an ordered pair G = U, ! , where: a. U is a set called the nodes of G; b. ! is a binary relation on U i.e., a subset of U  U called the edges of G. We write p ! q to mean p, q 2 !.

Often in working with graphs we single out a node which serves as a sort of home base". Technically:

De is a

nition: A pointed graph is
graph called the underlying

an ordered triple G graph of G and q0

=
2U

;

q0; U
q0 is

; ! , where
called the point

U;
of

! G.

De nition: Given a graph U; ! and two nodes p, q 2 U, we say q is accessible from p just in case p !* q. For any binary relation R, R* is the re exive transitive
closure of R.

Informally, q is accessible from p just in case there is a path  nite sequence of edges from p to q.

De nition: A pointed graph is called accessible or an apg i every node is accessible from the point. In this case, the point is called the root of the apg.

Thus, if G = q0; U; ! is an apg, then q0 is the root and every node in U is accessible from q0.
Trees are a familiar kind of apg. An apg is a tree just in case each node is accessible from the root via a unique path.

aOsbSsefrfvGat:iofn:2

Let G = Fg. Then

U

,

S,
U;

F
!F

be a feature graph, and let !F  U  U be de
is a graph.

ned

That is, given a feature graph, we can form the graph whose set of edges is the union of the interpretations of the feature names. Intuitively, this amounts to throwing away all the labels on the nodes and edges.

De nition: A feature structure is a quadruple

is a feature graph, and structure.

q0; U; !F is an apg.

Wqe0

;U;S;F ,
call q0 the

where
root of

U;
the

S;F
feature

Feature structures are what we will use as our mathematical idealizations of linguistic entities i.e. analyses of expressions.

2.3 Graph Notation
Nodes are represented by dots, and edges or arcs by arrows. Thus, in the graph ,! , if the left dot is p and the right dot is q, then p ! q. For a feature graph, an arrow labelled with a feature
name f means that fGp = q, where p is represented by the dot from which the arrow originates
!and q is represented by the dot the arrow points to. The root of a feature structure is indicated by
a short boldface arrow to a node: .

10

2.4 Feature structures generated by nodes in feature graphs
De nition: Let G = U; S; F be a feature graph and q 2 U a node. The feature structure generated by q in G; Gq, is q; Uq; Sq; Fq , where:
a. Uq = fp 2 U j q !F pg; b. for each s in S; Sqs = sG Uq; c. for each f in F; Fqf  = f G Uq  Uq. Thus the feature structure generated by a node q in a feature graph G contains only the nodes of G accessible from q; and the interpetations of species names and feature names are obtained by restricting their interpretations in G in the obvious way.

2.5 Feature structure homomorphisms and isomorphisms
Let G = Uq0; ; S; F and G0 = q00 ; U0; S0; F0 . De nition: A homomorphism from G to G0 is a function  : U ! U0 such that
a. q0 = q00 ;
b. for all q 2 U and f 2 F, if fG is de ned at q, then fG0 is de ned at q, and
fG0q = f Gq;
c. for all q 2 U and s 2 S, if q 2 sG, then q 2 sG0.

This is just like a homomorphism in algebra thinking of the interpretations of the feature names as the algebra operations, with the additional provisos that the species and the root must be preserved.

De nition: A homomorphism is called an isomorphism if it has an inverse which is
also a homomorphism. It follows that isomorphisms are both one-to-one and onto.

Thus isomorphic feature structures look the same"; they just have di erent nodes. It is easy

to see that, for any
isomorphism from G

feature structure G to G. Also, if  is a

= Uq0; ; S; F
homomorphism

, the identity function
from G to G0 and 0 is a

on U; idU, is an
homomorphism

from G0 to G00, then 0  is a homomorphism from G to G00.

In HPSG, as explained below, a grammar is a kind of logical theory whose intended interpre-

tations are feature structures. Thus the grammar picks out a certain class of feature structures,

namely the ones that are models of the grammar. Now clearly, for any feature structure, there are

in nitely many other feature structures in fact, a proper class of them isomorphic to it. However,

as linguists, when we are given a grammar, we don't really care about the whole class of models.

All we really care about is which feature structures are models up to isomorphism; e.g. the fact that

there is an in nity of di erent feature structure models of the sentence poor John ran away is not of

interest to us, since all these models are isomorphic. What we would really like is a way to pick out

from each isomorphism class of models a unique canonical representative. Then we could consider

the grammar to generate precisely the set of feature structures which are the canonical repre-

sentatives of the various isomorphism classes of models. This idea connects the model-theroetic

interpetation of grammars with the standard linguistic notion of strong generative capacity.

In fact, there is a rather simple mathematical technique for constructing such representative

feature structures; roughly speaking, the trick is to choose feature structures whose nodes are

equivalence classes of paths strings of feature names. The precise construction, which is a slight

variant of Moshier's 1988 notion of an abstract feature structure, is described in Pollard 1998.

11

2.6 Paths

A path is a member of F, i.e., it is a nite sequence string of feature names. As usual, the null

path path of length zero is denoted by . Given a feature
F : F ! U * U can be extended to a function F~ : F !

graph G U *U

= as

U; S; F
follows:

F~,

the function is de ned by

recursion on path lengths n such that:

a. if a path  is of length 0 i.e.,  =  then F~ = idU;

b. if  is of length n 0 and  = f 0 where f 2 F and 0 is a path of length n-1, then

F~ = F~0 F f .

That is, for a path of length n 0, the function it denotes is the composition of two functions: the one denoted by its nal subpath of length n-1, and the one denoted by its initial feature.

F~ is also written G. Thus

and if  = f0, then

G = idU

G = G0 f G
Also, f 1    f nG = f nG    f 1G.
2.7 King formulas
Given disjoint nite sets F feature names and S species names, the set K of King formulas
cf. King 1989 is de ned recursively as follows:
a. for each s in S, s 2 K; b. 2 K; c. for each path  and each formula 2 K;  :  2 K d. for 1 and 2 paths 1 =: 2 2 K; e. for 1 and 2 paths 1 6=: 2 2 K; f. for and  2 K;  ^  2 K; g. for and  2 K;  _  2 K; h. for and  2 K;  !  2 K; i. for and  2 K; :  2 K; j. nothing else is in K.
As described below, King formulas are used in HPSG both as descriptions of feature structures and
as constraints in grammars. To explain the di erence, we begin by de ning a satisfaction relation j= between feature structures and King formulas, analogous to the relation between rst-order
models and rst-order formulas.

12

2.8 Satisfaction
The satisfaction relation j= between a feature structure G = Uq0; ; S; F and a King formula is
de ned by structural recursion on formulas as follows:
a. G j= s i q0 2 sG; b. G j= never; c. G j=  :  i G is de ned at q0 and G j=Gq0  ; d. G j= 1 =: 2 i 1G and 2G are both de ned at q0 and have the same values; e. G j= 1 6=: 2 i 1G and 2G are both de ned at q0 and have di erent values; f. G j=  ^  i G j= and G j= ; g. G j=  _  i G j= or G j= ; h. G j=  !  i G j=6 or G j= ; i. G j= :  i G j=6 . Note: if G and G0 are isomorphic feature structures, is a King formula, and G j= , then also G0 j= . If G j= , then we say G satis es or describes G. A formula is called satis able if some
feature structure satis es it or, equivalently, if it describes some feature structure. For a set of
formulas , we say G j=  i G j= for every 2 .

2.9 Entailment and semantic equivalence
Let  be a set of formulas and a formula. Then we say  entails , written  j= , provided for all feature structures G, if G j=  then G j= . If  contains only one formula , we usually write j=  for  j= . If and  are two formulas such that j=  and  j= , then we say and  are semantically equivalent, written  . Equivalent formulas describe the same feature
structures.

2.10 Constraints and Grammars

Let be a formula and G a feature structure or more generally, a feature graph. Then we say G

models or, is a
e.g., a theory, we

model say G

of  provided,
models  if G

for every models

node q of for every

G;2Gqj=.

. For Note:

a
if G

set of formulas models , then

G j= , but the converse is in general false.

It is important to understand the relation between a description and a constraint. A formula

is called a description when we are concerned only with which feature structures satisfy it. By

contrast, a formula is called a constraint when we are concerned with which feature structures

model it.

A grammar is just a set of formulas viewed as constraints. If G is a grammar and G a feature

structure which models G, we also say G generates G, or G is well-formed relative to G. 3 We

can now consider some examples of constraints typically employed in HPSG grammars.

3Strictly speaking, the grammar generates only the models which are canonical representatives of isomorphism classes of models. See Pollard 1998.

13
2.11 Species disjointness
It is standard to assume that, in order to be well-formed, a feature structure must be such that each of its nodes belongs to only one species. We can enforce this condition by including in the
grammar, for each pair of distinct species names s and s0 the constraint :s ^ s0.
2.12 Closed World Assumption
Another standard assumption in HPSG is that there aren't any linguistic entities beyond the species
corresponding to the species names in S. This can be thought of as a version of the fragment
methodology. Another way to say this is that in order for a feature structure to be well-formed, every one of its nodes must belong to at least one and given species disjointness, therefore exactly one of the species. This can be expressed as the constraint:
_n si where S = fs1 ;    ; sng
i=1
Note: for 1;    ; n formulas, the notation _n i is a shorthand interpreted as follows:
i=1
for n=1, _n i means 1
i=1
for n 1, it means n_,1 i _ n
i=1
Thus, e.g., if n = 3, _n i is shorthand for  1 _ 2 _ 3. A similar convention applies for ^n i.
i=1 i=1
2.13 Feature Geometry Constraints
These are constraints which tell us what features di erent species have, and what species the values of those features belong to. In HPSG it is standardly assumed that for each feature, there is a set of species such that that feature is de ned for all nodes belonging to that species, but unde ned for all other nodes. Thus, for each feature f, there is a constraint of the form
f =: f $ _m ti
i=1
where each of the ti is a species. For example, in HPSG, words and phrases but nothing else have the feature phonology. Thus HPSG grammars contain the constraint4
 PHONOLOGY =: PHONOLOGY $ word _ phrase
Thus, anything that has a phonology attribute is either a word or a phrase, and every word or phrase has a phonology attribute.
Another type of feature geometry constraint tells, for a given species and a given feature de ned for that species, what species the value of that feature can be. Such constraints have the form
4As in propositional logic, $ " is shorthand for  !  ^ ! .

14

s ! f : _m ti
i=1
For example, a German HPSG grammar might include the constraint
noun ! CASE : nom _ acc _ gen _ dat.

2.14 Other constraints

Besides the feature geometry constraints, species disjointness, and the closed world assumption which are sometimes referred to collectively as ontological constraints, grammars must contain further constraints which serve to determine among those feature structures which satisfy the ontological constraints which feature structures are well-formed. These constraints do the work done in other frameworks by such devices as lexical entries, phrase-structure or immediate-dominance rules, and principles of well-formedness e.g., Subjacency, Binding Principle A, etc.. For example, it is standardly assumed in HPSG that for all headed phrases, the head features" are the same as on the head daughter. This is formally expressed by the constraint

headed-phrase ! SYNSEMjLOCjCATjHEAD =: HEAD-DTRjSYNSEMjXSLOCjCATjHEAD 

In HPSG, the lexicon is also a constraint. It has the form

word ! _N i
i=1
where each of the i is a lexical entry. That is, each lexical entry is a formula or description which presents one of the  nitely many options for a feature structure of species word to be a well-formed structural representation of a word. The lexicon, as a constraint, simply requires that every word take one of these options.
Similarly, there is a constraint on phrases of the form

!phrase _M i
i=1

where each of the i is an immediate dominance schema or phrase-type. That is, each of the

i presents one of the small number of options for a feature structure of species phrase to be a

well-formed structural representation of a phrase. Thus the as schematic immediate dominance rules such as the rules of

Xidtohtehoerysamdoe

kind of work in in GB theory.

HPSG

2.15 Formal properties of King formulas and grammars
Here we assemble some basic facts about formal properties of King formulas and grammars. For
this discussion, assume we are given a set O of ontological constraints as discussed in 11-13 above. A feature structure is called ontologically acceptable if it satis es all the ontological constraints
species disjointness, closed world, and feature geometry.
a. There is a proof theory for K which is sound and complete relative to ontological acceptability. Among other things, this means that for any two formulas and , O `  !  i every
ontologically acceptable feature structure that satis es also satis es .

15

b. Given a formula , it is decidable whether or not is satis ed by some ontologically acceptable feature structure or other. Likewise since negation is classical, it is decidable whether is valid relative to ontological acceptability i.e., whether every ontologically acceptable feature structure satis es .
c. Given a grammar G and a formula it is in general undecidable whether G predicts a feature structure which satis es or equivalently, whether there is a feature structure which models G and satis es .
d. Given a set of ontological constraints O and a formula , there is a formula 0 which is semantically equivalent to relative to O i.e., and 0 are satis ed by the same ontologically
acceptable feature structures such that 0 is a disjunction of conjunctions of formulas of one
of the three forms  : s, 1 =: 2, and 1 6=: 2. Thus, when K is being used only for
description as opposed to for constraints, no descriptive power is sacri ced by abstaining
from the use of : and !.

2.16 Attribute-value matrices AVMs

King formulas are useful when great formal precision is needed e.g., in proofs or in designing computer implementations. But they are too long and too hard to understand quickly for everyday linguistic analysis. For this reason, HPSG linguists usually use a di erent kind of description, attribute-value matrices avms, in normal practice. The general format of an avm is as follows:

a. a tag boxed numeral like 7  is an avm.

b. a species symbol e.g., word or a disjunction of species symbols e.g., word _ phrase is an avm; this kind of avm is called a sort description. Cf. section 19 below.

c. a bracketed description, including the empty description ` ', is an avm.

This

has

the

form

2
6466..p. ath1

pathn

3
AVM1 7775 AVMn

In any avm, any non-null combination of a, b, and c can be present. Thus, the following

possibilities are available:

a. just a tag: n ;

b. just a sort description: s, or s1 _    _ sn;

c. just a bracketed description;

d. both a and b, e.g., n s;

e.

both

a

and

c,

e.g.,

n

"   

16

f. both b and c, e.g.,

23
664s  775

g. all three, e.g.,

n

2664s 

3
577

Note that tags go outside the left bracket, and sort speci cations go inside the left bracket at the top or in some work, outside the left bracket at the bottom. Sort descriptions are equivalent to the identical King formulas. The lines inside a bracketed description are interpreted as equivalent to conjunction of King formulas. Co-occurrence of two or more instances of the same tag in an avm is equivalent to path equalities 1 =: 2. For example, the King formula
SUBJjHEAD:CASE: nom ^ AGR PER: 3rd ^ NUM: sing ^ HEAD: verb ^ VFORM: n ^ AUX:  ^ SUBJjHEADjAGR =: HEADjAGR

is equivalent to the avm

4666666662666666hsuebajdjh646626eavvaeufrdoxb6624racmagsre

nom
"per 1 num
3
n57777

agr 1

33
s3irndg757757777777777777

As a matter of style, when several paths share a pre x, that pre x is normally written only once for all the paths. Thus, we write:

rather than:

2
4head

"vform aux

n35

2h

i3

64head

vform
h

i

n 75

head aux

This example makes obvious the relative advantages and disadvantages of the two styles of description. Either one is satis ed by the following feature structure:

17

SUBJ
q
HEAD

AGR HEAD
qq

VFORM

CASE

q verb
AUX

q fin
q

PER
q
NUM
q nom

q 3rd q sing

AGR

There is no standard equivalent of path inequalities for avms, but one obvious way to do it is to list path inequalities as additional lines inside a bracketed description. Thus, e.g., the King
formula SUBJjINDEX 6=: OBJjINDEX would be expressed as the avm: hsubjjindex 6=: objjindexi

A less obvious but more readable alternative is the following:

4626os1ub=6bj:jjijni2nddeexx

3

1 2

577

2.17 Subsumption and uni cation of AVMs
Given two avms 1 and 2, we say that 1 subsumes 2 if the class of feature structures that 1
describes is at least as big as the class of feature structures that 2 describes. If 1 and 2 are King formulas equivalent to 1 and 2 respectively, then 1 subsumes 2 i 2 entails 1.
av3 miAs gea3qiunisivlcaealtellnet1dtatohned1u^n2ib2ec.atTtwihoounas,vomf 3s,1wwialinlthddeesqc2uriiboveralmtehnootrseKe cifneoagrrtefuocrrtemlys,utrlaauscutnu1ireascnawdthioic2nhroebfsopteh1ctaivn1edalyn.d2Ain2f
describe and no others.

2.18 Sorts and sort hierarchies
In writing HPSG grammars, certain disjunctions of species names are used repeatedly. For exam-
ple, in a German grammar, one might often have occasion to employ the disjunction nom _ acc _ gen _ dat as in the constraint at the end of Sec. 13. Similarly, the disjunction word _ phrase
recurs frequently. Frequently used disjunctions are usually abbreviated with a single symbol, so
that word _ phrase is usually abbreviated sign and nom _ acc _ gen _ dat is usually abbreviated
case. Such abbreviations make grammars much easier to read and write e.g.,  PHONOLOGY
=: PHONOLOGY $ sign, noun ! CASE: case . Of course, such abbreviations are mean-
ingless unless they are made explicit. This is typically done by means of a diagram called a sort hierarchy where a sort is just a symbol that is either a species name or an abbreviation for a disjunction of species names. Thus, the sort hierarchy
case

nom acc gen dat

18

is equivalent to de ning the symbol case" as an abbreviation for nom _ acc _ gen _ dat."
More generally, sorts can be used as abbreviations for disjunctions of sorts each of which in turn may be either a species name or an abbreviation. Thus, for example, the sort head may be used to abbreviate the disjunction of the parts of speech noun, verb, adj, prep, det, conj, deg for the sake of this example, we assume that these are all the parts of speech. But among the parts of speech we might want to group the rst four together as substantive and the last three as functional. Typically we group species or sorts together when there is some feature they have in common, or there is some constraint that applies to just them. We can do this by positing the sort hierarchy
head

substantive

functional

noun verb adj prep det deg conj Given two sorts 1 and 2 we say 1 is a subsort of 2 if it is more speci c than 2 in the sort hierarchy. Thus, e.g., prep and functional are subsorts of head. Alternatively, if 1 is a subsort of 2, we can also say 2 is a supersort of 1.
Note: for any sort hierarchy, it is always assumed that the subsorts are pairwise incompatible mutually exclusive. Thus, positing a hierarchy amounts logically to adding to the grammar a set of constraints of the form:
 $  1 _    _ n
and
: i ^ j for all i, j = 1, ... n, with i 6= j
2.19 Feature declarations
Feature declarations, employed in conjunction with sort hierarchies, are a convenient notational alternative to feature geometry constraints; feature declarations together with sort hierarchies are the ususal way to express ontological constraints informally.
For example, the constraint noun ! CASE : nom _ acc _ gen _ dat is expressed as the
feature declaration noun: case case , where the sort hierarchy headed by case is de ned as above. To take a more complex example, the sort hierarchy with feature declarations:
sign:"phonology list  synsem synsem

word phrase: "head-dtr

sign

non-head-dtrs list

is equivalent to the following set of feature geometry constraints assuming that word, phrase, and synsem, and list are species:
word _ phrase $ PHONOLOGY =: PHONOLOGY ^ SYNSEM =: SYNSEM phrase $ DTRS =: DTRS

19

word _ phrase ! PHONOLOGY: list ^ SYNSEM: synsem phrase ! HEAD-DTR: word _ phrase ^ NON-HEAD-DTRS: list

2.20 Lists
In HPSG, lists are usually introduced in the following way. We assume species names elist empty list and nelist nonempty list, and feature names first and rest, subject to the following feature geometry constraints:
nelist $ FIRST =: FIRST ^ REST =: REST nelist ! REST: elist _ nelist
Informally, we can say the same thing with a sort hierarchy and feature declarations, as follows:

list
elist nelist "first: object rest: list

where object is a sort that abbreviates the disjunction of all the species names. Thus if all the

sorts are arranged in a single hierarchy, object is at the top. The intuition is just that a list can be

either an empty list or a nonempty list, and in the latter case it has a first, which can be anything,

and a rest, which is a list.5 In avms, list descriptions are usually abbreviated by angle-bracket

notation. Thus, a, b, c abbreviates the avm:

2
666666646666666rfnieerlsissttt

2a 666666664frnieerlsissttt

2b 664fnierlisstt
rest

3
celist73577777577773757777777777777

Also, in avms, elist is often written . In practice, we usually want to work with lists all of whose members are of the same sort. In
order to do this, we would need to revise the sort hierachy to contain feature declarations along the following lines, with a new species of nonempty list nelist for each species or sort of thing
that we need lists of, as follows:

5The LISP notions LIST, NIL, CONS, CAR, and CDR correspond to list, elist, nelist, first and ,rest respectively.

list elist nelist "first: object
rest: list

20

"nfeilrisstts:ynsseymnsem



rest: elist _ nelist synsem

...

2.21 A new kind of formula: path relations
For each natural number n, we introduce a nite set of n-ary relation names. If R is an n-ary
relation name and 1;    ; n are paths, then R1;    ; n is a formula. Of course, to be useful,
we must be told what it means for a feature structure to satisfy a path relation formula. There are a number of di erent proposals about how to do this e.g. Richter in preparation, not all of which have the same consequences.

Append
Append is a ternary relation symbol. Path relation formulas employing Append are much-used in HPSG since they are used to express that one list is the concatenation of two other lists. More precisely, satisfaction for Append formulas is de ned in such a way that, for any three paths
G j ,1; 2; 3 6
= Append1; 2; 3 i
1. G j= 1 : list ^ 2 : list ^ 3 : list and
2. either
a. G j= 1 : elist ^ 2 =: 3 or b. G j= 1jfirst =: 3jfirst and G j= Append1jrest, 2; 3jrest
The two options are illustrated by the following two schematic graphs:

1 q elist

3

2 q list

6The following biconditional statement does not actually de ne what it means for a feature structure to satisfy an Append-formula, but it places a very strong constraint on possible de nitions.

21

1

REST
q

REST
q

REST
q

q elist

FIRST FIRST FIRST

q
3

FIRST FIRST FIRST

qqq

REST
q
2

REST
q

q REST

q list

Append-SYNSEMs

The ternary relation symbol Append-synsems is employed in an important constraint called the Valence Principle. Intuitively speaking, satisfaction for Append-synsems formulas is de ned in such
a way that G j= Append-synsems1; 2; 3 if 1; 2 and 3 are all lists and 3 is the concatenation
of the list of synsem values of the 1 list with the 2 list. More precisely, it is de ned in such a way that the following biconditional holds:
G j= Append-synsems1; 2; 3 i 1. G j= 1 : list ^ 2 : list ^ 3 : list and

2. either

a. G j= 1 : elist ^ 2 =: 3 or b. G j= 1 : nelist ^ 1jfirstjsynsem = 3jfirst ^ Append-synsems1jrest, 2; 3jrest

Schematically:

2 h ih ih i 3

j666421

synsem 1 , synsem 2 , synsem 3
4

7577

, ,3 1 2 3 4

The Valence Principle is the following constraint:

headed-phrase

!

22
666666666666666664666666666AAscshspyuoepprnbampp-jseedpd-enn-d-tmddddtr--tjtssrlryyrojnnscsssy3jeec1nmmas2tessjmv12aj,,llo54664c,,cssjcupo87rabmtjpjvsal

3

4 5

757

6
2
466csuobmjps spr

Append-sc synsems 3 , 6 , 9 

3

7 8 9

3775777777777777777775777777777

22

This says that, in a headed phrase, for each valence feature F, phrase, the F value of the head deaughter is the concatenation of the list of synsem values of the F-daughters with the F-value of the phrase itself.
An alternative functional" notation for AVMs employing relations is illustrated in the following expression of the Valence Principle"

2 23

66666666666666666664shcsuyoebnamjsdp-e-d-mddttjtrlrrojsscyjc1nas2tejmvajllo646ccssjcpuorabmtjpjvsal

4 5

577

6
2
646scuobmjps spr

spr-dtr 3

3

Append-synsems 1 , Append-synsems 2 , Append-synsems  3

,

546577377777777777775777777

This following conventionalized description of a sentence illustrates the e ect of the Valence Principle:

23

22

33

666466666synsem

j

loc

j

cat

head 3 66666664valence

2
46666csuobmjps spr

D D D

EEE57737777777775757777777

SH

2 NP nom Kim

22

33

666666646synsem

j

loc

j

cat

head 3 64666666valence

2
46666scuobmjps spr

D D D

2EEE37777577775777777775777

HC

2
666646666synsem

j

loc

j

cat

626666664vhaelaednc3 ev246666ercssbpuorbmjps

33

D D D

12EEE77753777577777775777777

1 NP acc

saw Sandy

24
REFERENCES
Brody, Michael. 1995. Lexico-logical form. Linguistic Inquiry Monograph. Cambridge, MA.: MIT Press. Jackendo , Ray S. 1996. The architecture of the language faculty. Cambridge, MA: MIT Press. King, Paul. 1989. A logical formalism for head-driven phrase-structure grammar. Manchester University Ph.D. Dissertation. Koster, Jan. 1987 Domains and dynasties. Dordrecht: Foris. Ladusaw, William. 1988. A proposed distinction between level and stratum. Linguistics in the morning calm 2, ed. by the Linguistic Society of Korea. Seoul: Hanshin Publishing Co. Moshier, Drew. 1988. Extensions to uni cation grammars for the description of programming languages. University of Michigan Ph.D. dissertation. Pullum, Geo rey K. 1989. Formal linguistics meets the boojum. Natural Language and Linguistic Theory 7: 137-143. Richter, Frank. In preparation. Eine formale Sprache fur HPSG und ihre Anwendung in einem Syntaxfragment des Deutschen. Ph.D. thesis, Eberhard-Karls-Universitat, Tubingen.

