284 IEEETRANSACTTONSINFORMATIOTHNEORYM,ARCH1974

We can recover j from g(j) as follows :

which gives

j, = s,j jk = jk+ 1 + gki mod 2
jn-1 = gnj + gLl

Thus

jnm2 = g,j + gi-l + gi-2,***.

n.
jk = m&k SnIj mod

2.

We know that the integers i and i', where 0 5 i < 2"-' and i' = i + 2"-l, differ in only one digit, i.e., i,, = 0, i,,' = 1, ik = ik',k = 1,2,. . a, n - 1. Hence

gl = ik + ikfl = g,i', if k 5 n - 2.

(2)

Furthermore,

while

gni + gi+ = in-1 gi' + gz-, = i,`-1 = gni + gj-l.

(3)

We have thus shown that, if we add together the first two columns of a 2"-level Gray code and copy the remaining n - 2 columns, the resulting n - 1 columns contain two identical parts. It remains to be proved that each half is a 2"-l-level Gray code. We denote the latter by G(i), 0 I i 5 2"-' - 1. Then

G,,-, = i,,-1, Gk = ik f ik+l, Oskrn-2.

The previous are identical to the expressions (2) and (3). Thus the n - 1 columns do consist of two repetitions of 2"-r-level Gray code. Now if we combine the first two columns again, we reduce each 2"-`-level Gray code into two 2n-2-level Gray codes, or, the complete array into four 2"-2-level Gray codes. This can continue until we have only m columns, which would be 2n-m repetitions of 2"`-level Gray code. We have thus derived the lemma.
REFERENCES
[l] M. Gardner, "Mathematical games," Sci. Amer., vol. 227, pp. 10&109, Aug. 1972.

Correction to "On the Error Probability for a Class of Binary Recursive Feedback Strategies"
J. PIETER M. SCHALKWIJK AND KAREL A. POST

In the above paperr, p. 499, (2) should have read

pn+ de)

(1 - Yn+JP + Yn+lq r (1 - Yn+1)[4J + (1 - dP1 + Yn+Iw =

P,(e), - ah + aP1
for e > a,

I (1 - Yn+J[%l I

(1 - xl+& + (1 - @I

+ Yn+lP + Yn+1[(1

Pnw, - a>s + aP1
for 0 < a,. (2)

Manuscript received September 14, 1973. The authors are with the Technological University, Eindhoven, The Netherlands. r J. P. M. Schalkwijk and K. A. Post, IEEE Trans. Inform. Theory, vol. IT-19, pp. 498-511, July 1973.

On the right side of p. 505, the fifth and sixth line from the bottom, the lower error exponent E- (R) is valid for the 1 output and the upper error exponent i? (R) for the 0 output.
ACKNOWLEDGMENT
The authors want to thank Dr. J. C. Tiernan for pointing out the mistake in (2).
Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate
L. R. BAHL, J. COCKE, F. JELINEK, AND J. RAVIV
Abstrucf-Tbe generalproblem of estimating the a posieriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived.
I. INTRODUCTION
The Viterbi algorithm is a maximum-likelihood decoding method which minimizes the probability of word error for convolutional codes [l 1, [2]. The algorithm does not, however, necessarily minimize the probability of symbol (or bit) error. In this correspondence we derive an optimal decoding method for linear codes which minimizes the symbol error probability.
We fhst tackle the more general problem of estimating the a posteriori probabilities (APP) of the states and transitions of a Markov source observed through a noisy discrete memoryless channel (DMC). The decoding algorithm for linear codes is then shown to be a special case of this problem.
The algorithm we derive is similar in concept lo the method of Chang and Hancock [3] for removal of intersymbol interference. Some work by Baum and Petrie [4] is also relevant to this problem. An algorithm similar to the one described in this correspondence was also developed independently by McAdam et al. [5].
II. THE GENERAL PROBLEM
Consider the transmission situation of Fig. 1. The source is assumed to be a discrete-time finite-state Markov process. The M distinct statesof the Markov source are indexed by the integer m, m = O,l,..., M - 1. The state of the source at time t is denoted by S, and its output by X,. A state sequence of the source extending from time t to t' is denoted by S,f' = &St+1,-. .,&, and the corresponding output sequence is x," = xt,xt+l,* f *,x*r.
The state transitions of the Markov source are governed by the transition probabilities
p,(m 1m') = Pr {S, = m 1St-, = m'}
and the output by the probabilities
qt(X / m',m) = Pr {X, = X 1S,-, = m'; S, = m}
where X belongs to some finite discrete alphabet.
Manuscript received July 27, 1972; revised July 23, 1973. This paper was presented at the 1972 International Symposium on Information Theory, Asilomar, Calif. January 1972.
L. R. Bahl and J. Cocke are with the IBM Thomas J. Watson Research Center, Yorktown Heights, N.Y.
F. Jelinek is with the IBM Thomas J. Watson Research Center, Yorktown Heights, N.Y., on leave from Cornell University, Ithaca, N.Y.
J. Raviv was with the IBM Thomas J. Watson Research Center., Yorktown Heights, N.Y. He is now at the IBM Scientific Center, Haifa, Israel.

CORRESPONDENCE

285

MARKOV SOURCE

Xt

DISCRETE MEMORYLESS
CHANNEL

Yt ' DECODER

Fig. 1. Schematic diagram of transmission system.

0
`-72 : (a)
t=o I 2 3 4

trellis is the corresponding APP Pr {S,- r = m'; S, = m I Yl'). The objective of the decoder is to examine Yi' and compute theseAPP.
For ease of exposition, it is simpler to derive the joint probabilities
A,(m) = Pr {S, = m; Y,`} and
o,(m',m) = Pr {S,-, = m'; St = m; Yl').
Since, for a given Yi', Pr {Y,`} is a cc,rstant, we can divide A,(m) and o,(m',m) by Pr { Yi'} (= J.,(O),which is available from the decoder) to obtain the conditional probabilities of (1) and (2). Alternatively, we can normalize A,(m) and o,(m',m) to add up to 1 to obtain the same result. We now derive a method for obtaining the probabilities I,(m) and q(m',m).
Let us define the probability functions
a,(m) = Pr {S, = m; Yi>
5 B,(m)= Pr {Yt+I I & = m>
y,(m',m) = Pr {S, = m; Y, j St-, = m'}.
Now
a,(m) = Pr {S, = m; Y,`} .Pr {Y:+, ) S, = m; Y,`?
= a,(m).Pr {I;`+r IS, = m}

= a,(m) * A(m).

(3)

(b) Fig. 2. (a) State transition diagram for 3-state Markov source. (b) Trellis
diagram for source of Fig. 2(a).

The middle equality follows from the Markov property that if S, is known, events after time t do not depend on Yi'.
Similarly,

The Markov source starts in the initial state S, = 0, and produces an output sequenceX1' ending in the terminal state S, = 0. Xi' is the input to a noisy DMC whose output is the sequence Yi' = Y,,Y,,. . .,Y,. The transition probabilities of the DMC are defined by R(. 1.) so that for all 1 5 t I z

o,(m',m) = Pr {S,- I = m'; Yi-`} * Pr {S, = m; Y, I St- 1 = m'} .Pr {Y;+i S, = m}

= kl(m'>. y,(m',m> * PAm>. Now for t = 1,2;. .,z

(4)

Pr tYl* I XI'> = fJl NYj I Xj>.
The objective of the decoderis to examine Y,' and estimate the APP of the states and transitions of the Markov source, i.e., the conditional probabilities

M-l
a,(m) = C Pr {S,-, = m'; St = m; Y,"} ? n '= o
= 3 Pr {S,-, = m'; Yi-`} . Pr {S, = m; & ) St.-, = m'}

Pr {S, = m 1 Y,"} = Pr {S, = m; Y,`}/Pr (Yi'} and

(1)

Pr {S,-, = m'; S, = m / Yl'}

= Pr {S,-, = m'; St = m; Y,"}/Pr {Y,`}. (2)

A graphical interpretation of the problem is quite useful. A time-invariant Markov source is generally representedby a state transition diagram of the type in Fig. 2(a). The nodes are the statesand the branchesrepresentthe transitions having nonzero probabilities. If we index the states with both the time index t and state index m, we get the "trellis" diagram of Fig. 2(b). The trellis diagram shows the time progression of the state sequences.For every state sequenceSIT there is a unique path through the trellis diagram, and vice versa.
If the Markov source is time variant, then we can no longer represent it by a state-transition diagram; however, it is obvious that we can construct a trellis for its state sequences.
Associated with each node in the trellis is the corresponding APP Pr {S, = m 1 Y1"} and associatedwith each branch in the

= s at- ,(m'> . h(m',m).

(5)

Again, the middle equality follows from the fact that events after time t - 1 are not influenced by Y:-' if S,-i is known. For t = 0 we have the boundary conditions

so(O) = 1, and so(m) = 0, for m # 0.

(6)

Similarly, for t = 1,2,. . ., z - 1

M-l
M m ) = JoPr {&+I = m '; V+I I St = m l

=~Pr{S,+,=m';Y,,,IS,=m)~Pr{Y~+a/S~+i=m'1

= 3 Dt+lW>. h+lhm'). The appropriate boundary conditions are
P,(O) = 1, and B,(m) = 0, for m # 0.

(7) (8)

286

IEEE TRANSACTIONS ON INFORMATION THEORY, MARCH 1974

Relations (5) and (7) show that a,(m) and b,(m) are recursively obtainable. Now

y,(m',m) = C Pr {S, = m I St-, = m'}
X
. Pr {X, = X 1S,-, = m', St = m} * Pr {G I X}

= c dm I m '>* qt(X I m ', m ) *NY, I X)

(9)

where the summation in (9) is over all possible output symbols X. We can now outline the operation of the decoder for computing
a,(m) and o&m',m). 1) so(m) and /3,(m), m = O,l, . . ., M - 1 are initialized ac-
cording to (6) and (8). 2) As soon as Y, is received, the decoder computes y$(m',m)
using (9) and at(m) using (5). The obtained values of a,(m) are stored for all t and m.
3) After the complete sequence Y1'has been received, the decoder recursively computes p,(m) using (7). When the bt(m) have been computed, they can be multiplied by the appropriate a,(m) and y,(m',m) to obtain l,(m) and a,(m',m) using (3) and (4).
We now discussthe application of this algorithm to the decoding of linear codes.

11
Fig. 3. Kate-l/2 encoder and its trellis diagram.

III. APPLICATION TO CONVOLUTIONAL CODES

Consider a binary rate ko/no convolutional encoder of overall constraint length k,v. The input to the encoder at time t is the block Z, = (it"`,it'2', . . . ,&@")) and the corresponding output is X, = (x,(l), . . .,xt("o)). Th e encoder can be implemented by k, shift registers, each of length v, and the state of the encoder is simply the contents of these registers, i.e., the v most recent input blocks. Representing the state as a kv-tuple, we have

We assume that the l,(m) have been computed as shown in the

previous section. Let A,(j) be the set of states S, such that

st(j) = 0. Note that A,(j) is not dependent on t. Then from (10)

we have

StW = it(j) > j = 1,2;+.,ko

which implies

St = (St(1),S;2),.-.,Sjkov))= (zt,zt_l,~~~,zt_v+l). (10)
By convention, the encoder starts in state So = 0. An information sequence I, r is the input to the encoder, followed by v blocks of all-zero inputs, i.e., by Z$+1 = O,O,. . .,O where t = T + v, causing the encoder to end in state S, = 0. The trellis structure of such a convolutional code is well known [2] and we assumethat the reader is familiar with it. As an example, we illustrate in Fig. 3 a rate-: code with v = 2 and its trellis diagram for z = 6. The transition probabilities p,(m I m') of the trellis are governed by the input statistics. Generally, we assume all input sequencesequally likely for t I T, and since there are 2kopossible transitions out of each state, p&m 1m') = 2-ko for each of these transitions. For t > T, only one transition is possible out of each state, and this has probability 1. The output X, is a deterministic function of the transition so that, for each transition, there is a 0 - 1 probability distribution qt(X ] m',m) over the alphabet of binary n-tuples. For timeinvariant codes qt(. I.) is independent of t. If the output sequence is sent over a DMC with symbol transition probabilities r(. I.), the derived block transition probabilities are
NY, I X,>= jnl r(y'j) I x,`j'>
where Y, = (J+(`), . . . ~~("0))is the block received by the receiver at time t. For instance, in a BSC with crossover probability pc
NY, I Xt) = CP,)~U- ~c)n-~
where d is the Hamming distance between X, and Y,. To minimize the symbol probability of error, we must determine
the most likely input digits it(j) from the received sequence Y1'.

Pr {i,"' = 0; Y17} = C l,(m). StE.4p)
Normalizing by Pr { Yl'} = a,(O) we have
Pr {i,`j' = 0 I Y,r> = a+) C A,(m). r S,E.4,(j)
We decode it(j) = 0 if Pr {it(j) = 0 ] Yl'} 2 0.5, otherwise it(j) = 1.
Sometimes it is of interest to determine the APP of the encoder output digits, i.e., Pr {x,(i) = 0 I Y,"}. One instance where such probabilities are needed is bootstrap hybrid decoding [6]. Let B,(j) be the set of transitions S,-, = m' + St = m such that the jth output digit x,(j) on that transition is 0. B,(j) is independent of t for time-invariant codes. Then
which can be normalized to give Pr {xt(j) = 0 I Y1'}. We can obtain the probability of any event that is a function of the states by summing the appropriate a,(m); likewise, the o,(m',m) can be used to obtain the probability of any event which is a function of the transitions.
Unfortunately, the algorithm requires large storage and considerable computation. All the values of a,(m) must be stored, which requires roughly 2"O. r storage locations. The storage size grows exponentially with constraint length and linearly with block length. The number of computations in determining the a,(m) (or j?,(m)) for each t are M. 2komultiplications and M additions of 2konumbers each. The computation of the y,(m',m) is quite simple and in practice is most easily accomplished by a

CORRESPONDENCE

287

110
[H 0 11

1I 0
0I

arbitrary memoryless channel. This result had previously only been known for the BSC (using syndromes and table-lookup decoding).

0 cc
3 11
Fig. 4. Parity check matrix and trellis diagram for (5,3) block code.
table lookup. For this reason it is easier to recompute the y,(m',m) in step 3) rather than to save them from step 2). Computing n,(m) requires Mmultiplications for each t and computing the APP of the input digits requires k,M/2 additions. In comparison, the Viterbi algorithm requires the calculation of a quantity essentially similar to y&`,m) with Me 2ko additions and M2ko-way compares for each t. In view of the complexity of the algorithm, it is practical only for short constraint lengths and short block lengths.
IV. APPLICATION TO BLOCK CODES The results of Section II can be applied to any code for which a coding trellis can be drawn. We now show how a trellis may be obtained for a linear block code. Let H be the parity check matrix of a linear (n,k) code, and let hi, i = 1,2,. .a,n be the column vectors of H. Let C = (Cl,CZ,* * *,c,,) be a codeword. We define the states S,, t = O,l,* * f,II pertaining to C as follows:
so = 0 and

V. COMMENTS AND GENERALIZATIONS
A brute-force approach to minimizing word or symbol error probability would work as follows: given the received sequence Yr' we could compute the APP Pr {X1' ] Yr'} for each codeword X1'. To minimize word error probability, we would pick the codeword having maximum value of Pr {X1' 1Y1'} among all codewords. To minimize the symbol error probability of the jth input digit, we compute C Pr {X1' ] Yr'}, where the sum is over all codewords havingjth input digit 0; if this sum 2 0.5, we decode the jth input digit as 0. In the case of linear codes we can avoid the calculation of Pr {X1' ] Y1"} for each possible codeword by taking advantage of the state structure of the code. The complexity of the brute-force method is proportional to the number of codewords, i.e., N 2k. In convolutional codes k = koT >> k,v which makes the trellis decoding approach attractive. In block codes, the trellis method is advantageous as long as r < k, i.e., for high-rate codes.
The algorithm derived in this correspondence cannot be considered as an attractive alternative to Viterbi decoding, because of its increased complexity. Even though Viterbi decoding is not optimal in the sense of bit error rate, in most applications of interest the performance of both algorithms would be effectively identical. The main virtue of the algorithm is in the fact that the APP of the information and channel digits are obtained, which can be useful in applications such as bootstrap decoding
M.
Many interesting generalizations of the algorithm are possible. We point out a few. First, the restriction that the starting and terminal states of the source be known can be removed by changing the initial conditions for so(m) and P,(m). Second, the algorithm can be made applicable to all finite-state channels by expanding the state-space to be the cross-product of the encoder states and the channel states. Finally, the extension to nonbinary codes is quite obvious.

St = St-1 + Cth, = k Cihiy
j=l

t = 1,2;..,n.

(11)

Obviously, S,, = 0 for all codewords and the current state S, is a function of the preceding state S,- 1 and the current input ct.
Equation (11) can be used to draw a trellis diagram for a block code with at most 2'states at each level where r = n - k. Each transition is labeled with the appropriate codeword symbol ct. As an example, a trellis for a block code with

11 1 0 1 0
H=O1l[Ol
is shown in Fig. 4. The structure of the trellis is irregular in comparison to the trellis of a convolutional code, since a block code is equivalent to a time-varying Markov source whereas a convolutional code is a stationary Markov source.
Forney (in a private communication) has pointed out that the number of states needed in the trellis can be reduced to less than 2' by rearrangement of the code bits. The interesting question of what is the minimum number of states needed is not dealt with here.
The algorithm derived here shows that any parity check code with r parity bits can be decoded with complexity N 2' on an

ACKNOWLEDGMENT
We are very grateful to Dr. G. D. Forney, Jr., for many helpful comments and suggestions. Most of the notation used in this correspondence is taken from a paper of his [7]. Also, he pointed out the isomorphism between this algorithm and the method of Chang and Hancock for intersymbol interference. We are also indebted to the other reviewer of this paper for his useful suggestions.

REFERENCES

HI

A. J. Viterbj, "Error bounds for convolutional totically optimum decoding algorithm," IEEE

codes and an asymTrans. Inform. Theory,

vol. IT-13, pp. 260-269, Apr. 1967.

121G. .D. Forney, Jr., "Review of random tree codes," NASA Ames

Research Center, Moffett Field, Calif., Appendix A of Final Report

on Contract NAS2-3637, NASA CR73176, Dec. 1967.

[31 R. W. Chang and J. C. ~ctn~g~;mory," IEEE

Hancock, "On Trans. Inform.

receiver structures Theory., vol. IT-12,

for pp.

channels 463-468,

[41 L. E Balm and T. Petrie "Statistical interference for probabilistic functions of finite state markov chains," Ann. Math. Starisr., vol. 37, pp. 1559-1563, 1966.
[51 P. L. McAdam, L. R. Welch, and C. L. Weber, "M.A.P. bit decoding of convolutional codes?" presented at the 1972 Int. Symp. Information Theory, Asilomar, Cald
161F. Jelinek and J. Cocke, "Bootstrap hybrid decoding for symmetrical binary input channels," Inform. Contr., vol. 18, pp. 261-298, Apr. 1971.
I71 G. D. Forney, Jr., "The Viterbi algorithm," Proc. IEEE, vol. 61, pp. 268-278, Mar. 1973.

