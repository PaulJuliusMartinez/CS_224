The Application of Telepresence and
Virtual Reality to Subsea Exploration

Butler P. Hine III, Carol Stoker, Michael Sims, Daryl Rasmussen, and Phil

Hontalas - NASA Ames Research Center

Terrence W. Fong, Jay Steele, and Don Barch -

Recom Technologies, Inc.

Dale Andersen - SETI Institute

Eric Miles - Stanford University

Erik Nygren - M.I.T.

Abstract

The operation of remote science exploration vehicles ben-
eﬁts greatly from the application of advanced telepresence
and virtual reality operator interfaces. Telepresence, or the
projection of the human sensory apparatus into a remote
location, can provide scientists with a much greater intui-
tive understanding of the environment in which they are
working than simple camera-display systems. Likewise
virtual reality, or the use of highly interactive three-dimen-
sional computer graphics, can both enhance an operator’s
situational awareness of an environment and also compen-
sate (to some degree) for low bandwidth and/or long time
delays in the communications channel between the opera-
tor and the vehicle. These advanced operator interfaces are
important for terrestrial science and exploration applica-
tions, and are critical for missions involving the explora-
tion of other planetary surfaces, such as on Mars. The
undersea environment provides an excellent terrestrial
analog to science exploration and operations on another
planetary surface.

Introduction

In the fall of 1993, NASA Ames deployed the Telepres-
ence Remotely Operated Vehicle (TROV) under the Ross
Sea ice near the McMurdo Science Station in Antarctica.
The goal of the mission was to operationally test the use of

telepresence and virtual reality technology in the operator
interface to the vehicle, while performing a benthic ecol-
ogy study. The vehicle was operated in two modes: (1)
locally, from above the dive hole through which it was
launched, and (2) remotely, over a satellite communica-
tions link from a control room at the NASA Ames
Research Center. Local control of the vehicle was accom-
plished using the Phantom control box containing joy-
sticks and switches, with the operator viewing stereo video
camera images on a stereo monitor.

Remote control of the vehicle over the satellite link was
accomplished using the Virtual Environment Vehicle
Interface (VEVI) control software developed at NASA
Ames. The remote operator interface included either a ste-
reo monitor (identical to that used locally) or a stereo
head-mounted head-tracked display. Signals to and from
the ﬁeld location, including compressed video, TCP/IP
data, and audio channels, were transmitted over a T1 satel-
lite link to NASA Ames Research Center. In addition to
the live stereo video from the satellite link, the operator
was able to view a computer-generated graphic representa-
tion of the underwater terrain, modeled from the vehicle’s
sensors. This virtual environment contained an animated
graphic model of the vehicle which reﬂected the state of
the actual vehicle, along with ancillary information such
as the vehicle track, science markers, and locations of
video snapshots. The actual vehicle was driven either from

the virtual environment or through a telepresence inter-
face. All vehicle functions could be controlled remotely
over the satellite link.

VEHICLE SYSTEMS

Vehicle

The Telepresence Remotely Operated Vehicle (TROV) is
based on a modiﬁed Phantom S2, built by Deep Ocean
Engineering (San Leandro, CA). The Phantom was modi-
ﬁed to include stereo cameras on a fast pan-tilt platform,
ﬁxed position zoom and belly cameras, ﬁber optic lines on
the tether, SHARPS navigation transponders, and a manip-
ulator arm. Figure 1 shows a photograph of the TROV in
the underwater environment. Figure 2 shows the vehicle
conﬁguration.

The Phantom is built around twin aluminum tubes, with a
hull body of molded rigid syntactic foam that is buoyant in

water, yielding a vehicle which is nearly neutrally buoyant.
The hollow tubes hold the electronics payload and the four
electric thruster motors. Two thrusters are mounted horizon-
tally, and two others are mounted at 45 degrees to vertical.
Thruster control provides four degree-of-freedom motion
(all three translations, plus yaw). The vehicle is incapable of
commanded pitch or roll. Electrical power for thrusters and
lights is supplied by wires in a 340 meter tether from the sur-
face console

Sensors

The primary sensor for the vehicle is a pair of stereo video
cameras mounted on a rapid pan and tilt platform. The stereo
cameras are mounted at approximately human interocular
distance (10 cm) and can slew +/-90 degrees at rates
approaching that of the human head. The system is designed
to simulate human head motions, slew rates, and eye posi-
tions, which is important in operating the vehicle in a telep-
resence mode. The vehicle also carries a ﬁxed orientation

FIGURE 1. The Telepresence Remotely Operated Vehicle (TROV) under the ice in the Ross Sea. The stereo

cameras on the pan-tilt platform are visible at center front. The manipulator arm is to the lower
right with respect to the vehicle body.

camera with a power zoom lens, used for close-up imag-
ing, and a downward pointing camera mounted under the
vehicle’s midsection, used to image the area directly
beneath the vehicle.. All four video signals from the vehi-
cle, and control signals to the vehicle such as focus, zoom,
and pan-tilt are transmitted on a ﬁber optic cable attached
to the main umbilical. In addition to the video cameras,
there are sensors for ambient and direct light, dissolved
oxygen, and temperature.

Manipulator

A manipulator arm is mounted on the crash frame that sur-
rounds the TROV hull. The arm (built by Benthos Inc.) is
approximately 0.5 m in length, and has two revolute
degrees of freedom. The manipulator end effector is a sim-
ple gripper claw. The control signals which drive the
manipulator are sent through the ﬁber umbilical. Since the
manipulator has few degrees-of-freedom, the operator typ-
ically uses the vehicle to provide extra degrees-of-free-
dom..

TROV 1993

Fiber termination bottle:

SHARPS
transponder

Dive planes

....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................
....................

&&&
---------------------
&&&
---------------------
&&&
---------------------
&&&
&&&
&&&
&&&
&&&
&&&
&&&
&&&
&&&
&&&
to 1100 ft. Umbilical
&&&
&&&
&&&
&&&
&&&
&&&
&&&
&&&
regulated 12vdc;
&&&
&&&
&&&
&&&
,,,,,,,,,,,,,,,,,,,,,
&&&
,,,,,,,,,,,,,,,,,,,,,
&&&
,,,,,,,,,,,,,,,,,,,,,
&&&
&&&

)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
)))
'''''
(((((
)))
'''''
(((((
)))
'''''
(((((
)))
'''''
(((((
)))

0000
6666
6666
0000
0000
6666
===
;;;
>>>
???
++++++
6666
0000
===
;;;
>>>
???
++++++
6666
0000
===
;;;
>>>
???
++++++
6666
0000
===
;;;
>>>
???
++++++
6666
0000
===
;;;
>>>
???
++++++
6666
0000
===
;;;
>>>
???
6666
0000

7777
7777
7777
7777
7777
7777
7777
7777
7777
7777

********
********
********
********
********
9999
9999
9999
9999

11111
11111
11111
11111

3−axis arm

8888
8888
:::
:::

1 Swing −
2 Swing +
3 Twist −
4 Twist +
5 Gripper −
6 Gripper +
7 nc
8 nc

1 Transmit +
2 Transmit −
Recieve +
3
Recieve −
4
5
Video −
Video +
6

tray:

Instrument
SPIO serial mudule
SeaLan fiber card
Arm relay board
DC−DC +15v
DC−DC −15v
DC−DC +24v

Depth−heading board
Pan−Tilt buffer amp
Sensors board

Dissolved O2 anode

Sensors:
1
2
3
4
5
6 Direct
7 Direct
8
Diffuse light sensor −
9 Diffuse light sensor +

Thermistor
Thermistor
Dissolved O2 cathode

light sensor −
light sensor +

Fiberoptic cable taped

4−channel
video mux

Focus relays
Serial−to−parallel;
16vdc power;
Camera board

SHARPS reciever

Stereo Cameras

1 Ground
2
3
4
5
6
7

Video Left
+16V
Focus Left
Focus Right
Focus Common
Video Right

Starboard Lights:

panhead
1
2
panhead
3 Sharps 1
4 Sharps 2
5
6
7

down
down
hull ground

SHARPS transponder

Pan & Tilt Power

1
Tilt−
2 Tilt+
3
Pan−
4 Pan+

Pan & Tilt Signals

+11.25 V
Pan Velocity
Pan Position
Tilt Velocity
Tilt Position
Signal Ground

1 Ground
2
3
4
5
6
7
8 nc
9 nc

Fixed Cameras:
Hull:

1 Gnd
2 Video #1
3
+12 Volts
4 Focus
Zoom
5
6
Direction
7 Video #2

Camera 1:
1 Gnd
2 Video #1
3
+12 Volts
4 Focus
Zoom
5
6
Direction
7 Gnd

Camera 2:
1 Gnd
2 Video #2
3
+12 Volts
4 Gnd

333
555
333
555
2222
333
555
2222
333
555
FIGURE 2. A schematic diagram of the

Port Light:

1
2
3

lights black
lights gray
hull ground

underwater vehicle, showing the
cameras, sensors, and manipulator.

Navigation

Localization of the TROV was achieved using a SHARP-
STM Navigation System (Marquest Group). This system
uses acoustic transponders to transmit and receive acoustic
line-of-sight ranging signals. Two transponders were
mounted on the TROV (front and rear) and three others
were suspended in the water column on cables so that they
formed an approximate equilateral triangle with 100 meter
legs (see ﬁgure 3). The three reference transponders were
connected to the control unit by coaxial cables, while the
two transponders on the vehicle were driven via an RS-
485 link through a twisted-pair in the main umbilical. The
SHARPS system works well only when the vehicle is
within 100 meters of all three transponders for triangula-
tion.

The SHARPSTM navigation system was controlled by an
i386 PC. The control software provided a two-dimensional
plot of the time history of the navigation track, as well as a
real-time numeric display of the position. This navigation
track was used by the local operators to determine the
vehicle’s position relative to the dive hole, as well as to
determine the progress on the science transects being stud-
ied. In parallel with the live display, the navigation data
were acquired by the VME control computer, and sent as
telemetry over the satellite link to the Ames’ control cen-
ter.

SHARPS Configuration

SHARPS
PC

100m − RG58

100m − RG58

100m − RG58

100m − Kevlar

TROV Umbilical

100m − Kevlar

100m − Kevlar

PHANTOM S2

100m

100m

100m

100m

100m

100m

FIGURE 3. A Schematic for a typical deployed

conﬁguration of the SHARPS
navigation equipment.

TROV
Control
System

IR Laser

TROV

NASA Ames
Control Center

IR Laser

m Wave

CODEC

IntelSat 178W

CODEC

Earth Station

Earth Station

m Wave

FIGURE 4. Schematic of the communications ﬂow between the vehicle and NASA Ame

Surface Controller

The surface control electronics of the Phantom S2 were
augmented by a VME-based control computer with digital
and analog input/output, running the VxWorks real-time
operating system. The control computer was interfaced to
the Phantom control box so that it provided switch clo-
sures and voltages to the control system. Local or remote
operation could be selected at any time by the local opera-
tor. Remote commands over the satellite link were
received through a TCP/IP connection on the VME board,
and translated into control voltages or switch closures.

Communications

Communications between the vehicle and the surface con-
troller occurred over twisted-pairs in the ROV umbilical,
augmented by a ﬁber optic line taped to the umbilical. The
ﬁber optic line carried the four simultaneous video chan-
nels, while the twisted-pairs carried sensor signals and
SHARPS pulses. Remote digital communications between
the vehicle and NASA Ames were established using multi-
ple links as shown in ﬁgure 4..

Live stereo video was transmitted from the location of the
operations hut on the sea ice via an infrared laser to a
receiver in McMurdo Station. From there, the video signal
was compressed and transmitted via a microwave trans-

mitter to a satellite transmission station on Black Island,
where it was retransmitted to NASA Ames over a T1 satel-
lite channel. The compressed video portion of the T1 link
used 768 Kbps, and the rest of the link was used to provide
a bidirectional TCP/IP link to the vehicle control computer
through which the command and telemetry signals trav-
eled, along with bidirectional telephone service

Architecture

The system architecture used to control this vehicle is an
example of the Ames Robotic Computational Architecture
(ARCA), which is a highly portable computational archi-
tecture designed at NASA Ames to provide a uniﬁed
framework for integrating diverse subsystems and compo-
nents for telerobotic applications.1

The architecture utilizes embedded hardware controllers,
and includes single-board computers (VME-based), inter-
face boards, and specialized processors (see Table 1).
These components are used primarily for on-board vehicle
processing and for synchronous tasks. VME bus was
selected since it is a well established standard, has a large
installed user and manufacturing base, and has a clear evo-
lutionary path to future hardware. In addition, many VME
manufacturers produce ruggedized and low power boards,
which are advantageous for on-board vehicle systems.

TABLE 1.

ARCA embedded processing hardware (VME bus based)

Type

Representative Use

Hardware

Single-board computer

Uniprocessor with multitasking

Interface boards

Specialized boards

Communications & device interfacing
via standard and custom protocols

Optimized processing for speciﬁc
embedded tasks

TABLE 2.

ARCA standalone processing hardware (UNIX workstations)

Type

Sun Microsystems, Inc.

Silicon Graphics, Inc.

Representative Use

UNIX applications
Compute Server
X Windows interfaces
UNIX applications
Compute Server
Interactive 3-D graphics

Heurikon HK68/V3D (Motorola 68030
CPU)
Xycom XVME-400 (RS232C)
Matrix MD-DAADIO (A/D, D/A, PIO)
National GPIB-1014 (IEEE-488)
Matrox VIP-640 (NTSC frame capture)
PMC DCX-VM100 (Motor controller)

Hardware

SparcStation 4/370 (SPARC RISC CPU)

4D/440 VGXT (MIPS R3000 CPU)
Indigo (MIPS R4000 CPU)

Standalone processing hardware is currently a mixture of
UNIX workstations as shown in Table 2. These systems
are used primarily for non real-time processing, compute
intensive applications, and operator interfaces. UNIX
workstations provide tools, networking capability, and an
environment well suited for software development. As
with the embedded hardware, the systems shown in Table
2 were chosen due to large installed base and expected
longevity. Silicon Graphics workstations, in particular,
have shown signiﬁcant improvements in recent years and
provide unparalled real-time, interactive graphics capabili-
ties

ARCA’s standardized communications provides a system
for interprocess communications and synchronization
across a heterogenous and distributed processing system.
Presenting a common programming interface, standard-
ized communications facilitates teamwork by enabling
modular development and reducing the difﬁculty of sys-
tems integration. At this time, standardized communica-
tions is achieved via the base layer of Carnegie Mellon
University’s Task Control Architecture (TCA).

TCA is a distributed, layered architecture with centralized
control. Communications occurs via coarse-grained mes-
sage passing between modules2. The base layer of TCA
implements a simple remote procedure call (RPC), in
which the central control determines which module han-

dles a particular message and in what order. This RPC
interface operates via Berkeley Unix TCP/IP protocols and
ethernet network transmission devices.

TCA was chosen because it is capable of providing inter-
process communications between processes in all ARCA
processing domains and across a wide range of computing
hardware. The primary limitation of TCA is that all com-
munications are routed through the central control facility.
As a result, the central process may become a bottleneck
and communcations will deadlock if the process execution
stops. Of secondary concern is the bandwidth provided by
TCP/IP. TCA’s effective throughput has been estimated to
be approximately 200 kilobytes per second.2

The advantages of TCA greatly outweigh its limitations.
TCA provides a ﬂexible, reliable, and easy to use commu-
nication method which greatly speeds program develop-
ment. Centralized message routing facilitates the
debugging of intermodule communications and coordina-
tion. Additionally, since TCA utilizes TCP/IP, it is intrinsi-
cally capable of long distance communications via the
Internet. We have successfully used TCA to provide reli-
able communication between sites in California and
France.3 The impact of centralized routing and transmis-
sion bandwidth can be reduced via appropriate usage, such
as limiting communications to coarse-grained message
passing.

OPERATOR INTERFACE

Local Operator Interface

The local operator sat in a heated hut and controlled the
vehicle using a modiﬁed Phantom joystick control box,
while viewing stereo video camera images on a Stereo-
GraphicsTM ﬁeld sequential stereo video system4. The Ste-
reoGraphicsTM system uses a 120Hz scan rate to
sequentially display the left and right video signals of a
stereo pair at 60 Hz each. The user wears a pair of Crystal
EyesTM glasses. Each lens of the liquid crystal glasses acts
as a shutter, synchronized with the images on the monitor,
so that the user alternately sees the scene from one camera
in each eye. The depth perception achieved with this sys-
tem is excellent, especially in comparison with older sys-
tems displaying 30 Hz to each eye. Unlike earlier setero
display systems, the StereoGraphicsTM system has no noti-
cable ﬂicker. The observer experiences the illusion of
looking through a window at the underwater scene. Other
monitors displayed another camera selected by the opera-
tor, and the vehicle’s track from the SHARPS navigation
system.

An Amiga 2000 computer controlled the position of the
pan and tilt camera platform in one of two modes: tracking
the operator’s head motion, or by using the mouse to posi-
tion a graphic icon. The Amiga also provided a graphics
overlay on the video display that included heading, depth,
time, and camera position. Finally, the Amiga served for
local data logging of navigation location, a time stamp,
and data from other science instruments.

Remote Operator Interface

Remote control of the vehicle over the satellite link was
accomplished using the Virtual Environment Vehicle
Interface (VEVI) control software developed at NASA
Ames. VEVI is a modular operator interface for direct
teleoperation and supervisory control of robotic vehicles.
VEVI utilizes real-time interactive 3D graphics and posi-
tion/orientation input devices to produce a range of opera-
tor interface modalities from ﬂat-panel (windowed or
stereoscopic) screen displays to head-mounted/head-track-
ing stereo displays. An operator using VEVI is shown in
Figure 5. The interface provides generic vehicle control
capability and has been used to control wheeled, air bear-
ing, and underwater vehicles in a variety of environments.

VEVI executes as a loosely-synchronous process on Sili-
con Graphics workstations, rendering a scene with models
of the controlled vehicle and the environment in which the
vehicle is operating. Feedback from on-board vehicle sen-
sors is used to update the simulated vehicle state and
world model. Operators interact asynchronously with
VEVI to control the graphical vehicle or to change view-
ing parameters (e.g., ﬁeld-of-view, viewpoint). Under
direct teleoperation, changes to the graphical vehicle are
communicated in real-time via TCA to the actual vehicle.
For supervisory control, task-level command sequences
are planned within VEVI, then relayed to the vehicle con-
troller for autonomous execution. This operating paradigm
enables vehicle control in the presence of lengthy trans-
mission delays and latencies, while increasing operator
productivity and reducing fatigue.

FIGURE 4. An operator in a helmet-mounted

display. The operator can choose either
live stereo video, computer-generated
graphics, or a mixture of the two. to be
displayed in the helmet

The remote operator interface included either a stereo dis-
play monitor similar to that used by the local operator, or a
stereo head-mounted, head-tracked display. In addition to
live stereo video, the remote operator could view a com-
puter-generated representation of the underwater terrain,
modeled from stereo images combined with navigational
information, and generated off-line using a Silicon Graph-

ics workstation. This virtual environment contained an
animated graphic model that reﬂected the state of the
actual vehicle, along with ancillary information such as
the vehicle navigation track, science markers, and loca-
tions of video snapshots. The vehicle could be driven
either from within the virtual environment or through a
live telepresence interface

The terrain models used in the virtual environment inter-
face were generated from the on-board sensors using two
techniques. One technique was to record the SHARPS
position information over the telemetry link while an oper-

ator (either local or remote) drove the vehicle over the terrain
at a near-constant altitude. This “ground-hugging” technique
provided streams of coordinate values along the vehicle
track which were interpolated onto a uniform terrain eleva-
tion map, provided the area coverage was sufﬁcient. If the
terrain coverage was sparse, then the interpolated terrain
contained too many algorithmic artifacts to be useful. The
second technique involved digitizing stereo camera frames,
and using them to create range maps. The range maps were
generated by running a Laplacian-of-Gaussian ﬁlter over the
images, and then using a windowed cross-correlation tech-
nique to produce disparity values everywhere in the image at

FIGURE 6. Screen shot of the Virtual Environment Vehicle Interface. The graphic representation of the TROV is
centered in the screen. The terrain map modelled from the vehicle’s sensors is shown in brown. The
rectangular lines are a lat-long grid for reference

which there was sufﬁcient contrast. These range maps,
when combined with the navigation information and the
pan-tilt camera angles, produced local terrain elevation
values in the conical ﬁeld-of-view from the current vehicle
position. Multiple terrain elevation wedges generated from
stereo frames taken while the vehicle moved were com-
bined by a simple weighted average interpolation tech-
nique to produce large area uniform grid terrain elevation
maps. Once the vehicle had covered an area of interest,
typically 100 meters-square, the algorithms required
approximately 10 minutes processor time to produce the
elevation map of the area. The terrain elevation maps cre-
ated with either of these techniques were input directly
into the VEVI software to visualize the vehicle in the ter-
rain (see ﬁgure 6). Once the terrain was input to the VEVI

software, the vehicle could be driven purely from within
the virtual environment. The registration between the vir-
tual terrain and the actual terrain was within an estimated
0.25 meters in areas with good coverage, which was sufﬁ-
cient for obstacle avoidance.

OPERATIONS

During the 1993 Antarctic mission,TROV was operated
nearly continuously for over two months locally and
remotely from NASA Ames. The primary science objec-
tive of the mission was to perform transects of selected
areas between 60 and 1000 feet depth, and produce a video
record of the benthic organisms. A secondary objective
was to perform sample collection tasks with the manipula-

FIGURE 7. A plan view map of the McMurdo site. The TROV operated at various points within line-of-sight

of the McMurdo Station

tor arm. The data from the mission are being analyzed, and
will be used to generate a multi-resolution terrain model of
the area, with the video record overlaid as textured graph-
ics on the terrain elevation data. The engineering and
human performance data obtained during the mission are
being used to further develop the ARCA and VEVI soft-
ware systems for future missions. Ultimately, we hope that
this approach to science exploration missions will prove
viable for use in the exploration of other planetary sur-
faces.

Field operations for local control of the TROV were set up
in a ﬁeld hut located on the McMurdo Sound sea ice. Fig-
ure 7 shows a plan view of the McMurdo Station, with the
approximate area of operations highlighted with the cross-
hatched oval. The hut contained a fuel-oil heater and was
mounted on skis so that it could be moved from one loca-
tion to another on the sea ice. Power was provided via two
generators. A hatch in the ﬂoor of the hut allowed the
TROV to be deployed and retrieved from inside the hut.
Because all the electronic systems were located inside,
most of the ﬁeld operations were conducted in a “shirt-
sleeve” environment. It was necessary to operate outside
in the Antarctic cold only to set up and service the naviga-
tion and video transmitter systems, and to handle the
TROV tether.

The ﬁeld experiment represented a collaboration between
a telepresence technology team from NASA Ames
Research Center and two scientiﬁc research teams inter-
ested in Antarctic marine biology. The ﬁrst team, inter-
ested in surveying the benthic ecology of McMurdo
Sound, was headed by Dr. Jim Barry of the Monterey Bay
Aquarium Research Institute (MBARI). The second team
was Antarctic Science Project S022 led by Dr.’s Jim
McClintock and Bill Baker. This team was interested in
the chemical analysis of samples collected by the TROV.

The benthic ecology was surveyed using the TROV stereo
cameras, close-up camera, and position information
obtained from the navigational system. Two types of
benthic surveys were performed. The ﬁrst involved the re-
survey of an area which had been set up for survey by
divers, and had been repeatedly surveyed over the years.
In this area, from a depth of 12 to 40 meters, a set of
transect lines had been laid out with a stake at either end.
The lines were 20 to 30 meters long. A survey along these
lines involved ﬁnding the line, the stakes at each end, and
then driving the TROV along the line at a constant height
of about 0.5 meter off the bottom. Stereo video, with the
video cameras pointed about 60 degrees below horizontal,

was recorded on Hi-8 video tape. A second video recorder
was used to record the left camera only. The rate of ﬂight
was slow enough to maintain the constant height, and to
visually resolve all the organisms in the ﬁeld of view.
After reaching the end of the transect line, the TROV
turned around or went back to the start and drove over the
line again, this time looking at the scene with the zoom
camera zoomed in to maximum magniﬁcation. This served
as a close-up lens to get detailed imaging of organisms in
the ﬁeld of view. Performing transects with the SHARPS
system was basically similar, with the exception that a
graphic display was tracked instead of a physical transect
line. Approximately 30 such lines were ﬂown in each tri-
angular grid. Six areas were surveyed at depths ranging
from 20 to 340 meters.

The manipulator arm was used to collect sample organ-
isms and to carry a collection basket. The basket was made
of PVC pipe drilled full of holes so as not to trap air,
weighted and covered with netting, with a hinged door on
top that could be pushed open. The basket had a handle
which could be grasped by the gripper. The operator
would normally place the basket on th bottom to use the
gripper to gather a sample. By noting the navigation coor-
dinates of the basket, the operator could drive around to
ﬁnd an interesting object, reach out with the arm and grab
it with the gripper claw, return it to the basket, and insert
the sample through the basket door. When the basket was
full, it was grabbed in the gripper and carried to the sur-
face ice hole. Using the joystick box and manipulator con-
troller, this was a three-handed operation. Work is
continuing to simplify the operators’ interface with simple
to use hand controllers, such as a data glove.

CONCLUSIONS

Telepresence technology and virtual reality technologies
provide signiﬁcant advantages for benthic exploratory
research, particularly for habitats where little or no prelim-
inary information exists. Normal ROV video observations
are most effective where the spatial dimensions and land-
scape characteristics are known by the researchers, allow-
ing the observer to place the ROV in a spatial perspective.
For unknown or poorly known environments, telepresence
provides a feeling of immersion in the environment, while
virtual reality provides a spatial perspective beyond the
immediate ROV camera view. Thus, the observer is capa-
ble of spatial orientation in 3 dimensions for the current
ROV video imagery (local spatial orientation), as well as a
larger spatial scale (virtual reality). Moreover, plots of the
ROV track in the virtual terrain provide a spatial ‘history’

REFERENCES

[1] Fong, T.W., “A Computational Architecture for
Semi-Autonomous Robotic Vehicles,” AIAA
Computing in Aerospace 9 Conference, San
Diego CA,1993.

[2] Lin, L., Simmons, R., and Fedor, C., “Experience

with a Task Control Architecture for Mobile
Robots,” CMU-RI-TR 89-29, Robotics Institute,
Carnegie Mellon University, 1989.

[3] Garvey, J.M., “A Russian-American Planetary
Rover Initiative,” AIAA 93-4088, AIAA Space
Programs and Technologies Conference,
Huntsville AL, 1993.

[4] “CrystalEyes Video System: User’s Manual,”

StereoGraphics Corporation, San Rafael CA,
1993.

on a landscape scale. This is a distinct advantage over
more conventional ROV video, where the myopia associ-
ated with a narrow ﬁeld of view and lack of knowledge of
the recent history of observations, makes it difﬁcult or
impossible to generate the integrated perspective that is
provided by these new technologies.

Regarding the objective of obtaining estimates of the den-
sities of the dominant benthic fauna along video transects
in McMurdo Sound, the TROV was a considerable
advance over conventional ROVs. The use of stereo video
is potentially a breakthrough as a research tool for scien-
tists in that it enables calculations of the spatial dimen-
sions of video images, and thus, quantiﬁcation of various
parameters in the ﬁeld of view (e.g. sizes of objects, dis-
tances between objects). Incorporation of high resolution
navigation equipment with stereo video further enhances
the quantitative nature of this system by providing highly
accurate measurements of the positions of objects, dis-
tances traveled, and distances between objects.

This experiment represents a ﬁrst in the use of of telepres-
ence and virtual reality for the exploration of a previously
unknown scientiﬁc environment, using rendered terrain
models in the environment. The use of telepresence and
virtual reality for scientiﬁc exploration promises to revolu-
tionize the study of hostile and extreme environments. By
keeping human controllers actively involved, but provid-
ing better interfaces to the machines and better data visual-
ization, better science can be achieved. The ﬁdelity of the
data record allowed by this technology, and the ease with
which it can be recreated for archival study offer a signiﬁ-
cant improvement in the capabilities for data analysis.
Finally, telepresence and virtual reality, both as a control
system and as a data visualization method, afford the
potential for distribution to a wider scientiﬁc community
than would be possible with more conventional methods.

