Exascale Challenges for the Computational Science Community
Horst Simon Lawrence Berkeley National Laboratory
and UC Berkeley
Oklahoma Supercomputing Symposium 2010 October 6, 2010

Key Message
· The transition from petascale to exascale will be characterized by significant and dramatics changes in hardware and architecture. · This transition will be disruptive, but create unprecedented opportunities for computational science.
Managed by the University of California for the U.S. Department of Energy

Overview
· From 1999 to 2009: evolution from Teraflops to Petaflops computing
· From 2010 to 2020: key technology changes towards Exaflops computing
· Impact on Computational Science
Managed by the University of California for the U.S. Department of Energy

J a g u a r: Wo rld 's m o s t p o we rfu l c o m p u te r s in c e 2009

#1 Nov. 2009

Peak performance System memory Disk space Processors Power

2.332 PF 300 TB 10 PB 224K 6.95 MW
Managed by the University of California for the U.S. Department of Energy

ASCI Red: World's Most Powerful Computer in 1999

#1 Nov. 1999

Peak performance System memory Disk space Processors Power

3.154 TF 1.212 TB 12.5 TB 9298 850 kW

Managed by the University of California for the U.S. Department of Energy

Comparison  Jaguar (2009) vs. ASCI Red (1999)

· 739x performance (LINPACK)

· 267x memory · 800x disk · 24x processors/cores

Parallelism and faster processors made about equal contributions to performance increase

· 8.2x power
Significant increase in operations cost
Essentially the same architecture and software environment

Managed by the University of California for the U.S. Department of Energy

Overview
· From 1999 to 2009: evolution from Teraflops to Petaflops computing
· From 2010 to 2020: key technology changes towards Exaflops computing
· Impact on Computational Science
Managed by the University of California for the U.S. Department of Energy

Traditional Sources of Performance Improvement are Flat-Lining (2004)
· New Constraints
­ 15 years of exponential clock rate growth has ended
· Moore's Law reinterpreted:
­ How do we use all of those transistors to keep performance increasing at historical rates?
­ Industry Response: #cores per chip doubles every 18 months instead of clock frequency!
Figure courtesy of Kunle Olukotun, Lance Hammond, Herb Sutter, and Burton Smith
Managed by the University of California for the U.S. Department of Energy

Performance Development

100 Pflop/s

32.4 PFlop/s

10 Pflop/s 1 Pflop/s

1.75 PFlop/s

100 Tflop/s 10 Tflop/s
1 Tflop/s 1.17 TFlop/s

SUM N=1

24.67 TFlop/s

100 Gflop/s 10 Gflop/s

59.7 GFlop/s

1 Gflop/s 100 Mflop/s

400 MFlop/s

N=500

Projected Performance Development

100 Pflop/s 10 Pflop/s 1 Pflop/s 100 Tflop/s 10 Tflop/s 1 Tflop/s 100 Gflop/s 10 Gflop/s 1 Gflop/s
100 Mflop/s

SUM N=1 N=500

JJJJJJJJJJJJJJJJJJJJJJJuuuuuuuuuuuuuuuuuuuuuuunnnnnnnnnnnnnnnnnnnnnnn-----------------------1101009991010091990009024596877896051433421530

Jack`s Notebook

Maximum
Average Minimum

1,000,000 100,000 10,000 1,000 100 10 1

# processors

Concurrency Levels

Multicore comes in a wide variety

­ Multiple parallel general-purpose processors (GPPs) ­ Multiple application-specific processors (ASPs)

Intel Network Processor

1 GPP Core

16 ASPs (128 threads)

18 18 18

RDRA M 1

Stripe
RDRA M 2

RDRA M 3

PCI
64b (64b) 66 MHz

Intel® XScale
TM Core 32K IC 32K DC

G A S K E T

Sun Niagara

QDR SRAM
1 E/D Q 11 88

QDR SRAM
2 E/D Q 11 88

QDR SRAM
3 E/D Q 11 88

QDR SRAM
4 E/D Q 11 88

MEv2 MEv2 MEv2 MEv2 1234
MEv2 MEv2 MEv2 MEv2 8765
MEv2 MEv2 MEv2 MEv2 9 10 11 12
MEv2 MEv2 MEv2 MEv2 16 15 14 13

IXP280

0Rbuf
64 @ 128B

S

P I

16b

4

or

Tbuf 64 @ 128B
Hash

C

S I

16b

X

C48S/R64s/1
-Sc2r8atc Fasht_wr -U1A6KRBT Timers -GPIO

BootROM/Sl owPort

8 GPP cores (32 threads)

IBM Cell 1 GPP (2 threads)
8 ASPs
Picochip DSP 1 GPP core 248 ASPs
Cisco CRS-1 188 Tensilica GPPs

Intel 4004 (1971): 4-bit processor, 2312 transistors, ~100 KIPS, 10 micron PMOS, 11 mm2 chip

1000s of processor cores per
die

"The Processor is the new Transistor"
[Rowen]
Managed by the University of California for the U.S. Department of Energy

What's Next?
Source: Jack Dongarra, ISC 2008
Managed by the University of California for the U.S. Department of Energy

Moore's Law reinterpreted
· Number of cores per chip will double every two years
· Clock speed will not increase (possibly decrease)
· Need to deal with systems with millions of concurrent threads
· Need to deal with inter-chip parallelism as well as intra-chip parallelism
Managed by the University of California for the U.S. Department of Energy

Performance Development

100 Pflop/s

32.4 PFlop/s

10 Pflop/s 1 Pflop/s

1.75 PFlop/s

100 Tflop/s 10 Tflop/s
1 Tflop/s 1.17 TFlop/s

SUM N=1

24.67 TFlop/s

100 Gflop/s 10 Gflop/s

59.7 GFlop/s

1 Gflop/s 100 Mflop/s

400 MFlop/s

N=500

Annual Performance Increase of the TOP500

Total Power Levels (kW) for TOP500 systems

Power Efficiency (Mflops/Watt) for different Processor Generations

Power Efficiency (Mflops/Watt) related to Interconnects

Power Consumption

Power Efficiency

Koomey's Law
· Computations per kWh have improved by a factor about 1.5 per year
· "Assessing Trends in Electrical Efficiency over Time", see IEEE Spectrum, March 2010
Managed by the University of California for the U.S. Department of Energy

Trend Analysis
· Processors and Systems have become more energy efficient over time ­ Koomey's Law shows factor of 1.5 improvement in computations/kWh
· Supercomputers have become more powerful over time ­ TOP500 data show factor of 1.86 increase of computations/sec per system
· Consequently power/system increases by about 1.24 per year
· Based on these projections: 495 Pflop/s LinpackRmax system with 60 MW in 2020
Managed by the University of California for the U.S. Department of Energy

Roadrunner - A Likely Future Scenario

System: cluster + many core node

Programming model: MPI+X

after Don Grice, IBM, Roadrunner Presentation, ISC 2008

Not Message Passing
Hybrid & many core technologies will require new approaches: PGAS, auto tuning, ?
Managed by the University of California for the U.S. Department of Energy

Why MPI will persist
· Obviously MPI will not disappear in five years
· By 2014 there will be 20 years of legacy software in MPI
· New systems are not sufficiently different to lead to new programming model
Managed by the University of California for the U.S. Department of Energy

What will be the "X" in MPI+X
· Likely candidates are
­ PGAS languages ­ OpenMP ­ Autotuning ­ CUDA, OpenCL ­ A wildcard from commercial space
Managed by the University of California for the U.S. Department of Energy

What's Wrong with MPI Everywhere?
Managed by the University of California for the U.S. Department of Energy

What's Wrong with MPI Everywhere?
· One MPI process per core is wasteful of intra-chip latency and bandwidth
· Weak scaling: success model for the "cluster era"
­ not enough memory per core
· Heterogeneity: MPI per CUDA threadblock?
Managed by the University of California for the U.S. Department of Energy

We won't reach Exaflops with the current approach
From Peter Kogge, DARPA Exascale Study
Managed by the University of California for the U.S. Department of Energy

... and the power costs will still be staggering
1000

System Power (MW)

100

10

1 2005

2010

2015

2020
From Peter Kogge, DARPA Exascale Study

Managed by the University of California for the U.S. Department of Energy

A decadal DOE plan for providing exascale applications and technologies for DOE mission
needs
Rick Stevens and Andy White, co-chairs
Pete Beckman, Ray Bair-ANL; Jim Hack, Jeff Nichols, Al GeistORNL; Horst Simon, Kathy Yelick, John Shalf-LBNL; Steve
Ashby, Moe Khaleel-PNNL; Michel McCoy, Mark Seager, Brent Gorda-LLNL; John Morrison, Cheryl Wampler-LANL; James Peery, Sudip Dosanjh, Jim Ang-SNL; Jim Davenport, Tom Schlagel, BNL; Fred Johnson, Paul Messina, ex officio

Process for identifying exascale applications and technology for DOE missions ensures broad community input

· Town Hall Meetings April-June 2007
· Scientific Grand Challenges Workshops Nov, 2008 ­ Oct, 2009
· Climate Science (11/08), · High Energy Physics (12/08), · Nuclear Physics (1/09), · Fusion Energy (3/09), · Nuclear Energy (5/09), · Biology (8/09), · Material Science and Chemistry (8/09), · National Security (10/09) · Cross-cutting technologies (2/10)
· Exascale Steering Committee
· "Denver" vendor NDA visits 8/2009 · SC09 vendor feedback meetings · Extreme Architecture and Technology
Workshop 12/2009
· International Exascale Software Project
· Santa Fe, NM 4/2009; Paris, France 6/2009; Tsukuba, Japan 10/2009

MISSION IMPERATIVES
FUNDAMENTAL SCIENCE
33

DOE mission imperatives require simulation and analysis for policy and decision making

· Climate Change: Understanding, mitigating and adapting to the effects of global warming
· Sea level rise
· Severe weather
· Regional climate change
· Geologic carbon sequestration
· Energy: Reducing U.S. reliance on foreign energy sources and reducing the carbon footprint of energy production
· Reducing time and cost of reactor design and deployment
· Improving the efficiency of combustion energy systems
· National Nuclear Security: Maintaining a safe, secure and reliable nuclear stockpile
· Stockpile certification
· Predictive scientific challenges
· Real-time evaluation of urban nuclear detonation
Accomplishing these missions requires exascale resources.

34

Exascale simulation will enable fundamental advances in basic science.

· High Energy & Nuclear Physics
· Dark-energy and dark matter · Fundamentals of fission fusion
reactions
· Facility and experimental design
· Effective design of accelerators · Probes of dark energy and dark matter · ITER shot planning and device control
· Materials / Chemistry
· Predictive multi-scale materials modeling: observation to control
· Effective, commercial technologies in renewable energy, catalysts, batteries and combustion
· Life Sciences
· Better biofuels · Sequence to structure to function

ITER

Hubble image of lensing

ILC
Structure of nucleons

These breakthrough scientific discoveries and facilities require exascale applications and resources.

Slide 35

Potential System Architecture Targets

System attributes
System peak

2010
2 Peta

Power

6 MW

System memory

0.3 PB

Node performance 125 GF

Node memory BW 25 GB/s

Node concurrency

12

System size (nodes)
Total Node Interconnect BW
MTTI

18,700 1.5 GB/s
days

"2015"

200 Petaflop/sec

15 MW

5 PB

0.5 TF

7 TF

0.1 TB/sec 1 TB/sec

O(100)

O(1,000)

50,000

5,000

20 GB/sec

O(1day)

"2018"

1 Exaflop/sec

20 MW

32-64 PB

1 TF

10 TF

0.4 TB/sec

4 TB/sec

O(1,000)

O(10,000)

1,000,000

100,000

200 GB/sec

O(1 day)

Slide 36

Comparison  "2018" vs. Jaguar (2009)

· 500x performance (peak) · 100x memory · 5000x concurrency

All performance increase is based on more parallelism

· 3x power

Keep operating cost about the "same"

Significantly different architecture and software environment

Managed by the University of California for the U.S. Department of Energy

DOE Exascale Technology Roadmap
Key Observations from DOE Exascale Architecture and Technology Workshop, San Diego, Dec. 2009, http://extremecomputing.labworks.org/ha rdware/index.stm
Managed by the University of California for the U.S. Department of Energy

Where do we get 1000x performance improvement for 10x power?
1. Processors 2. On-chip data movement 3. System-wide data movement 4. Memory Technology 5. Resilience Mechanisms

39

Managed by the University of California for the U.S. Department of Energy

Low-Power Design Principles

Tensilica XTensa Intel Atom
Intel Core2
Power 5

· Power5 (server) ­ 120W@1900MHz ­ Baseline
· Intel Core2 sc (laptop) : ­ 15W@1000MHz ­ 4x more FLOPs/watt than baseline
· Intel Atom (handhelds) ­ 0.625W@800MHz ­ 80x more
· Tensilica XTensa DP (Moto Razor) : ­ 0.09W@600MHz ­ 400x more (80x-120x sustained)

Managed by the University of California for the U.S. Department of Energy

Low Power Design Principles

Tensilica XTensa
Intel Core2
Power 5

· Power5 (server) ­ 120W@1900MHz ­ Baseline
· Intel Core2 sc (laptop) : ­ 15W@1000MHz ­ 4x more FLOPs/watt than baseline
· Intel Atom (handhelds) ­ 0.625W@800MHz ­ 80x more
· Tensilica XTensa DP (Moto Razor) : ­ 0.09W@600MHz ­ 400x more (80x-100x sustained)
Even if each simple core is 1/4th as computationally efficient as complex core, you can fit hundreds of them on a single chip and still be 100x more power efficient.
Managed by the University of California for the U.S. Department of Energy

Projected Parallelism for Exascale
How much parallelism must be handled by the program?
From Peter Kogge (on behalf of Exascale Working Group), "Architectural Challenges at the Exascale Frontier", June 20, 2008
Managed by the University of California for the U.S. Department of Energy

Conclusion: Solving Logic Power Drives Move to Massive Parallelism

· Future HPC must move to simpler power-efficient core designs

­ Embedded/consumer electronics technology is central to the future of HPC

­ Convergence inevitable

because it optimizes both

cost and power efficiency

How much parallelism must be handled by the program?

From Peter Kogge (on behalf of Exascale Working Group), "Architectural

Challenges at the Exascale Frontier", June 20, 2008

· Consequence is massive on-chip parallelism

­ A thousand cores on a chip by 2018

­ 1 Million to 1 Billion-way System Level Parallelism

­ Must express massive parallelism in algorithms and pmodels

46

­ Must manage massive parallelism in system software
Managed by the University of California for the U.S. Department of Energy

The Cost of Data Movement
How do those cores talk to each other?

47

Managed by the University of California for the U.S. Department of Energy

The problem with Wires:
Energy to move data proportional to distance
· Cost to move a bit on copper wire:
­ energy = bitrate * Length2 / cross-section area

· Wire data capacity constant as feature size shrinks

· Cost to move bit proportional to distance

· ~1TByte/sec max feasible off-chip BW (10GHz/pin)

· Photonics reduces distance-dependence of bandwidth

Photonics requires no redrive and passive switch little power

Copper requires to signal amplification even for on-chip connections

Managed by the University of California for the U.S. Department of Energy

The Cost of Data Movement

10000 1000 100 10 1

SMP MPI
now

Managed by the University of California for the U.S. Department of Energy

The Cost of Data Movement

10000 1000

SMP MPI

100

CMP

Cost of a FLOP

10 now

1

Managed by the University of California for the U.S. Department of Energy

PicoJoules

The situation will not improve in 2018
Energy Efficiency will require careful management of data locality
10000 1000 100
now 10 2018
1
Important to know when you are on-chip and when data is off-chip!
Managed by the University of California for the U.S. Department of Energy

Memory

58

Managed by the University of California for the U.S. Department of Energy

Projections of Memory Density Improvements
·Memory density is doubling every three years; processor logic is every two · Project 8Gigabit DIMMs in 2018 · 16Gigabit if technology acceleration (or higher cost for early release)
·Storage costs (dollars/Mbyte) are dropping gradually compared to logic costs ·Industry assumption: $1.80/memory chip is median commodity cost
Cost of Computation vs. Memory
Source: David Turek, IBM
Managed by the University of California for the U.S. Department of Energy

Cost of Memory Capacity 2 different potential Memory Densities
$600.00

Cost in Millions of Dollars

$500.00 $400.00 $300.00 $200.00

Cost in $M (8 gigabit modules) Cost in $M (16 Gigabit modules) 1/2 of $200M system

$100.00

$0.00 16 32 64 128 256
Forces us to strong scaling Petabytes of Memory Forces us to memory conservative communication (GAS)
Managed by the University of California for the U.S. Department of Energy

Exascale Memory Power Consumption (San Diego Meeting)

· Power consumption with standard technology roadmap

· Power consumption with investment in advanced memory technology

12 10.6 48

FPU Memory Interconnect

FPU Memory 12 10.6 Interconnect
6.4

70 MW total

20 MW total

Managed by the University of California for the U.S. Department of Energy

Memory Power Consumption in Megawatts (MW)

100 90 80 70 60 50 40 30 20 10 0 0.01

Memory Technology Bandwidth costs power

Stacked JEDEC 30pj/bit 2018 ($20M)
Advanced 7pj/bit Memory ($100M)
Enhanced 4pj/bit Advanced Memory ($150M cumulative) Feasible Power Envelope (20MW)

0.1 0.2 0.5

1

Bytes/FLOP ratio (# bytes per peak FLOP)

2

Managed by the University of California for the U.S. Department of Energy

Limiting Memory Bandwidth Limits System Scope
Managed by the University of California for the U.S. Department of Energy

Power Considerations Drive Future Architectures in the Exascale Era
· Massive parallelism with low power processors
· Limited amount of memory, low memory/flop ratios (processing is free)
· Cost of data movement, locality is becoming more important
Managed by the University of California for the U.S. Department of Energy

What are critical exascale technology investments?
· System power is a first class constraint on exascale system performance and effectiveness.
· Memory is an important component of meeting exascale power and applications goals.
· Programming model. Early investment in several efforts to decide in 2013 on exascale programming model, allowing exemplar applications effective access to 2015 system for both mission and science.
· Investment in exascale processor design to achieve an exascale-like system in 2015.
· Operating System strategy for exascale is critical for node performance at scale and for efficient support of new programming models and run time systems.
· Reliability and resiliency are critical at this scale and require applications neutral movement of the file system (for check pointing, in particular) closer to the running apps.
· HPC co-design strategy and implementation requires a set of a hierarchical performance models and simulators as well as commitment from apps, software and architecture communities.
69

Overview
· From 1999 to 2009: evolution from Teraflops to Petaflops computing
· From 2010 to 2020: key technology changes towards Exaflops computing
· Impact on Computational Science
­ Co-design
Managed by the University of California for the U.S. Department of Energy

The trade space for exascale is very complex.

20 MW power envelope
feasible systems

bytes/core envelope

$200M
cost envelope

Exascale Performance
envelope

nodes

memory

71

Co-design expands the feasible solution space to allow better solutions.

Application driven: Find the best technology to run this code. Sub-optimal

Application
 Model  Algorithms  Code

Now, we must expand the co-design space to find better solutions: ·new applications & algorithms, ·better technology and performance.

Technology
architecture programming model resilience power

Technology driven: Fit your application to this technology. Sub-optimal.
72

A first step toward co-design was the last exascale workshop.
· The approach will be to engage experts in computational science, applied mathematics and CS with the goal of
· Producing a first cut at the characteristics of systems that (a) could be fielded by 2018 and (b) would meet applications' needs
· Outlining the R&D needed for "co-design" of system architecture, system software and tools, programming frameworks, mathematical models and algorithms, and scientific application codes at the exascale, and
· Exploring whether this anticipated phase change in technology (like parallel computing in 1990s) provides any opportunities for applications. That is, whether a requirement for revolutionary application design allows new methods, algorithms, and mathematical models to be brought to bear on mission and science questions.
73

Summary of some priority research directions (PRD)
Black ­ Crosscutting workshop report Green ­ HDS interpretation
· Investigate and develop new exascale programming paradigms to support `billion-way' concurrency
­ Think 10,000 times more parallel ­ Expect MPI+X programming model ­ Think of algorithms that can easily exploit the intra node parallelism,
especially if CS researchers develop automatics tools for X
Managed by the University of California for the U.S. Department of Energy

Summary of some priority research directions (PRD) -- cont.
· Re-cast critical applied mathematics algorithms to reflect impact of anticipated macro architecture evolution, such as memory and communication constraints
­ Live with less memory/thread and less bandwidth
· Develop new mathematical models and formulations that effectively exploit anticipated exascale hardware architectures
­ Add more physics and not just more refinement
· Address numerical analysis questions associated with moving away from bulk-synchronous programs to multitask approaches
­ No more SPMD; think of mapping coarse grain data flow in frameworks
Managed by the University of California for the U.S. Department of Energy

Summary of some priority research directions (PRD) ­ cont.
· Adapt data analysis algorithms to exascale environments · Extract essential elements of critical science applications
as "mini-applications" that hardware and system software designers can use to understand computational requirements · Develop tools to simulate emerging architectures for use in co-design
­ Applied mathematicians/computer scientists should be ready to lead co-design teams
Managed by the University of California for the U.S. Department of Energy

Summary
· Major Challenges are ahead for extreme computing
­ Power ­ Parallelism ­ ... and many others not discussed here
· We will need completely new approaches and technologies to reach the Exascale level
· This opens up many new opportunities for computer scientists, applied mathematicians, and computational scientists
Managed by the University of California for the U.S. Department of Energy

Shackleton's Quote on Exascale
Ernest Shackleton's 1907 ad in London's Times, recruiting a crew to sail with him on his exploration of the South Pole
"Wanted. Men/women for hazardous architectures. Low wages. Bitter cold. Long hours of software development. Safe return doubtful. Honor and recognition in the event of success."
Managed by the University of California for the U.S. Department of Energy

