Proceedings of the International Congress of Mathematicians Berkeley, California, USA, 1986
Markov Random Field Image Models and Their Applications to Computer Vision
STUART GEMAN AND CHRISTINE GRAFFIGNE
1. Introduction. Computer vision refers to a variety of applications involving a sensing device, a computer, and software for restoring and possibly interpreting the sensed data. Most commonly, visible light is sensed by a video camera and converted to an array of measured light intensities, each element corresponding to a small patch in the scene (a picture element, or "pixel"). The image is thereby "digitized," and this format is suitable for computer analysis. In some applications, the sensing mechanism responds to other forms of light, such as in infrared imaging where the camera is tuned to the invisible part of the spectrum neighboring the color red. Infrared light is emitted in proportion to temperature, and thus infrared imaging is suitable for detecting and analyzing the temperature profile of a scene. Applications include automated inspection in industrial settings, medical diagnosis, and targeting and tracking of military objects. In single photon emission tomography, as a diagnostic tool, individual photons, emitted from a "radiopharmaceutical" (isotope combined with a suitable pharmaceutical) are detected. The object is to reconstruct the distribution of isotope density inside the body from the externally-collected counts. Depending on the pharmaceutical, the isotope density may correspond to local blood flow ("perfusion") or local metabolic activity. Other applications of computer vision include satellite imaging for weather and crop yield prediction, radar imaging in military applications, ultrasonic imaging for industrial inspection and a host of medical applications, and there is a growing role for video imaging in robotics.
The variety of applications has yielded an equal variety of algorithms for restoration and interpretation. Unfortunately, few general principals have emerged and no common foundation has been layed. Algorithms are by and -large-ad=hoet=they=are=typic^^ ically tuned to the particulars of the environment (lighting, weather conditions, magnification, and so on) in which they are implemented. It is likely that a
Research partially supported by Army Research Office Contract DAAG29-83-K-0116, National Science Foundation Grant DMS-8352087, and the General Motors Corporation.
© 1987 International Congress of Mathematicians 1986
1496

MARKOV RANDOM FIELD IMAGE MODELS

1497

coherent theoretical framework would support more robust and more powerful algorithms. We have been exploring an approach based upon probabilistic image models, well-defined principals of inference, and a Monte Carlo computation theory. Exploiting this framework, we have recently obtained encouraging results in several areas of application, including tomography, texture analysis, and scene segmentation.
As an illustration of our approach, we shall discuss here the application to texture analysis. Other applications, and more complete discussions of the foundations, can be found in [1, 3, 4, 10, 12, 13, 14, 17, 18, 23, 25, and 27]. In the section that follows, §2, we lay out, briefly, our paradigm in its general formulation. Then, in §3, the application to texture analysis is developed and illustrated by computer experiments. This application requires that we treat a somewhat unusual problem in parameter estimation, namely the estimation of parameters of a Markov random field from a single, large, sample. §4 details the estimation method used, and provides a proof of its consistency in the "large picture" limit, which is more appropriate than the usual "large sample size" limit.

2. Bayesian paradigm. In real scenes, neighboring pixels typically have similar intensities, boundaries are usually smooth and often straight, textures, although sometimes random locally, define spatially homogeneous regions, and objects, such as grass, tree trunks, branches and leaves, have preferred relations and orientations. Our approach to picture processing is to articulate such regularities mathematically, and then to exploit them in a statistical framework to make inferences. The regularities are rarely deterministic; instead, they describe correlations and likelihoods. This leads us to the Bayesian formulation, in which prior expectations are formally represented by a probability distribution. Thus we design a distribution (a "prior") on relevant scene attributes to capture the tendencies and constraints that characterize the scenes of interest. Picture processing is then guided by this prior distribution, which, if properly conceived, enormously limits the plausible restorations and interpretations.
The approach involves five steps, which we shall briefly review here (see [13 and 18] for more details). This will define the general framework, and then, in the following sections, we will concentrate on the analysis of texture, as an illustrative application.
Image models. These are probability distributions on relevant image attributes. Both for reasons of mathematical and computational convenience, we use Markov random fields (MRF) as prior probability distributions. Let us suppose that we index all of the relevant attributes by the index set S. S is application specific. It typically includes indices for each of the pixels (about 512 X 512 in the usual video digitization) and may have other indices for such attributes as boundary elements, texture labels, object labels on so on. Associated with each "site" s G S is a real-valued random variable X3, representing the state of the corresponding attribute. Thus X3 may be the measured intensity at pixel s

1498

S. GEMAN AND C. GRAFFIGNE

(typically, X3 G {0,..., 255}) or simply 1 or 0 as a boundary element at location s is present or absent.
The kind of knowledge we represent by the prior distribution is usually "local," which is to say that we articulate regularities in terms of small local collections of variables. In the end, this leads to a distribution on X -- {X3}aes with a more or less "local neighborhood structure" (again, we refer to [13 and 18] for details). Specifically, our priors are Markov random fields: there exists a (symmetric) neighborhood relation G = {G3}3eSì wherein Gs Ç S is the set of neighbors of 8, such that

U(X3 = x3\Xr = xr,reS,r^s)

= U(X3 = x3\Xr = xr,re G3).

n(a|6) is conditional probability, and, by convention, s £ G3. G symmetric means s £Gr <& r EG3. (Here, we assume that the range of the random vector X is discrete; there are obvious modifications for the continuous or mixed case.)

It is well known, and very convenient, that a distribution II defines a MRF

on S with neighborhood relation G if and only if it is Gibbs with respect to the

same graph, (S,G). The latter means that II has the representation

U(x) = \e~uW

(2.1)

where

U(x) = y£vc(x).

(2.2)

cec

G is the collection of all cliques in (S, G) (collections of sites such that every two

sites are neighbors), and Vc(x) is a function depending only on {x3}sec. U is

known as the "energy," and has the intuitive property that the low energy states

are the more likely states under II. The normalizing constant, z, is known as

the "partition function." The Gibbs distribution arises in statistical mechanics

as the equilibrium distribution of a system with energy function U.

As a simple example (too simple to be of much use for real pictures) suppose

the pixel intensities are known, a priori, to be one of two levels, minus one

("black") or plus one ("white"). Let S be the N x N square lattice, and let G

be the neighborhood system that corresponds to nearest horizontal and vertical

neighbors:

I
o--
I
o--

I
o
I
o

For picture processing, think of N as typically 512. Suppose that the only relevant regularity is that neighboring pixels tend to have the same intensities. An "energy" consistent with this regularity is the "Ising" potential:

U(x) = -ß^2x3xt,
(s,t)

ß>0,

MARKOV RANDOM FIELD IMAGE MODELS

1499

where ]T\ ^ means summation over all neighboring pairs s,t G S. The minimum of U is achieved when x3 = xt, Vs, t G S. Under (2.1), the likely pictures are therefore the ones that respect our prior expectations; they segment into regions of constant intensities. The larger /?, the larger the typical region. Later we will discuss the issue of estimating model parameters such as ß. (With energy (2.2), II in (2.1) is called the Ising model. It models the equilibrium distribution of the spin states of the atoms in a ferromagnet. These spins tend to "line up," and hence the favored configurations contain connected regions of constant spins.)
One very good reason for using MRF priors is their Gibbs representations. Gibbs distributions are characterized by their energy functions, and these are more convenient and intuitive for modelling than working directly with probabilities. See, for example, [12, 13, 14, 18, and 23] for many more examples, and §3 below for a more complex and useful MRF model.
Degradation model. The image model is a distribution U(-) on the vector of image attributes X = {X3}aes- By design, the components of this vector contain all of the relevant information for the image processing task at hand. Hence, the goal is to estimate X. This estimation will be based upon partial or corrupted observations, and based upon the prior information. In emission tomography, X represents the spacial distribution of isotope in a target region of the body. What is actually observed is a collection of photon counts whose probability law is Poisson, with a mean function that is an attenuated radon transform of X. In the texture labelling problem, X is the pixel intensity array and a corresponding array of texture labels. Each label gives the texture type of the associated pixel. The observation is only partial: we observe the pixels, which are just the digitized picture, but not the labels. The purpose is then to estimate the labels from the picture.
The observations are related to the image process (X) by a degradation model. This models the relation between X and the observation process, say Y = {Y3}seT. For texture analysis, we will define X = (Xp,XL), where Xp is the usual grey-level pixel intensity process, and XL is an associated array of texture labels. The observed picture is just Xp, and hence Y = Xp: the degradation is a projection. More typically, the degradation involves a random component, as in the tomography setting where the observations are Poisson variables whose means are related to the image process X. A more simple, and widely studied (if unrealistic) example is additive "white" noise. Let X = {X3}ses be just the basic pixel process. In this case, T = S, and for each s E S we observe Y3 = X3-\-r}3 where, for example, {rf3}3£s is Gaussian with independent components, having means 0 and variances a2.
Formally, the degradation model is a conditional probability distribution, or density, for Y given X: U(y\x). If the degradation is just added "white noise," as in the above example, then

n(^) = (^)H/2exp{-2^E^-^2}-

1500

S. GEMAN AND C. GRAFFIGNE

For labelling textures, the degradation is deterministic: n(y|a;) is concentrated on y = xp, where x = (xp, xL) has both pixel and label components.
Posterior distribution. This is the conditional distribution on the image process X given the observation process Y. This "posterior" or "a posteriori" distribution contains the information relevant to the image restoration or image analysis task. Given an observation Y = y, and assuming the image model (H(x)) and degradation model (n(2/|z)), the posterior distribution reveals the likely and unlikely states of the "true" (unobserved) image X. Having constructed X to contain all relevant image attributes, such as locations of boundaries, labels of objects or textures, and so on, the posterior distribution comes to play the fundamental role in our approach to image processing.
The posterior distribution is easily derived from "Bayes's rule":
,,<,,,,_ üöggfe).

The denominator, U(y), is difficult to evaluate. It derives from the prior and degradation models by integration: U(y) = fxU(y\x)U(dx), but the formula is computationally intractable. Happily, our analysis of the posterior distribution will require only ratios, not absolute probabilities. Since y is fixed by observation, 1/n(y) is a constant that can be ignored (see paragraph below on "computing").
As an example we consider the simple "Ising model" prior, with observations corrupted by additive white noise. Then

U(x)

= - exp < -ß
Z{

] T x3xt >
<-,*> J

and

The posterior distribution is then
U{x\y) = -- exp l -ß J2 xsXt ~ 5Z3 E f o « ~ Xa^ f ·
We denote by zp the normalizing constant for the posterior distribution. Of course, it depends upon y, but the latter is fixed. Notice that the posterior distribution is again a MRF. In the case of additive white noise, the neighborhood system of the posterior distribution is that of the prior, and hence local. For a wide class of useful degradation models, including combinations of blur, added or multiplicative "colored noise," and a variety of nonlinear transformations, the posterior distribution is a MRF with a more or less local graph structure. This is convenient for our computational schemes, as we shall see shortly. We should note, however, that exceptions occur. In tomography, for example, the posterior distribution is associated with a highly nonlocal graph. This situation incurs a high computational cost (see [14] for more details).

MARKOV RANDOM FIELD IMAGE MODELS

1501

MAP estimate. In our framework, image processing amounts to choosing a particular image x, given an observation Y = y. A sensible, and suitably-defined optimal, choice is the "maximum a posteriori," or "MAP" estimate: choose x to maximize n(z|2/). The MAP estimate chooses the most likely x, given the observation. In most applications, our goal is to identify the MAP estimate, or a suitable approximation. However, in some settings other estimators are more appropriate. We have found, for example, that the posterior mean (J xU(dx\y)) is more effective for tomography, at least in our experiments. Here, we concentrate on MAP estimation.
In most applications we cannot hope to identify the true maximum a posteriori image vector x. To appreciate the computational difficulty, consider again the Ising model with added white noise:

U(x\y) = -- exp \ -ß V x3xt - - ^ V ( j / a - x3)2 \ .

Zp [ <-.t>

l° ses

J

(2.3)

This is to be maximized over all possible vectors x = {x3}3es £ {--1>1}'S'with \S\ ~ 105, brute force approaches are intractable; instead, we will employ
a Monte Carlo algorithm which gives adequate approximations.
Maximizing (2.3) amounts to minimizing

Up(x) = -ßYl XaXt ~ 2Ö2 ] C ^ f l " x*)2
(s,t> s£S

which might be thought of as the "posterior energy." (As with zv, the fixed observation y is suppressed in the notation Up(x).) More generally, we write the posterior distribution as

±eM-Up(x)}

(2.4)

zp

and characterize the MAP estimator as the solution to the problem "choose x

to minimize Uv(x)." The utility of this point of view is that it suggests a further analogy to statistical mechanics, and a computation scheme for approximating

the MAP estimate, which we shall now describe.

Computing. Pretend that (2.4) is the equilibrium Gibbs distribution of a

real system. Recall that MAP estimation amounts to finding a minimal energy

state. For many physical systems the low energy states are the most ordered,

and these often have desirable properties. The state of silicon suitable for wafer

manufacturing, for example, is a low energy state. Physical chemists achieve low

energy states by heating and then slowly cooling a substance. This procedure

is called annealing. Cerny [5] and Kirkpatrick [21] suggest searching for good

minimizers of U(-) by simulating the dynamics of annealing, with U playing

the role of energy for an (imagined) physical system. In our image processing

experiments, we often use "simulated annealing" to find an approximation to

the MAP estimator.

1502

S. GEMAN AND C. GRAFFIGNE

Dynamics are simulated by producing a Markov chain, X(1),X(2),... with transition probabilities chosen so that the equilibrium distribution is the posterior (Gibbs) distribution (2.4). One way to do this is with the "Metropolis algorithm" [24]. More convenient for image processing is a variation we call stochastic relaxation. The full story can be found in [13 and 18]. Briefly, in stochastic relaxation we choose a sequence of sites s(l), s ( 2 ) , . . . G S such that each site in S is "visited" infinitely often. If X(t) = x, say, then Xr(t + 1) = xr, Vr ^ s(t), r G S, and X3^ (t + 1) is a sample from
U(X3{t) = -\Xr = xr,r^s(t)),
the conditional distribution on X3^ given Xr -- xr Vr ^ s(t). By the Markov property,
U(Xs{t) = -\Xr = xr,r^ s(t)) = U(X3{t) = -\Xr =xr,re Gps{t))

where {G^}3^s is the posterior neighborhood system, determined by the posterior energy Up(-). The prior distributions that we have experimented with have mostly had local neighborhood systems, and usually the posterior neighborhood system is also more or less local as well. This means that |G^(tJ is small, and this makes it relatively easy to generate, Monte Carlo, X(t + 1) from X(t). In fact, if fi is the range of X3(t), then

=wc>-i*-*.'* « W - E M U <2-5)

where

r = s(t),

(a,a(t)Xjr = S
I xr,

rÏ

s(t).

Notice that (fortunately!) there is no need to compute the posterior partition

function zp. Also, the expression on the right-hand side of (2.5) involves only

those potential terms associated with cliques containing s(t), since all other terms

are the same in the numerator and the denominator.

To simulate annealing, we introduce an artificial "temperature" into the pos-

terior distribution:

UT{X)

_

exp{-Up(x)/T} ztf) ·

As T --· 0,nr(-) concentrates on low energy states of Up. To actually find these states, we run the stochastic relaxation algorithm while slowly lowering the temperature. Thus T = T(t), and T(t) | 0. n T ( t ) ( - ) replaces n(-) in - computing--the traoisition-X(^)^=^-=X(M=-l) In-4l3]=-we=showed=thaty=undersuitable hypotheses on the sequence of site visits, s(l), s(2),... :

If T(t) > c/(l + log(l + *)), T(t) | 0, then for all c sufficiently large X(t) converges weakly to the distribution concentrating uniformly on {x: U(x) = min^ U(y)}.

More recently, our theorem has been improved upon by many authors. In particular, the smallest constant c which guarantees convergence of the annealing

MARKOV RANDOM FIELD IMAGE MODELS

1503

algorithm to a global minimum can be specified in terms of the energy function Up (see [15 and 19]). Also, see Gidas [16] for some ideas about faster annealing via "renormalization group" methods.
In the experiments with texture to be described here, MAP estimates are approximated by using the annealing algorithm. This involves Monte Carlo computer-generation of the sequence X(l), X(2),..,, terminating when the state ceases to change substantially.

3. Texture segmentation. Texture synthesis refers to computer generation

of homogeneous patterns, usually intended to match a natural texture such as

wood, grass, or sand. In many instances, Markov random fields provide good

models, and Metropolis-like Monte Carlo methods yield respectable facsimiles

of the real textures [8, 9]. Here we combine MRF texture models, for the pixel

process, with an Ising-like "texture label process," in order to segment and label

a scene consisting of patches of natural textures. The image model thereby

involves both a pixel process, of grey level intensities, and a label process, whose

components identify the texture type of each picture element in the scene. Our

approach is similar to those of Derin and Elliott [9] and Cohen and Cooper [7],

especially in our use of the two-tiered image model.

Image model. The image process comprises a pixel process and a label pro-

cess, X = {XP,XL}. As usual, the pixes sites form an TV x N square lattice,

say Sp. For each pixel site there is a corresponding label site, and thus the

graph associated with the image model has sites S = Sp U SL, where SL is

just a copy of Sp. The elements of Sp and SL index the components of Xp

and XL, respectively, so that Xp = {XP}3ESP

and XL = {Xg}3eSL. In the

experiments reported here, the pixels were allowed sixteen possible grey levels

Xp G { 0 , 1 , . . . , 15}, Vs G Sp, whereas the range of the labels depended upon

the actual number of textures in the scene, thus assuming this number to be

known a priori. Let M be the number of textures that are to be modelled. Then Xï G { 1 , 2 , . . , M } , V s 6 S L .

We shall develop the image model by first assuming that the texture type

is fixed, say "/" and constant over the scene. Conditioned on Xg = l G { 1 , 2 , . . . , M } , Vs G SL, the process Xp is a Markov random field:

I l O r ' W = l, 5 e SL)

=

4
Z

(T0

exp{-E/«)

(xp)}

where z^ is the usual normalizing constant z^ = YjXp exp{-E/W(zp)}. Only pair-cliques appear in the energy 17 W. There are six types of pair-cliques, as shown in Figure 1. These we index by i G {1,2,3,4, 5,6}. We denote by (s, t)i a

pair of sites s,t which form a type i clique, and by £)/s tv. the summation over all such pairs. With these conventions, the (conditional) energy is

tf(,)(*P) = - £ E ' i ' ^ f - s f ) , *(A) = (l + (|A|/,5)2)-1 (3.1)

1504

S. GEMAN AND G. GRAFFIGNE

for some fixed 8 > 0. Notice that ®(xf -- xf) is larger when xf = xf, and is monotonie in \xf -- xf\. Because of this, the texture-dependent parameters $i , . . . , 0Q determine the degree to which neighboring pixels, of a particular type of pair-clique, will tend to have similar grey-levels. In face, if 0\ ' > 0, then for texture "/" we expect pixel pairs x3 and xt, of clique type i, to typically have similar intensities. If 0[' < 0 then the tendency is to be different. Of course, these simple rules are complicated by the actions of the other five types of pair-cliques.

O JO O O

o o x> o
FIGURE 1. Pair-cliques for texture model.

The parameters 0\\i = 1,2,..., 6, / = 1,2,..., M, are estimated from pictures of the M textures, as explained in the following section (§4). On the other hand, <È>, and indeed the neighborhood structure, is ad hoc. We have used $ extensively in other applications in which our main concern is with the difference of intensities between neighboring pixels. Of course the quadratic, $(A) = A2, is simpler, but it unduly penalizes large differences. Having modeled the M textures, we now construct a composite Markov random field which accounts for both texture labels, XL = {Xf, s G SL}, and grey-levels, Xp = {Xf, s G Sp}. The joint distribution is

n ( x " = xpfXL = xL) = «Pi-UiiW-u^)}

(3>2)

in which U2 promotes label bonding (we expect the textures to appear in patches

rather than interspersed) and U\ specifies the interaction between labels and

intensities. Specifically, we employ a simple Ising-type potential for the labels:

u2(xL) = -ßY,^=x-+E wtâ)> P > 0 -
[stt] ses

(3-3)

Here ß determines the degree of clustering, [s, t] indicates a pair of nearest hor-

izontal or vertical neighbors, and w(-) is adjusted to eliminate bias in the label

probabilities (more on the choice of w(-) later).

To describe the interaction between labels and pixels we introduce the symbols

i~i,T2,... ,TQ to represent the lattice vectors associated with the 6 pair-cliques

(Figure, 1 ). Thus s and s-r-n are neighbors, constituting a pair with clique type

i. The interaction is then given in terms of pixel-based contributions,

H(xp, I, s) = - X X ^ O z f - <+Ti) + * ( * f - z f - r j }
i=l
and local sums of these called block-based contributions,
Z(xp, I, s) = - Y^ H(xp, I, t).
a teN3

(3-4) (3.5)

MARKOV RANDOM FIELD IMAGE MODELS

1505

Here, N3 is a block of sites centered at s (5 by 5 in all of our experiments), and the constant a is adjusted so that the sum of all block-based contributions reduces to U® (see (3.1)):

UW(xp) = J2z(xp,l,s). ses

(3.6)

This amounts to ensuring that each pair-clique appears exactly once (a = 50, for example, when N3 is 5 by 5). In terms of (3.4) and (3.5), the "interaction energy," U\(xp,xL), is written

U1(xp,xL) = '£z(xp,xï,s). ses

(3.7)

Because of (3.6), the model is consistent with (3.1) for homogeneous textures, Xf = /, Vs G S. The idea is that each local texture label, Xf, is influenced by the pixel grey levels in a neighborhood of s.
Finally, to clarify the bias correction term w(-), we briefly examine the local characteristics of the field, specifically the conditional distributions for the labels given all the intensity data and the values of the neighboring labels. (The actual neighborhoods of the Markov random field corresponding to (3.2) can be easily inferred from (3.3) and (3.7).) The log odds of texture type k to type j is

jU(XrL = k\Xt =xLa,s£ r; Xf = xp,seS)]
g \ u(xr^ = j\X} = xi, s ? r; Xt = xf,seS)j

= Z(xpJ,r)-Z(xp,k>r)

+ ß J2 (l«f=fc-l*f=i)+ «'(?·)-«'(*)
t: [t,r]

t = i seNr

+ ß Y,
ti [t,r]

(lxL=k-lxM)-{-w(j)-w(k).

The first term imposes fidelity to the "data" xp, and the second bonds the

labels. The efficacy of the model depends on the extent to which the first term

separates the two types k and j , which can be assessed by plotting histograms for

the values of this quantity both for pure k and pure j data. A clean separation of

the histograms signifies a good discriminator. However, since we are looking at

log odds, we insist that the histograms straddle the origin, with positive (resp.

negative) values associated with texture type fc (resp. j). The function w(-)

makes this adjustment.

Degradation model. The degradation is deterministic. The observation process is the pixel process Y = Xp, and hence the degradation is just the projection (XP,XL)-+XP.

Posterior distribution. In this special case, the posterior energy is the same

as the prior energy, but some of the components are fixed. In particular,

Tl((xp,xL)\y) =

-exp{-U1(xpixL)-U2{xL)}lxP=y.

1506

S. GEMAN AND G. GRAFFIGNE

FIGURE 2. Wood on plastic background.
Equivalently, we simply use Tl(xL\xp) as the posterior distribution:
U(xL\xp) = -- exp{-^i(xp,a5L) - U2(xL)}.
Zp
MAP estimate. Given an observation, Xp = xp, we shall seek xL to minimize U1(xp,xL) + U2(xL).
Computing. We use stochastic relaxation, with simulated annealing, as described in §2. A convenient starting point is arrived at by "turning off" the Ising term in the label model (3.3): we set ß = 0. Since this is the only label/label interaction term in the model, the MAP estimate of xL, with ß = 0, is determined by (locally) optimizing x% at each s G SL. The computation time is negligible. Thereafter, we set ß to the model value (see §4) and begin stochastic relaxation. In the experiments, each site was visited about 150 times.
Experimental results. Three experiments were done on texture discrimination, based on two images with two textures each and one with four. There are four textures involved: wood, plastic, carpet, and cloth. As mentioned above, the parameters were estimated from the pure types (see §4). There was no pre- or post-processing. In particular, no effort was made to "clean-up" the boundaries, ^expecting smooth- transitionsT^Theresult^ are shown in^Figufes 2r3;^antlTM4r these correspond to (i) wood on plastic, (ii) carpet on plastic, and (iii) wood, carpet, and cloth on plastic background. In each figure, the left panel is the textured scene, and the right panel shows the segmentation, with texture labels coded by grey level. It is interesting to note that the grey-level histograms of the four textures are very similar (Figure 5); in particular, discrimination based on shading alone is virtually impossible.

MARKOV RANDOM FIELD IMAGE MODELS

1507

FIGURE 3. Carpet on plastic background.

FIGURE 4. Wood, carpet, and cloth on plastic background.

1508

S. GEMAN AND C. GRAFFIGNE

CLOTH
i n -- i -- j -- T 1 -- i -- i -- i -- i -- i 1 -- i -- | -- i -- i 1 1 1 1-

CARPET

}\ :

PLASTTC

WOOD

FIGURE 5. Grey-level histograms.
The model is not really adequate for texture synthesis; samples generated from the model do not resemble the texture very well. Evidently, the utility of Markov random field models does not depend on their capacity for simulating real-world imagery. A more serious drawback of our model is that it is dedicated to a fixed repertoire of textures, viewed at a particular orientation and at a particular magnification, or range. The problem is easier if the goal is merely segmentation, without recognition. We are experimenting with segmentation algorithms that are scale and orientation independent. Indeed, there are no texture-specific parameters. These are built upon the same modelling/computing framework.

4. Parameter estimation. Maximum pseudolikelihood. The performance of the model is not unduly sensitive to the choice of 6 (see (3.1)) or ß (see (3.3)), which were determined by trial and error. On the other hand, the pair-clique parameters 0+ , i = 1,2,..., 6, Z = 1,2,...,M, characterize the M textures, and critically determine the ability of the model to segment and label. Needless to say, these must be systematically estimated. Trial and errors not feasible. - - _---------=_ -- -- -_=--==--,,-,, « We have estimated the parameters from samples of the M textures. These "training samples" contain only one texture each, and we used just one sample for each texture. For a fixed texture, say wood, and from a single sample, say xp, the problem then is to estimate 0i, 02?···, #6 in the model
U(Xp = xp;0) = exp{-U(xp;0)} z(0)

where

MARKOV RANDOM FIELD IMAGE MODELS

1509

t = l {a,t)i
and z(e) = ^2exp{-U(xp;6)}.

(We include 0 = (0i,..., 0ß) in n, U, and Z to emphasize the dependencies on
the unknown parameters.) The standard approach is to maximize the "likelihood" : choose 0 to maximize H(xp\ 0). Of course, maximizing n is equivalent to
maximizing logn. It is easily demonstrated that the latter is concave in 0 with
gradient

V logn(^; 9) = { £ *(äf - xt) - E9 Y;*(x?-x?)

(4.1)

. <<>,t)i

(s,t)i 2 = 1 , . . . , 6

where E$[] is expectation with respect to n(-;0). This suggests a gradient ascent procedure, but the expectation EQ[-] is intractable, involving summation over the entire range of Xp. In our experiments, we used a 16 grey-level scale for the pixels, and 204 x 204 lattices: the expectation in (4.1) has 16204 terms. An alternative to brute force evaluation is to use stochastic relaxation (see §2), which produces an (asymptotically) ergodic sequence XP(1),XP(2),... for any given 0, and from which expectations can be approximated by appropriate timeaverages. This, too, is computationally intensive, but feasible. In some settings we have found no alternative, and this Monte Carlo procedure has worked well, albeit slowly (see [22]). See also Hinton and Sejnowski [20] for a closely related algorithm, used to model learning in a theory of neuron dynamics.
For homogeneous random fields, such as our image models, Besag [2, 3] has proposed an ingenious alternative to maximum likelihood, known as "maximum pseudolikelihood." The pseudolikelihood function is

PL(xp; 0)= f ] n ( * f = ^\Xr = *f, r?s;0)
aeSp\dSp
where dSp is the boundary of Sp under the neighborhood system determined by the energy U, and Sp\dSp is the complement of dSp relative to Sp. The "pseudolikelihood estimator" is the 0 that maximizes PL(xp;0). In the next few pages we shall lend some analytic support, by establishing consistency of pseudolikelihood in the "large graph" limit. But first, we emphasize the overwhelming computational advantage. As with the log likelihood function, the log pseudolikelihood function, log PL(xp\0), is concave, but this time the gradient

1510

S. GEMAN AND C. GRAFFIGNE

is directly computable: Vlog PL(xp;6)
= V EE {OEÈ*«W # - # + J + *(af-Cr,)} sescp\d\as sp u=i -l0g2«p{é^*(O-^)+*(«-af-r4)}}]

(where £ ) a is summation over pixel grey levels, zero through fifteen in our experiments)

^esp\dsp
- EeMXf - Xf+Ti) + * ( X f - Xf__Ti)|XrF = xf ,r ? s}}
This time, the expectation is tractable. The conditional distribution on Xf, given Xf = xf, r ^ s, involves only those variables xf in the neighborhood of s. Furthermore, this time summation is over the range of Xf only, which has only sixteen values. In short, the gradient of the log pseudolikelihood is directly computable, and therefore gradient ascent is feasible without resorting to timeconsuming Monte Carlo methods. For the experiments discussed in the previous section, the pair-clique parameters were estimated, for each texture type, by gradient ascent of the pseudolikelihood function.
Some modifications of maximum and pseudolikelihood have been recently introduced by Chalmond [6]. A third alternative was suggested by Derin and Elliott [9, 11], and has been studied and analyzed extensively by Possolo [26]. This involves a regression fit of the log of the local conditional probabilities, and works best when there are a small number of values in the range of the random variables. For example, the method is very effective for Ising-like models.
Consistency of pseudolikelihood. We will study parameter estimation from a single realization of a finite-graph Markov random field. The typical framework for establishing consistency is in the limit as the number of samples increases. But we have in mind estimation from a single sample of the random field, with the number of sites large (e.g., 512 x 512). To study estimation in this "large graph" setting, we will imagine a sequence of samples, X{ï)lX(2)_i_L._L_}Jfrom^a sequence of Markov random fields, H i , n 2 , . . . , in which the latter are associated with an expanding sequence of regular graphs. We will assume that the sequence of distributions of these random fields has a common unknown parameter vector 0o G Rm. We will define the pseudolikelihood estimate, 9n = 0n(X(n)), for each sample, X(n), and show that 0n --· 9Q with probability one.
The samples X(1),X(2),... need not be independent. For example, we may wish to model the observations as subsamples from a single infinite volume

MARKOV RANDOM FIELD IMAGE MODELS

1511

Gibbs state. Then, there is one infinite-volume process X, e.g., X = {X3}ses, S == {(hj)' ~~ °° < hi < °°}, and the observations are associated with increasing subsets: X(k) = {X3}aesk with, e.g. Si Ç Si Ç ··· j U f c l i ^ = S. The sequence of distributions, n i , n 2 , . . . , is the sequence of conditional distributions, on {X3}3Esk, conditioned on {X3}3eS\sk>k = 1,2, Under a suitable "homogeneity" (translation invariance) assumption for the Gibbs potential, the theorem applies, guaranteeing consistency of the pseudolikelihood estimate. This is regardless of critical phenomena, or lack of spatial stationarity, both of which can occur with infinite volume Gibbs states having translation-invariant potentials [28].
Henceforth, we specialize to regular square lattices: S will represent the ridimensionai infinite square lattice. (Generalizations are straightforward.) For each n, Sn C S is a d-dimensional cube with sides length n. On S is a translationinvariant neighborhood system G = {G3}aes (s fi G3;s G Gr <=> r G G3;s G Gr <& s + r G Gr+T Vs,r,r G S). We will assume "finite" interactions: 3R 3 s G Gr => \s - r\ < R. We will denote the subgraph of (S,G) with sites Sn by (Sn, G). Associated with each n is a Markov random field, n n , on (Sn,G). The site variables, {X3}aesn, are assumed to have common range Q, with \Q\ < 00.
The distributions Hi,n2,... are related by their dependencies on a common unknown parameter 0Q G Rm. Pseudolikelihood exploits the dependencies of local conditional probabilities on this parameter. In particular, fix n and let x G QSn, the range of the random field with distribution n n . For each s, let 3x = {xr : r G G3 fl Sn}. Actually, 3x will be treated as a vector, in which the components are placed in some arbitrary order. "Local characteristics" of Un refers to the conditional probabilities Un(X3 = x3\aX = 3x; 0o) for each s G Sn, x G QSn. The distributions Hi, n2,... are tied together by the assumption that these local characteristics, which depend upon So, are independent of s and n, for all s in the interior of Sn. More precisely, letting S® = Sn\dSn under G, we assume that there exists *(·) = (*i(-)) · · · » *m(")) s u c n t n a t

I I A = XslsX = . , ; 0O) = e x p { g 0 J ( w ) } EaGnexP{öo'*(a,^)}
for all n, s G S®, x3, and 3x. Any homogeneous field with finite interactions is suitable, regardless of boundary conditions. Examples include the Ising model, and the texture model (for a single, homogeneous texture) developed in §3.
Whenever 8 G S^,Iin(X3 = x3\3X = 3x\0) does not depend on n. Since we will only be interested in local characteristics at interior sites, we henceforth drop the subscript n when writing conditional probabilities. Given X = x, a sample from n n , the pseudolikelihood function of 0 G Rm is
PLn(x;0)= f ] u(xs\sX]0)

= yi

exp{0-V(x3,3x)}

ses° £ a e n e x P { 0 " ^ ( a , a z ) } '

1512 S. GEMAN AND C. GRAFFIGNE

The pseudolikelihood estimate is the set Mn(x), of 0 t h a t maximize PLn(x;9):

Mn(x)

= <0eRm: [

PLn(x,0)

=

(js>ueRpm PLn(x,<t>)

\ J

.

In establishing consistency for pseudolikelihood estimation we will assume iden-
tifiability, in the following sense: DEFINITION. We will say t h a t 0O G R m , is identifiable if 0 ^ 0O => 3x3i3x,
such that Tl(x3\3x;0) ^ H(x3\3xm,0o).

THEOREM (CONSISTENCY OF PSEUDOLIKELIHOOD). For each n = 1,2, . . . , let X(n) be a sample from the Markov random field Un, with local characteristics (4.2). If 0o is identifiable, then
(a) P(\ogPLn(X(n);0) is strictly concave for all n sufficiently large ) = 1; (b) P(Mn(X(n)) is a singleton for alln sufficiently large ) = 1; (cj ^ ( s u p ö e M n ( x ( n ) ) \0 - 0o\ -> 0) = 1.

REMARKS. (1) Extensions to more general graph structures and interaction potentials are possible, and mostly routine.
(2) More relevant t o the problem of estimating 0O from a sample X(n), with n large, is the following immediate corollary:

lim P [ supp \0-0-o\0>oe] n--·oo \eeMn(x(n))
PROOF OF THEOREM. Let Nn = \S%\,

=0 Ve > 0.

Nn(ß) =

#{seSn):3X(n)=ß},

and Nn(a,ß) = #{* G S£: X3(n) = a, 3X(n) = /?},
using a and ß as generic elements of Q and filGa', respectively. The proof can be divided into five steps, which we now state as lemmas.

LEMMA 1. liminfn_>oo Nn(ß)/Nn > 0 a.s., Vß. LEMMA 2. limn-.oo Nn(a, ß)/Nn(ß) = U(a\ß; 0O) a.s., Va, ß. LEMMA 3. Let

K(6) = ^-{log PLn(X(n);0) -log PLn(X(n);0o)}

2 ^ NJVnL Z^ ^f NNn((Rß\) 106gn(a|/3;Ö0)' P{Fn{-) is strictly concave for all n sufficiently large ) = 1.
L E M M A 4 . Let

ß

MARKOV RANDOM FIELD IMAGE MODELS

1513

(a) With probability one, Ve > 0 36 > 0 3

limsup sup

sup (j>tH(Gn(0))(j) < -- 6

n-*oo |0-0o|<£0eflm, |0|=1

where H(Gn(0)) is the matrix of second derivatives respect to 0.
(b)Gn(0)<OV0,n. (c)Gn(0o) = OVn.

(Hessian)

of Gn(0)

with

LEMMA 5. Ve > 0,

lim sup \Fn(0) -Gn(0)\=O
n-+00\0-6o\<e

a.s.

With these pieces in place, we complete the proof as follows. Fix e > 0. From Lemma 4, conclude that

liminf inf (Gn(0o) - Gn(0)) > 0 a.s.

n - o o \e-0o\=eK

V'

V"

(4.3)

Since Fn is uniformly approximated by Gn (in the sense of Lemma 5), (4.3) also holds for Fn:
liminf inf (Fn(0o) - Fn(0)) > 0 a.s.

Since Fn is eventually strictly concave (Lemma 3), it eventually achieves its maximum, uniquely, in {0: \0 - 0O| < £}· Finally, since logPLn(X(n);0) = NnFn(0)+logPLn(X(n)] 0o), these same statements apply to logPLn(X(n); 0).
We now proceed to prove Lemmas 1-5. PROOF OF LEMMA 1. The first two lemmas are based on the following version of the "strong law of large numbers" :

PROPOSITION. For eachn= 1 , 2 , . . . , let Z1(n),Z2(n),.. .,Zmn(n) dom variables andY(n) be a random vector. Assume
(1) liminfn^oo mn/n > 0. (2) Z\(n),..., Zmn(n) are conditionally independent, given Y(n). (3) | ^ ( n ) | < B < o o Vi,n. Then

be ran-

mn i = i

0 a.s.

PROOF. The methods here are standard. We will provide an outline only.

Fix e > 0 and let An be the event

mn
ln= j ^-^2(Z^n)-E[Zi(n)\Y(n)]) >e

i=l

Then the usual exponential bounds (but derived by first conditioning on Y(n)) give P(An) = o(l/Cmn) for some G > 1. The rest follows from the Borei-Cantelli lemma: P(An infinitely often ) = 0.
Now back to the proof of Lemma 1: For any s G S, let

B3 = d{(8UG3)c} = {r: 3t e (sUG3)r EGt,

rfi(sUG3)},

1514

S. GEMAN AND C. GRAFFIGNE

i.e., the neighborhood of s U G3. For each n, choose si, 52,..., smn G Sn such that
(1) liminîn^oo mn/Nn > 0, (2) B3. QSn,i = l,...,mn, (3)i^j-+(siUG3i)nB3j=0, (e.g., regularly partition Sn into large cubes, with sizes independent of n, and big enough to accommodate s U Ga U B3, for some s). Fix ß and let Y(n) = {Xa(n) : s G U^Ti ft,}, and Z,(n) = la.x(n)=ß. By the Markov property, Zi(n),..., Zmn(n) are conditionally independent, given y(n). Hence, by the proposition,

1 5^"

mi«-

£(^
i=i

(n

)-

2

W

»

)

ly(»)

])

Using again the Markov property,

0 a.s.

E[Zi(n)\Y(n)] = U(3iX(n) = ß\X3(n),se B3i;0o) *

which can have only a finite number of possible values (corresponding to the |n|lBa*l configurations of {Xs(n)}sßa), all of which are positive. Hence, for
some e > 0,

1 -_
-- Y,E\Zi(n)\Y(n)]>e,
m"i=1

Vn,

and 1 I*1"
lim inf -- y ^ ZAn) > e a.s. mn x
Since JVn(/?) > £TM" ^(n)>it; a l s ofollowsthat liminf Nn(ß)/mn >  a.s. Finally, since liminf rnn/Nn > 0, liminf Nn(ß)/Nn > liminf Nn(ß)/mn · liminf mn/Nn >0.

PROOF OF LEMMA 2. Let G = {c* : i = 1,..., nc} be a coloring of (5, G). In other words, ci,C2,...,cUc partition S, and r,s G Ci --> r £ Gs. Because (S, G) is regular, we can assume that G is chosen so that liminf |S° fl Ci\/Nn > 0, i = l,...,nc.

For each i G { 1 , . . . , nc} define

Nn(ß\ Ci) = #{* eSZncn aX(n) = ß}, Nn(a, /?; a) = #{5 G S% fl c< : Xa(n) = a,3X(n) = ß}.
Fix i G { 1 , . . . , nc}, a and /?, and let
Za(n) = lx8(n)=a;ax(n)=/3 for each seS%n Ci. Let Bn = 9{(S°nci)c} (the neighborhood of S^PiCi) and let F(n) = {X3(n) : s G Bn}. Given F(n), the random variables Z3(n),s G S° H Ci, are independent
*It is well known that the local characteristics (4.2) determine these conditional probabilities as well. Hence, this conditional distribution is independent of n.

MARKOV RANDOM FIELD IMAGE MODELS

1515

(Markov property). By the proposition

1

£
IsSncil ses°nci

(Z.{n)-E[Za(n)\Y(n)))

0 a.s.

Using again the Markov property: ü7[.£a(n)|Y(n)] = U(a\ß;6o)l,x{n)=ß- Since

J2 Za(n) = Nn{a,ß;ci) and ^ l.x(n)=/? = Nnißw),

aes°r\ci

sS°nc(

\SZnci\ ANn{a,ß;ci)-U(a\ß;eo)-Nn(ß;Ci)\^0

Finally, recalling that liminf Nn(ß)/Nn > 0, a.s.:

Nn(a,ß) Nn(ß)

-U(a\ß;9o)

E"" Nn(a,ß;ci)

jrf.o.f,

^\KN^n"niß\ m) Tnifl)

a.s.

^tEZ MN \Nn(a,ß;ci)-n(a\ß;90)Nn{ß;ci)\

Nn
« 5JT3[·Nn{ß)

\s°ng\
Nn

|Sgnc<|*ANn(a,ß;ci)-U(a\ß;e0)Nn(ß;ci)\

--· 0 a.s.

PROOF OF LEMMA 3. Let H(Fn(0)) be the Hessian (matrix) of Fn(0), and let <f> G Rm. By routine calculation, we derive

4>*H(Fn($))4>

^NnN(ßn(ß) EâenOft · W&.fl - W ( " , / ? ) l f l ) ) 2 e*p{0 · V>(â, 0)}

p -·»

E*en«P{*-tf («>/?)}

where ^ö[-|^] is expectation on fi with respect to U(-\ß;0). Obviously,

and hence Fn(0) is concave. By Lemma 1, with probability one, infy Nn(ß)/Nn > 0 for all n sufficiently large. Suppose mfßNn(ß)/Nn > 0 and ^lH(Fn(0))^ = 0 for some 0 and <j> ^ 0. Then, for all a and ß,<j) · ^(ä,/9) = Bff[^(a,/9)|/?]. In particular, for every ß, (j> · ip(o>,ß) is independent of a. This implies that ü(a\ß\0 + <£) = n(a|/?;0o) for all a and /?, which contradicts the identifiability assumption. Hence Fn(0) is strictly concave whenever mfßNn(ß)/Nn > 0.
PROOF OF LEMMA 4. By the same argument used for Lemma 3, Gn(0) is strictly concave, whenever mißNn(ß)/Nn > 0. By Lemma 1, with probability one, there is a f > 0 such that inf/j Nn(ß)/Nn > ç for all n sufficiently large. Since (j)tH(Gn(0))^> is jointly continuous in <j>,0, and the finite collection of variables Nn(ß)/Nn, it must achieve its maximum on the compact set \(ß\ = 1, \0 -- 0Q\ < e,

1516

S. GEMAN AND C. GRAFFIGNE

and Nn(ß)/Nn G [ç, 1] for all ß. Part (a) of Lemma 4 now follows from the strict concavity of Gn(0).
For part (b), apply Jensen's inequality:

= l o g £ l I ( a | / ? ; 0 ) = logl = O.

Part (c) follows immediately from the expression for
PROOF OF LEMMA 5.
lim sup sup \Fn(0) - Gn(0)\
n->oo |0-0o|<e

Gn(0).

= lim sup sup
n->oo \0-9Q\<e

N (a,ß)< |0| sup

n(a|/?;0 )| ,,-oo ,ßl o S

TT/^I/?

/ J ·>

limSUPSUP

a,/5,|0-0o|<£

oa

n
Nn(ß)

-H(a\ß;ß0)

By Lemma 2,

lim sup sup
n-»oo a,/?

Nn(<*,ß) W,,(/3)

-U(a\ß;0o)

= 0 a.s.

Since n(a|/3; 9) ^ 0 for any a,ß,ö G Rm, and is continuous in 0 for each of the

finite numbers of a 6 fi, ß G filG"l,

is finite.

n(a|)M)

sup
a,/3,|0-0o|<£

log: 'n(a|/3;0o)

ACKNOWLEDGEMENTS. This work has benefited from a host of suggestions

by D. Geman and B. Gidas. D. Geman has experimented extensively with MRF

texture models, and with pseudolikelihood parameter estimation, and has shared

his conclusions. B. Gidas has shared his insight and knowledge of Markov random

fields, especially concerning the vagaries of infinite volume Gibbs states.

REFERENCES

1. E. Aarts and P. van Laarhoven, Simulated annealing: a pedestrian review of the

theory and some applications, NATO Advanced Study Institute on Pattern Recognition:

Theory and Applications, Spa, Belgium, June 1986.

2. J. Besag,, SpatiaLinteraction

and-thz-statistical^analysis^af^lattice^systems^Cwith^

discussion), J. Roy. Statist. Soc. Ser. B 36 (1974), 192-236.

3. , Statistical analysis of non-lattice data, The Statistician 24 (1975), 179-195.

4. , On the statistical analysis of dirty pictures (with discussion), J. Roy. Statist.

Soc. Ser. B 48 (1986).

5. V. Cerny, A thermodynamical approach to the travelling salesman problem: an ef-

ficient simulation algorithm, Preprint, Inst. Phys. and Biophys., Comenius Univ., Bratislava,

1982.

6. B. Chalmond, Image restoration using an estimated Markov model, Preprint, Math-

ematics Dept., University of Paris-Sud, Orsay, 1986.

MARKOV RANDOM FIELD IMAGE MODELS

1517

7. F. S. Cohen and D. B. Cooper, Simple parallel hierarchical and relaxation algorithms for segmenting noncausal Markovian random fields, IEEE Trans. Pattern Anal. Machine Intell., PAMI-9 (1987), pp. 195-219.
8. G. R. Cross and A. K. Jain, Markov random field texture models, IEEE Trans. Pattern Anal. Machine Intell., PAMI-5 (1983), pp. 25-40.
9. H. Derin and H. Elliott, Modelling and segmentation of noisy and textured images using Gibbs random fields, IEEE Trans. Pattern Anal. Machine Intell., PAMI-9 (1987), pp. 39-55.
10. P. A. Devijver, Hidden Markov models for speech and images, Nato Advanced Study Institute on Pattern Recognition: Theory and Applications, Spa, Belgium, June 1986.
11. H. Elliott and H. Derin, Modeling and segmentation of noisy and textured images using Gibbs random fields, Tech. Report ECE-UMASS-SE84-15, Dept. Elee. Comput. Eng., Univ. of Massachusetts, Amherst, Mass.
12. D. Geman, S. Geman, and C. Graffigne, Locating texture and object boundaries, Pattern Recognition Theory and Application (P. Devijver, ed.), NATA ASI Series, SpringerVerlag, Heidelberg, 1986.
13. S. Geman and D. Geman, Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images, IEEE Trans. Pattern Anal. Machine Intell. 6 (1984), 721-741.
14. S. Geman and D. E. McClure, Bayesian image analysis: an application to single photon emission tomography, 1985, Statistical Computing Section, Proceedings of the American Statistical Association, 1985, pp. 12-18.
15. B. Gidas, Non-stationary Markov chains and convergence of the annealing algorithm, J. Statist. Phys. 3 9 (1985), 73-131.
16. , A renormalization group approach to image processing problems, Preprint, Division of Applied Mathematics, Brown University, 1986.
17. U. Grenander, Lectures in pattern theory, vols. 1,11,111, Springer-Verlag, New York, 1976.
18. , Tutorial in pattern theory, Division of Applied Mathematics, Brown University, 1983.
19. B. Hajek, Cooling schedules for optimal annealing, Math. Oper. Res. (to appear). 20. G. E. Hinton and T. J. Sejnowski, Optimal perceptual inference, Proc. IEEE Conf. Comput. Vision Pattern Recognition, 1983. 21. S. Kirkpatrick, C. D. Gellatt, and M. P. Vecchi, Optimization by simulated annealing, Science 220 (1983), 671-680. 22. A. Lippman, A maximum entropy method for expert system construction, Ph.D. Thesis, Division of Applied Mathematics, Brown University, 1986. 23. J. Marroquin, S. Mitter, and T. Poggio, Probabilistic solution of ill-posed problems in computational vision, Artif. Intell. Lab. Tech. Report, M.I.T., 1985. 24. N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller, Equations of state calculations by fast computing machines, J. Chem. Phys. 21 (1953), 1087-1091. 25. D. W. Murray, A. Kashko, and H. Buxton, A parallel approach to the picture restoration algorithm of Geman and Geman on an SIMD machine, Preprint, 1986. 26. A. Possolo, Estimation of binary Markov random fields, Preprint, Department of Statistics, Univ. of Washington, Seattle, 1986. 27. B. D. Ripley, Statistics, images, and pattern recognition, Canad. J. Statist. 14 (1986), 83-111. 28. D. Ruelle, Thermodynamic formalism, Addison-Wesley, Reading, Mass., 1978.
B R O W N UNIVERSITY, P R O V I D E N C E , R H O D E ISLAND 02912, USA
B R O W N UNIVERSITY, P R O V I D E N C E , R H O D E I S L A N D 02912, USA

