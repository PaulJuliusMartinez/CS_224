Advances in Neural Information Processing Systems 7, eds., Cowan, J.D., Tesauro, G., and Alspector, J., MIT Press, Cambridge MA, 1995, pp633-640.
An AlternativeEMxpoedretlsfor Mixtures of

Lei Xu
Dept. of Computer Science, The Chinese University of Hong Kong Shatin, Hong Kong, Email lxu@cs.cuhk.hk

Michael I. Jordan
Dept. of Brain and Cognitive Sciences MIT
Cambridge, MA 02139

Geo rey E. Hinton
Dept. of Computer Science University of Toronto
Toronto, M5S 1A4, Canada

Abstract
An alternative model is proposed for mixtures-of-experts, by utilizing a di erent parametric form for the gating network. The modi ed model is trained by an EM algorithm. In comparison with earlier models|trained by either EM or gradient ascent|there is no need to select a learning stepsize to guarantee the convergence of the learning procedure. We report simulation experiments which show that the new architecture yields signi cantly faster convergence. We also apply the new model to two problems domains: piecewise nonlinear function approximation and combining multiple previously trained classi ers.
1 INTRODUCTION
For the mixtures of experts architecture (Jacobs, Jordan, Nowlan & Hinton, 1991), the EM algorithm decouples the learning process in a maner that ts well with the moudular structure and yields a considerably improved rate of convergence (Jordan & Jacobs, 1994). The favorable properties of EM have also been shown through the results of theoretical analyses (Jordan & Xu, 1n press Xu & Jordan 1994). One inconvenience of using EM on the mixtures of experts architecture is the non-

linearity of softmax gating network, which makes the maximization with respect to the parameters in gating network become nonlinear and unsolvable analytically even for the simplest generalized linear case. Jordan & Jacobs (1994) suggested a double-loop EM to attack the problem. An inner-loop iteration IRLS is used to solve the nonlinear optimization with considerably extra computational costs. Moreover, in order to guarantee the convergence of the inner loop, safeguard measures (e.g., appropriate choice of a step size) are required. We propose here an alternative model for mixtures-of-experts by using a di erent parametric form for the gating network. This model overcomes the disadvantage of the original model, and make the maximization with respect to the gating network solvable analytically. Thus, a single-loop EM can be used, and no learning stepsize is required to guarantee learning convergence. We report simulation experiments which show that the new architecture yields signi cantly faster convergence. We also apply the model to two problem domains. One is piecewise nonlinear function approximation with soft joints of pieces speci ed by polynomial, trigonometric, or other prespeci ed basis functions. The other is to combine classi ers developed previously|a general problem with a variety of applications (Xu et al, 1991, 1992). Xu & Jordan (1993) proposed to solve the problem by using the mixtures-of-experts architecture and suggested an EM algorithm for bypassing the di culty caused by the softmax gating networks. Here, we show that the algorithm of Xu & Jordan (1993) can be regarded as a special case of the single-loop EM given in this paper and that the single-loop EM also provides a further improvment.

2 MIXTURES-OF-EXPERTS AND EM LEARNING

The mixtures of experts model is based on the following conditional mixture:

P(yjx ) = XK gj(x )P(yjx j)

P (yjx

j)

=

j=1
(2

det

;j

);

1 2

expf; 21

y

;

fj (x

wj )]T ;j;1

y

;

fj (x

wj)]g

(1)

Xwhere
is the

consists output of

of the

jf-thj

the softmax function:

ge1Kx,paerntdnejt.coTnshiestsscoaflafrwgjgj(K1x gj(x ) = e j(x )= e i(x

f;)jgjK1=. ):

The 1

vector K is

fgji(vxenwbjy) (2)

i

In this equation, j(x ) j = 1 K are the outputs of gating network.

pPTrhoteclnedPpua(ryrea.(tm)Gjxeitv(ete)rn

), the

is estimated by Maximum Likelihood which can be made by the EM algorithm. It current estimate (k), it consists of two steps.

(ML) is an

L= iterative

X(1) E-step. For
then form a set

each of of

pair fx(t) y(t)g, compute objective functions:

hj(k)(y(t)jx(t))

=

P (jjx(t)

y(t)),

and

Qej( j) = hj(k)(y(t)jx(t)) ln P (y(t)jx(t) j) j = 1 K

t

X XQg( ) =

h(jk)(y(t)jx(t)) ln gj(k)(x(t) (k)):

tj

(3)

(2). M-step. Find a new estimate (k+1) = ff j(k+1)gKj=1 (k+1)g with:

j(k+1)

=

arg max j

Qej (

j)

j

=

1

K

(k+1) = arg max Qg( ):

(4)

In certain cases, tfhj(exmwajx)im=izawtjTionx

max j Qje( 1]. When can not be

j) can be solved by

pfejr(fxorwmje)d

is nonlinear analytically.

solving @Qej=@ with respect to

j= wj,

0, e.g., however,

Moreover, due to the nonlinearity of softmax eq.(2), max Qg( ) cannot be solved

analytically in any case. There are two possibilities for attacking these nonlinear

optimization problems. One is to make use of a conventional iterative optimization

technique (e.g., gradient ascent) to form an inner-loop iteration. The other is to

simply Usually,

nd a the

anlegworeistthimmsattehsautchpetrhfoartmQeja(

fj(ukl+l1m))axiQmjeiz(aj(tkio))n,

Qg( (k+1)) during the

Qg( (k)). M step are

referred as \EM" algorithms, and algorithms that simply increase the Q function

during the M step as \GEM" algorithms. In this paper we will further distinguish

between EM algorithms requiring and not requiring an iterative inner loop by the

single-loop EM and double-loop EM respectively.

aJod=roduabn1lea-nlodopJKaEc]Moabnsadl(g1soe9rm9it4ih)-mlicnobenaysriudfsejir(newgdjTtthxhee1Itc])earswaetitiovhfenlRioneneclauinrresaivjre(fxLj(e:a))s.t=TSqhuejTayrxepr1(oI]pRwoLsieStdh)

method to implement the inner-loop iteration. For

faonrdthfjis(xinnje)r,

Jordan loop. It

& Xu (in press) showed that can be shown that IRLS and

more general an extended

nonlinear IRLS can

bje(xused)

the extension are equivalent to

solving eq.(3) by the so-called Fisher Scoring method.

3 A NEW GATING NET AND A SINGLE-LOOP EM

tftFrijooa(rnixn(titohn)fegwmtojh)raiixsg=imnQawogljTd(mexl).od1iWm]e.lpe, Tontshheseeaibdtlneitoso,nevluwienseneeadefrooiittrhynetorhotefdhoscaouavfbsetelmesa-altoxshionampgtlaEek-Mljeo(soxoptrhEeG)MEa=nMaal.lgyjTFotoirxcirtah1lsmi]nsoagflnlouedr-loop EM, convergence is guaranteed automatically without setting any parameters or restricting the initial conditions. For double-loop EM, the inner-loop iteration can increase the computational costs considerably (e.g., the IRLS loop of Jordan & Jacobs,1994). Moreover, in order to guarantee the convergence of the inner loop, safeguard measures (e.g., appropriate choice of a step size) are required. This can also increase computing costs. For a GEM algorithm, a new estimate that reduces \Q" functions actually needs an ascent nonlinear optimization technique itself.

To overcome this disadvantage of the softmax-based gating net, we propose the

following

modi ed gj(x

gating network: ) = jP (xj j)=

P i

iP (xj

i)

P j

j=1

j

0

P (xj j) = aj( j);1bj(x) expfcj( j)T tj(x)g

(5)

where = exponential

ffamj ilyj.

j= The

1 most

coKmgm, otnheexPa(mxpj lej)i'ss

are the

density functions Gaussian:

from

the

P (xj j) = (2

det

j);

1 2

expf; 21

(x

;

mj )T

j;1(x ; mj)g

(6)

tInheeqp.a(r5t)i,tgijgo(jnx(xco)rr)=eissPpao(cnjtjduxian)lg=lytothjtePhp(exojjs-ttjeh)r=ieoPxrip(xeprrto)bnaebtP,ilo(itxbytaPi)n(=jedjxX)frtohmaitPBx(axyijsesai')s:rsuiglen:ed(7to)

i

Inserting this gj(x

)

into the P (yjx

m) o=dXelj eq.(Pj1P()x,(xwje)jg)ePt(yjx

j):

(8)

If we again

directly do ML estimate on nd that the maximization

this P max

Q(ygjx(

)

) and derive an EM algorithm, cannot be solved analytically.

we To

avoid this di

culty, we rewrite P(y x) = P(yjx

eq.(8) into: )P (x ) =

X

jP (xj j)P (yjx

j):

(9)

j

EpTejMhrifsoajrlsmguojgrMgoietfLhsttmhesse:taignmataattsieynmgbanmseeetdtaroincndalLe0rxe=pperrPetsetnnlenttasPt.i(oTyn(hti)fsoxcra(tjn)o)ibnteot

density. We accordingly determine the parameters made by the following the

(1) E-step. Compute

Phj(k)(y(t)jx(t)) =

j(k)P (x(t)j j(k))P (y(t)jx(t) j(k)) i i(k)P (x(t)j i(k))P (y(t)jx(t) j(k))

(10)

fTuhrtehnerletdeQcoejm( jp)osjed=in1to K to be the same as given in eq.(3), and Qg( ) can be

XQgj( j) = XXQ =

h(jk)(y(t)jx(t)) ln P (x(t)j j) j = 1 K

t t

hj(k)(y(t)jx(t)) ln j
j

with = f 1

Kg: (11)

(2). M-step. Find a new estimate for j = 1 K

Pj(k+1)

=

arg max (k+1) =

j Qej( j) arg max

j(k+1) = arg max j Q s:t: j j = 1:

Qjg( j)

(12)

The maximization for the expert nets is the same as in eq.(4). However, for the

gating net the maximizations now become analytically is from the exponential family. That is, we have:

solvable

as

long

as

P (xj

j)

P P Xj(k+1) =

t h(jk)(y(t)jx(t))tj(x(t)) t hj(k)(y(t)jx(t))

j(k+1)

=

1 N

hj(k)(y(t)jx(t)):
t

(13)

In particular, when P (xj j) is a Gaussian density, the update becomes:

P Xmj(k+1) = P X(jk+1) =

1 t h(jk)(1y(t)jx(t)) t hj(k)(y(t)jx(t))

t t

hj(k)(y(t)jx(t))x(t) hj(k)(y(t)jx(t)) x(t) ; m(jk)] x(t) ; m(jk)]T :(14)

Two issues deserve to be further emphasized:

(1) The gatting nets eq.(2) and eq.(5) lenxpbjli(cxi)tl+y scuj(chj)fTuntjc(txio)n;falnmaijly( ijn)sgt.eaIdn

become identical when j(x ) = ln j + other words, the he gatting net eq.(5) uses of implicitly the one given by a multilayer

farward networks.

(2) It follows from eq.(9) that max ln P(y x= ) = max lnP(yjx ) + ln P(xj )].

So, the solution given by eqs.(10) (11)(12)(13)(14) is actually di erent from the one

given by the original eqs.(3)(4). The fomer one tries to model both the mapping

from x to y and the input x, while the latter only model the mapping from x and

y. In fact, here we learn the paramters of the gatting net and the experts nets via

Pan(yajxsy)mimmpeltirciictalyl.rHeporweesevnetra, tiniotnheeqt.e(s9t)inogfpthhaesej,oitnhte

density P(y total output

x) which includes still follows eq.(8).

4 PIECEWISE NONLINEAR APPROXIMATION

aTphpeliesism. Wplehefnoremvefrjf(fjx(jx(wxwjw)j)j=)=XcanwwjTbiexjw1ir]ijt(itxse)nn+ointwtt0hhjee=foolnwllojTywicnajg(sxef)otrh1m]at

single-loop

EM (15)

i

twamcspharhaesieseetcsriictoetaiahnllslesec.iw,amjeAst(eiihxgxne)hitosuttamehrtrdeheei-xraolptetfcuar-aeerssexstiep-pjsoeeq(ifcsxr-uiet)tasxherapieadmresterbpptcaslriaesomjmnibs(olxoeefnnd)umitecnislsascplittQmihipoeaipncotesllseyiw,cnmnamirisone(emajnbxttiesrxajils1gpQo)etileeocejvcrom(eemsdwjeirs)t(iarsjxniejcar1x1p=lay1op)tl1piycrrxnaoiordlxldmyim.Ki0arOa,iltniiainnoepnpweu0sqrs..hoe.(ifxcI3uihn)-l

5 MULTI-CLASSIFIERS COMBINATION

Given pattern x ej produces

aclnasosuetspCuit

i=1 Pj(yjx)

Pj(yjx) = pj(1jx)

pj

M, (M

we jx)]

consider classi pj(ijx) 0

ers
X

ej that for pj(ijx) = 1

each

input (16)

i

The problem of Combining to give a summay P(yjx).

Multiple Classi This is general

ers (CMC) is problem with

tmoacnoymabpinpelitchaetisoenPsji(nyjpxa)t's-

tern recognition (Xu et al, 1991, 1992). Xu & Jordan (1993) proposed to solve CMC problem by regarding the problem as a special example of the mixture density problem eq.(1) with Pj(yjx)'s known and only the gating net gj(x ) to be learned. In

Xu & Jordan (1993), one problem encountered was also the nonlinearity of softmax gating networks, and an algorithm was proposed to avoid the di culty.

Actually, the single-loop EM given by eq.(10) and eq.(13) can be directly

P P Pehdecuqoqisj(vkmeu.()ida1d(ety3esi(t)otohn)bbj(joxksewt(o)cht(ilo)ytv)mh(ett)he=ejstexqh(.eegt(nq)j7u).(aC(mx)1=M4eir)nC)a.PPtXo(p(xAruyr(o(sttasb)&)ujnjlxemdj(m(Jkteo))d.))r=Petdhna(aoInyntm(it(g)i1pnjjx91a(ax9(rttt3=o)i))rc=,u)Pblway(eriy,P(=wtw)(ijxilxhlP((etKt)n)(j)nx.di(i(Pntk))(tjB)xehPqiy(jak.(t()jyc)7)(ot,)thm,)iejwsxpeh(qtaGtwi.r)c()iaoh1n:u0gseI)gsfqtiiubvahwaeneiess--,

tions are actually the

Tsahmereeafosrgej,

(x we

) and P see that

same by noticing that (y(t)jx(t)) in ones given the algorithm of Xu &

j(x) and Pj(~y(t)jx(t)) there are the in Sec.3 in spite of di erent notation. Jordan (1993) is a special case of the

single-loop EM given in Sec.3.

6 SIMULATION RESULTS

We compare the performance of the EM algorithm presented earlier with the orig-

inal model of mixtures-of-experts (Jordan & Jacobs, 1994). As shown in Fig.1(a),

twngPwaee(wtoyicjnaxoggrnaesntijiedcn)toegniressqnai.Ged(t2ema,r)ua,iexsbastlci1uyah(rnxedP-igo(ifx)ev-ree=exnnpj0t)be.ryaitnnsTedqehmq.e(.o12(nd5()exe)wlwiwsita)ihGlt=ghaoliurnKisTetshaixa=rmnf12tjg]a(..ixkvTeeFswnhoejrkb)=lyteh=1aee5rqnwe.ii(txnjT6epgr)xea.srtpti1Foe]onne.rdestsFtsfhoo,oerrfeatttohhhclhdeee

log-likelihood to converge to the value of ;1271:8. These iterations require about

1 351 383 MATLAB flops. For the old algorithm, we use the IRLS algorithm given

in Jordan & Jacobs (1994) for the inner loop iteration. In experiments, we found

that it usually took a large number of iterations for the inner loop to converge.

WToesfaovuencdomthpautttahtiiosndsi,dwneoltimoibtvtihouesmlyaixnimuuemncenutmhebeorvoerfailtlerpaetrifoonrsmbayncem, abxu=t

10. can

save computation. From Fig.1(b), we see that the outer-loop converges in about

16 iterations. Each inner-loop takes 290498 flops and the entire process requires

5 312 695 flops. So, we see that the new algorithm yields a speedup of about

4 648 608=1 441 475 = 3:9. Moreover, no external adjustment is needed to ensure

the convergence of the new algorithm. But for the old one the direct use of IRLS

will make the inner- loop diverge and we need appropriately to rescale (it can be

costly) the updating stepsize of IRLS.

Figs.2(a)&(b) show the results of a simulation of a piecewise polynomial approxi-

mation problem by the approach given in Sec.4. We consider a mixture-of-experts

model with K = 2. For expert nets, each P(yjx

rwPei(gtxrhesfsjji)(oxnis

wajg)ai=n wG3ajuxss3i+anwg2ivjexn2 +bywe1qj.(x6+). has been t quite well.

wW0ej.seIen

j) is Gaussian the new gating that the higher

given by eq.(1) net eq.(5), each order nonlinear

For multi-classi er combination, the problem and data are the same as in Xu & Jordan (1993). Table 1 shows the classi cation results. Com-old and Com-new denote the method given in in Xu & Jordan (1993) and in Sec.5 respectively. We

tsheeatthCaotmb;othneiwmpimropvreovtheseCcloamssi;coaltdi.on rate of each individual considerably and

Training set

Classifer 89:9%

e1

Classifer 93:3%

e1

Com ; old 98:6%

Com ; new 99:4%

Testing set 89:2%

92:7% 98:0% 99:0%

Table 1 An Comparison on the correct classi cation rates

7 REMARKS

Recently, Ghahramani & Jordan (1994) propose to solve function approximation

vliinaeaesrtfimj (axtiwngj )jo=inwt jTdexns1it]yanbdasGedauosnsitahne

mixture P (xj j)

Gaussians.

In

the

special

case

of

with equal priors, the method given in sec.3 provides the same result as Ghahramani

wwfH&oeiortlJwhlpoeairrvedseceasertpnw,hetei(chs1tece9atnm9soo4een)wtflhoaijnrol,tedhcaaoonormudfagtbphahippinstrpihonplxeigaeipsmpmeatarurotaalitmtolish-noceeltoaaecrpsrasipmszileaioeetrtsirehosta.ngotseFnnPouoefrnr(ttaylhhilnleexyrejmatfwrjjoo(frxjemj(,)xwewitjsweh) onjlt)idohkts=aetaGtwriosaejTupndsooisinjina(elxtinrne),oenuaa1ttsr.]

that the methods proposed in secs.3 & 4 can also be extended to the hierarchical

architecture (Jacobs&Jordan, 1993) so that single-loop EM can be used to facilitate its training.

References
Ghahramani, Z, and Jordan, M.I.(1994), Function approximation via density estimation using the EM approach,Advances in NIPS 6, eds., Cowan, J.D., Tesauro, G., and Alspector, J., San Mateo: Morgan Kaufmann Pub., CA, 1994. Jacobs, R.A., Jordan, M.I., Nowlan, S.J., and Hinton, G.E., (1991), Adaptive mixtures of local experts, Neural Computation, 3, pp 79-87. Jordan, M.I. and Jacobs, R.A. (1994), Hierarchical mixtures of experts and the EM algorithm. Neural Computation 6, 181-214. Jordan, M.I. and Xu, L. (in press), Convergence results for the EM approach to mixturesof-experts architectures, Neural Networks. Xu, L., Krzyzak A., and Suen, C.Y. (1991), ` Associative Switch for Combining Multiple Classi ers, Proc. of 1991 IJCNN, Seattle, July, Vol. I, 1991, pp43-48. Xu, L., Krzyzak A., and Suen, C.Y. (1992), Several Methods for Combining Multiple Classi ers and Their Applications in Handwritten Character Recognition, IEEE Trans. on ,SMC Vol. SMC-22, No.3, pp418-435. Xu, L., and Jordan, M.I. (1993), EM Learning on A Generalized Finite Mixture Model for Combining Multiple Classi ers, Proceedings of World Congress on Neural Networks, Portland, OR, Vol. IV, 1993, pp227-230. Xu, L., and Jordan, M.I. (1994), On Convergence Properties of the EM Algorithm for Gaussian Mixtures, submitted to Neural Computation.

y the log-likelihood

7

..

6 5 4 3

....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................

.

2 1 0

... ........

.

. ..
...

.......................

...... .

. ..

................

. .. .. .

................. .

.

..................................................................

. . . ... .. .

.. ... .........

. .................
.

..................... ..

... .. ....

. .
...

......... .

-1 .

-1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4

x
(a)

-1200 -1400 -1600 -1800 -2000 -2200 -2400 -2600 -2800 -3000 -3200
0 2 4 6 8 10 12 14 16 18 20 the learning steps
(b)

TFw2ihigteh=urtpwe0ro:1i7o:5lri,(nawe)1sh1t=eh0rre00o0:xu2sg5aihsamntuphdnleieyfscol=rformuoadm10rsaxyan+rd=eoamt20ah1+evxae"+srtiaaiamb102la=e+tea0"dn:8dam1azo2=dise=0lsf0:r82oo:fm4at2wxG=oa2eu0xs:14psiea4xrn]t2wnNiet(;th0s1.p0Tr1:i3:ho5)er.] ts obtained by the two learning algorithms are almost the same. (b) The evolution of the log-likelihood. The solid line is for the modi ed learning algorithm. The dotted line is for the original learning algorithm (the outer-loop iteration) .

y the log-likelihood

8

6 4 2 0 -2 -4

........................

...........................................................................

.....................................

..........................................................

............. ...... .............................................................................................................................................................................................................................................................................................................................................. ........................................................

x104 0 -0.5 -1 -1.5 -2 -2.5 -3 -3.5

-607.95

-6 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4
x
(a)

-4 0 2 4 6 8 10 12 14 16 18 20
the learning steps
(b)

NaF11i(gx40u3]r+w0e:ai1t235h:x)p.+PriaiToe4crh+eew"2tiws=xeo203c:r6ud;,rvw1pehos1le:yrt5neh] orwxomiuitsighauhlpnartiifphooperrrmcolx1oriau=mndda0sto:i4amoranevn.adtrh(iyaea)b=el1seat0ai200mnx0d2as+tzaedamis30 pmxfrl2oeo+smdaefGrl04so+amou"fsystxiwa=2no expert nets. (b) The evolution of the log-likelihood.

