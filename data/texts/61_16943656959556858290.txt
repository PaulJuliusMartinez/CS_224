Exploration in Metric State Spaces

Sham Kakade Gatsby Unit University College London London, England

SHAM@GATSBY.UCL.AC.UK

Michael Kearns

MKEARNS@CIS.UPENN.EDU

Department of Computer and Information Science University of Pennsylvania, Philadelphia, Pennsylvania

John Langford Math. Sci. Dept. I.B.M. T. J. Watson Research Center, Yorktown Heights, NY 10598

JCL@CS.CMU.EDU

A¢  b¡ stract
We present metric- , a provably near-optimal algorithm for reinforcement learning in Markov decision processes in which there is a natural metric on the state space that allows the construc-
£  ¡tion of accurate local models. The algorithm is
a generalization of the algorithm of Kearns
 £¡and Singh, and assumes a black box for approx ¤¡imate planning. Unlike the original , metric-
finds a near optimal policy in an amount of time that does not directly depend on the size of the state space, but instead depends on the covering number of the state space. Informally, the covering number is the number of neighborhoods required for accurate local modeling.
1 Introduction, Motivation, and Background
Recent years have seen the introduction and study of a number of representational approaches to Markov Decision Processes (MDPs) with very large or infinite state spaces. These include the broad family known as function approximation, in which a parametric functional form is used to approximate value functions, and direct models of the underlying dynamics and rewards, such as factored or Dynamic Bayes Net (DBN) MDPs. For each of these approaches, there are now at least plausible heuristics, and sometimes formal analysis, for problems of planning [2] and learning.
Less studied and more elusive has been the problem of global exploration, or managing the explorationexploitation trade-off. Here the goal is to learn a (glob-
¥ally) near-optimal -step policy in an amount of time
that has no direct dependence on the state space size, but only on the complexity of the chosen representation. Global exploration was first solved in the deterministic

finite-state setting [9, 10] and then progress slowed. It is only recently that provably correct and efficient al-
 £¡gorithms for exploration in small nondeterministic state
spaces became known (such as the algorithm[4] and its generalizations[5]). This approach has been generalized to factored MDPs under certain assumptions [3], but there remain many unresolved questions regarding efficient exploration in large MDPs, including whether model-based approaches are required 1.
In general, it is intuitively clear that any general exploration algorithm has a polynomial dependence on the size of the state (see [7] for a more formal statement). Hence, to obtain near-optimal algorithms with sub-linear dependence on the
  ¡size of the state-space further assumptions and restrictions
on the MDP must be made. The factored algorithm [3] considers one restriction where the MDP are represented in terms of a factored graph (ie a dynamic Bayes net). Here, the number of steps the agent must act in the MDP in order
¥to obtain a -step near optimal policy is polynomial in the
representation size of the factored graph.
In this work, we examine the problem of exploration in environments in which there is a metric on state-action pairs with the property that "nearby" state-actions can be useful in predicting state-action dynamics. Such conditions are common for navigation or control problems, but may be more broadly applicable as well. Given sufficient "nearby" experience to predict outcomes, we have an implicit nonparametric model of the dynamics in a neighborhood of the
1Recent work on gradient methods for approximate planning ([14, 1]) do not address exploration in the strong sense of interest here, but instead examines convergence to policies which small amounts of random exploration cannot improve (local optimality). In general, effective exploration may require the careful planning of a long sequence of steps that might never be encountered by a random walk. See [8] for a further discussion.

state-action space. These implicit models can be "pieced together" and used for planning on a subset of the global space.
One natural approach in the large-state space setting is aggregate state methods which group states together and assume Markov dynamics on these aggregate states [12, 13]. Clearly, this approach is useful only if a compatible set of aggregate states can be found which preserve the Markov dynamics on these aggregate states and where the size the aggregate state space is considerably smaller than that of the underlying state space. A benefit of this approach is that planning under this model can be done with traditional dynamic programming approaches on the aggregate states. Unfortunately, in many navigation domains, it appears that nontrivial state aggregation often destroys the Markov assumption required for planning in aggregate state methods (and we provide one such example later).
The local modeling assumption is not equivalent to an aggregate state method since we do not group any states together and do not assume a Markov property holds for ag-
¢  ¡gregate states. In fact, under this assumption (as in factored ), the size of the state space is not diminished in any real way, unlike in aggregate state methods. Hence, the com-
  ¡putational problem of planning is still with us strongly. As
with factored , we assume a "black box" planning algorithm to abstract away the difficulty of planning from that of exploration. This assumption is not meant to trivialize the planning problem, but is made in order to isolate and quantify the difficulty of exploration.
Given the ability to plan, we prove that the local modeling assumption implies the time required for global exploration depends only on the metric resolution and not on the size
 ¤¡of the state space. More precisely, we give a generalization
of the algorithm for metric MDPs which learns a (glob-
¥ally) approximately optimal -step policy in time depend-
ing only on the covering numbers, a natural and standard notion of the resolution required for local modeling under the metric.
Metric MDPs are a natural complement to more direct parametric assumptions on value functions and dynamics. These results provide evidence that, as for factored environments[3], effective exploration mechanisms are available for metric MDPs.
2WbsteaeDtethwee£fio.rnpkriotLibinoeatbnthisleita¡¤ys£ ntaodnfbdAeaarsdtshsteuaMtmerDepw£P¥taisrgoeditnvtiersnengc.eainveLdaecttiin¢ on¤¡s£¦t©a¥¨§t©ea n££ d. wiFt4  niocag36r85r@7daus9nn5Brid@Adme9DecDC pDCfreC a5Fli(iEvlcle)GiiPHtdniynIQ,oM9 vR5tahD9eT%sersPu¥UWSimansVtete.erX`UptvYhsaaSlstta ar¨¡ lt!£ li#Xn"rg.ewtforaDormbedefissnttaeahrteee$&£a'%dvewe¡)t(0erhari mgl£ee2ianrcie1st---

We first formalize the assumption that there is a no-

tion ics.

Tohfuds,islteatnbce¡8¤¡ £th¥¨ca©Qt6¥ #pd e¡¨r£em©fit8s

local modeling of dynammeasure the "distance" be-

mtm¨¡ wg£ eecett©ferriy@ncp t(¤¡ ow£i.b¥eoce.©,ys¥tbab)8¡t,¡8e¡¤¤¡b£g-g£ au c©QtQ© tt#i#hodd en¨¡ ¡¨yp£ £e¥¨pdcaf©oQ© i8r¥qnW8sor. hitTrhsehqeubfrio¡8er¡¨resp£ uta¤¥ hlltleQ©s6¥¡¨tr#re£ediaq¤¡ ©fn£gugi,r©Qleeacntihndfaeotqsryuthmaaills--l

ity. This is fortunate since demanding the triangle inequal-

tity limits the applicability of the notion in several natural
scenarios. Let metric denote the time required to evaluate the metric.

tstWav hhumeeecshulepnatcrtorrhohigwcapet.tehsprtbAatrytmoc¡ n¡¤vtig£thnihu cdeia©f-metcr# eoaadfmovl¡¤s£eruotr¤¥ aav-ncin©Qasdoyl¥6cavaore¨¡ fds£er2ea--dt©fneyuv fi,t.hn¤¡to£ghaiLftteieQ©osritetsn a,tweotexhf¡)-oieuauscctllostadivbro¡¤geren¦£ere)¥itnspnhtdw©ageeuix¥srrWs-siucvzywnoedvinoteev horrf
longer a cover.

Our first assumption is that the metric permits local mod-
a  eling of dynamics of an MDP with transition model and reward function :

wriLa ¢£ filtoal¡hMrc£Qmdba§ o£gs¡8l ¢dc¨¡M£g©feM¡ cld¤¡o£Q ©fp£P§od§i£e¥q@sde pled©f¡¤gl£puifiisno¨¥v,tcaregrQ©ninabt6¥dnhcAudetasesd§MruterimwDna¡ £npaaPu srct,idictaoiotonhrnd.e,ssaP§nuwnc¡¤cTMh£he¥¤ehu ocwrteQ©.dehri¥6eaetL hltV,eeaot xfu5toimtsra§p ¢tnsou£padd¡¥tn¥s£Qaeiynn§la£ganwb¤¡c ad£g©feshlctgita©frchoethee--,
maximum running time of Model.

Thus, with a sufficient number of local state-action expe-

riences, Model can form an accurate approximation of the

£ ¡¨g£ c©flocal environment. Note that there is no requirement that a

destination state be in the neighborhood of

-- we

ask only that nearby state-actions permit generalization in

next-state distributions, not that these distributions be on

nearby states. The next subsection provides natural exam-

ples where the Local Modeling Assumption can be met, but

we expect there are many rather different ones as well.

In addition to an assumption about the ability to build local (generative) models, we need an assumption about the ability to use such models in planning.

Approximate Planning Assumption. There exists an al-
gkegt pron,alroagiwwntehhnumreepMr,wpePDea(rlrPadbe noa $,ius'%wnadth`¡nh(0itdechRhea£pogrspuitstvaniamtenteniiasn£ filag,e¥rgtseie-mt$nsute'%erenrpoas`¡ ft0( iapPv opel£ laodimncliyT ocayndf$der( ol%'h mfwgo¡`ef(hrn£oeea.sune£p0uLapnvee --rt
bound the calls to the generative model.

Note that the Local Modeling Assumption does not reduce the state space size, so for an arbitrary and large MDP, great

computational resources may be required to meet the Approximate Planning Assumption. The purpose is not to falsely diminish the difficulty of this task, but to abstract
£  ¡it away from the problem of exploration-exploitation. The
same approach was necessary in analyzing factored- .
There are at least three broad scenarios where this assumption might be met. The first is settings where specialized planning heuristics can do approximate planning due to strong parametric constraints on the state dynamics. For example, the recent work on planning heuristics for factored MDPs is of this form. The second is the sparse sampling [6] approach, in which it has been shown that the Approximate Planning Assumption can in fact be met for arbitrary finite-action MDPs by a policy that uses a generative
hmodel as a subroutine. Here the sample complexity gen is ¥exponential in per state visited (see [6]), but has no de-
pendence on the state space size. The third setting requires a regression algorithm that is capable of accurately estimating the value of a given policy. This algorithm can be used iteratively to find a near-optimal policy [8].
At a high level, then, we have introduced the notion of a metric over state-actions, an assumption that this metric permits the construction or inference of local models, and an assumption that such models permit planning. We believe these assumptions are broadly consistent with many of the current proposals on large state spaces. We now provide an example that demonstrates the role of covering numbers, and then show that these assumptions are sufficient for solving the exploration-exploitation problem in time depending not on the size of the state space, but on the (hopefully much smaller) covering numbers under the metric.

2.1 An Example

We can imagine at least two natural scenarios in which the

Local Modeling Assumption might be met. One of these

is where there is sufficient sensor information and advance

knowledge of the expected effects of actions that the local

 hmodeling assumption can be satisfied even with

. As

a simple example, people can typically predict the approxi-

mate effects of most physical actions available to them im-

mediately upon entering a room and seeing its layout and

content (e.g., if I go left I will exit through that door; if I go

straight I will hit that wall). They could not make such pre-

dictions for unfamiliar distant rooms. Consider the MDP

where the state space is the Euclidean maze world shown

in Figure 1.(a), and where the agent is equipped with a vi-

sion sensor. In this world, it is plausible that the local dy-

namics can be predicted at any "seen" location. To apply

stchihgioshitcaenbaeilstywsb eisseig,nhwt ¡c£ e¤¡ £gamcnf© ud@spt£ ¡¤¥fip£ a¥¤rcsn©Qtdx¥s8 p¡¢ech ioftyheairfwmtiheseterr.iec.NexToihtsetestohlibanvtei-tohouifss-

(a) (b) (c)
Figure 1. (a) a maze world (b) a largest minimal cover for the line-of-sight metric (c) a largest minimal cover for the line of sight + Euclidean distance metric.

metric satisfies symmetry, but not the triangle inequality
u  ¡)u(which ¢would be somewhat unnatural in this setting). For
any , the covering number is the maximum number of points which can be positioned in the space so that no pair have line-of-sight. One maximal set is given by the dots in Figure 1.(b). Note that even though this a continuous state space, the covering number is much smaller, and naturally determined by the geometric properties of the domain.

It is unrealistic to assume that local dynamics are mod-

eled at distant locations as well as near locations which im-

plies that modeling error
ab sirgehat ¡cs¡¤o£gnca©fb# lde¡¤£pa¨¥ lcteQ© rq¥ n8¤at£'ive#h b

giesurcoltiwodesadnwe¡cfi¤¡ig£tnhc e©fd@ibspc¡t¡¤a¡¤p£ £gn¥¨ccc f©Q©e#.q¥ 8dI¡¤n£¦wt¨¥ hchQ©ies6¥rcec ah sheis,

a constant controlling the rate of modeling error with Eu-

clidean distance. Using this metric, the covers shown in

Figure 1.(c) might naturally arise. Note that (in general)

we are free to use actions as well as states in defining the

metric.

 hThe above examples are applicable to the

case

of the Local Modeling Assumption. The second natural

case is the more general "learning" setting, in which the

next-state dynamics permit some parameterization that is

smooth with respect to the distance metric, thus allowing

a finite sample of an environment to provide enough data

to fit a parametric next-state distribution for the neighbor-

hood. For instance, if reward appeared stochastically in

some region, it might be necessary to visit nearby states a

number of times before this distribution is learned. Alter-

natively, the dynamics could be different in different parts

of the state space. For instance, a skier moving down a hill

has dynamics dependent on the terrain conditions, such as

slope, snow type, and other factors.

Incidentally, Figure 2 illustrates the reason why standard

state space aggregation techniques [12] do not work here. In particular, for partitioning induced by a cover on a Eu-

clidean spaces there exist "corners" where 3 (or more) sets meet. When taking actions "toward" this corner from

within one of the sets, the distribution over the next aggre-

gate state set is inherently unstable.

3 Metric-¥§¦

¢  ¡

 ¢¡The algorithm, Metric- , is a direct generalization of the

algorithm[4]. We first outline this original algorithm.

C3 C1

C2

Figure 2. An example showing how simple state space aggregation does not work because the precise location within the aggre-
gate state  ¢¡ influences the next (aggregate) state outcome of an action (to ¤  £ or ¦  ¥ ).

¢  ¡
A crucial notion in is that of a "known" state -- a state

visited often enough such that the dynamics and rewards

are accurately modeled at this state. When the agent is not

in the current set of known states, the agent wanders ran-

domly to obtain new information. While at a known state,

it must decide whether to explore or exploit -- a decision

which can be made efficiently. Intuitively, the decision to

explore is made by determining how much potential reward

the agent can obtain by "escaping" the known states to get

maximal reward elsewhere. If this number is sufficiently

large, the agent explores. This number can be computed
aby planning to "escape" in a fictitious MDP explore which   ¡provides maximal reward for entering an unknown state.
The crucial step in the proof of is showing that either

the agent exploits for near optimal reward, or it can explore

quickly, which results in increasing the size of the set of

known states. Since the size of the known set is bounded,

the algorithm eventually exploits and obtains near optimal

reward.¢  ¡ MLspwteoahertciitecaer-hilnaccMctethisoeodn¤¡ ae£phlli¥¨giasnos©wgar¥¤aiAtpp£hafs¥ me¥¤iscrwumh¡¨e£ka¡¤pp£set¥f©yix¥ oo8 bdn.mtia--fUefineenretneildaninkmgcaeetetishlnl.yee,a¤  aHasnnt¡ ey,treeopc,uaeu ridar-eac"¡¤nll£ggktoconsof© roefi wtehtfhnoxme"r-

does not explicitly enumerate this set of known states, but

rather is only able to decide if a particular state-action is

known. Thus, in the most general version of our algorithm,

our model of the MDP is represented simply by a list of all

prior experience.  ¢¡

 ¢¡

As in the original , a key step in Metric- is the cre-

ation of the known MDP -- a model for just that part of

the global MDP that we can approximate well. Here the

known MDP at any moment is given as a generative model

that "patches together" in a particular way the generative

models provided by the planning algorithm at known states.

¨¡ e£  ©fMore precisely, the approximate known MDP generative

model takes any state-action

and a flag bit exploit and

operates as follows:

¡¤£gQ© 1. If is not a known state-action, output "fail" and
halt.

2.

E¤¡ £pl¨¥sceQ© ¥)Rg£piv¥ ¥¤ec ¨¡¨¡ £gd£ ¥c q¥ ©f8
algorithm Model;

and the prior experiences

¨¡ g£ c ©fin the § -neighborhood of £  let the resulting outputs be

and

¨

to .

3.

Iafnedxpl©oit

is .

1, set ¨©

¨  and  ©

 ; otherwise ¨©



© £ ¡ £g  ©f 4.

If for some action state-action, output

,

the and

p¨ aainr d

halt.

is

itself

a

known

5. Else output a special state  and reward  and halt.

Intuitively, we have described a generative model for two MDPs with identical transition dynamics, but differing rewards according to the value of the exploit bit. In both models, all transitions that end in a state with no known ac-
tions are "redirected" to a single, special absorbing state  ,
while all other transitions of the global MDP are preserved. Thus initially the known MDP dynamics are a small subset of the global MDP, but over time may cover much or all of the global state space. For rewards, when exploit is 1, rewards from the real environment are preserved, whereas when exploit is 0, reward is obtained only at the absorbing state, thus rewarding (rapid) exploration (escape from
a known state-actions). We shall use exploit to denote the
MDP corresponding to the generative model above when
a the exploit input bit is set to 1, and explore to denote the
MDP generated by setting exploit to 0.
Note that under our assumptions, we can always simulate the approximate known MDP generative model. We can also view it as being an approximate (hence the name) generative model for what we shall call the true known MDP -- the MDP whose generative model is exactly the same as described above, except where the local modeling algo-
rs¢  iut¡mh£Qmp§ g£ tiMc o©fnod,d§eebh l¡8¡¤i s£g,paQ© ne#rddfe¨¡§ £pc¥tc¡ (£Q©t¦h¥q8a t ise§, huini tmh).epTlLiheoisscVaml aM5 y§  ¢os tdi¡ elf£ll§ibg£necgf©oAnls yaa partial model of the global MDP, but it has the true prob-
abilities for all known state-actions. We shall use exploit to denote the MDP corresponding to the generative model
aabove with a perfect Model and the exploit input bit set to
1, and explore to denote the MDP generated with a perfect Model and exploit set to 0.

 £¡
Now we outline the full Metric- algorithm. It is important to emphasize that this algorithm never needs to explic-
  ¡itly enumerate the set of known state-actions.
Algorithm Metric-
b¡  Input: ¡  ¢  , Model, Plan (Output: A policy

£1. Use random moves until encountering a state with ©at least one known action (that is, where there are at  u ¡¤g£ c f© least -close previous experiences to ).

2. Execute Plan twice, once using the generative model
 a for exploit and once using the generative model for a (explore. Let the resulting policies be exploit and ( explore, respectively.

$ %  `¡ ( R£   (3. If

explore explore

§ , execute explore for the next

¥ steps, then go to Step 1.

(4. Else, HALT and output exploit.

The claim is that this algorithm finds a near optimal policy, in sample complexity and running time that depend only on the covering number under the metric. We now turn to the analysis.
4 Metric-¥ ¦ Analysis
We first state the main theorems2 of the paper.
In the following theorems, we use:

( e a1. is an optimal policy in
¥2. is the time horizon
 u3. and are the sample complexity and precision de-
fined in the Local Modeling Assumption
g
4. is the precision defined in the Approximate Planning Assumption

5. § is an accuracy parameter
£
6. a confidence parameter.

oWTW ug hitteph ou¤¡ prut seruomab¡a¥pac4obt.i£1lioilcint(pysyS (a.idnms pua lc£eh,,CtMahofamteettprr$ ilcae% -xt¢  i`¡mt(0y¡ o)Rsh£Staulpt¢U¥sp¤§¨foi$ n3s¦ U¥%e3©a §e¨ )¡ S(G

G¥u

¡¥ ¡8

£state

e £0

£
£
,

 . £ a§n d

¥2The form of these claims differs from the original state-

¥ment because the results hold without an assumption of a mixing
MDP. Theorems similar to the original can be constructed in

the metric case by making an additional assumption of mixing.

The "mixing free" form stated here is subject to fewer assump-

tions, and therefore more general. See [7] for details.

This shows that the sample complexity (the number of ac-
 ¡)utions required) is bounded in terms of the covering number (and not the size of the state space). In addition to bounding the sample complexity, we bound the time complexity.

£  ¡Theorem 4.2 (Time Complexity) Let ! be the overall

3 Gs" am"  ple S t P`¡ t h t  ¡ #

complexity.

metric £

 !

Metric- runs in time
plan £ gen model %£ $ &!

at .

most

AaV  5fe'tow§ ¢   bl¤¡ eep£ m¥B§a£gmn aQ© us -aarpe¢  pur¤¡ o£ps¤¥xe§ fig£ muc l©fa!itni§ ontheuof,para no£  dof¡ §isf. ¡¤f£Fofrir satl lw¨¡ s£petdadQ§teersfinu £ e,.
The original Simulation Lemma for had a dependence

on the size of the state space that we cannot tolerate in our

setting, so we first need an improved version:

a  uLemma 4.3 (Simulation Lemma) If

is an -

a £approximation of , then for any initial state , any

¥ (horizon , and any policy ,

§ $ %  ¡`0( R £ $ % `¡ 0( R £pd §f u ¡ ¥ £ 

tatPapu iipnaver.ptdoseThrolusyoh7f..n.xeXFdnLi&¡mLoe5ehh rreta t(5t¢8 ib(oei6yB@B@B@B@££¨¡nn9 £pXBAABBABA999,9t7§(¨¥CCCCEh7af§h0£oXeDDDDGXX ¡HFr999,¡&§5555pa77la5aQ) e8¢§d§nrnn77§¤¡¡Xot7§d£d¢88¢y&¡¡¨b5XX 5SQ£X aa¡&&¡s8X¡¡¨¨R§55tbQ£Q£5£baS ¡¡¡¨i§§,#Xte55l&¡p£88 !ei2r5¥tXXt§7e¢1y§7h¤¡¡¤££s21Q££QeX¥pX1Ro,§§&¡ 55¡&et£b5fV5-cX787XXet!4t5h§iXX53t¤¡vh£s&¡£Qie§5tnbe8S§al5777euy&¡tt 8¢a¤¡X5e.rXXXQ£!ta!h&&&¡¡¡iS§¡¨§nn555§£Q£eias¥n!¡§5in5s88§tciedXaeot¡¡¤¤!nnQ£Q£a§aod8¢§§ p55fl,¡¨reXX lw£io!!tesr§b§§en£7 asag¥ npdXbt§&¡ehiu5lcit--@BABC D

twhere we have used the triangle inequality and linearity of
expectation. Induction on implies that:

9

I

5 ¤HI

I I

TS

%RP Q9 IQ9 5U

¨¡ £

S

21G1G1 R£

U



5S¤H

% VP Q9 fI 9 5U

¨¡ £

S

2G1 G1 1 R £

U



I I I I



u ¥W1

paths

  a  uSince the rewards in are also

I

$I
I I

%

)¡ 0(  £ 

I

 U length

% paths in

X
¥

U-accurate, 9 `X Y S  ¨¡ £ X `Y

I I I I I



ua1

The result follows using the previous two equations.

Now we restate the "Explore-or-Exploit" lemma from [4].

( eLemma 4.4 (Explore or Exploit) Let be the optimal a ( epolicy for the global MDP , and let exploit be the optimal apolicy for the true known MDP exploit described above.
£ a ¡   ¢ Then for any state of exploit and for any § ,
either

$ % ¡`( e R£p exploit exploit $% ¡)( e £   §

( e aor ¥ aat

ltehaestop§ toimf laelapvoinligcytheekxpnloorwe fnosrtateesxpilnore

has probability steps in .

of

 £¡
One subtle distinction from the original algorithm ex-

ists. Here, although the algorithm plans to reach some un-

known state, by the time this state is reached, it might actually be known due to the Local Modeling Assumption.

Note that in the maze world example, the agent might plan
to escape by moving around a corner. However, when ac-
tually executing this escape policy, the states around the
¥corner could become known before they are reached in  £¡steps, if they come into line of sight beforehand.  £¡We now establish that Metric- ceases to explore in a rea-
sonable amount of time. In the original this was a con-

sequence of the Pigeonhole Principle applied to the number

of states. A similar statement holds here, but now we use the size of a cover under the metric. It is important to note

 )¡ u that this lemma holds whether or not the covering number is known.  £¡
  )¡ uLemma 4.5 (Exploration Bound) Metric- encounters at
most unknown state-actions.

 h vProof. First, consider the ¡¤£gcf©  tas follows: the state-action vset if

case. We construct a set at time is added to the

£ ¨¡ £ ¥ c© ¥  y v¥¤rb c¡ ¤¡ g£ c ©f# d ¤¡ £ ¥ c © ¥ 8   ua1

tNote that the state at time is unknown if and only if
£ ¡¨£ ¥  © ¥  y ¦¤) earlier state-actions3 b 8¡ ¡¨£e©f@d ¨¡ £ ¥  © ¥ 8  u

¤¡ g£ c f©  vand so if

is unknown, then it is added to . Thus,

v tthe size of at time is an upper bound on the number of

unknown state-action pairs encountered by the algorithm

t vbefore time . Since no element of covers another ele-

v vment in , is minimal. In particular, if any element is

v vremoved from the set of states covered by is reduced.

t v  ¤¡ u It follows that for all the size of is less than

,

¡)uand hence the algorithm cannot encounter more than

unknown state-actions.

 For the general case, consider constructing different v   v tsets, ¢21 121 ¤ . The state action at time is added to
S v¨§ u v¨§only one of the sets if there is no -close element in .

v¨§ v¨§By an analogous argument, if a state-action is unknown, it
is added to some , and so the sum of sizes of bounds

the number of unknown state-actions encountered by the

t v©§algorithm before time . Again, by construction, each is

t v §minimal for all . Hence, the size of each is bounded by

 )¡ u  and so the number of unknown state-actions encoun-

W )¡ utered by the algorithm is bounded by

.

We now provide the proofs of the main theorems.

Proof of 4.1. The exploration bound of Lemma 4.5 implies

W ¡)uwe encounter a known state after a number of actions that

is at most

, which bounds the number of successful

exploration attempts. Each attempted exploration occurs

$ %  ¡`( Rp£   $% ¡`( R £ when ' u ¡ ¥ p a§

explore explore

§ , and so

explore explore

£ . By definition of explore, the chance of suc-

U 3 U 3 gG G F¡  0 u ¡ ¥ rcmeosssft,ulTe x§¤ ¨ p¦ loG r¨ atioGn

is


greater than §

£

£

actions successful

. Hence, at exploration of

£

Sthe state spaces occurs with a chance of error. The total

number of actions before halting is less than the sum of the

exploration actions known states and the actions taken in

unknown states.

&$ % ¡`( e R£p  $ %  u ¡`¡ ( p R £p g § ¥The decision to halt occurs when
which implies explore explore

explore explore
£ £ ¤£

§,
due

to planning and simulation error. By the Explore or Exploit

lemma

$

%

¡`(
exploit

eexploit  £ 

r $

%

`¡ ( e  £ 

§

u ¡¥

£

 

g

1

aDue to simulation and planning error in computing an op-
timal policy in exploit,
$ % ¡)( R£p exploit exploit $ % ¡)( e £   §   u ¡ ¥ £    g 1
aThe result follows since a policy in has no less reward athan in exploit.

Proof of 4.2. It is never necessary to evaluate the metric
3 G £  ¡b" e"tw een two samples more than once. There are at most S 3 G# pairs of "sa" m ples, so line 1 of Metric- take time
t Sat most metric # computation. Step 2 is executed at
most ! times since at least one transition occurs before

reentering step 2. One call to Plan requires time at most

t rh tplan £
P¡`t h t  most !

gen model so the total time spent on step 2 is at
plan £ gen model . Step takes total time at

¡ most $ &! . The result follows by adding these times.

5 Discussion

 ¢¡ ¤  ¡It is difficult to quantify the exact scaling improvements of
metric- over because the improvements are inher-

ently dependent upon the exact form of the local model-

¤¡ u  ¢  ¡ing assumption. In the extreme case where the state-action

space is continuous and

is fi£ nit¡ e, has an infi-

nite sample complexity while metric- has a finite sample

¢  ¡complexity. In less extreme cases, the advantage of metricis (naturally) less extreme. It is worth noting that the

extreme case is not too unusual. Certainly, many control

problems are modeled using continuous (or virtually con-

tinuous) para ¤m¡ eters.
The metric- analysis implies that local modeling re-

quires weaker assumptions about the behavior of the world

than state aggregation. It is not necessary for aggregations

of states to have Markovian dynamics in order to engage

in successful exploration. Instead, all that we need is the

ability to generalize via local modeling. Of course, when

aggregations of states do have Markovian dynamics, state

aggregation may work well.

[12] T. Dean and R. Given. "Model Minimization in Markov Decision Processes". In AAAI, 1997.
[13] J. Rust, "A Comparison of Policy Iteration Methods for Solving Continuous-State, Infinite-Horizon Markovian Decision Problems Using Random, Quasi-random, and Deterministic Discretizations", http://econwpa.wustl.edu:8089/eps/comp/papers/9704/9704001.ps
[14] R. Sutton, D. McAllester, S. Singh and Y. Mansour. "Policy Gradient Methods for Reinforcement Learning with Function Approximation". In NIPS 13, 2000.

References
[1] J. Baxter and P. L. Bartlett. "Direct Gradient-Based Reinforcement Learning: I. Gradient Estimation Algorithms". Technical report , Australian National University, 1999.
[2] Carlos Guestrin, Relu Patrascu, and Dale Schuurmans, "Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs", ICML 2002, pages 235-242.
[3] M. Kearns and D. Koller. "Efficient Reinforcement Learning in Factored MDPs". Proceedings of IJCAI, 1999.
[4] M. Kearns and S. Singh. "Near-Optimal Reinforcement Learning in Polynomial Time". Proceedings of ICML, 1998.
[5] R. I. Brafman and M. Tennenholtz. "R-max ­ A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning". Proceedings of IJCAI, 2001.
[6] M. Kearns, Y. Mansour, and A. Ng. "A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes". Proceedings of IJCAI, 1999.
[7] S. Kakade. Thesis. University College London. 2003.
[8] S. Kakade and J. Langford. "Approximately Optimal Approximate Reinforcement Learning". Proceedings of ICML, 2002.
[9] S. Koenig and R. Simmons. "Complexity Analysis of RealTime Reinforcement Learning" Proceedings of the National Conference on Artificial Intelligence, pages 99-105, 1993.
[10] S. Thrun "Efficient exploration in reinforcement learning" Technical Report CMU-CS-92-102, Carnegie Mellon University, Computer Science Department, Pittsburgh, PA, 1992.
[11] A. W. Moore. "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional Statespaces". In NIPS 6, 1993.

