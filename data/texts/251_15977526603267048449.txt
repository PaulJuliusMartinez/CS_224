Probabilistic kernel regression models

Tommi S. Jaakkola and David Haussler
Department of Computer Science University of California Santa Cruz, CA 95064
f gtommi,haussler @cse.ucsc.edu

Abstract
We introduce a class of exible conditional probability models and techniques for classication/regression problems. Many existing methods such as generalized linear models and support vector machines are subsumed under this class. The exibility of this class of techniques comes from the use of kernel functions as in support vector machines, and the generality from dual formulations of standard regression models.
1 Introduction
Support vector machines 10] are linear maximum margin classi ers exploiting the idea of a kernel function. A kernel function de nes an embedding of examples into (high or in nite dimensional) feature vectors and allows the classi cation to be carried out in the feature space without ever explicitly representing it. While support vector machines are non-probabilistic classiers they can be extended and formalized for probabilistic settings 12] (recently also 8]), which is the topic of this paper. We can also identify the new formulations with other statistical methods such as Gaussian processes 11, 13, 4]. We begin by de ning the class of kernel regression techniques for binary classi cation, establish the connection to other methods, and provide a practical measure for assessing the generalization performance of these methods. Subsequently, we extend some of these results to sequential Bayesian estimation. Finally, we will provide a theorem governing general kernel reformulation of probabilistic regression models.
2 Binary classi cation
We start by considering Gaussian process classi ers 13, 4] that are fully Bayesian methods. To this end, de-

ne a set of zero mean jointly Gaussian random vari-

ables fZig, one corresponding to each example Xi to

be classi ed. Assume that the covariance ( iCov Z Zj)

between any two such variables is given by a kernel

function ( iK X Xj) of the corresponding examples (we

need the kernel function to be strictly positive de nite

in this case). Assume further that the binary 1 la-

bilofey(gSlistsihZtfieSci)iGfguwanahucrestersieioga,nenf.noveTrrahearxteiaeaebdmxleawpsmlietfp,hZleip(gvrzoe)tbch=taaobtr(is1alirft+eiXeessi;ugPbzt)s(h;Seuq1isjuZisespin)tethl=cye-

passed through transfer functions to yield probabilities

for the labels. Similar Z's (and hence similar labels)

are assigned to input vectors that are \close" in the

sense of the kernel. Given now a training set of labels

fSig and example vectors fXig we can, in principle,

compute the posterior distribution of the latent Gaus-

sian variables fZig and use it in assigning labels for

yet unknown examples the Gaussian variable Zt cor-

freZsipgocnodninstgrationetdhebynetwheexxaemdptlreaXintinigs

correlated with labels. The cal-

culations involved in this procedure are, however, typ-

ically infeasible.

Instead of trying to maintain a full posterior distribu-

ttiloenfoorvetrhethMe AlaPtenctonGaguusrsaitainonvafrZ^iaigb,leas,ndweasmsiagyn

setthe

label to a new example according to the probabil-

ity P (StjZ^t) = (StZ^t). De nition 1 below gives a

generic formulation of this procedure in terms of dual

parameters. The dual parameters arise from Legen-

dre transformations of concave functions (see e.g. 7])

tlohsesecsonlocgavPe(fSutnjZc^tti)o.nWs ienctohnissicdaesresaurcehtthreanclsafossrimcaattioionns

in more detail later in the paper.

De nition 1 We de ne a kernel regression classi er

to be any classi cation technique with the following

properties: 1) given any predicts the (maximum

perxoabmabpilleitvye)ctloarbeXl S,^thfoermXethaocd-

cording to the rule

X !T

S^ = sign

(iSiK X Xi)

i=1

Ppr(iSorijXasisum) p=tion(S, itZhei)seanseawbovvaer.iaUblnesdefrZtihgeaGreajuosisnitalny

(1)

Gaussian random variables with a covariance matrix given by

where (S1 )X1 : : : (ST XT ) are labeled training ex-

( iCov Z Zj) = Ef ( T Xi)( T Xj) g = XiT Xj (4)

amples, the i are non-negative coe cients, and the
k2e)rTnehlefucnoectiocnienKts(Xfi tXg jw)eiisghptoisnigtivtehe(stermaiin-i)ndgeexnaimte-.
ples in the classi cation rule are obtained by maximiz-

ing

X XJ( ) = ;12 i j

i j SiSjK(Xi Xj) + F ( i)
i

(2)

where is the prior covariance of . The logistic regression problem is thus equivalent to a particular Gaussian process classi er. Consequently, MAP estimation of the parameters in the logistic regression models corresponds exactly to the MAP Gaussian process formulation (De nition 1). The relation between MAP estimation and De nition 1 is presented more generally later in the paper (Theorem 1). We note

subject to 0 i 1, where the potential function F ( ) is continuous and concave (strictly concave whenever the kernel is only positive semi-de nite).

here only that the potential function F ( ) in the logistic regression case is the binary entropy function
F ( ) = ; log( ) ; (1 ; ) log(1 ; ) (see Appendix
E) and the kernel function is the covariance function

The assumptions of positive (semi-)de nite kernel ( iCov Z Zj) given by Eq. (4).

function and (strictly) concave and continuous potential functions F ( ) are introduced primarily to en-

2.1 The kernel function

sure that the solution to the maximization problem

is unique. In practice this solution can be achieved

monotonically by successively updating each individ-

ual coe cient i from

X(@
iJ
@

)=;

j

j SiSjK(Xi

Xj )

+

@ @

i

F

(

i) = 0

(3)

Here we discuss a few properties of the kernel function as given in Eq. (4). First, interpreting the kernel function as a covariance function for a Gaussian process classi er suggests treating it as a similarity function. In this sense, examples are similar when their associated labels would be a priori (positively) correlated. A simple inner product is not necessarily a good

while holding the other j, for j 6= i, xed. The solu-
tions to these one dimensional equations are relatively easy to nd for any kernel method in our class. The optimal solution is characterized by these xed point equations, reminiscent of mean eld equations.

Let us now consider a few examples to gain more in-

sight into the nature and generality of this class. Sup-

port vector machines1, for example, can be seen as

realizations of this class simply by setting F ( t) = t

(see e.g. 10]). Generalized linear models 6] can be also

seen as members of this class. For example, consider

a logistic regression model, where the probabilities for

the labels are given by j(P Si Xi ) = (Si T X) and

the prior distribution P ( ) over the parameters is a

zero can

mean Gaussian. With each input vector associate a new variable Zi = T Xi that

Xi
de

we nes

the conditional probability for the label Si through

measure of similarity since, for example, it is possible

for an example to be more similar to another example

than to itself. Typically, however, the kernel function

is not a simple inner product between the examples

but an inner product between feature vectors corre-

sponding to the examples. For example, in Eq. (4)

the feature vectors are X =

.1
2X

Any valid ker-

nel function can be reduced to a simple inner product

between (possibly in nite dimensional) feature vectors

10, 11]. When the feature mapping is non-linear, the

kernel can de ne a reasonable similarity measure in

the original example space even though this property

doesn't hold in the feature space.

In going from logistic regression models to Gaussian process classi ers the prior covariance matrix over the original parameters plays a special role in specifying the inner product in Eq. (4). In other words, the prior covariance matrix directly changes the met-

1Note that in support vector machines a bias term is added explicitly into the classi cation rule and treated separately in the optimization problem. In our formulation the bias term is realized indirectly through an additive constant in the kernel, where the magnitude of this constant

ric in the example space. This metric is, however, the

in;ve1r.sTe hoifswinhvaetrsise

natural in the parameter space, i.e., relation follows from a more general

property of Riemannian metrics in dual coordinate sys-

speci es the prior variance over the bias term. Put into tems.

our setting, support vector machines assume a at prior and consequently the two de nitions agree in so far as the

If we change the kernel function, our assumptions con-

constant term in the kernel is appropriately large.

cerning the examples (similarity, metric properties)

will change. This suggests that the modeling e ort in these classi ers should go into nding an appropriate kernel function. We can, for example, derive kernels from generative probability models 3] or directly encode invariances into the kernel function 1].

2.2 A measure of generalization error

De nition 1 provides us with a large class of techniques with relatively few restrictions on e.g. the choice of the kernel function. To compensate this exibility we must provide means for assessing their generalization performance in order to be able to limit the complexity of the nal classi er to an appropriate level. Our emphasis here is on practical measures.

Support vector machines attain sparse solutions in the sense that most of the coe cients i are set to zero as a result of the optimization. This computationally attractive property also yields a direct assessment of generalization 10]: the expected ratio of the number of non-zero coe cients to the number of training examples bounds the true generalization error. The applicability of this measure is limited to support vector machines, however, since the probabilistic classi ers generally do not attain sparse solutions (making the sparsity measure vacuous). The lemma below provides a more general cross-validation measure that applies to all kernel classi ers under De nition 1:

Lemma 1 For any training set D
examples and labels and for any

= fSt
kernel

XretggrtTe=ss1ioonf

classi er from De nition 1 the leave-one-out cross-

validation error estimate of the classi er is bounded

by

1

XT

step

0 @

;St

X

1 A(iSiK X Xi)

(5)

T t=1

i=6 t

where f tg are the coe cients optimized in the pre-
sense of all the training examples.

The step functions in the lemma count the number of

times the sign of the training label St disagrees with

the sign of the prediction based on the other exam-

ples.

If

we

include

the

missing

th
t

terms

in

the

predic-

tions, the error estimate would reduce to the training

error (cf. the prediction rule in De nition 1). The

cross-validation error bound is thus no more costly to

evaluate than the training error and obviously requires

no retraining. As for the accuracy of this bound we

note that in case of support vector machines, it can be

shown that the result above provides a slightly better

estimate than the sparsity bound2. The proof of the

lemma is given in Appendix A.

2The sparsity bound can be, in principle, de ned in

3 Bayesian formulation

In the above MAP formulation the kernel function itself remains xed, regardless of the nature or the number of training examples. This is in contrast with a full Bayesian approach where the kernel function would have to be modi ed based on observations. More precisely, in the above formulation it is the prior distribution over the parameters that speci es the (simple) inner product between the examples in a Bayesian setting, roughly speaking, this inner product would be de ned in terms of the posterior distribution. While the full Bayesian approach is unfortunately not feasible in most cases, it is nevertheless possible to employ approximate methods for updating the kernel function through observations. Several approaches have already been proposed for this purpose, including the use of Laplace approximation in the context of multi-class regression 13] and the use of variational methods 5]. Our approach is rather complementary in the sense that we provide a recursive variational approach that avoids the need for simultaneously optimizing a large number of variational parameters as discussed in 5].

3.1 Bayesian logistic regression

Here we consider a Bayesian formulation of the logistic regression models. We start by brie y reviewing the variational approximation technique 2] that enables us to estimate the posterior distribution over the parameters in these models. We subsequently extend this approximate solution for use with kernel functions. In Bayesian estimation we can, in principle, update the parameter distribution sequentially, one example at a time:

P ( jDt) / j(P St Xt )P ( jDt;1) = ( St T Xt ) P ( jDt;1)

(6) (7)

where Dt = f(S1 )X1 : : : (St Xt)g is the set of exam-
ples observed up to the time t. We constrain the above general formulation a bit by assuming that the prior
distribution P ( ) = P ( jD0) over the parameters is a
multivariate Gaussian with possibly arbitrary covariance structure. While such assumption does not by itself make the sequential updating feasible in terms being able to represent the true posterior distribution, it nevertheless opens the way to a closed form approximate solution. To this end we employ a variational

terms of \essential" support vectors rather than just those with non-zero coe cients. This would improve the estimate but would also make it much more di cult to evaluate in practice.

transformation of the logistic function as given by3.

(z) ( ) exp (z ; )=2 + ( )(z2 ; 2) (8)
(z) (9)

where is an adjustable parameter known as the variational parameter. Inserting the approximation (z) back into the sequential update equation Eq. (7) we obtain
P ( jDt) / ( St T Xt ) P ( jDt;1) (10)

Since the transformed logistic function ( ) is a quadratic function of its argument in the exponent,
it follows that any Gaussian prior P ( jDt;1) will re-
sult in a Gaussian posterior in this approximation. The mean and the covariance of this Gaussian can be related to the mean and the covariance of the prior through

t = ;t;1 ct t;1 Xt XtT t;1

t=

t(

;t;11

t;1

+

1 2

StXt

)

(11) (12)

where the subindex t referes to the set of examples
Dt = f(S1 )X1 : : : (St Xt)g observed so far. The
variational parameter de nes the extent to which the covariance matrix is updated, i.e., it de nes ct:

ct

=

2t 1 + 2 tXtT t;1Xt

(13)

where t = tanh( t=2)=(4 t). We would like to set the

variational parameter t so as to improve the accuracy

of the approximation. A suitable error measure for the

approximation can be derived from the fact that the

variational transformation introduces a lower bound.

The approximation in fact yields a lower bound on the

likelihood of the conditional observation jSt Xt

Z

Zj(P St Xt Dt;1) =

( St T Xt )P ( jDt;1)d

( St T Xt ) P ( jDt;1)d

(14)

where the last integral can be computed in closed form. The maximization of the bound yields a xed point equation for t 2]:

t2 = XtT tXt + ( Tt Xt)2

(15)

that can be solved iteratively (note that both t and t depend on t through the equations above).

3Other approximations are possible as well such as the Laplace approximation 9].

3.2 Kernel extension

In order to be able to employ kernels in the context

of these Bayesian calculations we have to reduce all

the calculations with the input examples Xk to ap-

propriate inner products. Such inner products can be

then replaced with arbitrary positive semi-de nite ker-

nels. As before, we de ne the a priori inner product

as is

pXoskTitiv0eXdke0

which nite.

is valid since the For simplicity, we

prior covariance assume that the

mean of the Gaussian prior over the parameters is zero.

Consequently, it remains to show that the sequential

updating scheme can be carried out by only referring

to the value and not the form of the inner products

( kK X kX 0 ) = XkT .k0X 0

We adopt the following compact representations of the posterior mean and the time dependent kernel:

(tK k

0
k

)

=

XkT

t kX 0

Mt(k) = Tt Xk

(16) (17)

It will become clear later that it is necessary to con-

side0r
kk

only .> t

predictive We would

quantities, i.e. those for which like to now express the update

equations for the mean and the covariance in terms

of these new quantities. Consider rst Eq. (11), the

covariance update formula. Prior and post multiply-

ing the update and using the

formula with de nition for

XKktT(kankd0

kX 0, respectively, ) given above, we

obtain:

;(tK k

0
k

)

=

Kt;1(k

0
k

)

ct Kt;1(k

t)Kt;1(t

0
k

)

(18)

Thus

the

(tK k

0
k

)

satisfy

a

simple

recurrence

rela-

tion that connects them back to the a priori kernel

(K0 k rence

rke0l)at=ionXfokTr

.k0X 0 Mt(k)

We can also derive a recurin a similar way (see Appendix

B) giving

Mt(k) = Mt;1(k)

+ct

St
4t

;

Mt;1(t)

Kt;1(t k) (19)

Since M0(k) = 0 by assumption (i.e. the prior mean is zero), the values Mt(k) can be rooted in the kernels and the observed labels St.

Finally, both Kt and Mt iterations make use of the coe cients ct,

ct =

1+2

2t tKt;1(t

t)

(20)

and t = tanh( t=2)=(4 t) which need to be speci ed. In other words, we need to be able to optimize the variational parameter t in terms of Kt;1, Mt;1, and

St alone in order to preserve the recurrence relations. Starting from the xed point equation Eq. (15) we get (the details can be found in Appendix C)

t2 = XtT tXt + ( tT Xt)2 = (tK t t) + Mt(t)2

(21) (22)

=

ct
2t

Kt;1(t t) +

ct
2t

2

Mt;1(t) +

1 2

St

Kt;1

(t

t)

2
(23)

Note that t appears on the right hand side only in the expressions ct=(2 t). It follows that to optimize t in the process of absorbing a new observation we only need to know Mt;1(t), Kt;1(t t) and St. How these values can be computed and stored e ciently is illustrated in the next section.

3.3 E cient implementation

Due to the form of the dependencies in the recurrence

relations for Kt and Mt, we can carry out the computa-

tions in the sequential estimation procedure e ciently

and compactly. To show this we proceed inductively.

Assume therefore that we have a lower diagonal matrix

Kt and Kt

a =

ve2466666cto...KKKr 000M(((312 t111o)))f

the
K1
.K.. 1

form (2 2) (3 2)

:::

3 757777

(K0 t 1) (K1 t 2) : : : Kt;1(t t)

Mt = M0(1) M1(2) : : : Mt;1(t) T (24)

which we have constructed from the already observed

examples up to (St Xt). To absorb a new training ex-

ample (St+1 Xt+1) or to evaluate the predictive prob-

ability of St+1 given Xt+1, we need to be able to op-

timize the variational parameter t+1 associated with

this example. Consider therefore the xed point equa-

tion (23). The required quantities are (tK t + 1 t + 1)

and Mt ponent

(t+1) corresponding to the
of the K matrix and the

next next

diagonal comcomponent of

the M vector, respectively. We start computing these

quantities by lling in the next row of K with the ker-

nels (K0 t + 1 k), k = f1 : : : t + 1g. Consequently, we

can apply the recurrence relation Eq. (18) to replace

these values (except the
k = f2 : : : t + 1g. Note,

rst one) with however, that

(K1 t + 1 we must

k), re-

place these values in the reverse order, from k = t + 1

down to k = 2, due to the dependence structure in the

recurrence relation. Following in this manner we can

ll in (tK 0 t mately get

+1 (tK t

k) +

for 1t

k
+

= 1)

ft0
in

+1 time

::: t
O(t2

+ 1g and
=2). (tM t

ulti+ 1)

can be computed even more directly by starting from

(M0 t + 1) = 0 and using the recurrence relation Eq.

(19) to
O(t).

give

(M1 t

+

1)

M2(t + 1)

:::

(tM t + 1) in time

4 Generic kernel regression

De nition 1 and some of the discussion in the previous sections can be generalized to a multi-class or to a continuous response setting.

Theorem 1 Let ( jP Y X ) be a conditional proba-

bility model over a discrete or continuous variable

Y , where X is a nite real vector of inputs and

thiTa=Xt f1)21)PF(2oYr:jX:a:ll

mg
)=

denotes
( jP Y Z1

values y of Y

the parameters. Assume
jZ2 : : : Zm) where Zi =
log ( ), mP y Z1 Z2 : : : Z

is a jointly concave continuously di erentiable func-

tion of Z1 Z2 the parameter

m: : : Z
vectors

3) The prior
f ig is a zero

distribution over mean multivari-

ate Gaussian with a block diagonal covariance matrix

rDes==podfniYdatignX(g tt1gotT=th12e,

: : : m). Then, given a training set the conditional probability model cormaximum a posteriori (MAP) set of

parameters has the form

j j(P Y X MAP ) = P (Y ^Z1 ^Z2 : : : Z^m)

(25)

where
f t ig

Z^i = attain

PtheTt=u1nitqui(eXmtTaxiiXmu),mthvealucoeeofcients

=

XJ (

)

=

;

1 2

i

t

t0

ti

t0 i(XtT

i tX 0 )

XT
+ Ft( t 1 t 2 : : : t m)
t=1

(26)

and the potential functions Ft( t 1 t 2 : : : t m) are the Legendre transformations of the classi cation loss
functions log P j( tY Z1 Z2 : : : Zm):

( X )Ft( t 1 t 2 : : : t m) =

Z1m::i:nZm

; jt iZi log ( )t mP Y Z1 Z2 : : : Z
i

(27)

The rather strong assumption of continuous differentiability can be easily relaxed to piece-wise di erentiability4. The proof is given in Appendix D. The Legendre transformations in the theorem are easy to compute in typical cases, e.g., when the conditional
4Piece-wise continuously di erentiable functions can be obtained as limits of continously di erentiable functions.

taapipmroopnpbeslaaebr(siii.nilenigt.,iAiensepZtpPhYee=(nYddPujiZxay1lEefZoZ.yr2T)m.h:ue:lW:aitneZinomnehr)acvpaaernroebldiseustcroeetdfstpml(aaXacfxetTedwfwuinXeitxch)--
any valid kernel function Ki(Xt X) such as the Gaussian kernel5. Note that De nition 1 makes stronger assumptions about the Legendre transformations and/or the positive de niteness of the kernel function so as to end up with a unique solution in terms of the new parameters . For the purpose of prediction the possible non-uniqueness is immaterial since the resulting predictive distribution remains unique. Concavity of the objective function also assures us that the solution is relatively easy to nd in all cases.

5 Discussion

In any classi cation/regression problem it is necessary to select an appropriate representation of examples as well as the model and parameter estimation method. In this paper we have focused on the latter, deriving a generic class of probabilistic regression models and a parameter estimation technique that can make use of arbitrary kernel functions. This allows greater exibility in specifying probabilistic regression models of various complexity levels without fear of local minima. We can also obtain quick assessments of their generalization performance. The issue concerning the choice of the kernel function or, equivalently, the representation of examples, has been addressed elsewhere 3, 1] and

References

1] Burges C. (1998). Geometry and invariance in kernel based methods. To appear in Advances in kernel methods { Support vector learning. MIT press.

2] T. Jaakkola and M. Jordan (1996). A variational approach to Bayesian logistic regression problems and their extensions. In Proceedings of the sixth international workshop on arti cial intelligence and statistics.

3] Jaakkola T. and Haussler D. (1998). Exploiting generative models in discriminative classi ers. Available at http://www.cse.ucsc.edu/ .research/ml/publications.html

4] D. J. C. MacKay. Introduction to gaussian processes. 1997. Available from .http://wol.ra.phy.cam.ac.uk/mackay/

;
e

5The
(Xt;X

)GT aui(sXsita;nX

kernel ).

is

given

by

( )Ki Xt X

=

5] Gibbs M. MacKay D. (1997). Variational Gaussian process classi ers. Draft manuscript, available at .ftp://wol.ra.phy.cam.ac.uk/mackay

6] McCullagh P. and Nelder J. (1983). Generalized linear models. London: Chapman and Hall.

7] Rockafellar R. (1970). Convex Analysis. Princeton Univ. Press.

8] Smola A., Schlkopf B., Mlller K., (1998). General Cost Functions for Support Vector Regression. ACNN'98, Australian Congress on Neural Networks.

9] D. Spiegelhalter and S. Lauritzen (1990). Sequen-

tial updating of rected graphical

conditional structures.

Npreotbwaobriklsiti2es0:on57d9i--

605.

10] Vapnik V. (1995). The nature of statistical learning theory. Springer-Verlag.

11] Wahba G. (1990). Spline models for observational data. CBMS-NSF Regional Conference Series in Applied Mathematics.

12] Wahba, G. (1997). Support Vector Machines, Reproducing Kernel Hilbert Spaces and the Randomized GACV. University of Wisconsin - Madison technical report TR984rr.

13] Williams C. and Barber D. (1997). Bayesian Classi cation with Gaussian Processes. Manuscript in preparation.

A Proof of the cross-validation bound

Consider

the

case

where

the

th
t

example

is

removed

from the training set D. In this case we would op-

timize the remaining coe cients D;t = f igi6=t by

maximizing

X XJD;t(

D;t)

=

;

1 2

i

j=6 t

i

jSiSjK(Xi Xj) + F (
i6=t

i)

By our assumptions, the solution is unique and we de-

note back

it by into

thtDe;ttr.aCinoinngsidseert

now the

adding

the

th
t

example

optimal setting of the

coe cients D;t will naturally change as they need

to be optimized jointly

adding

the

th
t

example,

with t. To we perform

assess the e ect of the joint optimiza-

tion as follows. First, we x t to the value that would

have resulted from the joint optimization, call it t .

The remaining coe cients D;t can be obtained from

a reduced objective function where all the terms depe-

dending solely on

t

are

omitted.

Thus

when

the

th
t

example is included, the remaining coe cients D;t C Fixed point equation for t

are obtained by maximizing
JD( D;t) = JD;t( D;t) ;

X
t St

i6=t

i iS K(Xt Xi) (28)

The objective here is to transform the tion

xed point equa-

Let D;t be the maximizing coe cients. Clearly

t2 = (tK t t) + Mt(t)2

(36)

JD( D;t) JD( tD;t)

(29)

Expanding each side according to eq. (28) and rear-

ranging

terms
t St

wXe geittSi

K

(Xt

Xi)

i6=t X

t St i iS K(Xt Xi)

i=6 t

into the form that explicates the dependence of the right hand side on t. Applying the recurrence relation for (tK t t) we nd

(tK t t)

;= Kt;1(t t)

ctKt;1(t

)2
t

= ;Kt;1(t t)(1 ctKt;1(t t))

=

ct
2t

Kt;1

(t

t)

(37) (38)
(39)

+JD;Xt( Dt ;t) ; JD;t( D;t)
t St i SiK(Xt Xi)
i6=t

(30)

wJJsthuDDhel;;te:lrttae((tsh)tw.eDte;Wesirtgm)hhnaeovonafefesvtEheuqrest.ehd(etr3s0>tt)chto0ebee,rymfwacectiintecadnantitnchsdaadtgtievestiJDtdtDt;heh;ettethc(deomersDtraire;rxscetittdm)naeirnzesdesof the cross-validation prediction (positive is correct)

where Kt;1(t t) is independent of t (depends only on 1 : : : t;1). Similarly, we expand Mt(t):

;Mt(t)

=

Mt;1(t) + ct

St
4t

Mt;1(t) Kt;1(t t)

(40)

= Mt;1(t)(1 ; ctKt;1(t t))

+

ctSt
4t

Kt;1(t

t)

(41)

its lower bound, the last term, is the one that appears in the lemma and uses only the coe cients optimized

=

ct
2t

Mt;1

(t)

+

ct
2t

St
2

Kt;1

(t

t)

(42)

in the presense of all the training examples.

Note used

nally that in the classi

when er and

tth=e

0

the

th
t

example

is

not

lemma holds trivially.

B Recurrence relation for Mt(k)
Let us start by simplifying the posterior mean update:

=

ct
2t

Mt;1(t) +

1 2

St

Kt;1

(t

t)

(43)

where the only dependence on t is now in ct=(2 t). Combining these two results gives

t2 =

ct
2t

Kt;1(t t) +

t = ( t;1 ; ct t;1XtXtT t;1)

(

t;;11

t;1

+

1 2

StXt

)

= t;1 ; ct(XtT t;1) t;1Xt

+

1 2

St(1

;

ct

XtT

t;1Xt)

t;1Xt

= t;1 ; ct(XtT t;1) t;1Xt

+

1 2

St

ct
2t

t;1Xt

(31) (32) (33)

=

t;1 + ct

St
4t

;

XtT

t;1

t;1Xt (34)

ct
2t

2

Mt;1(t) +

1 2

St

Kt;1

(t

t)

2
(44)

D Proof of Theorem 1

Given MAP

a training set of examples D = fYt
parameter solution is obtained by

XmtagxTti=m1i,ztinhge

the following penalized likelihood function

X XJ(

)

=

T
log P (YtjZt) ;
t=1

1 2

m i=1

iT

i;1 i

(45)

where ct=(2 terms

otw)feM(shetae(vket)hu=eseddeTttXnhiekti,foawncetoctfahcnattwg(ir1vite;enctithnXettTahbeott;vee1xXrte)t.s)uI=lnt

as

;Mt(k)

=

Mt;1(k) + ct

St
4t

Mt;1(t) Kt;1(t k)

(35)

where the rst term is the log-probability of the observed labels and the second comes from the log of the block-diagonal Gaussian prior distribution. We have omitted the terms that do not depend on the parameters and overloaded our previous notation in the
sense that Zt now refers to the vector fZt 1 : : : Zt mg.
The solution MAP is unique since J( ) is strictly concave in (owing to the log-prior term).

Now, by our assumptions log P (YtjZt) is jointly con-

cave continuously di erentiable function of Zt and thus

by convex duality (see e.g. 7]) we get: there exists a

function log P (Yt
Ft

Ft with
jZt) =
( t) =

the same properties such that

( Xm

min

t

(

i=1
Xm

mZitn i=1

)

t iZt i ; Ft( t)

(46)
)

t iZt i ; log P (YtjZt)

(47)

where f=t t 1 : : : t mg. These transformations

are also known as Legendre transformations and the

function Ft Note that

is known as the dual or the conjugate function

conjugate Ft of log

function.
P (YtjZt)

is in general di erent for each distinct Yt, hence the

additional subindex.

Let us now introduce these transformations into the

objective function J( ) and de ne J( ) as

J(

XT "X
)=
t=1 i

t iZt i ; Ft(

t

#
)

;

1 2

Xm
i=1

iT

i;1 i

(48)

where we have dropped the associated mimizations with respect to the coe cients. Clearly, J( ) = min J( ). The lemma below establishes the connection to Theorem 1:

Lemma 2 The objective function in Theorem 1 is
concave and is given by the negative of

J( ) = max J( )

(49)

This result implies that maximizing the objective function in Theorem 1 is equivalent to computing min J( ) in our notation here.

Proof: Recall that Zt i =
any xed setting of , J(

iT

Xt which implies that for ) is a quadratic function

of the parameters . We can therefore solve for the

maximizing :

X

i=

t i iXt

(50)

t

and substitute this back into J( ) giving

X XJ(

)=

1 2 i t t0

t i t0 i(XtT

i tX 0) ;

t

Ft( t)

(51)

which is indeed the negative of the objective function appearing in the theorem as desired. It remains to
show that J( ) is convex (or that ;J( ) is concave).
Note rst that the conjugate functions Ft are concave

and thus all ;Ft terms are convex. Also the rst term

in Eq. (51) corresponds to

1 2

X(
i

i

)T

i;1( i )

(52)

twhheeraeboevaechterimisisaallsinoeacornfvuenxc.tioWn iothfou,t

and hence additional

assumptions J( ) may not be strictly convex and

thus the solution in terms of may not be unique.

min J( ) and the associated remain unique, how-

ever. 2

Since, by de nition, maximizing J( ) means evaluating max min J( ) and because we have just shown that min max J( ) corresponds to maximizing the objective function in Theorem 1, it remains to show that \max min = min max " for J( ). We state this as a lemma:

Lemma 3 For J( ) given by Eq. (48)

max min J( ) = min max J( )

(53)

Proof: Let be the unique maximum of J( ) (the
left hand side above). is also nite6. Since in general for any xed

max min J( )

min max J( ) (54) max J( ) (55)

it su ces to show that there exists such that J( ) = max J( ). To this end, the niteness of together with the continuous di erentiability assumption guarantees that

t = rZt log P (YtjZt) j =

(56)

exists for all t. It can be shown that these are also the minimizing coe cients in our Legendre transformations. Consequently, the minimum is attained:

J( ) = min J( ) = J( )

(57)

At this minimum r J( ) j = vanishes and thus

r J( ) j = = r J( ) j = = 0

(58)

The last equality gives su cient guarantees that for our choice of

max J( ) = J( )

(59)

Comparing this with Eq. (57) completes the proof. 2
6The concavity of the log-conditional probabilities implies that they have sublinear asymptotics. Thus in the maximization the quadratic prior term will eventually dominate.

E Examples

Here we provide a few examples of how to compute the Legendre transformations. Consider rst the logistic regression case:

log P (StjZ) = log (StZ)

(60)

where Z = T X. Treating this log-probability as a function of Z, we can nd its Legendre transformation:

Ft( ) = mZaxf Z ; log (StZ) g

(61)

To perform this maximization, we take the derivative with respect to Z and set it to zero:

; St (;StZ) = 0

(62)

which implies that Z = ;St log(St )=(1 ; St ). Sub-
stituting this Z back into Eq. (61), we get

Ft( )

=

;

St

log

1

St
; St

; log(1 ; St ) (63)

= ( tH S )

(64)

where H( ) is the binary entropy function. If, in addition, we make a change of variables t = St , then Ft( t) = H( t) and is no longer a function of t. If the objective function in Theorem 1 is expressed in terms of these new t, it reduces to the form given in De nition 1.

These calculations can be generalized to the multi-

class setting where the probability model is the soft-

max:

Xm
j ;log P (Yt Z1 : : : Zm) = ZYt log eZi
i=1

(65)

with Zi = tained from

iT

X

.

Ft( ) = Z1m::a: xZm

The
(X
i

Legendre transformation is ob-
iZi ; ZYt + log Xm eZi ) (66)
i=1

Similarly to the two class case we nd the maximum by setting the derivaties with respect to Zi to zero:

f(onrottehethZatvathriiasib;ilmespYlitiseisu+ntihqPautejeZPeiuZpij

=0 i= to a

(67) 0). The solution constant:

;Zi = log( Yt i i) + constant:

(68)

Substituting these back into the transformation gives

Ft( ) = ; Xm ( Yt i ; i) log( Yt i ; i)
i=1

(69)

which is, not surprisingly, the entropy function. A
ofcohnramtn.agteIinoonft:vhaeFritna(ebwlte)sv=atrHiia=b(lets)Ytaitn;d=Fifsinmto1pllo:in: :egsertthdmeegtpr,eantnhdses-
objective function in Theorem 1 reduces to

X XJ(

)

=

;

1 2

i

t

t0

(

Yt i ;

t i)( Yt0 i ;

t0 i)(XtT

i tX 0 )

+ H( t)

(70)

t

We can also rewrite the predictive probability model in Theorem 1 in terms of the new t i and get

j P( ^ ^ ) =mP Y Z1 : : : Z

eZ^Y i eZ^i

where Z^i = Pt( Yt i ; t i)(XtT iX).

(71)

