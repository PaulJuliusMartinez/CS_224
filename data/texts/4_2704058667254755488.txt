Frank Rosenblatt Marvin Minsky
Bernard Widrow
Perceptrons

IBM 704


 The IBM 704,[1] the first massproduced computer with floating point arithmetic hardware, was introduced by IBM in 1954. The 704 was significantly improved over the IBM 701 in terms of architecture as well as implementations which were not compatible with its predecessor.

 use of core memory (instead of Williams tubes) and addition of three index registers. To support these new features, the instructions were expanded to use the full 36-bit word. The new instruction set became the base for the IBM 700/7000 series scientific computers. To quote the IBM 704 Manual of operation (see external links below):
 The type 704 Electronic Data-Processing Machine is a largescale, high-speed electronic calculator controlled by an internally stored program of the single address type. IBM stated that the device was capable of executing up to 40,000 instructions per second. IBM sold 123 type 704 systems from 1955 to 1960. The programming languages FORTRAN and LISP were first developed for the 704, as was MUSIC, the first computer music program by Max Mathews.

 Frank Rosenblatt - Wikipedia, the free encyclopedia

 Frank Rosenblatt (11 July 1928 ­ 11 July 1971) was a New York City born computer scientist who completed the Perceptron, or MARK 1, computer at Cornell University in 1960. This was the first computer that could learn new skills by trial and error, using a type of neural network that simulates human thought processes.

 Rosenblatt's perceptrons were initially simulated on an IBM 704 computer at Cornell Aeronautical Laboratory in 1957. By the study of neural networks such as the Perceptron, Rosenblatt hoped that "the fundamental laws of organization which are common to all information handling systems, machines and men included, may eventually be understood."

The gravestone of Frank Rosenblatt, Brooktondale, NY.

 A 1946 graduate of the Bronx High School of Science, Rosenblatt was a colorful character at Cornell in the early 1960s. A handsome bachelor, he drove a classic MGA sports car and was often seen with his cat named Tobermory. He enjoyed mixing with undergraduates, and for several years taught an interdisciplinary undergraduate honors course entitled "Theory of Brain Mechanisms" that drew students equally from Cornell's Engineering and Liberal Arts colleges.

 This course was a melange of ideas drawn from a huge variety of sources: results from experimental brain surgery on epileptic patients while conscious, experiments on measuring the activity of individual neurons in the visual cortex of cats, studies of loss of particular kinds of mental function as a result of trauma to specific areas of the brain, and various analog and digital electronic circuits that modeled various details of neuronal behavior (i.e. the perceptron itself, as a machine).

 In 1962 Rosenblatt published much of the content of this honors course in the book "Principles of neurodynamics: Perceptrons and the theory of brain mechanisms" (Spartan Books, 1962) which he used thereafter as a textbook for the course.

 Research on similar devices was also being done in other places such as SRI, and many researchers had big expectations on what they could do. The initial excitement became somewhat reduced, though, when in 1969 Marvin Minsky and Seymour Papert published the book Perceptrons with mathematical proofs that elucidated some of the characteristics of the threelayer feed-forward perceptrons. For one side, they demonstrated some of the advantages of using them on certain cases. But they also presented some limitations. The most important one was the impossibility of implementing general functions using only "local" neurons, that don't have all inputs available. This was taken by many people as one of the most important characteristics of perceptrons.

 Rosenblatt died in a boating incident in 1971. He is buried at Quick Cemetery in Brooktondale, New York. After research on neural networks returned to the mainstream in the 1980s, new researchers started to study his work again. This new wave of study on neural networks is interpreted by some researchers as being a contradiction of hypotheses presented in the book Perceptrons, and a confirmation of Rosenblatt's expectations, but the extent of this is questioned by some

 In 2004 the IEEE established the Frank Rosenblatt Award, for "outstanding contributions to the advancement of the design, practice, techniques or theory in biologically and linguistically motivated computational paradigms including but not limited to neural networks, connectionist systems, evolutionary computation, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained."[

 The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, published in 1958

 The physical connections of the nervous system which are involved in learning and recognition are not identical from one organism to another. At birth, the construction of the most important networks is largely random, subject to a minimum number of genetic constraints.

 The original system of connected cells is capable of a certain amount of plasticity; after a period of neural activity, the probability that a stimulus applied to one set of cells will cause a response in some other set is likely to change, due to some relatively long-lasting changes in the neuron themselves.

 Through exposure to a large sample of stimuli, those which are most "similar" (in some sense which must be defined in terms of the particular physical system) will tend to form pathways to the same sets of responding cells. Those which are markedly "dissimilar" will tend to develop connections to different sets of responding cells.

 The application of positive and/or negative reinforcement (or stimuli which serve this function) may facilitate or hinder whatever formation of connections is currently in progress.

 Similarity, in such a system, is represented at some level of the nervous system by a tendency of similar stimuli to activate the same sets of cells. Similarity ... depends on the physical organization of the perceiving system, an organization which evolves through interaction with a given environment. The structure of the system, as well as the ecology of the stimulus- environment, will affect, and will largely determine, the classes of "things" into which the perceptual world is divided.

 Perceptrons: An Introduction to Computational Geometry, Expanded Edition [Paperback] Marvin L. Minsky (Author), Seymour A. Papert (Author)

Amazon.com: Perceptrons: An Introduction to Computational Geometry

 Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades. It marked a historical turn in artificial intelligence, and it is required reading for anyone who wants to understand the connectionist counterrevolution that is going on today. Artificial-intelligence research, which for a time concentrated on the programming of ton Neumann computers, is swinging back to the idea that intelligence might emerge from the activity of networks of neuronlike entities. Minsky and Papert's book was the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain. Now the new developments in mathematical tools, the recent interest of physicists in the theory of disordered matter, the new insights into and psychological models of how the brain works, and the evolution of fast computers that can simulate networks of automata have given Perceptrons new importance.

 Witnessing the swing of the intellectual pendulum, Minsky and Papert have added a new chapter in which they discuss the current state of parallel computers, review developments since the appearance of the 1972 edition, and identify new research directions related to connectionism. They note a central theoretical challenge facing connectionism: the challenge to reach a deeper understanding of how "objects" or "agents" with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called "society theories of mind." Marvin L. Minsky is Donner Professor of Science in MIT's Electrical Engineering and Computer Science Department. Seymour A. Papert is Professor of Media Technology at MIT.

 Neural Nets and the Brain Model Problem, Ph.D. dissertation, Princeton University, 1954. The first publication of theories and theorems about learning in neural networks, secondary reinforcement, circulating dynamic storage and synaptic modifications.

 Marvin Minsky - Wikipedia, the free encyclopedia

 Marvin Lee Minsky was born in New York City to a Jewish family,[1] where he attended The Fieldston School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He served in the US Navy from 1944 to 1945. He holds a BA in Mathematics from Harvard (1950) and a PhD in the same field from Princeton (1954).[2] He has been on the MIT faculty since 1958. In 1959[3] he and John McCarthy founded what is now known as the MIT Computer Science and Artificial Intelligence Laboratory. He is currently the Toshiba Professor of Media Arts and Sciences, and Professor of electrical engineering and computer science.

 Bronx High School of Science  google map

 Marvin Minsky's Home Page

Exercise I: Lines and Hyper-planes
 Draw a line
 y=2x-1
 Draw addition of two lines
 f1(x)=y=2x-1  f2(x)=y=-3x+1  f(x)=f1(x)+f2(x)

Exercise II: Line fitting
 X=[ 1 -1 1.5 -1.5 2 -2]  Y =[ 1 -3 2 -4 3 -5]  Find a and b such that y=ax+b well fits
paired data in X and Y  Draw line and points

Exercise III: Noise data
 X=[ 1 -1 1.5 -1.5 2 -2]  Y = [1.5 -3.4 2.7 -3.9 2.7 -4.6]  Find a and b such that y=ax+b well fits
paired data in X and Y  Draw line and points

Exercise IV: Line fitting
 X=[ 1 -1 1.5 -1.5 2 -2]  Y =[1.00 5.00 1.25 7.25 2.00 10.00]  Find a and b such that y=ax+b well fits
paired data in X and Y

Exercise V: Quadratic curve fitting
 X=[ 1 -1 1.5 -1.5 2 -2]  Y =[1.00 5.00 1.25 7.25 2.00 10.00]  Find a, b and c such that y=ax^2+bx+c well
fits paired data in X and Y

