Taxonomies and Toolkits of Regular Language Algorithms
Bruce William Watson
Eindhoven University of Technology
teuDepartment of Mathematics and Computing Science

Copyright c 1995 by Bruce W. Watson, Eindhoven, The Netherlands. All rights reserved. No part of this publication may be stored in a retrieval system, transmitted, or reproduced, in any form or by any means, including but not limited to photocopy, photograph, magnetic or other record, without prior agreement and written permission of the author.
Cover design by Nanette Saes.
CIP-DATA KONINKLIJKE BIBLIOTHEEK, DEN HAAG Watson, Bruce William Taxonomies and toolkits of regular language algorithms / Bruce William Watson. | Eindhoven: Eindhoven University of Technology Thesis Technische Universiteit Eindhoven. | With index, ref. | With summary in Dutch. ISBN 90-386-0396-7 Subject headings: nite automata construction / taxonomies / pattern matching.

RTeagxuolanromLaiensguaangdeTAologlokrititshomf s
Proefschrift
ter verkrijging van de graad van doctor aan de Technische Universiteit Eindhoven, op gezag van de Rector Magnificus, prof.dr. J.H. van Lint, voor een commissie aangewezen door het College van Dekanen in het openbaar te verdedigen op
vrijdag 15 september 1995 om 16.00 uur
door
Bruce William Watson
geboren te Mutare, Zimbabwe

Dit proefschrift is goedgekeurd door de promotoren prof.dr. F.E.J. Kruseman Aretz prof.dr. P. Klint en de co-promotor dr.ir. C. Hemerik

Forty Years On
Forty years on, when far and asunder Parted are those who are singing today, When you look back, and forgetfully wonder What you were like in your work and your play; Then, it may be, there will often come o'er you Glimpses of notes like the catch of a song | Visions of boyhood shall oat them before you, Echoes of dreamland shall bear them along. Follow up! Follow up! Follow up! Follow up! Till the eld ring again and again, With the tramp of the twenty-two men, Follow up! Follow up! Routs and discom tures, rushes and rallies, Bases attempted, and rescued, and won, Strife without anger, and art without malice, | How will it seem to you forty years on? Then you will say, not a feverish minute Strained the weak heart, and the wavering knee, Never the battle raged hottest, but in it Neither the last nor the faintest were we! Follow up! Follow up! O the great days, in the distance enchanted, Days of fresh air, in the rain and the sun, How we rejoiced as we struggled and panted | Hardly believable, forty years on! How we discoursed of them, one with another, Auguring triumph, or balancing fate, Loved the ally with the heart of a brother, Hated the foe with a playing at hate! Follow up! Follow up! Forty years on, growing older and older, Shorter in wind, and in memory long, Feeble of foot and rheumatic of shoulder, What will it help you that once you were strong? God gives us bases to guard or beleaguer, Games to play out, whether earnest or fun, Fights for the fearless, and goals for the eager, Twenty, and thirty, and forty years on! Follow up! Follow up! Edward Ernest Bowen (1836-1901)

To my parents, Lyn and Mervin, and to my uncle Ken

Acknowledgements
There are a number of people1 that I would like to thank for their assistance (in various ways) during the research that is reported in this dissertation; I apologize in advance for any omissions. First and foremost, I would like to thank my parents, Mervin and Lyn Watson, and the rest of my immediate family, Ken Carroll, Renee Watson, and Richard Watson, for their loving support throughout the last 27 years. I also thank Nanette for her unfailing support during this research.
As good friends, colleagues, and mentors Frans Kruseman Aretz, Kees Hemerik, and Gerard Zwaan devoted a great deal of their time to directing my research, reviewing my ideas, and introducing me to the ner points of Dutch culture. As members of my kernel thesis committee, Roland Backhouse, Paul Klint, and Martin Rem, and my greater thesis committee, Emile Aarts, Hans Jonkers, and Jan van Leeuwen all provided a great deal of feedback on my research. The proofreading skills of Mervin Watson (grammatical), Nanette Saes (grammatical and translation to Dutch) and Richard Watson (technical) proved to be crucial in polishing this dissertation. Additionally, the other members of the original `PI' research group, Huub ten Eikelder, Rik van Geldrop, Erik Poll, and (temporarily) Fairouz Kamareddine served as sounding boards for some of my ideas.
Quite a few people perform research in areas related to my thesis. The following people were particularly helpful in providing feedback on some of my ideas: Valentin Antimirov, Carel Braam, Anne Bruggemann-Klein, Jean-Marc Champarnaud, Manfred Dalmeijer, Rik van Geldrop, Roelf van den Heever, Pieter 't Hoen, Anne Kaldewaij, Derrick Kourie, Harold de Laat, Bob Paige, Darrell Raymond, Tom Verhoe , Bart Wakkee, and Derick Wood.
A number of people were also helpful in pointing me to related research. These people are: Al Aho, Gerard Berry, John Brzozowski, Maxime Crochemore, Jo Ebergen, Nigel Horspool, Tiko Kameda, Thierry Lecroq, Ming Li, Andreas Potho , Marc Saes, Ravi Sethi, and Je rey Ullman.
As interesting as the development of taxonomies and toolkits is, it would not have been possible without the support of my friends (especially the `Waterloo gang': Frank, Karim, Anne, and Roger). In particular, Jan and Willy went to great lengths to make me feel at home in the Netherlands.
1All titles have been omitted and rst names are used.
i

ii

Contents

Acknowledgements

i

I Prologue
1 Introduction
1.1 Problem statement : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1.2 Structure of this dissertation : : : : : : : : : : : : : : : : : : : : : : : : : : 1.3 Intended audience : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :
2 Mathematical preliminaries
2.1 Notations and conventions : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.2 Basic de nitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.3 Strings and languages : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.4 Regular expressions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.5 Trees : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 2.6 Finite automata and Moore machines : : : : : : : : : : : : : : : : : : : : :
2.6.1 De nitions and properties involving FAs and MMs : : : : : : : : : : 2.6.2 Transformations on nite automata : : : : : : : : : : : : : : : : : :
2.6.2.1 Imperative implementations of some transformations : : :

1
3
3 4 5
7
7 8 13 16 18 19 21 28 32

II The taxonomies

35

3 Constructing taxonomies

37

4 Keyword pattern matching algorithms
4.1 Introduction and related work : : : : : : : : : : : : : : : : : : : : : : : : : 4.2 The problem and some na ve solutions : : : : : : : : : : : : : : : : : : : :
4.2.1 The (p+) algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : 4.2.1.1 The (p+, s+) algorithm and its improvement : : : : : : : : 4.2.1.2 The (p+, s?) algorithm : : : : : : : : : : : : : : : : : : :
4.2.2 The (s?) algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : 4.2.2.1 The (s?, p+) algorithms : : : : : : : : : : : : : : : : : : :

41
41 47 51 51 54 54 54

iii

iv CONTENTS

4.2.2.2 The (s?, p?) algorithm : : : : : : : : : : : : : : : : : : : 57 4.3 The Aho-Corasick algorithms : : : : : : : : : : : : : : : : : : : : : : : : : 57
4.3.1 Algorithm detail (ac) : : : : : : : : : : : : : : : : : : : : : : : : : 60 4.3.2 Method (ac-opt) : : : : : : : : : : : : : : : : : : : : : : : : : : : : 64 4.3.3 A Moore machine approach to the ac-opt algorithm : : : : : : : : 66 4.3.4 Linear search : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 69 4.3.5 The Aho-Corasick failure function algorithm : : : : : : : : : : : : : 70 4.3.6 The Knuth-Morris-Pratt algorithm : : : : : : : : : : : : : : : : : : 73
4.3.6.1 Adding indices : : : : : : : : : : : : : : : : : : : : : : : : 75 4.3.7 An alternative derivation of Moore machine M0 : : : : : : : : : : : 78 4.4 The Commentz-Walter algorithms : : : : : : : : : : : : : : : : : : : : : : : 82 4.4.1 Safe shift distances and predicate weakening : : : : : : : : : : : : : 83
4.4.1.1 General weakening strategies : : : : : : : : : : : : : : : : 87 4.4.1.2 The l = " and the no-lookahead cases : : : : : : : : : : : : 88
4.4.1.2.1 The no-lookahead shift function : : : : : : : : : : 89 4.4.2 A shift function without further weakening : : : : : : : : : : : : : : 90 4.4.3 Towards the CW and BM algorithms : : : : : : : : : : : : : : : : : 91 4.4.4 A more easily precomputed shift function : : : : : : : : : : : : : : : 93 4.4.5 The standard Commentz-Walter algorithm : : : : : : : : : : : : : : 94 4.4.6 A derivation of the Boyer-Moore algorithm : : : : : : : : : : : : : : 95 4.4.7 A weakened Boyer-Moore algorithm : : : : : : : : : : : : : : : : : : 97 4.4.8 Using the right lookahead symbol : : : : : : : : : : : : : : : : : : : 97 4.5 The Boyer-Moore family of algorithms : : : : : : : : : : : : : : : : : : : : 99 4.5.1 Larger shifts without using match information : : : : : : : : : : : : 103 4.5.2 Making use of match information : : : : : : : : : : : : : : : : : : : 108 4.6 Conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 111

5 A new RE pattern matching algorithm

115

5.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 115

5.2 Problem speci cation and a simple rst algorithm : : : : : : : : : : : : : : 117

5.2.1 A more practical algorithm using a nite automaton : : : : : : : : 118

5.3 Greater shift distances : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 121

5.3.1 A more e cient algorithm by computing a greater shift : : : : : : : 122

5.3.2 Deriving a practical range predicate : : : : : : : : : : : : : : : : : : 123

5.4 Precomputation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 127

5.4.1 Characterizing the domains of functions d1 and d2 : : : : : : : : : : 127

5.4.2 Precomputing function t : : : : : : : : : : : : : : : : : : : : : : : : 129

5.4.3 Precomputing functions d1 and d2 : : : : : : : : : : : : : : : : : : : 129

5.4.4 Precomputing function fr : : : : : : : : : : : : : : : : : : : : : : : 131

5.4.5 Precomputing sets Lq : : : : : : : : : : : : : : : : : : : : : : : : : : 132 5.4.6 Precomputing function emm : : : : : : : : : : : : : : : : : : : : : : 133

5.4.7 Precomputing function st and languages L0 and su (L0) : : : : : : 133

5.4.8 Precomputing relation Reach(M) : : : : : : : : : : : : : : : : : : : 135

CONTENTS

v

5.4.9 Combining the precomputation algorithms : : : : : : : : : : : : : : 135 5.5 Specializing the pattern matching algorithm : : : : : : : : : : : : : : : : : 135 5.6 The performance of the algorithm : : : : : : : : : : : : : : : : : : : : : : : 136 5.7 Improving the algorithm : : : : : : : : : : : : : : : : : : : : : : : : : : : : 137 5.8 Conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 138

6 FA construction algorithms

141

6.1 Introduction and related work : : : : : : : : : : : : : : : : : : : : : : : : : 141

6.2 Items and an alternative de nition of RE : : : : : : : : : : : : : : : : : : : 145

6.3 A canonical construction : : : : : : : : : : : : : : : : : : : : : : : : : : : : 151

6.4 "-free constructions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 153

6.4.1 Filters : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 157

6.5 Encoding Construction (rem-") : : : : : : : : : : : : : : : : : : : : : : : : 159

6.5.1 Using begin-markers : : : : : : : : : : : : : : : : : : : : : : : : : : 163

6.5.2 Composing the subset construction : : : : : : : : : : : : : : : : : : 165

6.6 An encoding using derivatives : : : : : : : : : : : : : : : : : : : : : : : : : 166

6.7 The dual constructions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 175

6.8 Precomputing the auxiliary sets and Null : : : : : : : : : : : : : : : : : : : 180

6.9 Constructions as imperative programs : : : : : : : : : : : : : : : : : : : : : 184

6.9.1 The item set constructions : : : : : : : : : : : : : : : : : : : : : : : 184

6.9.2 The Symnodes constructions : : : : : : : : : : : : : : : : : : : : : : 185

6.9.3 A dual construction : : : : : : : : : : : : : : : : : : : : : : : : : : : 187

6.10 Conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 188

7 DFA minimization algorithms

191

7.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 191

7.2 An algorithm due to Brzozowski : : : : : : : : : : : : : : : : : : : : : : : : 194

7.3 Minimization by equivalence of states : : : : : : : : : : : : : : : : : : : : : 196

7.3.1 The equivalence relation on states : : : : : : : : : : : : : : : : : : : 196

7.3.2 Distinguishability : : : : : : : : : : : : : : : : : : : : : : : : : : : : 198

7.3.3 An upperbound on the number of approximation steps : : : : : : : 199

7.3.4 Characterizing the equivalence classes of E : : : : : : : : : : : : : : 200

7.4 Algorithms computing E, D, or Q]E : : : : : : : : : : : : : : : : : : : : : 201

7.4.1 Computing D and E by layerwise approximations : : : : : : : : : : 201

7.4.2 Computing D, E, and Q]E by unordered approximation : : : : : : 203

7.4.3 More e ciently computing D and E by unordered approximation : 204

7.4.4 An algorithm due to Hopcroft and Ullman : : : : : : : : : : : : : : 205

7.4.5 7.4.6

Hopcroft's algorithm to
Computing (p; q) 2 E :

compute Q]E ::::::::

e :

:

ciently ::::

: :

: :

: :

: :

: :

: :

: :

: :

: :

: :

: :

207 210

7.4.7 Computing E by approximation from below : : : : : : : : : : : : : 212

7.5 Conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 213

vi CONTENTS

III The implementations

215

8 Designing and implementing class libraries

217

8.1 Motivations for writing class libraries : : : : : : : : : : : : : : : : : : : : : 219

8.2 Code sharing : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 219

8.2.1 Base classes versus templates : : : : : : : : : : : : : : : : : : : : : 220

8.2.2 Composition versus protected inheritance : : : : : : : : : : : : : : : 220

8.3 Coding conventions and performance issues : : : : : : : : : : : : : : : : : : 221

8.3.1 Performance tuning : : : : : : : : : : : : : : : : : : : : : : : : : : : 222

8.4 Presentation conventions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 223

9 SPARE Parts: String PAttern REcognition in C++

225

9.1 Introduction and related work : : : : : : : : : : : : : : : : : : : : : : : : : 225

9.2 Using the toolkit : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 226

9.2.1 Multi-threaded pattern matching : : : : : : : : : : : : : : : : : : : 229

9.2.2 Alternative alphabets : : : : : : : : : : : : : : : : : : : : : : : : : : 229

9.3 Abstract pattern matchers : : : : : : : : : : : : : : : : : : : : : : : : : : : 230

9.4 Concrete pattern matchers : : : : : : : : : : : : : : : : : : : : : : : : : : : 231

9.4.1 The brute-force pattern matchers : : : : : : : : : : : : : : : : : : : 232

9.4.2 The KMP pattern matcher : : : : : : : : : : : : : : : : : : : : : : : 232

9.4.3 The AC pattern matchers : : : : : : : : : : : : : : : : : : : : : : : 233

9.4.3.1 AC transition machines and auxiliary classes : : : : : : : : 233

9.4.4 The CW pattern matchers : : : : : : : : : : : : : : : : : : : : : : : 235

9.4.4.1 Safe shifters and auxiliary functions : : : : : : : : : : : : 236

9.4.5 The BM pattern matchers : : : : : : : : : : : : : : : : : : : : : : : 239

9.4.5.1 Safe shifters and auxiliary functions : : : : : : : : : : : : 240

9.4.6 Summary of user classes : : : : : : : : : : : : : : : : : : : : : : : : 243

9.5 Foundation classes : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 244

9.5.1 Miscellaneous : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 244

9.5.2 Arrays, sets, and maps : : : : : : : : : : : : : : : : : : : : : : : : : 245

9.5.3 Tries and failure functions : : : : : : : : : : : : : : : : : : : : : : : 247

9.6 Experiences and conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : 249

9.7 Obtaining and compiling the toolkit : : : : : : : : : : : : : : : : : : : : : : 250

10 FIRE Lite: FAs and REs in C++

253

10.1 Introduction and related work : : : : : : : : : : : : : : : : : : : : : : : : : 253

10.1.1 Related toolkits : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 253

10.1.2 Advantages and characteristics of FIRE Lite : : : : : : : : : : : : : : 254

10.1.3 Future directions for the toolkit : : : : : : : : : : : : : : : : : : : : 255

10.1.4 Reading this chapter : : : : : : : : : : : : : : : : : : : : : : : : : : 256

10.2 Using the toolkit : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 256

10.3 The structure of FIRE Lite : : : : : : : : : : : : : : : : : : : : : : : : : : : 259

10.4 REs and abstract FAs : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 260

CONTENTS

vii

10.5 Concrete FAs : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 262 10.5.1 Finite automata : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 263 10.5.2 "-free nite automata : : : : : : : : : : : : : : : : : : : : : : : : : : 264 10.5.3 Deterministic nite automata : : : : : : : : : : : : : : : : : : : : : 265 10.5.4 Abstract-states classes : : : : : : : : : : : : : : : : : : : : : : : : : 266 10.5.5 Summary of concrete automata classes : : : : : : : : : : : : : : : : 268
10.6 Foundation classes : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 268 10.6.1 Character ranges : : : : : : : : : : : : : : : : : : : : : : : : : : : : 269 10.6.2 States, positions, nodes, and maps : : : : : : : : : : : : : : : : : : 270 10.6.3 Bit vectors and sets : : : : : : : : : : : : : : : : : : : : : : : : : : : 271 10.6.4 Transitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 273 10.6.5 Relations : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 274
10.7 Experiences and conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : 275 10.8 Obtaining and compiling FIRE Lite : : : : : : : : : : : : : : : : : : : : : : 276

11 DFA minimization algorithms in FIRE Lite

277

11.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 277

11.2 The algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 278

11.2.1 Brzozowski's algorithm : : : : : : : : : : : : : : : : : : : : : : : : : 278

11.2.2 Equivalence relation algorithms : : : : : : : : : : : : : : : : : : : : 278

11.3 Foundation classes : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 281

11.4 Conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 282

IV The performance of the algorithms

283

12 Measuring the performance of algorithms

285

13 The performance of pattern matchers

287

13.1 Introduction and related work : : : : : : : : : : : : : : : : : : : : : : : : : 287

13.2 The algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 289

13.3 Testing methodology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 290

13.3.1 Test environment : : : : : : : : : : : : : : : : : : : : : : : : : : : : 290

13.3.2 Natural language test data : : : : : : : : : : : : : : : : : : : : : : : 291

13.3.3 DNA sequence test data : : : : : : : : : : : : : : : : : : : : : : : : 294

13.4 Results : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 294

13.4.1 Performance versus keyword set size : : : : : : : : : : : : : : : : : 294

13.4.2 Performance versus minimum keyword length : : : : : : : : : : : : 299

13.4.3 Single-keywords : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 303

13.5 Conclusions and recommendations : : : : : : : : : : : : : : : : : : : : : : : 305

viii CONTENTS

14 The performance of FA construction algorithms

311

14.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 311

14.2 The algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 311

14.3 Testing methodology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 312

14.3.1 Test environment : : : : : : : : : : : : : : : : : : : : : : : : : : : : 313

14.3.2 Generating regular expressions : : : : : : : : : : : : : : : : : : : : : 313

14.3.3 Generating input strings : : : : : : : : : : : : : : : : : : : : : : : : 315

14.4 Results : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 315

14.4.1 Construction times : : : : : : : : : : : : : : : : : : : : : : : : : : : 315

14.4.2 Constructed automaton sizes : : : : : : : : : : : : : : : : : : : : : : 319

14.4.3 Single transition performance : : : : : : : : : : : : : : : : : : : : : 321

14.5 Conclusions and recommendations : : : : : : : : : : : : : : : : : : : : : : : 324

15 The performance of DFA minimization algorithms

327

15.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 327

15.2 The algorithms : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 328

15.3 Testing methodology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 328

15.4 Results : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 330

15.5 Conclusions and recommendations : : : : : : : : : : : : : : : : : : : : : : : 335

V Epilogue

339

16 Conclusions

341

16.1 General conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 341

16.2 Chapter-speci c conclusions : : : : : : : : : : : : : : : : : : : : : : : : : : 344

16.3 A personal perspective : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 345

17 Challenges and open problems

347

References

349

Index 358

Summary

371

Samenvatting (Dutch summary)

373

Curriculum Vitae

375

List of Figures

2.1 An example of a tree : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 19

4.1 Taxonomy graph of pattern matching algorithms : : : : : : : : : : : : : : 44

4.2 Na ve part of the taxonomy : : : : : : : : : : : : : : : : : : : : : : : : : 48

4.3 The 3-cube of na ve pattern matching algorithms : : : : : : : : : : : : : 50

4.4 Example of a reverse trie : : : : : : : : : : : : : : : : : : : : : : : : : : : 53

4.5 Example of a forward trie : : : : : : : : : : : : : : : : : : : : : : : : : : 56

4.6 Aho-Corasick part of the taxonomy : : : : : : : : : : : : : : : : : : : : : 58

4.7 4.8

Example of function Example of function

f ef

: :

: :

: :

: :

: :

: :

:::::: ::::::

: :

:: ::

:::::: ::::::

: :

: :

: :

:::: ::::

65 72

4.9 Example of function N : : : : : : : : : : : : : : : : : : : : : : : : : : : : 79

4.10 Commentz-Walter part of the taxonomy : : : : : : : : : : : : : : : : : : 84

4.11 Boyer-Moore family part of the taxonomy : : : : : : : : : : : : : : : : : : 100

5.1 Example pattern matching FA : : : : : : : : : : : : : : : : : : : : : : : : 121 5.2 Precomputation algorithm dependency graph : : : : : : : : : : : : : : : : 136 5.3 Improved nite automaton : : : : : : : : : : : : : : : : : : : : : : : : : : 138

6.1 Taxonomy graph of automata constructions : : : : : : : : : : : : : : : : : 144

6.2 Tree view of an example RE. : : : : : : : : : : : : : : : : : : : : : : : : : 146

6.3 Tree form of an item. : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 147

6.4 An example of Symnodes : : : : : : : : : : : : : : : : : : : : : : : : : : : 150

6.5 Automaton CA((a ") (b )). : : : : : : : : : : : : : : : : : : : : : : : : 152

6.6 "-free constructions : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 154

6.7 Automaton (rem" CA)((a ") (b )) : : : : : : : : : : : : : : : : : : : : 156

6.8 6.9

AFAutpormoadtuocned(ubsyefCulosnstsruubcsteiton

rem" CA)((a ") (rem-", Wfilt) :

(b ::

)). ::

: :

: :

: :

: :

: :

: :

: :

: :

: :

: :

157 158

6.10 DFA produced by Construction (rem-", subset, use-s, Wfilt). : : : : 159

6.11 Encoding the "-free constructions : : : : : : : : : : : : : : : : : : : : : : 160

6.12 FA produced by Construction (rem-", sym, a-s). : : : : : : : : : : : : : 163

6.13 DFA produced by the McNaughton-Yamada-Glushkov construction. : : : 166

6.14 The derivatives constructions : : : : : : : : : : : : : : : : : : : : : : : : : 167

6.15 FA produced by Antimirov's construction. : : : : : : : : : : : : : : : : : 173

6.16 FA produced by Brzozowski's construction. : : : : : : : : : : : : : : : : : 173

6.17 The dual constructions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 176

ix

x LIST OF FIGURES
6.18 FA produced by Construction (rem-"-dual) : : : : : : : : : : : : : : : : 178 6.19 DFA produced by the Aho-Sethi-Ullman construction. : : : : : : : : : : : 180 7.1 Taxonomy graph of DFA minimization algorithms : : : : : : : : : : : : : 193 13.1 Algorithm performance versus keyword set size (superimposed) : : : : : : 295 13.2 Performance ratio of CW-WBM to CW-NORM versus keyword set size : 296 13.3 AC-FAIL performance versus keyword set size : : : : : : : : : : : : : : : 296 13.4 AC-OPT performance versus keyword set size : : : : : : : : : : : : : : : 297 13.5 CW-WBM performance versus keyword set size : : : : : : : : : : : : : : 297 13.6 CW-NORM performance versus keyword set size : : : : : : : : : : : : : : 298 13.7 Algorithm performance versus keyword set size (DNA | superimposed) : 298 13.8 Algorithm performance versus shortest keyword (superimposed) : : : : : 299 13.9 Performance ratio of CW-WBM to CW-NORM vs. the shortest keyword 300 13.10 AC-FAIL performance versus shortest keyword : : : : : : : : : : : : : : : 301 13.11 AC-OPT performance versus shortest keyword : : : : : : : : : : : : : : : 301 13.12 CW-WBM performance versus shortest keyword : : : : : : : : : : : : : : 302 13.13 CW-NORM performance versus shortest keyword : : : : : : : : : : : : : 302 13.14 Algorithm performance versus keyword length (DNA | superimposed) : 303 13.15 Algorithm performance (single-keyword) versus keyword length (superim-
posed) : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 304 13.16 KMP performance versus (single) keyword length : : : : : : : : : : : : : 305 13.17 AC-FAIL performance versus (single) keyword length : : : : : : : : : : : 306 13.18 AC-OPT performance versus (single) keyword length : : : : : : : : : : : 306 13.19 CW-WBM performance versus (single) keyword length : : : : : : : : : : 307 13.20 CW-NORM performance versus (single) keyword length : : : : : : : : : : 307 14.1 Construction times for FA constructions versus nodes : : : : : : : : : : : 316 14.2 Construction times for DFA constructions versus nodes : : : : : : : : : : 317 14.3 Construction times for FA constructions versus symbol nodes : : : : : : : 318 14.4 Construction times for DFA constructions versus symbol nodes : : : : : : 318 14.5 FA size versus nodes in the RE : : : : : : : : : : : : : : : : : : : : : : : 319 14.6 DFA size versus nodes in the RE : : : : : : : : : : : : : : : : : : : : : : : 320 14.7 FA size versus symbol nodes in the RE : : : : : : : : : : : : : : : : : : : 321 14.8 DFA size versus symbol nodes in the RE : : : : : : : : : : : : : : : : : : 322 14.9 Transition times for di erent types of automata : : : : : : : : : : : : : : 322 14.10 Transition times for FAs : : : : : : : : : : : : : : : : : : : : : : : : : : : 323 14.11 Transition times for "-free FAs : : : : : : : : : : : : : : : : : : : : : : : : 323 14.12 Transition times for DFAs : : : : : : : : : : : : : : : : : : : : : : : : : : 324 15.1 Performance versus DFA size | ASU and HU (superimposed) : : : : : : 330 15.2 Performance versus DFA size | BRZ, HOP and BW (superimposed) : : 331 15.3 ASU performance versus DFA size : : : : : : : : : : : : : : : : : : : : : : 332 15.4 HU performance versus DFA size : : : : : : : : : : : : : : : : : : : : : : 332

LIST OF FIGURES

xi

15.5 BRZ performance versus DFA size : : : : : : : : : : : : : : : : : : : : : : 333 15.6 HOP performance versus DFA size : : : : : : : : : : : : : : : : : : : : : 333 15.7 BW performance versus DFA size : : : : : : : : : : : : : : : : : : : : : : 334 15.8 BW performance versus DFA size, with anomaly : : : : : : : : : : : : : : 334 15.9 DFA fragment causing exponential BW behaviour : : : : : : : : : : : : : 335

Part I Prologue
1

Chapter 1
Introduction
In this chapter, we present an introduction to the contents and the structure of this dissertation.
1.1 Problem statement
A number of fundamental computing science problems have been extensively studied since the 1950s and the 1960s. As these problems were studied, numerous solutions (in the form of algorithms) were developed over the years. Although new algorithms still appear from time to time, each of these elds can be considered mature. In the solutions to many of the well-studied computing science problems, we can identify three de ciencies:
1. Algorithms solving the same problem are di cult to compare to one another. This is usually due to the use of di erent programming languages, styles of presentation, or simply the addition of unnecessary details.
2. Collections of implementations of algorithms solving a problem are di cult, if not impossible, to nd. Some of the algorithms are presented in a relatively obsolete manner, either using old notations or programming languages for which no compilers exist, making it di cult to either implement the algorithm or nd an existing implementation.
3. Little is known about the comparative practical running time performance of the algorithms. The lack of existing implementations in one and the same framework, especially of some of the older algorithms, makes it di cult to determine the running time characteristics of the algorithms. A software engineer selecting one of the algorithms will usually do so on the basis of the algorithm's theoretical running time, or simply by guessing.
In this dissertation, a solution to each of the three de ciencies is presented for each of the following three fundamental computing science problems:
3

4 CHAPTER 1. INTRODUCTION
1. Keyword pattern matching in strings. Given a nite non-empty set of keywords (the patterns) and an input string, nd the set of all occurrences of a keyword as a substring of the input string.
2. Finite automata (FA) construction. Given a regular expression, construct a nite automaton which accepts the language denoted by the regular expression.
3. Deterministic nite automata (DFA) minimization. Given a DFA, construct the unique minimal DFA accepting the same language.
We do not necessarily consider all of the known algorithms solving the problems. For example, we restrict ourselves to batch-style algorithms1, as opposed to incremental algorithms2. Some nite automata construction algorithms considered in Wat93a] can be used in a (rudimentary) incremental fashion (this is an coincidental side-e ect of the algorithm derivations presented there). A much more advanced treatment of incremental algorithms is given in HKR94], where the construction of nite automata (for compiler lexical analysis) is considered as an example.
In the following section, we present a broad overview of the structure of this dissertation, describing the solutions to the three de ciencies. Following this, is a discussion of the intended audience.
1.2 Structure of this dissertation
The dissertation is divided into ve parts. Parts I contains the prologue | the introduction (this chapter) and the mathematical preliminaries. Part V contains the epilogue | the conclusions, some challenges and directions for future work, the literature references, the index, the summary, the Dutch summary, and my curriculum vitae.
The di culty of comparing algorithms solving the same problem is addressed by constructing a taxonomy of all of the algorithms solving the particular problem. Part II presents a collection of such taxonomies. Since Chapter 3 contains an introduction to the method of constructing taxonomies, we present only a brief outline here. Each of the algorithms is rewritten in a common notation and inspected to determine its essential ideas and ingredients (collectively known as details). The details take one of two forms: problem details are restrictions of the problem, whereas algorithm details are transformations to the algorithm itself. Each algorithm can then be characterized by its set of constituent details. In constructing the taxonomy, the common details of several algorithms can be factored out and presented together. From this factoring process, we construct a `family tree' of the algorithms | indicating what any two of the algorithms have in common and where they
1A batch-style algorithm is one which performs some computation on its input, produces output, and terminates.
2An incremental algorithm is one which is able to deal with a change in the input without necessarily recomputing from scratch. For example, an incremental keyword pattern matching algorithm would be able to deal with the addition of a new pattern keyword, without redoing all of the precomputation.

1.3. INTENDED AUDIENCE

5

di er. After this introduction to the method of constructing taxonomies, the taxonomies themselves are presented in the remaining chapters of Part II. Pattern matching algorithms are considered in Chapter 4, FA construction algorithms in Chapter 6, and DFA minimization algorithms in Chapter 7. An additional chapter (Chapter 5) presents a new pattern matching algorithm (which was derived from one of the taxonomies), answering an open question posed by A.V. Aho in Aho80, p. 342].
Part III presents a pair of toolkits, thus solving the second of the three de ciencies previously outlined. The toolkits are implemented (as object-oriented C++ class libraries) directly from the taxonomies given in Part II. The inheritance hierarchy of each of the toolkits also follows directly from the family tree structure of the (respective) taxonomies. Chapter 8 provides an introduction to the design and implementation of class libraries, including the attendant problems and issues. The SPARE Parts3, a toolkit of pattern matching algorithms, is detailed in Chapter 9. Chapter 10 describes the FA construction algorithms implemented in FIRE Lite4, a toolkit of nite automata algorithms. The DFA minimization algorithms which are implemented in FIRE Lite are described in Chapter 11. This part assumes a good grasp of the C++ programming language and the related terminology; references for books covering C++ are given in Chapter 8.
In Part IV, we consider the practical performance of many of the algorithms derived in Part II and implemented in the toolkits, thereby addressing the third de ciency introduced above. A wide variety of input data was used to gather information on most of the algorithms presented in the taxonomies. Although there is little to say about such data-gathering methods, Chapter 12 lists the principles used. Chapters 13, 14, and 15 present data on the performance of the keyword pattern matching, FA construction, and DFA minimization algorithms, respectively.

1.3 Intended audience
The intended audience of this dissertation can be divided into a number of di erent groups. Each of these groups is mentioned in the following paragraphs, along with an outline of chapters and topics of particular interest to each group.
Taxonomists. A number of areas of computing science are now mature enough that the corresponding algorithms can be taxonomized. Creating a taxonomy serves to bring order to the eld, in a sense `cleaning it up'. Since the taxonomies also serve as useful teaching aids and surveys of the eld, it can be expected that more algorithm families will be taxonomized. The taxonomies given in Part II can be used as examples when creating new taxonomies. Algorithm designers. Embedded in each of the taxonomies are a number of new algorithms. The method of taxonomy development is well suited to the discovery
3String PAttern REcognition. 4FInite automata and Regular Expressions.

6 CHAPTER 1. INTRODUCTION
of new algorithms. Algorithm designers can use the method of taxonomization to develop new algorithms, and should read Part II. (Readers who think that the most e cient algorithms for these problems have already been discovered, should read Part IV, which shows that some of the new algorithms have practical importance.) The new regular expression pattern matching algorithm (given in Chapter 5) is a particularly good example of an algorithm which was designed using a mathematical approach to program construction. It is unlikely that the algorithm could have been developed using more traditional `hack and debug' techniques. Class library writers/generic program writers. Techniques for structuring and writing class libraries (especially generic class libraries | those that are type parameterized as in MeyB94]) are still in their infancy. Parts II and III together present a method of structuring class libraries and their inheritance hierarchies from the corresponding taxonomies. The fact that the taxonomies implicitly contain algorithm proofs, yields a high degree of con dence in the quality of the class libraries. Programmers. Programmers needing implementations of algorithms for keyword pattern matching, nite automata construction, or deterministic nite automata minimization should read the chapters of Part III which are relevant to their needs. A solid background in C++ and object-oriented programming is required. To best understand the algorithms, programmers should read the chapters of Part II which correspond to the toolkits they are using. When selecting an algorithm, a programmer can make use of the data and recommendations presented in Part IV. Software engineers. The taxonomies (Part II) provide a successful example of manipulating abstract algorithms, obtained directly from a speci cation, into more easily implemented and e cient ones. Furthermore, the abstract algorithms and the implementation techniques described in Chapter 8 combine well to produce the toolkits described in Part III.

Chapter 2
Mathematical preliminaries
In this chapter, we present a number of de nitions and properties required for reading this dissertation. This chapter can be skipped and referred to while reading individual chapters. De nitions that are used only in one chapter will be presented when needed, and can be considered `local' to that chapter.
2.1 Notations and conventions
In the taxonomies, we aim to derive algorithms that correspond to the well-known ones found in the literature. For this reason, we will frequently name variables, functions, predicates, and relations such that they correspond to their names as given in the literature.
Furthermore, we will adopt the commonly used names for standard concepts, such as O
for `big-oh' (running time) notation. While this makes it particularly di cult to adopt completely uniform naming conventions, the following conventions will be used for names that do not have historical signi cance.
Convention 2.1 (Naming functions, sets, etc.): We will adopt the following general
naming conventions: A; B; C for arbitrary sets. D; E; F; G; H for relations. V; W for alphabets. a; b; c; d; e for alphabet symbols. r; s; t; u; v; w; x; y;z for words (over an alphabet). L; P for languages. h; i; j; k for integer variables. M; N for nite automata (including Moore machines).
7

8 CHAPTER 2. MATHEMATICAL PRELIMINARIES

p; q; r for states, and Q for state sets.

Lower case Greek letters, such as ; ; for automata transition functions.

I; J for predicates used to express program invariants.

Functions, some relations, and predicates (other than those used to express program in-

variants) will frequently be given names longer than one letter, chosen to be suggestive of

their use. Sometimes subscripts, superscripts, hats, overbars or prime symbols will be used

in addition to one of the aforementioned names.

2

New terms will be typeset in an italic shape when they are rst mentioned or de ned.

Notation 2.2 (Symbol ?): We will frequently use the symbol ? (pronounced `bottom')

to denote an unde ned value (usually in the codomain of a function).

2

2.2 Basic de nitions

In this section, we present some basic de nitions which are not speci c to any one topic.

De nition 2.3 (Powerset): For any set A we use P(A) to denote the set of all subsets of A. P(A) is called the powerset of A; it is sometimes written as 2A in the literature. 2

De nition 2.4 (Alphabet): An alphabet is a nite non-empty set of symbols. We will

sometimes use the term character instead of symbol.

2

De nition 2.5 (Nondeterministic algorithm): An algorithm is called nondeterminis-

tic if the order in which its statements can be executed is not xed.

2

Notation 2.6 (Quanti cations): We assume that the reader has a basic knowledge of
the meaning of quanti cation. We use the following notation in this dissertation:

( a : R(a) : f(a))

where is the associative and commutative quanti cation operator (with unit e ), a is the dummy variable introduced (we allow the introduction of more than one dummy), R is the range predicate on the dummy, and f is the quanti cation expression (usually a function involving the dummy). By de nition, we have:

( a : false : f(a)) = e

The following table lists some of the most commonly quanti ed operators, their quanti ed symbols, and their units:

Operator _ (9) ^ (8) ( ) min (MIN) max (MAX) + ( )

Unit false true

+1 ?1 0

2.2. BASIC DEFINITIONS

9

2

Notation 2.7 (Sets): For any given predicate P, we use f a j P(a) g to denote the set of

all a such that P(a) holds.

2

De nition 2.8 (for-rof statement): The for-rof statement is taken from vdEi92].

Given predicate P and statement S, the statement for x : P ! S rof amounts to executing

statement list S once for each value of x that satis es P initially (assuming that there are

a nite number of such values for x). The order in which the values of x are chosen is

arbitrary.

2

Notation 2.9 (Conditional conjunction): We use cand and cor to refer (respectively)

to conditional conjunction and conditional disjunction.

2

De nition 2.10 (Sets of functions): For any two sets A and B, we use A ?! B to

denote the set of all total functions from A to B. We use A?6 ! B to denote the set of all

partial functions from A to B.

2

Notation 2.11 (Function signatures): For any two sets A and B, we use the notation

f 2 A ?! B to indicate that f is a total function from A to B. Set A is said to be

the domain of f while B is the codomain of f. We can also write dom(f) = A and

codom(f) = B.

2

Convention 2.12 (Relations as functions): For sets A and B and relation E A B, we can interpret E as a function E 2 A ?! P(B) de ned as E(a) = f b j (a; b) 2 E g, or as a function E 2 P(A) ?! P(B) de ned as E(A0) = f b j (9 a : a 2 A0 : (a; b) 2 E) g. 2

Notation 2.13 (Naturals and reals): We use the symbols N and R to denote the set

of all natural numbers, We will also de ne i; j

and )=

the
fk j

set i

of all k<

real
j^

numbers
k 2 Ng,

respectively.
(i; j] = f k j

De i<

ne k

N+
j

=
^

N
k

n 2

f0g. N g,

i; j] = i; j) (i; j], and (i; j) = i; j) \ (i; j].

2

De nition 2.14 (Relation composition): Given sets A; B; C (not necessarily di erent)
and two relations, E A B and F B C, we de ne relation composition (in x operator ) as
E F = f (a; c) j (9 b : b 2 B : (a; b) 2 E ^ (b; c) 2 F) g 2

We will also use the symbol for the composition of functions.

Remark 2.15: Note that function composition is di erent from relation composition.

This can cause confusion when we are viewing relations as functions. For this reason, it

will be clear from the context which type of composition is intended.

2

10 CHAPTER 2. MATHEMATICAL PRELIMINARIES

De nition 2.16 (Relation exponentiation): Given set A and relation E A A, we

de A)

ne relation exponentiation recursively as: E0 and Ek = E Ek?1 (k 1). Note that IA is

= IA (where IA the unit of .

is

the

identity

relation

on 2

De nition 2.17 (Closure operator on relations): Given set A and relation E
A, we de ne the -closure of E as: E = ( i : 0 i : Ei)
Note, if E is symmetrical then E is an equivalence relation.

A 2

De nition 2.18 (Idempotence): For any set A and function f 2 A ?! A, we say that

f is idempotent if f f = f.

2

Notation 2.19 (Alternation expressions): For any given predicate P, we use the

following shorthand

if P then e1 else e2

=

(

e1 e2

if P otherwise

2

De nition 2.20 (Minimum and maximum): De ne max and min to be in x binary
functions on integers such that
i maxj = if i j then i else j

i minj = if i j then i else j

2

Recall from Notation 2.6 that max and min have as units ?1 and +1, respectively.
We will sometimes de ne functions with codomain N; these functions will frequently
have de nitions involving MIN or MAX quanti cations, which can have value +1 or ?1 (respectively). For notational convenience, we assume +1; ?1 2 N.

Property 2.21 (Conjunction and disjunction in MIN quanti cations): For pred-
icates P; P 0 and integer function f we have the following two properties:
(MIN i : P(i) ^ P0(i) : f(i)) (MIN i : P(i) : f(i)) max(MIN i : P0(i) : f(i)) (MIN i : P(i) _ P0(i) : f(i)) = (MIN i : P(i) : f(i)) min(MIN i : P0(i) : f(i))
2

2.2. BASIC DEFINITIONS

11

Property 2.22 (MIN and MAX quanti cations with quanti ed ranges): Given
that universal quanti cation over a nite domain is shorthand for conjunction, and existential quanti cation over a nite domain is shorthand for disjunction, we have the following general properties (where P is some range predicate, f is some integer function, and the 8 and 9 quanti ed j is over a nite domain):
(MIN i : (8 j :: P(i; j)) : f(i)) (MAX j :: (MIN i : P(i; j) : f(i))) (MIN i : (9 j :: P(i; j)) : f(i)) = (MIN j :: (MIN i : P(i; j) : f(i)))
2

De nition 2.23 (Precedence of operators): We specify that set operators have the

following descending precedence: , \, and .

2

De nition 2.24 (Tuple projection operators): For an n-tuple t = (x1; : : : ; xn) we

use (1

the i

notation i(t) (1 n) to denote the

i
(n ?

naturally to sets of tuples.

n) to denote 1)-tuple (x1; : :

tuple element : ; xi?1; xi+1; : :

xi; we : ; xn).

use the Both i

notation i(t)

and

i

extend 2

Convention 2.25 (Tuple arguments to functions): For functions (or predicates) tak-

ing a single tuple as an argument, we usually drop one set of parentheses in a function

application.

2

De nition 2.26 (Tuple and relation reversal): For an n-tuple (x1; x2; : : :; xn) de ne
reversal as function R given by:

(x1; x2; : : : ; xn)R = (xn; : : : ; x2; x1)

2

Forward reference 2.27: We will also be de ning reversal of strings (in De nition 2.40).
These operators extend naturally to sets of tuples (relations) and to sets of strings (languages). A reversal operator is usually written as a post x and superscript operator; however, we will sometimes write it as a normal function. Reversal operators are their own inverses. In subsequent sections of this dissertation, we will also be de ning reversal operators for more complex structures, such as nite automata and regular expressions. 2

De nition 2.28 (Dual of a function): We assume two sets A and B whose reversal operators are RA and RB respectively. Two functions f 2 A ?! B and fd 2 A ?! B are
one another's dual if and only if

f(a)RB = fd(aRA)

2

De nition 2.29 (Symmetrical function): A symmetrical function is one that is its own
dual. 2

12 CHAPTER 2. MATHEMATICAL PRELIMINARIES

Proposition 2.30 (Symmetrical functions): The composition of two symmetricalfunc-

tions is again symmetrical.

2

Notation 2.31 (Equivalence classes of an equivalence relation): For any equiva-

lence relation E
a 2 a]E. We also

on set A, we use denote the set of

eqau]EivatolendceenocltaesstehseosfeEt

fb
by

j (a; b) 2
A]E; that

E is

g.

Note that

A]E = f a]E j a 2 A g

The set A]E is also called the partition of A induced by E. Since all partitions are induced

by a unique equivalence relation, we will sometimes also refer to the equivalence relation

inducing a particular partition.

2

De nition 2.32 (Index of an equivalence relation): For equivalence relation E on

set A, de ne index of E.

]E

=

j

A]Ej

(i.e.

the

number

of

equivalence

classes

under

E).

]E

is

called

the 2

De nition 2.33 (Re nement of an equivalence relation): For equivalence relations

E and E0 (on set A), E is a re nement of E0 (written E v E0) if and only if E E0. An

equivalent statement is that E v E0 if and only if every equivalence class (of A) under E

is entirely contained in some equivalence class (of A) under E0.

2

De nition 2.34 (Re nement relation on partitions): We can also extend our re-

nement relation to partitions. For equivalence relations E and E0 (on set A), we write

A]E v A]E0 if and only if E v E0.

2

Property 2.35 (Equivalence relations): Given two equivalence relations E; F of nite
index, we have the following property:

(E v F) ^ (]E = ]F) ) (E = F)

2

De nition 2.36 (Complement of a relation): Given two sets (not necessarily distinct)

A and B, and relation E A B we de ne the complement of relation E (written :E)

as :E = (A B) n E.

2

De nition 2.37 (Preserving a predicate): A function f 2 Bn ?! B (for xed n 1)
is said to preserve predicate (or property) P (on B) if and only if

(8 b : b 2 Bn \ (dom(f)) ^ (8 k : 1 k n : P ( k(b))) : P (f(b)))

2

Intuitively, a function f preserves a property P if, when every argument of f satis es P, the result of f applied to the arguments also satis es P.

2.3. STRINGS AND LANGUAGES
2.3 Strings and languages

13

In this section, we present a number of de nitions relating to strings and languages.

De nition 2.38 (Set of all strings): Given alphabet V , we de ne V to be the set of

all strings over V . For more on this, see De nition 2.48.

2

Notation 2.39 (Empty string): We will use " to denote the string of length 0 (the

empty string). Some authors use or to denote the empty string.

2

De nition 2.40 (String reversal function R): Assuming alphabet V , we de ne string

reversal function R recursively as "R = " and (aw)R = wRa (for a 2 V; w 2 V ).

2

De nition 2.41 (String operators ; ; ; ): Assuming alphabet V , we de ne four (in x) operators ; ; ; 2 V N ?! V as follows:

w k is the k minjwj leftmost symbols of w

w k is the (jwj ? k) max0 rightmost symbols of w

w k is the k minjwj rightmost symbols of w

w k is the (jwj ? k) max0 leftmost symbols of w

The four operators are pronounced `left take', `left drop', `right take', and `right drop'

respectively.

2

Property 2.42 (String operators ; ; ; ): Note that

(w k)(w k) = w

and

(w k)(w k) = w

2

Example 2.43 (String operators ; ; ; ): (baab) 3 = baa, (baab) 1 = aab, (baab) 5 =

baab, and (baab) 10 = ".

2

De nition 2.44 (Language): Given alphabet V , any subset of V is a language over V .
2

De nition 2.45 (Concatenation of languages): Language concatenation is an in x operator 2 P(V ) P(V ) ?! P(V ) (the dot) de ned as

L L0 = ( x; y : x 2 L ^ y 2 L0 : fxyg)

The singleton language f"g is the unit of concatenation and the empty language is the

zero of concatenation.

2

14 CHAPTER 2. MATHEMATICAL PRELIMINARIES

Notation 2.46 (Concatenation of languages): We will frequently use juxtaposition

instead of writing operator (i.e. we use LL0 instead of L L0). For language L and string

w, we take Lw to mean Lfwg.

2

De nition 2.47 (Language exponentiation): We de ne language exponentiation re-

cursively as follows (for language L): L0 = f"g and Lk = LLk?1 (k 1).

2

Note that V k is the set of all strings of length k over alphabet V .

De nition 2.48 (Closure operators on languages): We de ne two post x and superscript operators on languages over alphabet V . Operator 2 P(V ) ?! P(V ) (known
as Kleene closure) is L = ( i : 0 i : Li)

and operator + 2 P(V ) ?! P(V ) is
L+ = ( i : 1 i : Li)
Note that L = L+ f"g.

2

The language V is the set of all strings over alphabet V .

De nition 2.49 (Unary language operators : and ?): Assuming an alphabet V , prex operator : 2 P(V ) ?! P(V ) is de ned as

:L = V n L

while post x and superscript operator ? 2 P(V ) ?! P(V ) is de ned as

L? = L f"g

2

De nition 2.50 (Functions pref and su ): For any given alphabet V , de ne pref 2 P(V ) ?! P(V ) and su 2 P(V ) ?! P(V ) as

pref(L) = ( x; y : xy 2 L : fxg)

and

su (L) = ( x; y : xy 2 L : fyg)

Intuitively, pref(L) (respectively su (L)) is the set of all strings which are (not necessarily

proper) pre xes (respectively su xes) of strings in L.

2

Property 2.51 (Idempotence of pref and su ): It follows from their de nitions that

pref and su are both idempotent.

2

2.3. STRINGS AND LANGUAGES

15

Property 2.52 (Duality of pref and su ): Functions pref and su are duals of one
another. This can be seen as follows:
pref(LR) = f de nition of pref g
( x; y : xy 2 LR : fxg) = f xy 2 LR (xy)R 2 L g
( x; y : (xy)R 2 L : fxg) = f (xy)R = yRxR g
( x; y : yRxR 2 L : fxg) = f change of bound variable: x0 = xR, y0 = yR g
( x0; y0 : y0x0 2 L : fx0Rg) = f R distributes over g
( x0; y0 : y0x0 2 L : fx0g)R = f de nition of su g
su (L)R
2

Notation 2.53 (String arguments to functions pref and su ): For string w 2 V ,

we will write pref(w) instead of pref(fwg) (and likewise for su ).

2

Property 2.54 (Function su ): For non-empty language L and alphabet symbol a 2 V , function su has the property:
su (La) = su (L)a f"g
2

Property 2.55 (Non-empty languages and pref; su ): For any L =6 , " 2 pref(L)

and " 2 su (L).

2

Property 2.56 (Intersection and pref): Given languages A; B
we have the following property:
Afyg \ B = (A \ pref(B))fyg \ B

V and string y 2 V 2

De nition 2.57 (Pre x
phabet V , partial orders
u s v u 2 su (v).

and su
p and s

x partial orderings p and
over V V are de ned as u

s):
pv

For any given al-
u 2 pref(v) and
2

16 CHAPTER 2. MATHEMATICAL PRELIMINARIES

De nition 2.58 (Operator max we de ne binary in x operator max

s):
s

In a manner analogous to on strings as (provided x

integer
sy_y

operator s x):

max,

x max sy = if x s y then y else x

uTsheidWs ioenpceSoreuactltdoirohnias4va.e3lsgowivahesensnoacwniaetacinvoaenlosaigndodeurscqooumpamenrtuaittoacrtaitvfioeor.ntTshhienevpuorlneviitxnogofmrmdeaarxxingss;.

is ", which will be since it would not

be used in this dissertation, we do not de ne it here.

2

Property 2.59 (Function su ): If A and B are languages, then

su (A) \ B =6 A \ V B 6=

2

Property 2.60 (Language intersection): If A and B are languages over alphabet V and a 2 V , then

V A \ V B 6= V A \ B 6= _ A \ V B =6

and

V aA \ V B 6= V aA \ B =6 _ A \ V B =6

2

2.4 Regular expressions
In this section, we present some de nitions and properties relating to regular expressions.
De nition 2.61 (Regular expressions and their languages): We simultaneously de-
ne regular expressions over alphabet V (the set RE) and the languages they denote (given
by function LRE 2 RE ?! P(V )) as follows: " 2 RE and LRE (") = f"g 2 RE and LRE( ) = For all a 2 V , a 2 RE and LRE(a) = fag For E; F 2 RE { E F 2 RE and LRE(E F ) = LRE (E) LRE(F ) { E F 2 RE and LRE (E F ) = LRE(E) LRE(F ) { E 2 RE and LRE(E ) = LRE (E)

2.4. REGULAR EXPRESSIONS

17

{ E+ 2 RE and LRE (E+) = LRE(E)+ { E? 2 RE and LRE(E?) = LRE (E)?
Nothing else is in RE
Operators ; +; ? have the highest precedence, followed by , and nally .

2

Some authors write j or + instead of .
Note that regular expressions are syntactic objects which denote languages, even though
they may look the same (as in the cases of , ", and a 2 V ). Whether we are dealing with
regular expressions or languages will be clear from the context.

De nition 2.62 (RE reversal): Regular expression reversal is given by the post x (superscript) function R 2 RE ?! RE

"R
R

aR

(E0 (E0 (E

)REE1)1R)R

(E+)R

(E?)R

="

=

=a

= = =

(((EEE10RRR)))

(E(E0R1R))

= (ER)+

= (ER)?

(for all a 2 V )

Function R satis es the obvious property that

(8 E : E 2 RE : (ER)R = E ^ (LRE (ER))R = LRE(E))

2

Remark 2.63: The property satis ed by regular expression reversal implies that function

LRE is symmetrical.

2

De nition 2.64 (Regular languages): The set of all regular languages over alphabet V
are de ned as:

f LRE (E) j E 2 RE g

2

Remark 2.65: The set of regular languages could have been de ned in a number of

other (equivalent) ways, for example, as the set of all languages accepted by some nite

automaton.

2

18
2.5 Trees

CHAPTER 2. MATHEMATICAL PRELIMINARIES

In this section, we present a number of tree-related de nitions. We give a de nition of trees which is slightly di erent than the traditional recursive one: we use the tree domain approach to trees, which will allow us to easily access individual nodes of a tree.

Notation 2.66 (Strings over N+): In order to designate particular nodes of a tree, we

will be using strings over N+ (even though it is not nite, and therefore not an alphabet).

To avoid confusion when writing such a string (element of N+ ), we make the concatenation

operator explicit. Instead of writing it as the dot, for clarity we write it as . Furthermore,

we will write the empty string " as 0.

2

De nition 2.67 (Tree domain): A tree domain D is a non-empty subset of N+ such
that the following two conditions hold:

1. D is pre x-closed: pref(D) D.

2. For all x 2 N+ and i; j 2 N+: x j 2 D ^ i < j ) x i 2 D.

2

Example 2.68 (Tree domain): The set f0; 1; 2; 2 1; 2 2g is a tree domain. The set

f0; 1; 2 1; 2 2g is not a tree domain since it is not pre x-closed (it does not contain 2).

The set f0; 1; 2; 2 2g is not a tree domain since it does not satisfy the second requirement

(it should also contain 2 1 since it contains 2 2 and 1 < 2).

2

De nition 2.69 (Ranked alphabet): A ranked alphabet is a pair (V; r) such that V is an alphabet and r 2 V ?! N. r(a) is called the rank of symbol a. De ne Vn = r?1(n).
Symbols of rank 0 are called nullary symbols, while those of rank 1 are called unary symbols. 2

Example 2.70 (Ranked alphabet): The pair (fa; bg; f(a; 2); (b; 0)g) is a ranked alpha-

bet (with a as binary symbol and b as nullary symbol).
V2 = fag. There are no unary symbols.

We also have V0

= fbg and 2

De nition 2.71 (Tree): Let (V; r) be a ranked alphabet. A tree A over (V; r) is a function A 2 D ?! V (where D is a tree domain) such that

(8 a : a 2 D : r(A(a)) = (MAX i : i 2 N ^ a i 2 D : i))

Set D (equivalently dom(A)) are called the nodes of tree A. A(a) is the label of a. 2

De nition 2.72 (Set Trees): De ne Trees(V; r) to be the set of all trees over ranked

alphabet (V; r).

2

De nition 2.73 (Nodes of a tree): Assuming a tree A, we can make the following
de nitions:

2.6. FINITE AUTOMATA AND MOORE MACHINES

19

a 12 ba
12 bb

Figure 2.1: An example of a tree
Node 0 is called the root of A. A node with a nullary label is a leaf. Nodes that are not leaves are internal nodes.
2
Example 2.74 (Tree): Using the tree domain from Example 2.68 and the ranked alphabet
from Example 2.70, we can give the following tree (in tabular form): Node 0 1 2 2 1 2 2 Label a b a b b
The tree can also be presented in the more usual graphical form, as in Figure 2.1. 2
2.6 Finite automata and Moore machines
In this section, we de ne nite automata, Moore machines, some of their properties, and some transformations on them.
De nition 2.75 (Finite automaton): A nite automaton (also known as an FA) is a
6-tuple (Q; V; T; E; S; F) where Q is a nite set of states. V is an alphabet.
T 2 P(Q V Q) is a transition relation. E 2 P(Q Q) is an "-transition relation.

20 CHAPTER 2. MATHEMATICAL PRELIMINARIES

S Q is a set of start states.

F Q is a set of nal states.

2

Remark 2.76: We will take some liberty in our interpretation of the signatures of the

transition relations. For example, we also use the signatures T 2 V ?! P(Q Q), T 2

Q Q ?! P(V ), T 2 Q V ?! P(Q), T 2 Q ?! P(V Q), T 2 P(Q) V ?! P(Q),

and E 2 Q ?! P(Q). In each case, the order of the Qs from left to right will be preserved;

for example, the function T 2 Q ?! P(V Q) is de ned as T(p) = f (a; q) j (p; a; q) 2 T g.

The signature that is used will be clear from the context.

2

Remark 2.77: Our de nition of nite automata di ers from the traditional approach in
two ways:

Multiple start states are permitted.

The "-transitions (relation E) are separate from transitions on alphabet symbols (relation T).

2

Since we only consider nite automata in this dissertation, we will frequently simply use the term automata.
Convention 2.78 (Finite automaton state graphs): When drawing the state graph
corresponding to a nite automaton, we adopt the following conventions: All states are drawn as circles or ovals (vertices).
Transitions are drawn as labeled (with " or alphabet symbol a 2 V ) directed edges
between states. Start states have an in-transition with no source (the transition does not come from another state). Final states are drawn as two concentric circles or ovals. For example, the FA below has two states, one is the start state, and other is the nal state, with a transition on a:
a

2

2.6. FINITE AUTOMATA AND MOORE MACHINES

21

De nition 2.79 (Moore machine): A Moore machine (also known as an MM) is a
6-tuple (Q; V; W; T; ; S) where Q is a nite set of states.
V is an input alphabet.
W is an output alphabet.
T 2 P(Q V Q) is a transition relation. 2 Q ?! W is an output function.
S Q is a set of start states. 2
Note that an MM does not have any "-transitions, as this would complicate the de nition of the transduction of the MM (transduction will be de ned later).

2.6.1 De nitions and properties involving FAs and MMs

In this subsection we de ne some properties of nite automata. Many of these are easily extended to Moore machines, and we do not present the Moore machine versions. To make these de nitions more concise, we introduce particular nite automata M = (Q; V; T; E; S; F ), M0 = (Q0; V0; T0; E0; S0; F0), and M1 = (Q1; V1; T1; E1; S1; F1).

De nition 2.80 (Size of an FA): De ne the size of an FA as jMj = jQj.

2

De nition 2.81 (Isomorphism (=) of FAs): We de ne isomorphism (=) as an equiva-

lence relation on and there exists

aFAbisj.ecMti0onangd2MQ1 0a?re!isoQm1 osrupchhicth(watritten

M0

=

M1)

if

and

only

if

V0

=

V1

T1 = f (g(p); a; g(q)) j (p; a; q) 2 T0 g,

E1 = f (g(p); g(q)) j (p; q) 2 E0 g,

S1 = f g(s) j s 2 S0 g, and F1 = f g(f) j f 2 F0 g.

2

De nition 2.82 (Extending the transition relation T): We extend transition function T 2 V ?! P(Q Q) to T 2 V ?! P(Q Q) as follows:

T (") = E
and (for a 2 V; w 2 V )

T (aw) = E T(a) T (w) This de nition could also have been presented symmetrically.

2

22 CHAPTER 2. MATHEMATICAL PRELIMINARIES

Remark 2.83: We sometimes also use the signature T 2 Q Q ?! P(V ).

2

Remark 2.84: If E = then E =
states of M.

=

IQ

where

IQ

is

the

identity

relation

on

the 2

De nition 2.85 (The language between states): The language between any two states q0; q1 2 Q is T (q0; q1). Intuitively, T (q0; q1) is the set of all words on paths from q0 to q21.

De nition 2.86
given by function

(L?LMeft2aQnd?!rigPh(tVla)n, gwuhaegrees):

The

left

language

of

a

state

(in

M)

is

L?M(q) = ( s : s 2 S : T (s; q))

The right language of a state (in M) is given by function ?!L M 2 Q ?! P(V ), where

?!L M(q) = ( f : f 2 F : T (q; f))

The subscript M is usually dropped when no ambiguity can arise.

2

De nition 2.87 (Language of an FA): The language of a nite automaton (with alphabet V ) is given by the function LFA 2 FA ?! P(V ) de ned as:
LFA(M) = ( s; f : s 2 S ^ f 2 F : T (s; f)) 2

Property 2.88 (Language of an FA): From the de nitions of left and right languages
(of a state), we can also write:
LFA(M) = ( f : f 2 F : L?(f))

and
LFA(M) = ( s : s 2 S : ?!L (s))

2

De LFA

nition 2.89 (Extension of LFA):
(M). The choice of representative

language.

Function LFA
is irrelevant,

is extended to as isomorphic

FA]= as LFA( M]=) =
FAs accept the same 2

De nition 2.90 (Complete): A Complete nite automaton is one satisfying the following:

Complete(M) (8 q; a : q 2 Q ^ a 2 V : T(q; a) =6 )

Intuitively, an FA is Complete when there is at least one out transition from every state

on every symbol in the alphabet.

2

2.6. FINITE AUTOMATA AND MOORE MACHINES

23

Property 2.91 (Complete): For all Complete FAs (Q; V; T; E; S; F): ( q : q 2 Q : L?(q)) = V

2

Instead of accepting a string (as FAs do), a Moore machine transduces the string. The transduction of a string is de ned as follows.

De nition 2.92 (Transduction by a MM): Given Moore machine N = (Q; V; W; T; ; S) we de ne transduction helper function HN 2 P(Q) V ?! (P(W)) as

HN(Q0; ") = "

and (for a 2 V , w 2 V )

HN(Q0; aw) = f (q) j q 2 T (Q0; a) g HN(T (Q0; a); w)

The transduction of a string is given by function 2 V ?! (P(W)) de ned as

N(w) = f (s) j s 2 S g HN(S; w)

The codomains of HN and N are strings over alphabet P(W ). Note that both functions

depend upon N.

2

De nition 2.93 ("-free): Automaton M is "-free if and only if E = .

2

Remark S \ F =6

2.94:
.

Even if M

is "-free

it is still possible that

"

2

LFA (M ):

in this case 2

De nition 2.95 (Reachable states): For M 2 FA we can de ne a reachability relation

Reach(M) Q Q

as

Reach(M) = ( 2(T ) E) (In this de nition, we have simply projected away the symbol component of the transition relation. State p reaches state q if and only if there is an "-transition or a symbol transition from p to q.) Similarly, the set of start-reachable states is de ned to be (here, we interpret
the relation as a function Reach(M) 2 P(Q) ?! P(Q)):

SReachable(M) = Reach(M)(S)

and the set of nal-reachable states is de ned to be: FReachable(M) = (Reach(M))R(F )

The set of useful states is:
Reachable(M) = SReachable(M) \ FReachable(M)

2

24 CHAPTER 2. MATHEMATICAL PRELIMINARIES

Property 2.96 (Reachability): For automaton M = (Q; V; T; E; S; F), SReachable sat-
is es the following interesting property:
q 2 SReachable(M) L?M(q) =6
FReachable satis es a similar property:
q 2 FReachable(M) ?!L M(q) 6=
2

De nition 2.97 (Useful automaton): A Useful nite automaton M is one with only
reachable states:

Useful(M) (Q = Reachable(M))

2

De
only

nition 2.98 (Start-useful
start-reachable states:

automaton):

A

Useful s

nite automaton M is one with

Usefuls(M) (Q = SReachable(M))

2

De
only

nition 2.99 (Final-useful
nal-reachable states

automaton):

A

Useful

f

nite automaton M is one with

Usefulf(M) (Q = FReachable(M))

2

iRneTmraanrskfo2rm.1a0t0io:nU2s.e1f1u3l)s.

and For

Useful
all M

2f

are FA

closely related by FA we have Usefulf(M)

reversal Useful

(to s(M

be R).

presented 2

Property 2.101 (Implication of Usefulf): Usefulf has the property: Usefulf(M) ) (8 q : q 2 Q : L?(q) pref(LFA(M)))

2

De nition 2.102 (Deterministic nite automaton): A nite automaton M is deter-
ministic if and only if

It has one start state or no start states.

It is "-free.

2.6. FINITE AUTOMATA AND MOORE MACHINES

25

Transition function T 2 Q V ?! P(Q) does not map pairs in Q V to multiple
states.

Formally,

Det(M) jSj 1 ^ "-free(M) ^ (8 q; a : q 2 Q ^ a 2 V : jT(q; a)j 1)

2

De nition 2.103 (Deterministic FAs): DFA denotes the set of all deterministic nite

automata. We call FA n DFA the set of nondeterministic nite automata.

2

De nition 2.104 (Deterministic Moore machine): The de nition of a deterministic
Moore machine is similar to that of a DFA. A Moore machine N = (Q; V; W; T; ; S) is deterministic if and only if

It has one start state or no start states.

Transition function T 2 Q V ?! P(Q) does not map pairs in Q V to multiple
states.

Formally,

Det(N) jSj 1 ^ (8 q; a : q 2 Q ^ a 2 V : jT(q; a)j 1)

2

De nition 2.105 (Deterministic MMs): DMM denotes the set of all deterministic

Moore machines.

2

Convention 2.106 (Transition function of a DFA): For (Q; V; T; ; S; F) 2 DFA we

can consider the transition function to have signature T 2 Q V ?6 ! Q. The transition

function is total if and only if the DFA is Complete.

2

Property 2.107 (Weakly deterministic automaton): Some authors use a property
of a deterministic automaton that is weaker than Det; it uses left languages and is de ned as follows:
Det0(M) (8 q0; q1 : q0 2 Q ^ q1 2 Q ^ q0 =6 q1 : L?(q0) \ L?(q1) = ) 2

Remark 2.108:
there exists an M

2DeFtA(Msu)c)h thDaettD0(Met0)(Mis)e^as:ilyDeptr(oMve)d;.naWmeelcyan

also

demonstrate

that

(fq0; q1g; fbg; f(q0; b; q0); (q0; b; q1)g; ; ; ) In this FA, L?(q0) = L?(q1) = , but state q0 has two out-transitions on symbol b.

2

26 CHAPTER 2. MATHEMATICAL PRELIMINARIES

De nition 2.109 (Minimality of a DFA): An M 2 DFA is minimal as follows:

Min(M) (8 M0 : M0 2 DFA ^ LFA(M) = LFA(M0) : jMj jM0j)

Predicate Min is de ned only on DFAs. Some later de nitions are simpler if we de ne a minimal, but Complete, DFA as follows MinC(M)
(8 M0 : M0 2 DFA ^ Complete(M0) ^ LFA(M) = LFA(M0) : jMj jM0j)

Predicate MinC is de ned only on Complete DFAs.

2

Property 2.110 (Minimality of a DFA): An M, such that Min(M), is the unique

(modulo =) minimal DFA.

2

Minimality of DMMs is discussed in Section 4.3.3 on page 68.

Property 2.111 (An alternate de nition of minimality of a DFA): For minimizing
a DFA, we use the predicate Minimal(Q; V; T; ; S; F )

(8
^

q0; q1 : q0 2 Q ^
Useful(Q; V; T;

q1 2 Q
; S; F)

^

q0

=6

q1

:

?!L (q0)

=6

?!L (q1))

(This predicate is de ned only on DFAs.) A similar predicate (relating to MinC) is MinimalC(Q; V; T; ; S; F )

(8
^

q0; q1 : q0 2 Q ^
Usefuls(Q; V; T;

q1 ;

2
S;

FQ)^^qC0 o=6 mqp1le:te?!L(Q(q;0V);=6 T

?!L (q1))
; ; S; F

)

(This predicate is only de ned on Complete DFAs.)
We have the property that (for all M; MC 2 DFA such that Complete(MC))

Minimal(M) Min(M) ^ MinimalC(MC) MinC(MC)

Proof: FMoirnSibimnrecavelCit:(yMM, wi)n,eicmoonanllsyCid(peMrro)ivt,esactMolnienatirsmatpaoolnsCei(tMiovfe)th:eMfMionlliionmwCai(lnMCg()Mh.o)lIdn)s:o:rdMerintCo(Mpro).ve MinC(M) )

There is a start-unreachable
that :MinC(M).

state

(:Useful s(M ))

which

can

be

removed,

meaning

:Complete(M). It follows that :MinC(M). There are two states p; q such that ?!L (p) = ?!L (q). In this case, the in-transitions to q can be redirected to p, and q can be entirely eliminated, meaning that :MinC(M).

2.6. FINITE AUTOMATA AND MOORE MACHINES

27

We now continue with the proof that MinimalC(M) ) MinC(M). The following proof is

by contradiction. contradiction, we

Assume a DFA M = assume the existence

(oQf ;aVC; Tom; p;leSt;eFD)FsAucMh t0h=at(QM0i;nVi;mTa0l;C

(M). For the ; S0; F 0) such

tohf attheLirFAle(fMt l)an=guLaFgAe(sM, f0)L?an(qd)

jM0j < jq2Q

jgManj.dSfinL?ce(qb)ojthq

M and
2 Q0 g,

M0 are

are Complete, partitions of V

the sets with

jM0j = jf L?(q) j q 2 Q0 gj < jf L?(q) j q 2 Q gj = jMj

By the pigeonhole principle, there exist words x; y which are in the same equivalence class

of the M0 partition, but in di erent equivalence classes of the M partition. More precisely,

therHeoewxeisvterx,;?!yL2(TV z 2 V such that

such (S; x))

t6=ha?!Lt T(T0

(S0; (S;

x) = T 0 (S0; y) ^ T
y)) since MinimalC

(S; x) =6 T (S; y).
(M). It follows that

there

exists

z 2 ?!L (T (S; x)) 6 z 2 ?!L (T (S; y))

or, equivalently

xz 2 LFA(M) 6 yz 2 LFA(M)
Returning to M0, we have

T 0 (S0; xz) = T 0 (T 0 (S0; x); z) = T 0 (T 0 (S0; y); z) = T 0 (S0; yz)

and so

xz 2 LFA(M0) yz 2 LFA(M0) This gives LFA(M) 6= LFA(M0), which is a contradiction.

2

Two automata M and Minimal(M)

^anMd iMni0msaulcCh(tMha)twLoFuAl(dMb)e

= LFA(M0), :Complete(M) ^
isomorphic except for the fact

Complete (M 0), that M0 would

have a sink state.

Remark 2.112: In the literature the second conjunct in the de nition of predicate
MinimalC is sometimes erroneously omitted. The necessity of the conjunct can be seen by considering the DFA

(fp; qg; fag; f(p; a; p); (q; a; q)g; ; ; fpg)

H?!Le(rqe)

L?(p)
=.

= L?(q) =
Without the

(which is also second conjunct,

the this

this is not the case, as the minimal Complete

language of the DFA), ?!L (p) = fag , and

DFA DFA

would be considered Minimal
accepting is ( ; fag; ; ;

C; ;

clearly ). 2

28 CHAPTER 2. MATHEMATICAL PRELIMINARIES

2.6.2 Transformations on nite automata
In this section, we present a number of transformations on nite automata. Many of these could also be de ned for Moore machines, although they will not be needed.

Transformation 2.113 (FA reversal): FA reversal is given by post x (superscript) function R 2 FA ?! FA, de ned as:
(Q; V; T; E; S; F )R = (Q; V; T R; ER; F; S)

Function R satis es
(8 M : M 2 FA : (LFA(M))R = LFA(MR)):
and preserves "-free and Useful.

2

Remark 2.114: The property (LFA(MR))R
own dual, and is therefore symmetrical.

=

LFA (M )

means

that

function

LFA

is

its 2

Transformation 2.115 (Useless state removal): There exists a function useful 2 FA ?! FA that removes states that are not reachable. A de nition of this function is
not given here explicitly, as it is not needed. Function useful satis es

(8 M : M 2 FA : Useful(useful (M)) ^ LFA(useful(M)) = LFA(M))

and preserves "-free, Useful , Det, and Min.

2

Transformation 2.116 (Removing start state unreachable states): Transforma-

tisionnotusgeifvuelns

2 FA
here,

?!
as it

FA removes those states that are not start-reachable. is not needed. Function usefuls satis es

A

de

nition

(8 M : M 2 FA : Usefuls(usefuls(M)) ^ LFA(usefuls(M)) = LFA(M))

and preserves Complete, "-free, Useful, Det, and (trivially) Min and MinC.

2

Remark 2.117: A function
that are not nal-reachable.

Suuscehfual f

2 FA ?!
function is

FA not

could also needed in

be de ned, removing this dissertation.

states 2

Transformation 2.118 (Completing an FA): Function complete 2 FA ?! FA takes
an FA and makes it Complete. It satis es the requirement that:

(8 M : M 2 FA : Complete(complete(M)) ^ LFA(complete(M)) = LFA(M))

In general, this transformation adds a sink state to the FA. This transformation preserves

"-free, (trivially) Complete, Det, and MinC.

2

2.6. FINITE AUTOMATA AND MOORE MACHINES

29

Transformation 2.119 ("-transition removal): An "-transition removal transformation remove" 2 FA ?! FA is one that satis es
(8 M : M 2 FA : "-free(remove"(M)) ^ LFA(remove"(M)) = LFA(M))

There are several possible implementations of remove". The most useful one (for deriving nite automata construction algorithms in Chapter 6) is:

rem"(Q; V; T; E; S; F) = let

Q0 = fE (S)g f E (q) j Q V fqg \ T =6 g T0 = f (q; a; E (r)) j (9 p : p 2 q : (p; a; r) 2 T) g

F0 = f f j f 2 Q0 ^ f \ F 6= g

in

(Q0; V; T 0; ; fE (S)g; F 0)

end

In the above version of remove", each of the new states is one of the following: A new start state, which is the set of all states "-transition reachable from the old start states S.

A set of states, all of which are "-transition reachable from a state in Q which has a non-" in-transition.

Note that this transformation yields an automaton which has a single start state. 2

Given a nite automaton construction f 2 RE ?! FA, in some cases the dual of the
construction, R f R, can be even more e cient than f. For this reason, we will also be needing the dual of function rem".
Transformation 2.120 (Dual of function rem"): The dual of function rem" is de ned
as (R rem" R)(Q; V; T; E; S; F ) =
let Q0 = f(ER) (F)g f (ER) (q) j fqg V Q \ T =6 g T0 = f ((ER) (q); a; Q) j (9 p : p 2 Q : (q; a; p) 2 T) g S0 = f s j s 2 Q0 ^ s \ S 6= g
in (Q0; V; T 0; ; S0; f(ER) (F )g)
end
2

Transformation 2.121 (Subset construction): The function subset transforms an "-free FA into a DFA (in the let clause T0 2 P(Q) V ?! P(P(Q)))

subset(Q; V; T; ; S; F) = let T0(U; a) = f( q : q 2 U : T(q; a))g F0 = f U j U 2 P(Q) ^ U \ F 6= g
in (P(Q); V; T0; ; fSg; F0)
end

30 CHAPTER 2. MATHEMATICAL PRELIMINARIES

In addition to the property that (for all M 2 FA) LFA(subset(M)) = LFA(M), function
subset satis es
(8 M : M 2 FA ^ "-free(M) : Det(subset(M)) ^ Complete(subset(M)))
and preserves Complete, "-free, Det. Some authors call this the `powerset' construction. 2

In order to present a useful property of the subset construction, we need the following lemma.

Lemma 2.122 (Subset construction): Let

M0 = (Q0; V; T0; ; S0; F0)

and

M1 = (Q1; V; T1; ; S1; F1) = subset(M0)

be nite automata. For all w 2 V and q 2 Q1 w 2 L?M1(q) q = ( p : p 2 Q0 ^ w 2 L?M0(p) : fpg)
Proof:
We rewrite the left side of the above equivalence as follows:
w 2 L?M1(q) f M1 is a DFA g
q = T1 (S1; w)

We rewrite the right side of the equality in the right side of the above equivalence as follows:

( =

p:
f

p 2 Q0 ^
de nition

w of

2L?L?M0Mg0

(p)

:

fpg)

( p : p 2 Q0 ^ p 2 T0 (S0; w) : fpg)

= f set calculus g

T0 (S0; w)

We now need to prove q = T1 (S1; w) q = T0 (S0; w). We now proceed by proving that

(8 w : w 2 V : T1 (S1; w) = T0 (S0; w))

We prove this by induction on jwj.

Basis: For the case w = " we have:

2.6. FINITE AUTOMATA AND MOORE MACHINES

31

T1 (S1; ")
= f de nition of S1 g T1 (fS0g; ")
= f de nition of T1 2 Q1 V ?! Q1 g
S0
= f de nition of T0 ; no "-transitions g
T0 (S0; ")
Induction hypothesis: Assume that T1 (S1; w) = T0 (S0; w) holds for w : jwj = k. Induction step: Consider wa : jwj = k ^ a 2 V

T1 (S1; wa)
= f de nition of T1 2 Q1 V ?! Q1 g
T1(T1 (S1; w); a)
= f induction hypothesis g
T1(T0 (S0; w); a)
= f de nition of T1 using signature T1 2 Q1 ( q : q 2 T0 (S0; w) : T0(q; a))
= f de nition of T0 g
T0(T0 (S0; w); a)
= f de nition of T0 ; no "-transitions g
T0 (S0; wa)

V ?! Q1 g

2

Property
subset (M0)

2.123 (Subset construction): Let M0 = (Q0; V
be nite automata. By the subset construction, the

; T0; state

; S0; F0) and set of M1 is

PM(Q1 0=).

We have the following properties:

(8 p : p 2 P(Q0) : ?!L M1(p) = ( q : q 2 p : ?!L M0(q)))

and (from Lemma 2.122):

(8 p : p 2 P(Q0) : L?M1(p) = (\ q : q 2 p : L?M0(q)))

2

We can also de ne the subset construction for Moore machines.

32 CHAPTER 2. MATHEMATICAL PRELIMINARIES
Transformation 2.124 (Subset construction for MMs): Function subsetmm transforms a MM into a DMM (in the let clause T0 2 P(Q) V ?! P(P(Q)) and 0 2 P(Q) ?! P(W))
subsetmm(Q; V; W; T; ; S) = let T0(U; a) = f( q : q 2 U : T(q; a))g 0(U) = f (q) j q 2 U g
in (P(Q); V; P(W); T0; 0; fSg)
end
2
2.6.2.1 Imperative implementations of some transformations
In this section, we present some imperative implementations of a few of the nite automata transformations. These implementations will be used in Chapter 6 to present some nite automata construction algorithms from the literature.
All of the algorithms presented make use of a common `reachability algorithm' skeleton. The di erence lies in the way the transition relation is computed in each step. In each
of these algorithms, variables D and U (both with domain P(P(Q))) accumulate the set
of `done' and the set of `undone' (yet to be considered) states in the automaton under construction, respectively.
Algorithm 2.125 (Composition usefuls rem"): f (Q; V; T; E; S; F) 2 FA g S0; T 0 := fE (S)g; ;
D; U := ; S0;
do U 6= ! let u : u 2 U; D; U := D fug; U n fug; for p; a : p 2 u ^ a 2 V ^ T(p; a) 6= !
d := E (T(p; a));
if d 62 D ! U := U fdg ] d 2 D ! skip fi; T 0 := T 0 f(u; a; d)g rof od; F0 := f f j f 2 D ^ f \ F =6 g f LFA(D; V; T 0; ; S0; F 0) = LFA(Q; V; T; E; S; F ) g
2

2.6. FINITE AUTOMATA AND MOORE MACHINES

Algorithm 2.126 (Composition usefuls subset):

f (Q; V; T; ; S; F) 2 FA g

S0; T 0 := fSg; ;

D; U := ; S0;

do U 6= !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d := ( q : q 2 u : T(q; a));

if d 26 D ! U := U fdg

] d 2 D ! skip

fi;

T 0 := T 0 f(u; a; d)g

rof

od;

F0 := f f j f 2 D ^ f \ F =6 g

f f

(LDFA; V(D; T; V0;;

T0; ; S0; ; S0; F0)

F
2

0D) =FALgFA(Q;

V;

T

;

E;

S;

F

)

g

Algorithm 2.127 (Composition usefuls subset rem"):

f (Q; V; T; E; S; F) 2 FA g

S0; T 0 := fE (S)g; ;

D; U := ; S0;

do U =6 !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d := ( q : q 2 u : E (T(q; a)));

if d 26 D ! U := U fdg

] d 2 D ! skip

fi;

T 0 := T 0 f(u; a; d)g

rof

od;

F0 := f f j f 2 D ^ f \ F =6 g

f f

(LDFA; V(D; T; V0;;

T0; ; S0; ; S0; F0)

F
2

0D) =FALgFA(Q;

V;

T

;

E;

S;

F

)

g

This algorithm is the same as the one given in ASU86].

33 2 2

34 CHAPTER 2. MATHEMATICAL PRELIMINARIES

Part II The taxonomies
35

Chapter 3
Constructing taxonomies
In this chapter, we provide a brief introduction to the construction of taxonomies. The McGraw-Hill Dictionary of scienti c and technical terms provides the following (somewhat biology oriented) de nition of a taxonomy:
A study aimed at producing a hierarchical system of classi cation of organisms which best re ects the totality of similarities and di erences. Park89, p. 1892]. In a manner analogous to a biological taxonomy, we intend to classify algorithms according to their essential details. This classi cation, which is frequently presented in the form of a (directed acyclic) taxonomy graph, will allow us to compare algorithms and determine easily what they have in common and where they di er. In the following paragraphs, we detail the structure and construction of a taxonomy. Given a particular problem area (for example, keyword pattern matching), the algorithms will be derived from a common starting point. The starting point is usually a na ve algorithm whose correctness is shown easily. Each of the algorithms appears as a vertex in the taxonomy graph, and the rst algorithm is placed at the root of the taxonomy graph. The derivation proceeds by adding either problem or algorithm details. A problem detail is a correctness preserving restriction of the problem. Such a detail may enable us to make a change in the algorithm, usually to improve performance. The more speci c problem may permit some transformation which is not possible in the algorithm solving the general problem. An algorithm detail, on the other hand, is a correctness-preserving transformation of the algorithm itself. These algorithm details may be added to restrict nondeterminacy, or to make a change of representation; either of these changes to an algorithm, gives a new algorithm meeting the same speci cation. In the taxonomies presented in Chapters 4 and 6, the particular details are explicitly de ned and given mnemonic names. In the remaining taxonomy (Chapter 7), the details are only introduced implicitly. Both types of details are chosen so as to improve the performance of an algorithm, or to arrive at one of the well-known algorithms appearing in the literature. The addition of a detail to algorithm A (arriving at algorithm B) is represented by adding an edge from A to B (the vertices representing algorithms A and B, respectively) to the taxonomy graph. The edge is labeled with the name of the detail. The use of correctness preserving transformations, and the correctness of the algorithm at the root of the graph, means that
37

38 CHAPTER 3. CONSTRUCTING TAXONOMIES the correctness argument for any given algorithm is encoded in the root path leading to that particular algorithm.
It should be noted that, while the taxonomy is presented in a top-down manner, the taxonomy construction process proceeds initially in a bottom-up fashion. Each of the algorithms found in the literature is rewritten in a common notation and examined for any essential components or encoding tricks. The common encoding tricks, algorithm skeletons, or algorithm strategies can be made into details. The details making up the various algorithms can then be factored, so that some of them are presented together in the taxonomy graph | highlighting what some of the algorithms have in common.
A few notes on the taxonomy graph are in order. In some cases, it may have been possible to derive an algorithm through the application of some of the details in a di erent order1. The particular order chosen (for a given taxonomy) is very much a matter of taste. A number of di erent orders were tried, the resulting taxonomy graphs were compared, and the most elegant one was chosen. Frequently, the taxonomy graph is a tree. When the graph is not a tree, there may be two or more root paths leading to algorithm A. This means that there are at least two ways of deriving A from the na ve algorithm appearing at the root. It is also possible that not all of the algorithms solving a particular problem can be derived from a common starting point. In this case, we construct two or more separate taxonomy graphs, each with its own root.
This type of taxonomy development and program derivation has been used in the past. A notable one is Broy's sorting algorithm taxonomy Broy83]. In Broy's taxonomy, algorithm and problem details are also added, starting with a na ve solution; the taxonomy arrives at all of the well-known sorting algorithms. A similar taxonomy (which predates Broy's) is by Darlington Darl78]; this taxonomy also considers sorting algorithms. Our particular incarnation of the method of developing a taxonomy was developed in the dissertation of Jonkers Jonk82], where it was used to give a taxonomy of garbage collection algorithms. Jonkers' method was then applied successfully to attribute evaluation algorithms by Marcelis in Marc90]. A recent taxonomy (not using Jonkers' method) by Hume and Sunday HS91] gives variations on the Boyer-Moore pattern matching algorithms; the taxonomy concentrates on many of the practical issues, and provides data on the running time of the variations and their respective precomputations.
Two primary aims of the taxonomies are clarity and correctness of presentation. We abandon low levels of abstraction, such as indexing within strings. Instead, we adopt a more abstract (but equivalent) presentation. Because of this, all of the abstract algorithms derived in this dissertation will be presented in a slightly extended version of Dijkstra's guarded command language Dijk76]. The reasons for choosing the guarded command language are:
Correctness arguments are more easily presented in the guarded commands than in programming languages such as Pascal or C.
1Only some of the details may be rearranged, since the correctness of some details may depend upon the earlier application of some detail.

39
In order to present algorithms that closely represent their original imperative presentations (in journals or conference proceedings), we do not make use of other formalisms and paradigms, such as functional or relational programming. We will frequently present algorithms without full annotations (invariants, preconditions, and postconditions) since most of the algorithm skeletons are relatively simple, and the annotations do not add much to the taxonomic classi cation. Annotations will be used when they help to introduce a problem or an algorithm detail. This part is structured as follows: Chapter 4 presents a taxonomy of keyword pattern matching algorithms. The taxonomy concentrates on those algorithms that perform pattern matching of a nite set of keywords, and those algorithms that do not use precomputation of the input string. Chapter 5 gives a derivation of a new regular expression pattern matching algorithm. The existence (and derivation) of the algorithm answers an open question rst posed by A.V. Aho in 1980. The algorithm, which is a generalization of the Boyer-Moore keyword pattern matching algorithm, displays good performance in practice. Chapter 6 presents a taxonomy of algorithms which construct a nite automaton from a regular expression. All of the well-known algorithms (including some very recently developed ones) are included. Chapter 7 presents a taxonomy of deterministic nite automata minimization algorithms. All of the well-known algorithms, and a pair of new ones, are included.

40 CHAPTER 3. CONSTRUCTING TAXONOMIES

Chapter 4
Keyword pattern matching algorithms
This chapter presents a taxonomy of keyword pattern matching algorithms. We assume that the keyword set (the patterns) will remain relatively unchanged, whereas a number of di erent input strings (or perhaps a very long input string) may be used with the same pattern set. Because of this, we consider algorithms which may require some precomputation involving the keyword set1. The algorithms considered include the well-known Aho-Corasick, Knuth-Morris-Pratt, Commentz-Walter, and Boyer-Moore algorithms2. In addition, a number of variants (some of them not found in the literature) of these algorithms are presented.
The taxonomy is a much-revised version of one originally co-developed with Gerard Zwaan of the Eindhoven University of Technology. In the original version WZ92], Zwaan was the primary author of Part II of that paper (which gave derivations of the precomputation algorithms), while I was the primary author of Part I (the taxonomy proper). A version of Section 4.4 appeared in WZ95]. Gerard can be reached at .wsinswan@win.tue.nl
4.1 Introduction and related work
Keyword pattern matching is one of the most extensively explored problems in computing science. Loosely stated, the problem is to nd the set of all occurrences from a set of patterns in an input string.
This chapter presents a taxonomy of keyword pattern matching algorithms. The main results are summarized in the taxonomy graph presented at the end of this section, and in the conclusions presented in Section 4.6. A version of the taxonomy graph is presented in each section, highlighting the part of the taxonomy considered in that section.
We systematically present a number of variants of four well-known algorithms in a common framework. Two of the algorithms to be presented require that the set of patterns is a single keyword, while the other two require that the set of patterns is a nite set of
1An alternative is to require that the subject string remain unchanged, with various di erent pattern sets being used. In this case, precomputation involving the subject string is preferred.
2We restrict ourselves to these `classical' pattern matching algorithms and do not consider algorithms which are substantially di erent, such as those given in WM94].
41

42 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS
keywords. The algorithms are: The Knuth-Morris-Pratt (KMP) algorithm as presented in KMP77]. This algorithm matches a single keyword against the input string. Originally, the algorithm was devised to nd only the rst match in the input string. We will consider a version that nds all occurrences within the input string. The Boyer-Moore (BM) algorithm as presented in BM77]. This is also a single keyword matching algorithm. Several corrections and improvements to this algorithm have been published; good starting points for these are the bibliographic sections of Aho90, CR94, Step94]. The Aho-Corasick (AC) algorithm as presented in AC75]. This algorithm can match a nite set of keywords in the input string. The Commentz-Walter (CW) algorithm as presented in Com79a, Com79b]. This algorithm can also match a nite set of keywords in the input string. Few papers have been published on this algorithm, and its correctness, time complexity, and precomputation are ill-understood.
These four algorithms are also presented in the overview of Aho90]. The rst three algorithms are also covered quite extensively in a new book CR94].
The recent taxonomy of pattern matching algorithms presented by Hume and Sunday (in HS91]) gives variations on the Boyer-Moore algorithm; the taxonomy concentrates on many of the practical issues, and provides data on the running time of the variations, and their respective precomputation. In Chapter 9, we will consider a C++ class library (and many of the associated practical issues) implementing many of the algorithms presented in this taxonomy. In Chapter 13, we will consider the performance (in practice) of some of the algorithms implemented in the class library.
The taxonomy graph that we arrive at after deriving the algorithms is shown in Figure 4.1. Each vertex corresponds to an algorithm. If the vertex is labeled with a number, that number refers to an algorithm in this chapter. If it is labeled with a page number, that page number refers to the page where the algorithm is rst mentioned. Each edge corresponds to the addition of either a problem or algorithm detail and is labeled with the name of that detail (a list of detail names follows). Each of the algorithms will either be called by their algorithm number, by their name as found in the literature (for the well-known algorithms), or by the parenthesized sequence of all labels along the path from the root to the algorithm's vertex. For example, the algorithm known as the optimized Aho-Corasick algorithm can also be called (p+, e, ac, ac-opt) (it is also Algorithm 4.53 in this dissertation). All of the well known algorithms appear near the bottom of the graph. Due to its labeling, the graph can be used as an alternative table of contents to this chapter.
Four algorithm details (p+, s+, p?, and s?) are actually composed of two separate algorithm details. For example, detail (p+) is composed of details (p) and detail (+).

4.1. INTRODUCTION AND RELATED WORK

43

However the second detail must always follow either detail (p) or detail (s) and so we treat them as a single detail. The edges labeled mo and sl in Figure 4.1 represent generic algorithm details that still have to be instantiated. Possible instantiations are given by the two small trees at the bottom of Figure 4.1. The details and a short description of each of them are as follows:
p (Algorithm detail 4.4) Examine pre xes of a given string in any order.

p+ Examine pre xes of a given string in order of increasing length. p? As in (p+), but in order of decreasing length. s (Algorithm detail 4.6) Examine su xes of a given string in any order.

s+ Examine su xes of a given string in order of increasing length. s? As in (s+), but in order of decreasing length. rt (Algorithm detail 4.17) Usage of the reverse trie corresponding to the set
of keywords to check whether a string which is a su x of some keyword, preceded by a symbol is again a su x of some keyword.

ft (Algorithm detail 4.28) Usage of the forward trie corresponding to the set of keywords to check whether a string which is a pre x of some keyword, followed by a symbol is again a pre x of some keyword.

e (Problem detail 4.33) Matches are registered by their endpoints.

ac (Algorithm detail 4.42) Maintain a variable, which is the longest su x of the current pre x of the input string, which is still a pre x of a keyword.

ac-opt (Algorithm detail 4.52) A single `optimized' transition function is used to update the state variable in the Aho-Corasick algorithm.

ls (Algorithm detail 4.64) Use linear search to update the state variable in the Aho-Corasick algorithm.

ac-fail (Algorithm detail 4.71) Implement the linear search using the transition function of the extended forward trie and the failure function.

kmp-fail (Algorithm detail 4.75) Implement the linear search using the extended failure function.

44 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3

p
4.5
+
4.8

s

okw
?

se

+

4.10

rt

4.18

cw

nla
p.90

4.93
lla

cw-opt

p.91

? 4.36

ac

ac-opt

4.47
ls

4.53
ac-fail

| {z4.72}
AC

rla

bmcw

r-opt

p.92

near-opt

bm

p.99

p.93
norm

p.96
okw

p.95
bm

p.96
near-opt, norm mo

p
+?
4.23
ft

obm
4.145
indices

4.30 mo

kmp-fail
4.76
okw
indices
| {z4.8}4
KMP

4.154
sl
4.168
mi
| {z4.1}77
BM

sl

p.97

fwd rev om none sfc

fast slfc

Figure 4.1: A taxonomy of pattern matching algorithms. An explanation of the graph and its labels is given in the text of this section. Algorithm 4.53 corresponds to the optimized Aho-Corasick algorithm AC75, Section 6]. Algorithm 4.72 corresponds to the Aho-Corasick failure function algorithm AC75, Section 2, Algorithm 1]. Algorithm 4.84 corresponds to the Knuth-Morris-Pratt algorithm KMP77, Section 2, p. 326]. The algorithm of the vertex labeled p. 95 and with incoming edge labeled norm corresponds to the Commentz-Walter algorithm Com79a, Section II], Com79b, Sections II.1 and II.2]. The algorithm of the vertex labeled p. 96 and with incoming edge labeled bm corresponds to the Boyer-Moore algorithm BM77, Section 4]. Algorithm 4.177 corresponds to the Boyer-Moore algorithm as well BM77, Sections 4 and 5].

4.1. INTRODUCTION AND RELATED WORK

45

okw (Problem detail 4.77) The set of keywords contains one keyword.

indices (Algorithm detail 4.82) Represent substrings by indices into the complete strings, converting a string-based algorithm into an indexing-based algorithm.

cw (Algorithm detail 4.90) Consider any shift distance that does not lead to the missing of any matches. Such shift distances are called safe.

nla (Algorithm detail 4.103) The left and right lookahead symbols are not taken into account when computing a safe shift distance. The computation of a shift distance is done by using two precomputed shift functions applied to the current longest partial match.

lla (Algorithm detail 4.104) The left lookahead symbol is taken into account when computing a safe shift distance.

cw-opt (Algorithm detail 4.108) Compute a shift distance using a single precomputed shift function applied to the current longest partial match and the left lookahead symbol.

bmcw

(Algorithm detail 4.116) Compute a shift distance using a single precomputed shift function which is applied to the current longest partial match and the left lookahead symbol. The function yields shifts that are no greater than the function in detail (cw-opt).

near-opt (Algorithm detail 4.121) Compute a shift distance using a single precomputed shift function applied to the current longest partial match and the left lookahead symbol. The function is derived from the one in detail (bmcw), and it yields shifts which are no greater.

norm (Algorithm detail 4.127) Compute a shift distance as in (nla) but additionally use a third shift function applied to the lookahead symbol. The shift distance obtained is that of the normal Commentz-Walter algorithm.

bm (Algorithm detail 4.135) Compute a shift distance using one shift function applied to the lookahead symbol, and another shift function applied to the current longest partial match. The shift distance obtained is that of the Boyer-Moore algorithm.

rla (Algorithm detail 4.137) The right lookahead symbol is taken into account when computing a safe shift distance.

46
r-opt
obm mo

CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS
(Algorithm detail 4.141) Compute a shift distance using precomputed shift functions (applied to the current longest partial match and the left lookahead symbol) and a shift function applied to the right lookahead symbol. (Algorithm detail 4.144) Introduce a particular program skeleton as a starting point for the derivation of the di erent Boyer-Moore variants. (Algorithm detail 4.148) A match order is used to determine the order in which symbols of a potential match are compared against the keyword. This is only done for the one keyword case (okw). Particular instances of match orders are:
fwd (Algorithm detail 4.149) The forward match order is used to compare the (single) keyword against a potential match in a left to right direction.
rev (Algorithm detail 4.150) The reverse match order is used to compare the (single) keyword against a potential match in a right to left direction. This is the original Boyer-Moore match order.
om (Algorithm detail 4.151) The symbols of the (single) keyword are compared in order of ascending probability of occurrence in the input string. In this way, mismatches will generally be discovered as early as possible.

sl (Algorithm detail 4.167) Before an attempt at matching a candidate string and the keyword, a `skip loop' is used to skip portions of the input that cannot possibly lead to a match. Particular `skips' are: none (Algorithm detail 4.169) No `skip' loop is used. sfc (Algorithm detail 4.170 The `skip loop' compares the rst symbol of the match candidate and the keyword; as long as they do not match, the candidate string is shifted one symbol to the right. fast (Algorithm detail 4.171) As with (sfc), but the last symbol of the candidate and the keyword are compared and possibly a larger shift distance (than with with sfc) is used. slfc (Algorithm detail 4.172) As with (fast), but a low frequency symbol of the keyword is rst compared.

4.2. THE PROBLEM AND SOME NA VE SOLUTIONS

47

mi (Algorithm detail 4.176) The information gathered during an attempted match is used (along with the particular match order used during the attempted match) to determine a safe shift distance.

4.2 The problem and some na ve solutions

In this section, we start with a formal statement of the pattern matching problem that we consider. We present a number of simple algorithms of which the correctness is easily established. These simple algorithms are not particularly interesting or practical, but they provide convenient starting points for di erent branches of the taxonomy. The solid part of Figure 4.2 show the part of the taxonomy that we will consider in this section.
The problem is to nd all occurrences of any of a set of keywords in an input string.

De nition 4.1 (Keyword pattern matching problem): Given an alphabet V , an input string S 2 V , and a nite non-empty pattern set P V , establish

PM : O = ( l; v; r : lvr = S : flg (fvg \ P) frg)

We will sometimes refer to S as the subject string.

2

Example 4.2 (Pattern matching): Given input string S = hishershey (a `hershey' is a type of chocolate-bar available in North America) and keyword set P = fher; his; sheg over alphabet V = fe; h; i; r; s; yg, when PM holds we have
O = f("; his; hershey); (hi; she; rshey);(his; her;shey);(hisher; she; y)g

Notice that two matches are allowed to overlap, as in the case of leftmost she match and

the her match. This example will be used throughout this chapter.

2

Registering keyword matches (in the input string) as a set of strings is di cult and inefcient to implement in practice. A practical implementation would make use of indexing within the input string, and would encode matches using the indices, as is done in Chapter 9. In this chapter, we pursue this more abstract presentation (using strings and sets of strings) for clarity.
A trivial (but unrealistic) solution to the pattern matching problem is:
Algorithm 4.3 ():

O := ( l; v; r : lvr = S : flg (fvg \ P) frg) f PM g

2

48 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3

p
4.5
+
4.8

s

okw
?

se

+

4.10

rt

4.18

cw

nla
p.90

4.93
lla

cw-opt
p.91

? 4.36

ac

ac-opt

4.47
ls

4.53
ac-fail

| {z4.72}
AC

rla

bmcw

r-opt

p.92

near-opt

bm

p.99

p.93 p.96

norm

okw

p.95 p.96

bm near-opt, norm mo

p
+?
4.23
ft

obm
4.145
indices

4.30 mo

kmp-fail
4.76
okw

4.154
sl
4.168
mi

indices
| {z4.8}4
KMP

| {z4.1}77
BM

sl

p.97

fwd rev om none sfc

fast slfc

Figure 4.2: The algorithms considered in Section 4.2 are denoted by solid circles, connected by solid lines.

4.2. THE PROBLEM AND SOME NA VE SOLUTIONS

49

The sequence of details (between parentheses, after the algorithm number) describing this algorithm is the empty sequence. This algorithm appears at the root of the taxonomy graph.
There are two basic directions in which to proceed while developing na ve algorithms to solve this problem. Informally, a substring of S (such as v in the quanti cation of the above algorithm) can be considered a \su x of a pre x of S" or a \pre x of a su x of S". These two possibilities are considered separately below.
Formally, we can consider \su xes of pre xes of S" as follows:

( l; v; r : lvr = S : flg (fvg \ P) frg) = f introduce u : u = lv g
( l; v; r; u : ur = S ^ lv = u : flg (fvg \ P) frg) = f l; v only occur in the latter range conjunct, so restrict their scope g
( u; r : ur = S : ( l; v : lv = u : flg (fvg \ P) frg))

A simple nondeterministic algorithm is obtained by introducing the following algorithm detail:

Algorithm detail 4.4 (p): Examine pre xes of a given string in any order.

2

The resulting algorithm is:

Algorithm 4.5 (p):

O := ;
for u; r : ur = S ! O := O ( l; v : lv = u : flg (fvg \ P) frg)
roff PM g

2

Again starting from Algorithm 4.3() we can also consider \pre xes of su xes of S" as follows:

( l; v; r : lvr = S : flg (fvg \ P) frg) = f introduce w : w = vr g
( l; v; r; w : lw = S ^ vr = w : flg (fvg \ P) frg) = f v; r only occur in the latter range conjunct, so restrict their scope g
( l; w : lw = S : ( v; r : vr = w : flg (fvg \ P) frg))

As with Algorithm detail (p), we introduce the following detail.

Algorithm detail 4.6 (s): Examine su xes of a given string in any order.

2

50 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

s?p+
=
p?s+

s?p?
=
p?s?

s+p+
=
p+s+

=
p+s?

s+p?

Figure 4.3: The 3-cube of na ve pattern matching algorithms.

Using this algorithm detail yields the simple nondeterministic algorithm (s) which is anal-

ogous to Algorithm 4.5(p). Hence, it is not presented here.

The update of O (with another quanti cation) in the inner repetitions of algorithms

(p) and (s) can be computed with another nondeterministic repetition. In the case of (p), the inner repetition would consider su xes of u to give algorithm (p, s); similarly, in (s) the inner repetition would consider pre xes of u to give algorithm (s, p).

Each of (p, s) and (s, p) consists of two nested nondeterministic repetitions. In each case, the repetition can be made deterministic by considering pre xes (or su xes as the
case is) in increasing (called detail (+)) or decreasing (detail (?)) order of length. For each
of (p, s) and (s, p) this gives two binary choices. Along with the binary choice between (p,

s) and (s, p) this gives eight possible na ve algorithms, arranged in a 3-cube representing

the three binary choices; the cube is depicted in Figure 4.3 with vertices representing

the eight possible algorithms for the two nested repetitions. The edges marked `=' join

satlrgionrgitShmasndwhkiecyhwaorredssyemt mPeitsrimcailr;roforredex(awmitphler,etshpeecotrdtoerstinrinwghircehve(rps+a,l

s?) considers input of S and P) by the

order in which (s+, p?) considers S and P . Because of this symmetry, we present only four

algorithms in this section: (p+, s+), (p+, s?), (s?, p?), and (s?, p+). These algorithms

were chosen because their outer repetitions examine S in left to right order.

Forward reference 4.7: In Section 4.2.1, Algorithm 4.5(p) will be re ned further and

in Section 4.2.2, Algorithm (s) will be re ned. In Section 4.3, Algorithm (p+) will be

developed into the Aho-Corasick and Knuth-Morris-Pratt algorithms, while in Sections 4.4

and 4.5, Algorithm algorithms.

(p+,

s+)

will

be

developed

into

the

Commentz-Walter

and

Boyer-Moore 2

4.2. THE PROBLEM AND SOME NA VE SOLUTIONS

51

4.2.1 The (p+) algorithms
The (p) algorithm presented in the previous section can be made deterministic by considering pre xes of S in order of increasing length (Algorithm detail (p+)). The outer union quanti cation in the required value of O can be computed with a deterministic repetition:

Algorithm 4.8 (p+):

u; r := "; S; O := f"g (f"g \ P) fSg; do r =6 " !
u; r := u(r 1); r 1;
O := O ( l; v : lv = u : flg (fvg \ P) frg) odf PM g

2

Forward reference 4.9: This algorithm will be used in Section 4.3 as a starting point

for the Aho-Corasick and Knuth-Morris-Pratt algorithms.

2

The inner union quanti cation in the required value of O can be computed with a nondeterministic repetition. This algorithm is called (p+, s) but will not be given here.

4.2.1.1 The (p+, s+) algorithm and its improvement
Starting with algorithm (p+, s) we make its inner repetition deterministic by considering su xes of u in order of increasing length:
Algorithm 4.10 (p+, s+):

u; r := "; S; O := f"g (f"g \ P) fSg; do r =6 " !
u; r := u(r 1); r 1;
l; v := u; "; O := O fug (f"g \ P) do l 6= " !
l; v := l 1; (l 1)v;
O := O flg (fvg \ P) frg od odf PM g

frg;

2

Remark 4.11: This algorithm has O(jSj2) running time, assuming that intersection with

P is a O(1) operation.

2

52 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

We can make an improvement to the above algorithm by noting that v 2 P ) su (v) su (P), and therefore su (v) 6 su (P) ) v 62 P. Intuitively, when a string v is not in su (P), there is no string u such that uv 2 P. The property that we need can be stated
more precisely.

Property 4.12 (Su xes of P): Note that

(8 w; a : w 26 su (P) : aw 62 su (P)):

2

In other words, in the inner repetition when (l 1)v 26 su (P) we need not consider any longer su xes of u. This means that the inner repetition guard (l 6= ") can be strengthened
to

l 6= " cand (l 1)v 2 su (P):

The direct evaluation of (l 1)v 2 su (P) is expensive. Therefore, it is done using a
function (corresponding to P ) called a reverse trie Fred60], de ned as follows:

De nition 4.13 (Reverse trie corresponding to P ): The reverse trie corresponding

to P is de ned as function P;r 2 su (P) V ?! su (P) f?g de ned by

P;r(w; a)

=

(

aw
?

if aw 2 su if aw 26 su

(P ) (P )

2

Convention 4.14 (Reverse trie): Since we usually refer to the trie corresponding to P

we will write r instead of P;r.

2

Example 4.15 (Reverse trie): The reverse trie corresponding to our example keyword

set P = fher; his; sheg is show in Figure 4.4. The vertices in the directed graph represent

elements of su (P), while the edges represent the mapping of an element of su (P) and

an element of V to su (P). Note that cases where the reverse trie takes value ? are not

shown.

2

Remark 4.16: Since jsu (P)j is nite, function function, with ? meaning `unde ned'.

r

can

be

viewed

as

a

kind

of

transition 2

Algorithm detail 4.17 (rt): Given the reverse trie, the guard conjunct (l 1)v 2 su (P)

becomes r(v; l 1) =6 ?.

2

4.2. THE PROBLEM AND SOME NA VE SOLUTIONS

she s he

he

e

"

her h er

r

er

s

53

his h is

is

Figure 4.4: Example of a reverse trie.

Algorithm 4.18 (p+, s+, rt): u; r := "; S; O := f"g (f"g \ P) fSg; do r 6= " !
u; r := u(r 1); r 1;
l; v := u; "; O := O fug (f"g \ P) do l 6= " cand r(v; l 1) 6= ? !
l; v := l 1; (l 1)v;
O := O flg (fvg \ P) frg od odf PM g

frg;

2

Forward reference 4.19: Observe that u = lv ^ v 2 su (P) is an invariant of the inner

repetition, initially established by the assignment l; v := u; ". This invariant will be used

in Section 4.4 to arrive at the Commentz-Walter algorithms.

2

Remark 4.20: This algorithm has O(jSj (MAX p : p 2 P : jpj)) running time. The

precomputation of r is similar to the precomputation of the forward trie tion 4.26) which is discussed in WZ92, Part II, Section 6].

f

(see De

ni2

In practice, a reverse trie can be implemented as a table with jsu (P)j jV j entries, with elements of su (P) and V being encoded as integers and used to index the table. Such an
implementation is used in Chapter 9.

54 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.2.1.2 The (p+, s?) algorithm
In the previous section, we modi ed the inner repetition of algorithm (p+, s) to consider su xes of u in order of increasing length. In this section, we will make use of an inner repetition which considers them in order of decreasing length. This gives us the following algorithm:
Algorithm 4.21 (p+, s?):
u; r := "; S; O := f"g (f"g \ P) fSg; do r =6 " !
u; r := u(r 1); r 1; l; v := "; u;
do v 6= " ! O := O flg (fvg \ P) frg;
l; v := l(v 1); v 1
od; O := O fug (f"g \ P) frg odf PM g

2

Remark 4.22: This algorithm has O(jSj2) running time and it appears di cult to improve

its performance.

2

4.2.2 The (s?) algorithms
Algorithm (s) can be made deterministic by considering su xes of S in order of decreasing length. This results in the deterministic algorithm (s?) which will not be given here. Furthermore, the assignment to O in the repetition can be written as a nondeterministic repetition to give the algorithm (s?, p) which will not be given here.

4.2.2.1 The (s?, p+) algorithms
Starting with algorithm (s?, p) we make the inner repetition deterministic by considering pre xes of each su x of the input string in order of increasing length. The algorithm is:

Algorithm 4.23 (s?, p+):

l; w := "; S; O := ;
do w 6= " ! v; r := "; w; O := O flg (f"g \ P) do r =6 " !
v; r := v(r 1); r 1;
O := O flg (fvg \ P) frg

fwg;

4.2. THE PROBLEM AND SOME NA VE SOLUTIONS

55

od;
l; w := l(w 1); w 1
od; O := O fSg (f"g \ P) f PM g

f"g

2

Remark 4.24: This algorithm has O(jSj2) running time, like Algorithm 4.10(p+, s+). 2

In a manner similar to the introduction of the reverse trie (De nition 4.13 and Algorithm 4.18(p+, s+, rt)), we can strengthen the inner repetition guard. The following de nitions are re ections (under string reversal) of those presented starting on page 51.

Property 4.25 (Pre xes of P): Note that (8 u; a : u 26 pref(P) : ua 26 pref(P))

Given this property, we can strengthen the guard of the inner repetition to

r 6= " cand v(r 1) 2 pref(P)

This property is the re ection of Property 4.12.

2

E cient computation of the strengthened guard (r 6= " cand v(r 1) 2 pref(P)) can be
done by using the forward trie corresponding to P.

De nition 4.26 (Forward trie corresponding to P ): The forward trie function cor-

responding to P is f 2 pref(P) V ?! pref(P) f?g, de ned by

f (u;

a)

=

(

ua
?

if ua 2 pref(P) if ua 62 pref(P)

2

Example 4.27 (Forward trie): The forward trie corresponding to our example keyword

set P = fher; his; sheg is shown in Figure 4.5. In a manner analogous to the reverse trie

example, the vertices in the directed graph represent elements of pref(P), while the edges

represent the mapping of an element of pref(P) and an element of V to pref(P) (and

cases where the reverse trie takes value ? are not shown).

2

Algorithm detail 4.28 (ft): Given the forward trie, the guard conjunct v(r 1) 2

pref(P) now becomes f(v; r 1) 6= ?.

2

Remark 4.29: The forward trie detail (ft) is de ned and used symmetrically to the

reverse trie detail (rt) (see Algorithm detail 4.17).

2

56 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

"

hh

e he

r her

i

s hi s his

s

h sh

e she

Figure 4.5: Example of a forward trie.

Introducing algorithm detail (ft) yields

Algorithm 4.30 (s?, p+, ft):

l; w := "; S; O := ;
do w =6 " ! v; r := "; w; O := O flg (f"g \ P) do r =6 " cand f(v; r 1) 6= ? !
v; r := v(r 1); r 1;
O := O flg (fvg \ P) frg od;
l; w := l(w 1); w 1
od; O := O fSg (f"g \ P) f"g f PM g

fwg;

2

Remark 4.31: As in Forward reference 4.19, observe that w = vr ^ v 2 pref(P) is an
invariant of the inner repetition. It is initially established by the assignment v; r := "; w. 2

Remark 4.32: This algorithm has O(jSj (MAX p : p 2 P : jpj)) running time, like

Algorithm 4.18(p+, s+, rt).

2

4.3. THE AHO-CORASICK ALGORITHMS

57

4.2.2.2 The (s?, p?) algorithm Tpgirhveeenxinehsneeroref.rweItpsientriutonirondnienrogfotfaimldgeoecrriistehaOmsi(njS(gsj?l2e),n. pgt)hc.anThailssoyibeeldms aadlgeordietthemrm(isn?is,tipc?b)ywchoinchsidiserninogt

4.3 The Aho-Corasick algorithms
In this section, starting with the na ve Algorithm 4.8(p+) from Section 4.2, we derive the Aho-Corasick AC75] and Knuth-Morris-Pratt KMP77] algorithms and their variants. The Knuth-Morris-Pratt (KMP) algorithm is a well known single keyword matching algorithm operating in time linear in the length of the subject string. The article KMP77] gives an interesting account of the history of development of the algorithm. Aho and Corasick (AC) combined its essential idea with concepts from automata theory to obtain two multiple keyword matching algorithms, also operating in linear time.
The common aspect of all these algorithms is the construction of a kind of Moore3 machine (see De nition 2.79) during a preprocessing phase. Using this Moore machine, the subject string can be scanned in linear time. The variants of the AC and the KMP algorithms all make use of the same Moore machine, but each of them uses a di erent method to compute the next transition. In the failure function AC algorithm, computation of the transition function is implemented using the forward trie (corresponding to P), while in the KMP algorithm it is realized by indexing in the pattern. These di erences lead to tradeo s between the time to process the input string, the time to precompute the required functions, and the space to store the required functions. The optimized AC algorithm (Algorithm 4.53) can process each symbol of the input string in constant time,
but requires O(jpref(P)j jV j) time and space for precomputed functions. On the other hand, the failure function AC algorithm (Algorithm 4.72) can require O(jpref(P)j) time to process a given input symbol, but it only requires O(jpref(P)j) time and space for
precomputed functions. Although these algorithms are frequently presented in an automata-theoretic way, in
this dissertation the automata aspects of these algorithms will not be stressed. Instead, a more general (algorithmic) presentation will be used. Automata theoretic approaches to deriving these algorithms are presented in Sections 4.3.3 and 4.3.7. Both of these sections can be omitted without loss of continuity.
Figure 4.6 shows the part of the taxonomy that we will consider in this section. The algorithms to be presented are denoted by solid circles, connected by solid lines.
The triple format of set O used so far has been redundant. This redundancy can be removed by registering matches in S by their end-points only; that is, the rst component of the triple will be dropped. This modi cation is known as problem detail (e).
3The inventor of Moore machines, E.F. Moore, is not the co-inventor (with Boyer), J Strother Moore, of the Boyer-Moore pattern matching algorithms. E.F. Moore performed much of the original research into the minimization of DFAs | see Chapter 7.

58 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3

p
4.5
+
4.8

s

okw
?

se

+

4.10

rt

4.18

cw

nla
p.90

4.93
lla

cw-opt
p.91

? 4.36

ac

ac-opt

4.47
ls

4.53
ac-fail

| {z4.72}
AC

rla

bmcw

r-opt

p.92

near-opt

bm

p.99

p.93 p.96

norm

okw

p.95 p.96

bm near-opt, norm mo

p
+?
4.23
ft

obm
4.145
indices

4.30 mo

kmp-fail
4.76
okw

4.154
sl
4.168
mi

indices
| {z4.8}4
KMP

| {z4.1}77
BM

sl

p.97

fwd rev om none sfc

fast slfc

Figure 4.6: The algorithms considered in Section 4.3 are denoted by solid circles, connected by solid lines.

4.3. THE AHO-CORASICK ALGORITHMS

59

Problem detail 4.33 (e): Matches are registered by their end-points.

2

Dropping the rst component of the triples will allow us to make some e ciency improvements to the algorithms.
The desired value of O (with the rst component dropped) in postcondition PM can be rewritten as follows:

1( u; r : ur = S : ( l; v : lv = u : flg (fvg \ P) frg)) = f distributes over g
( u; r : ur = S : ( l; v : lv = u : 1(flg (fvg \ P) frg))) = f de nition of g
( u; r : ur = S : ( l; v : lv = u : (fvg \ P) frg)) = f distributes over g
( u; r : ur = S : ( l; v : lv = u : fvg \ P) frg) = f \ distributes over g
( u; r : ur = S : (( l; v : lv = u : fvg) \ P) frg) = f de nition of su g
( u; r : ur = S : (su (u) \ P) frg)

De nition 4.34 (Re ned postcondition): The derivation above yields a new postcon-
dition

PM e : Oe = ( u; r : ur = S : (su (u) \ P) frg)

2

Example 4.35 (End-point pattern matching): Assuming the input string and key-
word set from Example 4.2, when PM e holds we have
Oe = f(his; hershey); (she; rshey); (her;shey);(she; y)g

2

This postcondition is established by a modi ed version of Algorithm 4.8(p+) (we could have used Algorithm 4.5(p), however, we choose to use a deterministic algorithm):

Algorithm 4.36 (p+, e):

u; r := ";
do r =6 "

S;
!

Oe

:=

(f"g

\

P

)

fSg;

u; r := u(r 1); r 1;

odfOPeM:=eOge (su (u) \ P) frg

2 In the following sections, algorithm details unique to the AC and KMP algorithms will be introduced.

60 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3.1 Algorithm detail (ac)

In Algorithm 4.36, we see that new matches are registered whenever the condition su (u)\

P 6= holds, i.e. when one or more patterns occur as su xes of u, the part of the subject

string read thus far. The essential idea of both the AC and the KMP algorithms is the

use of an easily updateable state variable that gives information about (partial) matches

in su (u), and from which the set su (u) \ P can easily be computed.

In order to variable U and

facilitate attempt

ttohemuapindtaatienoifnvOaeriainntAUlgo=ritshum

(4u.3)6\(pP+.,

e) we When

introduce a new u is updated to

u(r 1), we require an update of U to maintain the invariant. We begin deriving this update

as follows:

su (u(r 1)) \ P = f Property 2.54, \ distributes over g
(su (u)(r 1) \ P) (f"g \ P) = f Property 2.56 g
((su (u) \ pref(P))(r 1) \ P) (f"g \ P)

From the above derivation, it seems di cult to derive an easily computed update for U.
The update of U could more easily be accomplished, given the set su (u)\pref(P) rather than the old value of U (which is su (u) \ P). The set su (u) \ pref(P) can be viewed as a generalization of the set su (u) \ P.
In order to obtain an algorithm that is more easily implemented in practice, we try to maintain invariant

U = su (u) \ pref(P)

which is initially established by assignment u; U := "; f"g since P =6 . Assuming U = su (u) \ pref(P) we derive the following update of U:

su (u(r 1)) \ pref(P) = f preceding derivation with pref(P) instead of P g
((su (u) \ pref(pref(P)))(r 1) \ pref(P)) (f"g \ pref(P)) = f Property 2.51 | idempotence of pref g
((su (u) \ pref(P))(r 1) \ pref(P)) (f"g \ pref(P)) = f U = su (u) \ pref(P), Property 2.55 | " 2 pref(P) g
(U(r 1) \ pref(P)) f"g

The new invariant relating U and u yields another interesting property:

Property 4.37 (Set U): From U = su (u) \ pref(P) and P pref(P) it follows that

U \ P = su (u) \ pref(P) \ P = su (u) \ P. We can use the expression U \ P in the

update of variable Oe.

2

4.3. THE AHO-CORASICK ALGORITHMS

61

This all leads to the following modi cation of Algorithm 4.36(p+, e):
Algorithm 4.38:

u; r := "; S;
do r =6 " !

U

:=

f"g;

Oe

:=

(f"g

\

P

)

U := (U(r 1) \ pref(P)) f"g;

u; r := u(r 1); r 1;

odfOPeM:=eOge (U \ P) frg

fSg;

2

It should be noted that variable u is now super uous. It will, however, be kept in all subsequent algorithms to help formulate invariants.
Returning to our introduction of variable U with invariant U = su (u) \ pref(P)
in Algorithm 4.38, we see no easy way to implement this algorithm in practice (given that U is a language) | it appears di cult to implement the update statement U :=
(U(r 1)\pref(P)) f"g. Therefore, we try to exploit the internal structure of U to obtain
an easier update statement. We proceed by using the following property.

Property 4.39 (Set su (u) \ pref(P)): For each u 2 V the set su (u) \ pref(P) is

nonempty, nite, and linearly ordered with

therefore has a maximal (partially) ordered with

erelesmpeecnttt(oMthAeXpres

w x

respect
: w 2 su

to the
(u) \

psruef(xPo)r:dwer)i.ng(Thes.seTt hiseaslseot

ordering, p, but that does not prove to be

particularly useful.) This maximal element also characterizes the set, since

su (u) \ pref(P) = su ((MAX s w : w 2 su (u) \ pref(P) : w)) \ pref(P)

2

Example 4.40 (Characterizing su (u) \ pref(P)): If we take u = hish (and therefore
r = ershey), we have
su (hish) \ pref(P) = f"; h; shg

with maximal (under s) element sh.

2

The implication of the above property is that we can encode any possible value of U by
an element of pref(P). Such an encoding is of great practical value since a state which is
a string is much more easily implemented than a state which is a language. Note that the
set of possible values that U can take is f su (w) \ pref(P) j w 2 V g. We now de ne
the encoding function.

62 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

De nition 4.41 (Encoding function enc): Bijective encoding function enc 2 f su (w) \ pref(P) j w 2 V g ?! pref(P)

is

enc(U) = (MAX s w : w 2 U : w)

with inverse enc?1(w) = su (w) \ pref(P). (Note that there are di erent uses of w in the

lines above.)

2

The fact that function enc is bijective means that f su (w) \ pref(P) j w 2 V g and pref(P) are isomorphic (and therefore have the same cardinality).
We replace variable U in the algorithm by variable q and maintain invariant q = enc(U), equivalently
q = (MAX s w : w 2 su (u) \ pref(P) : w) which is initially established by q := " (since enc(f"g) = "). The introduction of variable
q constitutes the essential idea of the Aho-Corasick family of algorithms. We call this algorithm detail (ac).
Algorithm detail 4.42 (ac): A variable q is introduced into Algorithm 4.36(p+, e) such
that
q = (MAX s w : w 2 su (u) \ pref(P) : w)
2

Property 4.43
enc that we can
su (q) \ pref(P

(Variable
rewrite the
) \ P = su

q): It follows

update of
(q) \ P.

Oe

from the in terms

update of of q, since

Oe U

using
\P =

U, and function
enc?1(q) \ P = 2

In order to make the update of program variable Oe more concise, we introduce the following auxiliary function:
De nition 4.44 (Function Output): Function Output 2 pref(P) ?! P(P) is de ned
by
Output(w) = su (w) \ P
2

Example 4.45 (Function Output):

w " h s he hi sh her his she

Output (w)

fherg fhisg fsheg

2

4.3. THE AHO-CORASICK ALGORITHMS

63

The update of Oe can be done by assignment Oe := Oe Output(q) frg.

Remark 4.46: The precomputation of function Output can be done in O(jpref(P)j) time;

an algorithm doing this is presented in WZ92, Part II, Section 6].

2

We now require an update of variable q, in order to maintain the invariant. The update of variable U was

U := (U(r 1) \ pref(P)) f"g

Given bijection enc and the invariant relating U and q, the update of q is:

q := enc((enc?1(q)(r 1) \ pref(P)) f"g)

We can manipulate the right side into a more readable form as follows:

enc((enc?1(q)(r 1) \ pref(P)) f"g) = f de nition of enc?1 g
enc(((su (q) \ pref(P))(r 1) \ pref(P)) f"g) = f Property 2.56 g
enc((su (q)(r 1) \ pref(P)) f"g) = f \ distributes over ; Property 2.55 | " 2 pref(P) g
enc((su (q)(r 1) f"g) \ pref(P)) = f Property 2.54 g
enc(su (q(r 1)) \ pref(P)) = f de nition of enc g
(MAX s w : w 2 su (q(r 1)) \ pref(P) : w)

We now have obtained algorithm

Algorithm 4.47 (p+, e, ac):

u; r := "; S;
do r =6 " !

q

:=

";

Oe

:=

Output (q)

fSg;

uq;:=r :=(MuA(rX1);srw1:; w 2 su (q(r 1)) \ pref(P) : w);

odfOPeM:=eOge Output(q) frg

2

Forward reference 4.48: Sections 4.3.2, 4.3.4, 4.3.5, and 4.3.6 are concerned with alter-
native ways of implementing assignment

q := (MAX s w : w 2 su (q(r 1)) \ pref(P) : w)

2

64 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS
4.3.2 Method ( )ac-opt
In this section, we aim for an implementation of the assignment given in Forward reference 4.48 by means of a simple statement of the form q := f(q; r 1), where f is a suitable transition function, de ned as follows:
De nition 4.49 (Function f): Function f 2 pref(P) V ?! pref(P) is de ned as f(q; a) = (MAX s w : w 2 su (qa) \ pref(P) : w)
2

Remark 4.50: Subscript
trie transition function f.

f in f is That is,

used
f

to
f

indicate that if we assume

thf actor?reisnpotnhdes

to the forward codomain of f

corresponds to the function not being de ned at that point. Compare Examples 4.27 and

4.51. 2

Example 4.51 (Function P = fher; his; sheg is shown

f): Function
in Figure 4.7.

f
In

corresponding to keeping with the

our example keyword above remark, we can

set see

that the graphical representation of the example forward trie (Example 4.27, Figure 4.5)

is contained (from a graph-theoretic point of view) in Figure 4.7.

2

Given function f, the assignment to q in Algorithm 4.47(p+, e, ac) can be written as q := f(q; r 1).

Algorithm detail 4.52 (ac-opt): Usage of function f to update variable q.

2

This leads to algorithm:

Algorithm 4.53 (p+, e, ac, ac-opt):

u; r := "; S;
do r 6= " !

q

:=

";

Oe

:=

Output (q)

q := f(q; r 1);

u; r := u(r 1); r 1;

odfOPeM:=eOge Output(q) frg

fSg;

2

Remark 4.54: Provided evaluating f(q; a) and Output(q)
stance, if f and Output are tabulated), Algorithm 4.53(p+, running time complexity.

are e,

O(1) operations
ac, ac-opt) has

(for in-
O(jSj) 2

4.3. THE AHO-CORASICK ALGORITHMS

65

" e; i; r; y

h r; y

e; i; y

hh e; i; r; y
s

h e
i

h
he s
h

e; i; r; y r her
s

s hi s his e; i; r; y

e; i; r; y

s

sh

hi

h sh

e she

s s

s

r; y e; i; y

r h

Figure 4.7: Example of function f. In Remark 4.50, we mention that f f. In this gure, the edges corresponding to f are thickened.

66 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

This is the Aho-Corasick optimized algorithm AC75, Section 6]. Historically, this algorithm was usually derived from a less e cient algorithm (known as the Aho-Corasick failure function algorithm); in the next section, we will derive the failure function algorithm. The algorithm given here is known as the `optimized' algorithm because it is able
to make transitions in constant time, while using O(jpref(P)j jV j) time and space to
precompute and store function f. This algorithm would be used instead of the failure function algorithm in cases where the input string is to be processed quickly at the cost
of precomputation time and space. The failure function algorithm requires O(jpref(P)j) time to process a symbol of the input string, while only using O(jpref(P)j) time and space
for precomputed functions. Precomputation of f is discussed in WZ92, Part II, Section 6]. It involves the so-called
failure function which is introduced in Section 4.3.4.
4.3.3 A Moore machine approach to the ac-opt algorithm
In this section, we use an automata-based approach to derive the Aho-Corasick optimized algorithm. This section may be omitted without loss of continuity.
We begin by examining the structure of Algorithm 4.38 (taken from page 61):

u; r := "; S;
do r =6 " !

U

:=

f"g;

Oe

:=

(f"g

\

P

)

U := (U(r 1) \ pref(P)) f"g;

u; r := u(r 1); r 1;

odfOPeM:=eOge (U \ P) frg

fSg;

This algorithm bears a resemblance to algorithms used to simulate deterministic Moore machines. Thanks to encoding function enc, we know that the set of values that U can
take is isomorphic to the set pref(P); the set of possible values for U is therefore nite (see
Property 4.39). In the simulated Moore machine, variable U corresponds to the current
state (while processing input string S), the expression U \ P corresponds to the output
function, and variable Oe can be viewed as an encoding of the output string of the Moore machine.

Remark 4.55: There are some algorithms in the literature (see, for example, GB-Y91])

that implement the state of the Moore machine by means of a bit vector in the case of

single keyword pattern matching. For a practical use of bit vectors to represent states, see

Chapter 10.

2

These Moore machine observations can be made more precise in the following de nition.

De nition 4.56 (Deterministic Moore
machine (corresponding to keyword set P)

machine M0): We de
M0 = (Q0; V; 0; 0; 0;

ne deterministic
fs0g) as

Moore

4.3. THE AHO-CORASICK ALGORITHMS

67

State set Q0 = f su (w) \ pref(P) j w 2 V g
Input alphabet V
Output alphabet 0 = P(P ) Transition function 0 2 Q0 V ?! Q0 de ned by

0(q; a) = (qa \ pref(P)) f"g

Output function 0 2 Q0 ?! 0 de ned by

0(q) = q \ P

Singleton start state set fs0g where s0 = "

Since M0 corresponds to P , simply drop the subscript P.

we

could

have

named

it

MP;0;

since

no

confusion

arises,

we 2

Forward reference 4.57: In Section 4.3.7, we show that Moore machine M0 can be

obtained in a di erent way (primarily using nite automata), while in Property 4.62 we

show that Moore machine M0 is minimal.

2

etoncMT, wh0 ee|sctaaannteesMnocMfodMmeooeorarecehemaMsail0cyhsiitnmaetpeMl.em0Tehanristeerdleasniunglutpsargainecstaic(seMe.tosTohorefe

strings). Given the bijection machine which is isomorphic encoding of M0 parallels the

introduction of variable q (along with functions Output and f) to replace variable U.

We can now give the isomorphic image of M0 under enc.

De nition 4.58 (Deterministic Moore machine M1): Moore machine M1 is the iso-
morphic image of M0 under enc. It is

M1 = (pref(P); V; P(P); f; Output; f"g)

where

State set pref(P) is the codomain of enc

The input and output alphabets are unchanged

f (see De nition 4.49) is obtained as

f(q; a) = enc( 0(enc?1(q); a)) = (MAX s w : w 2 su (qa) \ pref(P) : w)

Output (see De nition 4.44) is obtained as

Output(q) = 0(enc?1(q)) = su (q) \ P

68 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

Start state " is obtained as " = enc(f"g)

Since M0 depends upon P , so does M1.

2

The simulation of M1 yields Algorithm 4.53.

for

Interestingly, the Moore machine its language (it follows from the

M1 is in fact the isomorphism of

minimal deterministic Moore machine M0 and M1 that M0 is also minimal).

This will be shown in the following de nitions and properties.

Property 4.59 (Minimality of a Moore machine): A DMM (Q; V; ; ; ; fsg) is min-
imal if and only if all of its states are useful, and

(8 q0; q1 : q0 =6 q1 ^ q0 2 Q ^ q1 2 Q : (9 w : w 2 V : ( (q0; w)) =6 ( (q1; w)))) 2

Remark 4.60: This de nition can be viewed as a generalization of a property of min-
imality for deterministic nite automata (see Property 2.111) | replace ( (q; w)) by
(q; w) 2 F in the de nition where F is the set of nal states of the nite automaton. 2

Before presenting the derivation, we will require a property of function
of f from domain pref(P) V to pref(P) V ).

f

(the extension

Property 4.61 (Function f ): For q 2 pref(P) and z 2 V , we have

f (q; z) = (MAX s w : w 2 su (qz) \ pref(P) : w)

It follows that f (q; z) s qz.

2

Property Proof:

4.62

(Minimality

of

Moore

machine

M1):

Moore

machine

M1

is

minimal.

The following proof is by contradiction. Since all states of M1 are useful, assume that there

exist two states

q0; q1 : q0 2 pref(P) ^ q1 2 pref(P) ^ q0 =6 q1 ^ jq0j jq1j

such that

(8 w : w 2 V : Output( f (q0; w)) = Output( f (q1; w)))

(That is, q0w0 and

we assume that
q0w0 2 Output(

M1 is f (q0;

not minimal.) Choose w0 : q0w0 2 P . Then
w0)). In this case (from the assumptions)

f (q0; w0) =

q0w0 2 Output( f (q0; w0)) f assumption about q0 and q1 g
q0w0 2 Output( f (q1; w0)) f de nition of Output g

4.3. THE AHO-CORASICK ALGORITHMS

69

q0w0 2 su ( f (q1; w0)) \ P ) f de nition of su ; Property 4.61; transitivity of s g
q0w0 s f (q1; w0) s q1w0
) f transitivity of s g
q0w0 s q1w0
f property of s g
q0 s q1
) f jq0j jq1j g
q0 = q1 This is a contradiction. We conclude that Moore machines M0 and M1 are minimal.

2

4.3.4 Linear search

In this section, we return to our algorithmic derivation of the Aho-Corasick algorithms.

There are other ways of implementing the assignment in Forward reference 4.48 than

the one presented in Section 4.3.2. The presence of assignment hints that a linear search could be used.

the MAX
Rather than

susoinngthaesirnigghlet

side of the assignment

of the
MAX

form q := s quanti

f(q; r 1) (as was cation by means of

done in a linear

Section 4.3.2), we search of the form

can

try

to

compute

the

do :B(q; r 1) ! q := f(q) od

where f is a so-called failure function from states to states. The failure function will be used

to step through decreasing (under s) values of q (from the maximum) until the value of

the quanti cation is found. This may slow down the actual scanning of the subject string,

but with a suitable choice of f linearity can still be maintained.

The advantages of this approach (over the use of f in Section 4.3.2) lie in the lower

storage requirements and in the preprocessing phase. The storage requirements decrease

from O(jpref(P
precomputation

)j jV j)
can be

fpoerrfofrmtoedOi(njpOre(fjp(Pre)jf)(Pfo)rj)atfiamileu,raesfuonpcptoisoend.

Correspondingly, the
to O(jpref(P)j jV j)

time for the preprocessing of function f.

Both the AC and KMP failure function algorithms make use of such a failure function,

albeit in slightly di erent ways. Here, we give the common part of the derivations. The

di erences are dealt with in the two following sections.

In order to derive a speci cation for the linear search guard, and for the failure function,

we manipulate the right side of the assignment to q (from Forward reference 4.48) into a

suitable form.

We start with the rst two lines of the derivation on page 63 (that derivation was used

to obtain the right side of the update of q).

70 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

enc((enc?1(q)(r 1) \ pref(P)) f"g) = f de nition of enc?1 g
enc(((su (q) \ pref(P))(r 1) \ pref(P)) f"g) = f de nition of enc g
(MAX s w : w 2 ((su (q) \ pref(P))(r 1) \ pref(P)) f"g : w) = f for strings w; v : w 2 A fvg w 2 A _ w = v g
(MAX s w : w 2 (su (q) \ pref(P))(r 1) \ pref(P) _ w = " : w) = f domain split g
(MAX s w : w 2 (su (q) \ pref(P))(r 1) \ pref(P) : w) max s" = f change of bound variable: w = w0(r 1) g
(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0(r 1)) max s"
A linear search cannot be used to easily compute the above expression directly. In the next two sections, the expression will be further manipulated for the speci c linear searches.
Forward reference 4.63: Linear search will be used in Sections 4.3.5 and 4.3.6 to compute (MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0(r 1)) max s"
2 The use of linear search is expressed in the following program detail.
Algorithm detail 4.64 (ls): Using linear search to update the state variable q. 2

4.3.5 The Aho-Corasick failure function algorithm

In order to simplify quanti cation (as an

the linear search, intermediate step

we would like to in computing the

coonme pinutFeotrhwearfdolrloewfeirnegncMe 4A.6X3):s

(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0)

There is a potential problem with this approach: when the linear search computes " as the
value of this quanti cation, an if- statement is required to decide which of the following
two situations gave rise to the " (and therefore what the value of the quanti cation in Forward reference 4.63 is):

:(9
tion

w0 : w0 has an

2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0). The MAX empty range, and therefore value " (the unit of max s).

s

quanti

ca-

(r 1) 2 pref(P). The MAX
the value of the quanti cation

iss

quanti ".

cation

does

not

have

an

empty

range,

and

The linear search will make use of the following failure function.

4.3. THE AHO-CORASICK ALGORITHMS

71

De nition 4.65 (Failure function ff): Function ff 2 pref(P) ?! pref(P) is de ned
as

ff(q) = (MAX s w : w 2 su (q) n fqg \ pref(P) : w) Note that ff(") = " since " is the unit of max s.

2

Remark 4.66: The subscript f in the failure function ff is for forward. In Chapter 5, we

will use a reverse a precomputation compute ff.

faaligluorreithfumncftoior nfrfriswghiivcehn;wtilhl abteadlgeornitehdmancaoluolgdouesalsyi.lyInbethmatodcihaepdtet2ro,

Example 4.67 (Failure function): The failure function corresponding to our example
keyword set is:

w " h s he hi sh her his she ff(w) " " " " " h " s he

2

Using ff, the resulting linear search is:

q0 := q;

do q0 6= " ^ q0(r f (q0 = " ^ :(9

1) w0

62 pref(P : w0 2 su

)!
(q)

\q0p:=reff(fP(q)0):

od;
w0(r

1)

2

pref(P

)))

if

_
q0

q0 =

= "

^(M(rA1X) 62spwr0e:f(wP0

2 su )!q

(q) \
:= "

pref(P

)

^

w0(r

1)

2

pref(P

)

:

w0)

g

] q0 6= " _ (r 1) 2 pref(P) ! q := q0(r 1)

fi

f q = (MAX s w : w 2 su (q(r 1)) \ pref(P) : w) g

The second conjunct in the guard of the repetition can be evaluated cheaply using the

forward trie forward trie

f, f,

since q0(r 1) we can use it

26
to

pref(P )
evaluate

f (q0; both of

(r 1)) = ?. However, by extending
the repetition guard conjuncts4.

the

De nition 4.68 (Extended forward trie corresponding to P ): The extended for-

ward trie is function ef 2 pref(P) V ?! pref(P) f?g de ned by

ef (w;

a)

=

<8> :>

wa "
?

if wa 2 pref(P) if w = " ^ a 26 pref(P)
otherwise

2

4This is essentially an application of the sentinal technique often used with linear search.

72 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

"

hh

e he

r her

i

e; i; r; y

s

hi s his

s

h sh

e she

Figure 4.8: Example of function ef.

Note that f(q; a) = ef(q; a) except when q = " ^ a 62 pref(P) where f(q; a) = ? and
ef(q; a) = ".

Property 4.69 (Extended forward trie): Both conjuncts of the linear search guard
can be combined since

q0 =6 " ^ q0(r 1) 62 pref(P) ef(q0; r 1) = ?

As a side e ect of the introduction of ef, the if-
replaced by the single assignment statement q :=

statement after the ef(q0; r 1).

linear search can be 2

Example 4.70 (Extended forward trie): keyword set P = fher; his; sheg is shown in

Function ef corresponding to our example Figure 4.8. Visually, we can see that ef is

simply an extension of the forward trie in Example 4.27.

f, by comparing this

gure with Figure 4.5 given 2

Algorithm detail 4.71 (ac-fail): Introduction of the extended forward trie
failure function ff to implement the linear search updating state variable q.

ef

and

the 2

4.3. THE AHO-CORASICK ALGORITHMS

73

We can now eliminate variable q0 from the linear search, to obtain the following algorithm:

Algorithm 4.72 (p+, e, ac, ls, ac-fail):

u; r := "; S;
do r 6= " !

q

:=

";

Oe

:=

Output (q)

fSg;

do ef(q; r 1) = ? ! q := ff(q) od;

q := ef(q; r 1);

u; r := u(r 1); r 1;

odfOPeM:=eOge Output(q) frg

2

This algorithm is the Aho-Corasick failure function pattern matching algorithm AC75,

Section 2, Algorithm 1]. In Aho and Corasick's original paper, this algorithm is derived

rst; it is then used as a starting point to derive the optimized AC algorithm.

This algorithm still has O(jSj) running time complexity Aho90] but is less e cient

ftuhnacntiAonlgorfi,thrmequ4.i5ri3n(gp+O,(ejp, raecf,(Pac)j-)osppta)c.e.FuPnrcetcioonmpeuftcaatinonbeosfteoxretednmdeodrefoerwcairednttlyriethaenf

and failure function ff is discussed in WZ92, Part II, Section 6].

its

Since (in this domain to ff

algorithm) the failure function
2 pref(P) n f"g ?! pref(P).

ff is never applied to The function with the

", we can restricted

restrict domain

is slightly cheaper to precompute than the full function. In the next section, we will make

use of the full signature of ff.

4.3.6 The Knuth-Morris-Pratt algorithm
In this section, we would like to use a simpler linear search (compared to that used in the previous section) to compute the following expression (from Forward reference 4.63):
(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0(r 1)) max s"
We would like to rewrite the above quanti cation into
(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0)(r 1)
In this case, the quanti cation could be calculated by linear search, but (as in the previous section) we would still need to distinguish between two cases when the value of the
i?bqdueseia)nnntagtinfiaydcbtwahiteneioamenrmyaikspoetp"yteh(rrsiaaestneongrpeewaocgnaeelsVe7em0o)ef.tnotIthnatehobaeribdnuoeanrvrieyttoqoofuapamvenoraatiditxotcrhasiot.sin,o(nMwV;eatkehxiinstfger?neitdsqgutohifpreoeerduransatoiontmrwiemfi-lnlaeawxsltloaeswtlee(mmufrseoentnmott
in our rst linear search algorithm in the previous section.)

74 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

De nition 4.73 (Extension

mutative binary operator on V

is, |

?s is the unit
that is, (8 w

of max :w2V

s

. :

(MAX s w : w 2 : w) = ?s.

o(Wwfe?mfas?als=xsog,d?s)wes:)itnEh^ex(t?8(e?nswdstwom: wb=aex2?thVsse)t)oz.:ebrwBeoymaonfNaxasotstrsaiosnt?cigoisanct=oiv2n.ewc6aa)twne|denachttohaimova2net-

The property that ?s is the zero of string concatenation will be used later in the following
derivation. We can now rewrite the expression from Forward reference 4.63

(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0(r 1)) max s" = f De nition 4.73 | ?s is zero of concatenation and the unit of max s g
(MAX s w0 : w0 2 su (q) \ pref(P) ^ w0(r 1) 2 pref(P) : w0)(r 1) max s"

We are left with an expression containing cation is the same as the one given at the

a simpler beginning

Mof ASeXctiosnq4u.3a.n5t)i.

cation (the quanti It is therefore easier

to implement a linear search to compute the value of this quanti cation. The linear search

will traverse elements of (su (q) \ pref(P)) f?sg.

This straight-forward linear search yields the KMP algorithm. Before presenting the

linear search, we note that we will have a failure function with the same de nition as

was given in De
ff(") = (MAX
ff(") = ". (This

nition 4.65. The only change

vs ewrs:iown

2
of

su
the

(") n f"g : w) =
failure function

is: with the extension

f(fMisAsXomsetwim: ews

2:
called

of max w) = ?s

si,nwsteeahdavoef

the extended failure

function.)

To compute the desired MAX s quanti cation we can simply use the linear search:

q0 := q;
fqfdo:qq=0q==q0 0=6(((MrM?1AAs) XmcXaansxsdwwqs:0:"(wrw212)s62suup(r(qeq(f)r(\P1))p)!r\efpq(P0r:e=)f^(fPfw)(q(:r0)w1o))dg2; pref(P) : w) g

Remark 4.74: Strictly speaking, the conditional conjunction in the repetition guard

could also be written as a normal (unconditional) conjunction because ?s is the zero of

concatenation. As we shall see in Algorithm 4.84, conditional conjunction is necessary

when certain coding tricks are used.

2

Algorithm detail 4.75 (kmp-fail): The extended
implement the linear search for the update of q.

failure

function

ff

is

introduced

to 2

4.3. THE AHO-CORASICK ALGORITHMS

75

Eliminating variable q0 in the linear search leads to algorithm

Algorithm 4.76 (p+, e, ac, ls, kmp-fail):

u; r := "; S;
do r =6 " !

q

:=

";

Oe

:=

Output (q)

fSg;

dq :o=qq6=(r?1s)
u; r := u(r

cand q(r m1);arx1;s";

1)

26

pref(P

)

!

q

:=

ff

(q)

od;

odfOPeM:=eOge Output(q) frg

2 This algorithm does not appear in the literature. In some cases, the linear search in the algorithm above performs one more iteration than the one in Algorithm 4.72, meaning that it is slightly less e cient.

4.3.6.1 Adding indices

Historically, the KMP algorithm was designed using indexing within strings; this stems from e ciency concerns. Some of the most common uses of the KMP algorithm are in le-search programs and text editors, in which pointers to memory containing a string are a preferable method of accessing strings. In order to show the equivalence of this more abstract version of KMP, and the classically presented version we will now convert Algorithm 4.76 to make use of indexing within strings. To facilitate the use of indexing, we have to restrict the problem to the one keyword case, as stated in problem detail

Problem detail 4.77 (okw): P = fpg

2

Convention 4.78 (Shadow variables): Most shadow predicates and functions will be

`hatted' for easy identi cation. Variables i and j are so named (and not hatted) to conform

to the original publication of the algorithms.

2

We now introduce three shadow variables, and invariants that are maintained between the shadow variables and the existing program variables:
i : q = p1 : : : pi?1 where i = 1 q = " and i = 0 q = ?s. With this convention we
mirror the coding trick from the original KMP algorithm.

j : u = S1 : : : Sj?1 ^ r = Sj : : : SjSj. Also r 1 = Sj if 1 j jSj. Oce : Oe = ( x : x 2 Oce : f(p; Sx SjSj)g).
Naturally, we must de ne new predicates and a new failure function fbf on these shadow variables.

76 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

De nition 0; jpj] as

4.79

(Indexing extended failure function):

De

ne fbf

2

1; jpj + 1] ?!

fbf(i) = jff(p1 : : : pi?1)j + 1

and de ne j?sj = ?1.

2

Example 4.80 (Indexing extended failure function): For this single-keyword exam-
ple, we assume the keyword p = hehshe. In this case, our failure function ff is given as:

w " h he heh hehs hehsh hehshe
ff(w) ?s " " h " h he

The corresponding indexing failure function fbf is:

i 1234567 fbf(i) 0 1 1 2 1 2 3

2

The invariant relating u and q (q = (MAX
rewritten to relate j and i:

s w : w 2 su

(u) \ pref(P) : w)) can be

p1 : : : pi?1 = (MAX s w : w 2 su (S1 : : : Sj?1) \ pref(p) : w))

De nition 4.81 (New postcondition PdM e): Postcondition PMe can be rewritten in
terms of the shadow variables:

PdM e : Oce = ( j : 1 j jSj + 1 ^ p 2 su (S1 Sj?1) : fjg)

2

We can also note the following equivalences and correspondences:

Since q 2 pref(p) Similarly q 6= ?s

we have q(r 1) 62
0 < i and q = p

pref(p) i = jpj +

Sj 1.

6=

pi

when

i

jpj ^ j

jSj.

Assignment coding trick

q := q(r alluded

1) to

minaRxems "arcko4rr.7es4p. onds

to

i :=

i

+

1.

It

is

here

that

we

use

the

Assignment u; r := u(r 1); r 1 corresponds to j := j + 1.

The guard can be rewritten using the following equivalence r 6= " j jSj.

Assignment Oe := Oe Output(q) frg corresponds to

4.3. THE AHO-CORASICK ALGORITHMS

77

if
]

i i

=
6=

jpj jpj

+ +

1 1

! !

sOckei:p=

Oce

fjg

fi

The complete algorithm (written without the invariants relating shadow to non-shadow variables) is now:

u; r := "; S; q := "; Oe := Output(q) fSg;

i := 1; j := 1;

if i
]i
fi;

=
6=

jpj jpj

+ +

1 1

! !

OOccee

:= :=

fjg

do j jSj !

dqu;o:=r0:=q<(rui(1rc)am1n);darxS1j;s6="j;

pi ! q :=
i := i + 1; := j + 1;

ff

(q);

i

:=

fbf

(i)

od;

Oifei:== ] i =6

Oe
jpj jpj

Output (q)

+ +

1 1

! !

sOckei:p=

frg; Oce fjg

fi

odf PM e ^ PdM e g

We have introduced algorithm detail:

Algorithm detail 4.82 (indices): Represent substrings by indices into the complete

strings.

2

Remark 4.83: While we have introduced (indices) as an algorithm detail, it could also be

considered as a problem detail since it is being used to derive an algorithm which satis es

a postcondition given in terms of indices. We will continue to call it an algorithm detail,

since we will use it as a pure algorithm detail in Section 4.5.

2

Removing the non-shadow variables leaves us with the following algorithm:
Algorithm 4.84 (p+, e, ac, ls, kmp-fail, okw, indices):

i := 1; j := 1;

if
]

i i

=
=6

jpj jpj

+ +

1 1

! !

OOccee

:= :=

fjg

78 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

fi;

do j jSj !

do 0
i := i

< +

i cand
1;

Sj

=6

pi

!

i

:=

fbf

(i)

od;

j := j + 1;

if
]

i i

=
6=

jpj jpj

+ +

1 1

! !

sOckei:p=

Oce

fjg

fi

odf PdM e g

2 The above algorithm is the classic Knuth-Morris-Pratt algorithm KMP77, Section 2, Oprpe..(p3j3ep22tj6]i)t]t.isohpTnaathcaietns.hdaelPgtnhorueremictiohnbmmnerperhuoatfraseatpOipoep(tnjliSitcoijoaf)ntrfi)ouunninsscntnioinoefgvnfebtfrfibmf(gterche,aeaantnteodertaiatstlhilhanyanusbm2bebedeejrnSerjois.vfheoiSdtwteonrbrayabtgiyoceonPnsoevfroerffrbifnttihnrPeegq,eoruuirni9tree0ars, similar way to above, the precomputation of function ff (as discussed in WZ92, Part II, Section 6]) into using indices.

4.3.7 An alternative derivation of Moore machine M0
An interesting solution to the pattern matching problem involves using an automaton for the language V P. Usually, a nondeterministic nite automaton is constructed. The automaton is then simulated, processing input string S, and considering all paths through the automaton. Whenever a nal state is entered (after processing string u, a pre x of
S), a keyword match has been found (since u 2 V P, equivalently su (u) \ P =6 ,
by Property 2.59) and the match is registered; see for example AHU74, p. 327] for a description of this approach.
One particular ("-transition-free) transition function for the automaton is simply the forward trie for P, augmented with a transition from state " to itself on all symbols in
V (recall that pref(P) is the state set of the forward trie | see De nition 4.26). This automaton is de ned as (QN; V; N; ; fsNg; FN), where

State set QN = pref(P)

The input alphabet V

Transition function (trie-based) N 2 QN V ?! P(QN) is de ned (for q = ") by

N (q;

a)

=

(

f"; ag f"g

if a 2 pref(P)
otherwise

4.3. THE AHO-CORASICK ALGORITHMS
" hh e i

he

s e; h; i; r; s; y

hi

r her s his

79

s

h sh

e she

Figure 4.9: Example of function N. Notice the similarity with function ef.

and (for q 6= ")

N(q; a) = ( fqag

if qa 2 pref(P)
otherwise

and is extended to N 2 QN V ?! P(QN) in the usual way

Single start state sN = "

Final state set FN = P

It is useful to see the graphical representation of transition function N.

Efhxearm; hpisl;es4h.e8g5is(sFhuonwcntiionnFigNu)r:e

The 4.9.

function

N

corresponding

to

our

keyword

set

P

= 2

The simulation of this automaton can proceed as follows:

u; r := "; S;
Of ein:=var(iqaNnt\: do r 6= " !

qN FN qN

:= ) =

ffsrNgg;;
N("; u)

g

qN := ( q : q 2 qN : N(q; r 1));

u; r := u(r 1); u 1;

80 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS
odfOPeM:=eOge (qN \ FN) frg

Strictly speaking, the automaton is being used as a nondeterministic Moore machine. Each

path through the Moore machine is followed simultaneously; the output function is only

dTehNencoeaudntfpbouertwsofrumintetcetonifoatnhs ies sNtaN=te2sP(QthNfe??!Nnagl(s?NtaNtdeiess,onFueNtdp=uastPintnoobnempartecchisine)g.,

The output alphabet i.e. non- nal states).

N (q)

=

(

q
?N

if if

q q

2 62

FN FN

The nondeterministic Moore machine is now MN = algorithm, the set Oe is only updated when the output

(iQs Nno; tV;?NN.

;

N;

N; fsNg).

In the

The subset construction (with unreachable state removal) can be applied to the non-

deterministic Moore machine, to give a deterministic Moore machine MD. That is, MD =

m(uisneifsutlisc

subsetmm)(MN). In the following Moore machine M0 was de ned in

property, De nition

we will 4.56).

prove

that

MD

=

M0

(deter-

In the derivation that follows, we will use an interesting property of automaton MN.

Pq risopVerqt.yW4e.8w6ri(tTeraL?n(sqi)ti=onVfuqn.cTtihoins

N): For all
follows from

states q the fact

2 QN, the left language of
that the only cycles in the

transition graph are from start state sN to itself on every a 2 V .

2

Under the subset construction, the state set is P(QN) = P(pref(P)). The set of
reachable states is smaller, as will be shown below. A new output alphabet (under the
subset construction) is de ned as: D = P( N). The set of start-reachable states is

QD
= f Transformation 2.124 and Property 2.96 g f q j q 2 P(QN) ^ L?MD(q) =6 g
= f Property 2.123 | subset construction g f q j q 2 P(QN) ^ (\ p : p 2 q : L?MN(p)) =6 g
= f Property 2.107 | disjoint left languages in a DMM g f f p j p 2 QN ^ w 2 L?MN(p) g j w 2 V g
= f de nition: QN = pref(P) and Property 4.86 g f f p j p 2 pref(P) ^ w 2 V p g j w 2 V g
= f Property 2.59: w 2 V p p 2 su (w) g f f p j p 2 pref(P) ^ p 2 su (w) g j w 2 V g
= f de nition of \: p 2 pref(P) ^ p 2 su (w) p 2 pref(P) \ su (w) g

4.3. THE AHO-CORASICK ALGORITHMS

81

f f p j p 2 pref(P) \ su (w) g j w 2 V g = f set calculus g
f su (w) \ pref(P) j w 2 V g = f de nition of Q0 g
Q0
The deterministic output function D 2 QD ?! P( N) is
D(q)
= f Transformation 2.124 | subset construction g f N(p) j p 2 q ^ N(p) =6 ?N g
= f de nition of N g f p j p 2 q ^ p 2 FN g
= f de nition: FN = P g fpjp2q^p2P g
= f de nition of \ g fpjp2q\P g
= f set calculus g q\P
= f de nition 0 g
0(q)
Lastly, the deterministic transition function D 2 QD V ?! QD is
D(q; a)
= f Transformation 2.124 | subset construction g ( p : p 2 q : N(p; a))
= f de nition of N, " 2 q g ( p : p 2 q ^ pa 2 pref(P) : fpag) f"g
= f set calculus g (qa \ pref(P)) f"g
= f de nition of 0 g
0(q; a) From these derivations it follows that MD = M0.
Remark 4.87: Notice that the number of states of the Moore machine does not grow
during the subset construction. Perrin mentions the Aho-Corasick and Knuth-MorrisPratt Moore machines as examples of ones which do not su er from exponential blowup (i.e. the number of states grows exponentially) during the subset construction Perr90].

82 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

Indeed, we have shown a stronger result: the AC and KMP Moore machine (we use the

singular since they are isomorphic) does not su er from any increase in the number of

states under the subset construction (with start-unreachable states removed).

2

4.4 The Commentz-Walter algorithms
In this section, we discuss a number of algorithms that can be derived from the na ve Algorithm 4.18, viz. the Commentz-Walter (CW) algorithms Com79a, Com79b] and a multiple keyword version of the Boyer-Moore (BM) algorithm. The original single keyword version of the BM algorithm BM77], and those variants considered in HS91], will be discussed in Section 4.5. All of the algorithms derived in this section are also derived (with precomputation algorithms) in WZ95]. In that paper, some of the algorithm details are given di erent names, and part of the algorithm graph has a slightly di erent structure.
We will be using Algorithm 4.18(p+, s+, rt) as the starting point for algorithms in this section. For easy cross-referencing, we duplicate that algorithm here:
Algorithm 4.88 (p+, s+, rt):
u; r := "; S; O := f"g (f"g \ P) fSg; do r =6 " !
u; r := u(r 1); r 1;
l; v := u; "; O := O fug (f"g \ P) frg; do l 6= " cand r(v; l 1) 6= ? !
l; v := l 1; (l 1)v;
O := O flg (fvg \ P) frg od odf PM g

2 This algorithm traverses the subject string from left to right but does matching from right to left. As soon as a match fails, the starting point for matching is `shifted' to the right by the assignment u; r := u(r 1); r 1 and the matching starts again. The essential algorithm detail added in this section is that of shifts of the form u; r := u(r k); r k for k possibly greater than 1 (provided no matches are missed, of course). `Safe' shift distances can be determined from the symbols inspected during a match and some precomputed tables speci c to the patterns. This idea was introduced in the original BM single keyword algorithm BM77], which turns out to be extremely e cient in practice and has become very popular. The algorithm was extended to multiple keywords by Commentz-Walter Com79a, Com79b], much in the same way as the Aho-Corasick algorithms relate to the Knuth-Morris-Pratt algorithm. In Chapter 5, we will extend the CW algorithm to handle patterns that are arbitrary regular expressions. That extension answers an open question rst posed by A.V. Aho in 1980 Aho80, p. 342].

4.4. THE COMMENTZ-WALTER ALGORITHMS

83

It turns out that the CW algorithm is little-used in practice, if at all, due to a somewhat inaccessible description and the problem of correctly carrying out the rather intricate precomputations. The algorithm deserves better: extensive benchmarking, reported in Chapter 13, shows that the CW algorithm signi cantly outperforms the better-known AC algorithms in many cases.
The algorithms presented in this section all use the same algorithm skeleton for scanning the subject string, but di er in the shift distances used and the way these are computed. We now present the derivation of the common part; speci c shift distances are treated in Sections 4.4.2{4.4.8. The solid lines and solid circles of Figure 4.10 indicate the part of the taxonomy which we will be considering in this section.
In the next section, we will outline a general method of computing a `safe' shift distance.

4.4.1 Safe shift distances and predicate weakening
We begin by characterizing the `ideal' shift distance that can be used in the assignment u; r := u(r k); r k. Ideally, we would like to shift to the next keyword match to the right
(of the current position), a distance of (MIN n : 1 n jrj ^ su (u(r n)) \ P =6 : n).
(Note that this quanti cation can have an empty range if there is no `next match', and
therefore take value +1. For this reason, we extend the take and drop operators such that r (+1) = r and r (+1) = ".) This ideal shift distance can be explained intuitively
as the smallest shift distance n 1 such that a su x of u(r n) is a keyword and therefore a match will be found while scanning u(r n) from right to left.
Any smaller shift is also appropriate, and we de ne a safe shift distance as follows.

De nition 4.89 (Safe shift distance): A shift distance k satisfying

1 k (MIN n : 1 n jrj ^ su (u(r n)) \ P =6 : n)

is called a safe shift distance.

2

The use of a safe shift distance is embodied in the following algorithm detail.

Algorithm detail 4.90 (cw): A safe shift distance, k, is used in the assignment

u; r := u(r k); r k

of Algorithm 4.18(p+, s+, rt).

2

In WZ95], this algorithm detail has been renamed ssd (for `safe shift distance'). Computing the upperbound on k (the maximal safe shift distance) is essentially the
same as the problem we are trying to solve, and we aim at easier to compute approximations (from below) of the upperbound. Thanks to the following property, we can weaken the range predicate of the ideal shift to obtain an approximation.

84 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3

p
4.5
+
4.8

s

okw
?

se

+

4.10

rt

4.18

cw

nla
p.90

4.93
lla

cw-opt
p.91

? 4.36

ac

ac-opt

4.47
ls

4.53
ac-fail

| {z4.72}
AC

rla

bmcw

r-opt

p.92

near-opt

bm

p.99

p.93 p.96

norm

okw

p.95 p.96

bm near-opt, norm mo

p
+?
4.23
ft

obm
4.145
indices

4.30 mo

kmp-fail
4.76
okw

4.154
sl
4.168
mi

indices
| {z4.8}4
KMP

| {z4.1}77
BM

sl

p.97

fwd rev om none sfc

fast slfc

Figure 4.10: The algorithms considered in Section 4.4 are denoted by solid circles, connected by solid lines.

4.4. THE COMMENTZ-WALTER ALGORITHMS

85

Property 4.91 (Weakening of range predicates): For predicates J and J0 such that J ) J0, we have

(MIN i : J(i) : i) (MIN i : J0(i) : i)

2

Approximations will be obtained by weakening the predicate su (u(r n)) \ P 6= in the range of the upperbound. Since the ideal predicate (su (u(r n)) \ P =6 ) implies its
weakenings, the quanti cation with the weakening in the range will not be greater than the ideal shift (i.e. it will approximate the ideal shift distance from below, and will be a safe shift). The weakest predicate true is one such weakening; using it yields a shift distance of 1.
By considering di erent weakenings, several variants of the CW algorithm (amongst which, the BM algorithm) are obtained. The choice of which weakening to use is frequently a tradeo between larger shifts (resulting in a more e cient algorithm) and the greater cost of precomputation and storage of the resulting shift tables.
The idea of range predicate weakening turns out to be very useful, and it will also be used in Section 4.5 (to derive the Boyer-Moore family of algorithms) and in Chapter 5 (to derive a generalization of the Commentz-Walter algorithm).
In order to compute a safe shift distance, some additional information will be used. An interesting side-e ect of introducing the reverse trie (creating Algorithm 4.18) is that
the predicate u = lv ^ v 2 su (P) becomes an invariant of the inner repetition (see
Forward reference 4.19). Adding l; v := "; " to the initial assignments in Algorithm 4.18
turns u = lv ^ v 2 su (P) into an invariant of the outer repetition too. This additional
information (the invariant) will be used in nding weakenings of the ideal shift predicate. Most of the weakenings that we will derive depend only upon l and v; in Section 4.4.8, we will consider a shift that depends upon l, v, and r.

Notation 4.92 (Shift distance k): Due to this dependence on l, v and perhaps r, we

can view k as a function and write k(l; v; r) instead of k. In cases where the shift does not

depend upon r, we simply write k(l; v).

2

This yields the following algorithm scheme for all variants of the CW algorithm from which variants are obtained by substituting a particular function for k(l; v; r).

Algorithm 4.93 (p+, s+, rt, cw):

u; r := "; S;
l; v := "; "; O := f"g (f"g \ P) fSg; f invariant: u = lv ^ v 2 su (P) g do r 6= " !
u; r := u(r k(l; v; r)); r k(l; v; r);
l; v := u; "; O := O fug (f"g \ P) frg; f invariant: u = lv ^ v 2 su (P) g

86 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

do l =6 " cand r(v; l 1) 6= ? !
l; v := l 1; (l 1)v;
O := O flg (fvg \ P) frg od odf PM g

2

Previous presentations of the Commentz-Walter algorithms Com79a, Com79b, WZ92] all present phase shifted versions of this algorithm scheme. In those papers, the phase shifted version is chosen to simplify some of the de nitions, at the expense of complicating the algorithm. The algorithm above is considerably simpler. Other algorithm skeletons are
possible, as in FS93] where a single repetition containing an if- construct is used. We start the weakening of predicate su (u(r n)) \ P 6= with some general steps
(before proceeding to more speci c weakenings) under the assumption that u = lv ^ v 2 su (P) ^ 1 n jrj:

su (u(r n)) \ P =6 fu = lv g
su (lv(r n)) \ P =6 ) f l = (l 1)(l 1), l 1 2 V , monotonicity of su and \ g
su (V (l 1)v(r n)) \ P =6 f 1 n, r n = ((r n) 1)((r n) 1) = (r 1)((r n) 1) g
su (V (l 1)v(r 1)((r n) 1)) \ P =6 ) f n jrj, ((r n) 1) 2 V n?1, monotonicity of su and \ g
su (V (l 1)v(r 1)V n?1) \ P =6 f Property 2.59 g
V (l 1)v(r 1)V n?1 \ V P =6

The only reference to r in the last predicate is r 1. Since r 6= " (by the outer repetition guard in Algorithm 4.93) we have r 1 2 V . We no longer need the upper bound n jrj on n, and it can be dropped from the range of the MIN quanti cation for the shift distance.
String5 l 1 is known as the left lookahead symbol, v is known as the recognized su x
(since v 2 su (P ), by the invariant), and r 1 is known as the right lookahead symbol.

Remark 4.94: In the above derivation, we discarded all but a single symbol of l and r

(except in the case l = ", where we discard all of l). We could have kept more of either string

in our weakening (yielding a stronger predicate, and therefore a greater shift distance);

unfortunately, this would have given a funtion that is more di cult to precompute, and

a shift tables that require more space for storage. For example, we could have kept two

symbols of l and two of r, yielding a minimum storage requirement of O(jV j4 jsu (P)j).

The jsu (P)j term comes from the fact that v 2 su (P).

2

5It is a string since it is possible that l = ".

4.4. THE COMMENTZ-WALTER ALGORITHMS

87

Forward reference 4.95: In the following section, we will consider a further weakening
of the last predicate in the preceding derivation:
V (l 1)v(r 1)V n?1 \ V P 6= 2
In the next section, we consider some general weakening strategies which will be used later to derive particular weakenings. By discussing the strategies rst, we are able to develop an informal notation in which we can express the steps in obtaining a weakening, and can be used to develop new weakenings.

4.4.1.1 General weakening strategies
We present a number of weakening strategies, assigning each of them a name (which resembles an algorithm detail). By naming the strategies, we will be able to give a concise description of the steps involved in deriving the weakening. In this dissertation, we will only consider a few of the possible weakenings. The notation introduced in this section is intended to help other weakening-developers convey the general steps involved in deriving their weakenings. It is possible that two di erent sequences of strategies yield equivalent predicates.
Each of the following strategies is given simply as an implication (or an equivalence),
or as a relationship between two MIN quanti cations. In some cases, a reference to a
relevant property is given. In the following descriptions, we assume that A; B; C V
(they are languages), u; v 2 V , a 2 V , and J and J0 are predicates: discard Discard a conjunct: J ^ J0 ) J.

duplicate Duplicate a conjunct: J J ^ J.

split From Property 2.60:
V A \ V B =6 V A \ B =6 _ V B \ A 6=
and
V aA \ V B 6= V aA \ B 6= _ V B \ A 6=

decouple AuB \ C 6= ) AV jujB \ C =6

absorb V CA \ B 6= ) V A \ B 6=

q-split (MIN i : J(i) _ J0(i) : i) = (MIN i : J(i) : i) min(MIN i : J0(i) : i)
(see Property 2.21).

q-decouple (MIN i : J(i) ^ J0(i) : i) (MIN i : J(i) : i) max(MIN i : J0(i) : i)
(see Property 2.21).

88 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

enlarge For 1 k, (MIN i : k i ^ J(i) : i) (MIN i : 1 i ^ J(i) : i)

Some of these strategies are used directly in weakening the predicate, while some are
used after a weakening has been inserted into a MIN quanti cation. In some of the strate-
gies, it is necessary to specify which conjunct, word, or quanti cation is being manipulated. An example of this is given below.
Beginning with Forward reference 4.95, we make our rst weakening step | a decouple step. The step can also be given as a hint in the derivation:
V (l 1)v(r 1)V n?1 \ V P 6= ) f decouple r 1 g
V (l 1)vV V n?1 \ V P 6= f V V n?1 = V n g
V (l 1)vV n \ V P =6
Note that the last step involved straightforward manipulation instead of the application of a strategy. Such simple steps are not mentioned in the list of strategies applied.
The last predicate is totally free of r. In Section 4.4.8 we will consider an algorithm which makes use of the right lookahead symbol r 1.
Forward reference 4.96: In Sections 4.4.2{4.4.6 we will consider further weakenings of
the predicate (derived above):
V (l 1)vV n \ V P 6= 2
In the following section, we consider the l = " case (and the no-lookahead case) sepa-
rately. This will allow us to assume l 6= " in Sections 4.4.2{4.4.8.

4.4.1.2 The l = " and the no-lookahead cases

In the l = " case, the predicate in Forward reference 4.96 is equivalent to V vV n\V P 6= .
We apply a split step, yielding

V vV n \ P 6= _ V P \ vV n 6=

We can use this predicate, and one more step, to arrive at a practical shift distance. Starting with the ideal shift distance:

(MIN n : 1 n jrj ^ su (u(r n)) \ P 6= : n) f weakening steps above g
(MIN n : 1 n ^ (V vV n \ P =6 _ V P \ vV n 6= ) : n) = f q-split g
(MIN n : 1 n ^ V vV n \ P 6= : n) min(MIN n : 1 n ^ V P \ vV n =6

: n)

4.4. THE COMMENTZ-WALTER ALGORITHMS

89

In order to make this shift distance more concise, we de ne two auxiliary functions.

De nition 4.97 (Functions d1 and d2): Functions d1; d2 2 su (P) ?! N are de ned
by

dd21((xx))

= =

(MIN n : 1 (MIN n : 1

n ^ V xV n \ P 6= n ^ V P \ xV n =6

: n) : n)

2

Functions d1 and d2 were named by Commentz-Walter Com79a, Com79b].

Rtnbheoeavumtenrdiatherdcakasana4bn.ot9ave8keme:bpvyNta(yloMutreeaIn+Ntgh1eap,t.:atnpOhd2entqPthhueear:enjofptotjih)reemcradat2hixoannn1e,dvi,aennrtdhdtae1skoqceautsnhavenahtsliauhveicfeta+tadi1noisnt.eaimInnncpdetthesyeeadrrd,aeenfnugennevic,tetirmioognneraeodnaf2itndeig2rs

than (MIN p : p 2 P : jpj) max1. In Chapter 13, we show that this upperbound can

have a signi cant e ect on the practical performance of the Commentz-Walter algorithm

variants.

2

Forward reference 4.99: Functions d1 and d2 will also be used in Chapter 5 to derive

a generalization of the Commentz-Walter algorithm. In that chapter, precomputation

algorithms for d1 and d2 are also presented.

2

Example 4.100 (Functions d1 and d2): For keyword set fher; his; sheg we compute d1
and d2:
w " e r s er he is her his she
d1(w) 1 1 +1 2 +1 1 +1 +1 +1 +1
d2(w) 3 3 3 2 3 1 2 3 2 1 2

sUescitniognfsu,nwcetioansssumd1ealn6=d

d2, ".

our

l

=

"

shift

distance

is

d1(v) mind2(v).

In

the

following

4.4.1.2.1 The no-lookahead shift function We can discard all references to the left lookahead symbol, by using the l = " shift distance for the l =6 " case too. This weakening
step is referred to as discarding the lookahead symbol. The corresponding shift function is given as follows.
De nition 4.101 (Shift function knla): Shift function knla is de ned as: knla(l; v) = d1(v) mind2(v)
2

90 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

Remark 4.102: This shift function yields the smallest shift distances of all shift functions

to be considered in this section.

2

Since the various k shift functions are usually expressed in terms of more elementary functions, they are not usually tabulated (the k functions are computed on-the- y). The basic functions, however, are usually tabulated.

Algorithm detail
detail (nla).

4.103

(nla):

Calculating

the

shift

distance

using

knla

is

algorithm 2

The use of shift function knla results in algorithm (p+, s+, rt, cw, nla), which does

not appear in the literature. This algorithm is of no great practical interest, since the

precomputation is barely cheaper than any of the other variants (for example, shift function

kcw | see Section 4.4.5), and variants. The algorithm does,

the resulting shift however, use only

distances are
O(jsu (P)j)

less than in storage for

any of the other functions d1 and

d2.

4.4.2 A shift function without further weakening

For our rst shift function, we do not weaken the predicate in Forward reference 4.96 any further | we simply apply split, and q-split. This weakening, and most of the following ones, will include the left lookahead symbol (one of the weakenings will not). The use of the lookahead symbol is given in the following algorithm detail

Algorithm detail 4.104 (lla): The left lookahead symbol (l 1) is used in determining

the shift distance.

2

Applying split to the predicate in Forward reference 4.96 yields:
V (l 1)vV n \ P =6 _ V P \ vV n =6 We manipulate the resulting MIN quanti cation as follows:
(MIN n : 1 n ^ (V (l 1)vV n \ P =6 _ V P \ vV n 6= ) : n) = f q-split g
(MIN n : 1 n ^ V (l 1)vV n \ P 6= : n) min(MIN n : 1 n ^ V P \ vV n =6 : n) The second term of the in x min is simply d2(v). To give the shift function concisely, we
de ne another auxiliary function.
De nition 4.105 (Function dopt): Functions dopt 2 V su (P) ?! N is de ned by dopt(a; x) = (MIN n : 1 n ^ V axV n \ P 6= : n)
2

4.4. THE COMMENTZ-WALTER ALGORITHMS

91

Remark 4.106: Note that the quanti cation in the de nition of this function can have

an empty range, and therefore dopt can take value +1.

2

Due to the size of the resulting tables, we do not present an example of dopt here. Given this function, we can de ne the shift distance:

De

nition
kopt (l;

4.107
(
v) =

(Shift function kopt):

dd1op(tv()lm1;ivn)dm2(ivn)d2(v)

l l

Shift
=6 "
="

function

kopt

is de

ned as

2

The use of the particular shift derived above is given in the following algorithm detail.

Algorithm detail 4.108 (cw-opt):
algorithm detail (cw-opt).

Calculating

the

shift

distance

using

function

kopt

is 2

The resulting algorithm is (p+, s+, rt, cw, lla, cw-opt). From the informal description in their article, it appears that Fan and Su present a version of this algorithm FS93]. The algorithm was derived independently in WZ95], where the precomputation of the three auxiliary functions can also be found. This algorithm promises to be particularly e cient, although it is not one of the ones benchmarked in Chapter 13. The disadvantage
to the use of shift function kopt is that it requires storage O(jV j jsu (P)j), whereas some
of the other shift functions presented in this chapter require less storage.

4.4.3 Towards the CW and BM algorithms

In this section, we weaken the range predicate from function dopt further, decoupling the lookahead symbol and the recognized su x. We perform a type of decouple step, using following function:
De nition 4.109 (Function MS): Function MS 2 su (P) ?! P(V ) is de ned as MS(v) = f a j av 2 su (P) g

2

Example 4.110 (Function MS): Computing MS for keyword set fher; his; sheg yields:

w " e r s er he is her his she
MS(w) fe; r; sg fhg feg fig fhg fsg fhg

2

We will need a conjunct of the postcondition of the inner repetition of Algorithm 4.93:
(l 1)v 26 su (P); this follows from the negation of the inner repetition guard. Recall that we are considering the l =6 " case, so we may assume the termination condition of the inner repetition. It follows that (l 1) 62 MS(v), equivalently (l 1) 2 V n MS(v). We begin with
the range predicate of function dopt:

92 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

V (l 1)vV n \ P 6= f duplicate g
V (l 1)vV n \ P 6= ^ V (l 1)vV n \ P =6 f decouple v in rst conjunct g
V (l 1)V jvj+n \ P 6= ^ V (l 1)vV n \ P 6= ) f de nition of MS; (l 1)v 62 su (P) g
V (l 1)V jvj+n \ P =6 ^ V (V n MS(v))vV n \ P =6

In the following sections, we will further manipulate the shift predicate in the last line above. We can directly use the above predicate, by de ning the following shift function.
De nition 4.111 (Function dbmcw): De ne dbmcw 2 V su (P) ?! N by dbmcw(a; x) = (MIN n : 1 n ^ V aV jxj+n \ P 6= ^ V (V n MS(x))xV n \ P 6= : n)
2

Example 4.112 (Function dbmcw):
of this function is not given here.

Due

to

the

space

required

for

the

table,

an

example 2

Remark 4.113: Note that the quanti cation in the de an empty range, meaning that it can take value +1.

nition

of

function

dbmcw

can

have 2

The resulting shift distance is given as follows.

De

nition 4.114
kbmcw (l; v) =

((Sddhb1im(fvtcw)fm(ulni1nc;tvdi)2o(mnv)iknbmd2cw(v)):

Shift function kbmcw
l =6 "
l="

is de

ned as

2

Remark 4.115: The shift
kopt. That is, kbmcw kopt.

distance

given

by

kbmcw

is

never

greater

than

that

given

by 2

Using this shift distance is given in the following algorithm detail.

Algorithm detail 4.116
algorithm detail (bmcw).

(bmcw):

Calculating

the

shift

distance

using

function

kbmcw

is 2

The resulting algorithm (p+, s+, rt, cw, lla, cw-opt, bmcw) does not appear in the literature. The algorithm includes the (cw-opt) detail, since shift distance is derived from the one given in detail (cw-opt). It is given in WZ95], where the precomputation of the auxiliary function is discussed. Shift function kbmcw requires the same amount of storage as kopt. Function kbmcw is interesting because it combines the best of the Boyer-Moore and the normal Commentz-Walter algorithms (both to be presented later). An algorithm given by Baeza-Yates and Regnier (in B-YR90]) appears to be related to this one. Although it is not yet clear how their shift distance is obtained, it appears that it yields smaller shifts than those given with Algorithm detail (bmcw).

4.4. THE COMMENTZ-WALTER ALGORITHMS

93

4.4.4 A more easily precomputed shift function
In this section, we weaken the range predicate derived in the previous section, applying an absorb step. Starting from the predicate in the previous section, we derive:

V (l 1)V jvj+n \ P =6 ^ V (V n MS(v))vV n \ P =6 ) f absorb in second conjunct g
V (l 1)V jvj+n \ P 6= ^ V vV n \ P 6=

We can now de ne another auxiliary function, using this predicate in its range.

De nition 4.117 (Function dnopt): Function dnopt 2 V su (P) ?! N is de ned by dnopt(a; x) = (MIN n : 1 n ^ V aV jxj+n \ P =6 ^ V xV n \ P 6= : n)

2

Remark 4.118:
empty range, and

As the

with function function can

dopt take

, the quanti
value +1.

cation

in

function

dnopt

can

have

an 2

Due to the size of the resulting tables, we do not present an example of function dnopt here. Given dnopt, we can de ne the shift distance:

De nition 4.119 (Shift function knopt): Shift function knopt is de ned as

knopt(l; v) = ( dd1n(opvt)(ml 1i;nvd)2m(vi)n d2(v)

l =6 "
l="

2

Remark 4.120: Note that (due to the weakenings of the range predicate) the shift distance

given by knopt is never greater than that given by kbmcw . That is, knopt kbmcw .

2

The use of the particular shift derived above is given in the following algorithm detail.

Algorithm
is algorithm

detail 4.121 (near-opt):
detail (near-opt).

Calculating

the

shift

distance

using

function

kno2pt

The resulting algorithm (p+, s+, rt, cw, lla, cw-opt, bmcw, near-opt) does not appear in the literature. Note that the sequence of details includes the sequence of details from the previous section, since knopt derived from kbmcw . The storage requirements for this shift function are the same as the requirements for kopt. The advantage of using knopt is that the precomputation is cheaper.

94 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.4.5 The standard Commentz-Walter algorithm

We can further weaken the range predicate used in the previous section in the de nition of function dnopt. We apply q-decouple, followed by enlarge. We start our derivation with dnopt.

dnopt(l 1; v)
= f de nition of dnopt g (MIN n : 1 n ^ V (l 1)V jvj+n \ P =6 ^ V vV n \ P =6 : n) f q-decouple g (MIN n : 1 n ^ V (l 1)V jvj+n \ P 6= : n) max(MIN n : 1 n ^ V vV n \ P =6 : n)
= f de nition of function d1 g (MIN n : 1 n ^ V (l 1)V jvj+n \ P =6 : n) max d1(v)
= f changing bound variable: n0 = jvj + n g (MIN n0 : 1 + jvj n0 ^ V (l 1)V n0 \ P 6= : n0 ? jvj) maxd1(v) f enlarge g (MIN n0 : 1 n0 ^ V (l 1)V n0 \ P =6 : n0 ? jvj) maxd1(v)

We use the following auxiliary function to simplify the MIN quanti cation in the last
expression.

De nition 4.122 (Function charcw): De ne charcw 2 N V ?! N by

charcw(i; a) = (MIN n : 1 n ^ V aV n \ P =6 : n ? i)

Note that we could have given a more speci c signature, since the
is always in the range 0; (MAX p : p 2 P : jpj)].

rst argument to charc2w

Example 4.123 (Function charcw): Due to the space required for the tables, an example

of charcw is not using Example

given 4.130

here. The interested and Property 4.131.

reader

can

easily

construct

an

example

of

char

c2w

Remark 4.124: Note that it is possible for the quanti
to have an empty range, and therefore function charcw

cation in the can take value

de nition
+1.

of

char

c2w

We can now give the standard Commentz-Walter shift function.

De nition 4.125 (Shift function kcw): Shift function kcw is de ned as

kcw(l; v)

=

(

(char d1(v)

mcw (injvdj;2l(v1))

max

d1(v))

min d2(v)

l =6 "
l="

2

4.4. THE COMMENTZ-WALTER ALGORITHMS

95

Remark 4.126: Note that (due to the weakenings of the range predicate) the shift distance

given by kcw is never greater than that given by knopt. That is, kcw knopt.

2

The particular shift derived above is given in the following algorithm detail.

Algorithm detail 4.127
algorithm detail (norm).

(norm):

Calculating

the

shift

distance

using

function

kcw

is 2

The resulting algorithm (p+, s+, rt, cw, lla, cw-opt, bmcw, near-opt, norm)

is the normal Commentz-Walter algorithm (cf. Com79a, Section II] and Com79b, Sec-

tions II.1 and II.2]). The storage requirements for this shift function are O(jsu (P)j) for

functions d1 and d2, and O((MAX p :
function charcw makes it obvious that it

p2P
can be

: jpj)
stored

ijnVOj)(jfVorj)cshpaarccew

. (The de nition of with a small penalty

for computing the function.) As a result, kcw can be stored more economically than kopt,

knopt, or knopt. The precomputation required for kcw is also cheaper, the tradeo being a

smaller shift distance.

4.4.6 A derivation of the Boyer-Moore algorithm

In this section, we derive the multiple keyword Boyer-Moore algorithm starting with the weakening in Section 4.4.3. We begin our derivation with dbmcw:

dbmcw (l 1; v)
= f de nition of dbmcw g (MIN n : 1 n ^ V (l 1)V jvj+n \ P 6= ^ V (V n MS (v))vV n \ P 6= f q-decouple g (MIN n : 1 n ^ V (l 1)V jvj+n \ P =6 : n) max(MIN n : 1 n ^ V (V n MS(v))vV n \ P =6 : n)

: n)

We continue our derivation with the the rst operand of the in x max:

(MIN n : 1 n ^ V (l 1)V jvj+n \ P =6 : n) = f changing bound variable: n0 = n + v g
(MIN n0 : 1 + jvj n0 ^ V (l 1)V n0 \ P =6 : n0 ? jvj) f enlarge g
(MIN n0 : 1 n0 ^ V (l 1)V n0 \ P 6= : n0 ? jvj) f V (l 1)V n \ P 6= ) V (l 1)V n \ V P =6 g
(MIN n0 : 1 n0 ^ V (l 1)V n0 \ V P 6= : n0 ? jvj) = f non-empty range predicate g
((MIN n0 : 1 n0 ^ V (l 1)V n0 \ V P =6 : n0) ? jvj)

To present the resulting shift distance concisely, we de ne the following auxiliary functions.

96 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

De nition 4.128 (Function charbm): Function charbm 2 V ?! N is de ned as charbm(a) = (MIN n : 1 n ^ V aV n \ V P =6 : n)

2

De nition 4.129 (Function dbm): De ne dbm 2 su (P) ?! N by dbm(x) = (MIN n : 1 n ^ V (V n MS(x))xV n \ P 6= : n)

2

In WZ95], function dbm is called dvi.

Example 4.130 (Function charbm): Recall that we take our alphabet to be fe; h; i; r; s; yg.

a ehirsy charbm(a) 1 1 1 3 2 3

2

Functions charcw and charbm are related by the following interesting property.
Property 4.131 (Functions charcw and charbm): Note that: charbm(a) = charcw(0; a) min(MIN p : p 2 P : jpj) min1
From this property, and the above example, it is possible to construct an example of charcw2.

Example 4.132 (Function dbm): Computing dbm for keyword set fher; his; sheg yields:

w " e r s er he is her his she
dbm(w) 1 +1 +1 +1 +1 +1 +1 +1 +1 +1

2

De

nition
kbm (l;

4.133(
v) =

(BM shift function kbm): The d((1c(hva)rmbmin(ld12)(v?) jvj) maxdbm(v))

BM
min

shift function
d2(v) l =6 "
l="

kbm

is de

ned as:

2

Remark 4.134: The shift distance given by kbm is never greater than that given by kbmcw .

That is, kbm functions knopt

kbmcw . or kcw.

The

shift

distance

given

by

kbm

is

not

comparable

to

that

given

by 2

Algorithm
rithm detail

detail
(bm).

4.135

(bm):

Calculating

the

shift

distance

using

function

kbm

is

algo2

The resulting algorithm is the Boyer-Moore algorithm (p+, s+, rt, cw, lla, cw-opt, bmcw, bm). Adding problem detail (okw) (restricting P to one keyword) yields the wellknown BM algorithm, (p+, s+, rt, cw, lla, cw-opt, bmcw, bm, okw), which appears in the literature as BM77, Section 4]. Precomputation of functions dbm and charbm is discussed in WZ95].

4.4. THE COMMENTZ-WALTER ALGORITHMS

97

4.4.7 A weakened Boyer-Moore algorithm
Finally, we derive a weaker variant of the BM algorithm. This variant incorporates the weakenings involved in both the normal Commentz-Walter and the Boyer-Moore algorithms. The resulting shift function is given as follows:

De nition 4.136 (Weak BM shift function kwbm): The BM shift function kwbm is

de ned as:

kwbm(l; v) = ( d((1c(hva)rmbmin(ld12)(v?) jvj) max d1(v)) mind2(v)

l 6= "
l="

2

We do not introduce a new detail, since this algorithm combines details near-opt, norm, and bm.
The resulting algorithm is a weak Boyer-Moore algorithm (p+, s+, rt, cw, lla, cwopt, bmcw, near-opt, norm, bm). (Since there are two root-paths to this algorithm, the last three algorithm details could also have been ordered as bm, near-opt, norm.) This shift function yields a shift distance which is no greater than that given by the Commentz-Walter and Boyer-Moore shift functions.

4.4.8 Using the right lookahead symbol

In this section, we consider a shift function which uses the right lookahead symbol. Since we use the symbol, we introduce the following algorithm detail.

Algorithm detail 4.137 (rla): The right lookahead symbol (r 1) is used in determining

the shift distance.

2

We shall begin with the predicate given in Forward reference 4.95,

V (l 1)v(r 1)V n?1 \ V P 6=

We will apply the following steps:

1. duplicate.

2. decouple r 1 and (l 1)v.

3. absorb.

4. q-decouple.

5. Implicit split and q-split.

This is shown in the following derivation:

98 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

V (l 1)v(r 1)V n?1 \ V P 6= f duplicate g
V (l 1)v(r 1)V n?1 \ V P 6= ^ V (l 1)v(r 1)V n?1 \ V P =6 ) f decouple r 1 ( rst conjunct) and (l 1)v (second conjunct) g
V (l 1)vV V n?1 \ V P 6= ^ V V V jvj(r 1)V n?1 \ V P =6 ) f absorb g
V (l 1)vV n \ V P =6 ^ V (r 1)V n?1 \ V P =6 We can now use the last predicate in a MIN quanti cation:
(MIN n : 1 n ^ V (l 1)vV n \ V P 6= ^ V (r 1)V n?1 \ V P =6 : n) f q-decouple g
(MIN n : 1 n ^ V (l 1)vV n \ V P 6= : n) max(MIN n : 1 n ^ V (r 1)V n?1 \ V P =6 : n) = f Section 4.4.2 | de nitions of dopt; d2; implicit split, q-split; l =6 " g (dopt(l 1; v) mind2(v)) max(MIN n : 1 n ^ V (r 1)V n?1 \ V P 6= : n) = f changing of bound variable: n0 = n ? 1 g (dopt(l 1; v) mind2(v)) max(MIN n0 : 0 n0 ^ V (r 1)V n0 \ V P 6= : n0 + 1) = f non-empty range predicate g (dopt(l 1; v) mind2(v)) max(MIN n0 : 0 n0 ^ V (r 1)V n0 \ V P =6 : n0) + 1
We can now de ne an auxiliary function.
De nition 4.138 (Function charrla): Function charrla 2 V ?! N is de ned by charrla(a) = (MIN n : 0 n ^ V aV n \ V P =6 : n) + 1

2

Example 4.139 (Function charrla):
a ehirsy charrla(a) 1 2 2 1 1 4

2

This function can be used in the following shift distance:

De nition 4.140 (Shift function kropt): The optimized shift function with right looka-

head

is: kropt (l;

v;

r)

=

(

d(d1o(pvt)(ml 1i;nvd)2m(vi)nd2(v))

max char rla (r

1)

l 6= "
l="

2

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

99

Algorithm detail 4.141 (r-opt): Calculating
referred to as algorithm detail (r-opt).

the

shift

distance

using

function

kropt

is 2

The resulting algorithm is (p+, s+, rt, cw, lla, rla, r-opt). This algorithm does

not appear in the literature. It is not di a shift distance at least as large as kopt.

cult to see that Function charrla

furenqcutiiorneskOrop(tjVwj)illstaolwraagyes.

yield The

precomputation of charrla is similar to that for charbm (see WZ95]).

4.5 The Boyer-Moore family of algorithms

Since the appearance of the original Boyer-Moore algorithm BM77], many variations and improvements have been published. Most of these have been classi ed and discussed by Hume and Sunday in HS91], which also contains extensive benchmarking results. The material we present in this section supplements their work, as we provide derivations and correctness arguments for most of the algorithms in their paper. Figure 4.11 gives the part of the taxonomy graph which corresponds to this section.
The Boyer-Moore algorithm derivation in the previous section only accounted for one method of traversing the string variable u, in increasing order of v. In practice, when
P = fpg (P is a singleton set) other methods of comparing v to keyword p can be used. We therefore introduce problem detail (okw) (P = fpg, originally given on page 75).
Starting with the original problem speci cation, we derive the Boyer-Moore algorithm and its variants. The derivation presented here has a number of similarities with the one given in the previous section, in particular, the technique of predicate weakening (introduced in Section 4.4.1) will again be used to derive shift distances. Di erent weakening strategies (which will not be introduced explicitly as they were in Section 4.4.1.1) can be used in this section, thanks to problem detail (okw).
To make the following presentation more readable, we de ne a `perfect match' (as opposed to a failed, or partial, match) predicate and an auxiliary function.

De nition 4.142 (Perfect match predicate PerfMatch): We de ne a `perfect match'
predicate

PerfMatch((l; v; r)) (lvr = S ^ v = p)

Notice that p is an implicit parameter of PerfMatch.

2

We can rewrite the pattern matching postcondition in terms of predicate PerfMatch as:

O = ( l; v; r : PerfMatch((l; v; r)) : f(l; v; r)g)

To make the following presentation more readable, we introduce an auxiliary function.

De nition 4.143 (Shift function shift): De ne right shift function shift 2 (V )3 N ?!
(V )3 by

shift(l; v; r; k) = (l(vr k); (v(r k)) k; r k)

2

100 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

4.3

p
4.5
+
4.8

s

okw
?

se

+
4.10
rt
4.18
cw
4.93
nla lla
p.90
cw-opt
p.91

? 4.36

ac

ac-opt

4.47
ls

4.53
ac-fail

| {z4.72}
AC

rla

bmcw

r-opt

p.92

near-opt

bm

p.99

p.93 p.96

norm

okw

p.95 p.96

bm near-opt, norm mo

p
+?
4.23
ft

obm
4.145
indices

4.30 mo

kmp-fail
4.76
okw

4.154
sl
4.168
mi

indices
| {z4.8}4
KMP

| {z4.1}77
BM

sl

p.97

fwd rev om none sfc

fast slfc

Figure 4.11: The algorithms considered in Section 4.5 are denoted by solid circles, connected by solid lines. Additional algorithm details considered in this section are denoted by the two smaller graphs at the bottom of the gure.

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

101

We can now introduce the `ordinary Boyer-Moore' algorithm detail.

Algorithm detail 4.144 (obm): Use function shift and maintain invariant I1(l; v; r)

O

=

(
^

l0; v0; r0 : PerfMatch((l0; v0; r0)) ^ (l0v0 (lvr = S) ^ (jvj jpj) ^ (jvj < jpj )

<p lv) r = ")

:

f(l0;

v0;

r0)g)

2

Using this algorithm detail, we obtain a rst (deterministic) solution.
Algorithm 4.145 (okw, obm):

l; v; r := "; S jpj; S jpj; O := ;

f invariant: do jvj = jpj

!I1(l;

v;

r)

g

if v = p ! O := O f(l; v; r)g

] v 6= p ! skip

fi;

(l; v; r) := shift(l; v; r; 1)

odf PM g

2

This algorithm does not take into account how we evaluate v = p. Comparing symbols of v and p from left to right or right to left are two possible methods of evaluating v = p. The methods that we consider in this section all involve indexing in strings v and p. For this reason, we introduce algorithm detail (indices) (originally given on page 77).
To take advantage of the indexing, we de ne `match orders', which will enable us to consider a number of di erent ways of comparing v and p.

De nition 4.146 (Match order): A match order is a bijective function mo 2 1; jpj] ?!

1; jpj]. This function is used to determine the order in which the individual symbols of v

and p are compared.

2

The usefulness of match orders is expressed in the following property.

Property 4.147 (Match order): Since mo is bijective, we now have

(v = p) (8 i : 1 i jpj : vmo(i) = pmo(i))

2

The match order algorithm detail is:

Algorithm detail 4.148 (mo): The symbols of v and p are compared in a xed order

determined by a bijective function mo 2 1; jpj] ?! 1; jpj].

2

102 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

The particular match order used in an algorithm determines the third position of the algorithm name. Three of the most common match orders, (which represent particular instances of detail (mo) and appear in the smaller graph of Figure 4.11 with (mo) as root) are

Algorithm detail 4.149 (fwd): The forward (or identity) match order given by

mo(i) = i.

2

Algorithm detail 4.150 (rev): The reverse match order given by mo(i) = jpj ?

i + 1. This is the original Boyer-Moore match order.

2

Algorithm detail 4.151 (om): Let Prob 2 1; jpj] ?! R be the probability distri-
bution of the symbols of p in input string S; the domain of function Prob consists of indices into p. Let an `optimal mismatch' match order be any permutation mo such that

(8 i; j : 1 i jpj ^ 1 j i : Prob(mo(j)) Prob(mo(i)))

This match order is described as `optimal' because it compares symbols of p in order

of ascending probability of occurring in S. In this way, the least probable symbols of

p are compared rst, so on the average one can expect to nd any mismatch as early

as possible.

2

Example 4.152 (Match orders): We assume a single keyword hehshe (taken from
Example 4.80). The (fwd) match order is mo(i) = i, while the (rev) match order is
mo(i) = 6 ? i + 1 = 7 ? i. If we assume that symbol h is the least probable, followed by s and nally e, we obtain an (om) match order mo = f(1; 1); (2; 3); (3; 5); (4; 4); (5; 6); (6; 2)g. Another possible (om) match order is mo = f(1; 3); (2; 1); (3; 5); (4; 4); (5; 2); (6; 6)g. 2

Comparing v and p using match order mo is done by (program) function match speci ed by

f jvj = jpj g

i := match(v; p; mo)

f

I2(v;

p;

mo;

i)

:(1
^

i (8 j

:

jpj
1

+ 1) j<

^
i

(i jpj ) vmo(i)
: vmo(j) = pmo(j))

6= g

pmo

(i))

Property 4.153 p) (i = jpj+1),
symbol.

(Postcondition and that if i jpj

of match):
then vmo(i) is

From I2(v; p; mo; i) it follows that (v = the rst (in the given order) mismatching
2

An implementation of match is

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

103

func match(v; p; mo) !

i := 1;

do i jpj return i

cand

vmo(i)

=

pmo

(i)

!

i

:=

i

+

1

od;

cnuf

Adding indexing, mo, i, and match to Algorithm 4.145(okw, obm) results in
Algorithm 4.154 (okw, obm, indices, mo):

l; v; r := "; S jpj; S jpj; O := ;

f invariant: do jvj = jpj

!I1(l;

v;

r)

g

i := match(v; p; mo);

f if

Ii2(=v;jpp;j

mo; +1

i) g !O

:=

O

f(l; v; r)g

] i 6= jpj + 1 ! skip

fi;

(l; v; r) := shift(l; v; r; 1)

odf PM g

2

Remark 4.155: In some versions of the Boyer-Moore algorithms match is only executed

after a successful comparison of a symbol of p which is least frequent in S, and the cor-

responding symbol of v. In the taxonomy in HS91] this comparison is called the guard

and the symbol of p is called the guard symbol. We do not consider it here since it can be

viewed

as

additionally

requiring

that

pmo

(1)

is

a

symbol

of

p

with

minimal

frequency

in

S. 2

Remark 4.156: A number of variants of the Boyer-Moore algorithm are considered in

CR94, Chapter 4]. Some of these variants use information from previous match attempts

to reduce the number of symbol comparisons (in v and p) that occur in subsequent match

attempts. While such improvements appear to lead to algorithms which are e cient in

practice, the approach is not considered further in this dissertation.

2

4.5.1 Larger shifts without using match information
It may be possible to make an additional shift (immediately before match is invoked) providing no matches are missed. In this section, we consider making such a shift provided that it can be cheaply implemented. On certain computer architectures, some of the

104 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

shifts described below can be implemented with as few as three machine instructions (see
Chapter 9 for more on implementation issues). A shift of not greater than (MIN k : 0 k jrj ^ PerfMatch(shift(l; v; r; k)) : k) minjrj will be safe. The minjrj is used to ensure that jvj = jpj is maintained, as required for the precondition of match.

Remark 4.157: This de nition of a safe shift is similar to the one given in De nition 4.89,

rewritten to make use of function shift and predicate PerfMatch. In De nition 4.89, a lower

bound of 1 is placed on the bound variable in the MIN quanti cation, while in this section

we use a lower bound of 0. The di erence depends upon the position (in the algorithm) of

the shift: a lower bound of 0 is used for a shift immediately before a match attempt, while

a shift of 1 is used for a shift after a match attempt. That is, the lower bound 0 is used

here because there may be a keyword match at the current position in the input string (a

shift of distance 0), and function match is still to be executed. In Section 4.5.2, we will

again consider safe shift distances of at least one symbol.

2

The shift can be performed with the statement

f jvj = jpj g (l; v; r) := shift(l; v; r; (MIN k : 0 k jrj ^ PerfMatch(shift(l; v; r; k)) : k) minjrj) f jvj = jpj ^ (r = " _ PerfMatch((l; v; r))) g

The safe shift can be implemented in another way, as in the following de nition.
De nition 4.158 (Skip loop): We de ne the following algorithm fragment to be a skip
loop:

f jvj = jpj g do 1 jrj ^ :PerfMatch((l; v; r)) !
(l; v; r) := shift(l; v; r; (MIN k : 1 k jrj ^ PerfMatch(shift(l; v; r; k)) : k) minjrj) od f jvj = jpj ^ (r = " _ PerfMatch((l; v; r))) g

This implementation of the safe shift is known as a skip loop in the taxonomy of Hume

and Sunday HS91].

2

Remark 4.159: Since at most one step is taken by the skip loop (in its present form), this could have been implemented with an if- construct, however, the do-od construct
will prove to be more useful when the shift distance is approximated from below. 2

Convention 4.160 (Skip loop shift distance): The shift distance in the skip loop is

also known as a skip length.

2

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

105

As in Section 4.4, we note that calculating the MIN quanti cation is essentially as

di cult as the problem we are trying to solve. Since any smaller shift length su ces, we

consider weakenings of predicate PerfMatch in the MIN range predicate. Some weak-

eJpn3=6 (in(lg";s.v;Tarrh)e)e:seJ0p((va(jrl;t=ivc;uprlj)a))r(wfoertarskuoeemn,ienJjg1s(:(w1l;evr;erjc)h) ojspejn()v;t1toh=ebepprc1eh)d,eiaJcpa2(tt(eols;

v; r)) (vjpj = pjpj), and J1, J2 and J3 require that evaluate, requiring as few

as a single machine instruction on some machines.

Remark 4.161: Predicates J1 and J2
conjunction of any of these weakenings

are special cases of J3. We can of course and still have a weakening of PerfMatch.

take

the 2

Evaluating predicate PerfMatch in the skip loop guard is equivalent to an invocation of

function match. Fortunately, we can make use of the weakenings of PerfMatch here as well.

Since PerfMatch(l; v; r) ) J3(l; v; r) (and therefore :J3(l; v; r) ) :PerfMatch(l; v; r)), we

will use a weakening in place of PerfMatch in the skip loop guard.

For each weakening of PerfMatch, we assume that the weakening is used in place of

PerfMatch in the guard, and we consider the resulting shift length as calculated with the

quanti ed MIN under the assumption skip loop is equivalent to skip.

that

the

guard

holds.

In

the

case

of

J0,

the

entire

We consider the shift length for J3 before returning to J1 and J2 as special cases. We

need to compute

(MIN k : 1 k jrj ^ PerfMatch(shift(l; v; r; k)) : k)

In order to easily compute this we will weaken the range predicate, removing lookahead.

It is known (from follows (assuming

the 1

dk o-ojdrj,gjuvajr=d)jptjh,a:t J:3J((3l(;(vl;;

v; r)) holds. The derivation proceeds
r)) and xed j : 1 j jpj):

as

PerfMatch(shift(l; v; r; k))
f de nition of shift g
PerfMatch((l(vr k); (v(r k)) k; r k))
) f de nition of PerfMatch g
(v(r k)) k = p
f de nition of = on strings, k jrj, jvj = jpj g (8 h : 1 h jpj : ((v(r k)) k)h = ph)
f rewrite into indexing g (8 h : 1 h jpj : (v(r k))h+k = ph) ) f discard lookahead at r, jvj = jpj g (8 h : 1 h jpj ? k : vh+k = ph)
f change of bound variable: h0 = h + k g (8 h0 : 1 + k h0 jpj : vh0 = ph0?k) ) f one point rule | quanti cation at h0 = j; j jpj g

106 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

1 + k j ) vj = pj?k f assumption: :J3((l; v; r)) g
1 + k j ) vj = pj?k ^ vj 6= pj ) f transitivity of = g
1 + k j ) pj 6= pj?k

We will use the last line to present one skip distance, and the the third last line (1 + k

j)
and

vj so

= pj?k) to present the upperbound of

another
jrj on k

(greater) skip distance. can be dropped.

The

nal predicate is free of r,

Remark 4.162: As in the Commentz-Walter algorithm variant presented in Section 4.4.8,

we could have kept some right lookahead in the weakenings. Although this would have

yielded a family of e cient algorithms, we do not consider such weakenings in this disser-

tation.

2

In order to present the two skip distances concisely, we de ne an auxiliary function and a constant.

De
used

nition 4.163 (Function sl
in predicate J3), we can de

1neanfudncctoionnstsal 1nt2

sVl 2)?:!GNiveanndj

:1 j constant

sl

jpj 22

(the
N

index

sl 1(a)
sl 2

= =

(MIN k : 1 (MIN k : 1

k ^ (1 + k k ^ (1 + k

j j

) )

apj=6=ppj?j?kk))::kk))

Note that sl1 and sl2 both depend implicitly on j.

2

It follows a greater

from the derivation of skip distance than sl2.

the range predicates The disadvantage to

oufsitnhgessel 1twisothfuantcOti(ojnVsj,)tshtaotrasgl 1e

yields space

is required to tabulate it.

Remark
from two

4.164: In Section 4.5.2 we
functions computed for a di

will show how erent purpose.

each

of

sl 1

and

sl 2

can

be

obtained 2

Remark 4.165: It is not too di
are bounded above by j.

cult to see that the skip distances of both sl1 and s2l2

Ebextafme;phl;ei;4r.;1s6; 6yg(,Fwuenhcatvioenssl 2s=l 1

and sl2):
1 and

Assuming

keyword

hehshe,

j

=

4,

and

alpha-

a ehirsy sl1(a) 2 1 4 4 4 4

2

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

107

Either sl1(vj) minjrj or sl2 minjrj can be used as the skip distance. In the remaining

presentations of the algorithms, we will use sl1 exclusively. If a conjunct of any of J0,

J1, J2, or J3 approximated

is used as the

as a
max

weakening of PerfMatch, the of the individual skip lengths.

appropriate skip length can be A particularly interesting skip

length
a2V)

is that arising from predicate and sl2 = 1 and a skip length

J1 of

(in 1 is

which j always

= 1). In this case, sl1(a) used, regardless of p or v.

=

1

(for

all

Assuming J is a weakening of PerfMatch we introduce the following program detail.

Algorithm detail 4.167 (sl): Comparison of v and p is preceded by a skip loop based

upon weakening J of PerfMatch and some appropriate skip length.

2

Assuming some xed j : 1 j jpj we use J3 as an example of a weakening of PerfMatch
in
Algorithm 4.168 (okw, obm, indices, mo, sl):

l; v; r := "; S jpj; S jpj; O := ;

f invariant: do jvj = jpj

!I1(l;

v;

r)

g

f jvj = jpj g

do 1
(l;
od;

jrj
v; r)

^ :J3((l; v; r)) !
:= shift(l; v; r; sl1(vj)

min

jrj)

f jvj = jpj
i := match

(^v;(pJ;3m((ol;)v;;

r))

_

r

=

")

g

f if

Ii2(=v;jpp;j

mo; +1

i) g !O

:=

O

f(l; v; r)g

] i =6 jpj + 1 ! skip

fi;

(l; v; r) := shift(l; v; r; 1)

odBCPM g

2

Note that the e ciency, but not the correctness, of this algorithm is diminished by omitting the skip loop. Although the skip look looks costly to implement, it is usually compiled into a few machine instructions. In HS91], it is shown that the use of a skip loop can yield signi cant improvements to the running time of most BM variants.
We proceed by presenting four instances of detail (sl) (each based on a weakening of PerfMatch)6:

Algorithm detail 4.169 (none): The predicate J0 (de ned as true) is used as the

weakening of PerfMatch in the skip loop. Notice that in this case the skip loop is

equivalent to statement skip.

2

6The names of the details are taken from the taxonomy in HS91].

108 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

Algorithm detail 4.170 (sfc): The predicate J1 (de ned as v1 = p1) is used as

the weakening of PerfMatch, along with the corresponding skip length of 1, in the

skip loop.

2

Algorithm detail 4.171 (fast): The predicate J2 (de ned as vjpj = pjpj) is used as
the weakening of PerfMatch, along with the corresponding skip length, in the skip loop. 2

Algorithm detail 4.172 (slfc): Let pj be a symbol of p with minimal frequency

in S. Predicate J3 (de ned as vj = pj) is used as the weakening of PerfMatch, along

with the corresponding skip length, in the skip loop.

2

4.5.2 Making use of match information
Up to now information from previous matching attempts was not used in the computation of the shift distance (in fact, there was no computation and the shift distance in the update of (l; v; r) at the end of the outer repetition defaulted to 1). In this section, we will take into account the information from the immediately preceding matching attempt, in much the same way as was done in the Commentz-Walter algorithm (see Section 4.4).
We would like to compute a safe shift distance of
(MIN k : 1 k jrj ^ PerfMatch(shift(l; v; r; k)) : k)
and again we proceed with a weakening of the range predicate (see De nition 4.89). In the following derivation we will make use of part of the weakening derivation in Section 4.5.1,
and we will also assume 1 k jrj, jvj = jpj and the postcondition of match, namely
I2(v; p; mo; i). PerfMatch(shift(l; v; r; k))
) f derivation on page 105 g (8 h0 : 1 + k h0 jpj : vh0 = ph0?k) f change of bound variable: h0 = mo(h) g (8 h : 1 h jpj ^ 1 + k mo(h) : vmo(h) = pmo(h)?k) f I2(v; p; mo; i) g (8 h : 1 h jpj ^ 1 + k mo(h) : vmo(h) = pmo(h)?k) ^ (i jpj ) vmo(i) =6 pmo(i)) ^ (8 h : 1 h < i : vmo(h) = pmo(h))
) f one point rule ( rst quanti cation) | h = i g (8 h : 1 h jpj ^ 1 + k mo(h) : vmo(h) = pmo(h)?k) ^ (i jpj cand 1 + k mo(i) ) vmo(i) =6 pmo(i) ^ vmo(i) = pmo(i)?k) ^ (8 h : 1 h < i : vmo(h) = pmo(h))
) f combine 8 quanti cations, with restricted range, since 1 i jpj + 1 g (8 h : 1 h < i ^ 1 + k mo(h) : vmo(h) = pmo(h)?k ^ vmo(h) = pmo(h))

4.5. THE BOYER-MOORE FAMILY OF ALGORITHMS

109

^ (i jpj cand 1 + k mo(i) ) vmo(i) 6= pmo(i) ^ vmo(i) = pmo(i)?k) ) f transitivity of = in quanti cation, to eliminate dependence upon v g
(8 h : 1 h < i ^ 1 + k mo(h) : pmo(h) = pmo(h)?k) ^ (i jpj cand 1 + k mo(i) ) vmo(i) =6 pmo(i) ^ vmo(i) = pmo(i)?k) ) f transitivity of = in implication g (8 h : 1 h < i ^ 1 + k mo(h) : pmo(h) = pmo(h)?k) ^ (i jpj cand 1 + k mo(i) ) pmo(i) 6= pmo(i)?k ^ vmo(i) = pmo(i)?k)

De nition
denoted by

4.173
I3(i; k)

(Predicate I3): The
(here we have chosen

last predicate in the to make parameters

preceding mo, p and

derivation will v implicit).

be 2

Based upon previous match information, a safe shift distance is

(MIN k : 1 k ^ I3(i; k) : k)

Notice that this shift distance still depends on implicit parameters mo and p. In much of the literature, I3 is broken up into three conjuncts:

III33300000((0(iii;;;kkk)))

(8 (i (i

h

:1
jpj jpj

h<i^1 cand 1 + k cand 1 + k

+mmk oo((iim)) ))o(hvp)mm:oo((pii))m=6=o(hpp)mm=oo((pii))m??okk())h)?k)

In order to concisely present a shift distance, we de ne three auxiliary functions.

De nition N, char1 2

14;.j1p7j4+(1F]u?n!ctNio,nasnds1c,hcahra2r21,

and 1; jpj

char + 1]

2?)!: DNe

ne as

functions

s1

2

1; jpj + 1] ?!

s1(i) char 1(i) char 2(i)

= = =

(MIN k : 1 (MIN k : 1 (MIN k : 1

k k k

^ ^ ^

III33300000(0((iii;;;kkk))):::kkk)))

Function char1 implicitly uses v, and requires O(jV j (jpj + 1)) space for tabulation. 2

All three of these functions are bounded above by jpj. coahrnadcrIhtt1ahsraeh2lowOcuaa(lydnjpsbjby+eeienu1lods)tesesdpad;attgchhreeeaaftco,threworcihchseheanirifstI2.2ad(Ivitns;rtapatd;hnmeecoeofo;tlihbl)oaewhntwoincledhgesan,prIrt2eh30.0s(eeAin;Oskt(a)wjtV)iiotjhnI,(3s0j0wl0p(1eji;+awkn1)idl.)l)sTulssh2pe,iaseocminetlhefyoearrnchsccahhtraha1rra.t11

Example 4.175 (Functions s1 and char2): Using keyword hehshe and the (fwd) match
order, we obtain

i 1234567 s1(i) 1 1 2 2 4 4 4

and

110 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

i 1234567 char2(i) 1 1 1 1 1 1 1

The shift distance provided by char2 does not look promising. This is due to the particular

keyword choice. Keywords in which a single symbol is repeated (such as hhhh) yield greater

shift distances.

2

Using functions s1 and char1 yields a new, possibly smaller, shift distance

s1(i) maxchar1(i)

This is known as the match information detail.

Algorithm detail 4.176 (mi): Use information from the preceding match attempt by

computing the shift distance using functions s1 and either char1 or char2.

2

Adding this detail, and integer variable distance for clarity, results in the following BoyerMoore algorithm skeleton7 (cf. HS91, Section 4, p. 1224]):

Algorithm 4.177 (okw, obm, indices, mo, sl, mi):

l; v; r := "; S jpj; S jpj; O := ;

f invariant: do jvj = jpj

!I1(l;

v;

r)

g

f jvj = jpj g

do 1
(l;
od;

jrj
v; r)

^ :J3((l; v; r)) !
:= shift(l; v; r; sl1(vj)

min

jrj)

f jvj = jpj ^ (J3((l; v; r)) _ r = ") g

i := match(v; p; mo);

f if

Ii2(=v;jpp;j

mo; +1

i) g !O

:=

O

f(l; v; r)g

] i =6 jpj + 1 ! skip

fi;

d(li;svt;arn)c:e=:=shsi1ft((i)l;mv;arx; dcihsatra1n(cie);)

odf PM g

2

Remark 4.178: Precomputation
the original taxonomy WZ92].

of

functions

s1,

char 1,

and

char 2

is

brie

y

discussed

in 2

7Details (mo) and (sl) still have to be instantiated | weakening J3 is used for the skip loop.

4.6. CONCLUSIONS

111

Given xed j : 1 j jpj we can easily compute the function sl1 and constant sl2
from Section 4.5.1. This can be done for any particular mo. The functions are

sl 1(vj )
sl 2

= =

char char

12((mmoo??11((jj))))

Example 4.179 (Computing function sl2): Our example of sl2 used j = 4, so
sl2 = char2(mo?1(4)) = char2(4) = 1

2

4.6 Conclusions
The highlights of this taxonomy fall into two categories: general results of the derivation method and speci c results of the taxonomy. The general results are:
The method of re nement used in each of the derivations presents the algorithms in an abstract, easily digested format. This presentation allows a correctness proof of an algorithm to be developed simultaneously with the algorithm itself. The presentation method proves to be more than just a method of deriving algorithms: the derivations themselves serve to classify the algorithms in the taxonomy. This is accomplished by dividing the derivation at points where either problem or algorithm details are introduced. A sequence of such details identi es an algorithm. By pre x-factoring these sequences, common parts of two algorithm derivations are presented simultaneously. The taxonomy of all algorithms considered can be depicted as a graph: the root
represents the original (na ve) solution O := ( l; v; r : lvr = S : flg (fvg\P) frg);
edges represent the addition of a detail; and the internal vertices and leaves represent derived algorithms. This graph is shown in Figure 4.1 in Section 4.1. The utility of this graph is that it can be used as a table of contents to the taxonomy. Being interested in only a subset of the algorithms, for example the Aho-Corasick (AC) algorithms, does not necessitate reading all of the derivations; only the root-leaf paths that lead to the AC algorithms need to be read for a complete overview of these algorithms. The presentation was more than just a taxonomy. Instead of using completed derivations of known algorithms, which are frequently derived in di erent styles, all of the algorithms were derived in a common framework. This made it easier to determine similarities or di erences between algorithms for the purpose of classifying them.

112 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

The pattern matching overviews presented in Aho90, CR94, Step94] are excellent introductions to many of the algorithms presented in this chapter. Unfortunately, they do not present all variants of the algorithms, nor does it present them in a fashion that allows one to contrast the algorithms with one another. The taxonomy in this chapter accomplished precisely this goal, of presenting algorithms in one framework for comparison. In deriving the algorithms for this taxonomy every attempt was made to thoroughly explore all of the possible variants. Our taxonomy is a thorough introduction to all variants of the four principal pattern matching algorithms presented in Aho90, CR94, Step94].

Results concerning particular algorithms can be summarized as follows:

As stated in AC75], the AC algorithm is intended to be a generalization of the original Knuth-Morris-Pratt (KMP) algorithm | making use of automata theory. The classical derivations of the two (using automata and indices, respectively) do not serve to highlight their similarities, or di erences. When derived in the same framework, it becomes apparent that the AC algorithm cannot be specialized to arrive at KMP; this can be seen from the derivation of the AC algorithm subtree of the taxonomy tree. The linear search (introduced in Section 4.3.4) used in the failure function AC algorithm (Algorithm 4.72) is quite di erent from the linear search used in the abstract KMP algorithm (Algorithm 4.76). Indices could have been introduced in Algorithm 4.72, although this does not yield the classically presented KMP algorithm. The relationship between the AC and KMP algorithms is in fact that they have a common ancestor algorithm (p+, e, ac, ls).

The abstract intermediate KMP algorithm (Algorithm 4.76) is in fact a new algorithm, albeit a variant of the AC algorithm. The running time of this new algorithm does not appear to be any better than Algorithm 4.72. The transformation (by adding indices) of Algorithm 4.76 into the classically presented KMP algorithm (Algorithm 4.84) was demonstrated to be straightforward.

The original Aho-Corasick article AC75] presented the `optimal' version of the al-

gorithm after the failure function version of the algorithm. Although the optimal

algorithm was position of the

explained extended

in AC75] as forward trie

using a ef and

transition function failure function ff,

f which is a comour derivation pro-

ceeded much more smoothly by deriving an algorithm which is a common ancestor

of both the optimal and the failure function algorithms.

The pattern matching Moore machine with transition function f is the minimal Moore machine performing the Aho-Corasick pattern matching transduction.

`Predicate weakening' (of Sections 4.4 and 4.5) was instrumental in deriving various algorithms (and their correctness proofs) from the Commentz-Walter (CW) algorithm, in particular the Boyer-Moore (BM) algorithm. The CW algorithm has not

4.6. CONCLUSIONS

113

emerged as a popular string pattern matching algorithm partly due to the di culty in understanding it. The derivation presented in Section 4.4 arrives at the CW algorithm through a series of small transformations, starting with a na ve (quadratic running time) algorithm. This derivation makes the CW algorithm considerably easier to understand. Predicate weakening was also heavily used in deriving the `match-order' variant of the BM algorithm. Commentz-Walter's intention was to combine the BM algorithm with automata theory, to produce an algorithm dealing with multiple keywords. The relationship between the two algorithms has previously remained obscured by the styles of presentation of the two algorithms (indices in BM, and automata in CW). As seen from Section 4.4 the BM algorithm can indeed be arrived at in the same framework (as the CW algorithm) as a special case. The publication of the Hume-Sunday taxonomy HS91] motivated us to also derive the BM algorithm in an entirely di erent manner | making use of the concept of `match-orders'. In both papers by Commentz-Walter describing her algorithm (in particular the technical report Com79b]), the di erences between methods of determining a safe shift amount were not made explicit. Indeed, that some of these shift functions were distinct was not mentioned in all cases. The derivation of the CW algorithm given in this chapter clearly de nes the di erences between the shift functions. In the BM algorithm, the functions contributing to a shift have been presented in several separate papers since the introduction of the original algorithm. Until the publication of the taxonomy by HS91] it was di cult to examine the contribution of each shift function. Both Section 4.5 and HS91] present a shift as consisting of components that can be readily replaced by an equivalent component, for example: the `skip' loops, or the `match-orders'. HS91] emphasized e ects on running-time of each component. Our taxonomy has emphasized the derivation of each of these components from a common speci cation.

114 CHAPTER 4. KEYWORD PATTERN MATCHING ALGORITHMS

Chapter 5
A new RE pattern matching algorithm
This chapter presents a Boyer-Moore type algorithm for regular expression pattern matching, answering an open problem posed by A.V. Aho in 1980 Aho80, p. 342]. The new algorithm handles patterns speci ed by regular expressions | a generalization of the BoyerMoore and Commentz-Walter algorithms, both considered in Chapter 4.
Like the Boyer-Moore and Commentz-Walter algorithms, the new algorithm makes use of shift functions which can be precomputed and tabulated. The precomputation algorithms are derived, and it is shown that the required shift functions can be precomputed from Commentz-Walter's d1 and d2 shift functions.
In certain cases, the Boyer-Moore (respectively Commentz-Walter)algorithm has greatly outperformed the Knuth-Morris-Pratt (respectively Aho-Corasick) algorithm (as discussed in Chapter 13). In testing, the algorithm presented in this chapter also frequently outperforms the regular expression generalization of the Aho-Corasick algorithm.
An early version of this algorithm was presented in WW94]. The research reported in this chapter was performed jointly with Richard E. Watson of the Department of Mathematics, Simon Fraser University, Burnaby, British Columbia, V5A 1S6, Canada; he can be reached at .watsona@sfu.ca
5.1 Introduction
The pattern matching problem is: given a non-empty language L (over an alphabet1 V ) and an input string S (also over alphabet V ), nd all substrings of S that are in L. Several restricted forms of this problem have been solved (all of which are discussed in detail in Chapter 4, and in Aho90, WZ92]):
The Knuth-Morris-Pratt (Section 4.3.6 and KMP77]) and Boyer-Moore (Sections 4.4.6 and 4.5 and BM77]) algorithms solve the problem when L consists of a single word (the single keyword pattern matching problem). The Aho-Corasick (Section 4.3 and AC75]) and Commentz-Walter (Section 4.4 and Com79a, Com79b]) algorithms solve the problem when L is a nite set of
1Throughout this chapter we assume a xed alphabet V .
115

116 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM
(key)words (the multiple keyword pattern matching problem). The Aho-Corasick and Commentz-Walter algorithms are generalizations of the Knuth-Morris-Pratt and Boyer-Moore algorithms respectively. The case where L is a regular language (the regular expression pattern matching problem) can be solved as follows: a nite automaton is constructed for the language V L; each time the automaton enters a nal state (while processing the input string S) a matching substring has been found. This algorithm is detailed in Aho90, AHU74], and is a generalization of the algorithm presented in Section 4.3.7. Since it is also a generalization of the Knuth-Morris-Pratt and Aho-Corasick algorithms, we will refer to it as the GAC (generalized AC) algorithm. Until now, most practical algorithms solving the regular expression pattern matching problem are variants of the GAC algorithm. Although the Knuth-Morris-Pratt and Aho-Corasick algorithms have better worst-case running time than the Boyer-Moore and Commentz-Walter algorithms (respectively), the latter two algorithms are known to be extremely e cient in practice (see Chapter 13 and HS91, Wat94a]). Interestingly, to date no generalization (to the case where L is an arbitrary regular language) of the Boyer-Moore and Commentz-Walter algorithms has been discovered. In 1980, A.V. Aho stated the following open problem: It would also be interesting to know whether there exists a Boyer-Moore type algorithm for regular expression pattern matching. Aho80, p. 342]. In this chapter, we present such an algorithm. As with the Boyer-Moore and CommentzWalter algorithms, the new algorithm requires shift tables. The precomputation of these shift tables is discussed, and shown to be related to the shift tables used by the CommentzWalter algorithm. Finally, the new algorithm is specialized to obtain a variant of the Boyer-Moore (single keyword) algorithm | showing that it is indeed a generalization of the Boyer-Moore algorithm. The algorithm has been implemented, and in practice it frequently displays better performance than the GAC algorithm. The derivation of the new algorithm closely parallel the development of the CommentzWalter algorithm (see Section 4.4), especially in the use of predicate weakening to nd a practically computed shift distance. In the Commentz-Walter algorithm, information from previous match attempts is used to make a shift of at least one symbol; the shift functions are nite, and can therefore be tabulated. In the new algorithm, we also use information from previous match attempts; directly using the information may yield shift functions which are in nite, and therefore impossible to precompute. The main result in the development of the algorithm is a weakening step which allows us to use nite shift functions in place of the (possibly) in nite ones | thereby yielding a practical algorithm. It should be noted that there does exist another regular expression pattern matching algorithm (due to R. Baeza-Yates GB-Y91]) with good performance; that algorithm requires some precomputation on the input string, and is therefore suited to a di erent kind of problem than the one presented in this chapter. This chapter is structured as follows:

5.2. PROBLEM SPECIFICATION AND A SIMPLE FIRST ALGORITHM

117

Section 5.2 gives the problem speci cation, and a simple rst algorithm. Section 5.3 presents the essential idea of greater shift distances while processing the input text, as in the Boyer-Moore algorithm. Section 5.4 derives algorithms required for the precomputation of the shift functions used in the pattern matching algorithm. Section 5.5 specializes the new pattern matching algorithm to obtain the Boyer-Moore algorithm. Section 5.6 provides some data on the performance of the new algorithm versus the GAC algorithm. Section 5.7 discusses some techniques for further improving the performance of the algorithm. Section 5.8 presents the conclusions of this chapter.

5.2 Problem speci cation and a simple rst algorithm
We begin this section with a precise speci cation of the regular language pattern matching problem.
De nition 5.1 (Regular pattern matching problem): Given an alphabet V , an input string S 2 V , a regular expression E (the pattern expression), and regular language L V such that L = LRE (E), establish postcondition
RPM : O = ( l; v; r : lvr = S : flg (fvg \ L) frg) 2
In the remainder of this chapter, we will use language L instead of regular expression E in order to make the algorithm derivation more readable. Note that the encoding of set O is precisely the same as was used in Chapter 4.
We pattern our na ve rst algorithm after Algorithms 4.10 and 4.18 (from Chapter 4, pages 51 and 85). In this algorithm, the pre xes (u) of S and the su xes (v) of u are considered in order of increasing length2.

2Other orders of evaluation can also be used. This order is only chosen so as to arrive at an algorithm generally resembling the Boyer-Moore algorithm.

118 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

Algorithm 5.2:
u; r := "; S;
if " 2 L ! O := f("; "; S)g ] " 26 L ! O := fi; do r 6= " !
u; r := u(r 1); r 1; l; v := u; ";
if v 2 L ! O := O f(l; v; r)g ] v 62 L ! skip fi; do l 6= " cand (l 1)v 2 su (L) !
l; v := l 1; (l 1)v;
if v 2 L ! O := O f(l; v; r)g ] v 62 L ! skip fi od odf RPM g

2

Remark 5.3: Note that we have used if- constructs in giving the initialization and

updates to variable O (compare this with the use of and \ in presenting the initialization

and updates in Algorithm 4.10). This is done to facilitate the introduction of a nite

automaton in the next section.

2

Remark 5.4: The number of iterations of the inner repetition is O(jSj ((MAX w : w 2 L : jwj) minjSj)). This is not the same as the running time, as we have not taken the cost of operations such as " 2 L and v 2 L into account. The implementation of guard conjunct (l 1)v 2 su (L) and expression v 2 L (in the update of variable O) remain unspeci ed. 2

In order to make the algorithm more practical, we introduce a nite automaton, in much the same way that the reverse trie was introduced in Algorithm detail 4.17.

5.2.1 A more practical algorithm using a nite automaton
Since L is a regular language, we construct (from E) a (possibly nondeterministic) "-free nite automaton M = (Q; V; ; ; I; F ) accepting LR (the reverse language3 of L). The
transition function will be taken to have signature 2 P(Q) V ?! P(Q).
3The reverse is used, since we will be using automaton M to consider the symbols of substring v in right to left order instead of left to right order; this is analogous to the way in which the reverse trie was used with Algorithm 4.18.

5.2. PROBLEM SPECIFICATION AND A SIMPLE FIRST ALGORITHM

119

We will not explicitly present the automaton here (except in the examples); we assume that it was constructed (from regular expression E) by one of the well-known algorithms, for example those presented in Chapter 6. Since M is "-free, we have the property that
" 2 L I \F =6 (see Remark 2.94). Finite automata with "-transitions could have been
used; they are only excluded in order to simplify the de nitions given here. In a manner analogous to that in which the reverse trie was introduced into Algo-
rithm 4.10, we now make use of the automaton M. We introduce a new variable C (the
current state set) ranging over P(Q) with the invariant C = f q j q 2 Q ^ vR 2 L?(q) g
String v is reversed in the conjunct since v is processed in reverse. Given this invariant,
the conditional conjunct of the inner repetition guard in Algorithm 5.2, (l 1)v 2 su (L), is equivalent to (C; l 1) =6 . The new algorithm is: Algorithm 5.5:

u; r := "; S;

if I \ F 6= ! O := f("; "; S)g

] I \ F = ! O :=

fi;

do r =6 " !

u; r := u(r 1); r 1;

l; v; C := u; "; I;

if C \ F 6= ! O := O f(l; v; r)g

] C \ F = ! skip

fi; f invariant:

u

=

lv

^

C

=

fq

j

q

2

Q

^

vR

2

L?(q) g

g

do l =6 " cand (C; l 1) =6 !

l; v; C := l 1; (l 1)v; (C; l 1);

f C \ F =6 v 2 L g

if C \ F 6= ! O := O f(l; v; r)g

] C \ F = ! skip

fi

od

odf RPM g

2
Remark 5.6: There are a number of choices in the implementation of the nite automaton
M. In particular, if a deterministic nite automaton is used then the algorithm variable C would always be a singleton set (and the algorithm could be modi ed so that C ranges
over Q instead of P(Q)). The use of a deterministic automaton requires more costly

120 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

precomputation (of the automaton), but enables the algorithm to process input string

S faster. A nondeterministic automaton would involve cheaper precomputation, but the

input string would be processed more slowly as all paths in the automaton are simulated.

A hybrid solution is to begin with a nondeterministic automaton, and then construct

(and tabulate) a deterministic automaton on-the- y, as the nondeterministic automaton is

simulated. The performance of various types of nite automata will be considered again

in Chapter 14. In this chapter, we continue to use a possibly nondeterministic nite

automaton.

2

In order to make some of the derivations in subsequent sections more readable, we de ne some constants.

De nition 5.7 (Constants relating

de of

ne constant mq a shortest word

to in

be L.

the length of a Intuitively, mq

to automaton
shortest word in

ML?)(:q).FoDreeancehmsttaotebeqth2e

Q, we length

is the length of a shortest path from a start state

to state q in M, while m is the length of a shortest path from a start state to a nal state

in M. 2

Example 5.8 (Pattern and corresponding FA): As an example of a regular language pattern, and a corresponding nite automaton, consider the language L = fbd; degfcg fbg fbdag (over alphabet V = fa; b; c; d; eg). In this case, an automaton M (which is shown in Figure 5.1) accepts the language LR = fbgfcg fdb; edg fadbg. Coincidentally, automaton
M is a DFA. The left languages of each of the states (for the automaton in Figure 5.1) are as follows:

LLLLLLL???????(((((((0123456)))))))

= = = = = = =

f"g fag fbgfcg fadg fbgfcg fdg
fbgfcg feg fadbg fbgfcg fdbg fbgfcg fedg

Additionally, m = 3, L and automaton M

m0 = 0, m1 will be used

= as

m2 = 1, m3 our running

= m4 = 2, and m5 = example throughout

m6 this

= 3. Language chapter.

Within examples, we will use names (such as L, V , and M) to refer to the concrete

objects de ned above, as opposed to the abstract objects used elsewhere in this chapter.

2

5.3. GREATER SHIFT DISTANCES

121

5 b 3 d1
a
d0
6 d 4 e 2b

c

Figure 5.1: A nite automaton accepting the regular language LR = fbgfcg fdb; edg fadbg.

5.3 Greater shift distances

Upon termination
that C = f q j q

of
2

the inner
Q ^ vR

2repL?et(itqi)ogn.,

we know (by the invariant
This implies (8 q : q 2

of C

t:hveRinn2erL?re(pqe)t)i,tiaonnd)

equivalently

(8 q : q 2 C : v 2 L?(q)R)

In a manner analogous to the Commentz-Walter and Boyer-Moore algorithm derivations (Sections 4.4 and 4.5), this information can be used on a subsequent iteration of the outer repetition to make a shift k of more than one symbol by replacing the assignment u; r := u(r 1); r 1 by u; r := u(r k); r k.
In order to make use of this information (which relates v and C) on the rst iteration of the outer repetition, we make the invariant of the inner repetition an invariant of the outer repetition as well, by adding the (redundant) initialization l; v; C := u; "; I before the outer repetition4:

Algorithm 5.9:

u; r := "; S;

if I \ F =6 ! O := f("; "; S)g

] I \ F = ! O :=

fi;

l;
f

v; C := u; "; I; invariant: u =

lv

^

C

=

fq

j

q

2

Q

^

vR

2

L?(q) g

g

4This does not change the nature of the algorithm, other than creating a new outer repetition invariant. A similar initialization was added to the Commentz-Walter algorithm skeleton, Algorithm 4.93.

122 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

do r =6 " !

u; r := u(r 1); r 1;

l; v; C := u; "; I;

if C \ F =6 ! O := O f(l; v; r)g

] C \ F = ! skip

fi; f invariant:

u

=

lv

^

C

=

fq

j

q

2

Q

^

vR

2

L?(q) g

g

do l 6= " cand (C; l 1) =6 !

l; v; C := l 1; (l 1)v; (C; l 1);

f C \ F 6= v 2 L g

if C \ F 6= ! O := O f(l; v; r)g

] C \ F = ! skip

fi

od

odf RPM g

2

5.3.1 A more e cient algorithm by computing a greater shift

We wish to use a shift distance which is possibly greater than 1 by replacing the assignment u; r := u(r 1); r 1 by u; r := u(r k); r k (for 1 k). As with the Commentz-Walter and Boyer-Moore algorithms, we would like an ideal shift distance | the shift distance to the nearest match to the right (in input string S). Formally, this distance is given by:
(MIN n : 1 n jrj ^ su (u(r n)) \ L 6= : n). Any shift distance less than this is also
acceptable, and we de ne a safe shift distance (similar to that given in De nition 4.89).

De nition 5.10 (Safe shift distance): A shift distance k satisfying

1 k (MIN n : 1 n jrj ^ su (u(r n)) \ L 6= : n)

is a safe shift distance. We call the upperbound (the quanti cation) the maximal safe shift

distance or the ideal shift distance.

2

Using a safe shift distance, the update of u; r then becomes u; r := u(r k); r k. In order to
compute a safe shift distance, we will weaken predicate su (u(r n)) \ L 6= (which we
call the ideal shift predicate) in the range of the maximal safe shift distance quanti cation. This technique of using predicate weakening to nd a more easily computed shift distance was introduced in Section 4.4.1. The weakest predicate true yields a shift distance of 1. We begin by nding a weakening of the ideal shift predicate which is stronger than true, but still precomputable; later we will present precomputation algorithms for the resulting approximation.
In the following weakening, we will rst remove the dependency (of the ideal shift preciate) on l, then r, then v, leaving a weakening that only depends upon C and n

5.3. GREATER SHIFT DISTANCES

123

(and, of course, language L). The particular weakening that we derive will prove to yield

precomputable
(8 q : q 2 C : v

s2hiLf?t (tqa)bRl)e,s.weAbsseugmininwgit1h

n the

jrj and
ideal shift

the (implied) predicate:

invariant

u

=

lv

^

su (u(r n)) \ L =6 f invariant: u = lv g
su (lv(r n)) \ L 6= ) f discard lookahead to l: l 2 V , monotonicity of su and \ g
su (V v(r n)) \ L =6 ) f domain of r and n: n jrj, so (r n) 2 V n g
su (V vV n) \ L =6 f Property 2.59 g
V vV n \ V L 6= ) f invariant: (8 q : q 2 C : v 2 L?(q)R), monotonicity of \ g
(8 q : q 2 C : V L?(q)RV n \ V L 6= )

The predicate is now free of l; v; r and S and depends only on current state set C, automaton M, and language L (and therefore E). The fact that it is free of r allows us to drop the
conjunct n jrj from the quanti cation giving the shift distance. We will continue this
derivation from the last line.

Remark 5.11: As in the Commentz-Walter algorithms, we could have weakened the

predicate in the above derivation to use one character of lookahead, l 1. With a single

character of lookahead, it seems particularly di cult to arrive at easily precomputed shift

functions, and that approach is not pursued in this dissertation.

2

Forward reference
in nite (for a given q

5.12: The fact that the language L and the 2 Q) makes evaluation of this predicate, (8 q

languages
:q2C:V

L?L?(q()q)cRaVn

be
n\

V L 6= ), di cult. In the following subsection, we will introduce the essential ingredient

of this algorithm derivation, by deriving a more practical range predicate.

2

5.3.2 Deriving a practical range predicate

In order to further weaken the predicate in Forward reference 5.12 (and nd a more easily

cthomatpVuteL?d (wq)eRakenVingL)q,

we aim at and a nite

nite languages Lq (corresponding to each language L0 such that V L V L0. This is

q2
the

Q) such essential

ingredient which reduces the shift functions from being in nite (not precomputable) to

nite (which can be precomputed and tabulated). Possible de nitions of such languages

are as follows.

124 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

De nition 5.13 (Languages Lq and L0): De ne the following languages (for all q 2 Q)

LL0q

= =

su su

( L?(q)R) \ V mq minm (L) \ V m

2

Remark 5.14: The de nitions given here were chosen for their simplicity; other de ni-

tions are possible, but these particular ones lead to a generalization of the Boyer-Moore

algorithm.

2

Languages Lq satisfy an important property that we will require later.

Property 5.15 (Languages Lq): Assuming M 2 FA we have: Usefuls(M) (8 q : q 2 Q : L?(q) 6= ) (8 q : q 2 Q : Lq 6= )

2

In the following property, we show that this de nition of Lq satis es the required property.

Property Proof:

5.16

(Languages

Lq):

For

all

q

2

Q:

V

L?(q)R

V Lq and V L

V L0.

We can see that the de nition of Lq satis es a required property by considering a particular word w:

w 2 L?(q)R

) f de nition of mq: jwj mq (9 x; y : w = xy : x 2 V ^ y 2 su

m( L?q m(qi)nR)m^gjyj = mq minm)

f de nition of \ and V mq minm g

(9 x; y : w = xy : x 2 V ^ y 2 su ( L?(q)R) \ V mq minm)

f de nition of Lq g

(9 x; y : w = xy : x 2 V ^ y 2 Lq)

f de nition of concatenation of languages g

w 2 V Lq

VWe)cVoncL?lu(dqe)RthatVL?L(qq.)R

V Lq. It follows that V L?(q)R

V V Lq, and (since V V = 2

A similar proof applies to the L; L0 case. Note that Lq and L0 (for all q 2 Q) are nite
languages.

5.3. GREATER SHIFT DISTANCES

125

Example 5.17 (Lq and L0): Given our running example, we can see that L0 = fbda; bdb; deb; dcb; ecb; ccbg

and (for all states 0; : : : ; 6 in nite automaton M):

L0 L1 L2 L3 L4 L5 L6

= = = = = = =

f"g fag fbg fda; db; cbg feb; cbg fbda; bdb; dcb; ccbg fdeb; ecb; ccbg

2

We can continue our previous derivation of a useable range predicate, from Forward reference 5.12.
(8 q : q 2 C : V L?(q)RV n \ V L =6 )

) f Property 5.16 g

(8 q : q 2 C : V LqV n \ V L0 =6 ) f existentially quantify over all w 2 Lq g
(8 q : q 2 C : (9 w : w 2 Lq : V wV n \ V L0 6= ))

We now have a usable weakening of the range predicate of the ideal shift distance. Recalling Property 2.22, we can now proceed with our derivation (of an approximation),
beginning with the ideal shift distance:

(MIN n : 1 n jrj ^ su (u(r n)) \ L =6 : n)

f weakening of range predicate (see derivation above), free of r so drop n jrj g (MIN n : 1 n ^ (8 q : q 2 C : (9 w : w 2 Lq : V wV n \ V L0 =6 )) : n)
f Property 2.22 | conjunctive (8) MIN range predicate; jCj is nite g

(MAX q : q 2 C : (MIN n : 1 n ^ (9 w : w 2 Lq : V wV n \ V L0 6= ) : n)) max1 = f Property 2.22 | disjunctive (9) MIN range predicate; jLqj is nite g
(MAX q : q 2 C : (MIN w : w 2 Lq : (MIN n : 1 n ^ V wV n \ V L0 =6 : n))) max1

The second step (above) warrants further explanation. In the case that M 2 FA has no

start states (I = ), variable C will always be . Since this would yield a shift distance

of ?1 we use max1 to ensure that the shift distance is at least 1.

In value

the case where a
+1 | yielding

particular Lq = a shift distance

, the outermost which is not safe.

MIN quanti cation can take the
As mentioned in Property 5.15,

this can be avoided by since it is not possible

requiring that Useful
that q 2 C ^ Lq =

s((tMhis)

holds. can be

In practice, this is not necessary, seen by inspection of the second

conjunct of our algorithm invariant).

We now continue with the inner MIN quanti cation above:

126 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

(MIN n : 1 n ^ V wV n \ V L0 6= : n) = f Property 2.60 g
(MIN n : 1 n ^ (V wV n \ L0 6= _ wV n \ V L0 =6 ) : n) = f Property 2.21; ^ distributes over _ g
(MIN n : 1 n ^ V wV n \ L0 6= : n) min(MIN n : 1 n ^ wV n \ V L0 6=

: n)

This last line above can be written more concisely with the introduction of a pair of auxiliary functions.

De nition 5.18 (Functions d1; d2): We de ne two auxiliary functions d1; d2 2 V ?! N
as:

dd12((xx))

= =

(MIN n : 1 (MIN n : 1

n ^ V xV n \ L0 =6 n ^ xV n \ V L0 =6

: n) : n)

Since their

both d1 and domains as

d2 (

are q:

only
q2

Qapp: lLieqd).toIneleSmecetniotsno5f.L4.q1(,fowreawlliqll

2 Q), we could have given
give an even more useful

characterization of their domains.

2

Note that these functions are almost identical to those de ned in De nition 4.97 | only the domains are di erent.
Using the auxiliary functions, the last line of the derivation above can be written as
d1(w) min d2(w). The approximation of the ideal shift distance is then: (MAX q : q 2 C : (MIN w : w 2 Lq : d1(w) mind2(w))) max1
For readability, we de ne another auxiliary function.

De nition 5.19 (Function t): De ne auxiliary function t 2 Q ?! N as t(q) = (MIN w : w 2 Lq : d1(w) mind2(w))

2

Remark

5.20:

Functions

d1;

d2

and

t

are

easily

precomputed

as

discussed

in

Section

5.4. 2

Using function t gives a shift distance of
(MAX q : q 2 C : t(q)) max1

5.4. PRECOMPUTATION

127

The nal algorithm (using function t and introducing variable distance for readability) is:
Algorithm 5.21 (An e cient algorithm):

u; r := "; S;

if I \ F 6= ! O := f("; "; S)g

] I \ F = ! O :=

fi;

l;
f

v; C := u; "; I; invariant: u =

lv

^

C

=

fq

j

q

2

Q

^

vR

2

L?(q) g

g

do r =6 " !

distance := (MAX q : q 2 C : t(q)) max1;

u; r := u(r distance); r distance;

l; v; C := u; "; I;

if C \ F =6 ! O := O f(l; v; r)g

] C \ F = ! skip

fi; f invariant:

u

=

lv

^

C

=

fq

j

q

2

Q

^

vR

2

L?(q) g

g

do l 6= " cand (C; l 1) 6= !

l; v; C := l 1; (l 1)v; (C; l 1);

f C \ F =6 v 2 L g

if C \ F 6= ! O := O f(l; v; r)g

] C \ F = ! skip

fi

od

odf RPM g

2

5.4 Precomputation
In this section, we consider the precomputation of languages Lq and L0, and functions d1, d2, and t. The precomputation is presented as a series of small algorithms | each easier to understand than a single monolithic one. All algorithms are presented and derived in the reverse order of their application. In practice they would be combined into one algorithm, as is described in Section 5.4.9.

5.4.1 Characterizing the domains of functions d1 and d2

cSainncbeefutnackteinonassdd11a; dn2d2d2(areq

o:nqly2aQpp:liLedq )t?o !eleNm.eInntsoorfdLerq

(for all q 2 Q), their signatures
to make the precomputation of

128 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

the functions easier, we need a di erent characterization of their domains. To do this in

a simple manner, we state q in M, there is

require that Useful a path from q to a

nf(aMl s)tahteolidns;Min.tIunitCivhealyp,ttehri6s,mseevaenrsalthgeantefroarl

every nite

automata construction algorithms are given; many of those algorithms construct automata

with this property. to be more directly

Property useful.

Useful

f

(M )

implies

another

property

of

M

which

will

prove

Property 5.22
follows that

(Left

languages):

From

the

de

nition

of

Useful f

and

Property

2.101

it

(8 q : q 2 Q : L?(q) pref(LR))

2

From the and d2 as

property above, and the
follows (for all q 2 Q):

domains

of

d1

and

d2,

we

can

restrict

the

domains

of

d1

Lq

=
su

(fL?de(q)nRit)io\nVomf qLmqignm

f Property 5.22; monotonicity of su g

su (pref(LR)R) \ V mq minm

= f Property 2.52 | pref and su are duals; reverse is its own inverse g

su (su (L)) \ V mq minm

= f Property 2.51 | idempotence of su g

su (L) \ V mq minm

f mq minm m; w 2 V mq minm ) w 2 su (V m) g

su (su (L) \ V m)

= f de nition of L0 g

su (L0)

Given this
d1; d2 2 su

property (of
(L0) ?! N.

eSaicnhceLjqL),0jwise

can restrict the
nite, then jsu

domain
(L0)j is

of functions d1 and d2 so nite as well. Notice that

that this

new signature keyword set L0

(for d1 and in place of

d2) corresponds keyword set P.

to

that

given

in

De

nition

4.97,

with

(

nite)

Example 5.23 (Language L0): In our running example, where

L0 = fbda; bdb; deb; dcb; ecb; ccbg

we have

su (L0) = f"; a; b; da; db; eb; cb; bda; bdb;deb; dcb; ecb; ccbg

Given the de nitions of d1; d2, we can compute the two functions by hand:

5.4. PRECOMPUTATION

129

w " a b da db eb cb bda bdb deb dcb ecb ccb
d1(w) 1 +1 2 +1 +1 +1 +1 +1 +1 +1 +1 +1 +1
d2(w) 3 3 2 3 2 2 2 3 2 2 2 2 2 2
Before precomputing d1; d2, we concentrate on the precomputation of function t.

5.4.2 Precomputing function t

Assuming that functions d1 and d2 and sets Lq (for all q 2 Q) have been precomputed, we
can compute function t as follows (variable tee is used to accumulate shift function t):

Algorithm 5.24 (Computing t):

for q : q 2 Q !

tee(q) := +1

rof;

for rof

q;u : q 2
tee(q) :=
f tee = t

gtQee^(qu) m2iLnqd!1(u)

min

d2(u)

2

Notice that we impose no unnecessary order of evaluation in either of the two repetitions. An implementor of this algorithm is free to choose an order of evaluation which is most e cient for the encoding used in the implementation.

Example 5.25 (Function t): In our running example, we obtain the following values for

function 3; t(2) =

t2;(tg(i3v)en=t2h;etv(4a)lu=es2o;ft(L5q)

for all states q, = 2; t(6) = 2.

and

functions

d1

and

d2):

t(0)

=

1;

t(1)

= 2

5.4.3 Precomputing functions d1 and d2

With the domain of functions d1 Commentz-Walter precomputed

and d2 restricted to functions for ( nite)

su (L0),
keyword

sfuetncLt0ionCsodm17a9nad,

d2 are the Com79b].

We now present two algorithms, computing d1 and d2 respectively. The algorithms

are fully derived in WZ95], and are given here without proofs of correctness. The two

precomputation algorithms presented below depend upon the reverse failure function cor-

responding to keyword set L0.

De nition 5.26 (Function
to L0 is de ned as

fr):

Function fr 2 su

(L0) n f"g ?! su

(L0) corresponding

fr(u) = (MAX p w : w 2 pref(u) n fug \ su (L0) : w)

2

130 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

The reverse failure function is de ned analogously to the forward failure function | see

De nition 4.65 and Remark 4.66.

In the following two algorithms, we assume that function fr is precomputed (variables dee1 and dee2 are used to accumulate d1 and d2, respectively):

Algorithm 5.27 (Computing d1):

for u : u 2 su (L0) !

dee1 (u) := +1

rof;

for u : u 2 su (L0) n f"g !

rof fdedee1e1(f=r (ud)1)

:=
g

dee1

(fr(u))

min(juj

?

jfr

(u)j)

2
Again, notice that we impose no unnecessary order of evaluation in either of the two repetitions.
Algorithm 5.28 (Computing d2): for u : u 2 su (L0) ! dee2 (u) := +1 rof; for u : u 2 L0 !
v := u;
do v 6= " ! vif:=jujf?r(vjv);j < dee2(v) ! dee2(v) := juj ? jvj ] juj ? jvj dee2(v) ! v := " fi
od rof;
n := 1;
do su (L0) \ V n 6= ! for u : u 2 su (L0) \ V n ! dee2 (u) := dee2 (u) mindee2 (u 1) rof;
n := n + 1
odf dee2 = d2 g

2

Notice that the third (un-nested) repetition is a breadth- rst traversal of the set su (L0),

and the second (un-nested) repetition requires that function de nition of language L0, the depth of the traversal is m.

fr

is

precomputed.

By

the

Precomputation using these algorithms has been found to be cheap in practice Wat94a].

5.4. PRECOMPUTATION

131

5.4.4 Precomputing function fr
The following algorithm (taken largely from WZ92, Section 6, p. 33]) computes function fr (variable e r is used to accumulate fr):

Algorithm 5.29 (Computing fr):

for a : a 2 V !

if a 2 su (L0) ! e r(a) := "

] a 26 su (L0) ! skip

fi

rof;

n := 1;

f invariant: (8 u : u 2 su (L0) ^ juj do su (L0) \ V n 6= !

n : e r(u) = fr(u)) g

for u; a : u 2 su (L0) \ V n ^ a 2 V !

if au 2 su (L0) !

u0 := e r(u);

do u0 =6 " ^ au0 26 su (L0) !

u0 := e r(u0)

od;

if u0 = " ^ a 26 su (L0) ! e r(au) := "

] u0 6= " _ a 2 su (L0) ! e r(au) := au0

fi

] au 62 su (L0) ! skip

fi

rof;

n := n + 1

od

fn>mg

f e r = fr g

2
This algorithm also makes use of a breadth- rst traversal (of depth m) of the set su (L0).

Example 5.30 (Function fr): Consider the function fr for our running example: w : w 2 su (L0) n f"g a b da db eb cb bda bdb deb dcb ecb ccb
fr(w) " " " " " " b b " " " "

2

132 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

5.4.5 Precomputing sets Lq

The languages Lq can easily be precomputed using relation Reach(M) (see De nition 2.95) and two auxiliary functions.

De nition 5.31 (Functions st and emm): The auxiliary functions are st 2 su (L0) ?! P(Q) and emm 2 Q ?! 0; m] de ned as:
st(u) = f q j q 2 Q ^ uR 2 L?(q) g emm(q) = mq minm
2

Relation Reach(M) will be used along with the following property of nite automata.

Property 5.32 (Reachability and left languages): A useful property (of any nite automaton) is that (for all states q 2 Q):
pref( L?(q)) = ( p : p 2 Q ^ (p; q) 2 Reach(M) : L?(p))

2

Given relation Reach(M) and functions emm and st, we now derive an expression for Lq that is easier to compute (than the de nition):

Lq

=
su

(fL?de(q)nRit)io\nVomf qLmqignm

= f Property 2.52 | pref and su are duals g pref( L?(q))R \ V mq minm

= f Property 5.32 g ( p : p 2 Q ^ (p; q) 2 Reach(M) : L?(p))R \ V mq minm

= f \ and R distribute over g

( p : p 2 Q ^ (p; q) 2 Reach(M) : L?(p)R \ V mq minm) = f quantify over all words w : w 2 L?(p)R \ V mq minm g
( w; p : p 2 Q ^ (p; q) 2 Reach(M) ^ w 2 L?(p)R \ V mq minm : fwg)

= f de nition of \ g ( w; p : p 2 Q ^ (p; q) 2 Reach(M) ^ w 2 L?(p)R ^: fwg)
= f Lq su (L0); w 2 L?(p)R wR 2 L?(p) p 2 st(w) g

( w; p : p 2 Q ^ (p; q) 2 Reach(M) ^ w 2 su (L0) ^ p 2 st(w) ^ w 2 V mq minm : fwg)

= f w 2 V mq minm jwj = mq min m jwj = emm(q) g

( w; p : p 2 Q ^ (p; q) 2 Reach(M) ^ w 2 su (L0) ^ p 2 st(w) ^ jwj = emm(q) : fwg)

5.4. PRECOMPUTATION

133

Assuming that relation Reach and auxiliary functions emm and st are precomputed, we
can now present an algorithm computing Lq (for all q 2 Q, using variable ell to accumulate
the sets Lq):
Algorithm 5.33 (Computing Lq): for q : q 2 Q !
ell (q) :=
rof; for p; q; w : (p; q) 2 Reach(M) ^ w 2 su (L0) ^ p 2 st(w) ^ jwj = emm(q) !
ell(q) := ell(q) fwg roff (8 q : q 2 Q : ell(q) = Lq) g
2

5.4.6 Precomputing function emm
Assuming that function st and length m have already been computed, the following algo-
rithm computes function emm using a breadth- rst traversal of su (L0): Algorithm 5.34 (Computing emm):
for q : q 2 Q ! if q 2 I ! emm(q) := 0 ] q 26 I ! emm(q) := m fi
rof;
n := 1;
do su (L0) \ V n 6= ! for u : u 2 su (L0) \ V n ! for q : q 2 st(u) ! emm(q) := emm(q) minn rof rof;
n := n + 1
odf (8 q : q 2 Q : emm(q) = mq minm) g
2

5.4.7 Precomputing function st and languages L0 and su (L0)
The following algorithm makes a breadth- rst traversal (of depth m) of the transition graph of nite automaton M. It simultaneously computes function st, languages L0 and
su (L0), and m (the length of a shortest word in language L).

134 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

Languages L0 and su (L0) are used in most of the precomputation algorithms already presented. While the following algorithm computes language su (L0), it is also an example of a breadth- rst traversal of su (L0) without having to explicitly compute and store it;
instead, the algorithm traverses the transition graph of nite automaton M and implicitly
performs a breadth- rst traversal of su (L0).

Algorithm 5.35 (Computing st, L0, and su (L0)):

st("); current; SLprime; n; nal := I; f"g; f"g; 0; (I \ F = );

f invariant: current = su (L0) \ V n

^ SLprime = ( i : i n : su (L0) \ V i)

^0 n m

^ ( nal n = m) ^ (8 u : u 2 su (L0) ^ juj

n : st(u) = f q j uR 2 L?(q) g) g

do : nal !

current0 := ;

n := n + 1;

for u; a : u 2 current ^ a 2 V !

if (st(u); a) =6 !

f au 2 su (L0) \ V n g

st(au) :=
f (8 q : q

(st(u); a);
2 st(au) : au

2

L?(q)R)

g

current0 := current0 faug;

nal := nal _ (st(au) \ F 6= )

] (st(u); a) = ! skip

fi

rof;

current := current0;

SLprime := SLprime current

od

fn=mg

f current = su (L0) \ V m = L0 g

f f

SLprime = su (L0) (8 u : u 2 su (L0) :

g
st (u)

=

fq

j

uR

2

L?(q) g)

g

2

5.5. SPECIALIZING THE PATTERN MATCHING ALGORITHM

135

5.4.8 Precomputing relation Reach(M)
Relation Reach(M) can be precomputed by a re exive and transitive closure algorithm. The algorithm is (where IQ is the identity binary relation on state set Q):
Algorithm 5.36 (Computing Reach(M)):

Rch := ;
for q; a : q 2 Q ^ a 2 V ! Rch := Rch f(q; q)g (fqg (fqg; a))
rof; cfhRacnhge=:=IQtrue;2( ) g do change !
change := false;
for p; q; r : (p; q) 2 Rch ^ (q; r) 2 Rch ! change := change _ (p; r) 26 Rch; Rch := Rch f(p; r)g
rof odf Rch = Reach(M) g

2

5.4.9 Combining the precomputation algorithms
The precomputation algorithms can be combined into a single monolithic algorithm. Such an algorithm is essentially the sequential concatenation of the separate precomputation algorithms. The order in which the algorithms are applied is determined by their dependency graph, which is shown in Figure 5.2. A possible order of execution is obtained by reversing a topological sort of the dependency graph. One such order is: (Algorithms) 5.36, 5.35, 5.34, 5.33, 5.29, 5.27, 5.28, 5.24.

5.5 Specializing the pattern matching algorithm
By restricting the form of the regular expression patterns, we can specialize the pattern matching algorithm to obtain the Boyer-Moore and the Commentz-Walter algorithms. In this section, we specialize to obtain a variant of the Boyer-Moore algorithm that does not use a lookahead symbol.
To obtain the single-keyword pattern matching problem, we require that L be a single-
ton set; that is L = fpg (problem detail (okw) from Chapter 4), a language consisting of
a single keyword. We can now give a nite automaton accepting LR.

136 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

5.27 5.24 5.29
5.28

5.34 5.33
5.36

5.35

Figure 5.2: The dependency graph of the precomputation algorithms. An arrow from algorithm a to algorithm b indicates that algorithm b must be executed before algorithm a.

De nition 5.37 (Finite automaton accepting LR): We de ne deterministic nite au-

tomaton M = (su (p); V; ; ; f"g; fpg). The states are elements of su (p). We de ne

deterministic transition function 2 su (p) V ?! su (p) f?g (the special value ?

denotes an unde ned transition) as:

(w;

a)

=

(

aw
?

if aw 2 su
otherwise

(p)

2

Usefulf(M) holds. Given function , we have (for every state w 2 su (p)): L?(w) = fwRg

AwaTColfehluwesetaahuoyrasmlhsvy(iea,apfttw)msoiden(nwiighsnMtl=aeapvtnaeojicwsrnLetjdiswcweeauttniel;ldalrcrmstaL,uhliclwenuinit(=srLtrbsiet0ec)naL?,tt=dae(1snw(wtsdwau)Rt2)tehm(=svepuai)ncfr.uiwd(aFrpg2bru().elnw.encAtS)wtidisin)ontdncacietttathienoL-e?inssbeau(gtewlpilvyvd)e,aenainrssctiieoaanbdbcoyseleifedntvm(g(aawlCers)=itaioi=nnbnjtpleedtsjghse,1et(eLurwa;a0in)rln=gdm.dojiELriwcinletej=hsdmm2(f(ein)wjpnptgti)joss,.. string p) in the range 0; jpj], as was done in Section 4.3.6.1. By making use of this encoding,
and changing the domain of the variables u; r and functions d1; d2 to make use of indexing in input string S, we obtain the Boyer-Moore algorithm. The Commentz-Walter algorithm can similarly be obtained as a specialization.

5.6 The performance of the algorithm
Empirical performance data was gathered by implementing this algorithm in a grep style pattern matching tool, running under Unix (on a Sun Sparc Station 1+) and MS-Dos (on a 20 Mhz 386DX).

5.7. IMPROVING THE ALGORITHM

137

On each run, the new algorithm was used in addition to the old (generalized AhoCorasick) algorithm which constructs a nite automaton accepting the language V L. (For both the old and the new algorithms, only deterministic nite automata were used. The time required for precomputation was not measured, but for both algorithms it appeared to be negligible compared to the time required to process the input string.) In the cases
where m 6 (the length of the shortest word in L is at least 6), and jL0j 18, this new
algorithm outperforms the other algorithm. These conditions held on approximately 35% of our user-entered regular expression patterns.
In the cases where the new algorithm outperformed the traditional one, the di erences in execution speed varied from a 5% improvement to a 150% improvement. In the cases where the new algorithm was outperformed, its execution speed was never less than 30% of the execution speed of the traditional algorithm.
The conditions for obtaining high performance from the new algorithm (m 6 ^ jL0j
18) can easily be determined from automaton M. In a grep style pattern matching tool, the automaton M can be constructed for language LR. If the required conditions are met, the Boyer-Moore type pattern matcher presented in this chapter is used. If the conditions are not met, M can be reversed (so that it accepts language L), and converted to an automaton accepting V L. The traditional algorithm can then be used.

5.7 Improving the algorithm
In this section, we brie y consider two approaches to improving the expected performance of the algorithm. In the rst approach, we consider the use of a right lookahead symbol, to improve the shift distance. In the second approach, we consider how the choice of a particular FA can a ect the shift distance.
The use of a right lookahead symbol was rst discussed, in the context of the CommentzWalter algorithm, in Section 4.4.8. Had we retained a single symbol of right lookahead in this chapter, we would have arrived at the weakening
(8 q : q 2 C : V L?(q)R(r 1)V n?1 \ V L 6= )
in place of
(8 q : q 2 C : V L?(q)RV n \ V L 6= )
in the derivation on page 123. The introduction of sets Lq and L0 would remain unchanged, yielding the weakening
(8 q : q 2 C : (9 w : w 2 Lq : V w(r 1)V n?1 \ V L0 6= ))
in place of the one on page 125. We could then use the same techniques as used in Section 4.4.8 to introduce an auxiliary function and give a new shift distance which uses this right lookahead symbol. This is left to the reader.

138 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

5 b 3 d1
a
0 6 d 4 e 2b

50 b

d
30

c

Figure 5.3: Improved automaton, equivalent to the one shown in Figure 5.1.

Another possible technique for improvement in the algorithm involves the following

observation: it follows from the de nition of function t (De nition 5.19) that smaller sets

Lq can lead to greater shift distances. Consider the nite automaton used in the examples

| see Figure 5.1. In this automaton we will consider two states in particular: 3 and

5. We see that L3 = fda; db; cbg, L5 = fbda; bdb; dcb; ccbg, t(3) = 2, and t(5) = 2 (see

Examples 5.17 and 5.25). The relatively low shift distance for state 3 is due to the fact

that it is not possible to tell (without modifying the algorithm) from the state number (3)

whether the most recent in-transition was from state 1 or state 2.

If we were to split states 3 and 5, producing two new states 30 and 50, we would obtain

the FA shown in Figure 5.3. In this new automaton, we have the following changes:

L3 = fdag, L30
shift function is

= fdb; cbg,
changed to

L5 = fbdag, and L50
t(3) = t(5) = 3 and

= fbdb; dcb; ccbg.
t(30) = t(50) = 2.

Correspondingly, the Using this alternative

automaton, when in state 3 or state 5, the resulting shift is 3 symbols instead of 2.

In order to take advantage of this type of improvement, it is necessary to use nite

automata which distinguish as much as possible between di erent strings in their left

languages (as in the way we split state 3 to distinguish between the two in-transitions

in the original automaton). The tradeo is that the alternative automaton requires more

storage space. It would be interesting to know quantitatively what the tradeo s are between

automaton size and shift distances.

5.8 Conclusions
We have achieved our aim of deriving an e cient generalized Boyer-Moore type pattern matching algorithm for regular languages. The stepwise derivation began with a simple, intuitive rst algorithm; a nite automaton was introduced to make the implementation practical. The idea of shift distances greater than one symbol (as in the Boyer-Moore

5.8. CONCLUSIONS

139

and Commentz-Walter algorithms) was introduced. The use of predicate weakening was instrumental in deriving a practical approximation to the ideal shift distance.
Using a structural property of nite automata, the approximation was shown to be the composition of several functions, all but two of which are easily computed. The remaining two functions are the Commentz-Walter shift functions; an algorithm computing these functions has previously been derived with correctness arguments in WZ92, WZ95].
A Boyer-Moore algorithm was derived as a special case of our algorithm, showing our algorithm to be a truly generalized pattern matching algorithm.

140 CHAPTER 5. A NEW RE PATTERN MATCHING ALGORITHM

Chapter 6
FA construction algorithms
In this chapter, we present a taxonomy of functions and algorithms (collectively called constructions) which construct a nite automaton from a regular expression. The taxonomy presented here reuses some ideas of the earlier taxonomy given in Wat93a]. The taxonomy will be presented in a less rigorous fashion than the one in Chapter 4. We present a number of de nitions and theorems without full proofs; in those cases, the proofs are omitted for brevity.
6.1 Introduction and related work
The nite automaton construction problem is: given a regular expression E, construct
M 2 FA (or, in some cases, M 2 DFA) such that LRE(E) = LFA(M).
The central idea behind our taxonomy is as follows: we present a `canonical' construction whose correctness is easily seen. The states in a canonically constructed FA have internal structure, encoding information | namely the left language and the right language of the state. In subsequent constructions, we may discard some of the information (perhaps identifying states) or we may encode the states (for example, as regular expressions) for greater e ciency. Additionally, we can apply function rem" to remove "-transitions, or the subset construction yielding a DFA. We use these techniques to arrive at all of the well-known constructions.
In order to clearly present and derive the constructions, they are given as functions (instead of as algorithms). Since most of the constructions given in the literature (especially in text-books) are given as algorithms, we also select a few and present them again as imperative programs in Section 6.9.
The main nite automata constructions included in the taxonomy are: A variant of the Thompson construction as presented in Thom68], appearing as Construction 6.15. This construction, which is known in this dissertation as the `canonical' construction, produces a (possibly nondeterministic) nite automaton (possibly with "-transitions). It is based upon the concept of `items' which is borrowed from LR parsing Knut65]. 141

142 CHAPTER 6. FA CONSTRUCTION ALGORITHMS
The "-free item set construction, appearing as Construction 6.19. This construction is the composition of "-transition removal with the canonical construction. The deterministic item set construction, given on page 156 (as Construction (rem-", subset, use-s)) and as Algorithm 6.83. The construction produces a deterministic nite automaton and does not appear in the literature. An improvement of the item set construction. This construction (appearing on page 158 (as Construction (rem-", subset, use-s, Wfilt)) and not appearing in the literature) produces a deterministic nite automaton, and is also based upon the DFA item set construction. Furthermore, it is an improvement of DeRemer's construction (mentioned below). A variant (given in Section 6.9) is also related to the Aho-Sethi-Ullman deterministic nite automaton construction. The DeRemer construction as presented in DeRe74]. This construction (page 159 as Construction (rem-", subset, use-s, Xfilt)) produces a deterministic nite automaton. In this chapter, it is derived from the item set construction, although DeRemer made use of LR parsing in his derivation. The Berry-Sethi construction as presented in BS86, Glus61, MY60]. This construction appears as Construction 6.39. It is implicitly given by Glushkov Glus61] and McNaughton and Yamada MY60], where it is used as the nondeterministic nite automaton construction underlying a deterministic nite automaton construction. Berry and Sethi explicitly present this algorithm in BS86], where they relate it to Brzozowski's DFA construction Brzo64]. The McNaughton-Yamada-Glushkov construction as presented in MY60, Glus61]. This construction (Construction 6.44) produces a DFA. The dual of the Berry-Sethi construction. This construction (Construction 6.65) is the `mirror image' of the Berry-Sethi construction. A variant of this construction was also mentioned in passing by Aho, Sethi, and Ullman ASU86, Example 3.22, p. 140]; that variant appears in this chapter as Construction 6.68. The Aho-Sethi-Ullman construction as presented in ASU86, Alg. 3.5, Fig. 3.44]. This construction (Construction 6.69 and Algorithm 6.86) produces a deterministic nite automaton. It is, in a sense, the `mirror image' of a variant of the McNaughtonYamada-Glushkov construction. The Antimirov construction as presented in Anti94, Anti95]. This construction (Construction 6.55 in this dissertation) yields an "-free nite automaton. The Brzozowski construction as presented in Brzo64]. This construction (Construction 6.57) gives a deterministic nite automaton.

6.1. INTRODUCTION AND RELATED WORK

143

The resulting taxonomy graph is shown in Figure 6.1. This graph will be reproduced in each of the following sections of this chapter, indicating the subpart of the taxonomy considered in the section. Later in this section, we give a list of algorithm and problem details introduced and used in this chapter.
Another taxonomy of constructions (also by the present author) was given in Wat93a]. That taxonomy is structured around the idea of -algebras, deriving a few constructions that are not covered here. It is also much larger (textually) than the one presented in this chapter.
This chapter is structured as follows:

In Section 6.2, we introduce `items' and an alternative (tree-based) de nition of regular expressions.

Section 6.3 presents the `canonical' nite automata construction.

Section 6.4 is the beginning of the presentation of the "-free constructions.

In Section 6.5, we introduce a method of encoding the sets of items which are used as states in Section 6.4.

Section 6.6 introduces some constructions which use derivatives of regular expressions for states, providing an encoding of the constructions of Section 6.4.

Section 6.7 gives the duals of some of the constructions given in preceding sections.

The constructions given in Sections 6.5 and 6.7 make use of auxiliary functions and sets. In Section 6.8 we consider the precomputation of these functions and sets.

Section 6.9 gives the imperative programs which implement some of the constructions.

Section 6.10 contains the conclusions of this chapter.

The following is a list the algorithm and problem details with a short description of each:

rem-"

(Algorithm detail 6.18) "-transition removal function rem" is composed onto a construction.

use-s

(Algorithm detail is composed onto

6.21) Start-unreachable a construction.

state

removal

function

useful

s

subset (Algorithm detail 6.22) The subset construction is composed onto a construction.

filt (Algorithm detail 6.25) A lter is used to remove redundant items. Particular lters are:
Wfilt (Algorithm detail 6.26) Function W is used as a lter.

144 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

6.15

rem-" 6.19

rem-"-dual 6.63

subset

filt

pd

Ant. (6.55)6.27

use-s subset

6.83

sym sym
6.35 a-s a-s BS (6.39) 6.65

filt pd

filt use-s b-mark

p. 158 Brz. (6.57)

6.43 subset

subset use-s

e-mark subset
6.68
use-s subset

use-s

MYG (6.44)

use-s

6.85 ASU (6.86)

filt

subset subset use-s use-s filt

Wfilt

Xfilt

Figure 6.1: A taxonomy of nite automata construction algorithms. The larger graph represents the main part of the taxonomy, while the smaller graph represents the two instantiations of the filt detail that are discussed in this dissertation. The numbers appearing at some of the vertices correspond to the algorithm or construction numbers in the text of this chapter. In some cases, the algorithm is not presented explicitly, and the page number is given instead. The use of duality is clearly shown by the symmetry in the graph. The algorithms in the dashed-line subtree (on the right of the graph) are not treated in this dissertation, since they are the duals of algorithms in the left half and it is not clear that the duals would be more e cient or enlightening.

6.2. ITEMS AND AN ALTERNATIVE DEFINITION OF RE

145

Xfilt (Algorithm detail 6.31) Function X is used as a lter.

sym (Algorithm detail 6.33) States (sets of items) are encoded by elements of SymnodesE.

a-s (Algorithm detail 6.38) Auxiliary sets Follow E; FirstE; LastE, and predicate Null are used to encode the states and transitions.

b-mark

(Algorithm detail 6.42) Certain constructions are greatly simpli ed by prepending a symbol (the `begin-marker', usually written $) to the input regular expression E. In the dual constructions, an `end-marker' is appended instead.

pd (Algorithm detail 6.54) Partial derivatives are used to encode the states and transition relation in a construction.

rem-"-dual (Algorithm detail 6.62) The dual of the "-transition removal function is composed onto a construction.

e-mark

(Algorithm detail 6.67) As with detail b-mark, but an end-marker is appended to the regular expression. This detail is used for the dual constructions.

6.2 Items and an alternative de nition of RE
This chapter relies upon the tree de nitions given in Section 2.5. In order to present the canonical construction, we need the ability to refer to a particular subexpression of an RE,
and the place where it occurs in the RE. For example, in (a a) 2 RE, we would like to
be able to distinguish between the two a subexpressions. To do this, we view an RE as synonymous with its corresponding parse tree.
De nition 6.1 (Regular expressions): De ne the set of regular expressions over alphabet V as a set of trees. We take W = (V f ; "; ; ; ; +; ?g; r) as ranked alphabet where symbols in the set V f ; "g are all nullary, f ; +; ?g are all unary, and f ; g are
all binary. We can then de ne the set of regular expressions RE to be the set Trees(W ). 2
Using this de nition, we refer to the set of nodes of the parse tree of E 2 RE as dom(E),
and the operator at node e is E(e). The two a subexpressions above, in a a, would be nodes 1 and 2 respectively.

146 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

12

1 21 a "b Figure 6.2: Tree view of an example RE.

Functions on regular expressions, such as LRE , can by extended to work with the tree
versions of regular expressions. Given this equivalence, we will use the two views of regular expressions interchangeably; which one is used at any given time will be clear from the context.

Example 6.2 (Regular expression): We use the regular expression E = (a ") (b )

as our example. In the tree view, we have dom(E) = f0; 1; 2; 1 1; 1 2; 2 1g and

E = f(0; ); (1; ); (2; ); (1 1; a); (1 2; "); (2 1; b)g. This tree is shown in Figure 6.2. We

will use this regular expression in examples throughout this chapter.

2

Some of the following de nitions are given with respect to a particular E 2 RE.
We will be considering regular expressions with a dot placed somewhere in the regular expression. Such a regular expression, with the dot, is sometimes called an item or a dotted regular expression. We now give some formal de nitions relating to dottings.

De nition 6.3 (Item): An item (or dotted regular expression) is a triple (E; n; p) where:

E is the regular expression.

n 2 dom(E), that is n is a node in the tree interpretation of regular expression E.

p 2 fBEF ; AFTg is the position of the dot, either before or after the node n.

We use DRE to denote the set of all dotted regular expressions. We can also write an item

in a linear form, in which the dot (depicted as , not to be confused with the bullet used

for typesetting lists) is embedded in the regular expression itself. This is shown in the

example which follows.

2

Example 6.4 (Item): Given our regular expression, (a ") (b ), the following is an item:

((a ") (b ); 1; AFT)

We could also write this item as ((a ") ) (b ). The tree form of this item is shown in

Figure 6.3.

2

6.2. ITEMS AND AN ALTERNATIVE DEFINITION OF RE

147

12
1 21 a "b Figure 6.3: Tree form of an item.

Sometimes we will need access to the regular expression underlying a particular item. The following function gives such access.

De nition 6.5 (Function undot): Function undot de ned as undot(E; n; p) = E maps

an item to its underlying regular expression.

2

We will frequently be considering sets of dottings of a particular regular expression. In this case, we can drop the rst component of the dottings (the regular expression) in the set, since they will all be over the same regular expression. This is simply a notational convenience. We will freely alternate between the pair and triple form of items, choosing the form that best suits the application. The following de nition is the set of all dottings of a particular regular expression.

De nition 6.6 (Set DotsE): De ne the set DotsE as the set of all items over E. That is,

DotsE = dom(E) fBEF ; AFT g
Intuitively, DotsE is the set of all items D such that undot(D) = E.

2

148 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

Example 6.7 (DotsE): Given our example regular expression, (a ") (b ), we give the
set Dots(a ") (b ) as (where we have fully parenthesized the regular expression)
( ((a) (")) ((b) )) (( (a) (")) ((b) )) ((( a) (")) ((b) )) (((a ) (")) ((b) )) (((a) ( ")) ((b) )) (((a) (" )) ((b) )) (((a) (") ) ((b) )) (((a) (")) ( (b) )) (((a) (")) (( b) )) (((a) (")) ((b ) )) (((a) (")) ((b) )) (((a) (")) ((b) ) )
2

For every item, we can give a pair of regular expressions denoting (respectively) the language to the left and to the right of the dot. The regular expressions are given by the functions de ned as follows.

De nition 6.8 (Functions E? and ?!E ): We de ne functions E?; ?!E 2 DRE ?! RE

giving the regular respectively. The

dexepnreitsisoionnos fd?e!Enoitsinigndlauncgtiuvaegeosn

to the left and to the right the structure of items (we

of the dot take some

notational shortcuts in the de nition), assuming that F; F0, and F1 are regular expressions:

?????????!!!!!!!!!EEEEEEEEE (((((((((FFFFFFFFF0000;;+?00;;;;;11FF1BAFF11EF;11;ww;;w12FT21;;;)pp)p)ww))ww;;;;pppp))))

= = = = = = = = =

F

"???????!!!!!!!EEEEEEE (((((((FFFFFFF0110;;;;;;;wwwwwww;;;;;;;ppppppp)))))))

F1
F F

(FIutnicstiiomnpoE?rtacannt

to be

note that the right de ned similarly |

sides in the lines above are regular expressions.) however, the de nition is not needed explicitly in

this dissertation.

2

6.2. ITEMS AND AN ALTERNATIVE DEFINITION OF RE

149

Example 6.9 (?!E ): We present an example of only ?!E (using the informal notation for
items):
?!E ((a ") (b )) = f ?!E (D E) rule g
?!E ((a ") ) (b ) = f ?!E (E ) rule g
" (b )

Note that the result is a regular expression.

2

De nition 6.10 (Relation D, we de ne a larger binary

D): We
relation

de ne a binary relation
(called D) on items. D

D
is

on DotsE. Before de the smallest relation

ning such

that:

1. If F0; F1 2 RE, then (here we use in x notation for relation D):

" D"

(FF(((0FFF0 000()FF1FFF1)1)11)) (F0 ) F1 F0 (F1 ) (F((FF0 00))) ((((FFFF(((FFF0000 000+??)))))))++?

D D D D D D D D D D D D D D D D D

(F((FF00FF0 00())F(F1FF)1F1)11) (F0 F1) (F0 F1) ( F0) ((FF0 0)) (((((((FFFFFFF0000+??000)))))))++?

Note that the pair ( ; ) does not appear in relation D.

2. If F 2 RE and D0; D1 2 DRE such that (D0; D1) 2 D, then:

(a) (F (D0

D0; F F; D1

F )D21)D2.

D,

(D0

F; D1

F) 2 D, (F D0; F D1) 2 D, and

150 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

12

1 21 a "b Figure 6.4: The Symnodes appear as solid nodes in the tree.

(b) (D0; D1) 2 D, (D0+; D1+) 2 D, and (D0?; D1?) 2 D.

These are known as closure rules.

Note that (D0; D1) 2 D ) undot(D0) = undot(D1). (That is, all dottings related by D
have the same underlying regular expression.)
For E 2 RE, we de ne D to be the subset of D on dottings of E. More formally, de ne

D = D \ (DotsE DotsE)

Since D depends

relation, DE
The relation

is
D

a is

upon E, we sometimes write DE. Note that, although
nite one since it is a binary relation on DotsE (which called the dot movement relation.

D
is

is a

an in nite

nite set).
2

We do not present an example of relation D since one is implicitly included in Example 6.17.

De nition 6.11 (Set Symnodes): We de ne the set

SymnodesE = f e j e 2 dom(E) ^ E(e) 2 V g
That is, SymnodesE is the set of all nodes in E having labels in V .

2

Example 6.12 (Symnodes): Given our example regular expression:

Symnodes(a ") (b ) = f1 1; 2 1g
The set of symbol nodes are also shown as solid nodes in Figure 6.4.

2

De nition 6.13 called T a de ned

(Relation
as:

T ):

For

each

a

2

V,

we

de

ne

a

binary

relation

on

DotsE,

T a = f ((e; BEF ); (e; AFT)) j e 2 SymnodesE ^ E(e) = a g

6.3. A CANONICAL CONSTRUCTION

151

The T relations allow the dot to hop over symbol nodes in can also be combined into a ternary relation T DotsE V
inserted the third component, V , as the middle component).
indicate that the relation T depends upon E.

the tree E. All of the T a

SoDmotestEim(ensowteetwharitteweT

have

E

to 2

Example 6.14 (T ): Given our example regular expression, (a ") (b ), the relation T can be expressed as the two relations T a containing the single pair
((( a) ") ((b) ); ((a ) ") ((b) ))
and T b containing the single pair
(((a) ") (( b) ); ((a) ") ((b ) ))

2

6.3 A canonical construction

In this section, we present a `canonical' nite automata construction. The states of the constructed nite automaton will contain information | encoding the left and right languages of the states. In order to encode the information, each state will be an item | the dot in the item denotes a language to the left and to the right of the dot. The dot
movement relation D will be used as the "-transition relation, while the relation T will be
used as the symbol transition relation.
Using relations D and T , we can give our canonical construction as follows.

Construction 6.15 (): We de ne a canonical FA construction CA 2 RE ?! FA as

CA(E) = (DotsE; V; T E; DE; f Eg; fE g)

This construction is symmetrical.

2

This construction is also called Construction () (the empty sequence of details) since it will be used as the root of our taxonomy graph.

Remark 6.16: Notice that all states either have an in-transition on " or on a symbol in

V , but not both (and similarly with the out-transitions).

2

Example 6.17 (Construction CA): Given our example regular expression (a ") (b ),
we refer back to Example 6.7 for set of states of CA((a ") (b )). The resulting FA is shown in Figure 6.5. The states in the gure are numbered as follows:

152 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

q2 a q3

"

"

q0 " q1 "

q6 " q7 " q8 b

"

q4 "

" q5

"

q9 " q10 " q11

Figure 6.5: Automaton CA((a ") (b )).

qqq012

( ((a) (( (a) ((( a)

(")) ((b) )) (")) ((b) )) (")) ((b) ))

q3 (((a ) (")) ((b) ))

q4 (((a) ( ")) ((b) ))

q5 (((a) (" )) ((b) ))

q6 (((a) (") ) ((b) ))

q7 (((a) (")) ( (b) ))

q8 (((a) (")) (( b) ))

q9 (((a) (")) ((b ) ))

q10 (((a) (")) ((b) ))

q11 (((a) (")) ((b) ) )

2

In Example 6.17, we can see that states q0 and q11 are redundant | they can be merged

with states q1 and q10 respectively. Some other variants of Thompson's construction (see,

for example, AU92, HU79, Wood87]) take this (or other) redundancy into account and are

able to produce smaller automata for some regular expressions.
LRET(hE?e (ceo)r)reacntdne?!sLs CoAf(Eth)(ee)ca=noLniRcEal(?!cEo(nes)t)ru(ci.tei.onthfeollleofwtslafrnogmuatgheeoffacat

that L?CA(E)(e)
particular state

= |

a dotting of E | is equal to the language left of the dot, and likewise for the right

languages). For example, inspecting the automaton, fact that q9 = ((a ") (b

wcoensseideetrhtahte?!LriCgAh(tEl)a(qn9g)u=agefbogf
) )), we obtain:

state q9 of Example 6.17. . Considering the dottings

By (the

=

LRE

(?!E ((a ") (b ) f ?!E rule for g

))

=

LRE

(?!E ((b ) )) f ?!E rule for

g

6.4. "-FREE CONSTRUCTIONS

153

=

LRE

(?!E (b ) b ) f ?!E rule for

E

g

LRE (" b ) = f de nition of LRE g
fbg

For the start state, we have ?!L CA(E)( E) = LRE(?!E ( E)) = LRE(E), as desired. The
left and right language information is encoded in the states of the nite automaton as the language to the left and the language to the right of the dot.

6.4 "-free constructions

In this section, we present the rst of our "-free constructions. The part of the taxonomy

considered here is shown as the solid part in Figure 6.6. We can present the composition

rem" CA (see Transformation 2.119 for the de nition of rem") as a rst "-free construction (one which produces "-free automata). The following algorithm detail makes explicit the

fact that we will be using function rem" to produce such automata.

Algorithm
rithm detail

detail 6.18
rem-".

(rem-"):

Composing

function

rem"

onto

a

construction

is

algo2

Before presenting the composition, we note the following properties (which we will use to give the new set of states) of (Q; V; T; G; S; F) = CA(E):

For state q 2 Q, G (q) = D (q) and so G (S) = D ( E).

f G (q) j Q V fqg \ T =6 g = f D (e; AFT) j e 2 dom(E) ^ E(e) 2 V g = f D (e; AFT) j e 2 SymnodesE g.

The intuition behind the second property is: the only states in CA(E) with a non-" in-

transition are dottings of E of the form (e; AFT) where the label of node e is a symbol

in V
T .)

(that is, e is Assuming the

an de

enleitmioenntofofCSAy(mEn)oadnedsEt)h.e(cTohnitsexfotlolofwths efrloemt

the de nition of relation clause of the de nition of

rem" (page 29), we calculate the transition set (T 0 P(Q) V P(Q)) of (rem" CA)(E)

as follows:

T0
= f de nitions of CA(E) and rem" g f (q; a; D (r)) j (9 p : p 2 q : (p; a; r) 2 T ) g
= f change of bound variable: p = (e; BEF) ^ r = (e; AFT) ^ E(e) = a g f (q; a; D (e; AFT)) j (e; BEF) 2 q ^ E(e) = a ^ a 2 V g
= f de nition of SymnodesE; eliminate bound variable a g f (q; E(e); D (e; AFT)) j (e; BEF) 2 q ^ e 2 SymnodesE g

154 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

6.15

rem-" 6.19

rem-"-dual 6.63

subset

filt

pd

Ant. (6.55)6.27

use-s subset

6.83

sym sym
6.35 a-s a-s BS (6.39) 6.65

filt pd

filt use-s b-mark

p. 158 Brz. (6.57)

6.43 subset

subset use-s

e-mark subset
6.68
use-s subset

use-s

MYG (6.44)

use-s

6.85 ASU (6.86)

filt

subset subset use-s use-s filt

Wfilt

Xfilt

Figure 6.6: The constructions considered in Section 6.4 are shown as solid circles connected by solid lines. The smaller of the two graphs represents the two instantiations of the filt detail which are discussed in this section.

6.4. "-FREE CONSTRUCTIONS

155

We can now present the composite construction.

Construction 6.19 (rem-"): The composition is (rem" CA)(E) =

let

Q0 T0 F0

= = =

fD ( E)g f D (e; AFT) j f (q; E(e); D (e; AFT)) j (e; f f j f 2 Q0 ^ E 2 f g

e 2 Symnodes BEF) 2 q ^ e

E2gSymnodesE

g

in

(Q0; V; T 0; ; fD ( E)g; F 0)

end

An automaton constructed using this (composite) function has the following properties:

It has a single start state.

The single start state has no in-transitions.

All in-transitions to a state are on the same symbol (in V ).

2

This construction is sometimes known as the (nondeterministic) item set construction.
Example 6.20 (Construction (rem-")): Recalling CA((a ") (b )) from Example 6.17,
we obtain the following states for automaton (rem" CA)((a ") (b )).

( ((a) (")) ((b) ));

(( (a) (")) ((b) ));

((( a) (")) ((b) ));

(((a) ( ")) ((b) ));

q00

(((a) (" )) ((b) )); (((a) (") ) ((b) ));

(((a) (")) ( (b) ));

(((a) (")) (( b) ));

(((a) (")) ((b) ));

(((a) (")) ((b) ) )

(((a ) (")) ((b) ));

(((a) (") ) ((b) ));

q10

(((a) (")) ( (b) )); (((a) (")) (( b) ));

(((a) (")) ((b) ));

(((a) (")) ((b) ) )

(((a) (")) ((b ) ));

q20

(((a) (")) (( b) )); (((a) (")) ((b) ));

(((a) (")) ((b) ) )

156 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

a q00
b

q10 b q20

b

Figure 6.7: Automaton (rem" CA)((a ") (b ))

The structure of each state is more easily understood if we consider the FA given in
aESq2r0ixmeiasTrmielthaaphreclelheyra,se6ebsq.tu11l0el7oit(.sfibnNstygthoa"eatt-ueestestrtoathrmneoasaftaictts(hiotooaanunbtrseil)sesnfserfrhrweoooammwcsthnqaaq0tb9ie|nl)(eqFqn2f000riag=oimumsrfeteqhlqy963e;.t(qs7hte8.eh;tqaso1etf0t;iasfql,q1l1q0sg;10t)qa=.1t;eqfs2q(;3iq;n4q;6Eq;x5q;a7q;m6q;8pq;l7qe;16q0.8;1;q7q1)110gw;)qha1i1ncg2dh.

We could also present a start-reachability version of this construction. Such a construction would make use of the following algorithm detail.

aAclgoonsrtirtuhcmtiodne, ttaoilpr6o.d2u1ce(uasuet-osm):atCaowmitphosoenfluynscttairotn-rueasecfhualbsle(Tsrtaantesfso.rmation

2.116)

onto 2

Using this detail would yield function use-s). The example FA produced by

utsheisfuclos nstrreumc"tionCAwotuoldgibvee

Construction isomorphic to

(rem-", the one

produced in Example 6.20, since all states of the FA in that example are start-reachable.

(Note that it is possible for an automaton to have start-unreachable states. As an example,

consider the automaton corresponding to the regular expression a. This is left to the

reader.)

Alternatively, we could apply the subset construction as well as start-reachability. The

use of the subset construction is given by the following detail.

Algorithm detail 6.22 (subset): Compose the subset construction (function subset |

Transformation 2.121) onto a construction, to produce a DFA.

2

This would yield (rem-", subset,

ucosme-pso)s. itTehfius nlcatsitoncounssetrfuulcstionsuibssketnowrnema"s

CA and give Construction the `(deterministic) item set

construction'.

Example 6.23 (Construction (rem-", subset, use-s)): Recall the FA produced in

Example 6.20. That FA also happens to be a DFA. The composition rem" CA produces a similar DFA, with a sink state. We do not give detail. The resulting DFA is shown in Figure 6.8.

tohf eussteafutel ssetsuhbesreetin 2

6.4. "-FREE CONSTRUCTIONS

157

a q0
b

q1

b

a a

q3

q2

b

a; b

Figure 6.8: Automaton (useful s subset rem" CA)((a ") (b )).

6.4.1 Filters

In Construction 6.19, states are sets of items (of the regular expression E). Given state

q, when the successor states to q are being constructed, the only information in q that is

used are those items (e; of transition function T 0

BEF ) in the

2q let

where clause

e 2 SymnodesE. This follows
of that construction. In other

from the de nition words, for any two

states p and q such that p \ (SymnodesE fBEF g) = q \ (SymnodesE fBEFg), p and q

will have the same out-transitions.

The information that is used to determine if q is a nal state is the predicate E 2 q.

So, if the above condition holds on p and q, and E 2 p E 2 q, then the two states

will be indistinguishable | that is, their right languages will be the same, and the we can

use this information to identify the two states. We can therefore use a lter to remove

redundant information from the item sets p and q, allowing them to be identi ed. We can

now de ne such a lter.

De nition 6.24 (Item set lter W): Given the above discussion, we de ne the lter function W on sets of items

W(u) = u \ ((SymnodesE fBEF g) fE g) This lter was called Y when it was rst introduced in Wat93a].

2

We will show later that there are other possible lters. The use of a lter is known as algorithm detail filt.

Algorithm detail 6.25 (filt): Usage of an item set lter, such as W.

2

This detail is used to indicate that a lter is used. We will also de ne an algorithm detail for each of the actual lters; the sequence of details describing a construction will contain the name of the actual lter used in place of detail filt. The use of our rst lter is given by the following algorithm detail.

Algorithm detail 6.26 (Wfilt): Usage of lter W is detail Wfilt.

2

158 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

q0 a; b q1

b

Figure 6.9: FA produced by Construction (rem-", Wfilt)

This results in the following construction:

Construction 6.27 (rem-", Wfilt): The construction using the lter is:

let

Q0 T0 F0

= = =

fW(D ( E))g f (q; E(e); W(D f f j f 2 Q0 ^ E

f W(D
(e; AFT
2fg

(e; )))

AFT)) j e j (e; BEF)

2 2

Sqy^mneo2deSsyEmgnodes

E

g

in

(Q0; V; T0; ; fW(D ( E))g; F 0)

end

2

The following example shows the type of improvement that lter W can make.

Example 6.28 (Construction (rem-", Wfilt)): To show the operation of lter W, we
recall the set of states from Example 6.20. We obtain the following ltered states (where the state numbers are taken from Example 6.20):

W W W

(((qqq012000 )))

= = =

f((( a) f(((a) f(((a)

(")) ((b) )); (((a) (")) (( b) )); (((a) (")) (( b) )); (((a)

(")) (( b) )); (((a)
(")) ((b) ) )g (")) ((b) ) )g

(")) ((b) ) )g

From this, we can Wfilt), becoming

ssteaetethq1atinsttahteestwqo10

and state

qF20Aa(rwe hiidcehnitsi

ed also

under Construction (rem-", a DFA) shown in Figure 6.9.
2

Filters are also interesting (as the following example shows) in the case where the subset construction is used. An example of this will be given in Section 6.9, where we present an imperative program implementing Construction (rem-", subset, use-s, Wfilt). That construction was presented as Wat93a, Constr. 5.82].

Example 6.29 (Construction (rem-", subset, use-s, Wfilt)): Construction (rem-
", subset, use-s, Wfilt) produces a DFA which is identical to the one given in Example 6.28, with the addition of a sink state. The resulting DFA is shown in Figure 6.10. 2

6.5. ENCODING CONSTRUCTION (REM-")

159

q0 a; b q1

b

a

q2 a; b

Figure 6.10: DFA produced by Construction (rem-", subset, use-s, Wfilt).

Discarding more information than W already discards can be dangerous, since states
could then be identi ed which should not be identi ed. Of course, we could also devise
lters which discard less information than W. Such a lter would be called a safe lter.
One particular safe lter is of historical interest.

De nition 6.30 (DeRemer's lter): Filter X removes ( lters out) the following types

of items: any item containing a subexpression of the form (E F), (E ), or (E ) .

Clearly, this lter discards less than the ideal lter W.

2

The use of this lter is given by the following algorithm detail.

Algorithm detail 6.31 (Xfilt): Usage of lter X.

2

Use of this lter would yield Construction (rem-", subset, use-s, Xfilt). This construction was originally given by DeRemer in DeRe74], where an LR parsing algorithm was
modi ed for (compiler) lexical analysis. DeRemer attributes the de nition of X to Earley
Earl70]. Although this construction has been largely ignored in the literature, it was the
motivation for deriving the W lter and eventually the entire taxonomy presented in this
chapter. An example of the use of this construction follows.

Example 6.32 (Construction (rem-", subset, use-s, Xfilt)): The DFA resulting

from this construction is isomorphic to the one given in Example 6.23. This example

shows that lter W is frequently more e ective (and never less e ective) than lter X. See

Chapter 14 for data on the e ectiveness of the two lters in practice.

2

6.5 Encoding Construction (rem-")
While the use of lters can make the constructions more e cient in practice, they still produce automata whose states are sets of items. In this section, we explore even more compact representations of the states of the automata. The solid part of the graph in Figure 6.11 indicates the subpart of the taxonomy which is discussed in this section.

160 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

6.15

rem-" 6.19

rem-"-dual 6.63

subset

filt

pd

Ant. (6.55)6.27

use-s subset

6.83

sym sym
6.35 a-s a-s BS (6.39) 6.65

filt pd

filt use-s b-mark

p. 158 Brz. (6.57)

6.43 subset

subset use-s

e-mark subset
6.68
use-s subset

use-s

MYG (6.44)

use-s

6.85 ASU (6.86)

filt

subset subset use-s use-s filt

Wfilt

Xfilt

Figure 6.11: The constructions considered in Section 6.5 are shown as solid circles connected by solid lines.

6.5. ENCODING CONSTRUCTION (REM-")

161

We proceed by trying to characterize the set of states in Construction 6.19. Obviously, we can characterize each element of the set

f D (e; AFT) j e 2 SymnodesE g

by

an element in SymnodesE. To characterize the remaining

state

D

(

E)

in

Construction

6.19,

we

introduce

a

new

state s in the let clause. We now turn to encoding the set of nal states. In the unencoded

construction, a state (item set) f is nal if and only if E 2 f. For an encoded state

e2
the

SymnodesE, this is equivalent to E 2
unencoded construction) is nal if and

D (e; AFT). The remaining only if E 2 D ( E).

state

(D

(

E)

in

In providing an encoded version of the transition function, we take advantage of the

fact that the single start state in Construction 6.19 never has an in-transition. We divide

the de nition of the transition function into two pieces. The out-transitions from the start

state (a new state s, encoding the original start state D ( E)) are

f (s; E(e); e) j (e; BEF) 2 D ( E) ^ e 2 SymnodesE g

while the remaining transitions are

f (f; E(e); e) j (e; BEF) 2 D (f; AFT) ^ e 2 SymnodesE ^ f 2 SymnodesE g

The encoding of some of the states by elements of SymnodesE constitutes the following algorithm detail.

Algorithm detail 6.33 (sym): States are encoded by elements of SymnodesE. 2

Remark 6.34: This encoding is similar to the LR parsing technique of encoding sets of

items by the kernel items Knut65]. The closure operation can then be used to recover the

set from the kernel items.

2

Construction 6.35 (rem-", sym): Assuming E 2 RE, the encoded automaton construc-

tion is:

let s be a new state

in

let

Q T F

= = =

fsg f (s;

ES(ey)m; en)odj e(se;EBEF

)

2

D

(

E)

^

e

f (f; E(e); e) j (e; BEF) 2 D (f; AFT

)2^Sye;mfn2odSesyEmgnodes

fe if

jE
E

2D 2D (

(e; AFT)
E) then

^ e 2 Symnodes fsg else

E

g

E

g

in

(Q; V; T; ; fsg; F)

end

end

For any given E 2 RE, the FA produced by the above construction is isomorphic to the

one produced by Construction 6.19.

2

162 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

When elements of SymnodesE are used as states, they are frequently called `positions' in the literature ASU86, BS86, B-K93a, Chan92, CP92, tEvG93, MY60]. They can be encoded as integers, with a single traversal of the tree E being used to assign the integers to each node labeled by an element of V . Such an encoding is discussed in Chapter 10.
The above construction can still be made more e cient through the introduction of some auxiliary sets.

De nition 6.36 (Sets Follow; First; Last): Given E 2 RE, relation FollowE is a binary
relation on SymnodesE:
Follow E = f (e; f) j ((e; AFT); (f; BEF)) 2 D ^ e; f 2 SymnodesE g

Sets FirstE; LastE SymnodesE are de ned as:
FirstE = f e j (e; BEF) 2 D ( E) ^ e 2 SymnodesE g

and

LastE = f e j E 2 D (e; AFT) ^ e 2 SymnodesE g

Note that Follow ; First, and Last can also be viewed as functions with domain RE since

they depend upon the regular expression E.

2

In Section 6.8, we will present examples of these sets and methods for computing them.
In addition to making use of the auxiliary sets, we will also use the fact that E 2 D ( E) " 2 LRE(E). For brevity, we de ne a predicate on regular expressions. De nition 6.37 (Predicate Null): For E 2 RE, we de ne Null(E) " 2 LRE(E). 2

Algorithm detail 6.38 (a-s): The use of auxiliary
predicate Null constitute algorithm detail a-s.

sets

Follow E; FirstE; LastE,

and 2

Construction 6.39 (rem-", sym, a-s): Assuming E 2 RE, we use the auxiliary sets to
produce the FA:

let s be a new state

in

let in

Q T F

= = =

ffLfsa((gssq;t;EEES((eye)im);f;enNe)o)udjjleeles(2EE2F)FitrohsltleoEnwgEfs(qg)e^lsqe

2

Symnodes

E

g

(Q; V; T; ; fsg; F)

end

end

This construction is the Berry-Sethi construction BS86]. An alternative derivation of this

construction is given in the original taxonomy as Wat93a, Constr. 4.32].

2

6.5. ENCODING CONSTRUCTION (REM-")

163

a q0
b

q1 b q2

b

Figure 6.12: FA produced by Construction (rem-", sym, a-s).

The following is a short history of the algorithm.

Remark 6.40: The history of this algorithm is somewhat complicated. The following ac-

count is given by Bruggemann-Klein B-K93b]. Glushkov and McNaughton and Yamada si-

multaneously (and independently) discovered the same DFA construction Glus61, MY60].

Those papers used the same underlying FA construction to which they apply the subset

construction1. Unfortunately, neither of them present the construction without the sub-

set construction explicitly. The underlying FA construction was presented in some depth

(with correctness arguments) by Berry and Sethi in BS86, Alg. 4.4]. In their paper, Berry

and Sethi also relate the construction to the Brzozowski construction. In this chapter,

we adopt the convention that the FA construction (without subset construction) is named

after Berry and Sethi, while the construction with the subset construction is named after

McNaughton, Yamada, and Glushkov.

2

Example 6.41 (Construction (rem-", sym, a-s)): The computation of the auxiliary
sets is not discussed here | see Section 6.8. The resulting FA is shown in Figure 6.12. We brie y mention the state set: start state s is represented in the gure by q0, 1 1 is represented by q1, and 2 1 by q2. Note that the FA is always isomorphic to the one given in Figure 6.7 from Example 6.20 since we have only given an encoding of Construction 6.19.
2

We the

could above

aclosnoshtrauvcetiporne.seTnhteedaalgnorailtghomritwhomulidmbpeleCmoennsttinrugcftuionnct(ironemus-e"f,uslys mco,ma-pso,suedsew-sit)h.

In Section 6.8, we will discuss how to compute the auxiliary sets given in De nition 6.36.

In the following section, we consider another coding trick that proves to be particularly

useful in the preceding construction.

6.5.1 Using begin-markers
In this section, we examine a method of making Construction 6.39 more e cient. One place to improve the e ciency of an implementation of the construction, is to treat state
1The underlying construction may actually produce a nondeterministic nite automata.

164 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

s (the start state) the same as the other states. As the construction is now written, s is treated in special cases of the de nitions of the transition function and the set of nal states.
Fortunately, we can achieve this goal by concatenating a single symbol to the left of regular expression E. For example, we concatenate the symbol $ to give a new regular expression $ E. In this case, node 1 is the $ node, while any node q in E is now refered to as 2 q (this is known as pushing down the node q). Indeed, this gives us an encoding of
all of the states in Construction 6.39. Any state q (where q 2 fsg SymnodesE, the state
set of that construction) is encoded as an element of Symnodes$ E by
if q = s then 1 else 2 q

In our encoded construction, we can now simply use the state set Symnodes$ E. We also need to provide encoded versions of the transition relation, and the nal state set. In some
of the following paragraphs, we refer to the context of the inner let clause of Construc-
tion 6.39. First, we consider the part of the transition relation that relates to state s:

f (s; E(e); e) j e 2 FirstE g

Under the new encoding, we note that s (which will be encoded as 1) will have transitions to the encoded states 2 FirstE. Furthermore, we note that 2 FirstE = Follow $ E(1). (This is easily seen by the inductive de nitions presented in Section 6.8.) We obtain the following to the encoded part of the transition relation for start state s:

f (1; ($ E)(e); e) j e 2 Follow $ E(1) g

The other part of the transition relation remains almost the same as before:

f (f; ($ E)(e); e) j e 2 Follow $ E(f) g

which is identical for the part of the relation involving the start state. We can therefore use the single expression above for the entire transition relation. We can also give an encoded version of the set of nal states:

(2 LastE) if Null(E) then f1g else

Since 2 LastE = Last$ E n f1g (from the de nition of Last) and Null (E) 1 2 Last$ E,
we can rewrite the above expression as

Last$ E n f1g if 1 2 Last$ E then f1g else

Some simpli cation yields Last$ E. The use of this encoding is given in the following algorithm detail:

Algorithm detail 6.42 (b-mark): By prepending a symbol ($ in our case) to the regular

expression E, and using the above encoding, we improve Construction 6.39.

2

6.5. ENCODING CONSTRUCTION (REM-")

165

Note that (contrary to popular belief) it does not matter which particular symbol is chosen for the begin-marker. It is common to choose a special symbol so that it is obviously (to the reader) a begin-marker.
This gives us the following construction:

Construction 6.43 (rem-", sym, a-s, b-mark): Assuming E 2 RE, construct an FA

as follows:

let in

Q T F

= = =

fSy(fm;n($odeEs$)(Ee);
Last$ E

e)

j

e

2

F

ollow$

E(f

)

g

(Q; V; T; ; f1g; F)

end

The use of a begin-marker is a rather well-known encoding trick. This particular algorithm

appears in the literature as Wat93a, Constr. 4.38].

2

Since this construction produces FAs which are isomorphic to those produced by Construction (rem-", sym, a-s), we do not give an example here. We will see in Section 6.9 how the use of a begin-marker can greatly simplify a construction implementation.

6.5.2 Composing the subset construction

We can section,

also compose Constructions

function (rem-",

usysemfu,las -s)suabnsdet

with the two (rem-", sym,

main constructions of a-s, b-mark). One of

this the

major constructions is the McNaughton-Yamada-Glushkov construction.

Construction 6.44 (McNaughton-Yamada-Glushkov): Construction (rem-", sym,
a-s, subset, use-s) is the McNaughton-Yamada-Glushkov construction. McNaughton and Yamada originally presented the algorithm in MY60] while Glushkov independently derives the algorithm in Glus61]. See Remark 6.40 for a brief history of this construction.
2

An imperative algorithm implementing this construction is given in Algorithm 6.84. We present a brief example of an automaton produced by the construction.

Example 6.45 (McNaughton-Yamada-Glushkov): The DFA (with sink state) pro-

duced for regular expression (a ") (b ) is shown in Figure 6.13. Note that it is isomorphic

to the one shown in Figure 6.8 from Example 6.23.

2

The other resulting construction is (rem-", sym, a-s, b-mark, subset, use-s), which does not appear in the literature. It is, however, the dual of the Aho-Sethi-Ullman construction (which appears as Construction 6.69 in this chapter). An algorithm implementing it appears as Algorithm 6.85, where it is shown to be more elegant and concise than the McNaughton-Yamada-Glushkov construction. An example DFA produced by this construction would be isomorphic to the one given in the preceding example.

166 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

a q0
b

q1

b

a a

q3

q2

b

a; b

Figure 6.13: DFA produced by the McNaughton-Yamada-Glushkov construction.

6.6 An encoding using derivatives

In this section, we consider an alternative encoding of Construction (rem-") in which the states are regular expressions. The solid part of the graph in Figure 6.14 indicates the subpart of the taxonomy which is discussed in this section. For brevity, the new constructions presented in this section will be derived in a relatively informal manner.
Recall that we used the following state set in Construction (rem-")

Q0 = fD ( E)g f D (e; AFT) j e 2 SymnodesE g

In Section 6.5, we elected to use the set SymnodesE to encode the set appearing as the second operand of the operator above. That left us with the problem of encoding the remaining state. In this section, we encode each state q by the (unique) item d such that
q = D (d). The above state set would be encoded as:

f Eg f (e; AFT) j e 2 SymnodesE g

(Note that the item E could also have been written as (E; 0; BEF ). We will freely mix the two item notations, choosing the most appropriate one for a given application.) Given this, we can provide the following (encoded) version of Construction (rem-").

Construction 6.46 (Encoding (rem-")): Assuming regular expression E, the following automaton accepts LRE(E):

let

Q T F

= = =

f f f

Eg f (e; AFT) j e (q; E(e); (e; AFT)) j fjf2Q^E 2D

(2e;SByEmFn)od2esDE (f) g

g
(q)

^

e

2

Symnodes

E

g

in

(Q; V; T; ; f Eg; F)

end

2

6.6. AN ENCODING USING DERIVATIVES

167

6.15

rem-" 6.19

rem-"-dual 6.63

subset

filt

pd

Ant. (6.55)6.27

use-s subset

6.83

sym sym
6.35 a-s a-s BS (6.39) 6.65

filt pd

filt use-s b-mark

p. 158 Brz. (6.57)

6.43 subset

subset use-s

e-mark subset
6.68
use-s subset

use-s

MYG (6.44)

use-s

6.85 ASU (6.86)

filt

subset subset use-s use-s filt

Wfilt

Xfilt

Figure 6.14: The constructions considered in Section 6.6 are shown as solid circles connected by solid lines.

168 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

Example 6.47 (Alternative encoding of states): Given our example regular expres-

sion, (a ") b , the three states are Q = f ((a ") b ); (a ") b ; (a ") (b ) g. The

automaton is isomorphic to the one given in Example 6.20.

2

We will now consider a further encoding of the set of states: using function ?!E to map
each state (item) to a regular expression. In order to facilitate this, we write the transitions
using the signature T 2 Q V ?! P(Q) instead of T as a relation over Q V Q. T
can be rewritten, using this new signature, as:

T(q; a) = f (e; AFT) j (e; BEF) 2 D (q) ^ E(e) = a g

Note that we also have the property T(q; a) = T a(D (q)). The change of representation
(to regular expressions for states) will be easier with a de nition of the transition function which is inductive on the structure of items. The following de nition provides such an inductive transition function.

De nition 6.48 (Transition function t): De ne transition function t 2 DRE V ?!

P(DRE) such that
by induction on the

t(d; a) = T a(D (d)). The de
structure of DREs (assuming

nition (which is
E; E0; E1 2 RE

given and a

without
2 V ):

proof)

is

t((E; 0; AFT); a) =

t(("; 0; BEF); a) =

t(( ; 0; BEF ); a) =

t((b; 0; BEF); a) = if a = b then f(b; 0; AFT)g else

t((E0

E1; 0; BEF ); a) =

f

(Ef (0E0E1E; 11;

v; 2

p) v;

pj )(Ej (0E; v1;;pv);

2
p)

t2((Et(0(E; 01;;B0;EBFE)F; a));

g
a)

g

t((E0

E1; 0; BEF ); a)

=

fi(fE0 then else

E1; 1 v; p) j

f(E(E0;00;

AFT
E1; 2

)(E2v0;;Dpv);(jpE)(E02;10;t;(vB(;EEp)0F;20);tB((EEF1;);0a; B) gEF

);

a)

g

t((E ; 0; BEF); a) = f (E ; 1 v; p) j (E; v; p) 2 t(E; 0; BEF) g t((E+; 0; BEF); a) = f (E+; 1 v; p) j (E; v; p) 2 t(E; 0; BEF) g t((E?; 0; BEF); a) = f (E?; 1 v; p) j (E; v; p) 2 t(E; 0; BEF) g t((E0 E1; 1 w; p); a) = f (E0 E1; 1 v; p0) j (E0; v; p0) 2 t((E0; w; p); a) g t((E0 E1; 2 w; p); a) = f (E0 E1; 2 v; p0) j (E1; v; p0) 2 t((E1; w; p); a) g

6.6. AN ENCODING USING DERIVATIVES

169

t((E0

E1; 1

w; p); a)

=

fi(fE0 then else

Ef(1E;(1E0;00;vAE; pF10;)T2j)(E2v;0D;pv0);(jpE0()0E;2w1;;tv(p;()Ep0)0;2w;t(p()E; a1); 0g;

BEF

);

a)

g

t((E0 E1; 2 w; p); a) = f (E0 E1; 2 v; p0) j (E1; v; p0) 2 t((E1; w; p); a) g t((E ; 1 w; p); a) = f (E ; 1 v; p0) j (E; v; p0) 2 t((E; w; p); a) g
if (E; 0; AFT) 2 D (E; w; p) then f (E ; 1 v; p0) j (E; v; p0) 2 t((E; 0; BEF); a) g else

t((E+; 1 w; p); a) = f (E+; 1 v; p0) j (E; v; p0) 2 t((E; w; p); a) g if (E; 0; AFT) 2 D (E; w; p) then f (E+; 1 v; p0) j (E; v; p0) 2 t((E; 0; BEF); a) g else

t((E?; 1 w; p); a) = f (E?; 1 v; p0) j (E; v; p0) 2 t((E; w; p); a) g
Note that the transition relation induced by t is in nite. In our construction, we would only use that portion of t which applies to DotsE V (for our regular expression E). 2 Using the function t, we can de ne the following construction:
let Q = f Eg f (e; AFT) j e 2 SymnodesE g
T(q; a) = t(q; a)
F = f f j f 2 Q ^ E 2 D (f) g in
(Q; V; T; ; f Eg; F) end
Example 6.49 (Function t): As an example of the use of function t, consider the out-
transitions from (encoded) state ((a ") b ). Some lengthy calculations show that
t( ((a ") b ); a) = f((a ) ") b g t( ((a ") b ); b) = f(a ") (b ) g
t(((a ) ") b ; a) = t((a ") (b ) ; a) =
t(((a ) ") b ; b) = t((a ") (b ) ; b)
= f(a ") (b ) g 2

170 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

q q0

b6=yAstqho1euwrrhengieluxelta?!Eerne(cqxo0pd)rien=sgsis?o!Etne(pq?!,E1w)(.eq)rF.eoprTrtehusinesantmteelayay,chisdionefcntethifteyhsettwartoeegssut(laiantretsehxespipnrrceeesvsiiiotonuiss?!Ecpoo(nsqss)tibrduleecnttioohtnae)ts

the right language of state q, only states with the same right language will be identi ed.

(This is one of the cases in which identifying states is safe | it does not alter the language

accepted by the automaton.) This encoding can therefore be considered an optimization

to the previous construction.

Encoding the transitions of Construction 6.46 yields

f (?!E (q0); a; ?!E (q1)) j (q0; a; q1) 2 T g

In this new automaton, the states will be elements of RE. In a manner analogous to our use of the general (in nite) transition function t, we de ne a general transition
tfu0 n2c?t!Eio(nD(RwEh)ich Vals?o!inPdu(c?!Ees(DanREin))n(iwteherreela?t!Eio(nD)RoEn)RisEt.heTihmeangeewoftDraRnEsitiuonndefurn?!Ect)io, nis,
de ned as follows (in terms of function t):

t0(?!E (d); a) = ?!E (t(d; a))

We do this for all of the lines in the inductive de nition of t (De nition 6.48). For example,

cthonesdideernitthioenlionfetf0(o?r!Ei(tEem0 s

of the form E1; 0; BEF );

(E0 a) =

E1; 0; BEF ).

We

obtain

the

following

line

in

?!E (fif(E0 then else

E?(!EE1;(01f; 0(;EvA0;FpT)Ej)1(;2E2 0D;vv(;;Epp))0;j20(;EtB(1(E;EvF0;;)p0);

BEF); a) g) 2 t((E1; 0; BEF

);

a)

g)

Recall Using

(from page 162) this equivalence,

that (E0; 0; and moving

A?!EFTin)to2thDe

(E0; 0; BEF sets, yields

)

" 2 LRE (E0)

Null (E0).

f

?!Ei(fE0 then else

E1; 1 v; p) j (E0; fN?u!Ell((EE00) E1; 2

v; v;

p) p)

2 t((E0; 0; j (E1; v; p)

BEF); a) g 2 t((E1; 0; BEF

);

a)

g

6.6. AN ENCODING USING DERIVATIVES

Rewriting the sets (using the de nition of ?!E ) gives

f

?!Eeti(fhlEsee0n; v;fNp)?u!EllE((EE110j;)(vE; p0); vj;(pE)12; vt;(p(E) 20;

0; BEF); a) g
t((E1; 0; BEF

);

a)

g

171

Using the de nition of t gives

?!E (ti(f(E0; then else

0; BEF ); a))
?N!Eu(ltl(((EE01); 0;

E1
BEF

);

a))

The above derivation can then be simpli ed (using the de nition of t0) to

t0(E0 E1; a) = t0(E0; a) E1 if Null (E0) then t0(E1; a) else

Had we considered all of the other lines in the de nition of t, we would see that a number of them are redundant when rewritten for the de nition of t0 (they are subsumed by the some of the other lines). This rewriting process is not given in full here; instead, we immediately present the de nition of a function which satis es the requirement on t0 (though it has a slightly larger signature).

De nition 6.50 (Function @): We de ne function @ 2 RE V ?! P(RE). (For
historical reasons, we write @a(E) instead of @(E; a); this is intended to signify that @a(E) is the partial derivative of E with respect to a.) The de nition is by structural induction on regular expressions:

@a( ) =

@a(") @@@@@@aaaaaa((((((bEEEEE)00+?)))EE11))

= = = = = = =

if a = b then f"g else

(b 2 V )

@@@@aaaa((((EEEE00))))

EE@1a(Ei1f)Null
E

(E0)

then

@a(E1)

else

@a(E)

This function slightly larger

csionncteai?!nEs(tD0 R(wEh)en

viewed RE .

as

a

transition

relation),

though

the

signature

is 2

The de nition given here corresponds exactly to the one given by Antimirov in Anti94, Anti95], though his de nitions are derived in a more language-theoretic manner.

172 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

De nition 6.51 (Partial derivatives): For regular expression E, we de ne PDE to be
the image of

f Eg f (e; AFT) j e 2 SymnodesE g under function ?!E . That is (following the de nition of ?!E )
PDE = fEg ?!E (f (e; AFT ) j e 2 SymnodesE g)
Elements of the set PDE are the partial derivatives of E.

2

Example 6.52 (Partial derivatives): For our example regular expression, (a ") b ,
we have

PD(a ") b = f(a ") b ; " b g

2

Remark 6.53: Our de nition of partial derivatives corresponds very closely to the one

given by Antimirov in Anti94, Anti95]. There is, however, one di erence: start-reachability

is built into Antimirov's de nition. For example, given regular expression a, Antimirov's

de nition would yield the singleton set f ag for the partial derivatives, while our de nition

is PD a = f a; "g.

2

The use of partial derivatives is given by the following algorithm detail:

Algorithm detail 6.54 (pd): Partial derivatives (and transition function @) are used to

encode the automaton in Construction 6.46.

2

E" n2coLdRiEn(g?!Eth(ef

nal ))

sNtautlle(s?!Eof(Cf o))n.struction

6.46

is

easily

done

by

noticing

that

E

2 D (f)

Armed with this, we are nally in a position to give Antimirov's construction Anti94,

Anti95].

Construction 6.55 (rem-", pd): Given E 2 RE, we construct automaton

let in

T F

(E0; a)
= ff

=
jf

@2a(PED0)E

^

Null

(f

)

g

end (PDE; V; T; ; fEg; F)

2

Antimirov's papers contain much more information on the correctness of this construction as well as some examples in which the construction produces automata that are much smaller than those produced by Construction (rem-"). Antimirov derives this construction in a much more language theoretic way.

6.6. AN ENCODING USING DERIVATIVES

q0 a; b q1

b

Figure 6.15: FA produced by Antimirov's construction.

173

q0 a; b q1

b

a

q2 a; b

Figure 6.16: FA produced by Brzozowski's construction.

Example 6.56 (Antimirov's construction): Using our example expression, (a ") b ,

yields the FA shown in Figure 6.15. The state set is PD(a ") b , which we encode as

q0 = (a ") b and q1 = " in Example 6.28.

b

in the

gure. Interestingly, this FA is isomorphic to the one 2

We can compose the subset construction (with start-unreachable state removal) to Antimirov's construction. This yields Construction (rem-", pd, subset, use-s) | a variant of Brzozowski's construction.

Construction 6.57 (rem-",
onto Antimirov's construction

pd, subset,
is a variant of

use-s): The
Brzozowski's

composition construction.

of It

useful is not

spresseunbtseedt

explicitly here.

2

This corresponds almost exactly to Brzozowski's construction, as presented in Brzo64]. In the original version, no sink state is constructed. Although we have derived it from Antimirov's relatively new construction, it was in fact one of the rst DFA constructions to be developed, in the 1960s. In Anti94], Antimirov also derives Brzozowski's construction in this manner.

Example 6.58 (Brzozowski's construction): Using our example expression, (a ") b ,

yields the FA shown in Figure 6.16. In the original version, no sink state would have been

constructed. This DFA is isomorphic to the one in Example 6.29.

2

174 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

In our version of Brzozowski's construction, we have states which are elements of
P(PDE) and a transition function T 0 2 P(PDE) V ?! P(PDE) de ned as

T 0(p; a) = ( p0 : p0 2 p : @a(p0))

A possible further encoding of the resulting DFA is to represent each state2 fE0; : : : ; Ekg

(where each Ek is a partial derivative of E) by a single regular expression E0

Ek.

Unfortunately, this representation is not unique | we could have chosen a di erent order

to arrange the elements of the set of partial derivatives. In our variant of Brzozowski's

construction, the associativity, commutativity and idempotence laws of set union would

have allowed us to recognize that fE0; E1g, fE1; E0g and fE0; E1; E0g are the same state.

In the new representation, we would have the states (each of which is a regular expression)

E0 E1, E1 E0, and E0 E1 E0, which would are all syntactically di erent.

In order to recognize these regular expressions as denoting the same state, we de ne

the following equivalence relation on states.

De nition 6.59 (Similarity): Two regular expressions, E and F, are similar (written

E F ) if one can be transformed into the other using the rules E0 (E1 E2) (E0

E1) E2 (associativity), E0 E1 is an equivalence relation.

E1 E0 (commutativity), and E0 E0

E0.

Note

that 2

The states are now elements of RE] . Using similarity, the transition function would be de ned as

T ( F ] ; a) = da(F )]

where da(F ) is the (full, as opposed to partial) derivative of F with respect to a, as de ned below.

De nition 6.60 (Full derivatives): We de ne function d 2 RE V ?! RE. (For
historical reasons, we write da(E) instead of d(E; a).) The de nition is by structural induction on regular expressions:

da( ) =

da(") da(b)

=
= if a = b then " else

(b 2 V )

da(E0 da(E0

E1) E1)

= =

da(E0) da(E0)

Ed1a(Ei1f)Null (E0) then da(E1) else

ddaa((EE+))

= =

ddaa((EE))

E E

da(E?) = da(E)

Note that the right sides in the above de nition are regular expressions. This de nition

corresponds closely to the one given by Brzozowski in Brzo64].

2

2Recall that each of the DFA states is a set of states in Antimirov's construction.

6.7. THE DUAL CONSTRUCTIONS

175

The use of derivatives and similarity yields the classically presented Brzozowski construction.

Remark 6.61: In the FIRE Engine, similarity is not used. Instead, a total ordering on

regular expressions is de ned. The nodes in the full derivatives are then rotated so that,

for full derivative (E0 This yields a similarity

E1) E2, we normal form,

have E0 < E1 < E2 (where which encodes the rules of

< .

is

the

total

ordering). 2

6.7 The dual constructions

In some cases, the dual of a construction may be more e cient in practice. The correctness
of the dual of a construction can be seen as follows: assume construction f 2 RE ?! FA sRuc)h(Et)h)a=t LLFFAA((ff((EE)R))=)RL=REL(REE)(;EcRo)nRsi=derLRthEe(Edu).al of f, R f R; we have LFA((R f
In this section, we derive some of the more interesting dual constructions. In Figure 6.17, the solid part of the graph indicates the subpart of the taxonomy which is discussed in this section. Note that some of the dual constructions are not considered in this taxonomy at all (the dashed-line subpart of the taxonomy). We omit them since it appears di cult make them more e cient than their duals (which have already been considered).
Given the de nition of the dual of function rem", we can present the composition R rem" R CA.

Algorithm detail 6.62 (rem-"-dual): The use of composite function R
(Transformation 2.120) is detail rem-"-dual.

rem"

R 2

The use of this detail gives the following construction.

Construction 6.63 (rem-"-dual): The composition is (R rem" R CA)(E) =

let

Q0 T0 S0

= = =

f(DR) (E )g f (DR) (e; f ((DR) (e; BEF); E(e); q) f s j s 2 Q0 ^ E 2 s g

BEF) j e 2 j (e; AFT)

2Syqm^noed2esSEygmnodesE

g

in

(Q0; V; T 0; ; S0; f(DR) (E )g)

end

An automaton constructed using this (composite) function has the following properties:

It has a single nal state.

The single nal state has no out-transitions.

All out-transitions from a given state are on the same symbol (in V ). This follows from the duality between this construction and Construction (rem-"). 2

176 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

6.15

rem-" 6.19

rem-"-dual 6.63

subset

filt

pd

Ant. (6.55)6.27

use-s subset

6.83

sym sym
6.35 a-s a-s BS (6.39) 6.65

filt pd

filt use-s b-mark

p. 158 Brz. (6.57)

6.43 subset

subset use-s

e-mark subset
6.68
use-s subset

use-s

MYG (6.44)

use-s

6.85 ASU (6.86)

filt

subset subset use-s use-s filt

Wfilt

Xfilt

Figure 6.17: The constructions considered in Section 6.7 are shown as solid circles connected by solid lines.

6.7. THE DUAL CONSTRUCTIONS

177

Example 6.64 (Construction (rem-"-dual)): Recalling our example regular expres-
sion, (a ") (b ), we obtain the following item sets for states:

( ((a) (")) ((b) ));

(( (a) (")) ((b) ));

(((a ) (")) ((b) ));

(((a) ( ")) ((b) ));

q000

(((a) (" )) ((b) )); (((a) (") ) ((b) ));

(((a) (")) ( (b) ));

(((a) (")) ((b ) ));

(((a) (")) ((b) ));

(((a) (")) ((b) ) )

( ((a) (")) ((b) ));

(( (a) (")) ((b) ));

(((a ) (")) ((b) ));

(((a) ( ")) ((b) ));

q100

(((a) (" )) ((b) )); (((a) (") ) ((b) ));

(((a) (")) ( (b) ));

(((a) (")) (( b) ));

(((a) (")) ((b ) ))

( ((a) (")) ((b) ));

q200

(( (a) (")) ((b) )); ((( a) (")) ((b) ))

As in Example 6.20, the structure of each state is more easily understood if we con-

Esfqfri8qxdo0(mae;Ntrmqh1oqatp;t2htqleee3(itq;s6h2Fq,00.4aA1q=;t710q0)a5gf=;liqwlvq0f6the;;qhnqiq0cr1;7he;i;qneqq1a2;s9grEtq;e)a3qx.;t1rae0qem;s4vq;epa1qrr1l5seeg;e.qs66rt.S;ea1qair7m7ct.;hisqlat8aTba;rlthqleye9e,sg()qbsa10iyn0andnig"sdl-oettnqrha2e0e0nnoissasfeilttttihshooteneafmssts)eettiafsrqtoo00ef0msnsiastrqael.t1vth1eeTser|hsreseeevnrrteeeaarsomsucfehletsalriytebnaalgttechehFsefaArob(simelinest

shown in Figure 6.18.

2

Just as we introduced Algorithm detail (sym) into Construction (rem-") to encode most of the set of states by an element of SymnodesE, we can do the same with the above construction, to give Construction (rem-"-dual, sym) | which is not given here. (Since the introduction of detail (sym) is only an encoding, Construction (rem-"-dual, sym) is the dual of Construction (rem-", sym).) It turns out that the same auxiliary sets (Follow E, FirstE, and LastE) can also be used to improve this construction. This results

178 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

b
q000 a

b q100 a q200

Figure 6.18: FA produced by Construction (rem-"-dual)

in the following construction (using Algorithm detail a-s).

Construction 6.65 (rem-"-dual, sym, a-s): Assuming E 2 RE, we use the auxiliary
sets in the following construction:

let f be a new state

in

let in

Q T S

= = =

Fffffi(r(eges;t;EEE(S(eye)mi);f;fnqN)o)udjjeleqls(2E2EL)FatoshltlEeonwgEf(feg)

^e2 else

SymnodesE

g

(Q; V; T; ; S; ffg)

end

end

This construction is the dual of the Berry-Sethi construction. It appears in the literature

as Wat93a, Constr. 4.45]. Note the duality with Construction 6.39.

2

Example 6.66 (Construction (rem-"-dual, sym, a-s)): Using our regular expression,

(a ") b , the auxiliary sets are the same as those used in Example 6.41 and the FA is the

same as in Figure 6.18 from Example 6.64. In this example, the nal state f is represented

by q0, state 2 1 (the b node in the regular expression) is represented by q1, and state 1 (the a node) is represented by q2.

1 2

In Section 6.5.1, we made use of a begin-marker to make Construction (rem-", sym, as) more concise. We can introduce an end-marker (as the dual concept of a begin-marker) to make the above construction more concise.

6.7. THE DUAL CONSTRUCTIONS

179

Algorithm detail 6.67 (e-mark): By appending a symbol ($ in our case) to the reg-

ular expression E, and using an encoding similar to the one in Section 6.5.1, we improve

Construction 6.65.

2

Again, it does not matter which symbol is used as our end-marker symbol. The resulting construction is as follows.

Construction 6.68 (rem-"-dual, sym, a-s, e-mark): Assuming E 2 RE, we con-

struct an FA as follows:

let in

Q T S

= = =

fSy(em; n(Eode$s )E(e$ );
FirstE $

f

)

j

f

2

F

ollowE

$(e)

g

(Q; V; T; ; S; f2g)

end

The use of an end-marker is also a well-known encoding trick. This particular algorithm

appears as ASU86, Example 3.22, p. 140] as well as Wat93a, Constr. 4.48].

2

s)

We could compose function useful to get a DFA construction known

sas

subset with Construction (rem-"-dual, Construction (rem-"-dual, sym, a-s,

sym, asubset,

use-s). Alternatively, we could use the end-marker construction, yielding the following

construction.

Construction 6.69 (Aho-Sethi-Ullman): Construction (rem-"-dual, sym, a-s, e-

mark, subset, use-s) is known as the Aho-Sethi-Ullman construction ( ASU86, Alg. 3.5,

Fig. 3.44] and Wat93a, Constr. 4.50 and Alg. 4.52]).

2

This construction is known to be one of the most e cient constructions in practice. For a comparison of the performance of some of the constructions, see Chapter 14. An imperative algorithm implementing it is given in Algorithm 6.86. The following is an example of the Aho-Sethi-Ullman construction.

Example 6.70 (Aho-Sethi-Ullman): Using our running example regular expression
(a ") (b ), we append the end-marker to obtain ((a ") (b )) $. We compute the following auxiliary sets:

Symnodes((a ") (b )) $ = f1 1 1; 1 2 1; 2g.

First((a ") (b )) $ = f1 1 1; 1 2 1; 2g.

Follow ((a ") (b )) $ = f(1 1 1; 1 2 1); (1 2 1; 1 2 1); (1 1 1; 2); (1 2 1; 2)g.

Unlike our presentation, Aho, Sethi, and Ullman's presentation also removes the sink state

from the DFA. For this example, we also remove the sink state. We have the following two

states q0 = f1
Figure 6.19.

1

1; 1

2

1; 2g and q1 = f1

2

1; 2g. The resulting DFA is shown in 2

180 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

q0 a; b q1

b

Figure 6.19: DFA produced by the Aho-Sethi-Ullman construction.

6.8 Precomputing the auxiliary sets and Null

In this section, we consider how to precompute the sets Symnodes, First, Last, Follow , and predicate Null . They can all be computed using a single recursive traversal of the
tree E 2 RE. This is especially e cient when the set SymnodesE is encoded as a set of
integers, as is done in FIRE Lite and the FIRE Engine. In the following properties, we give inductive de nitions which are based upon the structure of regular expressions.

Property 6.71 (Inductive de nition of Null): The following de nitions follow from
the de nition of Null :

Null ( )

= false

Null (")

= true

Null (a)

= false

Null (E0 Null (E0

E1) E1)

= =

Null Null

(E0) (E0)

_ ^

Null Null

(E1) (E1)

Null Null Null

(((EEE000+?)))

= true = Null(E0) = true

2

Example 6.72 (Null(E)): Recalling our example (a ") (b ) 2 RE:

Null ((a ") (b ))
= f Null on a node g Null (a ") ^ Null (b )
= f Null on and nodes g (Null (a) _ Null (")) ^ true
= f Null on a and " nodes g (false _ true) ^ true
= f de nitions of _ and ^ g
true

2

6.8. PRECOMPUTING THE AUXILIARY SETS AND NULL

181

Property 6.73 (Inductive de nition of Symnodes): The following de nitions follow
from the de nition of Symnodes:

Symnodes =

Symnodes" Symnodesa

=
= f0g

(the root)

SSSSSyyyyymmmmmnnnnnooooodddddeeeeesssssEEEEE00000?+EE11

= = = = =

1 1 1 1 1

Symnodes Symnodes Symnodes

E0 E0 E0

SymnodesE0

SymnodesE0

2 2

Symnodes Symnodes

E1 E1

2

Example 6.74 (SymnodesE): Recalling our example (a ") (b ) 2 RE:

Symnodes(a ") (b )
= f Symnodes on a node g
1 Symnodesa " 2 Symnodesb
= f Symnodes on and nodes g
1 (1 Symnodesa 2 Symnodes") 2 (1 Symnodesb)
= f Symnodes on a, b, and " nodes g 1 (1 f0g 2 ) 2 (1 f0g)
= f is associative g f1 1 0; 2 1 0g
= f 0 is the unit of g f1 1; 2 1g

2

Property 6.75 (Inductive de nition of First): We present an inductive de nition for
FirstE based upon the structure of E:

First =

First" Firsta

=
= f0g

First First First First First

E0 E1 E0 E1 E0 E0+ E0?

= = = = =

1 1 1 1 1

First First First

E0 E0 E0

FirstE0

FirstE0

(the root)

2
if

NFuilrls(tEE01)

then

2

FirstE1 else

2

182 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

Example 6.76 (First): Again, using (a ") (b ) 2 RE:

First(a ") (b )
= f First on a node g 1 Firsta " if Null(a ") then 2 Firstb else
= f de nition of Null g
1 Firsta " 2 Firstb
= f First on and nodes g
1 (1 Firsta 2 First") 2 (1 Firstb)
= f First on a, b and " nodes g f1 1; 2 1g

2

Property 6.77 (Inductive de nition of Last): The following are derived in a similar
manner to First:

Last =

Last" Lasta

=
= f0g

Last Last Last Last Last

E0 E1 E0 E1 E0 E0+ E0?

= = = = =

1 2 1 1 1

Last Last Last

E0 E1 E0

LastE0

LastE0

(the root)

2
if

NLualsl t(EE11)

then

1

LastE0 else

2

Example 6.78 (Last): We use the regular expression (a ") (b ):

Last(a ") (b )
= f Last on a node g 2 Lastb if Null(b ) then 1 Lasta " else
= f de nition of Null g
2 Lastb 1 Lasta "
= f Last on and nodes g
2 (1 Lastb) 1 (1 Lasta 2 Last")
= f Last on a, b and " nodes g f1 1; 2 1g

2

Before giving an inductive de nition of Follow , we extend operator .

6.8. PRECOMPUTING THE AUXILIARY SETS AND NULL

183

Notation 6.79 (Extension of ): Given i 2 N and A dom(E) dom(E), we extend
as follows:

i A = f (i a; i b) j (a; b) 2 A g

This is purely a notational convenience.

2

Property 6.80 (Inductive de nition of Follow): We present an inductive de nition of
Follow :

Follow =

Follow " Follow a

=
= f0g

Follow Follow Follow Follow Follow

E0 E1 E0 E1 E0 E0+ E0?

= = = = =

1 1 1 1 1

Follow Follow Follow

E0 E0 E0

Follow E0

Follow E0

2 2 (1 (1

FFLLooaallllssoottwwEE00EE))11

(1 (1 (1

FFLiiarrssstttEEE000)))

(2 FirstE1)

2

Example 6.81 (Follow): Taking our usual regular expression (a ") (b ):

Follow (a ") (b )
= f Follow on a node g
1 Follow a " 2 Follow b (1 Lasta ") (2 Firstb )
= f de nitions of Lasta " and Firstb g 1 Follow a " 2 Follow b f(1 1; 2 1)g
= f de nitions of Follow a " and Follow b g 2 (1 Follow b (1 Lastb 1 Firstb)) f(1 1; 2 1)g
= f de nition of Firstb; Lastb; Follow b g 2 (1 (1 f0g 1 f0g)) f(1 1; 2 1)g
= f calculus g 2 f(1; 1)g f(1 1; 2 1)g
= f calculus g f(2 1; 2 1)g f(1 1; 2 1)g

Intuitively, this shows that (in the language denoted by regular expression (a ") (b )) an

a can be followed by a b and a b can be followed by a b.

2

Remark 6.82: When the integer encoding of SymnodesE is used, the inductive de nitions
above turn out to be the same ones that are given in, for example, ASU86]. There are a number of techniques to speed up the computation of these sets in practice. Of particular

184 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

importance are the methods described by Bruggemann-Klein in B-K93a] and by Chang

and Paige in Chan92, CP92]; a short summary of these methods appears in Wat93a,

Section 4.5.2]. Bruggemann-Klein's method works by manipulating the regular expression

into a form which is more suited to the computation of the auxiliary sets. The Chang-

Paige method works by computing the subparts of the auxiliary sets on an `as-needed'

(lazy) basis.

2

6.9 Constructions as imperative programs
In this section, we give some algorithms implementing constructions from the past few sections. These algorithms are in the form most often seen in text-book and journal presentations of constructions. We make use of the algorithms presented in Section 2.6.2.1. We will not concern ourselves with the computation of any of the auxiliary sets or relations.

6.9.1 The item set constructions

We can rem"

present an imperative program which computes CA | Construction (rem-", subset, use-s).

composite

function

useful

s

subset

Algorithm 6.83 (Implementing (rem-", subset, use-s)):

f E 2 RE g S; T := fD ( E)g; ;
D; U := ; S;
do U 6= ! let u : u 2 U; D; U := D fug; U n fug; for a : a 2 V ! d := ( e : (e; BEF) 2 u ^ E(e) = a : D (e; AFT)); if d 26 D ! U := U fdg ] d 2 D ! skip fi; T := T f(u; a; d)g rof
od; F := f f j f 2 D ^ E 2 f g f L^FA(D(D; V; V; T; T; ; ; ;SS; ;FF))2=DLFRAE (E)
^ Complete(D; V; T; ; S; F) g

2

6.9. CONSTRUCTIONS AS IMPERATIVE PROGRAMS

185

This algorithm is essentially the `deterministic item set construction', given in Wat93a, Constr. 5.69].
We can also present the ` ltered' version, using lter W. In order to do this, we simply
rewrite some of the statements in the above algorithm:
Assignment S := fD ( E)g becomes S := fW(D ( E))g.
Statement
d := ( e : (e; BEF) 2 u ^ E(e) = a : D (e; AFT))

is replaced by
d := W( e : (e; BEF) 2 u ^ E(e) = a : D (e; AFT))

The resulting algorithm implements Construction (rem-", subset, use-s, Wfilt). This algorithm does not appear in the literature. In Chapter 14, we see that it displays good performance in practice. It produces DFAs which are isomorphic to those produced by Construction (rem-"-dual, sym, a-s, subset, use-s) and the Aho-Sethi-Ullman construction.

6.9.2 The Symnodes constructions
We present an algorithm implementing Construction (rem-", sym, a-s, subset, use-s) which produces a DFA. Here, the rst iteration is unrolled to accommodate the special treatment of the start state in construction (rem-", sym, a-s), and some obvious improvements have not yet been made.

f E 2 RE g

let S = ffsgg : s is a new state;

T := ;

D; U := ; S;

let u : u 2 U;

f u = fsg g

D; U := D fug; U n fug;

for a : a 2 V !

d
f

:= (
p2u

p

:p2u: p = fsg

fe g

j

e

2

FirstE

^

E(e)

=

a

g);

if d 62 D ! U := U fdg

] d 2 D ! skip

fi;

T := T f(u; a; d)g

rof;

186 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

do U 6= !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d :=
if d

(
26

p:p2u D ! U :=

:f
U

e

je2 fdg

Follow

E(p)

^

E(e)

=

a

g);

] d 2 D ! skip

fi;

T := T f(u; a; d)g

rof

od;

F
f

:= f d j d 2 L^FA(D(D; V; V; T; T;

D ;
;

;S^S; ;FdF\))2L=aDsLFtERAE6=(E)g

if Null(E) then S else

^ Complete(D; V; T; ; S; F) g

Some simpli cation gives the following algorithm | the McNaughton-Yamada-Glushkov construction MY60, Glus61]; it is also given in Wat93a, Algorithm 4.42].
Algorithm 6.84 (Implementing McNaughton-Yamada-Glushkov):

f E 2 RE g

let S = ffsgg : s is a new state;

T := ;

D; U := S; ;

for a : a 2 V !

d := f e
U := U

j

e 2 First fdg;

E

^

E(e)

=

a

g;

T := T f(fsg; a; d)g

rof;

do U 6= !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d :=
if d

(
62

p:p2u D ! U :=

:f
U

e

je2 fdg

Follow

E(p)

^

E(e)

=

a

g);

] d 2 D ! skip

fi;

T := T f(u; a; d)g

rof

od;

F
f

:= f d j LFA(D;

d2
V;T

D ;

^d\
; S; F)

L=asLtERE6=(E)g

if Null(E) then S else

6.9. CONSTRUCTIONS AS IMPERATIVE PROGRAMS

187

^ (D; V; T; ; S; F) 2 DFA ^ Complete(D; V; T; ; S; F) g

2

To show the simplicity gained by the use of a begin-marker, we give an imperative algorithm implementing Construction (rem-", sym, a-s, b-mark, subset, use-s):
Algorithm 6.85 (Implementing (rem-", sym, a-s, b-mark, subset, use-s)):

f E 2 RE g

S; T := ff1gg; ;

D; U := ; S;

do U =6 !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d :=
if d

(
62

p:p2u D ! U :=

:f
U

e

je2 fdg

Follow

E(p)

^

E(e)

=

a

g);

] d 2 D ! skip

fi;

T := T f(u; a; d)g

rof

od;

F
f

L:^=FA(fD(Df; V;jV;fT; T2; ;D; ;S^S; ;FfF)\)2=LDaLsFtRA$EE(E6=)

g

^ Complete(D; V; T; ; S; F) g

2 This algorithm is somewhat more elegant and more practical than the one given as Algorithm 6.84, thanks to the fact that the start state does not require special treatment.

6.9.3 A dual construction
The following algorithm implements the Aho-Sethi-Ullman construction | Construction (rem-"-dual, sym, a-s, e-mark, subset, use-s).
Algorithm 6.86 (Implementing Aho-Sethi-Ullman):

f E 2 RE g

S; T := fFirst
D; U := ; S;

E

$g;

;

188 CHAPTER 6. FA CONSTRUCTION ALGORITHMS

do U =6 !

let u : u 2 U;

D; U := D fug; U n fug;

for a : a 2 V !

d :=
if d

(
26

p:p2u D ! U :=

^
U

E(p) =
fdg

a

:

Follow

E

$(p));

] d 2 D ! skip

fi;

T := T f(u; a; d)g

rof

od;

F := f f j f 2 D ^ 2 2 f g

f L^FA(D(D; V; V; T; T; ; ; ;SS; ;FF))2=DLFRAE (E)

^ Complete(D; V; T; ; S; F) g

2 This algorithm appears as ASU86, Alg. 3.5, Fig. 3.44] and as Wat93a, Constr. 4.50 and Alg. 4.52].
Comparing this algorithm with the variant of the McNaughton-Yamada-Glushkov algorithm (Algorithm 6.85) shows that the two are very similar, with two notable exceptions: the assignment to d in the inner repetition, and assignment to F are both considerably more e cient to implement in the above construction than in the McNaughton-YamadaGlushkov algorithm. The di erences in e ciency can be seen clearly from the data presented in Chapter 14.

6.10 Conclusions
A number of conclusions can be drawn about the taxonomy presented in this chapter:
The original algorithms were presented in widely di ering styles (some where aimed at compiler applications, while others were aimed at digital circuitry applications) over a period of many years. Interestingly, they can all be related in rather direct ways. This can be attributed to the following facts:
{ We used a `canonical' construction (directly related to the input regular expres-
sion), in which each state contains the `maximal' amount of information.
{ We introduced various e cient encodings of the canonical states. Some the en-
codings removed information, allowing states to be safely identi ed (and therefore creating smaller automata).

6.10. CONCLUSIONS

189

{ Some simple algorithmic building blocks (for example "-transition removal, the
subset construction, and start-unreachable state removal) were identi ed and factored out. This lead to concise descriptions of the algorithms (as compositions of functions), and subsequent transformation into imperative programs was particularly easy. The direct transformation of imperative programs would have been di cult.
The earlier taxonomy presented in Wat93a] contained two taxonomy trees. The development of that taxonomy seemed to indicate that the two subfamilies of algorithms were related, but could not be derived from one another. The taxonomy presented in this chapter shows otherwise: all of the constructions can be derived from a single canonical construction.
Many of the constructions produce automata whose states contain information. For example, the states of the canonical construction are `items', encoding the left and right languages of the states. This approach had two advantages:
{ The additional information made it easier to argue the correctness of each of
the constructions, especially the canonical construction.
{ The information could be encoded in various ways, leading to more e cient
constructions.
{ In some cases, the encodings may lead to states being identi ed | reducing the
size of the produced automata.
One of the most recently developed constructions (Antimirov's) was also successfully integrated into the taxonomy.
All of the constructions were successfully presented as compositions of mathematical functions (as opposed to only being presented as imperative programs). The corresponding imperative programs were also presented.
We can also draw some conclusions about the individual constructions and relationships between them:
{ The Berry-Sethi construction is an encoding of an "-transition removal function
composed with the canonical construction.
{ Antimirov's construction is an encoding of the Berry-Sethi construction. (Note
that Antimirov's construction may produce smaller automata than the BerrySethi construction, meaning that Antimirov's construction is an optimization.)
{ The deterministic item set construction can be improved through the use of
` lters'. One such lter yields DeRemer's construction, while another lter yields a new construction.
{ The McNaughton-Yamada-Glushkov is the subset construction composed with
the Berry-Sethi construction.

190 CHAPTER 6. FA CONSTRUCTION ALGORITHMS
{ The subset construction composed with the Antimirov's construction is a variant
of Brzozowski's construction.
{ The original version of Brzozowski's construction, with the similarity relation
on regular expressions, is shown to be a further encoding of our variant.
{ From the above three observations, we can conclude that the deterministic item
set construction, the McNaughton-Yamada-Glushkov construction, and Brzozowski's construction are encodings of one another. (It is possible, however, that Brzozowski's construction produces a smaller DFA.)
{ The Aho-Sethi-Ullman construction is the subset construction composed with
the dual of the Berry-Sethi construction.
{ The new ( lter-based) construction is an encoding of the Aho-Sethi-Ullman
construction.
The taxonomy presented here was crucial to the development of FIRE Lite, a C++ toolkit of construction algorithms. The structure of the taxonomy tree is re ected in the inheritance hierarchy of FIRE Lite.
Lastly, we note that there is room for improvement in the taxonomy presented in this chapter. The constructions which are based upon derivatives were added after some of the other constructions were taxonomized. As a result, it may be possible to factor their development even more.

Chapter 7
DFA minimization algorithms
This chapter presents a taxonomy of nite automata minimization algorithms. Brzozowski's elegant minimization algorithm di ers from all other known minimization algorithms, and is derived separately. All of the remaining algorithms depend upon computing an equivalence relation on states. We de ne the equivalence relation, the partition that it induces, and its complement. Additionally, some useful properties are derived. It is shown that the equivalence relation is the greatest xed point of a function, providing a useful characterization of the required computation. We derive an upperbound on the number of approximation steps required to compute the xed point. Algorithms computing the equivalence relation (or the partition, or its complement) are derived systematically in the same framework. The algorithms include Hopcroft's, several algorithms from text-books (including Hopcroft and Ullman's HU79], Wood's Wood87], and Aho, Sethi, and Ullman's ASU86]), and several new algorithms or variants of existing algorithms.
An early version of this taxonomy appeared in Wat93b].
7.1 Introduction
The minimization of deterministic nite automata is a problem that has been studied since the late 1950's. Simply stated, the problem is to nd the unique (up to isomorphism) minimal deterministic nite automaton that accepts the same language as a given deterministic nite automaton. Algorithms solving this problem are used in applications ranging from compiler construction to hardware circuit minimization. With such a variety of applications, the number of di ering presentations also grew: most text-books present their own variation, while the algorithm with the best running time (Hopcroft's) remains obscure and di cult to understand.
This chapter presents a taxonomy of nite automata minimization algorithms. The need for a taxonomy is illustrated by the following:
Most text-book authors claim that their minimization algorithm is directly derived from those presented by Hu man Hu 54] and Moore Moor56]. Unfortunately, most text-books present vastly di ering algorithms (for example, compare AU92],
191

192 CHAPTER 7. DFA MINIMIZATION ALGORITHMS
ASU86], HU79], and Wood87]), and only the algorithms presented by Aho and Ullman and by Wood are directly derived from those originally presented by Hu man and Moore. While most of the algorithms rely on computing an equivalence relation on states, many of the explanations accompanying the algorithm presentations do not explicitly mention whether the algorithm computed the equivalence relation, the partition (of states) that it induces, or its complement. Comparison of the algorithms is further hindered by the vastly di ering styles of presentation | sometimes as imperative programs, or as functional programs, but frequently only as a descriptive paragraph. For notational convenience, we restrict ourselves to producing minimal Complete DFAs. This is strictly a notational convenience, as the minimization algorithms can be modi ed to work for in-Complete DFAs. A Complete minimized DFA will (in general) have one more state (a sink state) than a in-Complete minimized DFA. All except one of the algorithms rely on determining the set of automaton states which are equivalent1. The algorithm that does not make use of equivalent states is discussed in Section 7.2. In Section 7.3 the de nition and some properties of equivalence of states is given. Algorithms that compute equivalent states are presented in Section 7.4. The main results of the taxonomy are summarized in the conclusions | Section 7.5. The minimization algorithm relationships are shown in a `family tree' in Figure 7.1. Unlike in Chapters 4 and 6, the algorithm and problem details remain implicit in the presentation of the algorithms. In the family tree, the details are shown as edges, depicting re nements of the solution. The principal computation in most minimization algorithms is the determination of equivalent (or inequivalent) states | thus yielding an equivalence relation on states. In this chapter, we consider the following minimization algorithms: Brzozowski's nite automaton2 minimization algorithm as presented in Brzo62]. This elegant algorithm (Section 7.2) was originally invented by Brzozowski, and has since been re-invented by a number of others (in some cases without credit to Brzozowski). Given a (possibly nondeterministic) nite automaton without "-transitions, this algorithm produces the minimal deterministic nite automaton accepting the same language. Layerwise computation of equivalence as presented in Wood87, Brau88, Urba89]. This algorithm (Algorithm 7.18, also known as Wood's algorithm in the literature) is a straightforward implementation suggested by the approximation sequence arising from the xed-point de nition of equivalence of states.
1Equivalence of states is de ned later. 2This algorithm also works on nondeterministic nite automata, in contrast with the other algorithms which only work on deterministic nite automata.

7.1. INTRODUCTION

193

Brzozowski (x 7.2)

Equivalence of states (x 7.3)

equivalence relation (x 7.4.1{7.4.5, 7.4.7)

pointwise (x 7.4.6)

approx. from above (x 7.4.1{7.4.5)

approx. from below (7.28)

imperative program

layerwise unordered state pairs

(7.27) memoization

(7.18) Naive

Improved

Hopcroft-Ullman (7.24)

(p. 212)

(7.19) eq. classes ASU (7.21)

(7.22) eq. classes (7.23) lists (p. 207) optimized list update Hopcroft (7.26)

Figure 7.1: The family trees of nite automata minimization algorithms. Brzozowski's minimization algorithm is unrelated to the others, and appears as a separate (single vertex) tree. Each algorithm presented in this chapter appears as a vertex in this tree. For each algorithm that appears explicitly in this chapter, the construction number appears in parentheses (indicating where it appears in this chapter). For algorithms that do not appear explicitly, a reference to the section or page number is given. Edges denote a re nement of the solution (and therefore explicit relationships between algorithms). They are labeled with the name of the re nement.

194 CHAPTER 7. DFA MINIMIZATION ALGORITHMS
Unordered computation of equivalence. This algorithm (Algorithm 7.19, not appearing in the literature) computes the equivalence relation; pairs of states (for consideration of equivalence) are chosen in an arbitrary order. Unordered computation of equivalence classes as presented in ASU86]. This algorithm (Algorithm 7.21) is a modi cation of the above algorithm computing equivalence of states. Improved unordered computation of equivalence. This algorithm (Algorithm 7.22, not appearing in the literature) also computes the equivalence relation in an arbitrary order. The algorithm is a minor improvement of the other unordered algorithm. Improved unordered computation of equivalence classes. This algorithm (appearing as Algorithm 7.23 in this dissertation, not appearing in the literature) is a modication of the above algorithm to compute the equivalence classes of states. This algorithm is used in the derivation of Hopcroft's minimization algorithm. Hopcroft and Ullman's algorithm as presented in HU79]. This algorithm (Algorithm 7.24) computes the inequivalence (distinguishability) relation. Although it is based upon the algorithms of Hu man and Moore Hu 54, Moor56], this algorithm uses some interesting encoding techniques. Hopcroft's algorithm as presented in Hopc71, Grie73]. This algorithm (appearing here as Algorithm 7.26) is the best known algorithm (in terms of running time complexity) for minimization. As the original presentation by Hopcroft is di cult to understand, the presentation in this chapter is based upon the one given by Gries. Pointwise computation of equivalence. This algorithm (Algorithm 7.27, appearing in the literature only in a form suitable for comparing types for structural equivalence) computes the equivalence of a given pair of states. It draws upon some non-automata related techniques, such as: structural equivalence of types and memoization of functional programs. Computation of equivalence from below (with respect to re nement). This algorithm (Algorithm 7.28, not appearing in the literature) computes the equivalence relation from below. Unlike any of the other known algorithms, the intermediate result of this algorithm can be used to construct a smaller (although not minimal) deterministic nite automaton.
7.2 An algorithm due to Brzozowski
Most minimization algorithms are applied to a DFA. In the case of a nondeterministic FA, the subset construction (function subset | Transformation 2.121) is applied rst, followed by the minimization algorithm. In this section, we consider the possibility of applying

7.2. AN ALGORITHM DUE TO BRZOZOWSKI

195

the subset construction (with start-unreachable state removal) after an (as yet unknown)

algorithm, thus yielding a minimal DFA. We now derive such an algorithm.

Let M0 = (Q0; V; T0; ; S0; F0) be the "-free nite automaton (not necessarily a DFA)

tLoFAb(eMm0i)n=imLizFeAd(aMn2d)M(a2n=d

(Q2; V; T2; ; S2; F2) of course MinC(M2)

be |

the see

minimized De nition

Complete DFA such that 2.109). (For the remain-

(dwCLQeeForA1ma;(opVpMfpl;te2lThty)e1is;=(tMhseeL;2cSF)stA1uio;h(bnFMos1lewd1)t)secs.=)umoncaLhsWktFretAueh(ucaMsrtteeio0qMo)nu.fi2lMrae=sitnt,hi(mwuaetsaelhfMCualv(1sPeirssoospmuseobermsteiyetnh)t2(oe.M1wrm111)oe).bdatia(asNtioneopetpdenoistftreheodamatutoUtMoMsme0if,anutlCaosn.n()dMMS2ti)1nhca=^et

From the de nition of MinimalC(M2) (which appears in Property 2.111), we require

(8 p; q : p 2 Q2 ^ q 2 Q2 ^ p =6 q : ?!L (p) =6 ?!L (q)) ^ Usefuls(M2) ^ Complete(M2)

For all of the

states subset

q 2 Q2 we have q 2
construction gives

P(Q1)

since

M2

=

(useful

s

subset)(M1). Property 2.123

(8 p : p 2 Q2 : ?!L (p) = ( q : q 2 Q1 ^ q 2 p : ?!L (q)))

We need a su cient condition on M1 to ensure MinimalC(M2). The following derivation gives such a condition:

Minimal C (M2)

(8

f de
p; q : p

nition of Minimal
2 Q2 ^ q 2 Q2 ^

C
p

(Property 2.111) g =6 q : ?!L (p) 6= ?!L (q))

^

Useful

s

(M2)

^

Complete

(M2)

(

(8

f Property p; q : p 2 Q1

2.123;
^q2

QM12^=p(6=useqfu: l?!Ls (ps)ub\se?!Lt)((qM) 1=) g

)

^

Useful f (M1)

f de nition of Det0 (Property 2.107) and Useful s, Usefulf (Remark 2.100) g

Det0(M1R) ^ Usefuls(M1R)

( f Property 2.107: Det0(M) ( Det(M) g

Det(M1R) ^ Useful s(M1R)

The required condition on M1 can be established by (writing reversal as a pre x function)
M1 T=h(eRcomupselefutelsmisnuibmseiztatiRon)(aMlg0o)r.ithm (for any "-free M0 2 FA) is

M2 = (useful s subset R useful s subset R)(M0)

This algorithm was originally given by Brzozowski in Brzo62]. A generalization of the algorithm was independently derived by Kameda and Weiner KW70] just after Brzozowski's presentation. The origin of this algorithm was obscured when Jan van de Snepscheut presented the algorithm in his Ph.D dissertation vdSn85], where the algorithm is attributed

196 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

to a private communication from Professor Peremans of the Eindhoven University of Technology. Peremans had originally found the algorithm in an article by Mirkin Mirk65]. Although Mirkin does cite a paper by Brzozowski Brzo64], it is not clear whether Mirkin's work was in uenced by Brzozowski's work on minimization. Jan van de Snepscheut's recent book vdSn93] describes the algorithm, but provides neither a history nor citations (other than his dissertation) for this algorithm.

7.3 Minimization by equivalence of states

In this section, we lay the foundations for those algorithms which determine the set of

equivalent states. Let M = (Q; V; T; ; S; F ) be a Complete DFA; this particular DFA will

be used throughout this section. We also assume that all of the states of M are start-

reachable, that is Useful the transition relation to

sb(eMth).e

Since M is deterministic
total function T 2 Q V

and
?!

Complete, we Q instead of T

will
2Q

also V

take
?!

P(Q).

In order to minimize the DFA M, we compute an equivalence relation E v Q Q. In

the following section, we will consider the de nition of this equivalence relation.

7.3.1 The equivalence relation on states

Given MinimalC(M) MinC(M), we will derive algorithms which ensure MinimalC(M). For the remainder of this chapter, we only consider ensuring
(8 p; q : p 2 Q ^ q 2 Q ^ p 6= q : ?!L (p) =6 ?!L (q))

The other two ensure and are

conjuncts left to the

of MinimalC(M), reader.

Useful s(M )

^

Complete (M ),

are

trivial

to

In order to minimize the DFA M, we compute equivalence relation E (on state set Q)

such that:

(p; q) 2 E (?!L (p) = ?!L (q))

(This relation is not to be confused with the "-transition relation of an FA which is also a binary relation on states. The name E has been used to signify `equivalence'.) Since this is an equivalence relation, we are really interested in unordered pairs of states. It is notationally more convenient to use ordered pairs instead of unordered pairs.
When two states are found to be equivalent under E, the states (and their transitions)
rceacnuIbrnseiovrieddeednrettioneictdoio,mnthpoeufrtee?!LbryewlsahhtriioicnnhkEfion,lglwotwehsenefareuodmtoamaparroteopcnue.rrtsyivoefdfuennctitioionn?!Lof.T(T.)his is an intuitive Property 7.1 (Function ?!L ): Function ?!L satis es
?!L (p) = ( a : a 2 V : fag ?!L (T(p; a))) (if p 2 F then f"g else )

7.3. MINIMIZATION BY EQUIVALENCE OF STATES

197

2

This allows us to give an alternate (but equivalent) characterization of equivalence of states.
Property 7.2 (Equivalence of states): Given the above property, we can rewrite E
into a recursive form. Relation E is the greatest equivalence relation (on Q) such that
(p; q) 2 E (p 2 F q 2 F) ^ (8 a : a 2 V : (T(p; a); T(q; a)) 2 E) 2

We will shortly present E as the greatest xed point of a continuous function.

De nition 7.3 (Equivalence relations on Q):
equivalence relations on state set Q.

We

de

ne

ERQ

to

be

the

set

of

all 2

Property 7.4 (ERQ): The set ERQ is a
least element IQ and greatest element Q

lattice Q.

under

the

re

nement

(v)

ordering,

with 2

De nition 7.5 (Function g): De ne function g 2 ERQ ?! ERQ as g(H) =

f (p; q) j (p; q) 2 H ^ (p 2 F q 2 F) ^ (8 a : a 2 V : (T(p; a); T(q; a)) 2 H) g

This function is continuous on the lattice of equivalence relations ERQ.

2

Property 7.6 (Fixed point characterization of E): Relation E is the greatest xed
point of function g on the lattice of equivalence relations EQ. More formally
E = (MAXv H : H v Q Q ^ H = g(H) : H)

Note that g(H) v H.

2

For more on this type of xed point characterization, see PTB85].

Remark 7.7: Any xed point of the equivalence in Property 7.2 can be used. In order to
minimize the automaton (instead of simply shrinking it), the greatest xed point is desired. 2

Property 7.8 (Computing E): Since g is continuous and the lattice is nite, we can

compute E by successive applications of function g, beginning with the `top' equivalence

relation > = Q Q. We use the following notation to refer to the result of each step (in

the computation of E) Ek = We can already make the
case, we can simplify the de

gk+1(>) (for k 0). nirtsitonsteopf ,gb(ysinnocteinfogrtahlalt(pE;0q)=2g(E>0,)

= (Q
p2F

n

F )2
q2

F 2. In this F) to give

g0(H) = f (p; q) j (p; q) 2 H ^ (8 a : a 2 V : (T(p; a); T(q; a)) 2 H) g

Note that Ek = g0k(E0).

2

198 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

Remark 7.9: We can also give an intuitive explanation of equivalence relation Ek at each

step of computing
(p; q) 2 Ek) if and
As a consequence,

the xed point. A pair of only if there is no string p and q are k-equivalent

states p; q are said to
w : jwj k such that
(k > 0) if and only if

bwe2k-?!eLq(upiv)a6 lenwt

2(w?!rLit(tqe)n.

they are both nal or both non- nal, and
for all a 2 V , T(p; a) and T(q; a) are (k ? 1)-equivalent (by the de nitions of ?!L and
T ).

2

Property 7.10 (E as a greatest xed point): We can also cast the recursive de nition

of E as the greatest solution of a set of (perhaps mutually recursive) equivalences. We wish

to obtain a set of jQj2 equivalences with left sides fp;q
to the system of equivalences is found, we will have
the jQj2 equations as follows (where p; q 2 Q):

(for fp;q

states (p;

p; q)

q 2 Q). When a solution 2 E. We de ne each of

fp;q (p 2 F q 2 F ) ^ (8 a : a 2 V : fT(p;a);T(q;a))

The xed point approximation begins with all of the fp;q true. At each step in the

approximation, any one of the equations can be updated, bringing the entire system closer

to a solution. Unlike the xed point approximation sequence outlined in Property 7.8,

the relation given by the fp;q may not be an equivalence relation at every step in nding

a solution to the system; the nal solution is, however, the equivalence relation E. An

interesting property (which we will not prove here) is that once an fp;q has become false,

it will not become true at a later step in the approximation sequence (this property is

similar to g(H) v H given in Property 7.6). Algorithms that make use of this method of

computing E are given in Sections 7.4.2{7.4.5 and 7.4.7. This approach is equivalent to

the approach taken in Property 7.8; in can, however, be implemented very e ciently in

some cases.

2

All previously known algorithms compute E by successive approximation from above
(with respect to v) | a standard approach for computing a greatest xed point. A new
algorithm in Section 7.4.7 computes E by successive approximation from below. In that section, the practical importance of this is explained.

7.3.2 Distinguishability
It is also possible to compute E by rst computing its complement D = :E. Relation D
(called the distinguishability relation on states) is required to satisfy
(p; q) 2 D (?!L (p) 6= ?!L (q))

7.3. MINIMIZATION BY EQUIVALENCE OF STATES

199

De nition 7.11 (Distinguishability of states): D is the least (under set containment3,
) relation such that

(p; q) 2 D (p 2 F 6 q 2 F) _ (9 a : a 2 V : (T(p; a); T(q; a)) 2 D)

2

Property 7.12 (Approximating D): As with equivalence relation E, relation D can be
computed by successive approximations (for k 0)

(p; q) 2 Dk+1 (p; q) 2 Dk _ (9 a : a 2 V : (T (p; a); T (q; a)) 2 Dk)

with D0 = could have

:E0 =
started

((Q n
with

F) the

F) (F complement

(Q n
of

F)). For all k 0
> (which we used

we have Dk = as our starting

:Ek.
point

We for

computing xed point also have the property

E); for e that Dk+1

ciency reasons we Dk for k 0.

would

start

with

D0

in

practice.

We 2

Rsaureechmsataihdraktto7wb.e123k:-?!LdAi(sspti)wn6 gituhwisEh2ekd,?!La(wn(qrii)nt.tteuAnits(ivpae; cqeo)xn2psleaDqnukae)tniiofcnea,nopdf

Dk is useful. A only if there is a

pair of states
string w : jwj

p; q k

and q are k-distinguished (k > 0,

some authors say k-distinguishable) if and only if

one is nal and the other is non- nal, or

there exists a 2 V such that T(p; a) and T(q; a) are (k ? 1)-distinguished.

2

7.3.3 An upperbound on the number of approximation steps

We can easily place an upperbound on the number of steps in the computation of E. (This is not the same as the complexity of computing E; instead, we show the number of steps required in an approximating sequence while computing E.)
Let Ej be the greatest xed point of the equation de ning E. We have the sequence of approximations (where IQ is the identity relation on states):

E0 E1

Ej IQ

The indices of some of the equivalence relations in the approximation sequence are known:
]IQ = jQj and ]E0 2. We can deduce that:

]E0 < ]E1 < < ]Ej ]IQ = jQj

In the case that ]E0 = 0 (when Q = ), we have that E0 is the greatest xed point. In the case that ]E0 = 1, either all states are nal states, or all states are non- nal ones; in

3Here, denotes normal set containment; re nement does not apply since D is not necessarily an equivalence relation.

200 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

both cases E0 is (for all i). Since
(jQj ? 2) max 0

the greatest j + 2 ]Ej steps for the

xed ]IQ

point. In the
= jQj we get j

casjeQtjh?at2.]ET0h=is

computation (starting at E0) of the

2, we have i + 2 ]Ei gives an upperbound of greatest xed point Ej

(using the approximating sequence given in Property 7.8).

A consequence of this can lead to some

this upperbound is that e ciency improvements

E to

a=lgoEr(ijtQhj?m2s)mcoaxm0p. uAtinsgwEe .shTahllissereeslualtteirs,

also noted by Wood in Wood87, Lemma 2.4.1] and by Brzozowski and Seger in BS95,

Theorem 10.7]. This upperbound also holds for computing D and Q]E by approximation.

7.3.4 Characterizing the equivalence classes of E

In Q

this section, we give a under E. The set Q]E

cihsatrhaectlearrigzeasttio(nunodfetrhve s)eptarQit]iEti,otnhePseotf

of Q

equivalence such that

classes

of

(8 Q0 : Q0 2 P : (8 p; q : p 2 Q0 ^ q 2 Q0 : (p; q) 2 E))

Our derivation proceeds as follows:

(8 Q0 : Q0 2 P : (8 p; q : p 2 Q0 ^ q 2 Q0 : (p; q) 2 E)) f Property 7.2 g
(8 Q0 : Q0 2 P : (8 p; q : p 2 Q0 ^ q 2 Q0 : (p 2 F q 2 F) ^ (8 a : a 2 V : (T(p; a); T(q; a)) 2 E))) f move a to outer quanti cation g
(8 Q0; a : Q0 2 P ^ a 2 V : (8 p; q : p 2 Q0 ^ q 2 Q0 : (p 2 F q 2 F) ^ (T(p; a); T(q; a)) 2 E)) f introduced equivalence class Q1 explicitly g
(8 Q0; Q1; a : Q0 2 P ^ Q1 2 P ^ a 2 V : (8 p; q : p 2 Q0 ^ q 2 Q0 : (p 2 F q 2 F) ^ (T(p; a) 2 Q1 T(q; a) 2 Q1)))

Given the last line above,
and for all Q0; Q1 2 P; a 2

VQ:]E

is

the

largest

(under

v)

partition

P

such

that

P

v

fQg

(8 p; q : p 2 Q0 ^ q 2 Q0 : (p 2 F q 2 F) ^ (T(p; a) 2 Q1 T(q; a) 2 Q1))

As with the sequence used to compute E, we can make the rst approximation step, leading to a simpler characterization of Q]E. To make this more readable, we de ne an auxiliary predicate.

De nition 7.14 (Predicate Splittable): In order to make this quanti cation more con-
cise, we de ne

Splittable(Q0; Q1; a) (9 p; q : p 2 Q0 ^ q 2 Q0 : (T (p; a) 2 Q1 6 T (q; a) 2 Q1))

2

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

201

Given the de nition of Splittable (and the last line of the derivation above), we can now characterize Q]E.

Property 7.15 (Characterization of such that P v Q]E0 and

Q]E):

Q]E is the largest (under v) partition P

(8 Q0; Q1; a : Q0 2 P ^ Q1 2 P ^ a 2 V : :Splittable(Q0; Q1; a))

This characterization will be used in the computation of Q]E.

2

In AHU74, p. 157{162], the above characterization of Q]E is stated as the coarsest partition problem. That problem can be phrased in one of two ways:

Q]E is (for all

the
a2

coarsest V ).

partition

of

Q

compatible

with

fQg

and

functions

Ta

2

Q

?!

Q

Q]E is (for all

the coarsest
a 2 V ).

partition

of Q

compatible with

Q]E0

and

functions

Ta

2

Q

?!

Q

The second formulation includes the rst step. In that book, only the single function

problem is considered, whereas each alphabet symbol a.

the

above

phrasing

includes

a

transition

function

Ta

for

When V means that

= fag (jV
computing

j

= 1), Q]E is

we have the single the single-function

transition function Ta 2 Q
coarsest partition problem.

?!
In

Q. This PTB85],

a linear time algorithm is given for this problem; the implication is that a DFA over a one

letter alphabet can be minimized in linear time (instead of O(jQj log jQj) for Hopcroft's

algorithm | the best known general algorithm).

7.4 Algorithms computing E, D, or Q]E
In this section, we consider several algorithms that compute D, E, or Q]E. Some of the algorithms are presented in general terms: computing D and E. Since only one of D or E is needed (and not both), such a general algorithm would be modi ed for practical use to compute only one of the two.

7.4.1 Computing D and E by layerwise approximations
We now present an implementation of the method of computing E outlined in Property 7.8. The following algorithm computes D and E (where variable k is a ghost variable, used only for specifying the invariant)

202 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

Algorithm 7.16:
G; H := D0; E0;
fdGooinldHfv;aHGr6=ioa=6ldnH;tGko:ld:oG=ld!=^;HDQk=6 ^HQH;ol0d=;gEk g GHGo::l==d; H:g0Ho(lHd;:o=ld)G; ; H; f G = :H g
k := k + 1
odf G = D ^ H = E g

2

We can expand the de nition of function g0 and give a more detailed computation of G, yielding
Algorithm 7.17:

G; H := D0; E0;

fdGooinldHfv;aHGr=6 ioa6=ldnH;tGko:ld:oG=ld!=^;HDQk=6 ^HQH;ol0d=;gEk g

Gold; Hold := G := ( p; q H := ( p; q
f G = :H g

G; H; : (p; q) : (p; q)

2 2

Gold Hold

_ ^

(9 (8

a a

: :

a a

2 2

V V

: :

(T(p; a); T(q; a)) 2 (T(p; a); T(q; a)) 2

Gold) : Hold) :

f(p; q)g); f(p; q)g);

k := k + 1

odf G = D ^ H = E g

2 This algorithm is said to compute D and E layerwise, since it computes the sequences Dk and Ek. The update of G and H in the repetition can be made with another repetition as shown in the program now following.
Algorithm 7.18 (Wood's algorithm | Layerwise computation of D and E):

G; H := D0; E0;

fGoinldv;aHrioaldn;tk:

:= G

;Q = Dk

Q;
^H

0; =

Ek

g

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

203

do Hf G=6 =6 HGoldold!^ H =6 Hold g

Gfoorld(;pH; qol)d if (9
] (8
fi

:= G; H;
: (p; q) 2 a:a2V a:a2V

Hold : (T : (T

!
(p; (p;

a); a);

T T

(q; (q;

a)) a))

2 2

GHoolldd))

! !

G; H
skip

:=

G

f(p; q)g; H n f(p; q)g

rof;

f G = :H g

k := k + 1

odf G = D ^ H = E g

2

The algorithm can be split into two: one computing only D, and the other computing only E. The algorithm computing only E is essentially the algorithm presented by Wood in Wood87, p. 132]. According to Wood, it is based on the work of Moore Moor56].
Its running time is O(jQj3). Brauer uses some encoding techniques to provide an O(jQj2)
version of this algorithm in Brau88], while Urbanek improves upon the space requirements of Brauer's version in Urba89]. None of these variants is given here. The algorithm computing only D does not appear in the literature.
With a little e ort this algorithm can be modi ed to compute Q]E.

7.4.2 Computing D, E, and Q]E by unordered approximation

Instead of computing each Ek (computing E layerwise), we can compute E by considering

pairs of states in an arbitrary order (as outlined in Property 7.10). In the following algo-

rithm, H is G is the set

tohfeaslletpaoifrsalolfpsatiarsteosf(spt;aqt)essu(cph; qt)hsautcfhp;tqhatffapls;qe

.

true at each step; similarly,

Algorithm 7.19:

fG;inHva:r=iaDnt0:; EG0=; :H ^ G D g do (9 p; q; a : a 2 V ^ (p; q) 2 H : (T(p; a); T(q; a)) 2 G) !
let p; q : (p; q) 2 H ^ (9 a : a 2 V : (T(p; a); T(q; a)) 2 G); f (p; q) 2 D g G; H := G f(p; q)g; H n f(p; q)g odf G = D ^ H = E g

2

At each step, the algorithm chooses This algorithm can be split into one

a pair (p; q) 2 H such
computing only D, and

that one

fp;q should not computing only

be E.

true

.

204 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

Remark 7.20: At the end of each iteration step, it may be that H is not an equivalence
relation | see Property 7.10. A slight modi cation to this algorithm can be made by adding the pair (q; p) to H whenever (p; q) is added, and also performing the following
assignment before the od:

H := (MAXv J : J H ^ J = J : J); G := :H

Addition of this assignment makes the algorithm compute the re nement sequence Ek (see

Property 7.10). This assignment may improve the running time of the algorithm if a cheap

method of computing the quanti ed MAX is used. The algorithm with this improvement

does not appear in the literature.

2

Converting the above algorithm to compute Q]E yields the following one (which is also given by Aho, Sethi, and Ullman in ASU86, Alg. 3.6]):

Algorithm 7.21:

Pfdodoi:n=(lPQfve9a:00t:Qr=:QiSQ=a]pE0nP0l;0fti;Q;t:Qnpta1f1Qj;b;Qalpa]eE0:2(:gQQvQQ000P0nf22Q^QvPP000T;n^Q(Q^pQ]Q;1QE00;a0;1a)1Qg)22200^gPQP:1^S^gpa;ali2t2taVVble:^(SQpSl00pi;tlQittat1ba;lbeal()eQ(gQ0;0Q; Q1;1a;)a));!

f f

(P8=Q0Q; Q]E1;ga

:

Q0

2

P

^

Q1

2

P

^

a

2

V

:

:Splittable (Q0; Q1; a))

g

This algorithm has running time O(jQj2).

2

7.4.3 More e ciently computing D and E by unordered approximation
We present another algorithm that considers pairs of states in an arbitrary order. This algorithm (which also computes D) consists of two nested repetitions. It is essentially the same as Algorithm 7.19, with a slight change in loop structure.
Algorithm 7.22:

fG;inHva:r=iaDnt0:;

E0; G=

:H

^

G

Dg

do (9 p; q; a : a 2 V ^ (p; q) 2 H : (T(p; a); T(q; a)) 2 G) !

let p; a : p 2 Q ^ a 2 V ^ (9 q : (p; q) 2 H : (T(p; a); T(q; a)) 2 G);

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

205

for q : (p; q) 2 H ^ (T(p; a); T(q; a)) 2 G ! G; H := G f(p; q)g; H n f(p; q)g
rof odf G = D ^ H = E g

2

This algorithm can also be modi ed to compute only D or only E. At the end of each outer iteration step, it may be that H is not an equivalence relation.
This can be solved with a symmetrical update of H and an assignment to H as can be done in Algorithm 7.19. This algorithm does not appear in the literature.
Modifying the above algorithm to compute Q]E is particularly interesting; the modi ed algorithm will be used in Section 7.4.5 to derive an algorithm (by Hopcroft) which is the best known algorithm for DFA minimization. The modi cation yields:
Algorithm 7.23:

Pfdoi:n=(lve9atQrQiQa]E1n1;0t;a;:a::QQQ]1E1

vP 2P 2P

v ^a

Q2]VE0

^a2V

g :^(9(9QQ0 0:

Q: Q0 022PP:

S: pSlpitlittatbalbel(eQ(Q0;0Q; Q1;1a;)a))));!

ffProooirlfndvQQP:a=000r:=i:P:a=Qn;Ptf0:np2QfjPQp]Eo0l2dgv^QP0SfQp^vli0TtPtna(opQlbd;le00ag;()QQ2000g;QQ11g;;a) !

od f (8 Q0 : Q0 2 P : :Splittable(Q0; Q1; a)) g

f f

(8 Q1; a : P = Q]E

gQ1

2

P

^

a

2

V

:

(8

Q0

:

Q0

2

P

:

:Splittable(Q0; Q1; a)))

g

2

The inner repetition `splits' each (In actuality, some particular Q0

eligible equivalence will not be split by

class (Q1;

aQ) 0ifw:itShplriettsapbelcet(Qto0;pQai1r;

(Q1; a).)

a).

7.4.4 An algorithm due to Hopcroft and Ullman
From the de nition of D, we see that a pair (p; q) is in D if and only if p 2 F 6 q 2 F or there is some a 2 V such that (T(p; a); T(q; a)) 2 D. This forms the basis of the algorithm
considered in this section. With each pair of states (p; q) we associate a set of pairs of states L(p; q) such that
(r; s) 2 L(p; q) ) ((p; q) 2 D ) (r; s) 2 D)

206 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

We start with D0 as our approximation of D. For each pair (p; q) (such that (p; q) 26 D0
| p and q are not already known to be distinguished) we do the following:

If there is an a 2 V such that we know that (T(p; a); T(q; a)) 2 D then (p; q) 2 D. We add (p; q) to our approximation of D, along with L(p; q), and for each (r; s) 2 L(p; q) add L(r; s), and for each (t; u) 2 L(r; s) add L(t; u), etc. If there is no a 2 V such that (T(p; a); T(q; a)) 2 D is known to be true, then for all b 2 V we put (p; q) in the set L(T(p; b); T(q; b)) since (T(p; b); T(q; b)) 2 D ) (p; q) 2 D. If later it turns out that for some b 2 V , (T(p; b); T(q; b)) 2 D, then we
will also put L(T(p; b); T(q; b)) (including (p; q)) in D.

In our presentation of the algorithm, the invariants given are not su cient to prove the correctness of the algorithm, but are used to illustrate the method in which the algorithm works.
Algorithm 7.24:

for (p; q) : (p; q) 2 (Q Q) !

L(p; q) :=

rof;

fGi:n=vaDri0a;nt: G D

^ (8 p; q : (p; q) for (p; q) : (p; q) 26
if (9 a : a 2 V

62D0D!0 : (8 r;
: (T(p; a); T

s: (q;

(r; s)
a)) 2

2 L(p; G) !

q)

:

(p;

q)

2

D

)

(r;

s)

2

D))

g

toadd; added := f(p; q)g; ;

f invariant: toadd D ^ added G ^ toadd \ added =

^ toadd added = ( p; q : (p; q) 2 added : L(p; q)) f(p; q)g g

do toadd =6 !

let (r; s) : (r; s) 2 toadd;

G := G f(r; s)g;

toadd; added := toadd n f(r; s)g; added f(r; s)g;

toadd := toadd (L(r; s) n added)

od

] (8 a : a 2 V : (T(p; a); T(q; a)) 62 G) !

for a 2 V : T(p; a) =6 T(q; a) !

f (T(p; a); T(q; a)) 2 D ) (p; q) 2 D g

L(T(p; a); T(q; a)) := L(T(p; a); T(q; a)) f(p; q)g

rof

fi

roff G = D g

2

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

207

This algorithm has running time O(jQj2) and is given by Hopcroft and Ullman HU79,
Fig. 3.8]. In HU79] it is attributed to Hu man Hu 54] and Moore Moor56]. In their description, Hopcroft and Ullman describe L as mapping each pair of states to a list of pairs of states. The list data-type is not required here, and a set is used here instead.
It is possible to modify the above algorithm to compute E. Such an algorithm does not appear in the literature.

7.4.5 Hopcroft's algorithm to compute Q]E e ciently
We now derive an e cient algorithm due to Hopcroft Hopc71]. This algorithm has also been derived by Gries Grie73]. This algorithm presently has the best known running time analysis of all DFA minimization algorithms.
We begin with Algorithm 7.23. Recall that the inner repetition `splits' each equivalence class Q0 with respect to pair (Q1; a). An observation (due to Hopcroft) is that once all equivalence classes have been split with respect to a particular (Q1; a), no equivalence classes need to be split with respect to the same (Q1; a) on any subsequent iteration step of the outer repetition Hopc71, pp. 190{191], Grie73, Lemma 5]. The observation is simple to prove: the equivalence classes never grow in size, and we need only prove that (for all equivalence classes Q0):
:Splittable(Q0; Q1; a) ) (8 Q00 : Q00 Q0 : :Splittable(Q00 ; Q1; a))
We can use this fact to maintain a set L of pairs, where each pair consists of an equivalence class and an alphabet symbols. We will then split the equivalence classes with respect to elements of L. In the original presentations of this algorithm Hopc71, Grie73], L is a list. As this is not necessary, we retain L's type as a set; in Chapter 11 (User function 11.7) we will see an e cient encoding of L as an array.

P L

:= :=

PQ]E0V;

;

f in^^vLLari=anft:()QQ1;P]Ea)=vj do L =6 !

(PQQ]1vE; agQ) 2]E(0P^

L

V

(P
)^

(9

V) Q0

:

Q0

2

P

:

Splittable (Q0;

Q1;

a))

g

let Q1; a : (Q1; a) 2 L;

LPffooi:rln=dvQQPf:oa=L000rr:=i:nP:a=bQnf;P:t(f0b:Qnp221Qfj;PQVap]Eo)0l2gd!gv;^QP0SfQp^vli0TtPtna(opQlbd;le00ag;()QQ2000g;Q;Q11g;;a) !

if (Q0; b) 2 L ! L := L n f(Q0; b)g f(Q00 ; b); (Q0 n Q00 ; b)g

208 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

]
fi

(Q0;

b)

26

L

!

L

:=

L

f(Q00 ; b); (Q0 n Q00; b)g

rof

rof

odffP(8=QQ0 :]EQg0 2 P : :Splittable(Q0; Q1; a)) g

The innermost update of L is intentionally clumsy and will be used to arrive at the algo-

rithm given aannddA(QnQ0o00th;hbae)sr.

by Hopcroft and Gries.
been split into Q0 n Q00

In the update of set and Q00 then (Q0; b)

L, is

riefp(lQac0e;db)(2inLL()fobrys(oQm0enbQ200 ;Vb))

observation due to Hopcroft is shown in the following lemma (which is taken

from Hopc71, pp. 190{191] and Grie73, Lemma 6]).

Lemma 7.25 (Redundant splitting): Splitting an equivalence class with respect to

aeqnuyivtwaloenocfe(cQla0s;sb)w, i(tQh00r;ebs)p,eacntdto(Qal0l nthQre00e; .b) (where Q00 Q0) is the same as splitting the

Proof:
We only prove that: if an b(Qe 00p;rbo)v, etnheannaitlongeoeudslnyo.t be

esqpuliitvawleitnhcerecslpaescstQ^toh(aQs0bneQen00 ;sbp)l.itTwheithtwroesrpeemcatintoin(gQc0a;sbe)s

and can

:Splittable(Q^; Q0; b) ^ :Splittable(Q^; Q00; b) f De Morgan g
:(Splittable(Q^; Q0; b) _ Splittable(Q^; Q00; b)) f de nition of Splittable g
:((9 p; q : p; q 2 Q^ : T(p; b) 2 Q0 6 T(q; b) 2 Q0) _ (9 p; q : p; q 2 Q^ : T (p; b) 2 Q00 6 T (q; b) 2 Q00)) f combine existential quanti cations g
:(9 p; q : p; q 2 Q^ : (T(p; b) 2 Q0 6 T(q; b) 2 Q0) _ (T(p; b) 2 Q00 6 T(q; b) 2 Q00 )) ) f Q00 Q0 g
:(9 p; q : p; q 2 Q^ : T (p; b) 2 Q0 n Q00 6 T (q; b) 2 Q0 n Q00) f de nition of Splittable g
:Splittable(Q^; Q0 n Q00; b) 2

Given the lemma above, for e ciency reasons we therefore choose the smallest two of
(ttQhhee0ntnhsQprel00ie;tbt(i)cno(gwmhhpaiacsrhiaenlvrgeerajQdisy0js,bmejQaenl00lejd,r)oannteodwLjQi.th0OnnreQstph00 jee)cotitnthoetrh(eQha0un;pbdd),aaitfned(Qofw0;seebat)d2Ld.Le,iItfthh(eQern0(;Qsbp00)l;it26bt)inLogr,

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

209

has not instead.

yet

been

done

and

we

remove

(Q0;

b)

from

L

and

add

(Q00 ; b)

and

(Q0

n

Q00 ; b)

Q.

LAassatlyr,eswuelto, bwseernveeetdhoantlbyyspstliatrtwinitghwreitshpePct=toQei]tEh0e=r (fQQnnFF;;bF)

g we have already split or (F; b) (for all b 2 V )

Hopc71, pp. 190{191], Grie73, Lemma 7]. This gives the algorithm:

Algorithm 7.26 (Hopcroft):

P
if

:=
jF

j

Q]EjQ0;

n

F

j

!

L

:=

fF

g

V

] jFj > jQ n Fj ! L := fQ n Fg V

fi;

f invariant: Q^]LE do L =6 !

v
=

P

v )

PQ]=E0

^L
Q]E

g

(P

V)

let Q1; a : (Q1; a) 2 L;

LPffooi:rln=dvQQPf:oa=L000rr:=i:nP:a=bQnf;P:t(f0b:Qnp221Qfj;PQVap]Eo)0l2gd!gv;^QP0SfQp^vli0TtPtna(opQlbd;le00ag;()QQ2000g;Q;Q11g;;a) !

if
]

((if]fQQi jj00QQ;; bb0000))jj

2 62
>

L ! L :=

L!

jjQQ00

n n

QQ0000 jj

L n f(Q0; ! L := L ! L := L

b)g f(Q00; b); (Q0 ff((QQ000 ;nb)Qg00; b)g

n

Q00;

b)g

fi

rof

rof

odffP(8=QQ0 :]EQg0 2 P : :Splittable(Q0; Q1; a)) g

2 The running time analysis of this algorithm is complicated and is not discussed here. It is shown by both Gries and Hopcroft that it is O(jQj log jQj), Grie73, Hopc71]. A simpler derivation of the running time of this algorithm is given by Keller and Paige in KP95].
A very di erent derivation of this algorithm is given by Keller and Paige in KP95]. In their paper, the algorithm is presented as an example of program derivation in their new framework. Interestingly, their derivation is not only clear, but they also manage to derive a new version which is more space e cient than the one presented here.

210 CHAPTER 7. DFA MINIMIZATION ALGORITHMS
7.4.6 Computing (p; q) 2 E
From the problem of deciding the structural equivalence of two types, it is known that equivalence of two states can be computed recursively by turning the mutually recursive set of equivalences fp;q (from Property 7.10) into a functional program. If the de nition were to be used directly as a functional program, there is the possibility of non-termination. In order for the functional program to work, it takes a third parameter along with the two states.
The following program, similar to the one presented in t-Ei91], computes relation E pointwise; an invocation equiv(p; q; ) determines whether states p and q are equivalent. It assumes that two states are equivalent (by placing the pair of states in S, the third parameter) until shown otherwise.
func equiv(p; q; S) ! if fp; qg 2 S ! eq := true ] fp; qg 62 S ! eq := (p 2 F q 2 F); eq := eq ^ (8 a : a 2 V : equiv(T(p; a); T(q; a); S ffp; qgg)) fi; return eq
cnuf
The 8 quanti cation can be implemented using a repetition
func equiv(p; q; S) ! if fp; qg 2 S ! eq := true ] fp; qg 62 S ! eq := (p 2 F q 2 F); for a : a 2 V ! eq := eq ^ equiv(T(p; a); T(q; a);S ffp; qgg) rof fi; return eq
cnuf
The correctness of this program is shown in t-Ei91]. Naturally, the guard eq can be used in the repetition (to terminate the repetition when eq false) in a practical implementation. This optimization is omitted here for clarity.
There are a number of methods for making this program more e cient. From Section 7.3.3 recall that E = E(jQj?2)max 0. We add a parameter k to function equiv such that

7.4. ALGORITHMS COMPUTING E, D, OR Q]E

211

an invocation equiv(p; q;
tion equiv(p; q; ; (jQj ?

; k) returns (p; q) 2 2) max0) returns (p;

qE) k2aEs

its as

result. It its result.

follows that an invocaThe recursion depth is

bounded by (jQj ? 2) max 0. The new function is

func equiv(p; q; S; k) ! if k = 0 ! eq := (p 2 F q 2 F) ] k =6 0 ^ fp; qg 2 S ! eq := true ] k =6 0 ^ fp; qg 62 S ! eq := (p 2 F q 2 F); for a : a 2 V ! eq := eq ^ equiv(T(p; a); T(q; a);S ffp; qgg; k ? 1) rof fi; return eq
cnuf

The third parameter S is made a global variable, improving the e ciency of this algorithm in practice. As a result, equiv is no longer a functional program in the sense that it now makes use of a global variable. The correctness of this transformation is shown in t-Ei91].
We assume that S is initialized to . When S = , an invocation equiv(p; q; (jQj ? 2) max0) returns (p; q) 2 E; after such an invocation S = . Algorithm 7.27 (Pointwise computation of E):

func equiv(p; q; k) ! if k = 0 ! eq := (p 2 F q 2 F) ] k 6= 0 ^ fp; qg 2 S ! eq := true ] k =6 0 ^ fp; qg 26 S ! eq := (p 2 F q 2 F); S := S ffp; qgg; for a : a 2 V ! eq := eq ^ equiv(T(p; a); T(q; a);k ? 1) rof; S := S n ffp; qgg fi; return eq
cnuf

2

212 CHAPTER 7. DFA MINIMIZATION ALGORITHMS

The procedure equiv can be memoized4 to further improve the running time in practice. This algorithm does not appear in the literature.

7.4.7 Computing E by approximation from below
This latest version of function equiv can be used to compute E and D (assuming IQ is the identity relation on states, and S is the global variable used in Algorithm 7.27):
Algorithm 7.28 (Computing E from below):

S; G; H :=
f invariant:

; G

; IQ; D

^

H

Eg

do (G H) 6= Q Q !

let p; q : (p; q) 2 ((Q Q) n (G H));

if equiv(p; q; (jQj ? 2) max 0) ! H := H f(p; q)g

] :equiv(p; q; (jQj ? 2) max 0) ! G := G f(p; q)g

fi

odf G = D ^ H = E g

2

Further e ciency improvements can be made as follows:

We change
(Q n F)).

the

initialization

of

G

to

G

:=

D0,

equivalently

G

:=

((Q

n

F

)

F)

(F

As in Remark 7.20, we make use of the fact that E = E ; obviously E is symmetrical, halving the required amount of computation | we can update H with the pair (q; p) whenever we add (p; q). H can also be updated at each iteration step by H := H . In Chapter 11 we will describe an implementation of the above algorithm that uses data-structures particularly suited to the -closure operation.

Make use of the facts that

(p; q) 62 E ) (8 r; s : r 2 Q ^ s 2 Q ^ (9 w : w 2 V : T (r; w) = p ^ T (s; w) = q) : (r; s) 62 E)
(p; q) 2 E ) (8 w : w 2 V : (T (p; w); T (q; w)) 2 E)

The rst implication states that if p; q are two distinguished states, and r; s are two
states such that there is w 2 V and T(r; w) = p ^ T(s; w) = q, then r; s are also
distinguished. The second implication states that if p; q are two equivalent states,
4Memoizing a functional program means that the parameters and the result of each invocation are tabulated in memory; if the function is invoked again with the same parameters, the tabulated return value is fetched and returned without recomputing the result.

7.5. CONCLUSIONS

213

and r; s are two states such that there is w 2 V and T(p; w) = r ^ T(q; w) = s,
then r; s are also equivalent. The rst of the two above facts is particularly di cult to implement in practice, since the transition functions would have to be traversed from right to left; this is backwards for most implementations, as is shown in Chapter 11.
Although this algorithm has worse running time than the O(jQj log jQj) of Hopcroft's
algorithm Hopc71, Grie73], in practice the di erence is often not signi cant (see Chapter 15 where a non-memoizing version of equiv was used). This algorithm has a signi cant advantage over all of the known algorithms: although function equiv computes E pointwise
from above (with respect to v, re nement), the main program computes E from below
(with respect to , normal set inclusion5). As such, any intermediate result H in the computation of E is usable in (at least partially) reducing the size of an automaton; all of the other algorithms presented have unusable intermediate results. This property has use in reducing the size of automata when the running time of the minimization algorithm is restricted for some reason (for example, in real-time applications).

7.5 Conclusions
The conclusions about minimization algorithms are: A derivation of Brzozowski's minimization algorithm was presented. This derivation proved to be easier to understand than either the original derivation (by Brzozowski), or the derivations given by Kameda and by van de Snepscheut. A brief history of the minimization algorithm was presented, hopefully resolving some misattributions of its discovery. The de nition of equivalence (relation E) and distinguishability (relation D) as xed points of certain functions proved easier to understand than many text-book presentations. The xed point characterization of E made it particularly easy to calculate an upperbound on the number of approximation steps required to compute E (or D). This upperbound later proved useful in determining the running time of some of the algorithms, and also in making e ciency improvements to the pointwise algorithm. The de nition of E as a greatest xed point helped to identify the fact that all of the (previously) known algorithm computed E from above (with respect to re nement). As such, all of these algorithms have intermediate results that are not usable in minimizing the nite automaton.
5This is set inclusion, as opposed to re nement, since the intermediate result H may not be an equivalence relation during the computation.

214 CHAPTER 7. DFA MINIMIZATION ALGORITHMS
We successfully presented all of the well-known text-book algorithms in the same framework. Most of them were shown to be essentially the same, with minor di erences in their loop structures. One exception was Hopcroft and Ullman's algorithm HU79], which has an entirely di erent loop structure. The presentation of that algorithm (with invariants) in this chapter is arguably easier to understand than the original presentation. Our presentation highlights the fact that the main datastructure in the algorithm need not be a list | a set su ces.
Hopcroft's minimization algorithm Hopc71] was originally presented in a style that is not very understandable. As with Gries's paper Grie73], we strive to derive this algorithm in a clear and precise manner. The presentation in this chapter highlights two important facts: the beginning point for the derivation of this algorithm is one of the easily understood straightforward algorithms; and, the use of a list data-structure in both Hopcroft's and Gries's presentation of this algorithm is not necessary | a set can be used instead.
This chapter presented several new minimization algorithms, many of which were variations on the well-known algorithms. Two of the new algorithms (presented in Sections 7.4.6 and 7.4.7) are not derived from any of the well-known algorithms, and are signi cant in their own right.
{ An algorithm was presented that computes the relation E in a pointwise manner.
This algorithm was re ned from an algorithm used to determine the structural equivalence of types. Several techniques played important roles in the re nement:
The upperbound on the number of steps required to compute E was used to improve the algorithm by limiting the number of pairs of states that need to be considered in computing E pointwise. Memoization of the functional-program portion of the algorithm can be used to reduce the amount of redundant computation.
{ A new algorithm was presented, that computes E from below. This algorithm
makes use of the pointwise computation of E to construct and re ne an approximation of E. Since the computation is from below, the intermediate results of this algorithm are usable in (at least partially) reducing the size of the DFA. This can be useful in applications where the amount of time available for minimization of the DFA is limited (as in real-time applications). In contrast, all of the (previously) known algorithms have unusable intermediate results.

Part III The implementations
215

Chapter 8
Designing and implementing class libraries
In this part of the dissertation, we will consider the design and implementation of class libraries of the algorithms derived in Part II. In this chapter, we brie y discuss some of the issues involved in designing, implementing, and presenting class libraries (or toolkits). The following description of a toolkit is taken from GHJV95, p. 26]:
A toolkit is a set of related and reusable classes designed to provide useful, general-purpose functionality. Toolkits don't impose a particular design on your application; they just provide functionality that can help your application do its job. They are the object-oriented equivalent of subroutine libraries. We will use the terms class library, library, and toolkit interchangeably. We will also use the term client to refer to a program that makes use of classes in the toolkit, or the author of such a program. The important aspects and design goals of a toolkit are: Toolkits do not provide a user interface. (Toolkits that do provide user interfaces should be placed in the category of `application program'.) The classes in the toolkit must have a coherent design, meaning that they are designed and coded in the same style. They have a clear relationship and a logical class hierarchy. The client interface to the library must be easily understood, permitting clients to make use of the library with a minimum of reading. The e ciency of using the classes in the toolkit must be comparable to hand-coded special-purpose routines | the toolkit must be applicable to production quality software. To provide an educational use for the toolkits, and to allow clients to easily modify classes and member functions, the method of implementation must be clear and understandable. The toolkits described in this part are implemented in the C++ programming language, which was chosen because of its widespread availability. E orts were made to refrain from
217

218 CHAPTER 8. DESIGNING AND IMPLEMENTING CLASS LIBRARIES

using obscure features of C++ (such as RTTI or name spaces), or language features not easily found in other object-oriented programming languages (such as multiple-inheritance).
Throughout this part of the dissertation, we assume that the reader is familiar with the C++ language and object-oriented terminology (especially the C++ variety). For the uninitiated, the C++ literature can be divided into three groups: introductions to C++ Lipp91, Stro91], advanced C++ Copl92, SE90], C++ tricks and techniques HN92, MeyS92, Murr93], and (of course) the draft standard. Introductions to object-oriented design and programming can be found in Booc94, Budd91, MeyB88, Tali94, Wein95].
The general process of library design will not be described here, as there is a large body of literature discussing this issue. The following books are of particular relevance:

GHJV95, Souk94] discuss `design patterns' (not related to our pattern matching problem) which are used heavily in library design.

CE95], Stro91, Chapter 13] and Stro94, Chapter 8] provide a general discussion of C++ library design.

MeyB94] is an excellent treatment of the design of a number of loosely coupled libraries in the Ei el programming language. Many of the concepts and techniques discussed in the book are broadly applicable to C++ as well.

Plau95, Teal93] discuss the design and implementation of speci c C++ libraries | the standard C++ library1 and the IOStreams (input and output) class libraries respectively.

The toolkit related terms that we will use in this part are de ned as follows.

De nition 8.1 (Toolkit terminology): We de ne the following types of classes:

User: A class intended for use by a client program.

Client: A class de ned in the client program.

Implementation: A class de ned in the toolkit for exclusive use by the toolkit. The class
is used to support the implementation of the client classes.

Foundation: Those implementation classes which are simple enough to be reused in other
(perhaps unrelated) class libraries.

Interface: An abstract (pure virtual) class which is declared to force a particular public
interface upon its inheritance descendants.

Base: An inheritance ancestor of a particular class.

Derived: An inheritance descendant of a particular class.

Note that the terms base and derived are relative.

2

1Plauger's book considers the implementation of an early, and now defunct, draft of the standard library

8.1. MOTIVATIONS FOR WRITING CLASS LIBRARIES
8.1 Motivations for writing class libraries

219

There are a number of motivations for creating the class libraries (which will be described in Chapters 9, 10, and 11):

Until now, few general purpose toolkits of pattern matchers or nite automata construction algorithms existed. The nite automata toolkits that do exist are not intended for general use in production quality software.

The level of coherence normally required to implement a toolkit was not previously possible. The literature on pattern matching algorithms was scattered and in some places incomplete. With the construction of the taxonomies, all of the algorithms are described in a coherent fashion, allowing us to base the class library structures on the taxonomy structure.

The uniformity of implementation that was possible (given the taxonomies) had two important e ects:
{ Clients need not examine the source code in order to make a decision on which
class to use; the quality of the implementations of each of the pattern matchers is roughly the same.
{ Uniformity gives greater con dence in the accuracy of relative performance com-
paring di erent algorithms (as is presented in Part IV of this dissertation).

The toolkits and the taxonomies can serve as examples of implementation techniques for class libraries; in particular methods for organizing template classes2 and class hierarchies.

Implementing the abstract algorithm can be painless and fun, given the taxonomy presentation of the algorithms and their correctness arguments.

8.2 Code sharing
One of the main aims of object-oriented programming is that it permits, and even encourages, code sharing (or code reuse). The code reuse in object-oriented programming corresponds neatly with the factoring of common parts of algorithms in the taxonomies.
Although code sharing can be achieved in a number of ways, in this section we discuss four techniques which could have been used in the design of the toolkits. The rst discussion centres around the use of base classes (with virtual member functions) versus templates. The second discussion concerns the use of composition versus protected inheritance.
2We use the term template class, as opposed to class template suggested by Carroll and Ellis in CE95]. Our choice was made to correspond to the term generic class used in some other object-oriented programming languages.

220 CHAPTER 8. DESIGNING AND IMPLEMENTING CLASS LIBRARIES
8.2.1 Base classes versus templates
A number of the pattern matching objects have common functionality, and it seems wasteful to duplicate the code in each of the speci c types of pattern matchers.
The obvious design involves creating a new base class and factoring the common code into the base class. Each of the pattern objects would then inherit from this base, and provide speci c virtual member functions to obtain the desired functionality. For example, the Commentz-Walter algorithms all share a common algorithm skeleton; they each have speci c shift functions. We could create a CW base class with the functionality of the skeleton, and provide a virtual `shift distance' member function to obtain the CommentzWalter variants.
The advantage of this approach is its elegance. It provides a relatively easy to understand class hierarchy, which largely re ects the structure of the taxonomy presented in Chapter 4. Furthermore, a member function which takes (as parameter) a pointer to a CW object need not know which particular variant (of a CW object) the pointer points to, only that the CW object satis es the general CW functionality. This solution provides code reuse at both the source language and executable image levels. The disadvantage is that it would require a virtual function call for every shift. Indeed, if the same technique was used to factor the common code from the Aho-Corasick variants, it would require a virtual function call for every character of the input string.
The other approach is to create a template class CW, which takes a `shifter class' as its template (type) parameter. We would then provide a number of such shifter classes, for use as template parameters | each giving rise to one of the Commentz-Walter variants. The primary advantage of this approach is that it is e cient: when used to implement the Aho-Corasick algorithms, each character in the input string will require a non-virtual function call (which may be inlined, unlike virtual function calls). The disadvantages are twofold: pointers to the variants of the CW algorithms are not interchangeable, and code will be generated for each of the CW variants. The code reuse is at the source level, and not at the executable image level.
It is expected that few clients of the toolkits will instantiate objects of di erent CW classes, for example. A programmer writing an application using pattern matching is more likely to choose a particular type of pattern matcher, as opposed to creating objects of various di erent types. For this reason, the advantages of the template approach are deemed to outweigh its disadvantages, and we prefer to use it over base classes in the toolkits.
8.2.2 Composition versus protected inheritance
Composition (sometimes called the has-a relationship) and protected inheritance (sometimes called the is-a relationship) are two additional solutions to code sharing. We illustrate the di erences between these two solutions using an example. When implementing a Set class, we may wish to make use of an already-existing Array class. There are two ways to do this: protected inheritance and composition.

8.3. CODING CONVENTIONS AND PERFORMANCE ISSUES

221

With protected inheritance, class Set inherits from Array in a protected way. Class Set still gets the required functionality from Array, but the protected inheritance prevents the is-a relation between Set and Array (that is, we cannot treat a Set as an Array). The advantage of this approach is that it is elegant, and it is usually the approach taken in languages such as Smalltalk and Objective-C Budd91]. The disadvantage is that the syntax of C++ places the inheritance clause at the beginning of the class declaration of Set, making it plain to all clients of Set that it is implemented in terms of Array. Furthermore, protected inheritance (and indeed private inheritance) is one of the rarely-used corners of C++, and it is unlikely to be familiar to the average programmer MeyS92, Murr93].
In a composition approach, an object of class Set has (in its private section) an object of class Array. The Set member functions invoke the appropriate member functions of Array to provide the desired functionality. The advantage of this approach is that it places all implementation details in the private section of the class de nition. The disadvantage is that it deviates from the accepted practice of inheriting for implementation in some other languages. It is, however, the standard approach in C++. At rst glance, it would appear that composition can lead to some ine ciency: in our example, an invocation of a Set member function would, in turn, call an Array member function. These extra function calls, usually called pass-throughs, are frequently eliminated through inlining.
There are no e ciency-based reasons to choose one approach over the other. For this reason, we arbitrarily choose composition because of the potential readability and understandability problems with protected inheritance.

8.3 Coding conventions and performance issues
At this time, coding in C++ presents at least two problems: the language is not yet stable (it is still being standardized) and, correspondingly, the standard class libraries are not yet stable.
In designing the libraries, every e ort was made to use only those language features which are well-understood, implemented by most compilers and almost certain to remain in the nal language. Likewise, the use of classes from the proposed standard library, or from the Standard Template Library SL94], was greatly restricted. A number of relatively simple classes (such as those supporting strings, arrays, and sets) were de ned from scratch, in order to be free of library changes made by the standardizing committee. A future version of the toolkits will make use of the standard libraries once the International Standards Organization has approved the C++ standard.
In the object-oriented design process, it is possible to go overboard in de ning classes for even the smallest of objects | such as alphabet symbols, and the states of a nite automaton. In the interests of e ciency, we draw the line at this level and make use of integers for such basic objects.
Almost all of the classes in the toolkits have a corresponding class invariant member function, which returns TRUE if the class is structurally correct, and FALSE otherwise. Structural invariants have proven to be particularly useful in debugging and in understand-

222 CHAPTER 8. DESIGNING AND IMPLEMENTING CLASS LIBRARIES ing the code (the structural invariant is frequently a good rst place to look when trying to understand the code of a class). For this reason, they have been left in the released code (they can be disabled as described in the next section).
We use a slightly non-traditional way of splitting the source code into les. The public portion of a class declaration is given in a .hpp le, while the private parts are included from a .ppp le. There is a corresponding .cpp le containing all of the out-of-line member function de nitions. A .ipp le contains member functions which can be inlined for performance reasons. By default the member functions in the .ipp le are out-of-line. The inlining can be enabled by de ning the macro .INLINING To implement such conditional inlining, the .ipp le is conditionally included into the .hpp or the .cpp le. The inlining should be disabled during debugging or for smaller executable images.
8.3.1 Performance tuning
The algorithms implemented in the taxonomy are already highly tuned from an algorithmic point of view. Clients that nd the performance inadequate should take the following steps in order until the performance is su cient:
1. Ensure that assertions are disabled (by de ning the NDEBUG macro | see ISO90, Section 7.2]).
2. Enable appropriate compiler optimizations. 3. De ne macro INLINING to obtain inlining of member functions. 4. Pro le the code to determine `hot-spots'. 5. Inline any out-of-line functions whose call-overhead is contributing to the hot-spots.
6. De ne and use special versions of the new and delete operators for the classes that
make extensive use of heap memory (and are contributing to the hot-spots). 7. If a lot of time is spent in copy constructors, convert the o ending class to make use
of use-counting. 8. When using a class with one or more virtual member functions, and the virtual func-
tion calls are causing hot-spots: atten the class hierarchy, eliminating the virtuality of the functions and convert calls to them into normal (non-virtual) function calls. (Note that this is a last resort, since every e ort has been made to reduce the number of virtual function calls, and it involves the modi cation of the library source code.) 9. Contact me.

8.4. PRESENTATION CONVENTIONS
8.4 Presentation conventions

223

In the following chapters, some of the source code of the toolkits will be presented. In the presentation of a particular source le, we typeset the program variables in a roman shape. When the same variable is discussed in running text, we typeset it in an italic shape.
Since composition and templates are preferred over inheritance, very little inheritance is present in the toolkits. As a result, we will not present any inheritance graphs in this dissertation.
Each class is presented in a standard format. The following items appear in the description of a class:

1. The name of the class and whether it is a user-level class (one for use by clients), or an implementation class (for use by other classes within the toolkit).
2. The Files clause lists the les in which the class declaration and de nition are stored.
The names of the les associated with a class are usually the class name, followed by .hpp for interface (or header) les, .ppp for private section les, .ipp for inline member function de nitions, and .cpp for out-of-line de nition les. In this clause, we only mention the root le name (without the su x). All le names are short enough for use under MS-Dos.
3. The Description clause gives a brief description of the purpose of the class. 4. The optional Implementation clause outlines the implementation of the class. 5. The optional Performance clause gives some suggestions on possible performance
improvements to the implementation of the class. 6. The description ends with the 2 symbol.

224 CHAPTER 8. DESIGNING AND IMPLEMENTING CLASS LIBRARIES

Chapter 9
SPARE Parts: String PAttern REcognition in C++
This chapter contains a description of a C++ pattern matching toolkit known as the SPARE Parts (String PAttern REcognition). Both the client interface and aspects of the design and implementation are considered.
9.1 Introduction and related work
The SPARE Parts is the second generation string pattern matching toolkit from the Eindhoven University of Technology. The rst toolkit (called the Eindhoven Pattern Kit, written in C, and described in Wat94a, Appendix A]) is a procedural library based upon the original taxonomy of pattern matching algorithms WZ92]. Experience with the toolkit revealed a number of de ciencies, detailed as follows. The rudimentary and explicit memory management facilities in C caused a number of errors in the code, and made it di cult to perform pattern matching over more than one string simultaneously (in separate threads of the program) without completely duplicating the code. While the performance of the toolkit was excellent, some of the speed was due to sacri ces made in the understandability of the client interface.
There are other existing pattern matching toolkits, notably the toolkit of Hume and Sunday HS91]. Their toolkit consists of a number of implementations of Boyer-Moore type algorithms | organized so as to form a taxonomy of the Boyer-Moore family of algorithms. Their toolkit was primarily designed to collect performance data on the algorithms. As a result, the algorithms are implemented (in C) for speed and they sacri ce some of the safety that would normally be expected of a general toolkit. Furthermore, the toolkit does not include any of the non-Boyer-Moore pattern matching algorithms (other than a brute-force pattern matcher) | most noticeably, there are no multiple keyword pattern matchers.
The SPARE Parts is a completely redesigned and object-oriented implementation of the algorithms appearing in Chapter 4. The SPARE Parts is designed to address the shortcomings of both of the toolkits described above. The following are the primary features of the library:
225

226 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
The design of the SPARE Parts follows the structure of the taxonomy in Chapter 4 more closely. As a result, the code is easier to understand and debug. In addition, the SPARE Parts includes implementations of almost all of the algorithms described in Chapter 4. The use of C++ (instead of C) for the implementation has helped to avoid many of the memory management-related bugs that were present in the original toolkit. The client interface to the toolkit is particularly easy to understand and use. The exibility introduced into the interface does not reduce the performance of the code in any signi cant way. The toolkit supports multi-threaded use of a single pattern matching object. The toolkit is presented largely in a top-down fashion | from the client level classes to the foundation classes. The reader is assumed to have an overview of the taxonomy given in Chapter 4. Chapter 9 is structured as follows: Section 9.2 gives an introduction to the client interface of the toolkit. It includes some examples of programs which use the SPARE Parts. Section 9.3 describes the design decisions that lead to the client interface de ned (with the use of abstract classes) in the toolkit. The design and implementation of concrete classes (implementing the client interface) is outlined in Section 9.4. Section 9.5 outlines the design and implementation of the foundation classes. Section 9.6 presents some experiences with the toolkit and the conclusions of this chapter. Some information on how to obtain and compile the toolkit is given in Section 9.7. This chapter can also be read e ectively with the source code of the toolkit.
9.2 Using the toolkit
In this section, we describe the client interface of the toolkit and present some examples of programs using the toolkit. The design issues that lead to the current client interface are not described here, but rather in Section 9.3.
The client interface de nes two types of abstract pattern matchers: one for single keyword pattern matching, and one for multiple keyword pattern matching. (A future version of SPARE Parts can be expected to include classes for regular expression pattern matching | for example, an implementation of the algorithm described in Chapter 5.) All of the single keyword pattern matching classes have constructors which take a keyword.

9.2. USING THE TOOLKIT

227

Likewise, the multiple keyword pattern matchers have constructors which take a set of keywords. Both types of pattern matchers make use of call-backs (to be explained shortly) to register matched patterns. In order to match patterns using the call-back mechanism, the client takes the following steps (using single keyword pattern matching as an example):
1. A pattern matching object is constructed (using the pattern as the argument to the constructor).
2. The client calls the pattern matching member function PMSingle::match, passing
the input string and a pointer f to a client de ned function which takes an int and returns an int1. (This function is called the call-back function.)
3. As each match is discovered by the member function, the call-back function is called; the argument to the call is the index (into the input string) of the symbol immediately to the right of the match. (If there is no symbol immediately to the right, the length of the input string is used.)
4. If the client wishes to continue pattern matching, the call-back function returns the constant TRUE, otherwise FALSE.
5. When no more matches are found, or the call-back function returns FALSE, the member function PMSingle::match returns the index of the symbol immediately to the right of the last symbol processed.
We now consider an example of single keyword pattern matching.
Example 9.1 (Single keyword matching): The following program searches an input
string for the keyword hisher, printing the locations of all matches along with the set of matched keywords:

#include "com-misc.hpp" #include "pm-kmp.hpp"
#include <iostream.h>

static int report( int index ) f
cout << index << \n ; return( TRUE );
g

int main( void ) f
auto PMKMP Machine( "hisher" ); Machine.match( "hishershey", &report );

10

1The integer return value is a Boolean value; recall that TRUE and FALSE have type int in C and C++. A recent draft of the C++ standard indicates that bool will be a new type (and a new keyword); the compilers used in the development of the SPARE Parts do not support this yet.

228 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++

return( 0 );
g

The header le com-misc.hpp provides a de nition of constants TRUE and FALSE.

Header le pm-kmp.hpp de nes the Knuth-Morris-Pratt pattern matching class, while

header le iostream.h de nes the input and output streams, including the standard out-

put cout. Function report is our call-back function, simply printing the index of the match

(to the standard output), and returning TRUE to continue matching. The main function

(the program mainline) creates a local KMP machine, with keyword hisher. The machine

is then used to nd all matches in string .hishershey (Recall that, in C and C++, a

pointer to the beginning of the string is passed to member match, as opposed to the entire

string.)

2

In addition to the KMP algorithm de ned in ,pm-kmp.hpp other single keyword pattern matchers are de ned in header le ,bms.hpp which contains suggestions for instantiating some of the Boyer-Moore variants. Additionally, a brute-force single keyword pattern matcher is de ned in .pm-bfsin.hpp
Multiple keyword pattern matching is performed in a similar manner, as the following example shows.
Example 9.2 (Multiple keyword matching): The following program searches an input
string for the keywords his, her, and she, printing the locations of all matches:

#include "com-misc.hpp" #include "string.hpp" #include "set.hpp" #include "acs.hpp"
#include <iostream.h>

static int report( int index, const Set<String>& M ) f
cout << index << M << \n ; return( TRUE );
g

10

int main( void ) f
auto Set<String> P( "his" ); P.add( "her" ); P.add( "she" ); auto PMACOpt Machine( P ); Machine.match( "hishershey", &report );
return( 0 );
g

Header le string.hpp de nes a string class, while set.hpp de nes a template class for sets of objects. Header le acs.hpp de nes the Aho-Corasick pattern matching classes. Function report is our call-back function, simply printing the index of the match (to the

9.2. USING THE TOOLKIT

229

standard output) and the set of keywords matching, and returning TRUE to continue

matching. Note that the call-back function has a di erent signature for multiple keyword

pattern matching: it takes the index of the symbol to the right of the match, and the set

of keywords matching with index as their right end-point.

The main function (the program mainline) creates a local AC machine from the keyword

set. The machine is then used to nd all matches in string .hishershey

2

In the following two sections, we consider ways to use the SPARE Parts more e ciently in certain application domains.

9.2.1 Multi-threaded pattern matching
One important design feature (as a result of the call-back client interface) of the SPARE Parts is that it supports multi-threading. This can lead to high performance in applications hosted on multi-threading operating systems. For example, consider an implementation of a keyword grep application, in which 1000 les are to be searched for occurrences of a given keyword. The following are three potential solutions:
In a sequential solution, a single pattern matching object is constructed and each of the 1000 les are scanned (in turn) for matches. In a na ve multi-threaded solution, 1000 threads are created (each corresponding to one of the input les). Each of the threads construct a pattern matching object, which is then used to search the le. An e cient solution is to create a single matching object, with 1000 threads sharing the single object. Each of the threads proceeds to search its le, using its own invocation of member function PMSingle::match. The last (most e cient) solution would not have been possible without the call-back client interface. The technical reasons why this is possible are considered further in Section 9.3.

9.2.2 Alternative alphabets
The default structure in the SPARE Parts is to make use of the entire ASCII character set as the alphabet. This can be particularly ine cient and wasteful in cases where only a subset of these letters are used. For example, in genetic sequence searching, only the letters a, c, g, and t are used. The SPARE Parts facilitates the use of smaller alphabets through the use of normalization. Header le alphabet.hpp de nes a constant ALPHABETSIZE (which, by default is CHAR MAX). The alphabet which SPARE Parts uses is the range 0; ALPHABETSIZE ). An alternative alphabet can be used by rede ning ALPHABETSIZE, and mapping the alternative alphabet in the required range. The mapping is performed by functions alphabetNormalize and alphabetDenormalize, both declared in alphabet.hpp (by default, these functions are the identity functions). The only requirement is that the functions map 0 to 0 (this is used to identify the end of strings).

230 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++

Example 9.3 (Genetic sequence alphabet): In the genetic sequence example, we
would make use of the following version of header :alphabet.hpp

#include <assert.h> #de ne ALPHABETSIZE 5

inline char alphabetNormalize( const char a ) f

switch( a ) f

case 0:

return( 0 );

case a : return( 1 );

case c : return( 2 );

case g : return( 3 );

case t : return( 4 );

default: assert( !"Non-genetic character" );

g

g

10

inline char alphabetDenormalize( const char a ) f

switch( a ) f

case 0:

return( 0 );

case 1:

return( a );

case 2:

return( c );

case 3:

return( g );

case 4:

return( t );

default: assert( !"Non-genetic image" );

g

g

20
2

9.3 Abstract pattern matchers
In this section, we brie y consider the two abstract pattern matching classes which are used to de ne the client interfaces of single and multiple keyword pattern matchers.
User class 9.4 (PMSingle) Files: pm-singl Description: Class PMSingle de nes the call-back client interface outlined in Exam-
ple 9.1. The brute-force, Knuth-Morris-Pratt, and Boyer-Moore classes implement the de ned interface.
Implementation: As an abstract class, there is no implementation.
2

9.4. CONCRETE PATTERN MATCHERS

231

User class 9.5 (PMMultiple) Files: pm-multi Description: This class de nes the call-back client interface outlined in Example 9.2.
The brute-force, Aho-Corasick, and Commentz-Walter classes implement the de ned interface.
Implementation: As an abstract class, there is no implementation.
2
The use of call-backs in the client interface warrants some further explanation. As mentioned in Section 9.2, the use of call-backs allows multiple threads to make use of a single pattern matching object. Let us consider another possible (perhaps more obvious) client interface. In the alternative interface, member functions are provided to:
Restart the matcher with a new input string
Determine if there is a valid match
Return the location of the current match
Advance to the next match The pattern matcher must contain state information such as: a pointer to the input string, the index of the current match, and a Boolean variable indicating if there is a valid match. Since all of this information is contained in a pattern matcher, multi-threaded use of a single object is not possible.
Although the call-back client interface must maintain the same state information, the information is stored in variables local to the match member function. Each thread making use of a pattern matcher has its own invocation of match, and therefore its own state information.
The call-back interface is not without its disadvantages. The most noticeable one is that call-backs require the client to write a function (in particular a free-standing function, as opposed to a member function of some other class) for use as the call-back function. This requirement may force the client to adopt a design approach that is not entirely objectoriented (due to the free-standing function). In practice, this disadvantage has proven to be relatively minor compared to the gains.

9.4 Concrete pattern matchers
In this section, we describe the classes which implement the interface de ned by classes PMSingle and PMMultiple. The treatment of each of the families of classes includes any auxiliary (non-foundation) classes used. A summary of the classes and their template parameters (if any) is given in Section 9.4.6. We rst consider the brute-force pattern matchers, followed by the KMP, AC, CW, and BM pattern matchers.

232 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
9.4.1 The brute-force pattern matchers
The brute-force pattern matchers are the most basic of the classes. While they are easy to understand, they are not intended for use in production quality software; they are used for benchmarking the other pattern matchers.
User class 9.6 (PMBFSingle, PMBFMulti) Files: ,pm-bfsin pm-bfmul Description: The brute-force pattern matchers are na ve implementations of pattern
matchers. They are only intended to form a baseline, against which the other (more e cient) classes can be measured. As a result, they are not intended for serious use.
Implementation: The implementations correspond (roughly) to Algorithm 4.10. Performance: Instead of improving the performance of these classes, the client should
make use of one of the other pattern matcher classes. 2
9.4.2 The KMP pattern matcher
User class 9.7 (PMKMP) Files: pm-kmp Description: This pattern matcher implements the Knuth-Morris-Pratt single keyword
pattern matching algorithm. It inherits from PMSingle and implements the interface de ned there.
Implementation: This class maintains the pattern keyword and a FailIdx representing
the indexing failure function. The implementation of member function match corresponds to Algorithm 4.84.
2
Implementation class 9.8 (FailIdx) Files: failidx Description: Class FailIdx is the indexing failure function for use in PMKMP. The con-
structor takes a keyword. The only interesting member function is one to apply the failure function.
Implementation: The class contains an array (of size jpj + 1 for keyword p) of integers,
representing the function. The constructor implements the classic KMP precomputation | see Wat94a, Appendix A].
2

9.4. CONCRETE PATTERN MATCHERS

233

9.4.3 The AC pattern matchers
We now consider the Aho-Corasick family of pattern matchers. As the derivation in Section 4.3 shows, all of the Aho-Corasick variants share the same algorithm skeleton. The primary di erence is in the mechanism used to compute the transition function on an input symbol. For speed, the AC algorithm skeleton is implemented via a template class as opposed to a base class (see Section 8.2.1). A number of variants (instantiations) of the Aho-Corasick objects are declared in the header ,acs.hpp which is intended for client use.
Implementation class 9.9 (PMAC) Files: pm-ac Description: Template class PMAC implements the skeleton of the AC algorithms. The
template argument must be one of the ACMachine: : : classes (called a transition machine) which is used to compute the next transition. The class inherits from PMMultiple and implements the corresponding interface. The header le is not intended to be used directly by clients; use acs.hpp instead.
Implementation: The implementation contains an ACMachine: : : object. The PMAC
constructor passes the keyword set to the transition machine constructor. The implementation of member function match is taken from Algorithm 4.47.
Performance: The class is already highly tuned. The member functions of the transition
machine (the template argument) should be inline for high performance. 2

9.4.3.1 AC transition machines and auxiliary classes
The transition machines are used in the AC skeleton (template class PMAC). The variety of transition machines corresponds to the di erent methods of computing the next transition discussed in Section 4.3.
Implementation class 9.10 (ACMachineOpt) Files: acmopt Description: This class provides an implementation of the optimized Aho-Corasick tran-
sition function, as described in Section 4.3.2. Member functions are provided to compute the next state (make a transition) and to compute the output (matched keywords) of a particular state (these member functions are the minimum interface required by template PMAC). It implements function f and Output (see De nitions 4.49 and 4.44).
Implementation: The class contains a Gamma and an ACOutput. The member functions
are pass-throughs to these classes.

234 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
2
Implementation class 9.11 (ACMachineFail) Files: acmfail Description: Class ACMachineFail implements the Aho-Corasick failure function method
of computing the next transition, as described in Section 4.3.5.
Implementation: The class contains an EFTrie (an extended trie function ef) and an
ACOutput. The computation of the transition function is done with a linear search, as detailed in Section 4.3.5. The output member function is simply a pass-through to the ACOutput.
2
Implementation class 9.12 (ACMachineKMPFail) Files: acmkmpfl Description: Class ACMachineKMPFail is an implementation of the (multiple keyword)
Knuth-Morris-Pratt method of computing the next transition, as described in Section 4.3.6.
Implementation: The class contains an FTrie and an ACOutput. The transition function
is computed by linear search | see Section 4.3.6.
Performance: Most ine ciencies are due to the di erence between this class and ACMa-
chineFail, as mentioned in the comment after Algorithm 4.76 on page 75. 2
Implementation class 9.13 (Gamma) Files: acgamma Description: Class Gamma implements the `optimized' Aho-Corasick transition function
f. The constructor takes an FTrie and an FFail. The main member function computes the image of the function, given a State and a character.
Implementation: Class Gamma is implemented via a StateTo< SymbolTo<State> >.
The constructor performs a breadth- rst traversal of the trie, using the failure function to compute function f.
Performance: There are methods of computing function f directly from the keyword
set without the FTrie. Computing the trie and the failure function independently is more costly (in both space and time), but provides a modular separation of the functions and keeps the constructor for Gamma manageable.

9.4. CONCRETE PATTERN MATCHERS

235

2
Implementation class 9.14 (EFTrie) Files: aceftrie Description: Class EFTrie implements the extended forward trie | function ef (see
De nition 4.68). The constructor takes an FTrie.
Implementation: This class is implemented with a StateTo< SymbolTo<State> >. The
constructor simply copies the FTrie and extends it.
Performance: The performance could be signi cantly increased by not copying the trie
from scratch. Unfortunately, this is not possible because a single FTrie is used to construct the EFTrie and the ACOutput objects in ACMachineFail so the States of the EFTrie and the ACOutput objects correspond.
2
Implementation class 9.15 (ACOutput) Files: acout Description: ACOutput implements the Aho-Corasick output function Output (see Def-
inition 4.44). The constructor takes the set of keywords, the corresponding forward trie (FTrie), and the corresponding forward failure function (FFail).
Implementation: This class contains a StateTo< Set<String> >. The constructor per-
forms a breadth- rst traversal of the trie, using the failure function to compute function Output | implementing the algorithm given in WZ92].
Performance: The high performance of this class depends quite heavily upon the use-
counting of class String (see User class 9.34). 2

9.4.4 The CW pattern matchers
As outlined in Section 4.4, all of the Commentz-Walter variants share a common algorithm skeleton. The di erence lies in how the safe shift distance is computed. For this reason, the skeleton is de ned as a template class as follows. The variants of the Commentz-Walter
algorithm are de ned (via typedef) in header .cws.hpp
User class 9.16 (PMCW) Files: pm-cw

236 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
Description: Class PMCW implements the Commentz-Walter skeleton. It inherits from
PMMultiple and implements the public interface de ned there. The template argument must be one of the CWShift: : : classes. The argument provides the safe shift distance during the scanning of the string.
Implementation: A PMCW contains a shifter object, an RTrie (a reverse trie for scan-
ning the string), and a CWOutput (output function for detecting a match). The constructor passes the set of keywords through to the sub-objects. The implementation of member function match is taken directly from Algorithm 4.93.
Performance: Improvements in performance can most easily be gained by improving the
shifter objects or the implementation of the reverse trie and output function. The member functions of the safe shift objects should be inline, since they are simple and they are called repeatedly in the inner repetition of match.
2
9.4.4.1 Safe shifters and auxiliary functions
The safe shifter classes form the basis of the Commentz-Walter algorithms. The derivation of the safe shifts is covered in Section 4.4. The choice of which one to use in a given application is dominated by a tradeo between precomputation time and greater shift distances. For applications in which the input string is relatively short, class CWShiftNLA has the fastest precomputation but provides the smallest shift distances. (Actually, CWShiftNaive provides a shift distance of 1; it is intended only for use in benchmarking the algorithms.) For an application in which the time to scan the string outweighs the time for precomputation, CWShiftRLA is the best choice.
Implementation class 9.17 (CWShiftNaive) Files: cwshnaiv Description: CWShiftNaive implements a na ve safe shift distance of 1 in the Commentz-
Walter algorithm. This class is intended for benchmarking use, as opposed to serious applications.
Implementation: The implementation is trivial since no data is stored. The shift distance
member function simply returns 1. 2
Implementation class 9.18 (CWShiftNLA) Files: cwshnla Description: This class implements the `no-lookahead' shift distance of De nition 4.101.

9.4. CONCRETE PATTERN MATCHERS

237

Implementation: The class contains a single shift function. In the constructor, local D1
and D2 shift functions are constructed. These shift functions are combined into a single shift. In the description in 4.4 the combining of the two would be done as the input string is scanned. They are combined at precomputation time for performance and space reasons.
Performance: The performance would be di cult to improve as the D1 and D2 shift
functions are already combined at precomputation time. 2
Implementation class 9.19 (CWShiftWBM) Files: cwshwbm Description: Class CWShiftWBM implements the `weak Boyer-Moore' shift distance |
see De nition 4.136.
Implementation: The implementation is through D1, D2, and CharBM. The amount of
shift distance contributed by each of the three functions are combined as the input string is scanned.
Performance: Since the shift distance is combined at string-scanning time, it is important
that auxiliary functions such as min and max are inline functions. 2

Implementation class 9.20 (CWShiftNorm) Files: cwshnorm Description: This class implements the `normal' Commentz-Walter shift function | see
De nition 4.125.
Implementation: The implementation is similar to that of CWShiftWBM, with the ex-
ception that the CharBM is replaced by a CharCW.
Performance: See the performance clause for CWShiftWBM.
2
Implementation class 9.21 (CWShiftOpt) Files: cwshopt Description: Class CWShiftOpt is an implementation of the `optimized'Commentz-Walter
shift distance | see De nition 4.107.

238 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
Implementation: The implementation is similar to that of CWShiftWBM, except that
the D1 and CharBM are replaced by a DOpt.
Performance: See the performance clause for CWShiftWBM.
2
Implementation class 9.22 (CWShiftRLA) Files: cwshrla Description: This class implements the `right lookahead' shift function as de ned in
De nition 4.140.
Implementation: The implementation is similar to CWShiftOpt, with the addition of a
CharRLA.
Performance: See the performance clause for CWShiftWBM.
2
Implementation class 9.23 (CharCW, CharBM, CharRLA) Files: , ,cwchar cwcharbm cwcharrl Description: These three classes are the shift functions which are based upon a single
character (the rst mismatching character). The de nitions of the functions can be found (respectively) in De nitions 4.122, 4.128, and 4.138.
Implementation: Classes CharBM and CharRLA are implemented through a single ar-
ray, while CharCW is implemented through two nested arrays. Their constructors all make breadth- rst traversals of the reverse trie. The shift member functions are trivial lookups.
2
Implementation class 9.24 (D1, D2, DOpt) Files: cwd1, cwd2, cwdopt Description: These three classes implement the Commentz-Walter shift component func-
tions d1, d2, and dopt (see De nitions 4.97 and 4.105). Classes D1 and D2 map a State to an integer, while DOpt maps a State and a character to an integer.
Implementation: The implementations of D1 and D2 are in terms of StateTo< int >, with DOpt as StateTo< SymbolTo< int > >. The constructors take an RTrie which
they traverse, implementing Algorithms 5.27 and 5.28 (for D1 and D2) and the dopt precomputation algorithm given in WZ95].

9.4. CONCRETE PATTERN MATCHERS

239

Performance: The derivations of the precomputation algorithms in WZ95] indicate that
the currently implemented algorithms are not likely to be improved. 2
Implementation class 9.25 (CWOutput) Files: cwout Description: CWOutput implements the Commentz-Walter output function (which de-
termines if a match has been found). The class maps a State (from the reverse trie) to a string if the State corresponds to a keyword. There is a member function which reports if a given State corresponds to a keyword.
Implementation: The implementation uses a StateTo<String*>. If the entry is 0, then
the corresponding State does not correspond to a keyword. If the entry is not 0, the entry points to the corresponding keyword. The constructor takes an EFTrie and the set of keywords.
2

9.4.5 The BM pattern matchers
Like the Aho-Corasick and the Commentz-Walter algorithms, all variants of the BoyerMoore algorithm share a common skeleton. Again, the skeleton is implemented as a template class, with the template parameters being used to instantiate the di erent possible variants. Examples of how to instantiate some of the variants are given in header le .bms.hpp The structure of these variants of the BM algorithms are derived fully in Section 4.5.
User class 9.26 (PMBM) Files: pm-bm Description: This template function implements the Boyer-Moore variants derived in
Section 4.5. It inherits from PMSingle and implements the public interface de ned there. The class takes three template parameters:
A `match order' which is used to compare the keyword to the input string. It must be one of the STrav: : : classes. A `skip loop' which is used to skip portions of the input text that cannot possibly contain a match. The argument must be one of the SL: : : classes. A `match information' shift distance class which is used to make larger shifts through the input string, after a match attempt. The argument must be one of the BMShift: : : classes.

240 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++

For more on each of these three components, see Section 4.5.
Implementation: The implementation contains a copy of the keyword and an object of
each of the template arguments. The implementation is taken directly from Algorithm 4.177.
Performance: A fast implementation relies on the skip loop, match order, and shifter
member functions all being inline. 2
The Boyer-Moore class PMBM takes three template arguments, making it one of the more complex template classes. We now consider an example of an instantiation of the class.
Example 9.27 (Instantiating template class PMBM): The rst of the template ar-
guments is the match order. For this, we select the `reverse' match order class STravREV. As our `skip loops', we select class SLFast1. For our shifter, we select BMShift11 shifter class. We can now declare a pattern matching object for string hehshe as follows:

#include "bms.hpp"

static PMBM< STravREV, SLFast1, BMShift11<STravREV> > M( "hehshe" );

Note that the same string traverser class must appear as the rst template argument

to PMBM and as the template argument to the shifter class.

2

9.4.5.1 Safe shifters and auxiliary functions
The match orders, the skip loops, and the shifters form the core of the implementation of the BM algorithm variants. The match order classes will be described in Implementation classes 9.39. The skip loop classes (class names starting with SL: : : ) and the shifters (class names starting with BMShift: : : ) are described below, along with some of the shift components.
Implementation class 9.28 (SLNone, SLSFC, SLFast1, SLFast2) Files: , , ,bmslnone bmslsfc bmslfst1 bmslfst2 Description: The skip loop classes are used to skip portions of the input string in which
no matches are possible. SLNone makes no shift through the input string (it is only included for completeness, since it is derived in Section 4.5.1 and in HS91]).
Implementation: All of the implementations follow directly from Section 4.5.1. The
constructors simply take the pattern keyword. Some of the skip loops store lookup tables to compute the shift distance. In the case of SLSFC, the shift distance is 1.

9.4. CONCRETE PATTERN MATCHERS

241

Performance: The current implementations are for maximumperformance in time. Space
is sacri ced in favour of speed. All of the member functions should be inlined. 2
Implementation class 9.29 (BMShiftNaive) Files: bmshnaiv Description: The na ve shift distance class, BMShiftNaive, provides a safe shift distance
of 1. It is intended only for benchmarking purposes. 2
Implementation class 9.30 (BMShift11, BMShift12) Files: ,bmsh1-1 bmsh1-2 Description: These two classes implement two of the possible shift distances considered
in Section 4.5.2 on page 110. Both classes are template classes, which expect a string traverser (STrav: : : ) class as their template argument. The particular traverser used in the instantiation must be the same traverser class used as the match order in PMBM.
Implementation: Class BMShift11 makes use of an S1 and a Char1, while BMShift12
makes use of an S1 and a Char2. In both cases, the two shift components are combined during the scanning of the string. The template argument is used to instantiate the correct versions of the component shift functions.
Performance: Since the shift components are combined during the scanning of the input
string, the performance could be improved by combining them in the constructor. 2
Implementation class 9.31 (Char1, Char2) Files: ,bmchar1 bmchar2 Description: These two classes are shift components, implementing functions char1 and
char2 given in De nition 4.174. Since these two functions depend upon the particular match order in use, these two classes are de ned as template classes. The template argument must be one of the STrav: : : classes.
Implementation: The de nitions of functions char1 and char2 contain MIN quanti ca-
tions. As a result, the constructors perform a linear search to compute the functions. The linear search is general, since it makes use of the template argument (the string traverser).

242 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
Performance: When STravFWD or STravREV are used as the template argument, the
linear search can be performed more e ciently (as in the classical Boyer-Moore precomputation). Such traverser speci c precomputation can be written as template instantiation overriding functions. (This has not yet been done | it should be done by clients who require higher performance from the SPARE Parts.)
2
Implementation class 9.32 (S1) Files: bms1 Description: This class implements the s1 shift component given in De nition 4.174. As
with shift components char1 and char2, this class depends upon the particular string traverser in use. S1 is a template class which expects a string traverser as its template argument.
Implementation: See the implementation of classes Char1 and Char2. Performance: See Char1 and Char2.
2

9.4. CONCRETE PATTERN MATCHERS
9.4.6 Summary of user classes

243

In this section, we present two tables which summarize the various user classes for pattern matching. The rst table summarizes the descendents of abstract single keyword pattern matching class PMSingle. All three of the concrete classes are described along with their possible template arguments (if any):

Class Description

PMBFSingle Brute-force pattern matcher

PMKMP Knuth-Morris-Pratt pattern matcher

PMBM Boyer-Moore pattern matcher template

Three template arguments required, as follows:

Match orders

STravFWD Forward (left to right)

STravREV Reverse (right to left)

STravOM Optimal mismatch (increasing frequency)

STravRAN Random

Skip loops

SLNone

No skip

SLSFC

Leftmost keyword symbol compared

(always 1 symbol shift)

SLFast1

Rightmost symbol compared

SLFast2

Rightmost symbol compared

(greater shift)

Shifters

BMShiftNaive Shift of one symbol

BMShift11 Shift without mismatching symbol

information

BMShift12 Shift with mismatching symbol

information

The following table summarizes the concrete class descendents of the abstract multiple keyword pattern matcher class PMMultiple. Two of them are template classes and their possible template arguments are summarized as well:

244 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++

Class Description

PMBFMulti Brute-force pattern matcher

PMAC Aho-Corasick pattern matcher template

Single template argument required:

Transition machines

ACMachineOpt Optimal transition function

ACMachineFail Failure function with extended trie

ACMachineKMPFail Knuth-Morris-Pratt failure function

with forward trie

PMCW Commentz-Walter pattern matcher template

Single template argument required:

Shifters

CWShiftNaive

Shift of one symbol

CWShiftNLA

No lookahead symbol used

CWShiftWBM

Weak Boyer-Moore

CWShiftNorm

Normal Commentz-Walter

CWShiftOpt

Optimized Commentz-Walter

CWShiftRLA

Lookahead right one symbol

9.5 Foundation classes
In this section, we consider the design and implementation of the foundation classes and functions. These classes and functions are not of primary concern to the client, but are used to construct classes which form the client interface. Some of these classes are reused in the FIRE Lite | a toolkit of nite automata algorithms described in Chapter 10.
A number of these classes will be replaceable by standard library classes once the draft C++ standard becomes stable and implementations of the draft standard start to appear.

9.5.1 Miscellaneous
A number of header les and their corresponding de nitions do not fall into a particular category. Header com-misc.hpp contains de nitions of constants TRUE and FALSE and integer maximum and minimum functions max and min.

Implementation class 9.33 (State) Files: state Description: Tries and nite automata require the de nition of states. This header con-
tains a de nition of states and some constants, in particular an INVALIDSTATE and a FIRSTSTATE. The FIRSTSTATE is used (by convention) as the start state in tries and nite automata.

9.5. FOUNDATION CLASSES

245

Implementation: State is not de ned as a class. Instead, it is typedef'd to be an integer
for e ciency reasons. 2
User class 9.34 (String) Files: string Description: The raw string conventions in C and C++ are too rudimentary to be used
e ectively and safely. This class provides a higher level (and safer) mechanism for using strings. The interfaces provides members for indexing the individual characters in the string, copying strings, assignment, length of the string, and an output operator (stream insertion).
Implementation: The class is implemented through use-counting with a private class.
This makes assignment and copying of strings particularly e cient, but it adds an additional level of indirection to many of the operations. The cost of the extra indirection was found to be negligible compared to the cost of creating complete copies of strings.
Performance: The length of the string is kept in the private class. It requires a complete
traversal of the string, using standard function strlen. Since this can be particularly ine cient for very large strings, it could be replaced with more e cient methods of determining the length when the string is constructed from a le. The ine ciency of the extra indirection will very likely be removed (when examining individual characters of the string) by a good optimizing compiler.
2

9.5.2 Arrays, sets, and maps
In this section, we describe the basic template classes used to construct more complex objects.
Implementation class 9.35 (Array) Files: array Description: As with strings, the raw C and C++ facilities for arrays are not safe and
exible enough for our purposes. An Array template class constructs arrays of objects of class T. The operators available in raw arrays are provided. Notable additions are: bounds-checked indexing into the array, a stream insertion operator (assuming that class T has an insertion operator), and the ability to resize the array dynamically.

246 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
Implementation: An Array is implemented by an array of objects, an apparent size (to
the client), and real size (for dynamic resizing). The class constant growthSize is used during resize operations to allocate some extra elements; these extra elements can be used later to avoid another call to the memory allocator.
Performance: The copy constructor and the assignment operator both make copies of
the underlying array. This costly operation could be avoided through the use of usecounting. The class constant growthSize is a tuning constant which is 5 by default. Other values may provide higher performance in certain circumstances.
2
Implementation class 9.36 (Set) Files: set Description: Sets of objects are used in a variety of places. The template class Set
implements a set of objects of class T. Common set operations (such as element add, remove, union, membership tests, size) are available, as well as a rudimentary iterator facility, and an insertion operator. The class is replaceable by a standard one, once the draft C++ standard is stable. The current draft of the standard proposes to use the Standard Template Library by Stepanov and Lee SL94]. The Standard Template Library de nition puts forth a more complex set of iterators than needed in the SPARE Parts.
Implementation: The implementation is via an Array. This makes management of the
size of the set particularly simple. Most of the member functions of Set are simple pass-throughs to the corresponding Array members.
Performance: The performance is most easily improved through modi cations in class
Array. 2
Implementation class 9.37 (StateTo) Files: stateto Description: Template class StateTo implements a function mapping a State to an object
of class T. Member functions are provided for setting up the function, applying the function, and for adjusting the range of states in the domain.
Implementation: StateTo is implemented in terms of an Array. The member functions
are mostly pass-throughs to the Array members.
Performance: Since the implementation is through class Array, the performance of StateTo
is most easily improved by improving Array.

9.5. FOUNDATION CLASSES

247

2
Implementation class 9.38 (SymbolTo) Files: symbolto Description: Template class SymbolTo implements a function from characters to an
object of class T. The characters in the domain are assumed to be in the range 0; ALPHABETSIZE). Member functions are provided for setting up the function and for applying it.
Implementation: Class SymbolTo is implemented in terms of an Array. The member
functions are mostly pass-throughs to the Array members.
Performance: The performance is most easily improved through improvements to class
Array. 2

9.5.3 Tries and failure functions
Tries and failure functions form the basis for the multiple keyword pattern matching algorithms. Tries and match orders (for use in the Boyer-Moore algorithms) are both implemented in terms of string traversers. String traversers, tries, and failure functions are described in this section.
Implementation class 9.39 (STravFWD, STravREV, STravOM, STravRAN) Files: , , ,stravfwd stravrev stravom stravran Description: String traversers are synonymous with match orders (from Chapter 4). For
a given string of length n, a string traverser is a bijection on 0; n). It can be used to traverse the characters of the string in a particular order. Class STravFWD corresponds to the identity function, which traverses the string in the left to right direction. Class STravREV allows one to traverse the string in the right to left direction. Classes STravOM and STravRAN correspond (respectively) to an optimal mismatch order (see Algorithm detail 4.151) and a random order. The latter two classes are used primarily in the Boyer-Moore algorithms, while the rst two nd use in class Trie. All of the traversers have constructors which take a keyword.
Implementation: Since class STravFWD implements the identity function, it does not
contain any private data. Class STravREV only contains the length of the keyword in order to implement the bijection. The other traversers have not yet been implemented.
Performance: Since these classes are used to consider the characters in strings, it is
important that they are inlined.

248 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
2
Implementation class 9.40 (Trie) Files: trie, tries Description: Template class Trie implements tries Fred60]. The template parameter
must be one of the string traverser classes. Trie has a constructor which takes a set of strings, yielding the corresponding trie. The string traverser determines in which order the strings are traversed in the constructor. Using STravFWD as the template parameter gives a forward trie (see De nition 4.26), while using STravREV gives a
reverse trie (see De nition 4.13). Forward and reverse tries are typedef'd in header
;tries.hpp this header is intended for use by clients. There are a special set of member functions to perform breadth- rst traversals of the trie, and to determine the depth (when the trie is considered as a tree) of a particular State (since each State corresponds to a string, the depth of the State is equal to the length of the string).
Implementation: Trie is implemented using a StateTo< SymbolTo<State> >. An ad-
ditional StateTo maps States to integers, keeping track of the depth of the States for the breadth- rst traversals. Almost all of the member functions are simple passthroughs. The constructor uses the depth- rst method of constructing the trie from the set of strings.
Performance: The use of nested mappings (StateTos and SymbolTos) is highly e cient
in time, but it is known that tries can be implemented much more space e ciently, as in AMS92]. The disadvantage to such an implementation is that each transition requires more time.
2
Implementation class 9.41 (Fail) Files: fail, fails Description: Failure functions are implemented using template class Fail. Fail has a
constructor which takes a Trie as parameter. The type of trie (forward or reverse) determines the type of failure function constructed (forward or reverse). Consequently, the template argument to Fail must be either STravFWD or STravREV. The template argument is only used to determine the type of the Trie taken as pa-
rameter by the constructor. Header fails.hpp contains typedefs of the forward
and reverse tries; this header le is intended for use by clients.
Implementation: A failure function is implemented as a StateTo<State>. The construc-
tor uses a standard breadth- rst traversal of the Trie (see WZ92] or Chapter 5 for algorithms constructing failure functions from tries).

9.6. EXPERIENCES AND CONCLUSIONS

249

Performance: The performance can only be improved through improvements to class
StateTo. 2

9.6 Experiences and conclusions
Designing and coding the SPARE Parts lead to a number of interesting experiences in class library design. In particular:
The SPARE Parts comprises 5787 lines of code in 59 .hpp, 32 .cpp, 43 .ppp, and 49 .ipp les. Compiling the les, with the Watcom C++32 Version 9.5b compiler, shows that the size of the object code varies very little for the various types of pattern matchers. The taxonomy presented in Chapter 4 was critical to correctly implementing the many complex precomputation algorithms. Designing and structuring generic software (reusable software such as class libraries) is much more di cult than designing software for a single application. The general structure of the taxonomy proved to be helpful in guiding the structure of the SPARE Parts. One of the debugging session lead to the discovery of a bug in the code for precomputation of failure functions. Further inspection showed that the C++ code was correctly implemented from the abstract algorithm presented in WZ92, Part II]. Unfortunately, part of the abstract algorithm used a depth- rst traversal of a trie, while the postcondition called for a breadth- rst traversal. In Chapter 13, we consider the relative performance of the algorithms implemented in the SPARE Parts. It is also helpful to consider how the implementations in the SPARE Parts fare against commercially available tools such as the fgrep program. Four fgrep-type programs were implemented (using the SPARE Parts), corresponding to the Knuth-Morris-Pratt, Aho-Corasick, Boyer-Moore, and Commentz-Walter algorithms. The four tools were benchmarked informally against the fgrep implementation which is sold as part of the MKS toolkit for MS-Dos. The resulting times (to process a 984149 byte text le, searching for a single keyword) are:

fgrep variant MKS KMP BM AC CW Time (sec) 3.9 5.1 4.2 4.7 4.0

These results indicate that using a general toolkit such as the SPARE Parts will result in performance which is similar to carefully tuned C code (such as MKS fgrep).

250 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++
Detailed records were kept on the time required for designing, typing, compiling (and xing syntax errors), and debugging the toolkit. The time required to implement the toolkit is broken down as follows (an explanation of each of the tasks is given below):
Task Design Typing Compile/Syntax Debug Total Time (hrs:min) 6:00 13:40 10:05 5:15 35:00 Most of these times are quite short compared to what a software engineer could expect to spend on a project of comparable size. The following paragraphs explain exactly what each of the tasks entailed: The design phase involved the creation of the inheritance hierarchy and the declaration (on paper) of all of the classes in the toolkit. (A C++ declaration provides names and signatures of functions, types, and variables, whereas a de nition provides the implementation of these items.) The design phase proceeded exceptionally smoothly, thanks to a number of things:
{ The inheritance hierarchy followed directly from the structure of the taxonomy. { The decisions on the use of templates (instead of virtual functions | see Chap-
ter 8) and call-backs were made on the basis of experience gained with the FIRE Engine. These decisions were also somewhat forced by the e ciency requirements on the toolkit, as well as the need for multi-threading.
{ Representation issues, such as the selection of data structures, were resolved
using experience gained with the earlier Eindhoven Pattern Kit. Once the foundation classes were declared and de ned, typing the code amounted to a simple translation of guarded commands to C++. The times required for compilation and syntax checking were minimized by using a very fast integrated environment (Borland C++) for initial development. Only the nal few compilations were done using the (slower, but more thoroughly optimizing) Watcom C++ compiler. The advantages of using a fast development environment on a single user personal computer should not be underestimated. Since the C++ code in the toolkit was implemented directly from the abstract algorithms (for which correctness arguments are given), the only (detected) bugs were those involving typing errors (such as the use of the wrong variable, etc.). Correspondingly, little time needed to be spent on debugging the toolkit.
9.7 Obtaining and compiling the toolkit
The SPARE Parts is available for anonymous ftp from ftp.win.tue.nl in directory:
/pub/techreports/pi/watson.phd/spare/

9.7. OBTAINING AND COMPILING THE TOOLKIT

251

The toolkit, and some associated documentation, are combined into a tar le. A number of di erent versions of this le are stored | each having been compressed with a di erent compression utility.
The SPARE Parts has been successfully compiled with Borland C++ Versions 3.1 and 4.0, and Watcom C++32 Version 9.5b on MS-Dos and Microsoft Windows 3.1 platforms. Since the Watcom compiler is also a cross-compiler, there is every reason to believe that the code will compile for Windows NT or for IBM OS/2. The implementation of the toolkit makes use of only the most basic features of C++, and it should be compilable using any of the template-supporting Unix based C++ compilers.
A version of the SPARE Parts will remain freely available (though not in the public domain). Contributions to the toolkit, in the form of new algorithms or alternative implementations, are welcome.

252 CHAPTER 9. SPARE PARTS: STRING PATTERN RECOGNITION IN C++

Chapter 10
FIRE Lite: FAs and REs in C++
This chapter describes a C++ nite automata toolkit known as FIRE Lite (FInite automata and Regular Expressions; Lite since it is the smaller and newer cousin of the FIRE Engine toolkit, also from the Eindhoven University of Technology). The client interface and aspects of the design and implementation are also described.
10.1 Introduction and related work
FIRE Lite is a C++ toolkit implementing nite automata and regular expression algorithms. The toolkit is a computing engine, providing classes and algorithms of a low enough level that they can be used in most applications requiring nite automata or regular expressions. Almost all of the algorithms derived in Chapter 6 are implemented. This chapter serves as an introduction to the client interface of the toolkit and the design and implementation issues of the toolkit.
10.1.1 Related toolkits
There are several existing nite automata toolkits. They are: The Amore system, as described in JPTW90]. The Amore package is an implementation of the semigroup approach to formal languages. It provides procedures for the manipulation of regular expressions, nite automata, and nite semigroups. The system supports a graphical user-interface on a variety of platforms, allowing the user to interactively and graphically manipulate the nite automata. The program is written (portably) in the C programming language, but it does not provide a programmer's interface. The system is intended to serve two purposes: to support research into language theory and to help explore the e cient implementation of algorithms solving language theoretic problems. The Automate system, as described in CH91]. Automate is a package for the symbolic computation on nite automata, extended regular expressions (those with the 253

254 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
intersection and complementation operators), and nite semigroups. The system provides a textual user-interface through which regular expressions and nite automata can be manipulated. A single nite automata construction algorithm (a variant of Thompson's) and a single deterministic nite automata minimization algorithm is provided (Hopcroft's). The system is intended for use in teaching and language theory research. The (monolithic) program is written (portably) in the C programming language, but provides no function library interface for programmers. According to Pascal Caron (at the Universite de Rouen, France), a new version of Automate is being written in the Maple symbolic computation system. The FIRE Engine, as described in Wat94b, Wat94c]. The FIRE Engine was the rst of the toolkits from the Computing Science Faculty in Eindhoven. It is an implementation of all of the algorithms appearing in two early taxonomies of nite automata algorithms which appeared in Wat93a, Wat93b]. The toolkit is somewhat larger than FIRE Lite (the FIRE Engine is 9000 lines of C++) and has a slightly larger and more complex public interface. The more complex interface means that the toolkit does not support multi-threaded use of a single nite automaton. The Grail system, as described in RW93]. Grail follows in the tradition of such toolkits as Regpack Leis77] and INR John86], which were all developed at the University of Waterloo, Canada. It provides two interfaces:
{ A set of ` lter' programs (in the tradition of Unix). Each lter implements an
elementary operation on nite automata or regular expressions. Such operations include conversions from regular expressions to nite automata, minimization of nite automata, etc. The lters can be combined as a Unix `pipe' to create more complex operations; the use of pipes allows the user to examine the intermediate results of complex operations. This interface satis es the rst two (of three) aims of Grail RW93]: to provide a vehicle for research into language theoretic algorithms, and to facilitate teaching of language theory.
{ A raw C++ class library provides a wide variety of language theoretic objects
and algorithms for manipulating them. The class library is used directly in the implementation of the lter programs. This interface is intended to satisfy the third aim of Grail: an e cient system for use in application software. The provision of the C++ class interface in Grail makes it the only toolkit with aims similar to those of the FIRE Engine and of FIRE Lite. In the following section, we will highlight some of the advantages of FIRE Lite over the other toolkits.
10.1.2 Advantages and characteristics of FIRE Lite
The advantages to using FIRE Lite, and the similarities and di erences between FIRE Lite and the existing toolkits are:

10.1. INTRODUCTION AND RELATED WORK

255

FIRE Lite does not provide a user interface1. Some of the other toolkits provide user interfaces for the symbolic manipulation of nite automata and regular expressions. Since FIRE Lite is strictly a computing engine, it can be used as the implementation beneath a symbolic computation application.
The toolkit is implemented for e ciency. Unlike the other toolkits, which are implemented with educational aims, it is intended that the implementations in FIRE Lite are e cient enough that they can be used in production quality software.
Despite the emphasis on e ciency in FIRE Lite the toolkit still has educational value. The toolkit bridges the gap between the easily understood abstract algorithms appearing in Chapter 6 and practical implementation of such algorithms. The C++ implementations of the algorithms display a close resemblance to their abstract counterparts.
Most of the toolkits implement only one of the known algorithms for constructing nite automata. For example, Automate implements only one of the known constructions. By contrast, FIRE Lite provides implementations of almost all of the known algorithms for constructing nite automata. Implementing many of the known algorithms has several advantages:
{ The client can choose between a variety of algorithms, given tradeo s for nite
automata construction time and input string processing time.
{ The e ciency of the algorithms (on a given application) can be compared. { The algorithms can be studied and compared by those interested in the inner
workings of the algorithms.

10.1.3 Future directions for the toolkit
A number of improvements to FIRE Lite will appear in future versions: Presently, FIRE Lite implements only acceptors. Transducers (as would be required for some types of pattern matching, lexical analysis, and communicating nite automata) will be implemented in a future version. A future version of the toolkit will include support for extended regular expressions, i.e. regular expressions containing intersection or complementation operators. Basic regular expressions and automata transition labels are represented by character ranges. A future version of FIRE Lite will permit basic regular expressions and transition labels to be built from more complex data-structures. For example, it will be possible to process a string (vector) of structures. (Version 2.0 of Grail included a similar improvement.)
1A rudimentary user interface is included for demonstration purposes.

256 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
10.1.4 Reading this chapter
The toolkit is presented largely in a top-down fashion. The chapter is structured as follows: Section 10.2 gives an introduction to the client interface of the toolkit. It includes some examples of programs which use FIRE Lite. Section 10.3 gives an overview of the structure of the toolkit. Section 10.4 outlines the client interfaces to regular expressions and nite automata. Section 10.5 presents the concrete classes which implement the interface de ned in the abstract classes. Section 10.6 outlines the design and implementation of the foundation classes. Some of the foundation classes in the SPARE Parts have been reused in FIRE Lite. Those classes have not been described here again | they can be found in Chapter 9. Section 10.7 presents some experiences with using the toolkit, and the conclusions of this chapter. Some information on how to obtain and compile the toolkit is given in Section 10.8.
10.2 Using the toolkit
In this section, we describe the client interface to the toolkit | including some examples which use the toolkit. The issues in the design of the current client interface are described in Section 10.4.
There are two components to the client interface of FIRE Lite: regular expressions (class RE) and nite automata (classes whose names begin with FA: : : ). We rst consider regular expressions and their construction.
Regular expressions are implemented through class RE. This class provides a variety of constructors and member functions for constructing complex regular expressions. Stream insertion and extraction operators are also provided. (The public interface of RE is rather fat, consisting of a number of member functions intended for use by the constructors of nite automata.) The following example constructs a regular expression.
Example 10.1 (Regular expression): The following program constructs a simple reg-
ular expression and prints it:
#include "re.hpp"
#include <iostream.h>
int main(void) f
auto RE e( B ); auto RE f( CharRange( a , z ) );

10.2. USING THE TOOLKIT

257

e.concatenate( f ); e.or( f ); e.star(); cout << e;
return( 0 );
g

10

The header re.hpp de nes regular expression class RE. The program rst constructs two regular expressions. The rst is the single symbol B. The second regular expression is a CharRange | a character range. The RE constructed corresponds to the range a; z] of characters. No particular ordering is assumed on the characters, though most platforms use the ASCII ordering. Character ranges can always be used as atomic regular expressions. The program concatenates regular expression f onto e and then unions f onto e. Finally, the Kleene closure operator is applied to regular expression e. The nal regular expression, which is ((B a; z]) a; z]) , is output (in a pre x notation) to standard output. 2
The abstract nite automata class de nes the common interface for all nite automata; it is de ned in .faabs.hpp A variety of concrete nite automata are provided in FIRE Lite; they are declared in header .fas.hpp There are two ways to use a nite automaton. In both of them the client constructs a nite automaton, using a regular expression as argument to the constructor. The two are outlined as follows:
1. In the simplest of the two, the client program calls nite automaton member function FAAbs::attemptAccept, passing it a string and a reference to an integer. The member function returns TRUE if the string was accepted by the automaton, FALSE otherwise. Into the integer reference it places the index (into the string) of the symbol to the right of the last symbol processed.

2. In the more complex method, the client takes the following steps (which resemble the steps required in using a pattern matcher mentioned in Chapter 9):

(a) The client calls the nite automaton member function FAAbs::reportAll, passing it a string and a pointer to a function which takes an integer and returns an integer. As in the SPARE Parts (Chapter 9), the function is the call-back function.
(b) The member function processes the input string. Each time the nite automaton enters a nal state (while processing the string), the call-back function is called. The argument to the call is the index (into the input string) of the symbol immediately to the right of the symbol which took the automaton to the nal state.
(c) To continue processing the string, the call-back function returns TRUE, otherwise FALSE.
(d) When the input string is exhausted, the call-back function returns FALSE, or the automaton becomes stuck (unable to make a transition on the next input

258 CHAPTER 10. FIRE LITE: FAS AND RES IN C++

symbol) the member function FAAbs::reportAll returns the index of the symbol immediately to the right of the last symbol on which a successful transition was made. The following is an example of the use of a nite automaton.
Example 10.2 (Finite automaton): The following program fragment takes a regular
expression, constructs a nite automaton, and processes an input string:

#include "com-misc.hpp" #include "re.hpp" #include "fas.hpp"
#include <iostream.h>

static int report( int ind ) f
cout << ind << \n ; return( TRUE );
g void process( const RE& e ) f
auto FARFA M( e ); cout << M.reportAll( "hishershey", &report );
return;
g

10

Header com-misc.hpp provides the de nition of constants TRUE and FALSE, while

header fas.hpp gives the declarations of a number of concrete nite automata. Function

report is used as the call-back function; it simply prints the index and returns TRUE to

continue processing. Function process takes an RE and constructs a local nite automa-

ton (of concrete class FARFA). It then uses the automaton processes string ,hishershey

writing the nal index to standard output before returning.

2

Given these examples, we can now demonstrate a more complex use of a nite automaton.
Example 10.3 (Regular expression pattern matching): In this example, we imple-
ment a generalized Aho-Corasick pattern matcher (GAC | as in Section 5.1) which performs regular expression pattern matching. Since regular expression pattern matching is not presently included in the SPARE Parts, this example illustrates how FIRE Lite could be used to implement such pattern matching for a future version of the SPARE Parts. (This example also highlights the fact that the call-back mechanism in FIRE Lite is very similar to the mechanism in the SPARE Parts.)

#include "re.hpp" #include "fas.hpp"

10.3. THE STRUCTURE OF FIRE LITE

259

#include "string.hpp"

class PMRE f
public:
PMRE( const RE& e ) : M( e ) fg int match( const String& S, int cb( int ) ) f
return( M.reportAll( S, cb ) );
g
private:
FARFA M;
g;

10

Headers re.hpp, ,fas.hpp and string.hpp have all been explained before. Class PMRE is the regular expression pattern matching class. Its client interface is modeled on the pattern matching client interfaces used in Chapter 9. The class has a private nite automaton (in this case an FARFA) which is constructed from an RE. (Note that the constructor of class PMRE has an empty body, since the constructor of FARFA M does all of the work.) The member function PMRE::match takes an input string and a call-back function. It functions in the same way as the call-back mechanism in Chapter 9. The member function is trivial to implement using member FAAbs::reportAll. Whenever the nite automaton enters an accepting state, a match has been found and it is reported. 2

An important feature of FIRE Lite (like SPARE Parts) is that the call-back client interface implicitly supports multi-threading. See Section 9.2.1 for a discussion of call-back functions and multi-threading.

10.3 The structure of FIRE Lite
It is helpful to have an overview of the structure of FIRE Lite and some of the main classes in the toolkit. In this section, we give such an overview.
In the construction algorithms of Chapter 6, the nite automata that are produced have states containing extra information. In particular, the canonical construction produces automata whose states are dotted regular expressions, or items. Some of the other constructions produce automata with sets of items for states, or sets of positions for states, and so on.
The constructions of Chapter 6 appear as the constructors (taking a regular expression) of various concrete nite automata classes in FIRE Lite. It seems natural that mathematical concepts such as items, sets of items, positions, and sets of positions will also appear in such an implementation. The only potential problem is the performance overhead inherent in implementing automata with states which are sets of items, etc.
The solution used in FIRE Lite is to implement states with internal structure as abstractstates during the construction of a nite automaton. The nite automata is constructed using the abstract-states (so that the constructor corresponds to one of the algorithms in

260 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
Chapter 6). Once the automaton is fully constructed, the abstract-states are too space and time consuming for use while processing a string (for acceptance by the automaton). Instead, we map the abstract-states (and the transition relations, etc) to States (which are simply integers) using a StateAssoc object. Once the mapping is complete, the abstractstates, their transition relations, and the StateAssoc object can be destroyed.
For all of the nite automata which are constructed from abstract-states, the constructor takes an initial abstract-state which is used as a `seed' for constructing the remaining states and the transition relation. For performance reasons, we wish to make the nite automaton constructor a template class whose template argument is the abstract-state class (this would avoid virtual function calls). Unfortunately, most compilers which are presently available do not support template member functions (which have recently been added to the draft C++ standard). As a result, we are forced to make the entire nite automata class into a template class. The main disadvantage is that this reduces the amount of object code sharing (see Chapter 8 for a discussion of the di erences between source and object code sharing and templates versus inheritance).
Most of the abstract-state classes have constructors which take a regular expression. As a result, an RE can be used as argument to most of the nite automata classes | a temporary abstract-state will be constructed automatically.
There are three types of abstract-states, corresponding to nite automata with "transitions (see class FAFA), "-free nite automata, and deterministic nite automata. Classes of a particular variety of abstract-state all share the same public interface by convention; the template instantiation phase of the compiler detects deviations from the common interface. For more on the three types of abstract-state, see Implementation classes 10.8, 10.11, and 10.13. For examples of particular abstract-state classes, see Section 10.5.4.
The transition relations on States are implemented by classes StateStateRel (for "transitions), TransRel (for nondeterministic transitions), and DTransRel (for deterministic transitions).
10.4 REs and abstract FAs
In this section, we consider the client interface to regular expressions and the abstract nite automaton class. The multi-threading aspects of the nite automaton interface is not discussed since these aspects are parallel to those presented in Section 9.2.1.
User class 10.4 (RE) File: re, reops Description: Class RE supports regular expressions. The client interface supports the
construction of regular expressions, using all of the operators given in Chapter 2. Operators for stream insertion and extraction are also supported. A number of special-purpose member functions provide information that is primarily used in nite

10.4. RES AND ABSTRACT FAS

261

automata constructions. These member functions could have been made protected or private (to hide them from clients), however, this would have required the nite automata classes to be friends of class RE.
Implementation: Regular expression are implemented as expression trees, with RE be-
ing a node in the tree. (Note that this corresponds to the tree de nition of regular expressions presented in Chapter 6.) RE contains an operator (the operator enumerations are de ned as class REops in header reops.hpp), instead of deriving speci c operator nodes from RE; this was done for simplicity. RE contains pointers to left and right subexpressions. Some of the member functions (which are used by nite automata constructors) perform a tree traversal on their rst invocation; the information is cached in the RE node, implementing a form of memoization. An alternative implementation would be to store the regular expression as a string in pre x notation | as is done in Grail RW93].
Performance: For higher performance, more of the member functions could make use of
memoization. It is not clear if the regular expression data structures used in FIRE Lite are more e cient than those used in Grail.
2
User class 10.5 (FAAbs) File: faabs Description: This abstract class provides the client interface to all of the nite automata
classes. It de nes the member functions shown in the examples in Section 10.2. For clarity, we present the header faabs.hpp here:

= (c) Copyright 1995 by Bruce W. Watson = == FIRE Lite class library. == $Revision:$ == $Date:$ #ifndef FAABS HPP #de ne FAABS HPP #de ne IN FAABS HPP

#include "string.hpp" == Give a generic interface to the nite automata in FIRE == Lite. This interface di ers from the one in the FIRE == Engine. The new interface makes use of `call-backs' in == the same way that the SPARE Parts class library does.
class FAAbs f
public:

10

262 CHAPTER 10. FIRE LITE: FAS AND RES IN C++

== In the concrete classes, there will be constructors == from regular expressions, etc.
== Is the input String accepted? This member returns the == TRUE if the string is accepted. The int& parameter will == contain the index to the right of the last character == processed. virtual int attemptAccept( const String& S,
int& index ) const = 0;

20

== Process the input String, calling back whenever an == accepting state is entered. The return value is the == index to the right of the last character processed.
== At each call-back, the index to the right of the last == processed symbol is passed. If the call-back function
== returns FALSE, the acceptance attempt is aborted. virtual int reportAll( const String& S,
int callBack (int) ) const = 0;

30

== How many states in this nite automaton? virtual int numStates() const = 0;
g;
#undef IN FAABS HPP
#endif

40

Implementation: As an abstract class, there is no implementation.

2

10.5 Concrete FAs
A number of concrete nite automata are provided to implement the client interface de ned by FAAbs. A summary of the classes and their template arguments (if any) is given in Section 10.5.5. The automata are divided into three types: automata with "-transitions, automata without "-transitions, and deterministic automata. All of the classes have names that are pre xed by FA. Some of the classes are in fact templates. The following sections contain descriptions of the di erent classes. All nite automata are declared in header .fas.hpp

10.5. CONCRETE FAS

263

10.5.1 Finite automata

There are two classes (User class 10.6 | a non-template class, and User class 10.7 | a template class) implementing nite automata with "-transitions.
User class 10.6 (FACanonical) File: fa-canon Description: This class is a non-template nite automaton class. It implements the
canonical nite automata construction algorithm (see Construction 6.15). It inherits from FAAbs and implements the required interface.
Implementation: The class is implemented by maintaining the dot movement relation
(as an ItemItemRel), the set of CharRange nodes (as a NodeSet), and the set of labels of these nodes (as a NodeTo<CharRange>). The simulation of the automaton is trivially implemented, with a single helper function.
Performance: The implementation of the main member functions is straight-forward.
Performance can be improved by improving the components. 2
User class 10.7 (FAFA) File: fa-fa
Description: This nite automata template class inherits from FAAbs and implements
that public interface. The template argument must be one of the abstract-state classes (names beginning with AS: : : ). The constructor takes an object of class AS: : : and uses it to construct the "-transition relation and the labeled transition relation as well as a single start state and the set of nal states. Most of the abstractstate classes have constructors which take an RE, enabling us to use an RE as the argument to the FAFA constructor.
Implementation: The abstract-states are described further in Implementation class 10.8.
Making the whole of class FAFA a template class would not be necessary if we could make the constructor a template member function. The template argument is only used in the construction of local variables in the constructor. At this time, most C++ compilers do not yet support template member functions. Making the whole class a template class has the disadvantage that code sharing is only done at the source level and objects of two di erent instantiations of the template are not interchangeable according to the language de nition, even though their subobjects have identical types. The constructor assumes that its argument (an object of the abstract-state class) is the start state. It uses a reachability based algorithm to construct the rest of the transition relations, and the set of nal states. A StateAssoc object (see Implementation class 10.24) is used to give names to each of the abstract-states.

264 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
Performance: The performance is already highly tuned. It may be improved by using
use-counting in the abstract-states. 2
Implementation class 10.8 (AS: : : ) File: as... Description: This family of classes implements abstract-states. An abstract-state is one
which contains more information than a usual State. The various classes do not have a common inheritance ancestor to ensure a consistent interface since the return types of some of the member functions must vary depending upon the abstract-state class. Instead, the common interface is established by convention | errors are detected by the template instantiating phase of the compiler.
Implementation: Each of the abstract-states is described individually in Section 10.5.4.
2
10.5.2 "-free nite automata
The "-free nite automata are also implemented via both a template and a non-template class. All of the automata of this type (including instantiations of the template) are declared in header .fas.hpp
User class 10.9 (FARFA) File: fa-rfa Description: This non-template "-free nite automaton class inherits from FAAbs and
implements the public interface de ned there. This class corresponds to the reduced nite automata de ned in the original taxonomy of construction algorithms, Wat93a]. The constructor takes a regular expression.
Implementation: This class is implemented through a pair of PosnSets (representing the
sets First and Last from Chapter 6), a PosnPosnRel (representing the Follow relation), a NodeTo<CharRange> (mapping the `positions' to their labels in the regular
expression), and an int (representing the Boolean Null | whether the automaton
should accept "). These components are constructed from the RE argument to the constructor.
Performance: The implementation is already highly tuned. The only area for improve-
ment would be in use-counting the subobjects. 2

10.5. CONCRETE FAS

265

User class 10.10 (FAEFFA) File: fa-effa Description: This template class inherits from FAAbs and implements the nite automata
interface. The template argument must be one of the "-free abstract-states (classes with names beginning with ASEF: : : ). Again, most of the abstract-states have constructors which take an RE, allowing us to use a regular expression as argument to the FAEFFA.
Implementation: The implementation corresponds very closely to that of FAFA (see
User class 10.7). 2
Implementation class 10.11 (ASEF: : : ) File: asef... Description: This family of classes implement "-free states for use as template argument
to class FAEFFA. Their interface closely parallels that of the AS: : : classes.
Implementation: Same as the AS: : : classes.
2

10.5.3 Deterministic nite automata
The deterministic nite automata are only implemented by a template class. Instead of manually instantiating the template, the client should include the header fas.hpp which declares the di erent variants.
User class 10.12 (FADFA) File: fa-dfa Description: FADFA is a template class which inherits from FAAbs and implements the
interface de ned there. The template argument must be one of the deterministic abstract-state classes | classes with names beginning with ASD: : : . As with the other abstract-state classes, the deterministic ones usually have constructors which take an RE | meaning that we can use an RE as argument to the FADFA constructor.
Implementation: The implementation parallels that of class FAFA (see User class 10.7).
2
Implementation class 10.13 (ASD: : : )

266 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
File: asd... Description: These classes implement deterministic abstract-states for use as template
arguments to class FADFA. Their (common) interface parallels that of the AS: : : classes.
Implementation: Same as the AS: : : classes.
2
10.5.4 Abstract-states classes
Three of the nite automata classes de ned so far are template classes, which expect their template argument to be an abstract-state. As a result, there are three primary types of abstract-state. The "-free and deterministic abstract-states have names pre xed (respectively) with ASEF: : : and ASD: : : . The other abstract-states have names pre xed with AS: : : . The types of abstract-states have been grouped together according to their general implementation ideas (as taken from Chapter 6).
Implementation class 10.14 (ASItems, ASEFItems, ASDItems) File: , ,asitems asefitem asditems Description: These abstract-states all implement constructions which are based upon
sets of items (dotted regular expressions).
Implementation: They all maintain a ItemItemRel (representing the dot movement re-
lation). The rst abstract-state to be constructed must have an RE as its argument; the dot movement relation is obtained from the RE. The remaining abstract-states are constructed from a private constructor.
Performance: These classes all pay a large penalty due to keeping their own ItemItemRel
(the dot movement relation). Use-counting ItemItemRel or its subobjects would make a signi cant improvement.
2
Implementation class 10.15 (ASDItemsDeRemer, ASDItemsWatson) File: ,asditder asditwat Description: These two abstract-states implement the ltered deterministic abstract-
states based upon sets of items.
Implementation: Their implementation is similar to those in Implementation class 10.14.
Additionally, objects of these classes all maintain their own copy of the lter, which is applied after the closure operation.

10.5. CONCRETE FAS

267

Performance: These classes su er from the same performance penalties as the ones in
Implementation class 10.14. 2
Implementation class 10.16 (ASEFPosnsBS, ASEFPosnsBSdual) File: ,asefpbs asefpbsd Description: These two abstract-states are used in the constructions which are based
upon `positions' in a regular expression (see Constructions 6.39 and 6.65). The rst class implements the Berry-Sethi construction, while the second one implements the dual of the Berry-Sethi construction (see Chapter 6).
Implementation: The implementation is via a local PosnPosnRel (the Follow relation)
and a local PosnSet (the Last relation). The rst abstract-state is constructed from an RE (from which it obtains the PosnPosnRel and the PosnSet). The remaining ones are constructed using a private constructor.
Performance: As with the item based abstract-states (Implementation class 10.14, for
example), these classes su er a penalty for keeping a local copy of the PosnPosnRel. This can be alleviated by use-counting PosnPosnRel.
2
Implementation class 10.17 (ASDPosnsMYG, ASDPosnsASU) File: ,asdpmyg asdpasu Description: These two abstract-states are deterministic versions of the abstract-state
classes described in Implementation class 10.16. The rst implements the McNaughtonYamada-Glushkov construction, while the second one implements the Aho-SethiUllman construction (see Chapter 6).
Implementation: The implementation parallels that given in Implementation class 10.16. Performance: These classes have the same performance problems as ASEFPosnsBS and
ASEFPosnsBSdual. 2
Implementation class 10.18 (ASEFPDerivative, ASDDerivative) File: ,asefpder asdderiv Description: These abstract-state classes are used to represent a derivative (a regular
expression or a set of regular expressions) in Antimirov's and Brzozowski's constructions. Most of the member functions (required of the ASEF: : : and ASD: : : interfaces) are derivative operations on the regular expression representation.

268 CHAPTER 10. FIRE LITE: FAS AND RES IN C++

Implementation: This class contains a local copy of the regular expression that it rep-
resents. The abstract-state member functions are pass-throughs to the derivatives member functions of class RE.
Performance: The copying and comparison operations on regular expressions can be
particularly slow. The performance of this class could be signi cantly improved by use-counting RE.
2

10.5.5 Summary of concrete automata classes

The following table presents a summary of the various concrete automata classes and their template arguments (if any):

Class Description

FACanonical Canonical automaton

FAFA Automaton template, with "-transitions

Single template argument required:

Abstract states

ASItems

Item sets (canonical)

FARFA "-free automaton

FAEFFA Automaton template, without "-transitions

Single template argument required:

Abstract states

ASEFItems

Items sets (no lter)

ASEFPosnsBS Berry-Sethi

ASEFPosnsBSdual dual of Berry-Sethi

ASEFPDerivative Antimirov

FADFA Deterministic automaton template

Single template argument required:

Abstract states

ASDItems

Items sets (no lter)

ASDItemsDeRemer Items sets (DeRemer lter)
ASDItemsWatson Items sets (W lter)

ASDPosnsMYG McNaughton-Yamada-Glushkov

ASDPosnsASU Aho-Sethi-Ullman

ASDDerivative Brzozowski

10.6 Foundation classes

A number of the foundation classes presented in Chapter 9 and used in the SPARE Parts have been reused in FIRE Lite. They are: State, String, Array, Set, and StateTo. In

10.6. FOUNDATION CLASSES

269

addition to the corresponding headers, FIRE Lite also makes use of headers com-misc.hpp and com-opt.hpp for various miscellaneous declarations and de nitions.
In this section, we consider the additional foundation classes that have been de ned for use in FIRE Lite. We begin with character ranges (and sets of them) which are used as atomic regular expressions. We then continue with bit vectors (bit strings) and sets of integers, followed by transition relations and other relations.

10.6.1 Character ranges
Instead of restricting atomic regular expression and transition labels (in nite automata) to single characters, we permit the use of a range of characters (assuming some ordering on the characters, such as the ASCII ordering). Such a range of characters is denoted by it upper and lower (inclusive) bounds, represented by a CharRange. Sets of such character ranges can be combined in a Set<CharRange>, or in CRSet in which the CharRanges may be split so that they are disjoint.
Implementation class 10.19 (CharRange) File: charrang Description: This class is used to represent a range of characters (under the character
ordering of the execution platform | usually ASCII). A CharRange can be constructed by specifying the lower and upper (inclusive) bounds, or a single character. Member functions include one (taking a character) which determines if the character falls within the range. An ordering is also de ned on the CharRanges.
Implementation: The lower and upper bounds are simply stored privately. The ordering
member function implements the lexicographic order on pairs of characters.
Performance: This class is used so heavily that all of the (small) member functions must
be inline. 2
Implementation class 10.20 (CRSet) File: crset Description: CharRanges can be combined into a CRSet. As the CharRanges are added,
they may be split so that none of them in a CRSet overlap. (Splitting, instead of joining, is used since CRSets are used to implement deterministic nite automata.) A CRSet is useful for constructing deterministic nite automata. Two CRSets can be combined into one. There are member functions to iterate over the set.
Implementation: The class is implemented via an Array<CharRange>. The member
functions to add new character ranges ensures that the elements of the array do not overlap.

270 CHAPTER 10. FIRE LITE: FAS AND RES IN C++

Performance: All of the member functions are small enough to be inline.

2

10.6.2 States, positions, nodes, and maps
The de nition of State is borrowed from the SPARE Parts. The concepts of items and positions (in a regular expression) are de ned in a similar way. Additionally, mappings from a state, a position, or a node to some type T are de ned.

Implementation class 10.21 (StatePool) File: st-pool Description: This class represents a set of states (starting at constant FIRSTSTATE)
which can be assigned one-by-one for use in constructing nite automata.
Implementation: An integer is stored to track the last state allocated.
2
Implementation class 10.22 (Node, Posn) File: node, posn Description: The nodes and the positions (those nodes that are labeled with a symbol)
of the tree representation of a regular expression are denoted by a Node (respectively Posn).
Implementation: For simplicity, nodes and positions are encoded as integers representing their pre-order traversal order. Their de nition is via typedefs.
2

Implementation class 10.23 (NodeTo, PosnTo) File: nodeto, posnto Description: These two classes operate in a manner similar to StateTo (see Implementa-
tion class 9.37). They are template classes that implement maps from nodes (respectively positions) to objects of the argument class. The member classes correspond roughly to those in StateTo.
Implementation: As with StateTo, classes NodeTo and PosnTo are implemented using
Array. 2

10.6. FOUNDATION CLASSES

271

Implementation class 10.24 (StateAssoc) File: st-assoc Description: This template class takes a single template parameter | usually one of the
abstract-state classes. The constructor of StateAssoc takes a reference to a StatePool. It associates States with objects of the template argument class by assigning a new State to each new object that is looked up. Due to possible ambiguity, it is not possible to use State as the template argument.
Implementation: The implementation is through an Array, which is linearly searched
during lookups. Any other (perhaps more e cient) implementation would have to make some assumptions on the template argument.
2

10.6.3 Bit vectors and sets
In this section, we describe bit vectors and integer sets for the basis for storing sets of States.
Implementation class 10.25 (BitVec) File: bitvec Description: Class BitVec implements strings of bits. Typical operations, such as bitwise
and, or, exclusive or, complement, and shift are provided. Additional member functions are provided to determine if a particular bit is set, and to set a particular bit. The binary operators require that the two BitVecs be of the same width (in number of bits). This class can be replaced by the bit vector class in the draft of the C++ standard.
Implementation: The class is implemented as Array< unsigned int >. Taken end-to-
end, these integers form the bit vector. Most of the member functions operate at the array level. Special member functions are provided to index a particular bit, according to its position in the array of integers and its shift distance from the right.
Performance: A low level class like this should be implemented in assembly language for
best performance. The bit index calculation member functions are heavily used and must be inline. The current performance is almost optimal for portable C++.
2

Implementation class 10.26 (IntSet) File: intset

272 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
Description: Class IntSet implements set of unsigned integers. Typical set operations are supported. This class performs more e ciently than would Set< unsigned int
>. There is a restriction: the largest integer that can be stored must be set by the client (it will not automatically grow as with template class Set).
Implementation: The class is implemented through BitVec. Most of the member func-
tions are simple pass-throughs to BitVec members.
Performance: Most of the member functions are simple and should be inlined.
2
Implementation class 10.27 (StateSet, NodeSet, PosnSet) File: , ,stateset nodeset posnset Description: These three classes support sets of States, Nodes, and Posns (respectively).
They are used in the construction of automata and in the processing of strings. Most of the typical set operations, such as union, intersection, di erence, membership, and cardinality, are supported.
Implementation: Since State, Node, and Posn are all typedef'd as int, these sets are all typedef'd as IntSet.
2
Implementation class 10.28 (ItemSet) File: itemset Description: Sets of items (dotted regular expressions) are represented by ItemSet. There
are two types of items: those that are a dot before a node in the regular expression tree, and those that are a dot after a node. The usual set operations are available. This class is more e cient than using template class Set.
Implementation: The dots before and the dots after are stored as two separate NodeSets.
The set operations are simple pass-throughs to the underlying NodeSet members.
Performance: The member functions are simple enough to be inlined.
2

10.6. FOUNDATION CLASSES

273

10.6.4 Transitions
Individual transitions and sets of transitions are implemented as individual classes for use in transition relations of nite automata.

Implementation class 10.29 (TransPair) File: tr-pair Description: A TransPair represents a transition: a destination State and a CharRange
label. The member functions consist only of access members.
Implementation: The class is little more than a struct with access member functions. Performance: All of the member functions must be inline.

2

Implementation class 10.30 (Trans) File: trans Description: A Trans implements a set of transitions. Over and above the usual set
operations, it includes members to compute the image (a StateSet) of the Trans under a character. The class is used to implement nondeterministic transition relations.
Implementation: The implementation is through a Array<TransPair>. Additionally,
an integer is used to store the width of the StateSet returned by the image function.

2

Implementation class 10.31 (DTrans) File: dtrans Description: DTrans represents a deterministic set of transitions. As with Trans, it
includes a number of set operations. A member function computes the image (a State) of the DTrans under a character; if there is no applicable transition, the image is constant INVALIDSTATE. The class is used to implement deterministic transition relations.
Implementation: The implementation uses a Array<TransPair>. The member func-
tions used to construct the set of transition functions assume that the caller (the code adding the transitions) will ensure that the set remains deterministic.

2

274 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
10.6.5 Relations
Relations are used to implement "-transitions and labeled transitions. The di erent types of relations are used for di erent types of automata, depending upon the type of states used in the automata.
Implementation class 10.32 (IntIntRel) File: in-inrel Description: A binary relation on integers is implemented using a IntIntRel. The image
(or the re exive and transitive closure) of an integer or an IntSet can be computed (as an IntSet). Pairs of integers can be added or removed from the relation. Additionally, the cross product of two IntSets can be added to the relation.
Implementation: The implementation uses an Array<IntSet>. Most of the member
functions simply make use of the underlying Array or IntSet member functions.
Performance: The re exive and transitive closure member function could be imple-
mented more e ciently. 2
Implementation class 10.33 (StateStateRel, NodeNodeRel, PosnPosnRel) File: , ,st-strel no-norel po-porel Description: The four classes represent binary relations on States, Nodes, and Posns
(respectively). See IntIntRel for an explanation of the member functions available. These classes are used directly in the nite automata implementations.
Implementation: The classes are typedefs of IntIntRel.
2
Implementation class 10.34 (ItemItemRel) File: it-itrel Description: ItemItemRels are binary relations on items (dotted regular expressions).
As with IntIntRel, the usual relation member functions are implemented. Additional member functions allow new pairs to be added to the relation.
Implementation: Since the ItemSets are already split into the before and after the node
components, we store the ItemItemRel as four NodeNodeRels representing the following pair types:
Before-nodes and before-nodes

10.7. EXPERIENCES AND CONCLUSIONS

275

Before-nodes and after-nodes After-nodes and before-nodes After-nodes and after-nodes
The member functions make use of the three sub-objects.
Performance: The performance depends entirely upon that of NodeNodeRel.
2
Implementation class 10.35 (TransRel) File: transrel Description: Class TransRel implements a nondeterministic labeled transition relation.
The class is used to implement the (non-") transitions in nite automata. Member functions are available to compute the image of a StateSet and a character and to add transition triples (source State, label, destination State) to the relation.
Implementation: The class uses a StateTo<Trans>. Most of the member functions are
pass-throughs to the Trans members. 2
Implementation class 10.36 (DTransRel) File: dtransre Description: Class DTransRel is a deterministic labeled transition relation. The class
is used as the transition relation in deterministic nite automata. The member functions correspond to those in TransRel.
Implementation: The class uses a StateTo<DTrans>. The member functions are pass-
throughs to the DTrans member functions. 2

10.7 Experiences and conclusions
A large number of people (worldwide) have made use of the FIRE Engine, and a number of people have started using FIRE Lite. As a result, a great deal of experience and feedback has been gained with the use of the nite automata toolkits. Some of these are listed here.
The FIRE Engine and FIRE Lite toolkits were both created before the SPARE Parts. Without experience writing class libraries, it was di cult to devise a general purpose toolkit without having a good idea of what potential users would use the toolkit for. FIRE Lite has evolved to a form which now resembles the SPARE Parts (for example, the use of call-back functions).

276 CHAPTER 10. FIRE LITE: FAS AND RES IN C++
The FIRE Engine interface proved to be general enough to nd use in the following areas: compiler construction, hardware modeling, and computational biology. The additional exibility introduced with the FIRE Lite (the call-back interface and multithreading) promises to make FIRE Lite even more widely applicable. Thanks to the documentation and structure of the FIRE Engine and FIRE Lite, they have both been useful study materials for students of introductory automata courses. The SPARE Parts was developed some two years after the taxonomy in Chapter 4 had been completed. By contrast, FIRE Lite was constructed concurrently with the taxonomy presented in Chapter 6. As a result, the design phase was considerably more di cult (than for the SPARE Parts) without a solid and complete foundation of abstract algorithms. After maintaining and modifying several upgrades of the FIRE Engine, FIRE Lite is likely to be considerably easier to maintain and enhance.
10.8 Obtaining and compiling FIRE Lite
FIRE Lite is available for anonymous ftp from ftp.win.tue.nl in directory:
/pub/techreports/pi/watson.phd/firelite/
The toolkit (and any related documentation) is combined into a tar le. A number of di erent versions of this le are stored | each having been compressed with a di erent compression utility.
FIRE Lite was primarily developed under Watcom C++32 Version 9.5b on MS-Dos. The toolkit was also successfully compiled with Borland C++ Versions 3.1 and 4.0 under Microsoft Windows 3.1. A number of people have successfully ported the FIRE Engine to Unix platforms and there is every reason to believe that FIRE Lite will also be easy to port to any platform with a good C++ compiler.
As with the SPARE Parts, a version of FIRE Lite will remain freely available (though I retain the copyright on my code). Contributions to FIRE Lite are welcome.

Chapter 11
DFA minimization algorithms in FIRE Lite
In this chapter, we describe the implementation of the DFA minimization algorithms in the nite automata toolkit known as FIRE Lite.
11.1 Introduction
The FIRE Lite nite automata toolkit contains a number of implementations of DFA minimization algorithms as member functions of the C++ class FADFA.
Other toolkits that are available are also described (and compared to FIRE Lite) in Chapter 10. Only the Grail toolkit supports C++ class libraries of nite automata in the same way as FIRE Lite. While Grail supports a number of things that FIRE Lite does not (such as string elements that are not characters, but more complex structures), it does not have a selection of minimization algorithms as extensive as those in FIRE Lite.
Providing clients of FIRE Lite with a number of di erent minimization algorithms has proven to be useful. As we shall see in Chapter 15, each of the algorithms has a di erent performance pro le (when graphed against statistics on the DFAs to be minimized). As a result, the choice of which algorithm to use will depend upon the client's particular application area.
To a large extent, these implementations are inherited from the previous Eindhoven nite automata toolkit known as the FIRE Engine. FIRE Lite was described in detail in Chapter 10.
This chapter is structured as follows: Section 11.2 provides a description of each of the algorithms. Section 11.3 describes the foundation classes that are used by the minimization algorithms. Section 11.4 presents the conclusions of this chapter.
Since the minimization member functions are part of the de nition of class FADFA (and therefore the member functions are bundled with the toolkit), no separate information is provided on obtaining and compiling the minimization algorithms.
277

278 CHAPTER 11. DFA MINIMIZATION ALGORITHMS IN FIRE LITE
11.2 The algorithms
As in the taxonomy presented in Chapter 7, we implement two basic types of algorithms: Brzozowski's minimization algorithm and the minimization algorithms based upon computing an equivalence relation on states. In the next two sections, these two types of algorithms are presented.
11.2.1 Brzozowski's algorithm
Brzozowski's algorithm involves two applications of a function which reverses the DFA and makes it deterministic (since the reversal of a DFA is not necessarily deterministic). Template class FADFA has a member function to reverse the automaton and perform the subset construction. This member function is then used in Brzozowski's minimization algorithm.
User function 11.1 (void FADFA::reverse) File: fa-dfa Description: This member function is used to reverse a deterministic nite automaton.
It reverses all of the transitions of the automaton and makes the automaton deterministic. It has no return value.
Implementation: Member function reverse uses the deterministic abstract state mech-
anism to reverse the automaton. It uses class ASDReverse, which is local to class FADFA. The local class assists in constructing the reverse of the transition relation.
2
User function 11.2 (void FADFA::minBrzozowski) File: fa-dfa Description: This member function implements Brzozowski's minimization algorithm. It
minimizes the FADFA on which it is called and it has no return value.
Implementation: This function makes two calls to member function FADFA::reverse.
2
11.2.2 Equivalence relation algorithms
As described in Chapter 7, a number of the minimization algorithms are based upon the computation of an equivalence relation on states (of the DFA). In this section, we describe the implementation of a number of these algorithms. Some special member functions are provided to assist in the actual compression of the FADFA once the equivalence relation has been computed. In Section 11.3, we will consider some foundation classes which implement equivalence relations.

11.2. THE ALGORITHMS

279

Implementation function 11.3 (void FADFA::compress) File: fa-dfa Description: Given some equivalence relation on States, this member function compresses
the FADFA. For each equivalence class of the relation, it constructs a State in the new automaton. Since some of the minimization member functions construct a StateEqRel and some others construct a StateStateSymRel, this member function is overloaded to deal with both.
Implementation: This member traverses the set of equivalence classes of its argument,
constructing a new State for each of them. It also constructs a DTransRel, representing the transition relation, and a new set of nal States.
2
Implementation function 11.4 (State FADFA::split) File: fa-dfa Description: This member function is used by some of the minimization algorithms to
split equivalence classes of the equivalence relation. It takes a pair of States p and q, a CharRange a, and a reference to a StateEqRel. It assumes that the p and q are representatives of their particular equivalence classes. It splits the equivalence class of p into the set of States that have a transition to the equivalence class of q on a, and those that do not. If there was a successful split, p will be a representative of one of the new equivalence classes (resulting from the split) and the unique representative of the other new equivalence class is returned. The special State INVALIDSTATE is returned if the split was not successful.
Implementation: The implementation is a trivial one. It makes use of the StateEqRel
received as argument, and the DTransRel of the automaton. 2

Given these basic helper member functions, we are now in a position to describe the minimization algorithms themselves.
User function 11.5 (void FADFA::minDragon) File: fa-dfa Description: This member function is an implementation of ASU86, Algorithm 3.6,
p. 141], presented as Algorithm 7.21 in this dissertation. It is named the `Dragon' minimization algorithm after Aho, Sethi, and Ullman's `Dragon book'.
Implementation: The algorithm is a straightforward implementation of Algorithm 7.21.

280 CHAPTER 11. DFA MINIMIZATION ALGORITHMS IN FIRE LITE
2
User function 11.6 (void FADFA::minHopcroftUllman) File: fa-dfa Description: This member function is an implementation of Hopcroft and Ullman's min-
imization algorithm HU79], appearing as Algorithm 7.24 in this dissertation.
Implementation: This algorithm computes the distinguishability relation D. Initially,
pairs of States that are distinguishable are those pairs where one is nal and the other is non- nal. The transition relation is followed in reverse, marking distinguishable pairs. The iteration terminates when all distinguishable States have been considered.
Performance: The algorithm can be expected to run quite slowly since the implementa-
tion of the transition relation DTransRel is optimized for forward transitions. 2
User function 11.7 (void FADFA::minHopcroft) File: fa-dfa Description: This function implements Hopcroft's minimization algorithm Hopc71]. The
algorithm is presented as Algorithm 7.26 in this dissertation.
Implementation: The member function uses some encoding tricks to e ciently imple-
ment the abstract algorithm. The combination of the out-transitions of all of the States is stored in a CRSet named C. Set L from the abstract algorithm is imple-
mented as a StateTo< int >. L is interpreted as follows: if State q is a representative,
then the following pairs still require processing (they would be in set L in the abstract algorithm):
( q]; C0); : : : ; ( q]; CL q]?1)
The remaining pairs do not require processing:
( q]; CL q]); : : : ; ( q]; CjCj)
This implementation facilitates quick scanning of L for the next valid State-CharRange pair to be considered.
2
Implementation function 11.8 (int FADFA::areEq) File: fa-dfa

11.3. FOUNDATION CLASSES

281

Description: This member function is a helper to FADFA::minWatson. It implements
Algorithm 7.27 described in Section 7.4.6.
Implementation: This member function takes two more parameters than the abstract
algorithm: a StateEqRel and a StateStateSymRel. These parameters are used to discover equivalence (or distinguishability) of States earlier than the abstract algorithm would.
Performance: This member function should use memoization.
2
User function 11.9 (void FADFA::minWatson) File: fa-dfa Description: This member function implements the new minimization algorithm appear-
ing in Section 7.4.7. This algorithm is particularly interesting since it computes the equivalence relation from the safe side. If follows that this algorithm is usable in realtime applications, where some minimization is desired once a deadline has expired (see Section 7.4.7). The present implementation does not support interruptions in the computation of the equivalence relation.
Implementation: Helper member function FADFA::areEq is used.
2

11.3 Foundation classes
Some simple foundation classes are needed only by the minimization member functions of FIRE Lite.
Implementation class 11.10 (StateStateSymRel) File: sssymrel Description: This class is used to implement symmetrical relations on States. In some
of the minimization algorithms, a symmetrical relation is used to keep track of the States which have been compared to one another for equivalence.
Implementation: The implementation closely parallels that of class StateStateRel (see
Implementation class 10.33). The member functions are modi ed to accomodate the symmetry requirement.
2
Implementation class 11.11 (StateEqRel)

282 CHAPTER 11. DFA MINIMIZATION ALGORITHMS IN FIRE LITE
File: st-eqrel Description: Class StateEqRel implements an equivalence relation on States. It is used
to accumulate the approximations of relation E for minimizing an FADFA. There are member functions for accessing a unique set of equivalence class representatives and for iterating over the equivalence classes of the relation. Other members are provided to split equivalence classes and to merge equivalence classes.
Implementation: The implementation is via a StateTo<StateSet*>. Two States that be-
long to the same equivalence class point to the same StateSet. This allows extremely fast tests for equivalence. The other member functions are simple manipulations of these structures.
2
Implementation class 11.12 (ASDReverse) File: asdrever Description: This abstract deterministic state (see Section 10.5.4 for more on abstract
states) is used to compute the transition relation and new set of nal states while reversing an FADFA.
Implementation: The implementation maintains a StateSet (representing the current
states) and pointers to the components of the FADFA. 2
11.4 Conclusions
A number of the minimization algorithms derived in Chapter 7 have been implemented in FIRE Lite. Although the algorithms have been quite easy to present in an abstract manner in Chapter 7, the work required to implement them was anything but easy. The C++ implementations are usually several times more verbose than the abstract algorithms. The algorithms with the best running time analysis (such as Hopcroft's) also have the most intricate data structures | and therefore, require the most attention in a C++ implementation. Conversely, a minimization algorithm such as Brzozowski's has no data structures and is particularly simple to implement in FIRE Lite. In Chapter 15, we will see how this di erence in di culty of implementation can a ect the running time in practice.

Part IV The performance of the algorithms
283

Chapter 12
Measuring the performance of algorithms
We consider brie y some of the issues involved in collecting algorithm performance data. Little is known about the real-life performance of the algorithms derived and implemented in the preceding two parts of this dissertation. Such information is crucial to choosing an appropriate algorithm for a given application. In this part, we present performance data for a number of the most important algorithms.
The commercial success of software and hardware products is frequently dictated by the product's performance in practice. As a result, a great deal is known about methods for collecting performance data. Unfortunately, most of what is known relates to collecting benchmarking data for marketing purposes. Collecting and presenting benchmarking (performance) data in a fair way is particularly di cult. The guiding principles used in collecting and analyzing the data given in this part are as follows:
The performance data should be normalized since we are primarily interested in relative performances of the algorithms. To present performance data that is independent of the particular machine used, we execute the benchmarks on a variety of hardware. The relative performance of the algorithms can then be compared across hardware platforms. Execute the benchmarks on a single-user machine with little or no operating system overhead. Since the benchmarking code should be compiled with compiler optimizations enabled, the assembly language output from the compiler should be inspected to ensure that the compiler does not eliminate signi cant amounts of code. (This actually occurred during the collection of the benchmarking data presented in Chapter 13. A great deal of work was required to prevent the optimizing compiler from `optimizing away' the benchmarking code.) If a multiple-user operating system is used (such as Unix), ensure that the benchmarker is the only person logged-in, and that no non-standard background tasks are executed.
285

286 CHAPTER 12. MEASURING THE PERFORMANCE OF ALGORITHMS
Use a machine with su cient physical memory so that demand-paging e ects are negligible. We ignore the e ects of caching, since they are usually inherent in the hardware, and are a ected minimally by the operating system. Make use of vast amounts of input data for the algorithms. The distribution of the input data is chosen to represent a typical use of the algorithm in practice, e.g. the pattern matching algorithms were tested using English language input data (and the resulting English distribution of words, word lengths, and letter frequencies). Since this is only representative of English text searches, we also make use of genetic sequence data as input; the genetic sequences typically consist of longer patterns than those in English, and have a four letter alphabet. Collect a number of statistics on the input data and the program performance. Later, statistical analysis can be used to determine the relevant parameters, which can then be selected for presentation. Using the presented performance data, make clear recommendations about which algorithm to use in a given situation. The performance data should be compared against the theoretical space and time predictions and against other benchmarking results. This part is structured as follows: Chapter 13 presents performance data for some of the pattern matching algorithms implemented in the SPARE Parts. The algorithms selected were those expected to have the best tradeo of precomputation time versus pattern matching performance. In Chapter 14, we consider the performance of a number of the FA construction algorithms implemented in FIRE Lite. We compare the performance of most of the well-known algorithms. The performance of the DFA minimization algorithms (implemented in FIRE Lite) is presented in Chapter 15.

Chapter 13
The performance of pattern matchers
This chapter presents performance data on some pattern matching algorithms, and recommendations for the selection of an algorithm (given a particular application). The pattern matching problem, and algorithms solving it, are considered in Chapter 4.
The performance of all of the algorithms (running on a variety of workstation hardware) was measured on two types of input: English text and genetic sequences. The input data, which is the same as that used in the benchmarks of Hume and Sunday HS91], were chosen to be representative of two of the typical uses of pattern matching algorithms. The di erences between natural language text and genetic sequences serve to highlight the strengths and weaknesses of each of the algorithms. Until now, the performance of the multiple-keyword algorithms (Aho-Corasick and Commentz-Walter) had not been extensively measured.
The Knuth-Morris-Pratt and Aho-Corasick algorithms performed linearly and consistently (on widely varying keyword sets), as their theoretical running time predicts. The Commentz-Walter algorithm (and its variants) displayed more interesting behaviour, greatly out-performing even the best Aho-Corasick variant on a large portion of the input data. The recommendations section of this chapter details the conditions under which a particular algorithm should be chosen.
An early version of this chapter appeared as Wat94a].
13.1 Introduction and related work
Each of the algorithms tested involves some sort of precomputation on the set of keywords. Since the time involved in pattern matching usually far outweighs the time involved in precomputation, the performance of the precomputation algorithms is not discussed in this dissertation.
Performance data for the following selection of algorithms are presented in this chapter: The Knuth-Morris-Pratt (KMP) algorithm | Algorithm 4.84, appearing on page 77. Two variants of the Aho-Corasick (AC) algorithm: the `optimized' version (AC-OPT | Algorithm 4.53 appearing on page 64), and the failure function version (AC-FAIL 287

288 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS
| Algorithm 4.72 appearing on page 73). Two variants of the Commentz-Walter (CW) algorithm: the normal CommentzWalter algorithm (CW-NORM | Algorithm 4.4.5, appearing on page 95), and the weak Boyer-Moore algorithm (CW-WBM | Algorithm 4.4.7, appearing on page 97). All of the algorithms considered in this chapter have worst-case running time linear in the length of the input string. The running time of the AC-OPT algorithm is independent of the keyword set, while that of the KMP, AC-FAIL, CW-NORM, and CW-WBM algorithms depends (linearly in the case of CW-WBM and CW-NORM) upon the length of the shortest keyword in the keyword set. The KMP, AC-FAIL, CW-NORM, and CW-WBM algorithms can be expected to depend slightly on the keyword set size. Unfortunately, little is known about the relative performance (in practice) of the multiple-keyword algorithms. Only the Aho-Corasick algorithms are used extensively. The Commentz-Walter algorithms are used rarely (if ever), due to the di culty in correctly deriving the precomputation algorithms for the CW algorithms. The performance of the single-keyword algorithms in practice has been studied: In Smit82], Smit compares the theoretical running time and the practical running time of the Knuth-Morris-Pratt algorithm, a rudimentary version of the Boyer-Moore algorithm, and a brute-force algorithm. In HS91], Hume and Sunday constructed a taxonomy and explored the performance of most existing versions of the single-keyword Boyer-Moore pattern matching algorithm. Their extensive testing singled out several particularly e cient versions for use in practical applications. In Pirk92], Pirklbauer compares several versions of the Knuth-Morris-Pratt algorithm, several versions of the Boyer-Moore algorithm, and a brute-force algorithm. Since Pirklbauer did not construct a taxonomy of the algorithms, the algorithms are somewhat di cult to compare to one another and the testing of the Boyer-Moore variants is not quite as extensive as the Hume and Sunday taxonomy. We adopt the approach (due to Hume and Sunday) of evaluating the algorithms on two types of input data: natural language input strings, and input strings encoding genetic (DNA) information. In order to compare our test results with those of Hume and Sunday, we use a superset of the test data they used in HS91]. This chapter is structured as follows: Section 13.2 brie y outlines the algorithms tested. Section 13.3 describes the testing methodology, including the test environment, test data (and related statistics), and testing problems that were encountered. Section 13.4 presents the results of the testing. Most of the results are presented in the form of performance graphs. Section 13.5 gives the conclusions and recommendations of this chapter.

13.2. THE ALGORITHMS
13.2 The algorithms

289

To re-cap what is known from Chapter 4, the algorithms to be considered are: The Knuth-Morris-Pratt algorithm (KMP). This algorithm combines the use of indexing into the input string (and the single-keyword pattern) with a precomputed `failure function' to simulate a nite automaton. The algorithm never backtracks in the input string. The optimized Aho-Corasick algorithm (AC-OPT). This algorithm uses a Moore machine to nd matches. The Moore machine detects all matches ending at any given character of the input string. The algorithm never backtracks in the input string (it is an on-line algorithm) and examines each character of the input string only once. The failure function Aho-Corasick algorithm (AC-FAIL). This algorithm is similar to the AC-OPT algorithm. The Moore machine used in AC-OPT is compressed into two data-structures: a forward trie, and a failure function. These two data-structures can be stored more space-e ciently than the full Moore machine, with a penalty to the running time of the algorithm. The algorithm never backtracks in the input string, but it may examine a single character more than once before proceeding; despite this, it is still linear in the length of the input string. The Commentz-Walter algorithms (CW). In all versions of the CW algorithms, a common program skeleton is used with di erent shift functions. The CW algorithms are similar to the Boyer-Moore algorithm. A match is attempted by scanning backwards through the input string. At the point of a mismatch, something is known about the input string (by the number of characters that were matched before the mismatch). This information is then used as an index into a precomputed table to determine a distance by which to shift before commencing the next match attempt. The two di erent shift functions compared in this chapter are: the multiple-keyword BoyerMoore shift function (CW-WBM) and the Commentz-Walter normal shift function (CW-NORM). Recall (from our weakening steps in Section 4.4) that the CW-NORM shift function always yields a shift that is at least as great as that yielded by the CW-WBM shift function.
The precomputation required for each of these algorithms is described in WZ92]. We are interested in measuring the time required to nd all matches in a large input
string. In order to do this, we use an algorithm skeleton which repeatedly advances to the next match, registering each one. To eliminate the overhead of function calls, and obtain the raw performance of each of the algorithms, we inline all of the member function calls by hand. For each of the algorithms to be tested, this yields a program which nds all matches in an input string. Interestingly, after inlining, these algorithms are in C ISO90, KR88], with all C++ features being eliminated during the inlining. The resulting C code is the same as that given in the earlier version of this paper Wat94a, Appendix A]. The C

290 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS
language was used in order to extract the maximum performance from the implementations | on most workstations, the C compiler is the one with the highest quality of optimization. Having all function calls inlined means that the benchmarking versions of the toolkit will have higher performance than the version for client use. The data presented in this chapter is useful for considering the relative performance of the algorithms.
E orts were made to implement the algorithms as e ciently as possible, while preserving readability. For example, in the algorithms used in the performance tests, indexing (as opposed to pointers) was used when accessing characters of the input string; most optimizing compilers are able to `pointerize' such indices. (The pattern matching toolkit presented in Wat94a, Appendix A] contains pointer versions of the algorithms which can be used with non-optimizing compilers.)
The performance of the precomputation algorithms was not extensively measured. Some simple measurements, however, indicate that all of the algorithms required similar precomputation times for a given set of keywords.
13.3 Testing methodology
The performance of each of the pattern matching algorithms is linear in the size of the input string. The performance of the AC variants and the KMP algorithm is largely independent of the keyword set, while the CW algorithms running time depends on the keyword set. The testing of these algorithms is intended to determine the relative performance of the algorithms on two types of test data (each having di erent characteristics):
English text was chosen as the rst type of input data since it is the most common input to pattern matching programs such as fgrep. DNA sequences were chosen as the second type of input data since genome mapping projects make heavy use of pattern matching algorithms, and the characteristics of the input data are unlike the natural language input data. The testing of the algorithms is also intended to explore the dependence of the performance of the Commentz-Walter algorithms upon the keyword sets.
13.3.1 Test environment
The tests were performed on a DEC Alpha workstation (running OSF/1) with a 100 Mhz clock. A smaller number of tests were also performed on a HP Snake workstation and a Sun Sparc Station 1+. The tests showed that the relative performance data gathered on the Alpha is typical of what could be found on other high performance workstations.
During all tests, only one user (the tester) was logged-in. Typical Unix background processes ran during the tests. Since the running time of the algorithm was obtained using the getrusage system call, these background processes did not skew the algorithm performance data. The data-structures used in the testing were all in physical memory

13.3. TESTING METHODOLOGY

291

during the tests. No page faults were reported by ,getrusage and all data was accessed before the timed run (to ensure that they were in physical memory). Methods of disabling the cache memory were not explored. All of the frequently accessed data-structures were too large (frequently a megabyte) to t in a rst level cache. The linear memory access behaviour of all of the algorithms means that performance skewing due to caching e ects was negligible.

13.3.2 Natural language test data
The test data is a superset of that used by Hume and Sunday HS91]. The input alphabet consists of the 52 upper-case and lower-case letters of the alphabet, the space, and the newline characters. The input string is a large portion (999952 bytes) of the bible, organized as one word per line. Each algorithm was run 30 times over the input string, e ectively giving an input string of approximately 28 Megabytes in length (assuming that a Megabyte is 220 bytes). The bible was chosen as input since Hume and Sunday used it (and we wish to facilitate comparison of our data with that of Hume and Sunday), and it is freely redistributable.
Some data on the words making up the input string is shown in the following table:
Word length Number of words 1 4403 2 32540 3 55212 4 43838 5 23928 6 13010 7 9946 8 6200 9 4152 10 1851 11 969 12 407 13 213 14 83 15 22 16 5 17 1
There are a total of 196780 words; the mean word length is 4:08 and the standard deviation is 1:97.
The single-keywords sets are the same as those used by Hume and Sunday. They consist of 500 randomly chosen words, 428 of which appear in the input string. Some data on the keywords are shown in the following table:

292 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS
Word length Number of words 10 21 3 14 4 47 5 68 6 103 7 79 8 79 9 49 10 29 11 14 12 9 13 7 14 0 15 0 16 1
Note that there are no words of length 1, 14, or 15. The mean word length is 6:95 and the standard deviation is 2:17.
The multiple-keyword sets were all subsets of the words appearing in the input string. Preliminary testing showed that the performance of the algorithms (on English text) is almost entirely independent of the number of matches in the input string (providing that some sensible method of registering matches is used). A total of 4174 di erent keyword sets were generated using a random number generator ran1 appearing in PTVF92, p. 280]. The relatively even distribution of keyword set sizes can be seen in the following table:

13.3. TESTING METHODOLOGY

293

Keyword set size Number of sets 1 212 2 217 3 206 4 193 5 199 6 200 7 217 8 206 9 234 10 202 11 235 12 219 13 210 14 210 15 177 16 221 17 196 18 222 19 200 20 198
The mean keyword set size is 10:47 and the standard deviation is 5:73. An additional statistic (concerning the multiple-keyword sets) was recorded: the length
of the shortest keyword in any given keyword set; for a given keyword set the CW-WBM and CW-NORM shift distances are bounded above by the length of the shortest keyword in the set. The data are as follows:
Length of shortest keyword Number of keyword sets 14 2 196 3 1033 4 1698 5 719 6 286 7 100 8 70 9 47 10 12 11 5 12 2 13 1 14 1

294 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS
The mean is 4:19 and the standard deviation is 1:36.
13.3.3 DNA sequence test data
The test data consists of the input string used by Hume and Sunday HS91], and randomly generated keyword sets. The input alphabet consists of the four letters a, c, g, and t (standing for adenine, guanine, cytosine, and thymine, respectively) used to encode DNA, and the new-line character. The input string is a portion (997642 bytes) of the GenBank DNA database, as distributed by Hume and Sunday. Each algorithm was run 30 times over the input string, e ectively giving an input string of approximately 28 Megabytes in length.
A total of 450 keyword sets were randomly chosen (the keywords are all substrings of the input string). Within each keyword set, all keywords were of the same length. The keyword sets were distributed evenly with set sizes ranging from 1 to 10 and keyword lengths ranging from 100 to 900 (in increments of 100).
13.4 Results
The performance of the algorithms was measured on thirty iterations over the input string. The running time on both types of test data was found to be independent of the number of matches found in the input string. This is mostly due to the fact that the algorithms register a match by recording one integer, and incrementing a pointer | cheap operations on most processors.
The performance of each algorithm was graphed against the size of the keyword sets, and against the lengths of the shortest keyword in a set. Graphing the performance against other statistics (such as the sum of the lengths of the keywords, or the length of the longest keyword in a set) was not found to be helpful in comparing the algorithms.
13.4.1 Performance versus keyword set size
For each algorithm, the average number of megabytes (of natural language input string) processed per second was graphed against the size of the keyword set. The four graphs (corresponding to AC-FAIL, AC-OPT, CW-WBM, and CW-NORM) are superimposed in Figure 13.1.
As predicted, the AC-OPT algorithm has performance independent of the keyword set size. The AC-FAIL algorithm has slightly worse performance, with a slight decline as the keyword set size increases. The CW-WBM and CW-NORM algorithms perform similarly to one another, with the CW-NORM algorithm performing slightly better. This follows from the fact that the shift predicate used in the CW-WBM algorithm is a weakening of the one used in the CW-NORM algorithm (see Section 4.4). The performance of both CW algorithms decreases noticeably with increasing keyword set sizes, eventually being outperformed by the AC-OPT algorithm at keyword set sizes greater than 13.

13.4. RESULTS

295

30 25 20 MB/s 15 10 5 0
0

CW-WBM CW-NORM
AC-OPT AC-FAIL

5 Number o1f0keywords 15

20

Figure 13.1: Algorithm performance (in megabytes/second) versus keyword set size. The performance lines of the CW-WBM and CW-NORM algorithms are almost coincidental (shown as the solid descending line).

Figure 13.2 presents the ratio of the CW-WBM performance to the CW-NORM performance | clearly showing CW-NORM outperforming CW-WBM. The gure also shows that the performance gap between the two algorithms widens somewhat with increasing keyword set size.
The AC algorithms displayed little to no variance in performance for a given keyword
set size. The median performance data (with +1 and ?1 standard deviation bars) for
AC-FAIL are shown in Figure 13.3, while those for AC-OPT are shown in Figure 13.4. In both graphs, the standard deviation bars are very close to the median, indicating that both algorithms display very consistent performance for a given keyword set size.
The CW algorithms displayed a large variance in performance, as is shown in Figures 13.5 and 13.6 (for CW-WBM and CW-NORM, respectively). These gures show a noticeable narrowing of the standard deviation bars as keyword set size increases. The variance in both algorithms is almost entirely due to the variance in minimum keyword length for a given keyword set size.
For each algorithm, the average number of megabytes of DNA input string processed per second was graphed against keyword set size. The results are superimposed in Figure 13.7. The performance of the algorithms on the DNA data was similar to their performance on the natural language input data. The performance of the AC-OPT algorithm was independent of the keyword set size, while the performance of the AC-FAIL algorithm declined slightly with increasing keyword set size.

296 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

CW-WBM / CW-NORM 1.02

1 0.98 0.96 0.94 0.92 0.9

+++++++

++++++++++

++++++++++++

++++++++++++++ +

+++++++++++++

+++++++++++++++ +

++++++++++++++++

++++++++++++++ + +

++++++++++++++++ +

++++++++++++++++ + +

+++++++++++++++++++

+++++++++++++++++ +

++++++++++++++++++ ++

+++++++++++++++++++++++ +

++++++++++++++++++

+++++++++++++++++++ +

++++++++++++++++++ +

++++++++++++++++++++++ +

+++++++++++++++++++++ +

++++++++++++++++++++++ +

0.88

0

5 Number o1f0keywords 15

20

Figure 13.2: The ratio of CW-WBM performance to CW-NORM performance versus keyword set size. Some data-points are greater than 1:00 (although theoretically this should not occur), re ecting timing anomalies due to the limited timer resolution.

AC-FAIL 30
AC-FAIL 3
25

20

MB/s 15

10

5 33333333333333333333

0

0

5 Number o1f0keywords 15

20

Figure 13.3: Performance variation (in megabytes/second) versus keyword set size for the
AC-FAIL algorithm. Median performance is shown as a diamond, with +1 and ?1 standard
deviation bars.

13.4. RESULTS

297

30 AC-OPT
AC-OPT 3
25

20

MB/s 15

10

5 33333333333333333333

0

0

5 Number o1f0keywords 15

20

Figure 13.4: Performance variation (in megabytes/second) versus keyword set size for the AC-OPT algorithm.

CW-WBM 30
CW-WBM 3
25

20

MB/s 15 3

10 5

3333333333333333333

0

0

5 Number o1f0keywords 15

20

Figure 13.5: Performance variation (in megabytes/second) versus keyword set size for the CW-WBM algorithm.

298 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

30 CW-NORM
CW-NORM 3
25

20

MB/s 15 3

10 5

3333333333333333333

0

0

5 Number o1f0keywords 15

20

Figure 13.6: Performance variation (in megabytes/second) versus keyword set size for the CW-NORM algorithm.

30 25 20 MB/s 15 10 5 0
0

Genetic sequence input CW-WBM CW-NORM AC-OPT AC-FAIL

2 Num4 ber of keywo6rds 8

10

Figure 13.7: Algorithm performance (in megabytes/second) versus keyword set size, for the DNA test data. The performance of the CW-NORM and CW-WBM algorithms are almost coincidental, shown as the descending solid line.

13.4. RESULTS

299

30

CW-WBM

25

CW-NORM AC-OPT

AC-FAIL

20

MB/s 15

10

5

0 0 2 4 Short6est keyw8ord lengt1h0 12 14

Figure 13.8: Algorithm performance (in megabytes/second) versus the length of the shortest keyword in a given set. The performance of the CW-WBM and CW-NORM algorithms are almost coincidental (shown as the ascending solid line).
The performance of the CW algorithms, which declined with increasing keyword set size, was consistently better than the AC-OPT algorithm. In some cases, the CW-NORM algorithm displayed a ve to ten-fold improvement over the AC-OPT algorithm.
13.4.2 Performance versus minimum keyword length
For each algorithm, the average number of megabytes processed per second was graphed against the length of the shortest keyword in a set. For the multiple-keyword tests the graphs are superimposed in Figure 13.8.
Predictably, the AC-OPT algorithm has performance that is independent of the keyword set. The AC-FAIL algorithm has slightly lower performance, improving with longer minimum keywords. The average performance of the CW algorithms improves almost linearly with increasing minimum keyword lengths. The low performance of the CW algorithms for short minimum keyword lengths is explained by the fact that the CW-WBM and CW-NORM shift functions are bounded above by the length of the minimum keyword (see Chapter 4). For sets with minimum keywords no less than than four characters, the CW algorithms outperform the AC algorithms.
As predicted, the CW-NORM algorithm outperforms the CW-WBM algorithm. The performance ratio of the CW-WBM algorithm to the CW-NORM algorithm is shown in Figure 13.9. The gure indicates that the performance gap is wide with small minimum keyword lengths, and diminishes with increasing minimum keyword lengths. (This e ect

300 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

CW-WBM / CW-NORM 1.02

1 0.98 0.96 0.94 0.92 0.9

+ ++++++++++++++++++++++ ++++++++++++++++++++++++++ ++++++++++++++ +++++++++++++ ++++++++++++++++ +++++++++++ +++++++ +++++ +++ ++ + + + +++ +

0.88 0 2 4 Short6est keyw8ord lengt1h0 12 14

Figure 13.9: The ratio of CW-WBM performance to CW-NORM performance versus the length of the shortest keyword in a set. Some data-points are greater than 1:00, re ecting timing anomalies due to the limited timer resolution.
of the diminishing performance gap is partly due to the distribution of keyword lengths in the test data.)
The AC-FAIL algorithm displayed some variance in performance, as shown in Figure 13.10. The apparent greater variance for shorter minimum keyword lengths is partially due to the distribution of keyword lengths in the test data. The AC-OPT algorithm showed practically no variance in performance, as shown in Figure 13.11.
The CW algorithms displayed a large variance in performance for given minimum key-
word lengths. The median performance (with +1 and ?1 standard deviation bars) of the
CW-WBM algorithm are shown in Figure 13.12, while those for CW-NORM are shown in Figure 13.13. The variance increases with increasing shortest keyword length. At a shortest keyword length of 11, the variance decreases abruptly due to the distribution of the shortest keyword lengths of the keyword sets; there are few keyword sets with shortest keyword length greater than 10 characters. The variance in the performance of the CW algorithms is due to the variance in keyword set size (for any given minimum keyword length).
The performance in megabytes of DNA input string processed per second of each algorithm was also graphed against keyword length1. The results are superimposed in Figure 13.14. The performance of the algorithms on the DNA data was similar (though not as dramatic) to their performance on the natural language input data. The performance of
1Recall that the keywords in a given keyword set were all of the same length.

13.4. RESULTS

301

30 AC-FAIL
AC-FAIL 3
25 20 MB/s 15 10
5 33333333333333
0 0 2 4 Short6est keyw8ord lengt1h0 12 14
Figure 13.10: Performance variation (in megabytes/second) versus the length of the shortest keyword in a set for the AC-FAIL algorithm.

AC-OPT 30
AC-OPT 3
25 20 MB/s 15 10
5 33333333333333
0 0 2 4 Short6est keyw8ord lengt1h0 12 14
Figure 13.11: Performance variation (in megabytes/second) versus the length of the shortest keyword in a set for the AC-OPT algorithm.

302 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

30 CW-WBM

CW-WBM 3

25

20 MB/s 15
10 5 0

33333333 333333

0 2 4 Short6est keyw8ord lengt1h0 12 14

Figure 13.12: Performance variation (in megabytes/second) versus the length of the shortest keyword in a set for the CW-WBM algorithm.

CW-NORM

30

CW-NORM 3

25

20 MB/s 15
10 5

33333333 333333

0

0 2 4 Short6est keyw8ord lengt1h0 12 14

Figure 13.13: Performance variation (in megabytes/second) versus the length of the shortest keyword in a set for the CW-NORM algorithm.

13.4. RESULTS

303

30 25 20 MB/s 15 10 5 0
0

Genetic sequence input CW-WBM CW-NORM AC-OPT AC-FAIL

200 Sho4rt0e0st keyword6l0e0ngth 800

1000

Figure 13.14: Algorithm performance (in megabytes/second) versus the length of the keywords in a given set, for the DNA test data. The performance of the CW-NORM and CW-WBM algorithms are almost coincidental, shown as the ascending solid line.
the AC-OPT algorithm was independent of the keyword length. Unlike on the natural language input, the AC-FAIL algorithm displayed no noticeable improvement with increasing keyword length; the performance of the AC-FAIL algorithm was little more than half of the performance of the AC-OPT algorithm.
As in the natural language tests, the performance of the CW algorithms improved with increasing keyword length. The rate of performance increase was considerably less than on the natural language input (see Figure 13.8). On the DNA input, the CW algorithms displayed median performance at least twice that of the AC-OPT algorithm.
13.4.3 Single-keywords
For the single-keyword tests, the average performance (of each algorithm) is graphed against the length of the keyword and superimposed in Figure 13.15.
The KMP, AC-FAIL, and AC-OPT algorithms displayed performance that was largely independent of the keyword length. The AC-OPT algorithm outperformed the other two, while the KMP algorithm displayed the worst performance. Although the KMP algorithm is similar in structure to the AC-FAIL algorithm, the heavy use of indexing (as opposed to the use of pointers in AC-FAIL) in the KMP algorithm degrades its performance. (The use of indexing makes the KMP algorithm more space e cient than the AC algorithms.) The performance of the CW algorithms improved almost linearly with the length of the keyword, with the CW-NORM algorithm outperforming the CW-WBM algorithm.

304 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

30

CW-WBM

25

CW-NORM AC-OPT

AC-FAIL

20 KMP

MB/s 15

10

5

0 024

6Keywor8d length10 12 14 16

Figure 13.15: Algorithm performance (in megabytes/second) versus the length of the (single) keyword. The performance of the KMP and AC-FAIL algorithms are shown as the coincidental dotted horizontal line, while those of the CW-WBM and CW-NORM algorithms are shown as the coincidental ascending solid line.

13.5. CONCLUSIONS AND RECOMMENDATIONS

305

30 25 20 MB/s 15 10 5 0
0

KMP
KMP 3

333333333333
2 4 6Keywor8d length10 12 14

3
16

Figure 13.16: Performance variation (in megabytes/second) versus the (single) keyword length for the KMP-FAIL algorithm.
The variance of the performance of the KMP and the AC-FAIL algorithms was minor, as shown in Figures 13.16 and 13.17 (respectively). The AC-OPT algorithm displayed no noticeable variance over the entire range of keyword lengths, as is shown in Figure 13.18. The CW algorithms showed some variance (increasing with longer keyword lengths) as shown in Figures 13.19 and 13.20 respectively.
The performance of the algorithms on the single keyword test data is in agreement with the data collected by Hume and Sunday HS91].
13.5 Conclusions and recommendations
The conclusions of this chapter fall into two categories: general conclusions regarding the algorithms and testing them, and conclusions relating to the performance of speci c algorithms. The general conclusions are:
The relative performance of the algorithms did not vary across the testing platforms (the DEC Alpha, HP Snake, and Sun Sparc Station 1+ workstations). Testing the algorithms on two vastly di ering types of input (English text and DNA sequences) indicates that varying such factors as alphabet size, keyword set size, and smallest keyword length can produce very di erent rates of performance increase or decrease.

306 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

30 25 20 MB/s 15 10 5 0
0

AC-FAIL

AC-FAIL 3

333333333333
2 4 6Keywor8d length10 12 14

3
16

Figure 13.17: Performance variation (in megabytes/second) versus the (single) keyword length for the AC-FAIL algorithm.

30 25 20 MB/s 15 10 5 0
0

AC-OPT

AC-OPT 3

333333333333

3

2 4 6Keywor8d length10 12 14 16

Figure 13.18: Performance variation (in megabytes/second) versus the (single) keyword length for the AC-OPT algorithm.

13.5. CONCLUSIONS AND RECOMMENDATIONS

307

30 25 20 MB/s 15 10 5 0
0

CW-WBM
CW-WBM 3 3 333333333333
2 4 6Keywor8d length10 12 14 16

Figure 13.19: Performance variation (in megabytes/second) versus the (single) keyword length for the CW-WBM algorithm.

30 25 20 MB/s 15 10 5 0
0

CW-NORM

CW-NORM 3 3

333333333333

2 4 6Keywor8d length10 12 14 16

Figure 13.20: Performance variation (in megabytes/second) versus the (single) keyword length for the CW-NORM algorithm.

308 CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS
Comparing algorithm performance to keyword set size and shortest keyword length proved to be more useful (in selecting an algorithm) than comparing performance to other statistics. The speci c performance conclusions are: The performance of the AC-OPT algorithm was independent of the keyword sets. The AC-FAIL and KMP algorithm performance increased slightly with increasing shortest keyword length and decreased with increasing keyword set size. The performance of the CW algorithms improved approximately linearly with increasing length of the shortest keyword in the keyword set. The rate of increase was much greater with natural language input than with DNA input. The performance of the CW algorithms declined sharply with increasing keyword set size. For a given keyword set size and shortest keyword length, the AC and KMP algorithms displayed little or no variance (in performance). The CW algorithms displayed slightly more variance in performance. As predicted in Section 4.4, the CW-NORM algorithm outperforms the CW-WBM algorithm. Under certains conditions, the di erence in performance can be substantial. (Furthermore, the cost of precomputation for the two algorithms is approximately the same.) The AC-OPT algorithm always outperforms the AC-FAIL algorithm; both require similar precomputation, but the AC-FAIL data structures can be made more space e cient. On the single-keyword tests:
{ The single-keyword test results were consistent with those presented by Hume
and Sunday HS91].
{ The AC-FAIL algorithm always outperforms the KMP algorithm. { In most cases, the CW algorithms outperform the AC algorithms.
In Aho90, p. 281], A.V. Aho states that In practice, with small numbers of keywords, the Boyer-Moore aspects of the Commentz-Walter algorithm can make it faster than the Aho-Corasick algorithm, but with larger numbers of keywords the Aho-Corasick algorithm has a slight edge.
Although Aho's statement is correct, with the performance data presented in this report, we are able to state more precisely the conditions under which the CommentzWalter algorithms outperform the Aho-Corasick algorithms.

13.5. CONCLUSIONS AND RECOMMENDATIONS

309

On the multiple-keyword natural language tests, the CW algorithms outperformed the AC algorithms when the length of the shortest keyword was long (in general, at least four symbols) and the keyword set size was small (in general, fewer than thirteen keywords). The performance di erence between the CW algorithms and the AC algorithms was frequently substantial. On the DNA tests, the CW algorithms substantially outperformed the AC algorithms. On these tests the keyword length was at least 100 and the number of keywords in the set was no more than 10. The DNA results show that the CW algorithms can yield much higher performance than the often-used AC-OPT algorithm in areas such as genetic sequence pattern matching. For applications involving small alphabets and long keywords (such as DNA pattern matching), the performance of the CW-NORM algorithm makes it the algorithm of choice. Only when the keyword set size is much larger than ten keywords should the AC-OPT algorithm be considered. The following procedure can be used to choose a pattern matching algorithm for a natural language pattern matching application:
if performance independent of keyword set is required then
AC-OPT
else if multiple-keyword sets are used then if fewer than thirteen keywords and the shortest keyword length is at least four then
CW-NORM
else
choose an AC algorithm
else (single-keyword sets) if space is severely constrained then
KMP
else if the keyword length is at least two then
CW-NORM
else
choose an AC algorithm

An AC algorithm can be chosen as follows:
if space e ciency is needed then
AC-FAIL

310
else
AC-OPT

CHAPTER 13. THE PERFORMANCE OF PATTERN MATCHERS

Chapter 14
The performance of FA construction algorithms
This chapter presents performance data on a number of FA (and DFA) construction algorithms. The time required to construct an FA was measured (for each of the constructions) as well as the time required for a single transition (for each of the types of FAs). The implementations given in FIRE Lite and the FIRE Engine were used (see Chapter 10). Additionally, we present recommendations for selecting a construction. The algorithms discussed here are a selection of the ones derived in the taxonomy in Chapter 6.
14.1 Introduction
Most of what is known about the relative performance of the automata construction algorithms is anecdotal. As with the minimization algorithms, most software engineers choose a construction algorithm that is simple or easy to understand. Such choices should, however, be based upon performance data about the algorithms.
In this chapter, we present performance data on eight of the most e cient and easiest to implement constructions.
This chapter is structured as follows: In Section 14.2, we list the algorithms used in collecting the performance data. The testing methodology used in benchmarking the algorithms is described in Section 14.3. The results of the benchmarking are presented in Section 14.4. Lastly, the conclusions and recommendations of this chapter are given in Section 14.5.
14.2 The algorithms
The algorithms tested were derived in Chapter 6. They have also been implemented in FIRE Lite. The implementations are discussed in detail in Chapter 10. For convenience,
311

312 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS
we will put the algorithms in two groups: those producing an FA, and those producing a DFA. The FA constructions are:
The canonical construction (TH, since it is a variant of Thompson's construction), appearing as Construction 6.15 in this dissertation. The Berry-Sethi construction (BS), given here as Construction 6.39 (rem-", sym, a-s). The dual of the Berry-Sethi construction (BS-D), appearing as Construction 6.65 (rem-"-dual, sym, a-s). The DFA constructions are: The Aho-Sethi-Ullman construction (ASU) | Construction 6.69 (rem-"-dual, sym, a-s, e-mark, subset, use-s). The deterministic item set construction (IC) | Construction (rem-", subset, uses) on page 156. DeRemer's construction (DER) | Construction (rem-", subset, use-s, Xfilt) on page 159. The ltered item set construction (FIC) | Construction (rem-", subset, use-s, Wfilt) on page 158. The McNaughton-Yamada-Glushkov construction (MYG), given as Construction 6.44 (rem-", sym, a-s, subset, use-s). For speci c information on these algorithms, see Chapter 6. Noticeably absent from this list are the derivatives-based algorithms (Brzozowski's and Antimirov's algorithms). The derivatives in these algorithms are the states. In their pure forms, the derivatives are stored as regular expressions. The space and time required to store and manipulate the regular expressions proved to be extremely costly, when compared to the representations of states used in some of the other constructions. The derivativebased algorithms consistently performed 5 to 10 times slower than the next slowest algorithm (the IC algorithm, in particular). Indeed, the preliminary testing could only be done for the smallest regular expressions without making use of virtual memory (which would further degrade their performance). No doubt the use of clever coding tricks would improve these algorithms greatly | though such coding tricks would yield a new algorithm.
14.3 Testing methodology
This section gives an overview of the methods used in gathering the test data. We begin with the details of the test environment, followed by the details of the methods used to generate the regular expressions for input to the algorithms.

14.3. TESTING METHODOLOGY

313

14.3.1 Test environment

All of the tests were performed on an IBM-compatible personal computer running MSDos. The machine has an Intel Pentium processor with a 75 Mhz clock, an o -chip cache of 256 kilobytes and main memory of 8 megabytes. During all of the tests, no other programs which could consume processing power were installed.
The test programs were compiled with the Watcom C++32 compiler (version 9.5a) with optimizations for speed. The Watcom compiler is bundled with an MS-Dos extender (used to provide virtual memory for applications with large data-structures) known as DOS/4GW. Since the use of virtual memory could a ect the performance data, all datastructures were made to t in physical memory.
Timing was done by reprogramming the computer's built-in counter to count microseconds. This reprogramming was encapsulated within a C++ timer class which provided functionality such as starting and stopping the timer. Member functions of the class also subtracted the overhead of the reprogramming from any particular timing run.

14.3.2 Generating regular expressions
A large number of regular expressions were randomly generated. As in Chapter 13, we used the random number generator appearing in PTVF92, p. 280]. The regular expressions were generated as follows:
1. A height in the range 2; 5] was randomly chosen for the parse tree of the regular expression. (Larger heights were not chosen for memory reasons; smaller heights were not chosen since the constructions were performing close to the clock resolution.)
2. A regular expression of the desired height was generated, choosing between all of the eligible operators1 with equal probability.
3. For the leaves, and " nodes were never chosen. The was omitted, since such regular expressions prove to be uninteresting (they simply denote the empty language). Similarly, the " was omitted, since the same e ect is obtained by generating ? nodes.
The following table shows the number of nodes in an RE and the number of REs with that number of nodes.
1Some operators will not be eligible. For example, to generate an RE of height 3, only the unary or binary operators can appear at the root.

314 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS Number of nodes Number of REs 2 500 4 84 5 118 6 59 7 246 8 346 9 279 10 413 11 251 12 264 13 227 14 196 15 131 16 73 17 73 18 75 19 45 20 40 21 17 22 12 23 6 24 5 25 1 26 1
There are a total of 3462 REs; the mean size is 9.63 nodes and the standard deviation is 4.72. The distribution of the number of nodes re ects the way in which the regular expressions were generated. Note that there are no regular expressions with a single node or with three nodes. These were omitted since the time to construct an FA from such small REs was usually below the resolution of the timer. (Two node regular expressions were used since they contain a or a + node at the root. All of the constructions require more time to construct an automaton corresponding to such an expression.) Furthermore, it is not possible to generate lengthy strings in the language of such regular expressions.
In the following table, we give the number of symbol nodes in an RE and the number of REs with that number of nodes.

14.4. RESULTS

315

Number of symbol nodes Number of REs 1 500 2 247 3 657 4 773 5 553 6 367 7 181 8 114 9 45 10 17 11 6 12 2
The mean number of symbol nodes is 4.00 nodes and the standard deviation is 1.99. Other statistics on the regular expressions were also collected, such as the height of the
REs, the star-height of the REs, and some measure of the inherent nondeterminism in the REs2. These statistics did not prove to be useful in considering the performance of the algorithms.

14.3.3 Generating input strings
For each type of automaton (FA, "-free FA, and DFA), we also present data on the time required to make a single transition. In order to measure this, for each RE used as input to the constructions we generate a string in the pre x of the language denoted by the RE. Strings of length up to 10000 symbols were generated.
The constructed automaton processes the string, making transitions, while the timer is used to measure the elapsed time. The time was divided by the number of symbols processed, yielding the average time for a single transition.

14.4 Results
The performance data will be presented in three sections. First, we present the time required to construct an automaton. Next, we present the size of the constructed automaton. Lastly, we consider the time required to make a single transition.

14.4.1 Construction times
For each of the generated regular expressions and each of the constructions, we measured the number of microseconds to construct the automaton. For many applications, the
2One estimate of such nondeterminism is the ratio of alternation (union) nodes and or + nodes to the total number of nodes in the regular expression.

316 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

2500 BS BS-D
2000 TH

Group 1

1500 s
1000

500

0 0 5 10 15 20 25 30 Number of nodes
Figure 14.1: Median construction times for FA constructions graphed against the number of nodes in the regular expressions. Note that BS and BS-D are superimposed as the higher ascending line.

construction time can be the single biggest factor in determining which construction to use.
First, the constructions are divided into two groups: those producing an FA (not a DFA), and those producing a DFA. The median performance of these two groups of constructions was graphed against the number of nodes in the regular expressions in Figures 14.1 and 14.2 respectively. All three of the algorithms in the rst group are predicted to perform linearly in the number of nodes in the input regular expression. In Figure 14.1, the performance of the BS and BS-D constructions was nearly identical, as could be predicted from their duality relationship given in Chapter 6. They both performed somewhat worse than the TH algorithm. The apparent jump in construction time (of the TH algorithm) for 25 node regular expressions is due to the fact that only a single such expression was generated. Had more regular expressions been generated, the median performance would have appeared as a straight line (following the linear performance predicted for the TH algorithm).
The scale on Figure 14.2 shows that the second group of constructions were much slower than the rst group. The ASU construction was by far the fastest, with FIC and MYG being the middle performers, and DER and IC being the slowest. In the range of 20 to 26 nodes, all of the constructions displayed peaks in their construction times. In these cases, some of the generated regular expressions have corresponding DFAs which are exponentially larger | forcing all of the constructions to take longer.
We now consider the performance of the same two groups of constructions, graphed

14.4. RESULTS

317

25000
20000
15000 s
10000

ASU DER
IC FIC MYG

Group 2

5000

0 0 5 10 15 20 25 30 Number of nodes
Figure 14.2: Median construction times for DFA constructions graphed against the number of nodes in the regular expressions. The lowest line is ASU performance, while the middle pair of lines are MYG and FIC; the highest pair of lines is DER and IC.

against the number of symbol nodes in the regular expressions. The graphs appear in Figures 14.3 and 14.4 respectively. These two graphs present similar information to Figures 14.1 and 14.2.

318 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

2500 BS
BS-D 2000 TH

Group 1

1500 s
1000

500

0 0 2 4 Num6ber of sym8 bols 10 12 14
Figure 14.3: Median construction times for FA constructions graphed against the number of symbol nodes in the regular expressions. The BS and BS-D performance is identical, graphed as the higher line.

30000

ASU

25000

DER IC

FIC

20000 MYG

Group 2

s 15000

10000

5000

0 0 2 4 Num6ber of sym8 bols 10 12 14
Figure 14.4: Median construction times for DFA constructions graphed against the number of symbol nodes in the regular expressions. The best performance was delivered by ASU, followed by FIC and MYG and lastly by DER and IC.

14.4. RESULTS

319

45

40

35

30

States

25 20

15

10

5

0 0

BS BS-D
TH 5

Group 1

10 15 20 Number of nodes

25

30

Figure 14.5: The number of states in the FA is graphed against the number of nodes in the regular expression used as input for the TH, BS, and BS-D constructions. Note that the BS and BS-D constructions produce automata of identical size.

14.4.2 Constructed automaton sizes
The size of each of the constructed automata was measured. The amount of memory space consumed by an automaton is directly proportional to the number of states in the automaton, and this data can be used to choose a construction based upon some memory constraints. Since the exact amount of memory (in bytes) consumed depends heavily on the compiler being used, we present the data in state terms.
Again, we group the non-DFA producing constructions and the DFA constructions. Figures 14.5 and 14.6 give the automata sizes versus number of nodes in the regular expressions, for the two groups of constructions. The former gure shows that the size of the TH-generated automata can grow quite rapidly. The BS and BS-D constructions produce automata of identical size (as can be seen from their derivations in Chapter 6). In the second gure (Figure 14.6), we can identify two interesting properties of the constructions: ASU and FIC produce automata of the same size, as do the pair IC and MYG. Given the superior performance of ASU (over FIC), there is little reason to make use of FIC. Similarly, the MYG construction out-performs IC, and there is no reason to use IC since the automata will be the same size.

320 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

20 18 16 14 12 States 10 8 6 4 2 0
0

ASU DER
IC FIC MYG
5

Group 2

10 15 20 Number of nodes

25

30

Figure 14.6: The number of states in the DFA is graphed against the number of nodes in the regular expression used as input for the ASU, DER, FIC, IC, and MYG constructions. Note that the FIC and ASU constructions produce DFAs of identical size (the superimposed lower line), as did the pair IC and MYG (the superimposed higher line).

14.4. RESULTS

321

45

40

BS BS-D

35 TH

30

States

25 20

15

10

5

0 02

Group 1 4 Num6ber of sym8 bols 10 12 14

Figure 14.7: The number of states in the FA is graphed against the number of symbol nodes in the regular expression used as input for the TH, BS, and BS-D constructions. Note that the BS and BS-D constructions produce automata of identical size (the lower ascending line).

Figures 14.7 and 14.8 give the automata sizes versus number of symbol nodes in the regular expressions, for the two groups of constructions. These two graphs provide similar information to that given in Figures 14.5 and 14.6.
14.4.3 Single transition performance
The time required to make a single transition was measured for FAs (using TH), "-free FAs (using BS), and DFAs (using FIC). The median time for the transitions has been graphed against the number of states in Figure 14.9. (Note that, for a given number of states, the number of each of the di erent types of automata varied.) The more general FAs displayed the slowest transition times, since the current set of states is stored as a set of integers, and the "-transition and symbol transition relations are stored in a general manner. The "-free FAs displayed much better performance, largely due to the time required to compute "-transition closure in an automaton with "-transitions. With the simple array lookup mechanism used in DFAs, it is not surprising that their transitions are by far the fastest and are largely independent of the number of states in the DFA.
The individual performance data for FAs, "-free FAs, and DFAs are shown in Figures 14.10, 14.11, and 14.12. The variance for general FAs and "-free FAs is quite large. In both cases, the time for a single transition depends upon the number of states in the current state set. The variance for a DFA transition appears to be quite large. This is

322 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

20 18

ASU DER

16 14

IC FIC MYG

12

States 10

8

6

4

2

0 02

Group 2 4 Num6ber of sym8 bols 10 12 14

Figure 14.8: The number of states in the DFA is graphed against the number of symbol nodes in the regular expression used as input for the ASU, DER, FIC, IC, and MYG constructions. Note that the ASU and FIC constructions produce DFAs of identical size, as do the MYG and IC pair.

All types

s

700 600 500 400 300

FA EFFA DFA

3+2

3 3333333333 33333

333

200 3 3

100 0

2+2+2+2+2+2+2+2+2+2+2+2222222222

22

0 5 10 15 20 25 30 35 40 45

Number of states

Figure 14.9: Median time to make a single transition in microseconds ( s), for FAs, "-free FAs, and DFAs, versus the number of states.

14.4. RESULTS

323

1100 1000

TH 3

FA

900

800

700 s 600
500 400 300 200 100

3

3333333333333 33

33333

0 5 10 15 20 25 30 35 40 45

Number of states

Figure 14.10: Performance variance for a single transition in microseconds ( s) in an FA,
versus the number of states. Also included are +1 and ?1 standard deviation bars.

58 56

EFFA 3

EFFA

54 3

52 s 50
48 46

33333333

443 3

42

2 4 6 8 10 12

Number of states

Figure 14.11: Performance variance for a single transition in microseconds ( s) in a "-free
FA, versus the number of states. Also included are +1 and ?1 standard deviation bars.

324 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

1.3 DFA 3
1.2

DFA
3

1.1 3

1 s

33

0.9 0.8 0.7

333333333333333

3

3 3

3

0.6 0 5 10 15 20 25 Number of states

Figure 14.12: Performance variance for a single transition in microseconds ( s) in a DFA,
versus the number of states. Also included are +1 and ?1 standard deviation bars.

largely due to the fact that single transitions are around the resolution of the timer, meaning that other factors play a role. Such factors include arti cial ones such as clock jitter and real ones such as instruction cache misses.
14.5 Conclusions and recommendations
The conclusions of this chapter are: The fact that some constructions derived in Chapter 6 are encodings of one another (and some others are duals of one another) can be seen in the data.
{ The BS and BS-D algorithms (they are duals of one another) have the same
performance characteristics.
{ The ASU and FIC algorithms construct automata of the same size. This can
been predicted by comparing the algorithm for Construction (rem-", subset, use-s, Wfilt) on page 185 with Algorithm 6.86.
{ Interestingly, IC and MYG produce automata of the same size. Indeed, the re-
sulting DFAs will always be isomorphic since their underlying FA constructions (Constructions 6.19 and 6.39, respectively) are simply encodings of one another.
The advantages of the ` lters' introduced in Section 6.4.1 can be seen in the performance of IC, which is worse than either DER or FIC. Furthermore, IC produces

14.5. CONCLUSIONS AND RECOMMENDATIONS

325

larger automata than either DER or FIC. As was shown in Chapter 6, the BS and BS-D constructions produce automata with sizes linear in the number of symbol nodes in the input regular expression. Predictably, the subset construction (with start-unreachable state removal) is a costly operation. All of the DFA construction algorithms were signi cantly slower than the general FA constructions. ASU is the subset construction composed with BS-D, while MYG is the subset construction composed with BS. Given the identical performance of BS and BS-D, it is interesting to note that ASU was signi cantly faster than MYG. Furthermore, ASU produced smaller DFAs than MYG. The time to make a transition can vary widely for di erent types of automata. As such, it can be an important factor in choosing a type of automaton.
{ Both FAs with "-transitions and "-free FAs have transition times that depend
upon the number of states in the automaton, however, FAs with "-transitions have signi cantly longer transition times.
{ DFAs have transition times which are largely independent of the size of the
automaton3. These times were around the resolution of the clock4. The following procedure can be used to choose a nite automata construction algorithm:
Choices := fTH, BS, BS-D, ASUg; if construction time is important then
Choices := Choices \ fTH, BS, BS-Dg
;
if automaton size is important then Choices := Choices \ fBS, BS-D, ASUg
;
if transition time is important then Choices := Choices \ fBS, BS-D, ASUg

3Although, inspecting the implementation reveals that very dense transition graphs will yield more costly transitions than a sparse transition graph.
4This does not invalidate the results, since the average transition time is taken over a large number of transitions.

326 CHAPTER 14. THE PERFORMANCE OF FA CONSTRUCTION ALGORITHMS

Chapter 15

The performance algorithms

of

DFA

minimization

In this chapter, we present data on the performance of ve DFA minimization algorithms implemented in FIRE Lite (see Chapter 11). The algorithms tested were derived in the taxonomy of DFA minimization algorithms in Chapter 7, and are implemented in FIRE Lite.
15.1 Introduction
Very little is known about the performance of DFA minimization algorithms in practice. Most software engineers choose an algorithm by reading their favourite text-book, or by at-
tempting to understand Hopcroft's algorithm | the best known algorithm, with O(n log n)
running time. The data in this chapter will show that somewhat more is involved in choosing the right minimization algorithm. In particular, the algorithms appearing in a popular formal languages text-book HU79] and in a compiler text-book ASU86] have relatively poor performance (for the chosen input data). Two of the algorithms which could be expected to have poor performance1 actually gave impressive results in practice. Recommendations for software engineers will be given in the conclusions of this chapter.
In short, this chapter is structured as follows: Section 15.2 gives an outline of the ve algorithms tested. Section 15.3 explains the methodology used in gathering the test data. The performance data for the ve algorithms are presented in Section 15.4. The conclusions and recommendations of this chapter are given in Section 15.5.
1They are Brzozowski's algorithm (which uses the costly subset construction) and a new algorithm which has exponential worst-case running time.
327

328 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS
15.2 The algorithms
The ve algorithms are derived in Chapter 7 and their corresponding implementations are described in detail in Chapter 11. Here, we give a brief summary of each of the algorithms (and an acronym which will be used in this chapter to refer to the algorithm):
The Brzozowski algorithm (BRZ), derived in Section 7.2. It is unique in being able to process a FA (not necessarily a DFA), yielding a minimal DFA which accepts the same language as the original FA. The Aho-Sethi-Ullman algorithm (ASU), appearing as Algorithm 7.21. It computes an equivalence relation on states which indicate which states are indistinguishable (see Chapter 7). The appearance of this algorithm in ASU86] has made it one of the most popular algorithms among implementors. The Hopcroft-Ullman algorithm (HU), appearing as Algorithm 7.24. It computes the complement of the relation computed by the Aho-Sethi-Ullman algorithm. Since this algorithm traverses transitions in the DFA in their reverse direction, it is at a speed disadvantage in most DFA implementations (including the one used in FIRE Lite and the FIRE Engine). The Hopcroft algorithm (HOP), presented as Algorithm 7.26. This is the best known
algorithm (in terms of running time analysis) with running time of O(n log n) (where
n is the number of states in the DFA). The new algorithm (BW) appearing as Algorithm 7.28 in the taxonomy. It computes the equivalence relation (on states) from below (with respect to the re nement ordering). The practical importance of this is explained in Section 7.4.7. These algorithms are the only ones implemented in FIRE Lite.
15.3 Testing methodology
This section gives an overview of the methods used in gathering the test data. The test environment is identical to that used in collecting the FA construction performance data (Chapter 14). However, we consider the methods used to generate the DFAs for input to the algorithms.
One caveat about the distribution of the test data used in collecting the benchmarks is in order. In practice, DFAs are usually obtained from one of two sources: they are constructed from regular expressions2, or they are generated from some other speci cation3. In the rst case, the DFAs have certain characteristics: they are usually not very large, they have relatively sparse transition graphs, and the alphabet frequently consists of the entire
2This is common in pattern matching and compiler lexical analysis applications of DFAs. 3This is common in modeling digital circuits.

15.3. TESTING METHODOLOGY

329

ASCII character set. In the second case, the DFAs can potentially consist of thousands of states, with dense transition graphs and relatively small (even binary or ternary) alphabets. The FIRE Lite is best structured for obtaining DFAs from regular expressions | as would be done in tools such as lex or grep | and so we only consider the minimization of the rst group of DFAs. Due to the memory limitations of the test environment and some space ine ciencies in FIRE Lite, we only consider the minimization of relatively small DFAs (fewer than 25 states). Although it is possible to extrapolate the performance data, the performance of the algorithms on much larger DFAs is di cult to forecast.
Random regular expressions were generated (using FIRE Lite and the techniques outlined in Chapter 14). The DFAs were constructed from the regular expressions, using the `item set' construction | Construction (rem-", subset, use-s) appearing on page 156. Some data on the constructed DFAs is as follows:

Number of states Number of DFAs 2 1585 3 395 4 541 5 490 6 454 7 303 8 266 9 232 10 150 11 104 12 85 13 61 14 37 15 25 16 27 17 20 18 15 19 10 20 9 21 8 22 9 23 7

There are a total of 4833 DFAs; the mean size is 5.24 states and the standard deviation is 3.65. Clearly, the DFA sizes are not evenly distributed. The size distribution of the DFAs results from the distribution of the randomly generated REs.

330 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS

300000 250000

ASU HU

200000

s 150000

100000

50000

0 0 5 10 15 20 25 Number of states
Figure 15.1: Median performance (in microseconds to minimize) versus DFA size.

15.4 Results
For each of the random DFAs and each of the ve algorithms, we measured the time in microseconds ( s) required to construct the equivalent (accepting the same language) minimal DFA.
The performance of the algorithms was graphed against the number of states in the original DFA. Graphing the performance against the number of edges in the original DFA was not found to be useful in evaluating the performance of the algorithms. The algorithms can be placed in two groups, based upon their performance. In order to aid in the comparison of the algorithms, we present graphs for these two groups separately.
The rst group (ASU and HU) are the slowest algorithms; the graph appears in Figure 15.1. The HU algorithm was the worst performer of the ve algorithms. It traverses the transitions (in the input DFA) in the reverse direction. A typical implementation of a DFA does not favour this direction of traversal. The ASU algorithm performed slightly better, although its performance is also far slower than any of BRZ, HOP, or BW. The second group (BRZ, HOP, and BW) are signi cantly faster; the corresponding graph appears in Figure 15.2. Note that this graph uses a di erent scale from the one in Figure 15.1. The data point (for 17 states) for the BW algorithm was dropped, since (at that point) the algorithm was more than 30 times slower than any of the other algorithms in this group. The complete (unedited, including the 17 state data point) data for the BW algorithm is presented in Figure 15.8.
We now turn to the performance of the individual algorithms. (The graphs to be presented use di erent scales for the y axis. For direct comparison between the graphs,

15.4. RESULTS

331

50000

45000

BRZ HOP

40000 BW

35000

30000 s 25000

20000

15000

10000

5000

0 05

10 15 Number of states

20

25

Figure 15.2: Median performance (in microseconds to minimize) versus DFA size.

use the superimposed graphs, Figures 15.1 and 15.2.) The performance variance of ASU and HU are shown in Figures 15.3 and 15.3 respectively. Both of these algorithms display quite small variance in performance for a given size of input.
The performance variance of BRZ, HOP, and BW are shown in Figures 15.5, 15.6, and 15.7. BRZ and HOP both display quite small variance in performance, while the performance variance for BW is very large in some cases. Figure 15.7 shows the performance variance for the BW algorithm with the 17 state DFA data point removed. Figure 15.8 shows all of the BW performance variance data (including the 17 state data point). Al-
though the BW algorithm usually displays O(n log n) performance (in the tested range of
DFA sizes), the latter graph shows that, for certain input DFAs, the BW algorithm can occasionally give exponential performance. In the following paragraph, we brie y describe one type of DFA which can cause this behaviour.
Figure 15.9 gives part of a DFA which can cause the BW algorithm's exponential running time. (See Chapter 7 for a more in-depth discussion of the algorithm.) Depending upon the numbering (integer encoding in the implementation) of the states, the algorithm may begin by testing states p and q for equivalence. The algorithm considers each alphabet ctrsueyhcmaoulilcbryeso,ilvot(hefalesy;t,aba;lttgh:eo:issr:)iitesihnndmtutauiemlrstnuod.settBthdeeeergmtifenaircnnmtiinnitnggheawiftiiftttphhh01eeaa,pinniadttierdqsge10et(raeprr00e0em;nqeci00qno0)udesiaivnnwagdlheoen(ftptht01,0eh;erqet10cp0s).00taTaatrnheedisseqiqus00unuaifvrsoeeardlteeuqntnuota.itvcEehaloveoernnsstet-; the rst pair of states for consideration. Had the integer representations of the states been permuted, a di erent pair of states would have been chosen as the starting point and the

332 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS

400000 350000 300000 250000 s 200000 150000 100000 50000
0 -50000
0

ASU 3

ASU

3 333333333333333333333
5 10 15 20 Number of states

25

Figure 15.3: Performance variance (in microseconds to minimize) versus DFA size for the
ASU minimization algorithm, including +1 and ?1 standard deviation bars.

350000 300000 250000 s 200000 150000 100000 50000
0 0

HU 3

HU

3

3 3 3 333333333333333333
5 10 15 20 Number of states

25

Figure 15.4: Performance variance (in microseconds to minimize) versus DFA size for the
HU minimization algorithm, including +1 and ?1 standard deviation bars.

15.4. RESULTS

333

35000
30000 BRZ 3

BRZ

25000 20000 s 15000 10000 5000
0 0

33 33
33333 3333333333333
5 10 15 20 Number of states

25

Figure 15.5: Performance variance (in microseconds to minimize) versus DFA size for the
BRZ minimization algorithm, including +1 and ?1 standard deviation bars.

80000 HOP 3
70000

HOP

60000 50000 s 40000 30000 20000 10000
0 0

3 33 3333333333333333333
5 10 15 20 Number of states

25

Figure 15.6: Performance variation (in microseconds to minimize) versus DFA size for the
HOP minimization algorithm, including +1 and ?1 standard deviation bars.

334 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS

140000

120000

100000

80000

60000

s

40000 20000

0

-20000

-40000

-60000

-80000 0

BW 3

BW

3 33333333333333
5 10 15 Number of states

3 33333
20

25

Figure 15.7: Performance variation (in microseconds to minimize) versus DFA size for the
BW minimization algorithm, including +1 and ?1 standard deviation bars.

6e+06 5e+06 4e+06 3e+06 2e+06 s 1e+06
0 -1e+06 -2e+06 -3e+06 -4e+06
0

BW-S 3

BW-S

3 333333333333333 333333

5 10 15 20 Number of states

25

Figure 15.8: Median performance (in microseconds to minimize) versus DFA size for the BW minimization algorithm.

15.5. CONCLUSIONS AND RECOMMENDATIONS

335

a p
b

a p00
b
a p10
b

p000 p100 p020 p300

a q
b

a q00
b
a q10
b

q000 q100 q200 q300

Figure 15.9: Part of a DFA which causes exponential running time in the BW algorithm.

exponential behaviour would not have occured.
15.5 Conclusions and recommendations
We can draw the following conclusions from the data presented in this chapter4: Given their relative performance, the ve algorithms can be put into two groups, the rst consisting of ASU and HU, and the second consisting of BRZ, HOP, and BW. The HU algorithm has the lowest performance of all of the algorithms. This is largely due to the fact that it traverses the transitions of the DFA in the reverse direction | a direction not favoured by most practical implementations of DFAs. The ASU algorithm also displays rather poor performance. Traditionally, the algorithm has been of interest because it is easy to understand, as we saw in Chapter 7. The simplicity of BRZ minimization algorithm makes it even more suitable for teaching purposes. The HOP algorithm is the best known algorithm (in term of theoretical running
time), with O(n log n) running time. Despite this, it is the worst of the second group
of algorithms. With its excellent theoretical running time, it will outperform the BRZ and BW algorithms on extremely large DFAs. With memory constraints, we were unable to identify where the crossing-point of their performance is.
4Keeping in mind the caveats mentioned in Section 15.3

336 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS
The BRZ algorithm is extremely fast in practice, consistently outperforming Hopcroft's algorithm (HOP). This result is surprising, given the simplicity of the algorithm. The implementation of this algorithm constructs an intermediate DFA; the performance can be further improved by eliminating this intermediate step. The new algorithm (BW) displayed excellent performance. The algorithm is frequently faster than even Brzozowski's algorithm. Unfortunately, this algorithm can be erratic at times | not surprising given its exponential running time. The algorithm can be further improved using memoization | see Section 7.4.7. Given these conclusions, we can make the following recommendations: 1. Use the new algorithm (BW, appearing as Algorithm 7.28 in this dissertation), especially in real-time applications (see Section 7.4.7 for an explanation of why this algorithm is useful for real-time applications). If the performance is still insu cient, modify the algorithm to make greater use of memoization. 2. Use Brzozowski's algorithm (derived in Section 7.2), especially when simplicity of implementation or consistent performance is desired. The algorithm is able to deal with an FA as input (instead of only DFAs), producing the minimal equivalent DFA. When a minimization algorithm is being combined with a FA construction algorithm, Brzozowski's minimization algorithm is usually the best choice. The DFA construction algorithms are usually signi cantly slower than the FA construction algorithms, as is shown in Chapter 14. For this reason, a FA construction algorithm combined with Brzozowski's minimization algorithm will produce the minimal DFA faster than a DFA construction algorithm combined with any of the other minimization algorithms. Brzozowski's algorithm can be further improved by eliminating the DFA which is constructed in an intermediate step. 3. Use Hopcroft's algorithm (Algorithm 7.26) for massive DFAs. It is not clear from the data in this chapter precisely when this algorithm becomes more attractive than the new one or Brzozowski's. 4. The two most-commonly taught text-book algorithms (the Aho-Sethi-Ullman algorithm, Algorithm 7.21, and the Hopcroft-Ullman algorithm, Algorithm 7.24) do not appear to be good choices for high performance. Even for simplicity of implementation, Brzozowski's algorithm is better. The following procedure can be used to choose a deterministic nite automata minimization algorithm:
Choices := fBRZ, BW, HOPg; if easy of understanding is important then
Choices := Choices \ fBRZ, BWg

15.5. CONCLUSIONS AND RECOMMENDATIONS ;
if the application has real-time deadlines then Choices := Choices \ fBWg
;
if asymptotic performance is important then Choices := Choices \ fHOPg
;
if good average-case performance is important then Choices := Choices \ fBRZg
;

337

338 CHAPTER 15. THE PERFORMANCE OF DFA MINIMIZATION ALGORITHMS

Part V Epilogue
339

Chapter 16
Conclusions
The conclusions of this dissertation are summarized in this chapter. We rst present the more general conclusions. Some chapters are accompanied by speci c conclusions. A selection of those conclusions will be presented as well.
16.1 General conclusions
The general conclusions are: Each of the taxonomies presented in this dissertation covers an area of computer science which has been active for at least thirty- ve years. The original presentations of the algorithms di er vastly in their purposes (the intended use of the algorithms) and presentation styles. A great deal of reverse engineering was required to identify the inner workings of the algorithms. Additionally, one of the most powerful techniques in computing science, abstraction, was most helpful in nding the common threads in the families of algorithms. The main goal of this research was to improve the accessibility of the algorithms. This has been achieved in a number of ways, thanks to the fact that some of the research has already been reported in preliminary papers:
{ The taxonomies have been used at a number of institutes as reference material
for teaching.
{ The toolkits have been used in industry for the creation of reliable software
components and at universities for teaching the essential concepts of reusable software construction.
{ The performance data has been used extensively by software engineers to select
high performance algorithms for applications. The research reported in this dissertation ranges from the more theoretical taxonomies and algorithm derivations to the very practical toolkits and algorithm performance benchmarks. Although these areas seem to be at opposite ends of the spectrum, it proved useful as well as easy to consider them together.
341

342 CHAPTER 16. CONCLUSIONS
The taxonomies are more than just surveys. They explain and classify the algorithms in such depth that the corresponding literature (the original articles describing the algorithms and subsequent surveys) is no longer required reading. Similarly, some existing implementations of well-known algorithms are made obsolete by the toolkits, since the toolkits contain implementations of many di erent algorithms solving the same problem. A single taxonomy construction methodology and framework was successfully used to develop taxonomies for three totally di erent problems in computing science. However, successfully using the same methodology does not necessarily mean that all of the algorithms must be derived using the same formalisms. The formalisms used for each of the three problems can be described as follows:
{ Pattern matching. The algorithms were derived as imperative programs, with
a largely formal approach to the derivation.
{ Finite automata construction. The constructions were given as mathematical
functions, each being derived as the composition of a number of other functions. Algorithms implementing the functions were also considered, although outside the framework of the taxonomy.
{ Deterministic nite automata minimization. The minimization algorithms were
derived as a mixture of the two formalisms mentioned above. In constructing the taxonomies, we did not adhere strictly to Jonkers' methodology. In his dissertation, at least one of the algorithms was derived in a formal manner. In the taxonomies in this dissertation, a relatively liberal approach to the interpretation of the details is adopted. It does not appear that taxonomies could be constructed for all types of algorithm families. The algorithms treated in this dissertation have something signi cant in common: they all have simple speci cations of both the input and the output. In particular, it is not clear how easy it would be to taxonomize algorithms such as those based upon heuristics. Some of the algorithms treated in the taxonomies are usually considered to be inaccessible in the standard literature. For example, the e cient Commentz-Walter pattern matching algorithm and Hopcroft's deterministic nite automata minimization algorithm are both relatively di cult to understand. The re nement-based derivation of these algorithms, and their corresponding implementations in the toolkits, should make them more accessible than before. Truly capitalizing on the organization provided by the taxonomies required the construction of the toolkits. Even with the taxonomies, constructing the toolkits still required a remarkable number of software engineering decisions. Conclusions can be drawn for both algorithm theoreticians and software engineers:

16.1. GENERAL CONCLUSIONS

343

{ Part III points out that the completion of a taxonomy still leaves a number of
software engineering decisions. Some examples of these decisions are those that lead to the call-back public interface in the class libraries | and the consequent possibility of multi-threading.
{ The nature of the taxonomies, in which common parts of algorithms are factored,
can be used to great e ect in reusing parts of class libraries. As outlined in Chapter 8, a number of design issues surround reuse, for example abstract classes and virtual member functions versus templates.

The toolkits began as a sidetrack from the primary task of developing the taxonomies. With the taxonomies in-hand, the toolkits were implemented extremely rapidly | eliminating much of the development time usually required for such toolkits. Two clearly identi able aspects of the toolkits were in uenced by the taxonomies:
{ Since all of the abstract algorithms were presented (in the taxonomies) in the
same formalism, it was possible to create a coherent toolkit with the C++ classes making use of a common set of foundation classes.
{ The inheritance hierarchy is usually one of the most di cult parts of a class
library to design. Given a taxonomy family tree, the corresponding class inheritance hierarchy was easy to structure.

Toolkits which provide more than one algorithm solving the same problem are di cult to use e ectively without information on the performance of the algorithms in practice. The information in Part IV provides data (and analysis of the data) and recommendations on the use of algorithms in the toolkits. Part IV shows that the algorithms with the best theoretical complexity are not always the fastest in practice, even on non-trivial input.

Experience with the object-oriented toolkits (and with older C toolkits) show that the object-oriented approach to toolkit design does not a ect the algorithm performance signi cantly.

At rst glance, it may appear that the taxonomies only serve to classify existing algorithms. By combining the taxonomy details in new ways, or by making use of techniques developed during the taxonomization, it is possible to construct new algorithms. In particular, the following interesting algorithms were derived:
{ The use of `predicate weakening' was used in Chapter 4 to derive interesting
new variants of the Commentz-Walter multiple-keyword pattern matching algorithms.
{ A new regular expression pattern matching algorithm was developed in Chap-
ter 5 | answering an open question posed by A.V. Aho.

344 CHAPTER 16. CONCLUSIONS
{ A number of new nite automata construction algorithms were derived in Chap-
ter 6. Unfortunately, the performance data presented in Chapter 14 shows that several of these algorithms are not particularly useful in practice. Some do, however, serve the purpose of being easy to understand.
{ A new deterministic nite automata minimization algorithm, the only existing
algorithm suitable for use in real-time applications, was derived in Chapter 7.
16.2 Chapter-speci c conclusions
Some of the chapter-speci c conclusions are: Chapter 4. The Aho-Corasick and the multiple keyword Knuth-Morris-Pratt pattern matching algorithms all share a common skeleton. They di er only in the implementation of a particular Moore machine transition function. Similarly, the CommentzWalter algorithms also share a common skeleton, di ering in the shift function used to skip portions of the input string. Chapter 4. The technique of `predicate weakening' was extremely useful in developing new shift functions for both the Commentz-Walter and the Boyer-Moore pattern matching algorithms. Chapter 4. Another taxonomy of pattern matching algorithms, due to Hume and Sunday, was easily incorporated into the taxonomy in this dissertation. Chapter 5. There is a Boyer-Moore type algorithm for regular expression pattern matching (see Chapter 5 for the algorithm and its precomputation), answering an open question posed by A.V. Aho in Aho80, p. 342]. The derivation relied heavily on techniques developed for the taxonomy of keyword pattern matching algorithms, and it is doubtful that the algorithm could have been easily invented without the use of these techniques. Preliminary testing of the algorithm shows that it is frequently faster than a generalization of the Aho-Corasick algorithm. Chapter 6. The earlier taxonomy presented in Wat93a] contained two taxonomy trees. The derivations presented there seemed to indicate that the two subfamilies of algorithms were related, but could not be derived from one another. The taxonomy presented in this dissertation shows that they can all be derived from a single `canonical' algorithm. Chapter 6. The canonical algorithm encoded the `maximal' amount of information in states. All of the other algorithms were derived by either changing the representation of the states or by omitting some of the information, thereby merging states. Furthermore, all of the algorithms were derived as compositions of mathematical functions.

16.3. A PERSONAL PERSPECTIVE

345

Chapter 6. Even the most recent algorithms, such as those presented by Antimirov in Anti94, Anti95] and by Antimirov and Watson in AW95], were incorporated into the taxonomy. Chapter 7. Brzozowski's minimization algorithm proved to be extremely easy to derive and to understand. Unfortunately, it does not appear possible to derive it from the other minimization algorithms. In the past, the origins of this algorithm have also been accidentally misattributed. Chapter 7. A new minimization algorithm, suitable for real-time applications, was derived. The algorithm computes a relation (for use in minimization) as a xed point, from the safe side. This implies that its intermediate computations are usable in reducing the size of (but perhaps not minimizing) the DFA. Chapter 9. Designing and structuring generic software is much more di cult than designing software for a single application. The general structure of the pattern matching taxonomy proved to be helpful in guiding the structure of the SPARE Parts. Chapter 10. Finite automata toolkits, such as the FIRE Engine and FIRE Lite, have proven to be general enough to nd use in the following areas: compiler construction, hardware modeling, and computational biology. Chapter 15. Despite its exponential worst-case running time, Brzozowski's minimization algorithm has excellent practical performance for realistic input. Additionally, the new minimization algorithm also displays excellent practical running time, even though it also has exponential worst-case running time.

16.3 A personal perspective
The research reported in this dissertation, and my experiences while performing and writing it, show that a delicate balance between the practical and the theoretical is crucial to a computer scientist.

346 CHAPTER 16. CONCLUSIONS

Chapter 17
Challenges and open problems
While the research reported in this dissertation has answered a number of questions, and brought some order to the eld of regular language algorithms, it has also suggested some new directions for research. The following problems (which are of various levels of di cult) are suitable for Master's or Ph.D students:
1. Keyword pattern matching: (a) Expand the taxonomy to include Commentz-Walter and Boyer-Moore algorithms which retain information about previous matches, for reuse in subsequent match attempts (see Remark 4.156). (b) Consider the use of `match orders' (Section 4.5) for the Commentz-Walter multiple-keyword algorithms. (c) Construct taxonomies of: approximate pattern matching algorithms, multidimensional pattern matching algorithms, and tree and graph pattern matching algorithms.
2. The new regular expression pattern matching algorithm: (a) Explore the use of other weakenings, and use `strategies' (Section 4.4) to catalogue them. (b) Explore the use of a left lookahead symbol. (c) Quantify the e ects (on the performance of the algorithm) of the type of nite automaton used (deterministic or nondeterministic automata; with or without "-transitions). (d) Consider a version of the algorithm which reuses previous match information.
3. Extend the research to di erent pattern matching problems, such as multi-dimensional pattern matching and tree pattern matching.
4. The taxonomy of nite automata constructions:
347

348 CHAPTER 17. CHALLENGES AND OPEN PROBLEMS
(a) Modify the algorithms to include the construction of Moore machines and Mealy machines (regular transducers).
(b) Include the construction of tree automata and graph automata. 5. SPARE Parts:
(a) Add tree pattern matching to the toolkit. (b) Expand the toolkit to deal with strings of (C++) objects, instead of only char-
acters. 6. FIRE Lite:
(a) Add regular transductions to the toolkit. (b) Expand the toolkit to deal with strings of (C++) objects, instead of only char-
acters. (c) Implement the minimization algorithms that are not presently included in the
toolkit. 7. The performance of DFA minimization algorithms:
(a) Benchmark the minimization algorithms for the types of DFA which typically arise in digital circuit applications.

References

AC75] Aho80] Aho90] AHU74] AMS92] ANSI95] Anti94] Anti95]
Aoe94] ASU86]

Aho, A.V. and M.J. Corasick. E cient string matching: an aid to bibli-
ographic search, Comm. ACM, 18(6) (1975) 333{340.
Aho, A.V. Pattern matching in strings, in: R.V. Book, ed., Formal Language Theory: Perspectives and Open Problems. (Academic Press, New York, 1980) 325{347. Aho, A.V. Algorithms for nding patterns in strings, in: J. van Leeuwen, ed., Handbook of Theoretical Computer Science, Vol. A. (North-Holland, Amsterdam, 1990) 257{300. Aho, A.V., J.E. Hopcroft, and J.D. Ullman. The Design and Analysis of Computer Algorithms. (Addison-Wesley, Reading, MA, 1974). Aoe, J.-I., K. Morimoto, and T. Sato. An e cient implementation of
trie structures, Software | Practice and Experience, 22(9), (September 1992)
695{712. American National Standards Institute. Programming language C++. Working Paper for Draft Proposed International Standard for Information Systems, (ANSI Committee X3J16/ISO WG21, 31 January 1995). Antimirov, V.M. Partial derivatives of regular expressions and nite automata constructions, Technical Report CRIN 94-R-245, CRIN, France, December 1994. Antimirov, V.M. Partial derivatives of regular expressions and nite automata constructions, in: E.W. Mayr and C. Puech, eds., Twelfth Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science 900 (Springer-Verlag, Berlin, 1995) 455{466. Aoe, J.-I. Computer Algorithms: String Pattern Matching Strategies. (IEEE Computer Society Press, 1994). Aho, A.V., R. Sethi, and J.D. Ullman. Compilers: Principles, Techniques, and Tools. (Addison-Wesley, Reading, MA, 1988).
349

350 REFERENCES

AU92] AW95] B-K93a] B-K93b] BL77]
BM77] Booc94] Brau88] Broy83]
Brzo62]
Brzo64] BS86] BS95] B-YR90]

Aho, A.V. and J.D. Ullman. Foundations of Computer Science. (Computer Science Press, New York, 1992). Antimirov, V.M. and B.W. Watson. From regular expressions to small NFAs through partial derivatives, Unpublished Manuscript, April 1995. Bruggemann-Klein, A. Regular expressions into nite automata, Theo-
retical Computer Science 120 (1993) 197{213.
Bruggemann-Klein, A. Private communication (July 1993). Backhouse, R.C. and R.K. Lutz. Factor graphs, failure functions and bi-trees, in: G. Goos and J. Hartmanis, eds., Fourth Colloquium on Automata, Languages and Programming, Lecture Notes in Computer Science 52 (Springer-Verlag, Berlin, 1977) 61{75. Boyer, R.S. and J.S. Moore. A fast string searching algorithm, Comm.
ACM, 20(10) (1977) 62{72.
Booch, G. Object oriented analysis and design, with applications. (Benjamin/Cummings, Redwood City, CA, 2nd edition, 1994).
Brauer, W. On minimizing nite automata, EATCS Bulletin 35 (June
1988). Broy, M. Program construction by transformations: a family tree of sorting programs, in: A.W. Biermann and G. Guiho, eds., Computer Program Synthesis Methodologies (1983) 1{49. Brzozowski, J.A. Canonical regular expressions and minimal state graphs for de nite events, in: Mathematical theory of Automata, Vol. 12 of MRI Symposia Series. (Polytechnic Press, Polytechnic Institute of Brooklyn, NY, 1962) 529{561.
Brzozowski, J.A. Derivatives of regular expressions, J. ACM 11(4) (1964)
481{494. Berry, G. and R. Sethi. From regular expressions to deterministic au-
tomata, Theoretical Computer Science 48 (1986) 117{126.
Brzozowski, J.A. and C.-J. Seger. Asynchronous circuits. (Springer, Berlin, 1995). Baeza-Yates, R. and M. Regnier. Fast algorithms for two dimensional and multiple pattern matching, in: J.R. Gilbert and R. Karlsson, eds., Second Scandinavian Workshop on Algorithm Theory, Lecture Notes in Computer Science 447 (Springer-Verlag, Berlin, 1990) 332{347.

REFERENCES

351

Budd91] Budd94] CE95] CH91] Cham93]
Chan92]
Com79a]
Com79b]
Copl92] CP92]
CR94] Darl78] DeRe74]
Dijk76]

Budd, T.A. An introduction to object-oriented programming. (AddisonWesley, Reading, MA, 1991). Budd, T.A. Classic data structures in C++. (Addison-Wesley, Reading, MA, 1994). Carroll, M.D. and M.A. Ellis. Designing and coding reusable C++. (Addison-Wesley, Reading, MA, 1995). Champarnaud, J.M. and G. Hansel. Automate: A computing package for
automata and nite semigroups, J. Symbolic Computation 12 (1991) 197{220.
Champarnaud, J.M. From a regular expression to an automaton, Technical Report, IBP, LITP, Universite Paris 7, Paris, France, Working document 23 September 1993. Chang, C.-H. From regular expressions to DFAs using compressed NFAs, Ph.D dissertation, Computer Science Department, Courant Institute of Mathematical Sciences, New York University, NY, October 1992. Commentz-Walter, B. A string matching algorithm fast on the average, in: H.A. Maurer, ed., Proc. 6th Internat. Coll. on Automata, Languages and Programming (Springer-Verlag, Berlin, 1979) 118{132. Commentz-Walter, B. A string matching algorithm fast on the average, Technical Report TR 79.09.007, IBM Germany, Heidelberg Scienti c Center, 1979. Coplien, J.O. Advanced C++: programming styles and idioms. (AddisonWesley, Reading, MA, 1992). Chang, C.-H. and R. Paige. From regular expressions to DFAs using compressed NFAs, Technical Report, Computer Science Department, Courant Institute of Mathematical Sciences, New York University, NY, 1992. Crochemore, M. and W. Rytter. Text Algorithms. (Oxford University Press, Oxford, England, 1994). Darlington, J. A synthesis of several sorting algorithms, Acta Informatica
11 (1978) 1{30.
DeRemer, F.L. Lexical analysis, in: F.L. Bauer and J. Eickel, eds., Compiler Construction: an Advanced Course, Lecture Notes in Computer Science 21 (Springer-Verlag, Berlin, 1974) 109{120. Dijkstra, E.W. A discipline of programming. (Prentice Hall, Englewood Cli s, NJ, 1976).

352 REFERENCES

Earl70] vdEi92]
tEvG93]
t-Ei91]
EM85] Fred60] FS93]
GB-Y91]
GHJV95]
GJ90] Glus61] Gold93] Grie73] HKR94]

Earley, J. An e cient context-free parsing algorithm, Comm. ACM 13(2)
(February 1970) 94{102. van den Eijnde, J.P.H.W. Program derivation in acyclic graphs and related problems, Computing Science Report 92/04, Eindhoven University of Technology, The Netherlands, 1992. ten Eikelder, H.M.M. and H.P.J. van Geldrop. On the correctness of some algorithms to generate nite automata for regular expressions, Computing Science Report 93/32, Eindhoven University of Technology, The Netherlands, 1993. ten Eikelder, H.M.M. Some algorithms to decide the equivalence of recursive types, Computing Science Report 91/31, Eindhoven University of Technology, The Netherlands, 1991. Ehrig, E. and B. Mahr. Fundamentals of Algebraic Speci cation 1: Equations and Initial Semantics. (Springer-Verlag, Berlin, 1985).
Fredkin, E. Trie memory, Comm. ACM 3(9) (1960) 490{499.
Fan, J.-J. and K.-Y. Su. An e cient algorithm for matching multiple pat-
terns, IEEE Trans. on Knowledge and Data Engineering 5(2) (April 1993)
339{351. Reprinted in Aoe94]. Gonnet, G.H. and R. Baeza-Yates. Handbook of Algorithms and Data Structures (In Pascal and C). (Addison-Wesley, Reading, MA, 2nd edition, 1991). Gamma, E., R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of Reusable Object-Oriented Software. (Addison-Wesley, Reading, MA, 1995). Grune, D. and C.J.H. Jacobs. Parsing Techniques: A Practical Guide. (Ellis Horwood, West Sussex, England, 1990). Glushkov, V.M. The abstract theory of automata, Russian Mathematical
Surveys 16 (1961) 1{53.
Goldberg, R.R. Finite state automata from regular expression trees, The
Computer Journal 36(7) (1993) 623{630. Gries, D. Describing an algorithm by Hopcroft, Acta Informatica 2 (1973)
97{109. Heering, J., P. Klint, and J. Rekers. Lazy and incremental program
generation, ACM Trans. on Programming Languages and Systems 16(3)
(1994) 1010{1023.

REFERENCES

353

HN92]

Henricson, M. and E. Nyquist. Programming in C++: Rules and recommendations, Technical Report M90 0118 Uen, Ellemtel Telecommunication Systems Laboratories, Alvsjo, Sweden, 1992.

Hopc71] Hopcroft, J.E. An n log n algorithm for minimizing the states in a nite automaton, in: Z. Kohavi, ed., The Theory of Machines and Computations. (Academic Press, New York, 1971) 189{196.

HS91] Hume, S.C. and D. Sunday. Fast string searching, Software|Practice and
Experience 21(11) (1991) 1221{1248.

HU79] Hopcroft, J.E. and J.D. Ullman. Introduction to Automata, Theory, Languages, and Computation. (Addison-Wesley, Reading, MA, 1979).

Hu 54] Huffman, D.A. The synthesis of sequential switching circuits, J. Franklin
Institute 257(3) (1954) 161{191 and 257(4) (1954) 275{303.

ISO90] ISO/IEC. Programming languages | C. International Standard 9899:1990, (ISO, 1st edition, 15 December 1990).

John86] Johnson, J.H. INR: A program for computing nite automata, Technical Report, Department of Computer Science, University of Waterloo, Waterloo, Canada, January 1986.

Jonk82]

Jonkers, H.B.M. Abstraction, speci cation and implementation techniques, Ph.D dissertation, Eindhoven University of Technology, The Netherlands, 1982. Also appears as MC-Tract 166, Mathematical Center, Amsterdam, The Netherlands, 1983.

JPTW90] Jansen, V., A. Pothoff, W. Thomas, and U. Wermuth. A short guide
to the Amore system, Aachener Informatik-Berichte 90(02), Lehrstuhl fur
Informatik II, (Universitat Aachen, January 1990).

KMP77] Knuth, D.E., J.H. Morris and V.R. Pratt. Fast pattern matching in
strings, SIAM J. Comput. 6(2) (1977) 323{350.

Knut65] Knuth, D.E. On the translation of languages from left to right, Inform.
Control 8 (1965) 607{639.

KP95] Keller, J.P. and R. Paige. Program derivation with veri ed transforma-
tions | a case study, Comm. on Pure and Applied Mathematics 48 (1995) In
press.

KR88] Kernighan, B.W. and D.M. Ritchie. The C Programming Language. (Prentice Hall, Englewood Cli s, NJ, 2nd edition, 1988).

354 REFERENCES

KW70] Leis77]
Lipp91] Marc90] MeyB88] MeyB92] MeyB94] MeyS92] Mirk65] Moor56]
Murr93] MY60] Myhi57] Nero58] Park89] Perr90]

Kameda, T. and P. Weiner. On the state minimization of nondeterministic
nite automata, IEEE Trans. on Computers C-19(7) (1970) 617{627.
Leiss, E. Regpack: An interactive package for regular languages and nite automata, Research Report CS-77-32, Department of Computer Science, University of Waterloo, Waterloo, Canada, October 1977. Lippman, S.B. C++ primer. (Addison-Wesley, Reading, MA, 2nd edition, 1991). Marcelis, A.J.J.M. On the classi cation of attribute evaluation algorithms,
Science of Computer Programming 14 (1990) 1{24.
Meyer, B. Object oriented software construction. (Prentice Hall, Englewood Cli s, NJ, 1988). Meyer, B. Ei el: The Language. (Prentice Hall, Englewood Cli s, NJ, 1992). Meyer, B. Reusable Software: The Base Object-Oriented Component Libraries. (Prentice Hall, Englewood Cli s, NJ, 1994). Meyers, S. E ective C++: 50 speci c ways to improve your programs. (Addison-Wesley, Reading, MA, 1992).
Mirkin, B.G. On dual automata, Kibernetika 2(1) (1966) 7{10.
Moore, E.F. Gedanken-experiments on sequential machines, in: C.E. Shannon and J. McCarthy, eds., Automata Studies. (Princeton University Press, Princeton, NJ, 1956) 129{153. Murray, R.B. C++ strategies and tactics. (Addison-Wesley, Reading, MA, 1993). McNaughton, R. and H. Yamada. Regular expressions and state graphs
for automata, IEEE Trans. on Electronic Computers 9(1) (1960) 39{47.
Myhill, J. Finite automata and the representation of events, Technical Report WADD TR-57-624, Wright Patterson AFB, Ohio, 1957, 112{137.
Nerode, A. Linear automaton transformations, Proc. AMS 9 (1958) 541{
544. Parker, S.P., ed., Dictionary of scienti c and technical terms. (McGrawHill, New York, 4th edition, 1989) Perrin, D. Finite Automata, in: J. van Leeuwen, ed., Handbook of Theoretical Computer Science, Vol. B. (North-Holland, Amsterdam, 1990) 1{57.

REFERENCES

355

Pirk92] Plau95] PTB85]
PTVF92]
RS59] RW93]
Ryt80] SE90] SL94] Smit82] vdSn85]
vdSn93] Souk94] SS-S88]

Pirklbauer, K. A study of pattern-matching algorithms, Structured Pro-
gramming 13 (1992) 89{98.
Plauger, P.J. The Draft Standard C++ Library. (Prentice Hall, New Jersey, 1995). Paige, R., R.E. Tarjan and R. Bonic. A linear time solution to the single
function coarsest partition problem, Theoretical Computer Science 40 (1985)
67{84. Press, W.H., S.A. Teukolsky, W.T. Vetterling and B.P. Flannery. Numerical Recipes in C: The Art of Scienti c Computing. (Cambridge University Press, Cambridge, England, 2nd edition, 1992). Rabin, M.O. and D. Scott. Finite automata and their decision problems,
IBM J. Research 3(2) (1959) 115{125.
Raymond, D.R. and D. Wood. The Grail papers: Version 2.0, Department of Computer Science, University of Waterloo, Canada, January 1994. Available by ftp from .CS-archive.uwaterloo.ca Rytter, W. A correct preprocessing algorithm for Boyer-Moore string-
searching, SIAM J. Comput. 9(2) (1980) 509{512.
Stroustrup, B. and M. Ellis. The annotated C++ reference manual. (Addison-Wesley, Reading, MA, 1990). Stepanov, A. and M. Lee. Standard Template Library, Computer Science Report, Hewlett-Packard Laboratories, 1994. Smit, G. de V. A comparison of three string matching algorithms, Software
| Practice and Experience 12 (1982) 57{66.
van de Snepscheut, J.L.A. Trace theory and VLSI design, Ph.D dissertation, Faculty of Mathematics and Computing Science, Eindhoven University of Technology, The Netherlands, 1985. Also available as Lecture Notes in Computer Science 200 (Springer-Verlag, Berlin, 1985). van de Snepscheut, J.L.A. What computing is all about. (Springer-Verlag, New York, 1993). Soukup, J. Taming C++: Pattern Classes and Persistence for Large Projects. (Addison-Wesley, Reading, MA, 1994). Sippu, S. and E. Soisalon-Soininen. Parsing Theory: Languages and Parsing, Vol. 1. (Springer-Verlag, Berlin, 1988).

356 REFERENCES

Step94] Stro91] Stro94] Tali94] Teal93] Thom68] Urba89] Wat93a]
Wat93b]
Wat94a]
Wat94b]
Wat94c]

Stephen, G.A. String searching algorithms. (World Scienti c, Singapore, 1994). Stroustrup, B. The C++ programming language. (Addison-Wesley, Reading, MA, 2nd edition, 1991). Stroustrup, B. The Design and Evolution of C++. (Addison-Wesley, Reading, MA, 1994). Taligent. Taligent's Guide to Designing Programs: Well-Mannered Object Oriented Design in C++. (Addison-Wesley, Reading, MA, 1994). Teale, S. C++ IOStreams Handbook. (Addison-Wesley, Reading, MA, 1993).
Thompson, K. Regular expression search algorithms, Comm. ACM 11(6)
(1968) 419{422.
Urbanek, F. On minimizing nite automata, EATCS Bulletin 39 (October
1989). Watson, B.W. A taxonomy of nite automata construction algorithms, Computing Science Report 93/43, Eindhoven University of Technology, The Netherlands, 1993. Available for ftp from ftp.win.tue.nl in directory ./pub/techreports/pi/automata/ Watson, B.W. A taxonomy of nite automata minimization algorithms, Computing Science Report 93/44, Eindhoven University of Technology, The Netherlands, 1993. Available for ftp from ftp.win.tue.nl in directory ./pub/techreports/pi/automata/ Watson, B.W. The performance of single-keyword and multiple-keyword pattern matching algorithms, Computing Science Report 94/19, Eindhoven University of Technology, The Netherlands, 1994. Available for ftp from ftp.win.tue.nl in directory ./pub/techreports/pi/pattm/ Watson, B.W. An introduction to the FIRE Engine: A C++ toolkit for FInite automata and Regular Expressions, Computing Science Report 94/21, Eindhoven University of Technology, The Netherlands, 1994. Available for ftp from ftp.win.tue.nl in directory ./pub/techreports/pi/automata/ Watson, B.W. The design and implementation of the FIRE Engine: A C++ toolkit for FInite automata and Regular Expressions, Computing Science Report 94/22, Eindhoven University of Technology, The Netherlands, 1994. Available for ftp from ftp.win.tue.nl in directory ./pub/techreports/pi/automata/

REFERENCES

357

Wein95] WM94] Wood87] WW94]
WZ92]
WZ95]

Weiner, R. Software Development using Ei el: There can be life other than C++. (Prentice Hall, Englewood Cli s, NJ, 1995). Wu, S. and U. Manber. A fast algorithm for multi-pattern searching, Technical Report TR94-17, University of Arizona, Tucson, Arizona, May 1994. Available by e-mail from .udi@cs.arizona.edu Wood, D. Theory of Computation. (Harper & Row, New York, 1987). Watson, B.W. and R.E. Watson. A Boyer-Moore type algorithm for regular expression pattern matching, Computing Science Report 94/31, Eindhoven University of Technology, The Netherlands, 1994. Available by e-mail from .watson@win.tue.nl Watson, B.W. and G. Zwaan. A taxonomy of keyword pattern matching algorithms, Computing Science Report 92/27, Eindhoven University of Technology, The Netherlands, 1992. Available by e-mail from watson@win.tue.nl or .wsinswan@win.tue.nl Watson, B.W. and G. Zwaan. A taxonomy of sublinear multiple keyword pattern matching algorithms, Computing Science Report 95/13, Eindhoven University of Technology, The Netherlands, 1994. Available by e-mail from .wsinswan@win.tue.nl

Index

, see transduction

, see item dot

, see composition

, see tree paths, concatenation operator

", see empty string

9, see quanti cation, existential

8, see quanti cation, universal

],f

, see Aho-Corasick, transition function see index of equivalence relation

\, see quanti cation, intersection

=, see isomorphism

, see left drop

, see left take

p, see pre x ordering

s, see su x ordering

, see tuple projection

, see tuple projection

v, see re nement

, see right drop

, see right take

, see similarity

j j, see set cardinality

ef, see trie, extended forward

f, see trie, forward

r, see trie, reverse

, see quanti cation, union

+, see algorithm details, orders, increas-

ing length

?, see algorithm details, orders, decreas-

ing length

a-s, see algorithm details, auxiliary sets absorb, see weakening strategy, absorb ac, see algorithm details, Aho-Corasick ac-fail, see algorithm details, Aho-Corasick,
failure function

ac-opt, see algorithm details, Aho-Corasick, optimized
,aceftrie 235 ,acgamma 234 ACMachine: : : , 233 ACMachineFail, 234, 235, 244 ACMachineKMPFail, 234, 244 ACMachineOpt, 233, 244 ,acmfail 234 ,acmkmpfl 234 acmopt, 233 acout, 235 ACOutput, 233{235 ,acs.hpp 228, 233 added, 206 AFT, see item dot position, after Aho, A.V., i, 5, 39, 41{44, 50, 51, 57, 62,
66, 69, 70, 73, 81, 82, 111, 112, 115, 116, 137, 142, 165, 179, 180, 185, 187, 190{192, 204, 220, 228, 231, 233{235, 239, 244, 249, 258, 267, 268, 279, 287{289, 308, 312, 328, 336, 343, 344, 349, 350, 372, 374 Aho-Corasick output function, 62{64, 67, 68, 73, 75{ 77, 233, 235, 366 transition function, 64{69, 73, 112, 233, 234, 358 algorithm details "-removal, 142{144, 153{156, 158{163, 165{167, 172, 173, 175{178, 184, 185, 187, 312, 324, 329, 367 "-removal (dual), 144, 145, 154, 160, 167, 175{179, 185, 187, 312, 368

358

INDEX Symnodes encoding, 144, 145, 154, 160{ 163, 165, 167, 176{179, 185, 187, 312, 369 Aho-Corasick, 42{44, 48, 58, 60, 62{ 64, 73, 75, 77, 84, 100, 112, 358 failure function, 43, 44, 48, 58, 72, 73, 84, 100, 358 optimized, 42{44, 48, 58, 64, 66, 73, 84, 100, 358 auxiliary sets, 144, 145, 154, 160, 162, 163, 165, 167, 176, 178, 179, 185, 187, 312, 358 begin-marker, 144, 145, 154, 160, 164, 165, 167, 176, 187, 360 Boyer-Moore, 44{46, 48, 58, 84, 96, 97, 100, 101, 103, 107, 110, 360, 366 Commentz-Walter, 44, 45, 48, 58, 83{ 85, 90{93, 95{97, 99, 100, 361 Boyer-Moore variant, 44, 45, 48, 58, 84, 92, 93, 95{97, 100, 360 near opt., 44, 45, 48, 58, 84, 93, 95, 97, 100, 366 normal, 44, 45, 48, 58, 84, 95, 97, 100, 366 optimized, 44, 45, 48, 58, 84, 91, 92, 95{97, 100, 361 right optimized, 44, 46, 48, 58, 84, 99, 100, 367 DeRemer's, 142, 144, 145, 154, 159, 160, 167, 176, 312, 370 end-marker, 144, 145, 154, 160, 167, 176, 179, 187, 312, 362 lters, 143, 144, 154, 157, 160, 167, 176, 363 KMP failure function, 43, 44, 48, 58, 74, 75, 77, 84, 100, 364 linear search, 43, 44, 48, 58, 70, 73, 75, 77, 84, 100, 112, 365 lookahead left, 44, 45, 48, 58, 84, 90{93, 95{97, 99, 100, 365

359 none, 44, 45, 48, 58, 84, 90, 100, 366 right, 44, 45, 48, 58, 84, 97, 99, 100,
368 match information, 44, 47, 48, 58, 84,
100, 110, 365 match orders, 43, 44, 46, 48, 58, 84,
100{103, 107, 110, 365 forward, 44, 46, 48, 58, 84, 100, 102,
109, 364 optimal mismatch, 44, 46, 48, 58,
84, 100, 102, 366 reverse, 44, 46, 48, 58, 84, 100, 102,
368 orders
decreasing length, 50, 358 decreasing pre xes, 42, 43, 50, 57,
366 decreasing su xes, 42, 43, 50, 54,
56, 57, 368 increasing length, 50, 358 increasing pre xes, 42, 43, 50, 51,
53{57, 59{64, 73, 75, 77, 82, 83, 85, 90{93, 95{97, 99, 112, 366 increasing su xes, 42, 43, 50, 51, 53, 55, 56, 82, 83, 85, 90{93, 95{ 97, 99, 368 partial derivatives, 144, 145, 154, 160, 167, 172, 173, 176, 366 pre xes, 42{44, 48{51, 54, 57{59, 84, 100, 366 skip loops, 43, 44, 46, 48, 58, 84, 100, 107, 110, 368 fast, 44, 46, 48, 58, 84, 100, 108, 363 rst character, 44, 46, 48, 58, 84, 100, 108, 368 least frequent, 44, 46, 48, 58, 84, 100, 108, 368 none, 44, 46, 48, 58, 84, 100, 107, 366 start-useless state removal, 142{144, 154, 156, 158{160, 163, 165, 167, 173, 176, 179, 184, 185, 187, 312, 324, 329, 370

360 string indexing, 44, 45, 48, 58, 77, 84, 100, 101, 103, 107, 110, 364 subset construction, 142{144, 154, 156, 158{160, 165, 167, 173, 176, 179, 184, 185, 187, 312, 324, 329, 369 su xes, 43, 44, 48{51, 54, 58, 84, 100, 368 tries forward, 43, 44, 48, 55, 56, 58, 84, 100, 364 reverse, 43, 44, 48, 52, 53, 55, 56, 58, 82{85, 90{93, 95{97, 99, 100, 368 Watson's lter, 142{144, 154, 157{160, 167, 176, 185, 312, 324, 370
Alpha, 290, 305 ,alphabet.hpp 229, 230 alphabetDenormalize, 229 alphabetNormalize, 229 ALPHABETSIZE, 229, 247 Amore, 253, 353 Antimirov, V.M., i, 142, 171{174, 189, 190,
312, 345, 349, 350 Array, 220, 221, 245{247, 268{271, 273,
274 array, 245 as..., 264 AS: : : , 263{266 asd..., 266 ASD: : : , 265{267 ,asdderiv 267 ASDDerivative, 267, 268 ,asditder 266 ASDItems, 266, 268 ,asditems 266 ASDItemsDeRemer, 266, 268 ASDItemsWatson, 266, 268 ,asditwat 266 ,asdpasu 267 ,asdpmyg 267 ASDPosnsASU, 267, 268 ASDPosnsMYG, 267, 268 ,asdrever 282

INDEX
ASDReverse, 278, 282 ,asef... 265 ASEF: : : , 265, 266 ,asefitem 266 ASEFItems, 266, 268 ,asefpbs 267 ,asefpbsd 267 ASEFPosnsBS, 267, 268 ASEFPosnsBSdual, 267, 268 ASItems, 266, 268 ,asitems 266 Automate, 253{255, 351 automaton completion, 28, 361 b-mark, see algorithm details, begin-marker Backhouse, R., i, 350 Baeza-Yates, R., 92, 116, 350, 352 BEF, see item dot position, before Berry, G., i, 142, 162, 163, 178, 189, 190,
267, 268, 312, 350 `big-oh', 51, 53{57, 63, 64, 66, 69, 73, 78,
86, 90, 91, 95, 99, 106, 109, 118, 201, 203, 204, 207, 209, 213, 327, 328, 331, 335 BitVec, 271, 272 bitvec, 271 bm, see algorithm details, Boyer-Moore ,bmchar1 241 ,bmchar2 241 bmcw, see algorithm details, CommentzWalter, Boyer-Moore variant ,bms.hpp 228, 239 bms1, 242 ,bmsh1-1 241 ,bmsh1-2 241 BMShift11, 240, 241, 243 BMShift12, 241, 243 BMShift: : : , 239, 240 BMShiftNaive, 241, 243 ,bmshnaiv 241 ,bmslfst1 240 ,bmslfst2 240 ,bmslnone 240

INDEX
,bmslsfc 240
bool, 227
Borland, 250, 251, 276 Boyer, R.S., 38, 39, 41, 42, 44{46, 50,
57, 82, 85, 92, 95{97, 99, 101{103, 110, 112, 115{117, 121, 122, 124, 135{139, 225, 228, 230, 237, 239, 240, 242{244, 247, 249, 288, 289, 308, 344, 347, 350, 355, 357 Boyer-Moore match order, 101{103, 107{111, 365 match procedure, 102{105, 107{110, 365 shift function, 99, 101, 103{105, 107, 108, 110, 368 skip loop, 106, 107, 109{111, 368 Brauer, W., 203, 350 Broy, M., 38, 350 Bruggemann-Klein, i, 163, 184, 350 Brzozowski, J.A., i, 142, 163, 173{175, 190{196, 200, 213, 267, 268, 278, 282, 312, 327, 328, 336, 345, 350 C, 38, 225, 227, 228, 245, 249, 253, 254, 289, 290, 343, 353 C++, 5, 6, 42, 190, 217, 218, 221, 225{ 228, 244{246, 249{251, 253{255, 260, 263, 271, 276, 277, 282, 289, 313, 343, 348, 349, 351, 353{357 CA, see constructions, canonical Champarnaud, J.M., i, 351 Chang, C.-H., 184, 351 char, 109{111, 241, 242 Char1, 241, 242 Char2, 241, 242 CharBM, 237, 238 charbm, 96, 97, 99 CharCW, 237, 238 charcw, 94{96 ,charrang 269 CharRange, 257, 263, 269, 272, 279, 280 CharRLA, 238 charrla, 98, 99

361 codom, see codomain codomain, 9, 361 ,com-misc.hpp 228, 244, 258, 268 ,com-opt.hpp 268 Commentz-Walter, B., 41, 42, 44, 45, 50,
53, 82, 85, 86, 89, 92, 94, 95, 97, 106, 108, 112, 113, 115, 116, 121{ 123, 129, 135{137, 139, 220, 231, 235{239, 244, 249, 287{290, 308, 342{344, 347, 351 Complete, see nite automata, complete complete, see automaton completion composition, 9, 10, 21, 29, 32, 33, 80, 153, 155{157, 165, 173, 175, 179, 184, 195, 358 constructions canonical, 151{153, 155{157, 175, 184, 361 Corasick, M.J., 41{44, 50, 51, 57, 62, 66, 69, 70, 73, 81, 82, 111, 112, 115, 116, 137, 220, 228, 231, 233{235, 239, 244, 249, 258, 287{289, 308, 344, 349 cout, 228 Crochemore, M., i, 351 CRSet, 269, 280 crset, 269 current, 134 cw, see algorithm details, Commentz-Walter cw-opt, see algorithm details, CommentzWalter, optimized cwchar, 238 ,cwcharbm 238 ,cwcharrl 238 cwd1, 238 cwd2, 238 cwdopt, 238 cwout, 239 CWOutput, 236, 239 ,cws.hpp 235 CWShift: : : , 236 CWShiftNaive, 236, 244 CWShiftNLA, 236, 244

362
CWShiftNorm, 237, 244 CWShiftOpt, 237, 238, 244 CWShiftRLA, 236, 238, 244 CWShiftWBM, 237, 238, 244 ,cwshnaiv 236 ,cwshnla 236 ,cwshnorm 237 ,cwshopt 237 ,cwshrla 238 ,cwshwbm 237 D1, 237, 238 d1, 89{98, 115, 126{130, 136, 238 D2, 237, 238 d2, 89{98, 115, 126{130, 136, 238 Darlington, J., 38, 351 dbm, 96 dbmcw , 92, 95 DEC, 290, 305 decouple, see weakening strategy, de-
couple dee1, 130 dee2, 130
delete, 222
DeRemer, F.L., 142, 159, 189, 268, 312, 351
Det, see nite automata, deterministic property
Det0, see nite automata, deterministic property, weak
DFA, see nite automata, deterministic Dijkstra, E.W., 38, 351 discard, see weakening strategy, discard
conjunct DMM, see Moore machines, deterministic
D, see dot relations, movement D, see dot relations, movement (in nite)
dnopt, 93, 94 dom, see domain
con-, see O
domain, 9, 12, 18, 145{147, 150, 153, 183, 362
DOpt, 238

INDEX

dopt, 90, 91, 93, 98, 238 dot relations
hop, 150, 151, 153, 168, 369 movement, 149{151, 153, 155, 158, 161,
162, 166, 168{170, 172, 175, 184, 185, 362 movement (in nite), 149, 150, 362 Dots, see items (of a regular expression) dottings, see items (of a regular expression) DRE, see regular expressions, dotted DTrans, 273, 275 dtrans, 273 ,dtransre 275 DTransRel, 260, 275, 279, 280 duplicate, see weakening strategy, duplicate conjunct

?!EE?,,

see see

regular regular

expression, expression,

left of dot right of dot

"-removal, 29, 32, 33, 141, 143, 153, 155{

157, 175, 184, 367

general, 29, 368

e, see problem details, end-points

e-mark, see algorithm details, end-marker

Earley, J., 159, 352

e r, 131

EFTrie, 234, 235, 239

Eindhoven Pattern Kit, 225, 250

ell, 133

emm, 132, 133

empty string, 13{21, 23{25, 28{33, 47, 51{

56, 59{68, 70{79, 81{83, 85, 86,

88, 89, 91{94, 96{98, 101, 103{

105, 107, 110, 118{122, 125, 127{

131, 134, 136, 141{163, 165{185,

187, 189, 192, 195, 196, 260, 262{

266, 268, 273, 275, 312, 313, 315,

321{325, 329, 347, 358, 363, 367,

368

enc, see encoding function

encoding function, 62, 63, 66{68, 70, 362

INDEX
enlarge, see weakening strategy, enlarge range
"-free, see nite automata, "-free equiv, see equivalence procedure equivalence class, 12, 22, 174, 200, 201,
203{205, 207{209 equivalence procedure, 210{213, 363 equivalence relations
on states, 197, 363 ER, see equivalence relations, on states FA, see nite automata FA, 262 ,fa-canon 263 fa-dfa, 265, 278{281 ,fa-effa 265 fa-fa, 263 fa-rfa, 264 FA: : : , 256 FAAbs, 261{265
::attemptAccept, 257 ::reportAll, 257{259 faabs, 261 ,faabs.hpp 257, 261 FACanonical, 263, 268 FADFA, 265, 266, 268, 277{279, 282 ::areEq, 280, 281 ::compress, 279 ::minBrzozowski, 278 ::minDragon, 279 ::minHopcroft, 280 ::minHopcroftUllman, 280 ::minWatson, 281 ::reverse, 278 ::split, 279 FAEFFA, 265, 268 FAFA, 260, 263, 265, 268 Fail, 248 fail, 248 FailIdx, 232 ,failidx 232 fails, 248 ,fails.hpp 248

363

failure function

forward, 71{78, 363

indexing, 75{78, 363

reverse, 71, 129{131, 363

ffbff

, ,

see see

failure failure

function, function,

forward indexing

fr, see failure function, reverse

FALSE, 221, 227, 228, 244, 257, 258

false, 8, 180, 198, 203, 210

Fan, J.-J., 91, 352

FARFA, 258, 259, 264, 268

,fas.hpp 257{259, 262, 264, 265

fast, see algorithm details, skip loops,

fast

FFail, 234, 235

fgrep, 249, 290

filt, see algorithm details, lters

lters

DeRemer's, 145, 159, 370

Watson's, 143, 157{159, 185, 268, 370

nal, 134

nal-unreach. removal, 28, 370

nite automata, 4, 5, 19{25, 28{30, 32, 33,

120, 124, 125, 137, 138, 141, 151,

156, 158, 161{163, 165, 173, 175,

177{179, 194{196, 253, 260, 262,

286, 311, 312, 314{316, 318, 319,

321{325, 328, 336, 363, 371, 373

"-free, 23{25, 28{30, 118, 119, 142,

143, 153, 195, 260, 264{266, 268,

315, 321{323, 325, 363

complete, 22, 23, 25{28, 30, 184, 186{

188, 192, 195, 196, 361

deterministic, 4, 5, 25{27, 29, 30, 33,

57, 120, 141, 142, 156, 158, 159,

163, 165, 166, 173, 174, 179, 180,

184{188, 190{192, 194{196, 201,

205, 207, 214, 277, 278, 286, 311,

312, 315{322, 324, 325, 327{336,

345, 348, 362, 371, 373

minimality of, 26{28, 195, 196, 365

364
deterministic property, 25, 28, 30, 195, 362
weak, 25, 195, 362 useful, 24, 26, 28, 370
nal, 24, 128, 136, 195, 370 start, 24, 26, 28, 124, 125, 195, 196,
370 FIRE Engine, 175, 180, 250, 253, 254, 275{
277, 311, 328, 345, 356 FIRE Lite, 5, 175, 180, 190, 244, 253{259,
261, 268, 269, 275{277, 281, 282, 286, 311, 327{329, 345, 348 First, see symbol nodes, rst FIRSTSTATE, 244, 270 Follow , see symbol nodes, follow relation FReachable, 23, 24 ft, see algorithm details, tries, forward FTrie, 234, 235 fwd, see algorithm details, match orders, forward Gamma, 233, 234 ,getrusage 290, 291 Glushkov, V.M., 142, 163, 165, 166, 186, 188{190, 267, 268, 312, 352 Grail, 254, 255, 261, 277, 355 grep, 136, 137, 229, 329 Gries, D., 194, 207{209, 214, 352 growthSize, 246
H, see transduction, helper function
Hemerik, C., i, 376 Hopcroft, J.E., 191, 194, 201, 205, 207{
209, 213, 214, 254, 280, 282, 327, 328, 336, 342, 349, 352, 353 HP, 290, 305 Hu man, D.A., 191, 194, 207, 353 Hume, S.C., 38, 42, 99, 104, 113, 225, 287, 288, 291, 294, 305, 308, 344, 353 IBM, 251, 313, 351 ,in-inrel 273 index of equivalence relation, 12, 199, 200, 358

INDEX
indices, see algorithm details, string indexing
,INLINING 222 INR, 254, 353
int, 227, 238, 264, 272, 280
Intel, 313 IntIntRel, 273, 274 IntSet, 271{274 intset, 271 INVALIDSTATE, 244, 273, 279 ,iostream.h 228 isomorphism, 21, 22, 26, 358 ,it-itrel 274 item dot, 146, 148, 149, 151{153, 155, 157{
159, 161, 162, 166, 168, 169, 172, 175, 177, 184, 185, 358 item dot position after, 146{148, 150, 153, 155, 158, 161, 162, 166, 168{170, 172, 175, 184, 185, 358 before, 146{148, 150, 153, 155, 157, 158, 161, 162, 166, 168{171, 175, 184, 185, 360 ItemItemRel, 263, 266, 274 items (of a regular expression), 147{151, 169, 362 ItemSet, 272, 274 ,itemset 272 Jonkers, H., i, 38, 342, 353 Kameda, T., i, 195, 213, 354 kbm, 96 kbmcw , 92, 93, 96 kcw, 90, 94{96 Keller, J.P., 209, 353 Klint, P., i, 352 kmp-fail, see algorithm details, KMP, failure function knla, 89, 90 knopt, 93, 95, 96 Knuth, D.E., 41, 42, 44, 50, 51, 57, 73, 78, 81, 82, 112, 115, 116, 228, 230,

INDEX

232, 234, 243, 244, 249, 287{289, 344, 353 kopt, 91{93, 95, 99 kropt, 98, 99 Kruseman Aretz, F.E.J., i, 376 kwbm, 97

language

of a nite automaton, 22{24, 26{30,

32, 33, 141, 175, 184, 186{188, 195,

365

of a regular expression, 16, 17, 117,

141, 146, 152, 153, 162, 166, 170,

172, 175, 184, 186{188, 365

L?!LL?FA,,

, see language, of a see left language see right language

nite automaton

LRE, see language, of a regular expression

Last, see symbol nodes, last

Lee, M., 246, 355

van Leeuwen, J., i, 349, 354

left drop, 13, 51, 53{56, 59, 61, 63, 64, 66,

73, 75{77, 79, 82, 83, 85, 86, 99,

101, 103, 105, 107, 110, 118, 119,

121, 122, 127, 130

left language, 22{25, 27, 30, 31, 80, 119{

125, 127, 128, 132, 134, 136, 137,

152, 365

left take, 13, 51, 53{56, 59{61, 63, 64, 66,

69{77, 79, 82, 83, 85{88, 97{99,

101, 103, 105, 107, 110, 118, 119,

121{123, 125, 127, 137

lex, 329

lla, see algorithm details, lookahead, left

ls, see algorithm details, linear search

main, 228, 229 Marcelis, A.J.J.M., 38, 354 match, see Boyer-Moore, match procedure match, 228, 231{233, 236 match set, 91{93, 95, 96, 365
MAX , see quanti cation, maximum max, 16, 70, 71, 73{77, 200, 210{212

365

max, 237, 244

MAX

p

, see x

quanti

cation, maximumpre-

MAXv , see quanti cation, maximumpar-

tition

MAX

s

, see x

quanti

cation, maximumsuf-

McNaughton, R., 142, 163, 165, 166, 186,

188{190, 267, 268, 312, 354

mi, see algorithm details, match informa-

tion

Microsoft, 251, 276

MIN , see quanti cation, minimum

Min, see nite automata, deterministic,

minimality of

min, 237, 244

MinC, see nite automata, deterministic,

minimality of

Minimal, see nite automata, determinis-

tic, minimality of

MinimalC, see nite automata, determin-

istic, minimality of

Mirkin, B.G., 196, 354

MKS, 249

MM, see Moore machines

mo, see Boyer-Moore, match order

mo, see algorithm details, match orders

Moore machines, 21, 23, 25, 32, 67, 365

deterministic, 25, 26, 32, 68, 80, 362

Moore, E.F., 7, 19, 21, 23, 25, 28, 31, 57,

66{69, 78, 80{82, 112, 191, 194,

203, 207, 289, 344, 348, 354

Moore, J.S., 38, 39, 41, 42, 44{46, 50,

57, 82, 85, 92, 95{97, 99, 101{103,

110, 112, 115{117, 121, 122, 124,

135{139, 225, 228, 230, 237, 239,

240, 242{244, 247, 249, 288, 289,

308, 344, 347, 350, 355, 357

Morris, J.H., 41, 42, 44, 50, 51, 57, 73,

78, 81, 82, 112, 115, 116, 228, 230,

232, 234, 243, 244, 249, 287{289,

344, 353

MS, see match set

366 INDEX

MS-Dos, 136, 223, 249, 251, 276, 313 N, see naturals naturals, 9, 10, 13, 18, 89, 90, 92{94, 96,
98, 99, 106, 109, 126{128, 183, 366 non-zero, 9, 18, 366 N+, see naturals, non-zero NDEBUG, 222 near-opt, see algorithm details, Commentz-
Walter, near opt.
new, 222
nla, see algorithm details, lookahead, none ,no-norel 274 Node, 270, 272, 274 node, 270 NodeNodeRel, 274 NodeSet, 263, 272 ,nodeset 272 NodeTo, 263, 264, 270 nodeto, 270 none, see algorithm details, skip loops,
none norm, see algorithm details, Commentz-
Walter, normal Null, 145, 162, 164, 170{172, 174, 178,
180{182, 186, 264
O, see `big-oh'
obm, see algorithm details, Boyer-Moore okw, see problem details, one keyword om, see algorithm details, match orders,
optimal mismatch OS/2, 251 Output, see Aho-Corasick, output func-
tion
P, see powerset
p, see algorithm details, pre xes p+, see algorithm details, orders, increas-
ing pre xes p?, see algorithm details, orders, decreas-
ing pre xes Paige, R., i, 184, 209, 351, 353, 355 partial derivatives, 172{174, 366

pattern matching, 47, 49, 51, 53{56, 59,

61, 63, 64, 66, 73, 75{78, 80, 82,

86, 101, 103, 107, 110, 366

end-point registration, 59, 61, 63, 64,

66, 73, 75{78, 80, 366

indexing, 76{78, 366

regular expression, 117{119, 122, 127,

368

PM , see pattern matching

PM e, see pattern matching, end-point reg-

PdM e,

istration see pattern

matching,

end-point

reg-

istration, indexing

PD, see partial derivatives

pd, see algorithm details, partial deriva-

tives

Pentium, 313

PerfMatch, 99, 101, 104, 105, 107, 108

Perrin, D., 78, 81, 354

Pirklbauer, K., 288, 355

pm-ac, 233

,pm-bfmul 232

,pm-bfsin 232

,pm-bfsin.hpp 228

pm-bm, 239

pm-cw, 235

pm-kmp, 232

,pm-kmp.hpp 228

,pm-multi 231

,pm-singl 230

PMAC, 233, 244

PMBFMulti, 232, 244

PMBFSingle, 232, 243

PMBM, 239{241, 243

PMCW, 235, 236, 244

PMKMP, 232, 243

PMMultiple, 231, 233, 236, 243

PMRE, 259

::match, 259

PMSingle, 230{232, 239, 243

::match, 227, 229

,po-porel 274

Posn, 270, 272, 274

INDEX posn, 270 PosnPosnRel, 264, 267, 274 PosnSet, 264, 267, 272 ,posnset 272 PosnTo, 270 posnto, 270 Potho , A., i, 353 powerset, 8, 9, 13, 14, 16, 19{23, 25, 29,
31, 32, 62, 67, 78{81, 91, 118, 119, 132, 153, 168, 170, 171, 174, 195, 196 Pratt, V.R., 41, 42, 44, 50, 51, 57, 73, 78, 81, 82, 112, 115, 116, 228, 230, 232, 234, 243, 244, 249, 287{289, 344, 353
pref, see pre xes
pre x ordering, 15, 358 pre xes, 14, 15, 18, 24, 55{57, 60{64, 66{
76, 78{81, 128, 129, 132, 367 Prob, see probability probability, 102, 367 problem details
end-points, 42{44, 48, 57{64, 73, 75, 77, 84, 100, 112, 362
one keyword, 44{46, 48, 58, 75, 77, 84, 96, 99{101, 103, 107, 110, 135, 366
,process 258 q-decouple, see weakening strategy, de-
couple, quanti cation q-split, see weakening strategy, split, quan-
ti cation quanti cation
existential, 9, 11, 29, 68, 70, 71, 124, 125, 137, 153, 199, 200, 202{208, 212
intersection, 31, 80 maximum, 11, 18, 53, 56, 94, 95, 118,
125{127 maximum partition, 197 maximum pre x, 129 maximum su x, 61, 71, 74

367 minimum, 10, 11, 83, 85, 87{90, 92{
96, 98, 104{106, 108, 109, 122, 125, 126 union, 10, 13{15, 22, 23, 29{33, 47, 49, 51, 59, 75, 76, 79, 81, 99, 101, 111, 117, 126, 127, 132, 134, 174, 184{188, 195, 196, 202, 206 universal, 11, 12, 17, 18, 22, 24{26, 28{31, 52, 55, 68, 74, 101, 102, 105, 108, 109, 121, 123{125, 128, 131, 133, 134, 137, 195{198, 200{ 210, 212 R, see reals r-opt, see algorithm details, CommentzWalter, right optimized ran1, 292 Raymond, D.R., i, 355 Rch, 135 RE, see regular expressions RE, 256{261, 263{268 re, 260 re.hpp, 257, 259 Reach, 23, 132, 133, 135 reals, 9, 102, 367 re nement, 12, 196{198, 200, 204, 213, 358, 365 Regnier, M., 92, 350 Regpack, 254, 354 regular expression left of dot, 148, 152, 362 right of dot, 148, 149, 152, 153, 168, 170{172, 362 regular expressions, 16, 17, 29, 115, 145, 146, 148{151, 161, 162, 165, 168, 170{172, 174, 175, 178{182, 184{ 187, 253, 260, 313{315, 329, 367 dotted, 146, 148, 149, 168, 170, 171, 362 Rem, M., i rem", see "-removal rem-", see algorithm details, "-removal

368 INDEX

rem-"-dual, see algorithm details, "-removal (dual)
remove", see "-removal, general REops, 261 reops, 260 ,reops.hpp 261 report, 228, 258 rev, see algorithm details, match orders,
reverse reverse, 278 right drop, 13, 51, 53, 82, 86, 118, 119,
122, 127 right language, 22, 24, 26, 27, 31, 152, 153,
195, 196, 198, 199, 365 right take, 13, 51{53, 82, 86{88, 90{98,
118, 119, 122, 123, 127 rla, see algorithm details, lookahead, right RPM , see pattern matching, regular ex-
pression rt, see algorithm details, tries, reverse RTrie, 236, 238 s, see algorithm details, su xes s+, see algorithm details, orders, increas-
ing su xes s?, see algorithm details, orders, decreas-
ing su xes S1, 241, 242 Set, 220, 221, 235, 246, 268, 269, 271, 272 set, 246 set cardinality, 12, 13, 21, 25{27, 30, 31,
51{53, 57, 63, 64, 66, 68, 69, 73, 75{78, 83, 86{99, 101{111, 118, 122{125, 128, 130{134, 136, 137, 198{201, 203, 204, 207{213, 232, 280 ,set.hpp 228 Sethi, R., i, 142, 162, 163, 165, 178{180, 185, 187, 189{191, 204, 267, 268, 279, 312, 328, 336, 349, 350 sfc, see algorithm details, skip loops, rst character shift, see Boyer-Moore, shift function

similarity, 174, 175, 358 sl, see Boyer-Moore, skip loop sl, see algorithm details, skip loops SL: : : , 239, 240 SLFast1, 240, 243 SLFast2, 240, 243 slfc, see algorithm details, skip loops,
least frequent SLNone, 240, 243 SLprime, 134 SLSFC, 240, 243 Smit, G. de V., 288, 355 Snake, 290, 305 van de Snepscheut, J.L.A., 195, 196, 213,
355 Sparc Station, 136, 290, 305 SPARE Parts, 5, 225{227, 229, 242, 246,
249{251, 256{259, 268, 270, 275, 276, 286, 345, 348 split, see weakening strategy, split Splittable, 200, 201, 204, 205, 207{209 SReachable, 23, 24 ,sssymrel 281 set, 132{134 ,st-assoc 270 ,st-eqrel 282 ,st-pool 270 ,st-strel 274 Standard Template Library, 221, 246, 355 start-unreach. removal, 28, 32, 33, 80, 143, 156, 157, 163, 165, 173, 179, 184, 195, 370 State, 234, 235, 238, 239, 244{246, 248, 260, 264, 268, 270{275, 279{282 state, 244 StateAssoc, 260, 263, 270, 271 StateEqRel, 279, 281, 282 StatePool, 270, 271 StateSet, 272, 273, 275, 282 ,stateset 272 StateStateRel, 260, 274, 281 StateStateSymRel, 279, 281

INDEX StateTo, 234, 235, 238, 239, 246, 248, 249,
268, 270, 275, 280, 282 ,stateto 246 Stepanov, A., 246, 355 STrav: : : , 239, 241 STravFWD, 242, 243, 247, 248 ,stravfwd 247 STravOM, 243, 247 ,stravom 247 STravRAN, 243, 247 ,stravran 247 STravREV, 240, 242, 243, 247, 248 ,stravrev 247 String, 235, 245, 268 string, 245 ,string.hpp 228, 259 strlen, 245
struct, 273
Su, K.-Y., 91, 352 subset, see subset construction, nite au-
tomata subset, see algorithm details, subset con-
struction subset construction
nite automata, 29{31, 33, 156, 157, 165, 173, 179, 184, 194, 195, 369
Moore machines, 32, 80, 369 subsetmm, see subset construction, Moore
machines
su , see su xes
su x ordering, 15, 16, 61, 68, 69, 358 su xes, 14{16, 52, 53, 59{64, 67{71, 73,
74, 76, 78, 80, 81, 83, 85, 86, 88{ 93, 95, 96, 118, 119, 122{125, 128{ 134, 136, 369 Sun, 136, 290, 305 Sunday, D., 38, 42, 99, 104, 113, 225, 287, 288, 291, 294, 305, 308, 344, 353 sym, see algorithm details, Symnodes encoding symbol nodes, 145, 150, 153, 155, 157, 158, 161, 162, 164{166, 169, 172, 175, 177{181, 183, 185, 369

369 rst, 145, 162, 164, 177{183, 185{187,
264, 364 follow relation, 145, 162, 164, 177, 179,
180, 182, 183, 186{188, 264, 267, 364 last, 145, 162, 164, 165, 177, 178, 180, 182, 183, 186, 187, 264, 267, 365 SymbolTo, 234, 235, 238, 247, 248 ,symbolto 247 Symnodes, see symbol nodes
T , see dot relations, hop
tar, 251, 276 tee, 129 Thompson, K., 141, 152, 254, 312, 356 toadd, 206 ,tr-pair 272 Trans, 273, 275 trans, 273 transduction, 23, 358
helper function, 23, 364 TransPair, 272 TransRel, 260, 274, 275 ,transrel 274 tree paths
concatenation operator, 18, 19, 146, 148, 150, 163, 164, 168{170, 178, 179, 181{183, 358
Trees, see trees, set of trees
set of, 18, 145, 369 universal domain, 370 Trie, 247, 248 trie extended forward, 71{73, 79, 112, 234,
235, 358 forward, 53, 55, 56, 64, 65, 71, 72, 358 reverse, 52, 53, 82, 86, 358 trie, 248 tries, 248 ,tries.hpp 248 TRUE, 221, 227{229, 244, 257, 258

370

true, 8, 85, 105, 107, 122, 180, 198, 203, 210, 211
tuple projection, 11, 12, 23, 59, 135, 358
typedef, 235, 245, 248, 270, 272, 274

U, see trees, universal domain

Ullman, J.D., i, 142, 165, 179, 180, 185,

187, 190{192, 194, 204, 207, 214,

267, 268, 279, 280, 312, 328, 336,

349, 350, 353

undot, 147, 150

Unix, 136, 251, 254, 276, 285, 290

unsigned int, 271

Urbanek, F., 203, 356

use-s, see algorithm details, start-useless

state removal

Useful, see nite automata, useful

useful, see useless state removal

uUuUussssseeeeelffffeuuuusllllssfsf,,,s,stssseaeeeeeteeestrnnaenamriitttl-e-eouuvaannauurrlte,etoaoa2mcmc8hh,aa..3ttrar7aee,0,mmuuososevevfafauullll,,

nal start

void, 278{281

W, see lters, Watson's
Watcom, 249{251, 276, 313 Watson, B.W., 345, 350, 356, 357, 376 Watson, R.E., i, 115, 357 weakening strategy
absorb, 87, 93, 97, 98, 358 decouple, 87, 88, 91, 92, 97, 98, 362
quanti cation, 87, 94, 95, 97, 98, 367
discard conjunct, 87, 362 duplicate conjunct, 87, 92, 97, 98, 362 enlarge range, 88, 94, 95, 363 split, 87, 88, 90, 97, 98, 368
quanti cation, 87, 88, 90, 97, 98, 367
Wfilt, see algorithm details, Watson's lter
Windows, 251, 276

INDEX
Wood, D., i, 191, 192, 200, 202, 203, 357
X , see lters, DeRemer's
Xfilt, see algorithm details, DeRemer's lter
Yamada, H., 142, 163, 165, 166, 186, 188{ 190, 267, 268, 312, 354
Zwaan, G., i, 41, 357, 376

Summary
A number of fundamental computing science problems have been studied since the 1950s and 1960s. For each of these problems, numerous solutions (in the form of algorithms) have been developed over the years. In these collections of solutions, we can identify the following three de ciencies:
1. Algorithms solving the same problem are di cult to compare to one another. This could be due to the use of di erent programming languages, paradigms, styles of presentation | or simply the addition of unnecessary details.
2. Collections of algorithm implementations solving a problem are di cult to nd. Some of the algorithms are presented in a relatively obsolete manner, either using a nowdefunct notation or obsolete programming language, making it di cult to either implement the algorithm or nd an existing implementation.
3. Little is known about the comparative practical running time performance of the algorithms. The lack of existing implementations in one and the same framework, especially of some of the older algorithms, has made it di cult to determine the running time characteristics of the algorithms. Selection of an algorithm must then be made on the basis of the theoretical running time, or simply by guessing.
In this dissertation, a solution to each of these de ciencies is presented. To present the solutions, we use the following three fundamental computing science problems:
1. Keyword pattern matching in strings. Given a nite non-empty set of keywords (the patterns) and an input string, nd the set of all occurrences of a keyword as a substring of the input string.
2. Finite automata (FA) construction. Given a regular expression, construct a nite automaton which accepts the language denoted by the regular expression.
3. Deterministic nite automata (DFA) minimization. Given a DFA, construct the unique minimal DFA accepting the same language.
In the following paragraphs, we will outline the solutions presented for each of the de ciencies.
371

The di culty in comparing algorithms is overcome by creating a taxonomy of algorithms for a given problem. Each of the algorithms is rewritten in a common notation and is examined to determine the essential ingredients that distinguish it from any other algorithm. These ingredients (known as details) can take the form of problem details (a restriction on the class of problems solved), or algorithm details (some correctness-preserving change to the algorithm to improve e ciency). Once each algorithm has been reduced in such a way, it can be characterized by its set of details. In presenting the taxonomy, the common details of several algorithms can be factored and presented together. In this fashion, a `family tree' of the algorithms is constructed, showing clearly what any two algorithms have in common and where they di er. Because the root of the family tree is a na ve, and correct, algorithm and the details are applied in a correctness-preserving manner, the correctness argument for each of the algorithms is implicit in the taxonomy.
The common notation and presentations in the taxonomies enable us to implement the algorithms uniformly, in the form of a class library (also known as a toolkit). The factoring of essential details, inherent in the taxonomies, leads to factoring of common components in the inheritance hierarchy of the class library. Object-oriented concepts, such as virtual (or deferred) member functions, inheritance and template classes, prove to be useful in presenting a coherent class library, the structure of which re ects the taxonomy from which it was created. For the rst time, most (if not all) solutions are presented in a single class library, giving clients of the library a large choice of objects and functions.
With a class library that contains most of the known solutions, we are nally able to gather data on the performance of the algorithms in practice. Since the algorithms are taken from a single class library which was implemented by one person, and the quality of implementation of the class library is homogeneous, the relative performance data gathered (comparing algorithms) is not biased by the implementation. This performance data allows software engineers to make more informed decisions (based upon their needs, and the characteristics of their input data) concerning which algorithm and therefore which objects and functions to use in their applications.
The development of the taxonomies has not been without its spin-o s. In each of the three taxonomies presented, signi cant new algorithms have been developed. These algorithms are also implemented in the corresponding class libraries. The techniques developed in the taxonomy of pattern matching algorithms proved to be particularly useful in deriving an algorithm for regular expression pattern matching, and in doing so, answering an open question posed by A.V. Aho in Aho80, p. 342].

Samenvatting (Dutch summary)
Er bestaan een aantal fundamentele problemen in de informatica die al sinds begin jaren 50 en 60 bestudeerd worden. Voor elk van deze problemen zijn er in de loop der tijd een groot aantal oplossingen (in de vorm van algoritmen) ontwikkeld. Bij deze oplossingen kunnen we drie verschillende tekortkomingen onderscheiden:
1. Algoritmen die een en hetzelfde probleem oplossen zijn moeilijk met elkaar te vergelijken. Dit kan worden veroorzaakt door het gebruik van verschillende programmeertalen, paradigmata, presentatiestijlen of eenvoudigweg door toevoeging van onnodige details.
2. Collecties van implementaties van algoritmen die hetzelfde probleem oplossen, zijn moeilijk te vinden. Sommige algoritmen worden op een vrij obsolete manier gepresenteerd (hetzij door het gebruik van een achterhaalde notatie dan wel door toepassing van verouderde programmeertalen). Dit maakt het moeilijk de algoritme te implementeren of een bestaande implementatie te vinden.
3. Er is weinig bekend over de relatieve snelheid van de algoritmen in de praktijk. Omdat implementaties in een en hetzelfde raamwerk ontbreken (vooral daar waar het oudere algoritmen betreft), is het moeilijk om de snelheidskarakteristieken te bepalen. De keuze van de algoritme moet dan gemaakt worden op basis van de theoretische snelheid of gewoon door gissen.
In dit proefschrift wordt voor ieder van deze tekortkomingen een oplossing gepresenteerd aan de hand van de volgende drie fundamentele informatica-problemen:
1. Patroonherkenning in symboolrijen. Uitgaande van een eindige niet-lege verzameling van sleutelwoorden (de patronen) en een invoerrij, vind de verzameling voorkomens van een sleutelwoord als een subrij van de invoerrij.
2. Constructie van eindige automaten (FA). Gegeven een reguliere expressie, construeer een eindige automaat die de taal van de reguliere expressie accepteert.
3. Minimalisatie van deterministische eindige automaten (DFA). Gegeven een DFA, construeer de unieke minimale DFA die dezelfde taal accepteert.
In de volgende alinea's zullen de oplossingen voor de drie verschillende tekortkomingen kort uiteengezet worden.
373

De moeilijkheid van het vergelijken van algoritmen wordt opgelost door een taxonomie van algoritmen voor een gegeven probleem te creeren. Iedere algoritme wordt herschreven in een gemeenschappelijke notatie. Daarnaast wordt bekeken wat de essentiele ingredienten zijn die de verschillende algoritmen van elkaar onderscheiden. Deze ingredienten of 'details' kunnen ingedeeld worden in probleem-details (een restrictie op het soort problemen dat opgelost wordt) of in algoritme-details (een correctheid-behoudende transformatie van de algoritme ter verbetering van de e ciency). Zodra een algoritme op een dergelijke manier ontleed is, kan deze gekarakteriseerd worden op basis van de verzameling details. In de taxonomie kunnen de gemeenschappelijke details van twee algoritmen gefactoriseerd en gemeenschappelijk gepresenteerd worden. Op deze manier wordt er een stamboom van algoritmen gecreeerd, die duidelijk aangeeft in welke opzichten verschillende algoritmen met elkaar overeenkomen dan wel van elkaar verschillen. Omdat de wortel van de stamboom een na eve (en correcte) algoritme is, en de details in een correctheidbehoudende manier toegepast worden, is het correctheidsargument voor iedere algoritme impliciet aanwezig in de taxonomie.
De algemene notatie en presentaties in de taxonomieen stellen ons in staat om de algoritmen op uniforme wijze, in de vorm van een class library (ofwel toolkit), te implementeren. De factorisering van essentiele details, inherent aan de taxonomieen, leidt tot factorisering van algemene componenten in de inheritance hierarchy van de class library. Object-georienteerde concepten, zoals virtual (of deferred) member functions, inheritance en template classes, blijken nuttig voor het presenteren van een coherente class library waarvan de structuur de corresponderende taxonomie weergeeft. Voor het eerst worden vrijwel alle oplossingen gepresenteerd in een enkele class library, waardoor de gebruikers van de bibliotheek een grote keuze hebben uit objecten en functies.
Met een class library die bijna alle bekende oplossingen bevat, zijn we eindelijk in staat om gegevens ten aanzien van de praktijkprestaties van de algoritmen te verzamelen. Aangezien de algoritmen afkomstig zijn uit een enkele class library welke ge mplementeerd is door een en dezelfde persoon en de kwaliteit van de implementatie van de class library homogeen is, zijn de relatieve gegevens ten aanzien van de prestaties van de algoritmen niet be nvloed door de implementatie. Deze gegevens stellen software-ingenieurs in staat om (afhankelijk van hun behoeften en de aard van hun input data) beter gefundeerde keuzen te maken met betrekking tot de in hun applicaties te gebruiken algoritmen en de daarbij behorende objecten en functies.
De ontwikkeling van de taxonomieen heeft bovendien enkele bijprodukten opgeleverd. In ieder van de drie taxonomieen die gepresenteerd worden, zijn belangrijke nieuwe algoritmen ontwikkeld. Deze algoritmen zijn ook ge mplementeerd in de corresponderende class libraries. De technieken ontwikkeld in de taxonomie van de patroonherkenning-algoritmen bleken vooral nuttig te zijn voor de ontwikkeling van een algoritme voor reguliere expressie patroonherkenning, waarmee een oplossing wordt gegeven voor een open probleem gesteld door A.V. Aho in 1980 Aho80, p. 342].

Curriculum Vitae
I was born on 10 October 1967 in the town of Mutare (Nyika for \the river of ore"), in Eastern Zimbabwe. Until recently, the town was known as Umtali (the Shangane form of Mutare) and the country was known as Rhodesia. After living in Pretoria and Durban (South Africa), and attending school in England for a year, my family and I immigrated to Canada.
From September 1980 to June 1985, I attended Charles Bloom Secondary School (junior high-school) followed by Vernon Secondary School (senior high-school), graduating with Honours in Mathematics and receiving a scholarship to the University of Waterloo on 30 June 1985. From September 1980 until September 1985, I served as a Royal Canadian Air Cadet, rising to the rank of Flt.Sgt., receiving the Duke of Edinburgh Award and the Sir William Bishop Award, and qualifying as a pilot on 25 August 1985.
In September 1985, I became a student of the Faculty of Mathematics at the University of Waterloo in Ontario, Canada. On 26 October 1991, I received the Bachelor of Mathematics | Honours Joint Combinatorics and Optimization/Computer Science Co-operative Program, having specialized in compilers and computer architecture.
From January 1986 until August 1990, I held a number of jobs which were relevant to my education and career:
1986 Software Engineer at the Canada Land Data System, Environment Canada, Ottawa. 1986 Programmer-Analyst at the Canadian Imperial Bank of Commerce, Toronto. 1987-1988 Software Engineer at Waterloo Microsystems, Waterloo. 1988 Software Engineer at Rockwell, California. 1988-1989 Team Software Engineer at the Computer Systems Group, University of Wa-
terloo, Waterloo.
1989-1990 Compiler Engineer at Microsoft Corporation, Redmond, Washington. 1990 Instruction Set Architect at the Digital Engineering Department, Eindhoven Univer-
sity, Eindhoven. After a practical-oriented education at Waterloo and su cient work experience, I applied and was accepted to work towards a Ph.D in one of the best-known computing
375

science faculties: at the Eindhoven University of Technology. From September 1991 until August 1995, I worked with Prof.Dr. F.E.J. Kruseman Aretz, Dr.Ir. Kees Hemerik and Dr.Ir. Gerard Zwaan on developing the taxonomies that would become the cornerstones of this dissertation.
While the development and benchmarking of algorithm implementations began as late night hobbies, they would later become the second and third parts of this thesis research. The toolkits have also become highly popular amongst software engineers and researchers worldwide.
During my four years of study for a Ph.D, I was fortunate enough to travel to a number of other universities and institutes to report on various aspects of my research:
Sept. 1991 The EuroMicro Symposium in Vienna, Austria | Compiling for a high-level
computer architecture.
Nov. 1993 The Computing Science Netherlands Symposium, Utrecht, The Netherlands
| A taxonomy of keyword pattern matching algorithms (Best Paper Award).
May 1993 The University of Waterloo, Canada | A taxonomy of keyword pattern match-
ing algorithms.
Sept. 1994 The University of Waterloo, Canada | Algorithms for minimizing determin-
istic nite automata.
Oct. 1994 The University of Pretoria, South Africa | Object-oriented compiler construc-
tion and Taxonomies and the mathematics of program construction.
Feb. 1995 Simon Fraser University, Canada | The minimization of deterministic nite
automata.
Feb. 1995 The University of Victoria, Canada | A new string pattern matching algo-
rithm.
May 1995 The SAICSIT 95 Symposium on Research and Development, Pretoria, South
Africa | (Invited Keynote Speaker) Trends in compiler construction and (Tutorial) Taxonomies and toolkits: uses for the mathematics of program construction.
May 1995 The University of Cape Town, South Africa | A new regular expression pat-
tern matching algorithm.
Sept. 1995 Universitat Munchen, Germany | Class library implementation methods.
Bruce W. Watson, 27 July 1995 e-mail: watson@win.tue.nl

