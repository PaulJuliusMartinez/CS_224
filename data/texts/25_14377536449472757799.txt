2 Margin Distribution and Soft Margin
John Shawe-Taylor
Department of Computer Science Royal Holloway, University of London Egham, Surrey TW20 0EX, UK j.shawe-taylor@dcs.rhbnc.ac.uk http://www.cs.rhbnc.ac.uk/people/sta /shawe-taylor.shtml
Nello Cristianini
Department of Engineering Mathematics, University of Bristol Queen's Building, University Walk, Bristol BS8 1TR, UK nello.cristianini@bristol.ac.uk http://zeus.bris.ac.uk/ ennc/nello.html Typical bounds on generalization of Support Vector Machines are based on the minimum distance between training examples and the separating hyperplane. There has been some debate as to whether a more robust function of the margin distribution could provide generalization bounds. Freund and Schapire (1998) have shown how a di erent function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as to give an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. Furthermore, we show that the approach can be viewed as a change of kernel and that the algorithms arising from the approach are exactly those originally proposed by Cortes and Vapnik (1995). Finally, we discuss the relations of this approach with other techniques, such as regularization and shrinkage methods1.
1. Parts of this work have appeared in Shawe-Taylor and Cristianini (1999b,a)
Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

6 Margin Distribution and Soft Margin

2.1 Introduction

non-separable data margin distribution

The presence of noise in the data introduces a trade-o in every learning problem: complex hypotheses can be very accurate on the training set, but have worse predictive power than simpler and slightly inaccurate hypotheses. Hence the right balance between accuracy and simplicity of a hypothesis needs to be sought and this is usually attained by minimizing a cost function formed of two parts, one describing the complexity of the hypothesis, the other measuring its training error. In the case of linear functions this leads to an additional di culty as the problem of minimising the number of training errors is computationally infeasible if we parametrize the problem in terms of the dimension of the inputs (Arora et al., 1997). We avoid this apparent impasse by bounding the generalization in terms of a di erent function of the training set performance, namely one based on the distribution of margin values, but not directly involving training error. We will show in this paper that minimising this new criterion can be performed e ciently.
When considering large margin classi ers, where the complexity of a hypothesis is measured by its margin with respect to the data, the presence of noise can lead to further problems, for example datasets may be non-separable, and hence their margin would be negative, making application of the non-agnostic result impossible. Moreover solutions found by maximizing the margin are not stable with respect to the training points { slight modi cations in the training set can signi cantly change the hypothesis { a brittleness which makes the maximal margin solution somehow undesirable. These problems have led to the technique of the \soft-margin", a procedure aimed at extending the large margin algorithms to the noisy case by permitting a trade-o between accuracy and margin.
Despite successes in extending this style of analysis to the agnostic case (Bartlett,
1998) (see (??) in this book) and applying it to neural networks (Bartlett, 1998),
boosting algorithms (Schapire et al., 1998) and Bayesian algorithms (Cristianini et al., 1998), there has been concern that the measure of the distribution of margin values attained by the training set is largely ignored in a bound in terms of its minimal value. Intuitively, there appeared to be something lost in a bound that depended so critically on the positions of possibly a small proportion of the training set.
Though more robust algorithms have been introduced, the problem of robust bounds has remained open until recently. Freund and Schapire (1998) showed that for on-line learning a measure of the margin distribution can be used to give mistake bounds for a perceptron algorithm, and a bound on the expected error. Following a similar technique, in this paper we provide theoretical pac bounds on generalization using a more general function of the margin distribution achieved on the training set; we show that this technique can be viewed as a change of kernel and that algorithms arising from the approach correspond exactly to those originally proposed by Cortes and Vapnik (1995) as techniques for agnostic learning. Finally, we will show that the algorithms obtained in this way are intimately related to

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

2.2 Margin Distribution Bound on Generalization

7

certain techniques, usually derived in the framework of regularization or of Bayesian analysis and hence this work can be used to provide a learning-theoretic justi cation for such techniques.
Note that this style of analysis can also be used to transfer other hard margin results into a soft margin setting, and furthermore it can be extended to cover the nonlinear and regression cases (Shawe-Taylor and Cristianini, 1998).

2.2 Margin Distribution Bound on Generalization

fat shattering dimension

We consider learning from examples of a binary classi cation. We denote the domain

soefqtuheenpcreoibsletmypbicyalXlyadnednoatseedqubeynzce=of((ixn1p;uyt1s)b; y: :x:;=(x(mx;1y;m: :):);

xm) 2 2 (X

Xm. A training f 1; 1g)m and

the set of training examples by errors of the function f on the

Sse.qBuyenEcerzz(f.

)

we

denote

the

number of classi

cation

As we will typically be classifying by thresholding real valued functions we

introduce the notation T (f) to denote the function giving output 1 if f has output

greater than or equal to and 1 otherwise. For a class of real-valued functions H

the class T (H) is the set of derived classi cation functions.

De nition 2.1 Let H be a set of real valued functions. We say that a set of points X is -shattered by H if there are real numbers rx indexed by x 2 X such that for all binary vectors b indexed by X, there is a function fb 2 H satisfying fb(x) rx + , if bx = 1 and
fb(x) rx , otherwise.

The relevance of the fat shattering dimension and margin for learning is illustrated in the following theorem which bounds the generalization error in terms of the fat shattering dimension of the underlying function class measured at a scale proportional to the margin.

Theorem 2.1 (Shawe-Taylor et al., 1998) Consider a real valued function class H having fatshattering dimension bounded above by the function fat : R ! N which is continuous from the right. Fix 2 R. Then with probability at least 1
a learner who correctly classi es m independently generated examples S with h = T (f) 2 T (H) such that = mini yi(f(xi) ) > 0 will have the error of h bounded from above by

(m; k;

)=

2 m

k log2

8em k

log2(32m) + log2

8m

;

where k = fat( =8) em.

The rst bound on the fat shattering dimension of bounded linear functions in a nite dimensional space was obtained by Shawe-Taylor et al. (1998). Gurvits (1997) generalised this to in nite dimensional Banach spaces. We will quote an improved

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

8 Margin Distribution and Soft Margin

version of this bound for inner product spaces which is contained in (Bartlett and Shawe-Taylor, 1999) (slightly adapted here for an arbitrary bound on the norm of the linear operators).

Theorem 2.2 Fat shattering of linear functions

(Bartlett and Shawe-Taylor, 1999) Consider a Hilbert space and the class of linear

functions L of norm less than or equal to B restricted to the sphere of radius R

about the origin. Then the fat shattering dimension of L can be bounded by

fatL( )

BR

2
:

map to a separation space

We rst summarise results from Shawe-Taylor and Cristianini (1999b). Let X be an inner product space. We de ne the following inner product space derived from X.

De nition 2.2

Let Lf (X) be the set of real valued functions f on X with countable support

supp(f) (that is functions in Lf(X) are non-zero for only countably many points)

for which kfk2 =

tXhe sumfo(xf )t2he

squared

values

x2supp(f)

converges. hf; gi =

WXe de

ne the inner f(x)g(x):

product

of

two

functions

f; g

2 Lf (X),

by

x2supp(f)

Note that the sum which de nes the inner product can be shown to converge by using the Cauchy-Schwartz inequality on the di erence of partial sums and hence showing that the partial sums form a Cauchy sequence. Clearly the space is closed under addition and multiplication by scalars.
Now for any xed > 0 we de ne an embedding of X into the inner product space X Lf(X) as follows: : x 7! (x; x), where x 2 Lf (X) is de ned by x(y) = 1, if y = x and 0, otherwise. Embedding the input space X into X Lf(X) maps the training data into a space where it can be separated by a large margin classi er and hence we can apply Theorem 2.1. The cost of performing this separation appears in the norm of the linear operator acting in Lf (X) which forces the required margin. The following de nition speci es the amount by which a training point has to be adjusted to reach the desired margin .
For a linear classi er (u; b) on X and margin 2 R we de ne

d((x; y); (u; b); ) = maxf0; y(hu; x i b)g:

This quantity is the amount by which (u; b) fails to reach the margin on the point
(x; y) or 0 if its margin is larger than . For a misclassi ed point (x; y) we will have
d((x; y); (u; b); ) > , and so misclassi cation is viewed as a worse margin error, but is not distinguished into a separate category. We now augment (u; b) to the

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

2.2 Margin Distribution Bound on Generalization

9

lu^in=ea0@r fuu;nc1tionXal

d((x; y); (u; b);

1 )y xA :

(x;y)2S

in the space X Lf(X). The action of the additional component is exactly enough

to ensure that those training points that failed to reach margin in the input

space now do so in the augmented space. The cost of the additional component

is in its e ect of increasing the square of the norm of the linear functional by

D(S; (u; b); D(S; (u; b);

)2= )=

s2,

where
X d((x;

y);

(u;

b);

)2:

(x;y)2S

At the same time the norm of the training points has been increased by the additional component x. Taking both these adjustments into account and verifying that the o -training set performance of the augmented classi er matches exactly the original linear function gives the following theorem as a consequence of Theorems 2.1 and 2.2.

Theorem 2.3 bound for a xed (Shawe-Taylor and Cristianini, 1999b) Fix > 0, b 2 R. Consider a xed but
map unknown probability distribution on the input space X with support in the ball of radius R about the origin. Then with probability 1 over randomly drawn
training sets S of size m for all > 0 the generalization of a linear classi er u on X with kuk = 1, thresholded at b is bounded by

(m; h;

)=

2 m

h log2

8em h

log2(32m) + log2

8m

where

h=

64:5(R2 +

2)(1 + D(S; (u; b); )2=
2

2)

;

;

provided m 2= , h em and there is no discrete probability on misclassi ed training points.

Note that unlike Theorem 2.1 the theorem does not require that the linear clas-

si er (u; b) correctly classi es the training data. Misclassi ed points will contribute

more to the quantity D(S; (u; b); ), but will not change the structure of the result.

This contrasts with their e ect on Theorem 2.1 where resorting to the agnostic

version introduces a square root into the expression for the generalization error.

In practice we wish to choose the parameter in response to the data in order to

minimize the resulting bound. In order to obtain a bound which holds for di erent

values of it will be necessary to apply the Theorem 2.3 several times for a

nite subset of values. Note that the minimum of the expression for h (ignoring

the=copnRstDan.tTahneddissucpreptreessseint gofthvae ludeesnommuisntabtoercho2 s)enis

(R + D)2 attained when to ensure that we can get

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

10 Margin Distribution and Soft Margin

a good approximation to this optimal value. The solution is to choose a geometric sequence of values { see Shawe-Taylor and Cristianini (1999b) for details.

Theorem 2.4 (Shawe-Taylor and Cristianini, 1999b) Fix b 2 R. Consider a xed but unknown
bound for optimal probability distribution on the input space X with support in the ball of radius R map about the origin. Then with probability 1 over randomly drawn training sets
S of size m for all > 0 such that d((x; y); (u; b); ) = 0, for some (x; y) 2 S, the generalization of a linear classi er u on X satisfying kuk 1 is bounded by

(m; h;

)=

2 m

h log2

8em h

log2(32m) + log2

2m(28 + log2(m))

;

where

h=

65 (R + D)2 + 2:25RD]
2

;

for D = D(S; (u; b); ), and provided m maxf2= ; 6g, h em and there is no
discrete probability on misclassi ed training points.

As discussed above the bound can be used for classi ers that misclassify some training points. The e ect of misclassi ed points will only be felt in the value of D. Such points do not change the form of the expression. This is in contrast with traditional agnostic bounds which involve the square root of the ratio of the fat
shattering dimension and sample size (see for example expression (??) in this book).
If a point is an extreme outlier, it is possible that its e ect on D might be such that the bound will be worse than that obtained using the agnostic approach (where the `size' of misclassi cation is irrelevant). However, it is likely that in usual situations the bound given here will be signi cantly tighter than the standard agnostic one. The other advantage of the new bound will be discussed in the next section where we show that in contrast to the computational di culty of minimizing the number of misclassi cations, there exists an e cient algorithm for optimizing the value of h given in Theorem 2.4.

2.3 An Explanation for the Soft Margin Algorithm
The theory developed in the previous section provides a way to transform a non linearly separable problem into a separable one by mapping the data to a higher dimensional space, a technique that can be viewed as using a kernel in a similar way to Support Vector Machines.
Is it possible to give an e ective algorithm for learning a large margin hyperplane in this augmented space? This would automatically give an algorithm for choosing the hyperplane and value of , which result in a margin distribution in the original space for which the bound of Theorem 2.4 is minimal. It turns out that not only is the answer yes, but also that such an algorithm already exists.

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

2.3 An Explanation for the Soft Margin Algorithm

11

The mapping de ned in the previous section implicitly de nes a kernel as separation kernels follows:

soft margin

k(x; x0) = h (x); (x0) i = h(x; x); (x0; x0) i = hx; x0 i + 2h x; x0 i = hx; x0 i + 2 x(x0)

By using these kernels, the decision function of a SV machine would be:

f(x) = Xm iyik(x; xi) + b = Xi=m1 iyi hx; xi i + 2 x(xi) + b
i=1

and the Lagrange multipliers i would be obtained by solving the Quadratic

Programming problem of minimizing in the positive quadrant the dual objective

function:

L = Xm i = Xi=m1 i = Xim=1 i = Xim=1 i
i=1

1 2 1 2 1 2 1 2

Xm iX;jm=1 iX;jm=1 iX;jm=1
i;j=1

yiyj yiyj yiyj yiyj

i i i i

jk(xi; xj) j hxi; xj i + jhxi; xj i jhxi; xj i

2 i(j)]

2 2

1 2 1 2

Xm Xi;mj=1
i=1

yiyj
2i

i

j i(j)

This is exacly the dual QP problem that one would obtain by solving the soft margin problem in one of the cases stated in the appendix of Cortes and Vapnik (1995):
minimize : 12hu; u i + C X i2

subjectto : yj hu; xj i b] 1 j
i0

The solution they obtain is:

L=X i

X yiyj i jhxi; xji

1X
4C

i2

which makes clear how the trade o parameter C in their formulation is related to

the kernel parameter .

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

12 Margin Distribution and Soft Margin

2.4 Related Techniques

covariance of augmented data equivalent techniques

Another way of looking at this technique is that optimizing the soft margin, or enlarging the margin distribution, is equivalent to replacing the covariance matrix K with the covariance K0

K0 = K + I

which has a heavier diagonal. Again, there is a simple relationship between the trade o parameter and the and C of the previous formulations. So rather than using a soft margin algorithm, one can use a (simpler) hard margin algorithm after adding I to the covariance matrix. This approach has also been considered by Smola and Scholkopf (1998) for the regression case where they also introduce an upper bound on the size of the 's in order to improve robustness to outliers.
Figure 2.4 shows the results of experiments performed on the ionosphere data of the UCI repository (Merz and Murphy, 1998). The plot is of the generalization error for di erent values of the parameter .
This technique is well known in classical statistics, where it is sometimes called the \shrinkage method" (see Ripley (1996)). Basically, in Bayesian discrimination
(see section ??) it suggests replacing the empirical covariance function with some
function closer to the identity I, by choosing an element of the line joining them (1 ) + I. A redundant degree of freedom is then removed, leaving with the new covariance + I. In the case of linear regression this technique, known as ridge regression, can be derived from assuming Gaussian noise on the target values. It was originally motivated by the trade o between bias and variance (Hoerl and Kennard, 1970) and leads to a form of weight decay. This approach is equivalent to a form of regularization in the sense of Tikhonov. The theory of ill-posed problems was developed by Tikhonov in the context of solving inverse problems (Tikhonov and Arsenin, 1977). Smola and Scholkopf (1998) derived ridge regression using dual variables and for example C. Saunders (1998) have applied this to benchmark problems. It is well known that one can perform regularization by replacing the covariance matrix XT X with XT X + I, and learning machines based on Gaussian Processes implicitly exploit this fact in addition to the choice of kernel.
Another explanation proposed for the same technique is that it reduces the number of e ective free parameters, as measured by the trace of K. Note nally that from an algorithmical point of view these kernels still give a positive de nite matrix, and a better conditioned problem than the hard margin case, since the eigenvalues are all increased by . The so-called box constraint algorithm which minimises the 1-norm of the slack variables is not directly comparable with the 2-norm case considered here.

Remark 2.1
Note that

RqX i2 = RD =

2=

=

1 4C

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

2.4 Related Techniques
0.14

13

0.12

0.1

0.08

0.06

0.04

0.02 0123456
Figure 2.1 Generalization error as a function of , in a hard margin problem with augmented covariance K0 = K + I, for ionosphere data. so a choice of in the margin distribution bound controls the parameter C in the soft margin setting, and the trade-o parameter in the regularization setting. A reasonable choice of can be one that minimizes some VC bound on the capacity, for example maximising the margin in the augmented space, or controlling other parameters (margin; eigenvalues; radius; etc). Note also that this formulation also makes intuitive sense: a small corresponds to a small and to a large C: little noise is assumed, and so there is little need for regularization; vice versa a large corresponds to a large and a small C, which corresponds to assuming a high level of noise. Similar reasoning leads to similar relations in the regression case.
2.5 Conclusion

Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

14 Margin Distribution and Soft Margin
The analysis we have presented provides a principled way to deal with noisy data in large margin classi ers, and justi es the like the soft margin algorithm as originally proposed by Cortes and Vapnik. We have proved that one such algorithm exactly minimizes the bound on generalization provided by our margin distribution analysis, and is equivalent to using an augmented version of the kernel. Many techniques developed for the hard margin case can then be extended to the soft-margin case, as long as the quantities they use can be measured in terms of the modi ed kernel (margin, radius of the ball, eigenvalues).
The algorithms obtained in this way are strongly related to regularization techniques, and other methods developed in di erent frameworks in order to deal with noise. Computationally, the algorithm can be more stable and better conditioned than the standard maximal margin approach.
Finally, the same proof technique can also be used to produce analogous bounds for nonlinear functions in the classi cation case, and for the linear and nonlinear regression case with di erent losses, as reported in the full paper (Shawe-Taylor and Cristianini, 1998).
Acknowledgements
This work was supported by the European Commission under the Working Group Nr. 27150 (NeuroCOLT2) and by the UK EPSRC funding council. The authors would like to thank Colin Campbell and Bernhard Scholkopf for useful discussions. They would also like to thank useful comments from an anonymous referee that helped to re ne De nition 2.2.
Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

References
S. Arora, L. Babai, J. Stern, and Z. Sweedyk. Hardness of approximate optima in lattices, codes, and linear systems. Journal of Computer and System Sciences, 54(2):317{331, 1997.
P. Bartlett and J. Shawe-Taylor. Generalization performance of support vector machines and other pattern classi ers. In B. Scholkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods | Support Vector Learning, pages 43{54, Cambridge, MA, 1999. MIT Press.
P. L. Bartlett. The sample complexity of pattern classi cation with neural networks: the size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525{536, 1998.
V. Vovk C. Saunders, A. Gammermann. Ridge regression learning algorithm in dual variables. In J. Shavlik, editor, Machine Learning Proceedings of the Fifteenth International Conference(ICML '98), San Francisco, CA, 1998. Morgan Kaufmann.
C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273 { 297, 1995.
N. Cristianini, J. Shawe-Taylor, and P. Sykacek. Bayesian classi ers are large margin hyperplanes in a hilbert space. In J. Shavlik, editor, Machine Learning: Proceedings of the Fifteenth International Conference, San Francisco, CA, 1998. Morgan Kaufmann.
Y. Freund and R.E. Schapire. Large margin classi cation using the perceptron algorithm. In J. Shavlik, editor, Machine Learning: Proceedings of the Fifteenth International Conference, San Francisco, CA, 1998. Morgan Kaufmann.
L. Gurvits. A note on a scale-sensitive dimension of linear bounded functionals in banach spaces. In Proceedings of Algorithm Learning Theory, ALT-97, pages 352{363. Springer Verlag, 1997.
A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55{67, 1970.
C. J. Merz and P. M. Murphy. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/ mlearn/MLRepository.html].Irvine, CA: University of California, Department of Information and Computer Science.
B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University
Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

16 REFERENCES
Press, Cambridge, 1996. R. Schapire, Y. Freund, P. Bartlett, and W. Sun Lee. Boosting the margin: A new
explanation for the e ectiveness of voting methods. Annals of Statistics, 1998. (To appear. An earlier version appeared in: D.H. Fisher, Jr. (ed.), Proceedings ICML97, Morgan Kaufmann.). J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44(5):1926{1940, 1998. J. Shawe-Taylor and N. Cristianini. Further results on the margin distribution. In Proceedings of the Twelth Annual Conference on Computational Learning Theory, COLT'99, 1999a. J. Shawe-Taylor and N. Cristianini. Margin distribution bounds on generalization. In Proceedings of the European Conference on Computational Learning Theory, EuroCOLT'99, pages 263{273, 1999b. J. Shawe-Taylor and Nello Cristianini. Robust bounds on generalization from the margin distribution. NeuroCOLT Technical Report NC-TR-1998-020, ESPRIT NeuroCOLT2 Working Group, http://www.neurocolt.com, 1998. A. Smola and B. Scholkopf. On a kernel-based method for pattern recognition, regression, approximation and operator inversion. Algorithmica, 22:211 { 231, 1998. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. W. H. Winston, Washington, D.C., 1977.
Smola, Bartlett, Scholkopf, and Schuurmans: Advances in Large Margin Classi ers 1999/10/06 12:53

