List Structure" Measurements, Algorithms,
and Encodings
Douglas W. Clark
Department of Computer Science Carnegie-Mellon University Pittsburgh, Pa. 15213 August 1.976
This work was done while the author was supported by a grant from the Xerox Corporation Palo Alto Research Center. Partial. support was provided by the Defense Advanced Research ProJects Agency under contract F44620-73-C-O074, monitored by the Air Force Office of Scientific Research.

Copyright (_) 1976 by Douglas W. Clark

ABSTRACT

This thesis is about list structures: how they are used tn practice, how they

can be moved .and copied efficiently, and howthey can .be represented by space-

saving encodings. The approach taken to these subjects is mainly empirical.

Measurement

results are based on five large pl_ograms written in Interlisp, a

sophisticated Lisp system that runs on the PDP-l O.

Static data were collected at the end of typical runs of the programs, and all list structure used as data by them was measured (about 50,000 cells each). Strong regularities were discovered. In each program, about one-third of all cars pointed to lists, the rest mainly to literal atoms and small integers; roughly three-fourths of cdrs pointed to lists, the rest mainly to the atom NIL. List pointers generally pointed to a location physically nearby .in memory, a condition that appears to depend only on the sequential allocation of new list cells. Atom pointers were distributed approximately according to Zipf's law, which models word occurrence tn natural ]anguage text.

Less agreement was found among the programs when dynamic references to list structure were considered. During short runs of the programs, most of these references were due to the functions car and cdr, which occurred about equally

often. Dynamic data type distributions only roughly corresponded to the static ones and were different from program to program= dynamic list pointer distances

tended to be small, but did not closely match the static resultsl and the

distributions of dynamic atom references were much more skewed than Zlpf"s law

" would predict. Further measurements showed that repeated reference to cells !s

likely: half or more of all references referenced cells.

were to one of the ten most recently

lii

Iv ABSTRACT

Linearizationis

the rearrangement in memory of a list structure in such a way

that consecutive cdrs (or, alternatively, cars) of a list are allocated sequential cells

whenever possible. Two algorithms for creating a !lnearlzed copy of a list are

presented; the first destroys the original list during processing, the second does

not. Neither algorithm uses mark bits, and neither needs more than a constant

amount of working storage. Both are significantly faster than the best previous

(linear-time)

algorithms for these problems.

The static and dynamic effects of linearizing all lists in the five programs are

studied. Lineartzation,

of course, greatly increases the static locality of list

structure, successfully placing over 98 percent of each program's ltst cdrs In the

following cell in memory. This locality deteriorates slowly during subsequent

program execution.

Page-faults due to list-cell references are reduced only

slightly if execution

is preceded by linearization,

but this small effect is

magnified by the large cost of a fault.

The tendency of pointers to have predictable values can be exploited In the

design of space-saving compact encodings of list structure. Examples of such

encodings are presented, and various means of dealing with escapes--pointers that cannot be directly accomodated by a particular encoding---are discussed. For

one of the programs a design is presented that achieves a one-third reduction in

space over the straightforward

Interlisp

mechanisms

and occaisional linearization

implementation.

More complex escape

would yield further savings. A lower

bound on the bit requirements of all possible encodings is given by the entropy of a list cell, which is calculated for the five programs to be 10 to 15 bits before

linearization, 7 to 1_ bits after.

ACKNOWLEDGMENTS

Thanks, first, to my thesis adviser, Sam Fuller, for his enlightened supervision

of this work, and to the other members of my committee, Dan Bobrow, Butler

Lampson, and Raj Reddy, for their help and for their service. The ideas and

suggestions

of Dan Bobrow have been particularly important. The work of

Chapter 2, together with parts of Chapter/5 (Section g) and Chapter 6 (Section 6),

was done jointly with Cordell Green, and is .also reported in our paper, "An ..

Empirical Study of List Strcct ure in Lisp". [Cla76b]; I appreciate very much his

contributions

to this thesis. Thanks to the authors of the five Interlisp programs

studied here for submitting their programs.to mine for measurement: Lyn Bates, Peter Deutsch, Butler Lampson, Larry Mas.inter, and Earl Sacerdott. Lyn Bates has been especially helpful with her program, SPARSER. Measurements of these

l_rograms during execution were made with a modified version of a PDP-10 simulator developed by Rusty Bobrow. Iam grateful to him for helping me with the simulator and for carefully reviewing [Cla76b]. The algorithms of Chapter 4 have profited from the comments of Guy Alines, Ellis Cohen, and Peter Deutsch.

Alice Hartley was of great help during the writing and implementation of the listlinearizing garbage collector described in Chapter 5. The appearance of this document is largely the work of Brian Reid and David Lamb, for whose capable assistance I am grateful. Thanks, finally, to Th_r_se Flaherty for numerous helpful suggestions, especially on Chapter 6, and for her generous support throughout the course of this work.

CONTENTS

1. Introduction 1. Introduction

and Motivation

2. Terminology and Notation 3. Description of Programs 4. Synopsis of Thesis

2. Static Measurements of List Structure 1. Introduction 2. Data Types 3. List Pointers 4. Effect of the Cons Algorithm 5. Atom Pointers 6. Number Pointers

3. Dynamic Measurements 1. Introduction

of List Structure

1.1. Data collection technique

1.2. Comparisons with static measurements

2. Data Types 3. List Pointers

4. Atom Pointers

5. Primitive Operations on List Cells
....
6. I/placa and Bplacd

vii

1 2 2 5 8
11 1g 13 15 17 2.1 2.3
2.7 2.8 2.8 2.9 30 31 34 38 41

viii CONTENTS

7. Patterns of List Cell Reference

7.1. Inter-reference 7,2. Stack distances

distances

7.3. References to list pages

4. Linearization Algorithms 1. Introduction

2. An Efficient List-Moving 2.1. Introduction

Algorithm Using Constant Workspace

2.2. An example

2.3. The algorithm 2.4. Improvements,

variations, and extensions

3. Copying List Structures 3.1. Introduction

Without Auxiliary Storage

3.2. Copying a binary tree

3,3. Copying an (almost) arbitrary list

3.4. Copying arbitrary lists

3.5. Comparison 3.6. Conclusion

with Fisher's algorithm

5. Effects of Linearization 1. Introduction 2. Static Effects 2.1. Pointer distances 2.2. Pointers on the page 3. Minsky's Linearization Algorithm 3.1, Why another algorithm? 3.2. Description of the algorithm 3.3, Implementation of the algorithm

.

43 44 46 47
51 52 54 54 56 58 60 61 61 62 66 74 78 83
85 86 /}7 87 93 95 95 96 97

CONTENTS
4. Persistence of Lineartty 5. Effects of Linearization on Paging

6. Compact Encodings 1. Introduction

of List Structure

2. A Simple Example 3. Linearization

4. Escape Mechanisms 4.1. Next-word escapes

4.2. Indirect relative pointers

4.3. Indirect tables

4.4. Hash linking

4.5. Summary

5. A Better Encoding for SPARSER

6. Entropy of Pointers 7. Costs and Implementation

Questions

7.1. Frequency of escape 7.2. Impact on Lisp primitives

7.3. Atom frequencies

7. Conclusion 1. Summary of Results 2. Topics for Further Work 2.1. Measurements 2.2. Algorithms 2.3. Encodings

Ix
99 103
109 110 111 115 117 118 119 120 121 12_ 123 129 134 136 136 138
139 140 143 143 144 145

x
Appendices

CONTENTS

A. List Pointer Distances

B. Rplaca and Rplacd

C. Memory References in Algorithm III and Fisher's Algorithm
1. Analysis of Algorithm III
2. Fisher's Algorithm 3. Analysis of Fisher's Algorithm

147
153
157
157
159 160

Bibliography

163

Introduction
Chapter 1 introduces the subjects and objectives of this work. Much of this thesis deals with measurements of list structure in five large Lisp programs; these programs are briefly d.escribed. The small amount of Lisp terminology and notation used throughoUt is explained. A synopsis of the remaining chaptersisgiven.

2

INTRODUCTION

CHAP. 1

1. Introduction

and Motivation

This thesis is about list structures: how they are used in practice, how they

can be moved and copied efficiently, and how they can be represented by space-

saving encodings. The approach taken to these subjects is mainly empirical. structures in several large Lisp programs are studied, and experiments

List are

conducted on them to see how time and especially space efficiency might be

improved. Such improvement is the ultimate motivation of this work.

A low-level point of view will be taken throughout. Thus we will not be concerned with what list structures mean to the programs that build and use

them, nor with high-level functions that operate on lists, nor in fact with any condition or event that does not have meaning to all Lisp programs, however

unusual or primitive. the hardware level.

We will concentrate instead on what happens just above Topics of interest include various observable regularities

among individual pointers, execution frequencies of the most primitive listaccessing Lisp functions, and algorithms for copying arbitrary list structures without using a stack.

What follows is a brief explanation of the Lisp terminology used in this

thesis. Then the programs chosen for empirical study are described. remainder of the thesis is outlined.

Finally the

2. Terminology and Notation
Appreciation of this thesis depends not at all on an understanding of Lisp; only the most rudimentary acquaintance with list structure is necessary. Most of the Lisp terminology to be used is illustrated in Figure 1-1. (For more information, see McCarthy [McC62,], Weissman [Wei67], and Teitelman [Tei74].)

SEC 2

TERMINOLOGYAND NOTATION

3

Figure 1--l(a) shows tI_at a li,.t cell is a pair ot pcdnters called car and cdr. By

conv_,ntion,

car is al_,a,y_ shown _n the left, and cdr on the right. Car and cdr

fields each. contain a pointer to another list cell or to som,:_ nonlist object, possibly

an atom. Car may be thought of asa pointer to the contents of acell, and cdr asa

link to the next cell in a list. A special atom, NIL, is used as a list terminator

(in

cdr) and isoquivalent

to, the; empty list. Thus Figure 1 el(b) isa representation

of

the two-atom

list (AB). Throughout

this thems, pointers to atoms will be

r_-,preaented

by the names of the atoms, as in Figure 1-1(c); this makes for less

cluttered

figures.

In addition, the term "atom" will hereafter mean "literal,.---i,e.,

non-numeric_atom

other th/_ll Nil.."

Lisp allows point_::rs to 11enlist obj_,cts other than atoms, depending

on the

implementation.

Pointers to objects such as int,,gers, Iloating-point

numbers,

strings, arrays, and so on, lnight appear in car and cdr.

Arbitrarily

complex ]i._t ._trllclures may be constructed

in Lisp. Figure l-l(d)

shows a structurecontainin,{circ_larityand

shared substructures.

Programs and

data can both t,r, r_pre_.:;on_,,d a:; lists in I,i.<;p [McC_;Z], but the empirical

this thesis x_eill concentrate

o1_ li:,t structures usc, d a._;data,

work of

All maniplllatlons4

of li-:;t ;lructure in l,lsp _J,,, _,:complJshed

by five primitive

fl_nctions.

The fnnctior,.:_ar[x]andcdr[x]

rea,:i _,i_,,carand cdr fields era li.;t cell

x. (Ileroaft._,r ilalic_,: will be u_-:_,.I!__,distinguist_ t,:._ _,r_-r,1_tho functions car and cd.r
and the fiol,ts car an4 r:,tr. TI,_ ;: "Five cars _+:_,| thror_, c4rs point to atoms," and

"After ninety executions

of (:dr...") Existing f_qds can be changed by means of

the fllnctions

rplaca and rplacd: rplaca[x;y]

will wrlt_ the value y in the car

field of cell x; rplacd[x;y]

will do the same for cdr. The function cons creates

now list cells: cons[x;y]

will obtain an unoccupl,.d

write x in its car, and write y in its cdr.

r'oll from the storage allocator,

CAR CDR

list pointer

atom pointer

(b) _ atom
A

l-
atom
B

\, atom
NIL

(_) _ AI ,---F--_BL/]
J, I
(d) _/
ll-I-I <JFL/_ -
Figure 1-1. Lisp terminology and notation: (a) a list cell; (b) detailed representation of a short list; (c) abbreviated representation of the same; (d) a list structure with sharing and a cycle.

SEC. g

TERMINOLOGY AND NOTATION

5

The Lisp implementation

used for all of the work reported in this thesis is

Interlisp-10

[Tei74], a successor of BBN-IAsp [Bob67] that runs on the DEC

PDP-10 computer under the Tenex operating system [Bob7fi]. (Interlisp is being

implemented

on other machines as well; hereafter, however, "Interlisp" will refer

exclusively

to the PDP-10 implementation.)

Tenex divides the PDP-10's 18-bit

virtual address space into 51 fi-word pages.

List cells in Interlisp occupy individual 36-bit words, and car and cdr are each full 18-bit virtual addresses, This implementation of pointers is the usual one for Lisp systems (e_g_, [Bar68, McC62, Moo74, Qua69]), and allows list cells to be distributed arbitrarily throughout memory.

Nonlist data types in Interlisp include literal atoms, numbers of various kinds, strings, arrays, user-defined types, and others. Objects of only a single type reside o11 each page; this makes for fast type-checking.

3. Description of Programs

The empirical sections of this thesis deal with five Interlisp programs.

S_,v_:,ral criteria were "os_cl in _;electiI_g _hese systems: it was desired that the

programs str_cture,

be large, that they have or be able to generate large amounts of list that they use their list structure in complex ways, and that they be

repre:_entative,

inzofar _z po,s,_'_ble, of" sophiztjcated l_st-processi_g

applications.

Most of the measureme__ts done here are fui_ctionz of the size o_ complexity of list

structure,

and it was hoped that the programs Chosen according to these criteria

would yield rough upper bounds for the larger population of all Lisp programs.

Using these criteria as guidelines, appropriate Lisp systems were informally

6

INTRODUCTION

CHAP. 1

requested

from research groups engaged in different applications.

Several

solicited programs were not used, either because they possessed too little list

structure,

or because they were too incompatible with the measurement

programs. The programs finally selected are described below.

NOAH, written by Earl Sacerdoti of the Stanford Research Institute, is a

planning program that produces a plan which is represented as a procedural net

[Sac75]. The planning domain is the assembly of an air-compressor.

NOAH is

written in QLISP [1-¢eb73], an extension of Interlisp that includes most current

artificial

intelligence

language features such as pattern-directed

invocation,

assertional data bases indexed by contexts, pattern matching, etc. The program

t_nds to use most of the very high-level language features. Much of the list

structure occurs a,s part of QI, ISP's discrimination net and as entries in this net.

These lists share all isomorphic substructures.

PIVOT, written by L. Peter Deutsch of the Xerox Palo Alto Research Center, is

a program verification

system [Deu73a]. It takes as input a program and a

description of the program, and produces a proof that the program satisfies the

description.

The main uses of list structures are to represent arithmetic and

logical expressions that describe programs, and to represent context trees of these

expressions that hold for alternate execution paths of the programs being verified.

Two interesting

a_pects of PIVOT are that expressions are represented with

maximal sharing of subexpressions,

and that the expressions are usually short

lists of length three or four.

SPAI_SER, written by Madeleine Bates of Bolt Beranek and Newman, Inc.

[Bat75], i.s the .synt_3ctic component of a speech understanding system. Its purpose

is to analyze syntactically structures for constituents

disjoint portions of an utterance, to provide syntactic of the partial utterance, and to make predictions for

SEC. ,3

DESCRIPTION OF PROGRAMS

7

syntactic classes of words that would be needed to complete the utterance. It uses

list structure ( 1) to represent a grammar of English in the augmented-transition-

network utterance,

formalism,

(?,) to represent

(3) to represent predictions

possible parse paths through a partial which form the interface with other

components

of the system, and (4) to describe tile structure of syntactic

constituents. processing.

Only the space for the first of these is fixed; the others grow during The parse paths involve much sharing of substructures.

CONGEN is a chemical structure generator written by Larry Masinter and N. S.

Sridharan [Smi74] as part of the Stanford Heuristic Programming Project. The

program takes as input a molecular formula and produces a list of all

nonisomorphic

graph structures that can be built out of those atoms without

violating valence constraints.

The most common list structures are connection-

table representations groups.

of labeled graphs and permutation-list

representations

of

WIRE is a wire-listing program written by Butler Lampson of the Xerox Palo

Alto Research Center. WIRE takes as input a symbolic description of the location

of integrated circuits on a wire-wrap board, along with a description of their

interconnections.

These data are represented internally as lists of signals, each a

collection of interconnected

pins. The program checks for various kinds of

interconnection

errors and rearranges the data into several appropriate output

formats, aimed variously

wire-wrap

machine.

at the logic designer, a human wire-wrapper,

and a

8

INTRODUCTION

CHAP. 1

4. Synopsis of Thesis

Chapters 2_ and 3 report the results of a series of measurements

of list

structure use in the five programs. Chapter 2 deals with the characteristics

of list

structures

as they sit in melnory at a single point in time, while Chapter 3 is

concerned

with tlle programs' use of their lists during execution.

Topics

addrezsed in Chapter 2_ include the frequency with which the various Interllsp

data types are pointed to in list structure, the likelihood that a list pointer points

to a memory location physically nearby, and the frequency distribution

of

pointers to atoms. In Chapter 3 the dynamic analogs of these topics are covered:

how often durinF_ execution are reference:_ made to pointers of a given data type, or to list pointers with a give_n degree of physical adjacency, or to particular

atolnz? Other subjects of Chapter 3 include the execution frequency of the

primitive

list-acce._Mng functions and the patterns of reference to individual

cells: once a cell is referenced, is it likely to be referenced again soon?

Many of the measurements of these chapters depend on the physical location of list cells in memory. Linearization is the process by which these locations are changed so that l i._t cdrs (or, alternatively, cars) point to the following cell in memory whenever possible. New fast algorithms that accomplish linearization ar_ presented in Chapter 4. Thezo algorithms operate in time proportional to the number of cells linearized, and u:;e working .storage of fixed size. They are faster than the best previous algorithm_ _'or the same problems.

The effects of linearization in real Lisp systems are the subjects of Chapter 5.

List-cell adjacency in the five programs following linearization

is reported.

Dynamic experiments

rely on an implementation

of MinsKy's linearization

algorithm [Min63], which is described in Chapter 5. These experiments include

SEC. 4

SYNOPS|S OF THESIS

9

an investigation

of the deterioration of linearityduring

program execution,

brief look at how linearization affects the paging behavior of programs.

and a

Chapter 6 proposes some techniques for list structure representations

that are

more space-efficient

than the simple one used by most Lisp systems, namely,

letting each pointer be a full memory address. These compact encodings of list

structure take advantage of the regularities observed in Chapter g by using small

pointers to hold frequently occurring values, and using full-size memory

addresses only when needed. Calculations of the information content or entropy

of pointers give lower bounds on possible space savings due to encodings of list

structure.

Results from Chapter 3 are used to evaluate the execution-speed

cost of

such schemes. The extent to which linearization improves the encodability of list

structure is also discussed.

Chapter 7 concludes the thesis by summarizing suggesting a number of topics for further work.

its principal results and

2
Static Measurements of List Structure

Static measurements

of the list structures of five large Lisp

programs are reported and analyzed in this chapter. These data reveal a

great deal of regularity, or predictability, among pointers. In the five programs, car pointers were found to point to lists a little under one-

third of the time, and to literal atoms andsmall integers most of the rest of the time. About one cdr in four was NIL, and the other three

were almost always list pointers. There was a merked tendency for a

list pointer to point to a location physically nearby in memory.

Pointers to atoms were distributed in about the same way as instances

of individual words in a large body of natural language text, a

phenomenon explanations

sometimes

known as Zipf's law. Some

for these empirical regularities are considered.

possible

11

12

STATICMEASUREMENTOSFLISTSTRUCTURE

CHAP.;2

1. Introduction

This chapter is concerned with various measurable characteristics of list structure as it sits in memory. Questions about the manipulation of lists by running programs are covered in Chapter 3; here we are concerned solely with static properties of list structures.

Each of the five programs discussed in Chapter 1 was run on a typical task

that was intensive enough to force list storage to be garbage-collected

and reused

many times. At the end of the run, a measurement program traversed all lists that were either the top-level binding of an atom or the property list of an atom,

recording various characteristics of each cell as it was found. The total number of

cells found in each program was 49593 in NOAH, 4?,958 in PIVOT, 48156 in

SPARSER, 65366 in CONGEN, and 5112.0 in WIRE. A separate count of all list cells

(including "garbage" cells) in CONGEN and WIRE revealed that this method found

87 percent and 94 percent, respectively, of all existing list structure in the two

programs.

Active (i.e., nongarbage) list cells not found by this method include those

pointed to only by one or more of the following: variables bound on the stack,

pointers contained in arrays, pointers within compiled or interpreted functions, and car or cdr of some other list cell not found. All programs except NOAH were

measured with an empty stack, and all five were, in the main, compiled rather

than interpreted.

Those functions that did execute interpretively,

and which

therefore were defined by list structures, were not examined by the measurement

program. Other function definitions in list form, such as those saved on atom

· property lists by the programs' authors during compilation, were examined, but

thore are probably few of these: This is consistent with the intent of this work to

measure list structure data, rather than programs.

SEC. 2

DATA TYPES

13

2. Data Types

It seems quite reasonable to suppose a priori that car and cdr pointers are used

in different ways by Lisp programs. Likewise, it seems reasonable that pointers to

different data types might display very different sorts of regularity.

These two

expectations

suggest that measurements be made separately for car and cdr, and

separately also for the various data types of Interlisp. This section reports the

static frequency of occurrence of the various data types in car and cdr for each of

the five programs.

,.

The frequency of occurrence of pointers to each data type is shown in Tables

2-1 and 2-2, for pointers in car and cdr respectivelY. Recall that the type ATOMS

does not include NIL or any numeric atoms, which are shown separately in the tables. The other Interlisp types shown are as follows: SMALL INTS are integers in

the ranp, e - 1536 to + 1536; LARGE INTS are all other integers; FLOATS are floating-

point numbers; STRINGS are character arrays of numbers and/or pointers.

strings; and ARRAYS are one-dimensional

These tables indicate a striking uniformity among the five programs studied.

Cdrs, by and large, point either to lists or to NIL, the former about three times as

often as the latter. This means that the average length of lists is four. Cars point

to lists a little under one-third of the time, and to literal atoms and small integers

most of the rest of the time. Atoms are rare in cdr; NIL is rare in car; and large

integers, floating-point

numbers, arrays, and strings are rare in both. (It should

be noted that the paucity of pointers to, for example, arrays does not mean that the

programs structure.)

use few arrays, only that arrays are rarely pointed to from within

list

1,1

STATICMEASUREMENTOSFLISTSTRUCTURE

CHAP. 2

Table 2-1 Percentage occurrence of data types in car

NOAH

],ISTS A'I'OM S Nil, SMAT,I, INTS LARGE INTS FI,OATS STRINGS ARRAYS ()TILERS

30.6 58.4
4.4 4.7 1.3
.5 .2 .0

PIVOT
29.0 58.1
1.6 4.1 6.4
.6 .1 -

SPARSER CONGEN WIRE

31.5 39.8
5.6 21.1
1.7 .0 ·2 .0

28.5 34.2__,
1.0 34.7
1.45 .0 .1 .0

23.3 39.0
3.1 33.0
1.4 .0 .3 .0

Note: In this and :_ucceeding tables, "-" means a negligible quantity, and ".0" means zero.

Table 2-2 Percentage occurrence of data types in cdr

NOAH

I,ISTS ATOMS NIL SMALL INTS I,ARGE INTS FLOATS STRINGS ARRAYS OTHERS

74.5 .8
24.4 .1 .2 .0 ,0 -

PIVOT
66.7 4.5
26.6 1.0 1.0 .1 .0 -

SPARSER CONGEN

73.7 1.1
24.2 .5 .5 .0 .0 .0 -

73.0 .5
2,5.6 .9 .0 .0 .0 .0 .0

WIRE
74.8 .5
2,4.6 .1 .0 .0 -

SEC. 3
3. List Pointers

LIST POINTERS

..

15

A first guess at a possible source of regularity among list pointers might be the frequency of occurrence of pointers to individual cells. Marked regularity of this kind would exist only if there were considerable sharing of list cells among many pointers. This turns out, however, not to be the case; very few cells in the sample programs were pointed to more than once. (A quick and dirty verification of this may be found in Tables Z-1 and z-g. add together the percentages of cars and cdrs that point to lists and you get at most a few percent over a hundred. That is, list pointers outnumber list cell_ by a very small amount. This means little sharing.)

A more fruitful source of regularity turns out to be the physical proximity in

memory of pointer and pointee. Let a list pointer's distance be defined as the

at, solute value of the difference between the address of the cell pointed to and the

address of the cell containing the pointer. If this difference is positive, a pointer

will be said to point [orward; otherwise it points backward. Although Lisp

permits lists to be distributed arbitrarily throughout memory, it was found that

in practice list pointer distances are quite small most of the time. These distances

depend, of course, in Section 4.

on the allocation

strategy

for list cells, which

will be explored

Consider, for example, cdr list pointers in PIVOT. Figure g-1 shows the percentage of these pointers pointing a given distance away, for distances up to 16. Over half point to one of the two adjacent cells, and the percentages shown fall off quickly with increasing distance.
Large pointer distances were not entirely absent from the programs, however.

.3O

backward

forward

1|6 , 1!2

8!

4, 1 1 4
distance

! Ii
8 12 16

Figure Z-1. Cdr list pointer distances in PIVOT. The vertical axis is the fraction of all list pointers in cdr.

$EC. 3

LIST POINTERS

17

If we compress logarithmically the horizontal axis of Figure 2-I, grouping

distances according to the nearestsmaller power of 2, we get Figure 2.-2,which

also shows listpointer data for car. PIVOT is a fair representativeof the five

programs for this measure. Complete list pointer data for all of them is presented

in Appendix A, along with similar data collected after list ltnearization,

the

subject of Chapters 4 and 5. (The distance cell--was never observed.)

zero--i.e.,

a pointer to its own

Several strong statements can be made about these data. First, in either direction the distance 1 is far and away the most common distance in car and cdr, for all of the programs. Second, as distance doubles, the incremental gain in the number of pointers accounted for usually declines. And third, long-distance pointers are more common in car than in cdr.

The strong empirical regularity displayed by list pointer distances suggests a simple and natural space-saving encoding for them. a list pointer could be represented by an offset relative to the location of the pointer itself. Since most offsets are small, the bulk of list pointers could then occupy a field considerably
smaller than that required for uniform addressing of all possible list cells. Such compact encoding schemes are the subJect of Chapter 6.
.'

4. Effect of the Cons Algorithm
Searching for an explanation of the strong results of the previous section would probably lead an Interlisp afficionado straight to the storage allocator for list cells. When a new list cell is created in Lisp, the function cons must decide .where in memory to put the new cell. The cons algorithm used by Interlisp was

CAR

backward

i2o _I !
!
-_s ___ forward
i

,o5.

>7 7 6 5 4 3 2 1 0

0 1 2 3 4 5 6 7 >7

Llog2distanceJ

Figure g-g. Histograms of list pointer distances in PIVOT. The vertical axis is the percentage of all list pointers in car or cdr.

SEC. 4

EFFECTOFTHECONSALGORITHM

19

first described in a slightly different form by Bobrow and Murphy [Bob67]. It is designed to improve the paging characteristics of Lisp programs by attempting to place on the same page list cells that are likely to be referenced at about the same time, namely, successive cdrs of a list. Here is the algorithm, reproduced from [TeJ74]:

cons [x;y] is placed 1) on the page with y if y is a list and there is room; otherwise 2) on the page with x if x is a list and there is room; otherwise · 3) on the same page as the last cons if there is room; otherwise 4) on any page withat least 16 free list cells.

Interlisp keeps a separate free-list in address order on each page of list storage, and does not move lists during garbage collections.

It appears from results cited in Section 3 that this cons algorithm is quite

successful: most list pointers were found to point very nearby in memory. Cdr list

pointers, particularly,

were extremely local. But how much of this apparent

success is directly attributable to the cleverness of the cons algorithm, and how

much would have been achieved with a simple, straightforward

cons?

This question was investigated by letting two of the programs re-create their

list structures using a "naive" cons algorithm, and then re-collecting list pointer

data as described in Section 3. The naive algorithm skips steps 1 and 2. of the

Inter]isp cons routine (above) and begins at step 3. It is easy to see that this results

in the system keeping, in effect, a single long free-list of empty cells and allocating new cells from the front of that list. The presence of step 4 means that

pages with more than 16 free cells appear at the front of the free-list, but this

seems unimportant.

Except for this detail, the naive cons used is essentially the

one described by McCarthy et al. for Lisp 1,5 [McC6g].

ZO

STATIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 2,

SPARSER and WIRE were subjected to this simple cons routine. Since both programs started out with some list structure over which there was no control, the comparison dealt only with list cells created during the runs--198fiO cells for SPARSER, 34531 ceils for WIRE. Garbage collection of list storage occurred 14 times during 5PARSEn's run, 11 times during WIRE's.

The result was this: the distributions of list pointer distances produced by

the naive cons were only slightly different from those produced by the clever

cons of Interlisp.

That is, the number of list pointers found pointing a given

distance in a given direction was about the same after the run using the naive

cons as after the run using the clever cons. This does not demonstrate that

particular pointers have generally the same properties after the two runs, but this

does seem very likely.

The Interlisp cons was designed to make a list pointer point to a location on the same memory page as the pointer itself, and Table Z-3 compares the porcentages of new list pointers pointing on the pa_e after each run of each program. The clever cons makes only a few percent difference, possibly in the form of a trade-off between car and cdr. Thus, if car and cdr are considered together, at most one percent fewer list pointers point on the page under the naive
cons.

Table 2-3 Percentage of list pointers pointing on the page

CAR
CLEVER NAIVE

SPARSER WIRE

63.8 41 .fi

67.6 37.2.

CDR
CLEVER NAIVE
85.0 82.6 97.8 97.7

TOTAL
CLKVER NAIVE
78.6 78.1 85.1 84.1

SEC.. 4

EFFECT OF THE CONS ALGORITHM

2.1

We conclude that (at least for WIRE and SPARSER) the list-structure

locality

observed in Section 3 is ahnost independent of the clever cons routine; that it is

essentially a function of the order in which new list cells were created by the

programs, and of the fact that the free-list is kept in address order. That is, cells

in a list tend to be physically nearby in memory because they were created close

together in time, not because the storage allocator forced them to be physically

close. It is therefore reasonable to suppose that similar locality would be found in

any Lisp implementation

that kept its free-list in address order.

5. Atom Pointers

While a very small number of different list pointer distances accounted for a very large percentage of pointers to lists, such a strong regularity did not appear among pointers to literal atoms. In each program the atom most frequently pointed to accounted for only a few percent of all pointers to atoms; while, at the other end of the scale, as many as half of all atoms were pointed to only once, accounting, for another few percent of atom pointers. In this section we examine the static frequency distributions of atom pointers in each of the five programs.

. Figure 2-3 shows how pointers to atoms (in both car and cdr)were

distributed

in the five programs. (The atom NIL is, as usual, excluded.) The

number of occurrences of each atom is plotted against its frequency" rank, where

the most common atom has frequency rank 1, the second most common has

frequency

rank 2, and so on. Both axes are logarithmic.

If a similar plot were

made of the frequency of word occurrence in a large body of running English

text, the result would be approximately

a straight line with slope -1. This

property is sometimes called Zipf's law [Zip49], and is characteristic

of many

O
8
£]
_1 O8 O'-
Z W
rrrr O:::3 (O
08
U_
O
rr w m
z£

CONGEN -" "- PIVOT - = NOAH = = SPARSER , 4 WIRE
slope -1 (Zipf's law)

v--

1" r

T----

1 10 100 1000

FREQUENCY RANK

Figure Z-3. corresponding

Frequency

distributions

of atom pointers.

to Zipf's law is shown for comparison.

A line with slope

SEC. 5

ATOM POINTERS

23

natural languages; times as frequently

it states that the n th most common word occurs about 1/n as the most common word.

Figure 2.-3 shows rough agreement between the five curves and Zipf's law,

e_pecially if one is willing to disregard the most frequent few tens of atoms.

(Note that the steps to the right of each curve reflect the fact that atom pointers

can occur only an integral number of times.) CONGEN's curve stands out from the

other four in that Zipf's law underestimates,

rather than overestimates,

the

frequencies

of its most common atoms. PIVOT's curve is perhaps better

appro×imated

by a line of slightly flatter slope than the others. PIVOT and

CONGEN differ also in their total number of atoms: PIVOT has the greatest number

of atoms of the five programs, and CONGEN has the least. These differences

suggest that a frequency-based

encoding of atom pointers would be most

successful for CONGEN, and least successful for PIVOT. A quantitative measure of

this success will be given in Chapter 6.

6. Number Pointers

In .Section 2 of this chapter we observed that pointers to numbers, especially

small integers, account for a large fraction of cars in three of the programs

(SPAItSEI1, CONGEN, and WIRE). A closer look at these pointers seems warranted.

One might be tempted by the results on atom pointers to think that the

distributions

of number

to be the case.

pointers

would follow Zipf's law, but this turns out not

Figure 2-4 shows the number of occurrences of pointers to each number versus its frequency rank, just as was done for atoms in Figure fi-3. None of the

0 0 O'q
o
__-,
_
O" Z III rr rr O O
O8
i1
O
rr iii rn Z£
1

CONGEN --- -_..PIVOT = - NOAH -. -- SPARSER
, : WIRE

L
10 100
FREQUENCY RANK

t000

Figure 2-4. Frequency

distributions

of number pointers.

",SEC6.

NUMBERPOINTERS

2,5

five curves seems particularlywell modeled by the Zipfdistribution_neither does there appear to be any substantialsimilarityamong them. One area of agreement, however, not apparent in the figure, is the tendency for small non-negative integers to be heavily pointed to. The numbers 0 through 15, in particular, account for the following percentagesof allnumber pointers:NOAH, 30.2 percent_ PIVOT, 19.4 percent; SPARSER, 22.7 percent;CONGEN, 95.4 percent; and WIRE, 29.7 percent.
Another area of agreement seems to be the largenumbers of singly-occurring numbers. A plausible explanation for thisis that large integers (absolute value greater than 15;36)and floating-pointnumbers are very unlikely to be equal, and are therefore likelyto be pointed tojustonce.

3
Dynamic Measurements of List Structure

Measurements

of list structure

references during program

execution show some disagreement and some agreement with the

results of the previous chapter. The distributions of dynamic atom

references,

for example, are more skewed than Zlpf's law; while

dynamic list-pointer distances are about as likely to be small as are

static distances. Additional measurements show that car and cdr are

executed about equally often, and together account for most dynamic

references operations

to list cells. Data type distributions before and after rplac are reported. Patterns of repeated reference to individual

cells are briefly investigated, as are references to list pages.

27

28

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

1. Introduction
The static results of Chapter 2. suggest a host of corresponding measurements of how Lisp programs use their list structure during execution. In this chapter CONGEN, NOAH, and SPARSER are analyzed from the dynamic point of view.

1.1 Data collection technique

The tool used for the dynamic measurements of this chapter was a PDP-I0 simulator developed by Robert Bobrow at Bolt Beranek and Newman, Inc. Bobrow uses this simulator for his own study of various performance aspects of Interlisp, notably its paging activity [BobR76]. Some simple modifications of this program produced an instrument suitable for list-structure measurements.

Bobrow's simulator was modified as follows. Whenever the simulated

effective address calculation yielded the address of a list cell, several items were

written out on a special disk file, hereafter called the trace file: the opcode of the

(about-to-be-simulated)

instruction whence came the address, the address itself,

the contents (car and cdr) of the cell addressed, and if the instruction is either

r/,laca or rplacd, the new value of the appropriate pointer. Interlisp has a page table called TYPTAB which gives, among other things, the data type of each page in a running program's address space. (Recall that Interlisp puts objects of just one

type on each page,) The simulator writes TYPTAB out on a file for its own

purposes.

This file, together with the trace file, gives enough information

to

perform, off-line, many measurements of dynamic list structure use.

We can, for example, discover which Lisp primitive functions are being executed by looking at the opcodes in the trace file. This is the subject of Section

SEC. I

INTRODUCTION

29

5. Other measurements

parallel those of Chapter 2: the data types that appear

during executions of car and cdr are reported in Section 2; list pointer distances are covered in Section 3; and dynamic references to atoms are discussed in Section

4. Section 6 analyzes the operations rplaca and rplacd, and Section 7 considers patterns of repeated references to list cells.

the

1.2 Comparisons with staticmeasurements

Itwill be interestingto make some comparisons between the dynamic results presented here and the static results of Chapter 2. But some practical considerations intervene to make a directcomparison difficult.Foremost among these factors is the computational cost of the PDP-10 simulator:simulating the programs rather than executing them directlycosts roughly a factor of 60 in execution speed. Programs were simulated,therefore,on rather modest tasks that created considerably lessliststructurethan the versions of Chapter 2, and that required no garbage collections.

Comparisons will be made, then, between these modest dynamic runs and the staticliststructures extant at the end of the runs. The sizesof the latterare as

follows: 8643 cellsin CONGEN, 33073 in NOAH, and 28365 in SPARSER. For these comparisons to make sense, however, we will need to make an assumption.

During execution of these programs, as with any Lisp programs, new cells are created and old ones become inaccessible("garbage"). Thus the cells that participatedynamically are probably not the same as those existing at the end of

the run. They are alsoprobably not the same as those existingat any fixed point during the run. The assumption we will make isthat the staticliststructure at

the end is representative,

according to the characteristics

chapter, of the structures referenced dynamically.

measured in this

30

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

2. Data Types

Following Chapter 2, we might first ask how often the various data types of

Interlisp are referenced dynamically by the functions car and cdr. Each time

car[x] or cdr[x] is executed, the car or cdr field of cell x has a particular data

type, and all references to cars and cdrs of each type can be recorded for each of

the dynamic traces. Table 3-1 shows the results for car pointers, together with

the corresponding

static data mentioned in Section 1.

TABLE 8-1 Static and dynamic percentages of data types in car

LISTS ATOMS SMALLS NIL OTHERS

CONGEN
STATIC DYNAMIC
25.1 49.1 32.3 11.3 31.1 35.8 10.7 3.7
0.8 -

NOAH
STATIC DYNAMIC
38.5 23.8 49.9 58.2
1.5 14.4 5.7 0.5 4.4 3.1

SPARSER
STATIC DYNAMIC

35.1 51.3
7.6 P..3 3.6

25.7 51.5 21.5
1.3 ,-

Our search for pleasing empirical regularities, so often successful in Chapter 2, is frustrated by Table 3-1 in two different ways: static and dynamic results do not agree, and the ways in which they disagree are different among the programs. For example, CONGEN makes dynamic reference to its atomic cars more often, and to its list cars less often, than those data types appear statically; while NOAH does just the reverse. Small integers are referenced far more often than they appear in NOAH and SPARSER, but only slightly more in SPARSER.

Cdrs are more regular than cars, as can be seen ill Table 3-2.. Dynamic

referonces

to lists exceed static occurrences in all three programs, and NIL is

referenced less often than it appears in all three.

SEC. 2

DATA TYPES

31

Table 3-2 Static and dynamic percentages of data types in cdr

CONGEN
STATIC DYNAMIC

NOAH
STATIC DYNAMIC

SPARSER
STATIC DYNAMIC

LISTS ATOMS SMALLS NIL OTHERS

72..7 0.8 3. I
23.3 0.0

78.3 -
2.0 19.6 0.0

75.4 91.0 2.2 0.2 - 0.1
22.2 8.7 0.2 0.0

67. I 1.4 0.6
30.5 0.4

80.5 -
19.4 -

The static/dynamic

diff_'r_nces show that the programs are not making

uniform dynamic references to all of their list structures, nor to a representative

subset of them. The differences among the programs, especially with respect to

cars, suggest that they are doing different things with their list structures.

Both

propositions are quite reasonable, of course.

Note, however, that the static and dynamic data do not show remarkable

disparities:

floating-point

numbers,

for example,

have not become

overwhelmingly

prominent in the dynamic setting. Executions of car generally

yic:ld a list, an atom, or a small integer; executions of cdr most often result in a

li._t, and it not that, then probably the atom NIL. So while the strong similarities

of Chapter 2. elllde us here, we do observe grossly similar characteristics type use in the three programs.

of data-

3. List Pointers
_Ve know from Chapter 2 that list pointers most often point to a nearby cell in memory, and that the two immediately adjacent cells are very likely to be pointed

32.

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

to. Does this property hold in the dynamic setting? That is, when car[x] or

calf[x] is executed and the result is a list pointer, is the new list likely to be close

to x? The intuitive that intuition.

answer is surely yes, and this section presents data to support

Dynamic list pointer distances were tabulated in the obvious way from the

trace files. Figure 3-1 shows the results for SPARSER, along with the

corresponding

static data. For the dynamic data, percentages shown are of the

total number of car or cdr executions that yielded list pointers; for the static data,

of the total number of cars or cdrs that were lists, just as in Chapter g.

The basic agreement between the static and dynamic data seems clear:

roughly speaking, the number of list pointers accounted for declines with the

logarithm of distance. But just as with references to data types, there are some

disagreements

between the static and dynamic results. Some pointer distances are

traversed dynamically more often, and some less often, than would happen if the

static lists were referenced in a uniform way. These facts characterize the

CONGEN and NOAH data as well.

The percentage of list pointers that point on the page is shown in Table 3-3 for all three programs. As Figure 3-1 would suggest, there is little difference between the static and dynamic data.

34

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

Table 3-3 Percentage of list pointers pointing on the page

CONGEN NOAH SPARSER

CAR. CDR TOTAL

STATIC

97.1

DYNAMIC 93.g

97.8 95.6

97.6 94.8

STATIC

72.3

DYNAMIC 79.5

90.6 87.6

84.4 86.0

STATIC

84.4

DYNAMIC 88.9

94.6 86.8

91.1 87.3

4. Atom Pointers
Chapter 2 might lead us to expect that Zipf's law would model the distribution of dynamic references to atoms just as it does the static distribution. This turns out, however, not to be so. In this section a dynamic reference to an atom will mean an execution of car or cdr whose result is an atom pointer. As usual, the special atom NIL is excluded.
Figure 3-2 gives the number of references to each atom versus its frequency rank for NOAH and SPARSER. t Both static and dynamic results are shown. The
tCONGEN Is omitted on the grounds of Insufficient data. Section 2 showed that a meager I 1.3 percent of cars yielded atom pointers, and the small amount of static list structure In CONGEN was noted In Section 1. The results of this section do apply approximately to CONGEN's atoms, but the data are too thin for meaningful comparisons with the other programs.

5KC. ,4

ATOMPOINTERS

35

"static curves of Figure 3-2 are, as we expect, closely approximated by Zipf's law. (These are not the same data as in Chapter 2, however.) Notice that the dynamic curves are quite like one another, but quite unlike the static curves: they are markedly steeper and have noticeable downward concavity. The dynamic curves are particularly steep for atoms referenced about twenty times or less.

In Figure 3-7. the static and dynamic curves for each program are based on two separate rankings of the atoms. One might reasonably expect these rankings to be very similar, but in fact they are not. Figure 3-3 shows how the ranks of individual atoms in NOAtt are related. (SPARSER displays similar behavior.) Atoms with static or dynamic rank up to 40 are plotted. A point (x,y) in Figure 3-3 means that the atom with static rank x has dynamic rank y= for example, static atom number 4 has dynamic rank 17..

If the two rankings were identical, Figure 3-3 would show all points lying

on the line y = x; if they were exactly opposite, the points would lie on the curve

x + y = k+l, where k is the number of ranks. If the two were completely

uncorre]ated,

the points would be scattered randomly throughout the L-shaped

region defined by the property: at least one rank is 40 or less. (Notice that the

logarithmic

axes of Figure 3-3 would concentrate randomly distributed

toward the upper and right extremes of the plot.) None of these conditions

points holds.

The rankings are moderately correlated, with a small number of individual atoms

having wildly different static and dynamic ranks.

It is possible that an atom pointed to In some list might never be referenced

dynamically.

In Figure 3-3, atoms with static ranks 7., 5, and 6 have this

property.

It is plainly impossible, however, to reference an atom that never

appears statically. That there are such great differences between the static and

dynamic ranks of atoms, particularly those of samll rank, is quite surprising.

It is

0
NOAH ·- -0- SPARSER

Figure 3-Z. SPARSER.

Atom pointers

(static) and references

(dynamic)

tn NOAH and

(D
_t ·

0·

··

·

o t

Z

I

LJ-_

· ·I O · +I

·

/

· 0

· Q· ///
+,,

·
o· e

·

Z /jJ · ·

/· /·

41

/ /

/O

t

/·

·

/
/t / / /, /

t

· ·
·
e
·

// / / t / 1 / / / /

], L / /
t,j
.......
1

'+
4

· ·

·

0- ....... "Ir

T

10 40 100

STATIC RANK

"e-- -- e,..--_ ·
4+0

·
100"0

Figure 3-3. Scatter dial, ram of static and dynamic
least one rank less than 40+

ranks of NOAH atoms with at

38

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

dJff'icult to see how static atom 2, pointed to fully 627 times, could have escaped

dynamic reference entirely. It is perhaps less startling that dynamic references

might concentrate on a rarely-appearing

atom, such as is the case with dynamic

atom 19, whose static rank is 938.

Another way to look at the data is by means of a plot of the cumulative

percentage of static pointers and dynamic references versus static and dynamic

rank: Figures 3-4 and 3-5 show the appropriate four curves for NOAH and

SPARSER, respectively,

on semilogarithmic

plots. Figure 3-4 shows, for example,

that in NOAH the 20 most commonly-appearing

static atoms account for 2.9

percent of all static atom pointers, and 2.2. percent of all dynamic references; the

ZO most frequently referenced dynamic atoms account for 62. percent of all

dynamic references, but only 9 percent of all static occurrences.

5. Primitive Operations o=1List Cells

Reading or writing a list cell can only occur during execution of one of five

I,i._p primitive functions: car, cdr, rplaca, rplacd, and cons. The operations car and cdr fetch the car or cdr field of an existing cell; rplaca and rplacd replace the

car or cdr field of an existing cell with some specified new value; and cons creates
a new cell. The traces of list cell references considered in this chapter record all
q
e._-ecutions of these five primitive functions. In Interlisp, car, cdr, rplaca, and

rplacd require exactly one PDP-10 instruction each (HRRZ, HLRZ, HRRM, and

HRLM, respectively).

Cons takes two: one memory read to get the new free-list

link contained in what will be the new cell, and one write to install the new cell

itself (the instructions

for these are MOVE and POP, respectively).

(It should be

RANK
Figure 3-4. Cumulativo distributions of atoms in NOAH.

Figure 3-5. Cumulative distributions of atoms in SPARSER.

SEC. 5

PRIMITIVE OPERATIONS ON LIST CELLS

41

noted that the cons function involves a good deal more computation that only two of its many instructions reference list cells.)

than this, but

An obvious question to ask at this point is how often each of the list-

referencing representing

primitives is executed. If we lump together the instruction pairs cons and express the number of executions of each function as a

percentage of the total, we get Table 3-4. The total number of function

executions

in the trace files is 260659

254409 for SPARSER.

for CONGEN, 2.17942

for NOAH, and

Table 3-4 Execution frequency of Lisp primitives

car cdr cons rplaca rplacd

CONGEN
37.0 44.7 11.5
0.4 6.5

NOAH,
45.2. 47.0
4.6 I .fi 2..0

SPARSER
4.6.7 48.0
3.2. 0.3 1.8

Table 3-4 shows that the three programs are strikingly similar. Car and cdr account for the great bulk of all primitive function executions, with cdr a shade more common than car. Cons is next, followed by rplacd and rplaca, which together happen more than half as often as cons.

6. Rplaca and Rplacd Although rplaca and rplacd are rare overall, they occur frequently relative to

42.

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. 3

cons, and a closer look at them seems warranted. An interesting question to ask is how often pointers of various types are replaced by pointers of various other typ_. Table 3-5 shows this for rplacd in SPARSER (an adjacent list pointer points forward or backward one cell). For example, Table 3-5 says that 31.4 percent of 5PARSER's rplacds replaced NIL by an adjacent list pointer, and a total of 46.6 p¢_rcent replaced NILs by pointers of all types. Appendix B reports complete data for rplaca and rplacd in CONGt:N. : OAH, and 5P,: I;'_Erl.

Table 3-5 Replacement oP data types by rplacd in SPARSER

Type of old cdr
adjacent list other list NIL column sum

Type of new cdr

adjacent list

other list

NIL

0.2 0.5 9.3 6.2 7.2 30.0

31.4 11.4

3.8

37.8 19.1 43. I

row sum
10.0 43.4 46.6 100.0

5pv_ral broad g_,neralizations

may be made about rplacd, although,

as

Appendix B documents, close agreement among the programs does not exist. First,

th_ replacement of NIL by a list pointer is common, and the list pointer is likely to

point nearby. This happens, of course, when a list is built front-to-back,

possibly

with the Lisp functions nconc or tconc [Tel74], rather than back-to-front

with

cons. Second, NIL replaces a list pointer frequently.

Third, rplacd tends to

incroase the numb_,r of pointers with small distances. For example, Table 3-5

shows that in SPARSER adjacent list pointers make up 10 percent of the cdrs

replaced, but 37.8 percent of the new cdrs. Appendix B shows similar behavior in

CONGEN, but only a weak resemblance in NOAH. Fourth, the data-type

SEC. 6

RPLACA AND RPLACD

43

distributions

of old cdrs and new cdrs disagree with both the static and dynamic

distributions

reported in Table 3-2. This is due mainly to the heavy participation

by NIL in both ends of the rplacd operation.

Generalizations

about rplaca are more difficult: Appendix B shows substantial

differences

among the three programs in the ways that they use rplaca. In

S]_ARSER, the most frequently occurring combination of old type and new type is

the replacement of one (non-NIL) atom pointer by another (26.4 percent of the rplac¢_s); in CONGEN it is the replacement of one list pointer whose distance is

greater than 32 by another (31.9 percent); in NOAH it is the replacement of one

small integer by another (a whopping 70.3 percent). NILs are surprisingly common among old cars in CONGEN (34.5 percent) and SPARSER (50.7 percent),

though not in NOAH. As was the case with other measurements the programs tend to disagree much more than they do about cdr.

of car pointers,

7. Patterns of List Cell Reference

Previous sections of this chapter have suggested that list structures are not

accessed in a uniform manner. The divergence between static and dynamic results

of various kinds has shown that computation, at least in the three programs

measured, tends to focus on small parts of the totality of list structures, probably

making repeated references to some cells. Thus Section R showed that dynamic

references to cars and cdrs of certain data types were not at all in proportion to the

static occurrence of those types. Section 4 indicated that dynamic atom references

were considerably

more concentrated among a small number of atoms than Zipf's

law would predict, and revealed that certain atoms abundantly pointed to were

never referenced.

Section 5 showed that certain listpointer distances were more

4,1

DYNAMIC MEASUREMENTS OF LIST STRUCTURE

CHAP. ,3

li k_,ly to occur dynanlically than statically. In this section we will examine the patterns of dynamic reference to list cells in the hope of explaining some of these differences.

7.1 Inter-reference

distances

For each reference to a list cell, let its inter-reference

distance be defined as

the number of cell references (read or write) since the last reference to that same

cell. The first reference to a cell will obviously not have an inter-reference

distance. These distances can be trivially computed from a program's trace file,

which records in sequence all list-cell references made during a short run of the

prod, ram. Thus distance 1 corresponds to two successive referonces to the same cell, as would happen, for example, if car[x] were followed by cdr[x] with no

list-cell references intervening.

Recall from Section 5 that a single execution of

cons will also yield two adjacent references to the same cell.

Figure 3-6 shows the cu,nulative percentage of all references in the trace file

versus inter-reference

distance, for CONGEN, NOAH, and SPARSER. Distances

greater than 1000 accounted for 4 percent of CONGEN's references, 11 percent of

NOAIt's, and 12. percent of SPARSER's, and do not appear in the figure. In CONGEN,

1 1 percent of the total were first references; in NOAH, 5 percent; in SPARSER, 4

porcent. As we would surely expect, Figure 3-6 shows a large amount of repeated

reference to cells. In both programs, for example, just about half of all references

were to a cell that occurred among the previous ten references.

Executions of cons have two important effects on these data. First, as was

m_ntioned

above, every cons results in an inter-reference

distance of 1- in

CONGEN a little under half of the distance 1 references are due to cons; in NOAH,

about one-seventh; in SPARSER, one-tenth. Second, the first of the two references

Z
o
b_
< I::E
L.L..
L,U >
-::.:J3
:::3
0
,1-
0"
1

CONGEN · · NOAH · · SPARSER

iT
10 100
INTER-REFERENCE DISTANCE

1
1000

Figure 3-6. Cumulative NOAH, and SPARSER.

distributions

of inter-reference

distances in CONGEN,

46

DYNAMIC MEASUREMENTS OF LIST STRUCTURE;

CHAP. 3

made by cons is always a first reference. In fact, cons is responsible for most of the first references in all three programs: 98 percent of them in CONGEN, 89 percent in NOAH, 77 percent in SPARSER. This indicates that during the program runs measured, most of the cells referenced at all were themselves created during the run.

7.2 Stack distances

Suppose now that each time a list cell is referenced, it is brought to the top of

a stack of some fixed depth. If the cell is already present somewhere in the stack,

it is simply removed from that position and pushed on top; in this case the stack

distance will be defined as the depth in the stack of the new reference. The top of

the stack has depth 1. If the new reference is to a cell not currently present in the stack, its stack distance will be called infinite, it will be pushed on top of the

stack, and the deepest element---the Least Recently Used, obviously---will

be

dropped off the bottom. Note that a reference's stack distance, if not infinite, is

just one more than the number of distinct cells referenced to the same cell.

since the last reference

The interesting question to ask now is how efficient stacks of various sizes

will be in capturing originally designed

list-cell references. The algorithm of Mattson, et al. [MatT0],

for simulation studies of page-replacement

strategies, can be

used to answer this question. This algorithm uses the fact that the number of

roferences found in an LRU stack of size K is just the number of occurrences of

stack distances 1 through K in the reference string. Exploiting this fact, the

algorithm efficiently calculates success rates for a whole range of stack sizes

during a single pass over the reference string.

_SEC. 7

PATTERNS OF LIST CELL REFERENCE

47

Results are shown in Figure 3-7 for stack sizes up to 1000. The curves of

Figure 3-7 lie above those of Figure 3-6 at a distance that varies up to about 10

percent. 1.

The two sets agree, of course, at inter-reference

distance or stack depth

A practical implication of Figure 3-7 lies in the use of a cache for list

structure.

If the cache replacement rule is LRU and if only single cells are

brought in when a fault occurs, Figure 3-7 gives precisely the hit ratios one

would observe for caches of various sizes. A hit ratio of 90 percent would seem to

require a cache size of about 1000.

7.3 References to list pages

If we regard each list-cell reference as a reference only to its page (of 51?.-

cell capacity) and run the algorithm of Mattson, et al. again, we get Figure 3-8. The number of pages ever referenced was 78 in NOAH, 71 in CONGEN, and 84 in

SPARSER. The pronounced knees of the curves in Figure 3-8 indicate, not

surprisingly, referenced

that most references to list cells are to a small number of recently pages. The figure shows, for example, that the 3 most recently

referenced (distinct) list pages account for over 93 pe.reent of all list references.

The implications for memory management schemes are clear.

.1-

1

lO!

16o

100! 0

STACK DISTANCE

Figure 3-7. Cumulative distributions of stack distances in CONGEN, NOAH, and SPARSER. The stack holds Individual list cells.

NUMBER OF PAGES

1

4 10

40 60

1 iL

.95"

Figure 3-8. References to list pages in CONGEN, NOAH, and SPARSER.

4 Li nearlz"atlon" Alg or ith ms

This chapter presents two algorithms that linearize lists, that is,

rearrange memory

them so that cdr (or car) points to the very next cell tn whenever possible. The first algorithm linearlzes a list by

moving original

it to a block of contiguous free cells while destroying the list. The second restores the original list after the copying has

been done. Neither algorithm uses mark bits, neither needs more than

a constant amount of working storage, and both are faster than the best

previous algorithms for these tasks.

51

52

LINEARIZATIOANLGORITHM,S

CHAP.4

1. Introduction

Linearization

is the process by which list structures are rearranged in such a

way that consecutive cdrs (or cars) of a list are allocated to sequential cells in memory whenever possible. Linearization in the cdr direction will mean that

cdrs are allocated sequentially_ car direction will mean that cars are. Figure 4-1 shows how a list would lie in memory following linearization in each direction.

.Before linearization,

of course, cells of that list might have been distributed

anywhere in memory.

Static and dynamic effects of linearization in the five Interlisp programs are

the subjects of Chapter 5. In this chapter we a.re concerned with algorithms that

accomplish linearization.

In particular, algorithms will be presented that do this

in linear time and constant workspace, original list structure and its copy.

other than the space required

to hold the

Some untangling of terminology is appropriate at this point. Moving a list and copying a list have come to mean the creation of a list structure identical to

some original in such a way that the original can be destroyed during processing, or cannot be destroyed, respectively [Rei73, Lin74, Fis75]. £inearization of the list occurs if it is moved or copied into a block of contiguous storage locations, provided the list is traversed consistently in either the car or cdr direction.

Linearization

is sometimes called comPaction ' although there are other,

nonlinearizing

ways to compact a list (e.g., [Knu68, p. 42.1]). If all active lists in

a system are moved, then what is accomplished is a type of garbage collection: all

of the original structures, together with all of the inactive structures, become

free storage, and list processing proceeds on the copies of the active lists.

Linearization

for its own sake has not been the focus of algorithms in the

CAR DIRECTION
AB ..._.."..-.. t '"
yD

CDR DIRECTION
.-c. _D.-.D _ DD
ABSD

Figure 4-1. Linearlzation is shared.

of the list ((A B) C ((A B) D)), in which the subltst (A B)

54

LINEARIZATION ALGORITHMS

CHAP. 4

literature, collecting

as we shall see; it is most commonly or list-moving algorithm.

a side-effect

of a garbage-

The remainder of this chapter consists of two self-contained papers that have

_ppeared sections

elsewhere. Section g is [Cla76a], Section 3 is [Cla75a]. Becausethese

can stand by themselves,

there will be some understandable

reintroduction

of terminology and other minor redundancies.

2. An Efficient List-Movirg Algorithm Using Constant Workspace ¢

2.1 Introduction

The problem addressed here is how to move an arbitrary Lisp-type list structure using an amount of storage (other that that required to hold the original list and the copy) that does not grow with the size or complexity of the list. Unlike the problem of copying a list [Fis75], moving a list allows the old list to be destroyed during processing. Following Lisp conventions [McC6g], assume that each list cell contains two pointers, called car and cdr, which can point to any list cell or to nonlist items, which are called atoms.

If a stack of sufficient depth is available, moving a list structure is relatively

straightforward.

Fenichel and Yochelson [Fen69] give a recursive list-moving

algorithm as part of their garbage collector. Minsky's algorithm [Min63], also

tCopyrtght (_) 1976, Association for Computing Machinery, appeared In Cqmm. ACM L_, 6 (June 1976).

Inc. Reprinted by permission.

This section

SEC. 2

AN EFFICIENT LIST-MOVING ALGORITHM USING CONSTANT WORKSPACE

55

intended for use in a garbage collector, optimizes use of Its stack but still needs more than a constant amount of working Storage. Both of these algorithms require time proportional to the number of cells moved.
Cheney's algorithm [Che70] was the first that needed workspace of only constant size. Although it was originally intended for "compact lists"--i.e., those cdrs that point to the following location in memory are simply omitted---it can easily be adapted to work on Lisp-type structures. In this paper "Cheney's aJgorithm" wtll mean the Lisp-oriented version of the original. Retngold's algorithm [Ret73] employs the Deutsch-Schorr·-Waite list-tracing technique [Knu69, p. 417; 5ch67] to avoid using a stack. This idea was suggested by Fenichel and Yochelson [Fen69]. Reingold's algorithm traces lists tn the car direction; that is, it follows car before cdr if there is a choice. In the interest of uniformity, "Reingold's algorithm" will hereafter mean the simple modification of the original that traces in the cdr direction instead.
Cheney's and Reingold's algorithms are both linear in the number of cells moved, and both require at least two visits to each cell. The algorithm presented here is also linear, but must revisit only those cellsboth of whose pointers point to lists. A recent empirical study of list structure data in Llsp [Cla76b] found that in the programs considered, about one-third of cars and three-fourths of cdrs were lists. Assuming that cars and cdrs are independent as to their data types, this means that in real list structures only about one-fourth of list cells would need to be visited twice by the present algorithm.

56

LINEARIZATION ALGORITHMS

CHAP. 4

2.2 An example

The operation of the algorithm is illustrated by example in Figure 4-2.. The

list structure to be moved is ((A)B (A)C (D)), where the two occurrences of the

sublist (A) are in fact the same cell. Figure 4-2(a) shows the original structure.

In the figure, car is the left-hand pointer of a cell, and cdr the right-hand one. A

diagonal slash designates the list-terminating

atom NIL.

The algorithm first copies the top level of the list, with some changes, into sequential locations in the new list area. The state of affairs after this has been done is shown in Figure 4-2(b), in which the following conditions hold:

(1) Old cars have been replaced by "forwarding addresses": car(x) is x's new

location. This technique, or some variant of it, is employed by all of the other

list-moving

and copying algorithms discussed here [Che70, Deu76, Fen69, Fis75,

Min63, Rei73]. Discovery of a forwarding address where an ordinary expected inhibits the creation of spurious copies of shared cells.

car was

(2) In the copy, atomic cars and the one atomic cdr (so far) have their final values.

(3) List cars in the copy point to the original sublists in the old list area.

(4) List cdrs in the copy have their final values, and all point to the next consecutive cell in memory.

(5) Old top-level cells with list cars have been linked together through cdrs in LIFO order on a list k. The first such cell encountered terminates having NIL in its cdr.

their k by

The algorithm must now, in effect, "pop" the k-list and move the old sublist

(a)

]_- __]+----- k

olisldt

k

_ I
I II I

i I ,
Ii

[_-*1_
(b)

'_sr'eea;
area

_ d/
! , l iI

I
I i II

_
II I

1¢)

I

II

II

I l II

I _I II ,

I _,_ , , I i I

!,

lI

!

I[ , , ± L ,

I .................

//

',

//

'

! /-------=

t

tl ,I, · I T Tt _ _ _

h-*[NN-_NN-._t+Ig_-_"N2]gI_

(dl

I
._1'

Figure 4-2. Moving procc,::;,_in_:',; (d) final fig_l re.

the list ((A) B (A) C (D)): (a) initial state; state. Dotted pointers are those unchanged

(b) and (c) during from the previous

58

' "_^_'Z_Tii:t_,: _,:__ ....... ·

CHAP.4

pointed to by car of the copy of the fl:._-,_:__::'<_-__:_ k..:_._-:_:__a,r'(c,_r_k)). The top level of this sublistwill be moved as de_cr_:-,aebdove: a.,na<._.::::_-_:r:eel_swith list

cars are encountered, the}" wi_l be ._::::_,_

.............

_....

popped,the

corresponding

list car _n :he co_:!.: ::_i:l be sea _:_ its f_.na! value. This

valhe will be the next free cell in the new a:e._: _:Tt_:e s:_.bli:_. ',_"a.: :,uoa:_ready been

moved, or the forwarding addressof [hesub_:_,:'c_.a_r,k(_,c,.a._r.,(c.a,r_'"_.)_,:i;_has.

structure have been copied. One cell re:_,_:::_::_ o:: ::i:_: k,--:>:s: 7'be _orwarding

address left in the old shared _:_b_.ist ':_ " .........

_*,,_ _,_;_by preventing

creation of a second copy. Figure 4 '__

: _,:.:....:....

.....

significantly

from those of Cheaey a_::_ )-_-':_.:iol_,_, (::_(.>nev'z e:gor!:hm, after

copying the top level of a list much as, ;,,__._:?_.:g_._,_ _!:_ :,s _,__-,__:_,.:_:_..i._...v ,.::ch cell of

the copy to find sublists, Beingo_a__ _l_:_:ithm _ac_:_:.:: ._; '_:_'_:__._cells _o its

version of the k-list during copying_ at:

_,::__:,,.,::._:.::-,_. new list cdrs

only when backtracking up the ll_t ::_c_. __-::-_;,_ ...._:_,_': :_:_'_::z_:":....'_'_ of cells) with atomic cars.

2.3 The algorithm

Although the algori:hm doe:: _:_-;_;e'_:i:,::::: :::

:_:::._:: ': : .... :: _._:)e.sneed

to be able to tellwhether ;_po:ute:::><_.:.:.:.:_.:.,"_,.:... ::_:._:"::..,:._.:...:._.:,:_::_._.

new region to be a block of con'_:_:::: :::_ :.,::::_,: ::: :,_ :: ,_:_:,::<-_' _'_ ::,:,_::::::::_,:s_::_::n pit by comparing the pointer with t.h:._dd::__:_.:.:.::.:?.:._::,._:::.:0::

predicate new(x) be true if and only _,: _<_:::,_::::!as_>___",_ _,:-._i-,:::::::_:_::_: :;::.:::_:i,,e_ :he free variable n point to the first avaJlab:e c:_'::: ":: ::', ::.: '.:.: ::::::,::::_c::: _:::_u,.,_...:=.*,.'h,".3..*,.e.ach

list cell occupies one word of ,_,.: ,:_, ,:

:':::.,::::ash:or:ibm

SEC. 2

AN EFFICIENT LIST-MOVING ALGORITHM USING CONSTANT WORKSPACE

59

given below will move the list pointed to by h from the old list region to the new.

On termination of the algorithm, free cell in the new area.

h will point to the new list, and n to the next

PART A: Copy the top level of a list.

A1. [Initialize.] x,-h, h_n, and k_NIL. A_. [Save car and cdr.] a-car(x) and d*cdr(x).

A3. [Store forwarding address.] car(x)_m
A4. [Is car a list?] If a is a list then cdr(x)_k and k,-x. (k points to the most recently visited cell whose car is a list.)

AS. [Copy old car.] car(n),-a. (If a is an atom, car(n) has its final value
now,)

A6. [Compute and write new cdr.] If d is an atom then cdr(n),-d, n_n+l,

and go to step B1. If new(car(d)) then cdr(n),-car(d), n,-n+l, and go to

step B1. Otherwise d must be an unvisited list, so cdr(n)_n+l, n,-n+l,

x,-d, and return to step A2. (In all cases cdr(n) gets its final value in

this step. If d is an atom or an already-visited

list, cdr-direction

tracing stops and we go to Part B to find the most recently seen

sublist. Otherwise we continue cdr-following and return to step A2.)

PART B: Find the most recently visitedsublist.
Bl. [k=NIL?] If k=NIL then the algorithm terminates with h pointing to the new list and n to the next free cell.
B2. [Remove first element of k-list.] x_car(car(k)), t_k, and k,-cdr(k).
B3. [Compute and write new car.] If new(car(x)) then car(car(t)),-car(x) and return to step B1. Otherwise car(car(t))_n and go to step A2. (Car of the copy of the cell just removed from k gets its final value in this step.)

6('_

LINEARIZATIOANLGORITHMS

CHAP.4

2.4 Improvements, variations, and extensions

The algorithm as it stands puts onto the k-list cells with list cars and atomic

cdrs (or already copied cdrs), only to remove such cells immediately in order to

copy the sublist. An improvement in the algorithm, therefore, would be to avoid

these, undesireable second visits by adding a cell to k if and only if car is a list and

cdr is an unvisited list. Since cdr must be checked for this property anyway (to

establish effort.

its new value), this speed-up would require no additional computational

If this change is made, then every cell on th_ _ K-list will have the following

property, cdr of the copy of the cell will be the next sequential cell in the new list

area. This redtllldancy

make:_ it possible to keep tile k-list in the copy rather than

in the original list, since the cdrs temporarily displaced by the links of k can

easily be recomputed during the second visit to each cell. Keeping k in the copy

eliminates one memory fetch when each element of k is removed: car(k) is desired

instPad of car(car(k)). previous section.

The resulting algorithm is faster than the one given in the

If referenc_ counts are available for each list cell, a further improvement

is

possible, namely, avoiding the storage of a forwarding address for cells with a

refer_mce count _f onp. (This clearly requires that the k-list be kept in the copy.)

This idea wa5 used by Deutsch and Bobrow in lheir iinearization

algorithm

[Deu76]. If, moreover, all cells have a reference count of one, then the list can be

moved without altering the original structure.

If many list cells are pointed to more than once, it may be profitable to

evaluate new(car(car(x)))

when a cell x with a list car is [irst visited. If

car(car(x)) is a new list pointer (as it would be, in this case, much of the time),

SEC. 2

AN EFFICIENT LIST-MOVING ALGORITHM USING CONSTANT WORKSPACE

61

then x need not be added to the k-list, thus saving the second visit and the attendant overhead. If, on the other hand, few cells are shared (as appears to be

the case in real Lisp programs [Cla76b]), this check would not be worthwhile.

The reason is that in the usual case new(car(car(x)))

would be false, x would be

attached to the k-list, and when, sometime later, x was removed from the k-list,

new(car(car(x)))

would have to be evaluated again to make sure x had not been

encountered in the interim,

The new area into which a list is moved need not be a block of contiguous locations. New cells could be acquired from an arbitrary free-list by an obvious change in the algorithm. If old and new list areas overlap in memory, however, the function new(x) would probably require a mark bit in each cell. (Clearly this change would not permit the k-list links to be kept in the new cdrs, as suggested above.)

3. Copying List Structures Without Auxiliary Storage
3.1 Introduction
The problem considered in this section is the creation of a copy of an arbitrary Lisp-type list structure without the use of a stack or any other working storage that depends on the size or complexity of the list to be copied. Apart from a fixed number of program variables, the only storage available is that occupied by the original list and the copy. Copying differs from moving a list [Che70] in that the original structure may not be destroyed during processing.
Algorithms for the constant workspace copying problem have been given by

62.

LINEARIZATIOANL' GORITHMS

CHAP.4

Lindstrom

[Lin74] and Fisher [Fis75]. Lindstrom showed how to copy an

arbitrary n-cell list structure in time O(n log n) if a mark bit is available in each

cell, and in time O(n 2) if there are no mark bitsl both algorithms can copy into an

arbitrary list of available cells. Fisher's algorithm takes only linear time and

needs no mark bits, but makes a minor sacrifice in generality, the free-list must

be a block of contiguous cells.

The algorithm to be presented in this paper is significantly

faster than

Fisher's, and has the same free-list restriction. While Fisher's algorithm requires

three passes over the data, the algorithm of this paper requires slightly more than

two full passes, depending on the degree of sharing in the structure to be copied. In addition, a pass here is more efficient than in Fisher's algorithm, especially

when there are many pointers to atoms.

The principal difficulty in copying lists is that several pointers can point to the .same cell. The algorithm will therefore be introduced in three stages that reflect the complexity of these multiple references. In the following section an algorithm is given for copying lists when there is no sharing at all. (This algorithm is also presented in [Cla75b]. It appears here for expository purposes.) Section 3.3 extends this algorithm to cover an intermediate case, and in Section 3.4 arbitrary list structures are allowed. A comparison of the algorithm of Section 3.4 with Fisher's algorithm is offered in Section 3.5 and Appendix C. Section 3.6 concludes the paper.

3.2 Copying a binary tree
Following Lisp conventions [McC62], assume that a list cell contains two pointers, called car and cdr, which may point to any list cell or to nonllst items called atoms. Atoms themselves are not copied. Each list cell occupies one memory

SEC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

83

location. In this paper all algorithms trace lists in the cdr direction; trace cdr before car if both point to lists.

that is, they

We will assume in this section that the list structure to be copied contains no multiply referenced cells, and is therefore a binary tree with nonlink information at the leaf nodes only. This assumption greatly simplifies the copying task. Observe that cells with at least one atom can be dealt with in a single visit: atoms are copied directly, and the list pointer (if there is one) in the copy will point to the next consecutive cell in the free area.

Clearly, then, the only difficulty in copying a structure of this kind is what

to do when a cell contains two list pointers. Were sufficient auxiliary storage

available, such cells could be stacked [Knu68, violate the constant workspace constraint.

pp.327-328],

but this would

We can evade this problem by keeping a stack in the copy of the structure itself. Fir, u re 4-3 shows how this can be done. The list structure to be copied is

shown in Figure 4-3(a). In Figure 4-3(b) cdr-direction

tracing has stopped

temporarily

at a cell with two atoms. The first four cells of the list have been

visited, and copies of cells with at least one atom have their final values. 'The

variable avail points to th_ next free ceil in the copy area.

Copie._ of cells with two list pointers have been linked together in LIFO order on the "k-list": this is a linked stack of cells on which more work needs to be done. The cars of such cells have been copied without change; this permits tracing of the lists they point to when the k-list is "popped". As further lists are traced, the k-list will grow and shrink exactly like a stack. Whenever tracing stops at a cell with two atoms, car of the first cell on the k-list is the next sublist to be traced.

h _ (a)

-o. _ m_--- A . -- 5 . ---_B _ h'" "_ I _ _{ [ + _ [ _

_
i

_

V [:A[ :I "_ _ I_1 _ Y/kk
I"'''''--'''''----'--''----------'--'------------------
,_

__ tavail

I ,--'--°"

I

ollidst _ area
new
_ list area
7,

r ....................
i

_

(c)

k kavail

Figure 4-3. Copying previous figure.

a binary tree. Dotted pointers

are those unchanged

from the

SEC.3

COPYINGLISTSTRUCTUREWSITHOUTAUXILIARYSTORAGE

65

Two crucial observations need to be made about cells on the k-list. First, the final value of cdr of these cells is a pointer to the next sequential cell in memory. This redundancy is what permits the k-list links to be kept in the new cdrs. Second, the final value of car of such a cell is the value of avail at the time the cell is removed from the k-list. This is apparent in Figure 4-3(b). Therefore both car and cdr may be given their final values when the cell is popped.

Figure 4-3(c) shows what happens after the first k-list cell is removed and

the corresponding

sublist traced. Figure 4-3(d) shows the structure after the last

k-list cell has been removed and copying has finished.

Algorithm I, below, copies the tree structure pointed to by h into the block of

free storage beginning at location avail. On termination of the algorithm, v will

point to the copy, and avail to the next available cell in the copy area. Algorithm I

is significantly

faster than the one given by Lindstrom [Lin73], which requires

three visits to each cell; Lindstrom's algorithm, however, can copy into an

arbitrary free-list.

66

LINEARIZATIOANLGORITHMS

CHAP.4

ALGORITHM I

Part A: Copy the top level of a list.

A1. [Initialize.] k,-NIL, v,-avail, and x,-h. (x points to the current original cell.)

A2. [Get new cell.] n,-avail, avail,-avail+l. (n points to the copy of x.)

A3. [process cell x.] Find the data types of car(x) and cdr(x). Go to the appropriate box below:

atom
car(x)
list

atom

car(x)

car(n)_car(x)

cdr(n)_cdr(x)

go to B 1

car(n)_avail cdr(n)_cdr(x) x_car(x)

list car(n)*car(x) cdr(n)_avail x_cdr(x)
car(n)_car(x) cdr(n)_k k_n
x.-cdr(x)

A4. [Loop.] Go to Step A2.

Part B: Find the most recently seen sublist.

BI. [Done?] Ifk=NILthenhal_.

B2. [Pop k-list.] to AZ.

t_cdr(k), x-car(k), car(k)*avail, cdr(k),-k+l,

k,-t, and go

;3.3 Copying an (almost) arbitrary list

Suppose now that the list structure to be copied may contain cells pointed to

more than once, that is, sharing and cycles may exist in the list. If Algorithm I

were applied to such a structure, the "copy" would be nonisomorphic

to the

SEC. 3

COP'liNG LIST STRUCTURES WITHOUT AUXILIARY STORAGE

67

original in that shared cellswould be copiedas many times as there were paths to them from the head of the list.And if there were cycles in the structure, Algorithm I would loop indefinitely.

The traditional method for dealing with these problems is to plant a "forwarding address" in (say)car of each cellof the originallistwhen it isfirst visited [Che70, Fis75, Lin74, Min63]. The forwarding addresspoints to the copy of the cell in which it is found. If,during tracingof the originalstructure, a forwarding address should be discoveredwhere an ordinary car was expected, a pointer to that cellcould be "forwarded" to the copy, andno spurious copies made. This technique will be used here. Since the new listarea isassumed to be a block of con tiguous locations,checking fora forwarding addresscan be accomplished by comparing a pointer with the addressboundaries of the region. Let the predicate new(x) be true ifand only ifx pointstoa cellin the new listarea.

The forwarding addresstechnique has two immediate consequences. First,an

old car displaced by a forwarding addressmust be saved somehow. This issimple

when car isan atom, for then old and new carsare the same; but ifcar isa list,old

and new cars will not in general have the same value, so the old value must be

salted away in the copy cell. The second consequence is that at some point the

forwarding

addresses must be removed and the old cars restored. This suggests a

sf,(-ond pass over the original old cars.

structure,

whose main purpose is the restoration

of

The second pass must do other work as well. Observe that with each cell of

the original list are associated five quantities of interest (old and new car, old and new cdr, forwarding address), but only four places to put them (car and cdr fields

of the original cell and the copy). Obviously, then, not all five items can be stored

between

passes; some must be computed or recomputed during Pass Two. This

68

LINEARIZATIOANLGORITHMS

CHAP.4

computation

is greatly simplified by the fact that the second pass can mimic

exactly the order in which the first pass visits cells.

It will be convenient to identify pointers as of three types, pointers to atoms,

pointers already

to cells not yet visited during the current
=
visited. Let "UV pointer" refer to a pointer

pass, and pointers to cells to a "UV cell", that is, an

unvisited one. An "AV pointer" will point to an "AV cell", that is, an already

visited one. These definitions depend, of course, on the order in which cells are

visited. An AV cell can be identified during the first pass by the presence of a

forwarding

address in its car. During the second pass, art AV cell is one in which

car is not a forwarding address, that is, a cell whose forwarding address has

already been removed. When multiple pointers to a single cell exist, one of them

will be UV, and the rest AV.

We may now make a crucial observation ahout AV and UV pointers., the new value of a_y UV pointer can be recomputed during the second pass, but the new value of an AV pointer cannot. Consider first the case of a UV pointer. If it is followed immediately after discovery during the trace, its new value will be a pointer to the following cell in the copy area. If not, it must be car of a cell on the k-list, and its new value will be the value of avail when the cell is removed. In either case the new value can be recalculated during the second pass, provided the two passes visit cells in exactly the same order. Therefore, no new values for UV pointers need to be stored during the first pass.

Now consider an AV pointer. Its new value will be the forwarding address of the AV cell it points to. But during the second pass that forwarding address will be removed before the AV pointer is encountered. Thus there is no choice but to store in one of the four available places the new value of each AV pointer found.

SEC. 3

COPYING I.IST STRUCTURES WITHOUT AUXILIARY STORAGE

69

If a pointer points to an atom, old and new values are identical, so only one value needs to be saved between passes. If a pointer is a [IV pointer, only its old value needs to be saved, since its new value can be recalculated during the second pass. Only in the case of an AV pointer must two values be preserved. Recalling that one of the available storage locations is reserved for a cell's forwarding addross, these observations mean that only cells with two AV pointers require more than four places to put things. This problem will be dealt with in Section .3.4; for the moment assume that there are no such ceils.

Note that if a cell has two [IV pointers, only three items need to be saved. The fourth place (the new cdr) can be used, happily, to store a k-list link, just as was done in Algorithm I.

Figure 4-4 shows the operation of the two-pass algorithm on a list with

shared substructures

and cycles. Figure 4-4(a) shows the original structure, and

Figure 4-4(b) shows the state just before the first cell is removed from the k-list.

Th_ cars of original cells visited thus far have been replaced by forwarding

addresses. Except for ceils on the k-list, a]l AV pointers in the copy have their

final values, these having been obtained from the appropriate forwarding

addresses. Note that in Figure 4-4(b) k points to a cell with an AV pointer in its

car. When this cell i,5 removed from the k-list, its new car must be saved, as was

observed above. Since the old car must also be preserved, it is moved to cdr of the

nf:w cell, where the k-list link is no longer needed.

"l'l_is is shown in Figure 4-4(c), in which both k-list cells have been removed and P,_ss One is complete. The "*" in the first new cdr is a marker that indicates that the cell containing it should be added to the k-list during Pass Two. This small luxury is made possible by the fact that only three quantities need be stored on behalf of such cells. (A reserved atom is not necessary for this job; any atom

It

,_ ......

lIl_l \

-_ .........

]

I

.....

_.

IILI

4/4 _ \,_ 41

_--4-z[71 !_1_i []:171!,I_1 F-l-q

1' - I_ Zt__

Lav_::

(b)

rII....

"_ ·

Ir I·

, · "_

':.'T' .__'I

m

I| _ .--. D..i

II

I

' - -i i Iii . 1, --. iIi¢., QII-I. "" r" .... ""'I" I

i iI.'I" I·

i' i #I"_r".....

"i_%1"'J#r'='='_m,'=_".iII_,lr"I,

';- :

11

_---->!;1-1I:!_! I"I_! [AI_I_--I-.I_!

<_)

I ,_.'.,-..,.,-r,-.,r,-...-_..:.. T_]

I
.1¢-......
· II

· r----._

·|

·

t
I

v-"_l.2.MJ

_

I'i_i

_ l-J
' _ il _'
II
,,_ i _ y--l-- i- !

(d)

h---gR:_---gI'T,I-[_V--l-_lI_Al_ I r .....-,r"---, r-----....· 1

(e)

Figure 4-4. Copying a list with sharing and cycles.

SEC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

7'1

will do.) Figure 4-4(d) shows the progress of Pass Two up to popping the k-list for thp first time; it corresponds to Figure 4-4(b) for Pass One. In Pass Two the k-list contains only those cells known to have UV pointers in their cars, in other words, cells found with the atom _ in their cdrs.
Algorithm II, below, will copy the list at h into the area beginning at avail, and terminate with v pointing to the copy. Step A5 carLnot deal with cells having two AV pointers; this problem will be remedied in Section 3,4.
A close examination of Step A5 will reveal that during Pass Two, the question of whether an original car or cdr is an AV or a UV pointer can be answered without fetching from memory the cell it points to. This permits the eight-way decision made in Step A5 to be repeated in Step C4 by looking only at car and cdr of the original cell and the copy. The importance of this will be seen in Section 3.4.

72

LINEARIZATIOANLGORITHMS

'

CHAP. 4

ALGORITHM II

Pass One, Part A: Copy the top level of a list.

A1. [Initialize.] k_NIL, v-avail, x_h.

A2. [Get new cel].] n,-avail, avatl_avail+l.

A3. [Save car and cdr.] a_car(x), d_cdr(x).

A4. [Store forwarding address.] car(x),n.

A5. [Process cell x.] Find the data types of a and d. ' If one is a list, check car of the cell it points to to discover whether it is UV or AV. Go to the appropriate box below:

atom

atom car(n)_a cdr(n)_d
go to B1

d AV list car(n)_a cdr(n)_car(d) go to B1

UV list car(n).a cdr(n)_-avail
x_d

a AV list
UV list

car(n)_car(a) cdr(n)_a go to B 1
car(n)_a cdr(n).-d x_a

car(n)_a cdr(n)_car(d) x_a

car(n)_-a cdr(n),-k k4-n
x_d

A6. [Loop.] Go to Step A2.

8EC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

73

Pass One, Part B: Find the most recently seen sublist.

B1. [Done?] If k=NIL then go to Step C1. B2. [Pop k-list.] t_k, k_cdr(k), x,-car(t).

B3. [x already copied?] If new(car(x)) and go to Step B 1.

then cdr(t),-car(t),

car(t),-car(x),

B4. [Leave marker, return to part A.] cdr(t)_ _, and go to Step A2.

Pass Two, Part C: Trace the top level of alist.

C1. [Initialize.]

k_NIL, avail,.v, x_h.

C2. [Get new cell.] n_avail, avail*avail+l.
C3. [Save car and cdr.] a_car(n), d*cdr(x). (Note car(n), not car(x). Note
also that car(n) may not be the original car(x); it might be the original cdr(x) instead. See Steps A5 and B2.)

C4. [Process cell x.] Find appropriate box below:
atom

the data d

types of a and list

d, and go to the

car(x)_-a atom go to DI

car(x),-a. If cdr(n)=n+ I then x*d

Otherwise, go to D I.

a

If cdr(n) is an atom then

If cdr(n)=* then car(x),-a,

car(x)_a, car(n)_avail, x*a. cdr(n)_k, k-n, x_d.

list Otherwise, car(x)_cdr(n), cdr(n),-d, and go to DI.

If new(calf(n)) then car(x),-a, car(n)*avail, x_a.
Otherwise, car(x)_cdr(n), cdr(n)_avail, x*d.

C5. [Loop.] Go to Step C2.

?'4

LINEARIZATIOANLGORITHMS

CHAP. 4

Pass Two, Part D: Find the most recently seen sublist.

D1. [Done?] If k=NIL then halt.

D2. [Pop k-list.] to C2.

t,-k, k,-cdr(k), x,-car(t), car(t),-avail,

cdr(t)_-t+l,

and go

;3.4 Copying arbitrary lists

We turn now to the problem of cells with two AV pointers. Since such cells

require, according to the current two-pass scheme, that five things be stored in

four places, another not sufficient.

method must be found for dealing with them.

Two passes are

Between the current two passes, therefore, we introduce a special pass that will handle these cells. During the first pass, no new values will be stored for them. The copy of such a cell will be in fact an exact copy of the original, the old car will get a forwarding address (as it must), and the old cdr will be used to link 'all such cells together on the "b-list".

After Pass One is finished, the b-list will be processed. When a cell is

removed from the b-list, its original car and cdr will be restored by following the

forwarding

address to the cell's copy. Furthermore, the new values of car and cdr

are the forwarding addresses of the cells pointed to by the old car and cdr. Final values can therefore be given to both pointers of both cells.

Two questions arise about the use of this technique. First, how can we be sure that the necessary forwarding addresses will still be there? Put another

way, is it possible for a b-list cell to contain a pointer to a cell already removed

from the b-list, end whose forwarding address, therefore, possible, provided the b-list is processed in LIFO order.

is lost? It is plainly

not

SEC. 3

COPYINGLISTSTRUCTUREWS ITHOUT"AUXILIARSYTORAGE

75

Second, can we stillguarantee that the second passwill be an exact replicaof the first? The problem here is that during Pass Two certain cells (those previously on the b-list)will appear to have been visitedalready,when in fact they have not been. But recallfrom Section3.3 that there isenough information in the celland itscopy to reconstructexactly Which cellwas visitednext during Pass One, and whether pointerspointed toAV or UV cells,without looking at the cellspointed to by the originalcarand cdr. W'hat will happen with former b-list cells is that they will be visitedwith the expectationof finding a forwarding address in car;ifitismissing,then the cellwas on the b-list.
Figure 4-5 shows how the b-listworks. Figure 4-5(a) isthe originallist,and Figure 4-5(b) shows the stateof affairsjustbefore visitinga celldestined for the b-list.Figure 4-5(c) shows the situationafterthe visit,which alsomarks the end of Pass One. Note that the originalcar and cdr arestoredin the copy of the cell.
Next comes the processingof cellson the b-list,during which final values are written in each originalcelland itscopy. This isdone in Figure 4-5(d). Pass Two then begins. Figure 4-5(e) forPassTwo correspondstoFigure 4-5(b) for Pass One. It also illustratesthe hazard of relyingon forwarding addresses,instead of the trace order, to set new values of UV pointers.Whereas car(k),-car(car(k)) would have worked in Figure 4-4(d),itwill clearlyfailin Figure 4-5(e) because car(car(k))isno longer a forwarding address.
Algorithm IIcan now be extended to cover arbitrarylists.First,cellswith two AV pointers should be put on the b-list(b should be initializetdo NIL in Step A I). In other words, the empty box in Step A5 should be filled with the following.

h -'-_ T1q'"_' L_-4'-__ 1
ava ii
_
k--#

(b)

. ....
/, r---ava ii
_...._ l_L/_---t-l.,.l
k--#L ................. 2

h....qTF_""

h----_!q'-'_!_t_""

;I 1 x.._l."
a/,r ...... I....... \'-V"

(c)

" ..........

,,...._.

IA_ I"I'L!

,,.... A t-" ,q,, t

( f)

;o , ,-......,.,-
Os° i!

j

(d)

Figure 4-5. Copying with the b-list.

SEC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

77

car(n),-a cdr(n).-d cdr(x),-b
b4-x
go to B 1

When executed:

Pass One finishes

(at Step B1), the following

statements

should be

1. [Done?] If b=NIL then go to C1. (Begin Pass Two if the b-list is empty.)

2. [Pop b-list.] x_b, b_cdr(b), n_car(x). (n points to the copy of x.)

3. [Set final values and loop.] a-car(n), cdr(x)*cdr(n), car(n)*car(car(n)),

cdr(n)_car(cdr(n)),

car(x)_a, and go to Step 1. (Car(x) is the last to be

written because of the possibility that x might contain a pointer to

itself.)

One change must be made in Pass Two. Each new value of x must be checked to see if it was on the b-list. If it was, car(x) will not be a forwarding address, and no work will need to be done on either x or n. We insert, therefore, the following statement between Steps Cfi and C3:
C2.5 [Was x on.b-list?] If new(car(x)) is false then go to Step D1.

Let Algorithm III denote Algorithm II with these additions. The careful reader will have noticed that Pass Two of Algorithm III can be simplified slightly if more cells are put on the b-list during Pass One. Those cells eligible are ones containing one atom pointer and one AV pointer. If AV pointers are rare overall, this change would speed up Pass Two of the algorithm. Addition of these (few)

78

LINEARIZATION ALGORITHMS

CHAP, 4

cells to the b-list would free the second pass of having to check for certain cases in Step C4. The savings would thus be proportional to the number of cell_ with one atom pointer and one list pointer (AV or UV); the cost would be proportional only to the number of cells with one atom pointer and one AV pointer.

3.5 Comparison with Fisher's algorithm

Because Algorithm III and Fisher's algorithm are both linear in the number of

"cells copied, a comparison of the two must look at how much computatational

work is done per cell, and not just at the number of cell visits, The analysis of

this section and Appendix C will measure how many times a list cell must be read

or written by each algorithm. This simplifies the comparison task considerably

by ignoring such things as arithmetic operations, instruction fetches, and the time

required by LISP primitives such as atom. For systems in which reading or

writing a list cell is expensive relative to, say, fetching an instructi0n---for

example, a system where the copying algorithm is microprogrammed

or resides in

a cache--the

analysis done here realistically measures the computational effort

involved. In other systems the work measured here is just part of the total.

The straightforward

but tedious counting of memQry accesses in the two

algorithms is done in Appendix C. The speed of each algorithm depends crucially

on the extent of multiple pointers to the same cell. For a given list structure, let a

be the fraction of cars that point to lists; let d be the fraction of cdrs that do. Let b

be the fraction of cells that go on the b-list in Algorithm III; let k 1 and k 2 be the fractions of cells that go on the k-list during Pass One and Pass Two, respectively.

Appendix C shows that Algorithm IH will execute

T = (5+2a+d+3b+2k1+2k2)n

reads and writes of list cells to copy an n-cell structure,

SEC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

79

Fisher's execute

algorithm

(as it appears, slightly optimized, in Appendix C) will

T'= (10+Za+d+c 1+2c2)n memory operations on list cells to copy an n-cell structure. The parameter c 1 is the fraction of cells whose cars were UV pointers during the first pass of Fisher's

algorithm; structures

c 2 is the fraction of cells whose cdrs were AV pointers. Thus, for with little sharing, c 1 is approximately a, and c 2 is approximately zero.

We can easily show that T is always less than T'. Because no cell can be on

both the k-list Then

and the b-list in either pass of Algorithm

III, b+klS 1 and b+kzs 1.

T = (5+2a+d+Z(b+k 1)+(b.k2)+k2)n < (9+2a+d)n < T'.

The relationships among the various parameters of T and T' can be seen if we

make one simplifying

assumption.

Observe that Algorithm III and Fisher's

algorithm do not visit cells in the same order: where Algorithm III processes

deferred list cars in LIFO order (via the k-list), Fisher's algorithm uses FIFO order

(via a sequential scan of the copy, in the manner of Cheney [Che70]). This implies

that an AV pointer for one algorithm may turn out to be a UV pointer for the

other. The assumption we will make is that the frequency of occurrence of AV

pointers in car, in cdr, and in both, will be the same for both algorithms. (Note

that the total number of AV pointers must be the same for both algorithms,

independent of the assumption.)

Under this assumption, Figure 4-6 illustrates the relationships among the parameters when cells are classified according to the table of Step A5 in Algorithm " III. Note that the containment of one area by another (e,g., area b is contained by area c 2) does not mean that particular cells in the first category are necessarily in the second; it means only that the number of cells in the first is no greater than the number of cells in the second (e.g., c 2 > b).

atom

CDR AV list

UV list

iatom

c2

!

I

CAR AV list a I b ! I i
i ,_-

I d!
I I I
I II
il ;I III Ii
_ II

UV list I

el !i I

--" -.

k2
JJ l I
7---i., I i

Figure 4-6, algorithm.

Relationships

among the parameters

of Algorithm

III and Fisher's

SEC. 3

COPYING LIST STRUCTURES WITHOUT AUXILIARY STORAGE

81

Table 4-1 shows the number of list cell references that each algorithm would

make for a variety of n-cell list structures. Both algorithms are at their best for a linear list of atoms (Table 4-1 (a)), but the extremes of relative performance come,

as might be guessed, worst case structure

when there is considerable sharing. Table 4-l(c) shows the for Algorithm III relative to Fisher's algorithm: a full binary

tree whose leaf nodes all contain two/iV pointers. For such a structure, Fisher's

algorithm would execute 1.26 times as many memory accesses as Algorithm III.

Algorithm III does best in comparison

structure

of Table 4-1(d): a list in which

with Fisher's algorithm all cars are UV pointers

for the (except,

necessarily, one) and all cdrs are /iV pointers. For this structure, twice as many

list-cell references take place in Fisher's algorithm as in/ilgorithm III.

Table 4-1(e) uses parameter values derived from measurements

of real list

structures

in five large Lisp programs, reported in [Cla76b]. That study found

that roughly one-third of cars and three-fourths

Of cdrs pointed to lists. This

means that the fraction of pointers that were/iV was .333+.75-1:.083,

since for

each list cell there is exactly one UV pointer. If we assume that/iV pointers were

as common among car list pointers as among cdr list pointers, then the fraction of

c_::lls containing UV cars is .307, and the fraction containing UV cdrs is .692. This

means that c 1:.307 and c2:.058. If we assume that car being a list is independent

of cdr being a list, then k 1=.333..692=.23 and k2=.307..692=.212.

If/iV pointers

occur independently

in car and cdr, then b=.026..058=.002.

For structures with

these characteristics,

Fisher's algorithm will execute about 1.62 times as many

list-cell references as will/ilgorithm

III.

Comparison

Table 4-1 of Algorithm III with Fisher's Algorithm

LIST STRUCTURE (a) List of atoms
a=0, d=], k1=k2=b=0, c1=c2=0
(b) Balanced binary tree a=d=k1=k2=.5, b=0, Ci=o5 , c2=0

Memory References

Fisher 's
algorithm T'

Algorithm III T

mT' T

11 n

6 n Io83

__ __

12 n

8.5 n

1.41

(c) Worst case for Algorithm 1[I a=d=l, kl=k2=b=Cl=C2=.5

ii
!

__ __
··t

14.5 n
1

(d) Worst case for Fisher's

algorithm

a=d=l, kl=k2-b=0 , c1=c2-1

, ,| _ ,

_.. · _

16 n

(e) Typical LISP case from [C_]_:i_rr_(_.i
a=o333, d=o75, ki=.23 , k = 212 b=.002, C1=o307, c2=.0582 " ,

11o84 n

11.5 n

1.26

8n 7.3 n

2 1.62

SEC. 3 3.6 Conclusion

COPYINGLiSTSTRUCTUREWS ITHOUTAUXILIARYSTORAGE
"

83

An algorithm has been given for copying arbitrarilylinked Lisp-type list

structures into a block of sequentialmemory locationswithout the use of a stack.

Under some simple assumptions about computation time, the algorithm has been

shown to be fasterthan the bestprevious algorithm,that of Fisher [Fis75]. The

speed-up factor is between I.P.6and 2, and for some liststructures found in

practiceisabout 1.62.

'

5 Effects of Linearization

The static and dynamic effects of list Unearizati'on on real Lisp

programs are the subjects of this chapter. Pointer distance

distributions programs.

following

linearizatlon are reported for the five

Over 98 percent of each program's list cdrs were

successfully located in the following cell in memory by cdr-direction

linearization.

The locality of linearized list structure tends to

deteriorate slowly during subsequent program execution, as is shown

by measurements of two programs. These measurements required the

implementation

of a working tinearizer in Interlisp, and this

implementation

is described. The effect of Iinearizatlon on paging

activity is studied.

85

86

EFFECTS OF LINEARIZATION

CHAP. 6

1. Introduction

In this chapter we investigate what happens when list llnearization is done in a working Lisp system. Linearization promises a number of benefits, which we may roughly break down into static and dynamic effects.

One clear advantage of linearization from the static point of view is that it

makes list pointers, especially those in the direction of linearization, much more

likely to point close by. The full implications of this will be explored in Chapter

6; suffice it to say for the moment that the more one knows about a pointer

without looking at it, the greater success one is likely to enjoy in representing it

in a small number of bits. This suggests that fewer bits might be used to represent

list structures.

An6ther static benefit is the more efficient use of memory: all lists

are compacted into one end of list storage. This would be true also of compaction

strategies other than linearization, of course. b_ discussed in Section 2 of this chapter.

Static effects of linearizatton

will

These static results have dynamic implications as well. If a pointer is likely

to point nearby, then it is likely to point on the same page, and is also likely therefore not to cause a page-fault in a two-level memory system. In addition, if

list pages are full, fewer of them are likely to be in a program's working set,

resulting in lower dynamic memory requirements. The effects of linearlzation paging performance will be examined in Section 5.

on

The static and dynamic results of this chapter come from two different

experimental

settings, corresponding to Chapters 2 and 3, respectively.

Static data

were gathered on the very large systems of Chapter 2., using a llnearlzation

algorithm that did not permit computation to continue after it was run. Dynamic

SEC. I

INTRODUCTION

87

data came from the more modest Chapter 3 versions of the systems, and the

linearlzation

algorithm used was considerably more elaborate than the static one.

This algorithm is the subject of Section 3.

2. Static Effects

This section addresses how linearization affects the list pointer distance

statistics presented in Section 3 of Chapter g. All of the lists whose measurements

are reported in Chapter Z were linearized

pointer distance measurements

collected

in the car and cdr directions, and the exactly as before. The linearlzatton

algorithm assumed a single block of freestorage for the linearlzed lists; Tenex page boundaries were ignored. The algorithm, approximately that of Fenichel and

Yochelson [Fen69], was used for measurement purposes only, and it destroyed original lists without creating copies. It therefore did not permit computation

the to

proceed after a linearization was done.

2.1 Poi,ter distances

Figures 5-1 and 5-Z show how linearization affects PIVOT's cars and cdrs,

respectively.

These figures include the information of Figure 2'Z. There are

several interesting points to note.

First, forward pointers in the direction of linearization (car or cdr) only have distance 1. It is easy to see why this must be so. If cdrs are linearized, for example, then there are exactly two possibilities for every list cdr: either it points to the following cell, or it points to a shared cell previously allocated. In the latter case, the shared cell must have a smaller address than the cell containing the

90

EFFECTS OF LINEAR!ZATION

CHAP. 5

cdr, and the cdr will therefore point backwards. (The backward-pointing

cdrs are

the "AV pointers" of Chapter 4, Section 3.) In fact, backward-pointing

cars also

point to shared cells, by the same reasoning; but note that forward-pointing

cars

(following cdr-direction linearization) mightalso point to shared cells.

Second, backward pointers are very rare after either kind of linearization. Roughly speaking, this says that shared cells are rare too, but the situation is a little confusing. There might, after all, be many pointers to a single cell, and it is also possible for a shared cell to be pointed to by forward pointers only. t

Third, whenever one pointer in a cell is a list and the other is a nonlist, the list pointer will point to the following cell after either kind of linearization. For example, in Figure 5-2., 75.5 percent of PIVOT's list cdrs point to the next cell after cars are linearized. This is obviously due to the distribution of data types in car and cdr: car is a nonlist most of the time, and cdr is a list most of the time.

Fourth, an interesting characteristic of list structure is revealed by the results for pointers not in the direction of linearization. Consider a cell whose cdr points to a nonshared list. After the linearization of cars, that celt's cdr distance will be exactly one greater than the size, in list cells, of the entire structure pointed to by car. We noted above that a cdr distance of one corresponds to a car structure of size zero, namely an atom. Likewise, the car distance after cdrs are

"l'Measurements made by Cordell Green [Gre75], using slightly different versions of the programs and a
different tracing technique, found that between 1.4 and 2.4 percent of each program's list cells were pointed to more than once. Most of these cells were pointed to Just twice. The most frequently referenced cell in each program attracted between .1 and ,5 percent or all list pointers (as many as several hundred pointers).

SEC. 2

STATIJC EFFECTS.

B1

linearized gives the size of the cdr structure, provided car points to a nonshared

list. We have no way of knowing, unfortunately,

the size of the cdr structure

when car is an atom, or the size of the car structure when cdr is. But Figures 5-1

and 5-2 can give us the distributions of the sizes of (nonshared) cdr and car

structures,

respectively, when the other pointer is a nonshared list too_ the two

relevant curves are forward-pointing

cars after cdr-direction

linearization

in

Figure 5-2..

5-1, and forward-pointing

cdrs after car,direction linearization in Figure

The point of all this is that those curves show that car structures tend to be

smaller than cdr structures.

(This statement is a little strong: as was mentioned

above, if one pointer is a non-list, Figures 5-1 and 5-g reveal nothing about the

size of the other pointer's structure, if there is one.) Note that the data type

information

of Chapter 2 does not predict this, although at first glance it might

appear to. Figure 5-3 illustrates. It shows two list structures of 16 cells whose

data type distributions are identical and are in rough agreement with the Chapter

P. results. Figure 5-3(a) is a short, deep, car-heavy structure, whereas Figure 5-3(b) is long, shallow, and cdr-heavy. In (a) each cell with two list pointers has

car pointing to a bigger structure than cdrl in (b) the reverse is true. Figures 5-1 and 5-2 tell us that PIVOT's list structures are more like (b) than (a), but the

Chapter 2 data alone could not distinguish between the two. This result a_rees

with our intuition about Lisp lists: long shallow than short deep ones.

lists are more likely

to occur

Appendix A reports in detail list pointer measurements for all five programs

before and after linearization.

All are very much like PIVOT. Table 5-1

summarizes these data by showing the percentage of pointers pointing to the next

cell after each kind of linearization.

Perhaps the most striking feature of Table

(a)

_KIo, _LVI

M1
i_1°I , _4Trd--_ JIM
I

;I
li I
i',ii - i

II_'AI - i

"1_ ly I

.J_l-, .JHIA

_ IEI ,-l-_ FL/d

!_!-I _
( ( (((ABCD)

--J,'cl-,! _DVI
EF) GH) IJ) KL)

(b)

_1 ,,]q_lCl .-_!',,I--_>t"1F"-t->t1,.,-t-->it.1-4-->Il.,-,-I-->IK1I"

'_I '_f ,,
l'kl--]-_aL/1 I[)i--_EV1

f
!(__1

,
__ [-_;1

((AB)C(DE)F(GH)I(J)KL)

Figure 5-3. Two list structures with identical data type distributions in car and in cdr.

SEC. 2

STATIC EFFECTS

93

5-1 is that over 98 percent of each program's list cdrs were successfully

linearized. the cars.

Note also the high percentage of cdr successes even.after linearizing

Table 5-1 Percentage of list .pointers pointing to following cell

after car-dir. lin.
after cdr-dir. tin.

NOAH

CAR 79.5

CDR 77.4

CAR

R7.3

CDR

98.8

PIVOT 95.7 75.5 41.7 99.1

SPARSER CONGEN 78.5 93.6 79.3 71.2 33.3 24.3 98.7 98.3

WIRE 90.2 77.1 19.1 99.3

2.2 Pointers on the page

Another way of comparing the two types of linearization is to examine their

effects on the percentage of list pointers found pointing on the (5 lfi-cell) Tenex

page. This is shown in Table 5-2. Note that at most .6 percent fewer list cdrs point on the page after car-direction linearization than after cdr-direction, while

car-direction

linearization puts from 5 to 10 percent more cars on the page than

does cdr-direction.

Table 5-3 shows the percentages of all list pointers-car

and

cdr---that point on the page before and after linearization, and car-direction

linearization

does marginally better than cdr-direction, according to this measure.

That it does not do dramatically better, as might be suggested by Table 5-2, is due to the fact that list pointers are much more common in cdr than in car.

94

EFFECTS OF LINEARIZATION

CHAP. 5

Table 5-2 Percentage of list pointers pointing on the .page

NOAH PIVOT SPARSER CONGEN WIRE

CAR LIST POINTERS

before after

after

lin. car-dir, cdr-dir,

79.1 80.6 86.5 97.1 74.0 84.7 87.8 97.6 55.9 90.3

74.3 92..7 75.9 90.7 80.0

CDR LIST POINTERS

before after

after

lin. car-dir, cdr-dir.

89.8 98.1 92.7 98.9 89.5 98.4 96.2 98.2 97.0 98.8

98.7 99.3 98.9 98.8 99.1

Table 5-3 Percentage of all list pointers pointing on the page

before linearization

NOAH PIVOT SPARSER CON GEN WIRE

86.7 90.8 84.9 93.8 87.3

after car-direction
93.0 98.4 94.3 98.0 96.8

after cdr-direction
91.6 97.3 92.0 96.5 94.6

SEC. 3

MINSKY'S LINEARIZATION ALGORITHM

3. Minsky's Linearization Algorithm

95

3.1 Why another algorithm?

The list-moving algorithm of Chapter 4 was not the one chosen for dynamic.

linearization

experiments.

The reasons for this are pragmatic. Recall that the

aJgorithm requires the original list structure and its copy to be present in memory

simultaneously.

This will sometimes be impossible in an Interlisp system, since

there may, in fact, be no list storage available when it is desired that linearization

be done. Clearly, then, secondary storage will have to be used in some way.

It is also true that the constant workspace constraint satisfied by the Chapter

.4 algorithm is unnecessarily stringent for an Interlisp linearizer. There is in the

system, after all, a fine stack mechanism of capacity sufficient collection. There is also a mechanism, used by the regular garbage

for garbage collector, for

rnarkin_ individual list cells using a bit-table. We therefore want an algorithm

that does not require that space for the ]inearized lists be allocated beforehand, that may use a stack and mark bits if it needs them.

but

Enter Minsky's algorithm [Min63], which turns out to be exactly what is

needed. It operates by creating on secondary storage a file of triples (x,y,z) such

that in the linearized list, location x should be a list cell whose car is y and whose

cdr is z. This file is created during cdr-direction

traversal of the original

structure, and is not read until the traversal is finished, whereupon the entire file

is read back on top of the original. Forwarding addresses (see Chapter 4) are stored in cdr of the original cell, and a single mark bit is needed for each cell. Recursion is used when car and cdr both point to unmarked lists.

98

EFFECTSOF LINEARIZATION

CHAP. 5

3.2 Description of the algorithm
We will need some definitions and auxiliary functions before introducing Minsky's algorithm. First, let the function mark[x] plant a one-bit mark in cell x, and let mkd[x] be true if and only" if x is either a marked list or an atom. The function v[x] will be the post-linearization, value of the pointer x: if x is atomic, then v[x] = x; otherwise v[x] is x's forwarding address, namely, cdr[x]. Finally, the function write[x;y;z] will write the triple (x,y,z) onto secondary storage. When read back, (y.z) should be stored at location x. The global variable n will point to the next available free location, and should be initialized appropriately. Algorithm M, below, is a slightly rewritten version of Minsky's original, and corrects a minor bug.

Algorithm M

I. [Save car and cdr.]a,-car[x]d,,-cdr[x].

ft. [Mark cellx, storeforwarding address.]mark[x], cdr[x]_-n.

3. [Write tripleforx.]Go to the appropriatebox below:

mkd[ d]

not mkd[d]

mkd[a]

write[n;v[a];v[d]] n_n+l return

not mkd[a]

write[n;n+ n_n+ 1 x_a go to Step

1;v[d]] 1

.......

write[n;v[a];n+ n*n+l x,-d

1]

go to Step 1
,, i

n_n+ 1 linearize[d] Ifmkd[a] then
and return.

write[cdr[x];v[a];v[d]]

Otherwise, write[cdr[x];n;v[d]], x.-a, go to Step 1.

SEC. 3

MINSKY'S LINEARIZATION ALGORITHM

97

Algorithm M traverses listsin the cdr-direction, just as do the algorithms of

Chapter 4. Itskey idea is shared with those algorithms: if at most one pointer in a

cell is a list, then everything---its new location, its new car, its new cdr---is

known at the time of the first visit to that cell; only if both pointers are

unmarked

lists must the cell be visited a second time. Second visits are

accomplished by calling the algorithm recursively on cdr, then writing out the

triple for the original cell, and then traversing car. The least transparent part of

Algorithm M involves these recursive calls. In the lower right box of Step 3, the

principal difficulty is that during the recursive application of the algorithm to

cdr[x], car[x] might itselfbe encountered. Therefore car[x] must be checked for markedness after the execution of linearize[d] in Step 3.

The recursive calls can stack as deep as the longest cdr-direction chain of cells whose cars are unmarked lists. As Minsky pointed out [Min63], a smaller stack would be needed if car-direction traversal were done instead.

3.3 Implementation

of the algorithm

In a system containing pointers, moving things around is a very tricky business. One must be certain that all pointers to moved objects are correctly updated. Care must also be taken that objects not be moved on top of other objects legitimately pointed to elsewhere in the system. The implementation of Minsky's linearization algorithm in Interlisp,therefore, is a good deal more involved than the simple statement of Algorithm M might suggest. (This is a common property of _arbage collection algorithms presented in isolation from running systems. Whereas a few column inches in a journal article may suffice to state the algorithm, an actual implementation invariably involves a host of other considerations that make the resulting code enormous and unfathomable by comparison.)

98

EFFECTS OF LINEARIZATION

CHAP. 5

It should be clear that linearizing all lists in the system amounts to a standard garbage collection of list storage. The approach taken in implementing Algorithm M, therefore, was to patch it into the Interlisp garbage collector. This takes care of several problems but introduces some others.

The chief advantage of doing things this way is that existing garbage

collection mechanisms pointer in the system.

can be used for the tedious task of finding every list List pointers can be found within lists and associated with

atoms, obviously, but in Interlisp they may also be present in arrays and hash-

arrays, on the stack, in user-defined data types, and elsewhere. (See [Tei74] for details.) Relying on the garbage collector ensures that all active lists are found and

that no garbage lists are.

Some drawbacks of this approach result from concessions that were made to the garbage collector. First, not all lists were linearized. There is in Interlisp a way to declare certain pages to be part of the system and therefore not susceptible to garbage collection; this facility allows the sharing of pages between users and speeds up the garbage collection process. Insisting that these system lists be linearized would have required numerous additional changes to the garbage collector and did not seem worth the trouble, given the relatively small amount of system list structure in the programs, and the small amount of observed dynamic reference to system lists.

Another minor disadvantage of relying so heavily on the garbage collector

was that data types other than lists were garbage-collected during a linearlzatlon.

(Specifically,

data types of fixed leng, th---atoms, numbers, and others--were

collected.

See [Tei74] for details.) Preventing this from happening, just as

insisting on the linearization of system lists, would have entailed more extensive

overhaul of the garbage collector than seemed worthwhile.

$EC. 3

MINSKY'S LINEARIZATION ALGORITHM

99

So in the rest of this chapter it should be understood that when a linearizatlon

is done, system collected.

lists are not included,

and fixed-length

data types are garbage-

4. Persistence of Linearity

We will look in this section at the question of whether linearized list

structure tends to remain linear during subsequent program execution, or tends to

"deteriorate."

This bears crucially on how often the relatively expensive process

of linearization needs to be done in order to maintain a desired level of benefit.

The following experiment was performed in CONGEN and SPARSER. A task

big enough to force several garbage collections of list storage was executed in

three different

settings:

(1) with no linearization

whatever;

(2) with

linearization

done before starting the task; and (3) with linearization done

roughly in the middle of the task. List pointer distance distributions were measured at the end of the task. The goal here was to see how much deterioration

of linearity would be observed with various linearization strategies. Note that

because data are gathered at the end, linearizations other than the most recent one

are irrelevant; it matters not, in setting 3 above, whether an initial linearization is

done. Note also that, as in most list processing situations, these tasks started with

some active list structure, created more during execution, and garbaged some of

both type.s. The vital statistics are as follows: SPARSER started with 24K active

list cells and ended with 38K, enduring three garbage collections along the way;

CONGEN began with 5K, finished wi_h 35K, and collected garbage three times.

Things were arranged so that the list storage allocated by Interlisp to the

programs---45K runs.

cells in SPARSER, 80K in CONGEN--was not increased during the

100

EFFECTS OF LINEARIZATION

CHAP. 5

Figure 5-4 shows the results of this experiment. Pointer distances are

measured

in a way slightly different from previous figures: cumulative

percentage is shown versus distance in either direction, as if a figure like 5-2 had

been folded along its vertical axis, overlapping columns summed, and the

resulting curve made cumulative.

These plots mask some information,

but are

compact and sufficient for present purposes.

As we would surely expect from a cdr-direction

linearizer, cdr pointer

density decreases as the most recent linearization fades into the past. The car

situation

is somewhat different.

SPARSER's cars, except for those pointing

forward or backward less than 4 cells, are affected very little by linearization.

In

CONGEN, the mid-run linearization initial linearization does little.

seems to make car distances greater, while the

Two broad effects can account for the persistence of linearity. These are the

tendency of pre-linearization

structures (old lists) not to get changed very much,

and the tendency of structures created after a linearization (new lists) to exhibit

greater-than-average

pointer locality. These tendencies were measured in the

following way: after an initial linearization, the tasks of Figure 5-4 were run up to their first garbage collection, and list pointer distributions were measured in

the now-familiar

way, but separately for old and new lists. New cells dominated

old cells in CONGEN (14K vs. 5K), but there were more old cells than new in

SPARSER (24K vs. 9K).

Figure 5-5 shows the results. The most striking feature is the persistence of linearity in the old cdrs. It appears than old cdrs are not being changed (with rplacd) very much. Neither, it turns out, are old cars, although this is not apparent in Figure 5-5. Compared with unlinearized cdrs in Figure 5-4, new cdrs are better (more linear) in SPARSER and about the same in CONGEN.

10 z

[.log:, distanceJ 4 8 12

16

_)og2 distanceJ 10 . . . 4.. "_-8 L-" " !2 ;-" " 1.6.

_.61 //
_t/

ff
__.6/
cAR _

CDR

0 "2_

_ °.24
(a) SPARSER
NO LINEARIZATION : _ _- LINEARIZATION AT START ...... LINEARIZATiON IN MIDDLE

Llog2 distanceJ

10 . . . 4 . . , .8..

12

16

o_0-8.
Z /l
I:1::

/ it/
I

/ -> t
,:::3

..... _..,
cAR

10
b-.8
_1

[Jog2 distanceJ 4 8 12

16

cDF_

0 °2 0 '2 _ (b) CONGEN

Figure 5-4. Effects

cumulative direction.

fraction

of three linearization schemes. Each plot shows the of list pointers versus the logarithm of distance in either

[log2 distanceJ 10 4 8 12 16
· " ' " " ' ' " "' ' ' " ' "' "
z
i0-- .8
,< ..-1n4,".

0 1
z
0.8
<: LL

__

CAR

_

Z3 Z3

:_.4, 0

D.4, 0

,2 .2 (a) SPARSER

Uog2 distance] 4 8 12

f
//
z

I -'-'''_

16

CDR

--" NEW LISTS - · OLD LISTS BOTH

10......

Uog2 distance] 4 --.,...., 8 , . .1..2..

, , 16

¢/
g.8 /
ii
-L>U.6 //
d
_.4 0

CAR

Llog= distanceJ 10 £- , ,.,, 4 · _ " 8, ;, , , 1,2 · . , 1,6 ,

z
0.8//
11
-L_>U.6
_j
_.4, 0

CDR

.2 .2
(b) CONGEN

Figure 5-5. Linearity of old (pre-linearization) cells.

and new (post-linearization)

list

SEC. 4

PERSISTENCE OF LINEARITY

103

New and old cars are interestingly different in the two programs. Many explanations of this can be imagined. For instance, if cdr structures tended to be large, linearization might well make cars less linear, as happens with CONGEN in Figure 5-5. If cdr structures were small instead, or if the creation of car pointers often occurred a long time (many conses) after creation of the cells pointed to, then linearization would improve the cars, as happens in Figure 5-5 for SPARSER.

5. Effects of Linearization on Paging

The possible improvement of paging performanc_ due to list-linearlzation

was

investigated with the experimental apparatus of Chapter 3. The PDP- 10 simulator

developed at Bolt Beranek and Newman by Robert Bobrow [BobR76] was used to generate a trace file of all references to list cells during (of necessity) short runs of SPARSER and CONGEN. This was done with and without an initial

linearization.

The efficiency of various sizes of list memory using an LRU page-

replacement

strategy was then evaluated with the algorithm of Mattson, et al.

[Mat70], just as was done in Section 7 of Chapter 3.

We might reasonably expect performance to improve substantially

when an

initial linearization

is done. There is, first of all, the memory compaction

achieved: fewer pages, potentially many fewer, contain lists after lineartzatton than before. Then there is the concentration of list pointers on the page: since a

pointer, especially a cdr, is more likely to point nearby after linearization, it is less

likel3r to point off the page and therefore also less likely to cause a page-fault

when it is traversed during execution. There is finally the effect on the free-list:

linearization

leaves the free-list perfectly linear, linking together a single block

of contiguous (conses).

cells, thus encouraging pointer locality in subsequent cell-creations

104

EFFECTSOF LINEARIZAT[ON

CHAP.5

..

In view of these arguments, percentage of list cell references

Figure 5-6 is a little surprising. It shows the in the runs of SPARSER and CONGEN that are to

the k most recently referenced pages, for k from 1 to the number of list pages in

the programs. These runs occurred just after the time of the mid-stream

linearization

in the tasks analyzed in Section 4. For the runs without

linearization,

a standard garbage collection was done instead, to compensate for

possible extra-linearization

effects of the linearizer, discussed in Section 3. Vital

statistics: CONGEN's run consisted of Z01,000 references to 72 list pages without

linearization,

60 pages with; SPARSER's contained 312.,000 references to 9P. pages

without linearization, 90 with. Figure 5-6 shows that the linearized curve lies

above the unlinearized one at a roughlyconstant

small distance: between 2 and 3

percent for SPARSER, a fraction of 1 percent for CONGEN.

We considered

three propositions in support of substantial

paging

performance

gains from linearization: memory compaction, concentration

of

pointers, and linearity of the free-list. A closer look at each of these will reveal

why their effects appear minor in Figure 5_6.

Consider first the compaction argument. If a program's lists were thinly

scattered over a large number of pages so that each page contained many garbage

cells, then linearization would probably deliver large performance gains which would show up in Figure 5-6. Since no such gains appear, it must be the case

either that both programs' lists are fairly well cencentrated on the page without

linearization,

or that a small subset of all list pages attracts a disproportionate

number of references. A quick check of the density of live cells on the page in

SPARSER showed that at least the first of these propositions is true.

Now consider the pointer concentration argument: that linearization increases the number of on-page pointers and therefore decreases the likelihood of faulting

NUMBER OF PAGES

1

4 10

40

CONGEN

.._"
so .I .s

s*

I f f
/ !
/
/
Z / / SPARSER
0 //
--- /
I0-- //
'_ //

/
uJ /
>/
-- /
U/

with linearization without

_ .85- /
0/
!
/ ! l

100

.80-
Figure 5-6. References to list pages in CONGEN and SPARSER, with and without initial llnearization.

106

EFFECTS OF LINEARIZATION

CHAP. 5

when traversing one of them. This is quickly defla_ted by the observation that only a small increase in on-page pointers occurs when lists are linearized.

Section 2 showed this increase to be around 8 or 10 percent (of all list pointers). Figure 5-4 offers further support. Notice that although the curves for cdr show a dramatic increase in small pointer distances through linearization, at distances of 2.7 or 2 8 (around the size of a page), over 90 percent of cdr list pointers are accounted for, whether or not linearization is done. The distribution of car list

pointer distances other than small ones is little affected by (cdr-direction)

linearization,

as Figure 5-4 shows. Thus the effects of linearization are are not

strongly together.

in evidence

when distances UP to about the page size are grouped

Consider finally the free-list argument. A comparison of Figures 5-4 and 5-5

will show why this argument carries little weight. The curves for new (i.e.,

post-linearization)

list cells in Figure 5-5 are not very different from those for

unlinearized

lists in Figure 5-4. In other words, the long linear free-list is not

sufficiently

different from the free-list that exists without linearization,

to

increase substantially the pointer locality of new cells.

From the point of view of Figure 5-6, then, linearization seems to have only a small effect on the paging activity of these two programs during the measured

runs. But there is an important pragmatic issue that we have not yet discussed:

the cost of a page-fault. If we make the reasonable assumption that a reference to

secondary memory takes on the order of 5000 times longer than a reference to

primary memory, then a success rate of 90 percent, or even 99 percent, give miserably poor performance.

would

Figure 5-7 shows observed page-fault probability versus number of pages in

core on a semilogarithmic

plot which affords a closer look at what happens for

SEC. 5

EFFECTS OF LINEARIZATiON ON PAGING

107

moderate to large numbers of pages. These curves are Just the curves of Figure 5-6 subtracted from 1.

Consider SPARSER, for which linearization does much more than it does for CONGEN. If 44 pages of lists are kept in core and replaced according to the LRU rule, SPARSER would experience a (list page) fault probability of .005 if lists had not initially been linearized, but .001 if they had. Under the assumption that 1 fault takes the time of 5000 successful references, this 44 page limit would mean that the average list cell reference would take more than four times longer if lists were not first linearized. Another way to 10ok at the data is to ask how many list pages must be in core to achieve a given fault rate, say .001. For SPARSER, the answer is 65 pages without linearization, 44 with---a full one-third reduction in dynamic memory space devoted to lists.

In CONGEN the effect of linearization is not nearly so strong, but is still

significant.

If 23 list pages were kept in core, CONGEN would endure a page-fault

rate of .0016 if lists were not first linearized, .001 if they were. This disparity

would yield a slowdown of 1.67 for list cell references during the run without

linearization.

To achieve a pa_e-fault

would be needed.

rate Of .001 without

linearizing,

27 pages

So although linearization results in just a small increase in the percentage of

successful references to the most recently referenced pages, when the high cost of

page-faults is taken into account, this small increase can yield a large reduction in

the time taken by references to list cells. This conclusion can only cautiously be applied to other programs, particularly since the tWO measured programs differed

significantly

in the degree to which linearjzation helped.

6 " Compact Encodings of List Structure

In this chapter we show how the regularities discussed in Chapter

2 can be exploited in the design of space-efficient

encoded

representations

of list structure. Several examples of such encodings

are presented and evaluated. Alternative methods of dealing with

"escapes"--exceptional

pointers that do not fit into a particular

encoding--are

discussed. Lower bounds on the space efficiency of

encodings are given by the entropy of car and cdr, which is calculated for each of the systems of Chapter g. Linearization, of course, greatly

improves the space-efficiency

of a list structure encoding. Various

questions of cost and implementation are discussed.

109

! 10

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

1. Introduction

The data of Chapter 2 show that pointers in list structures are far more likely to have some values than others. List pointers, in particular, if expressed as relative offsets, are quite likely to have one of a verysmall range of values. These characteristics of lists suggest that if some sort of "compact encoding" were used, the average size of pointers could be reduced below the 18 bits used in Interlisp.
A homely example of this idea is Morse Code: in order to reduce the size of messages, letters of the alphabet are represented by binary codewords whose length is inversely proportional,roughly, tO the frequency of the letters' occurrence in English. Thus the common letter "e" iS represented by a single "dot", while rare letters such as "j" and "q" require:four dots and dashes each.

Perhaps the most obvious list structure encoding--omitting

explicit cdr

pointers altogether and using physical adjacency instead--has been proposed by

Hanson [Han69], Greenblatt [Gre74], and. Van der Poel [Van?]; Bobrow suggests a

more complex scheme involving hash linking [Bob75]. These ideas will be

discussed

and analyzed

possibilities.

in the coming sections,

along with several

other

The encodings discussed in this chapter will be restricted in the following way. We will assume throughout that all list cells occupy one word of memory, and that car and cdr fields are of fixed size in each word. The car and cdr fields will not be big enough to represent all possible values of car and cdr; therefore some "escape" mechanism will be needed to give access to a longer pointer when one is needed. These escapes will be specified by particular reserved bit-patterns, or codewords, in the small fields. The early sections of this chapter will ignore

SEC 1

II_ITRODUCTION

III

tile particular mechanism used for escapes, concentrating only on minimizing h_w often it is needed. Section 4 will deal with escape possibilities in some detail.

Although the data used here come from a PDP-10 implementation

of Lisp, we

will not be bound in this chapter to the word size of that machine, or in fact to

any particular hardware configuration at all. Discussion of some of the hardware

issues that arise will be gi-,_n in Section 7.

2. A Simple Example

Suppose that we want to represent list structures like those of SPARSER using l_-bit cells instead of the 36-bit cells used on the PDP-IO. In this section we will

il; formally develop a zimplp encoding for SPARSER using 18-bit cells. The design

will be based mainly on reasonable intuition about SPARSER's empirical

characteristics;

a more rigorous approachwill be taken in Section 5.

The first i._._1_oto be :;r'ttlod is the allotment of bits to car and cdr. Recalling

th_ distrib_]tion

of data types in car and cdr, and the observation

that atom

pointers, plentiful in car, are distributed roughly according to Zipf's law, it would

st,pro rea._onable to give more bits to car than to cdr. Cdrs, after all, are mostly NII,

or lists a short distance away, whilp cars often point to one of a largo numb_r of atoms. In the ab._,nco, a_ 1he moment, of strong reasons for any particular

allotment, let us give 12, bits to car and 6 to cdr. There are then 21_'=4096 possiblo "compact" cars and _6 = 64 cdrs.

The next issue is the implementation

of escapes from the small pointer fields.

This will be addressed in detail in Section 4. For purposes of the present encoding,

one of the _implest schemes will be used: one valise of car and one of cdr (all

1 1 2.

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

zeroes, say) will indicate that the true value of the pointer in question is to be

found in a global hash table. When (say) car[x] has the reserved value, the address x will be hashed to produce an index into the table, wherein the true

value of car[x] can be found, This idea is called "hash linking" by Bobrow [Bob75]. (Actually, since car and cdr could both have the escape value, one more

bit would need to be hashed along with x. Alternatively, functions or two separate tables could be used.)

two different hash

Of the 64 possible cdrs, one is used up by the escape code, and one should clearly be used for NIL, the atom that accounts for about one-fourth of all cdrs. Other atoms, as well as nonlist pointers, are rare in cdr, so the allocation of some few codes for the most common atoms or arrays or whatever would probably be a bad idea. Better, surely, to use all the remaining codes for relative offset list pointers, which are known to have a reasonable payoff_ All nonlist pointers other than NII., then, will be required to use the escape, Let the 62 list codewords represent the offsets -31 to +31, omitting 0. (The 6g-wide "window" could, of course, be placed anywhere, but this choice is the simplest.)

The allocation of car codes is more difficult, since other data types abound in car. One code for the escape is needed, and one also _or Nil, (,5,,6 percent of cars in

SPARSER). For simplicity, let us give car the same ability to address lists as cdr, namely 62 codewords representing the pointer distances -31 to +31. Atoms

account for 31.5 percent of SPARSER's cars, and small integers for 21.1 percent.

Let us arbitrarily decide to let cars directly specify small integers in the range -29 to +29, consuming 1025 of the available car codes_ (Recall that Interlisp small

integers are in the range -1536 to +1536.) There remain 4096-1-1-62-

1025 = 3007 codes for atoms, which will turn out to be more than enough SPARSER.

for

St:_ _

A SIMPLE EXAMPLE

I 13

We have now specified a simple encoding scheme for car and cdr. What

savings in space, if any, will this design have over the Interlisp implementation

of

list cells? The empirical data of Chapter 2 and Appendix A provide almost enough

information

to calculate this savings for each program. We need only calculate,

for a particular program, the percentages of cars and cdrs thai will fit into the

small fields. Those values that require the escape to be used will need additional

bits, tho exact numbpr depending on the escape implementation

chosen. For the

moment, let c and c be the numbers of bits consumed by escape values of car and

cdr, rpspoctively, over and above the bits in the codeword itself. Let p and I! be the

fractions of cars and cdrs that fit into the small field. Then the average number of

bits required by car and cdr are 12+( 1-p)c and 6+( 1-_)c, respectively.

Values of p and t_ for SPARSEB can easily be found. Consider first p. All NILs

will fit into the encoding, accounting for .242 of all cdrs. List pointers with distances between -31 and +31 account for .811 of list cdrs (see Appendix A); lists

are .737 of SPARSER's cdrs; therofore .811..737 = .598 is the fraction of all cdrs

that are lists representable by a short pointer. Cdrs that are neither lists nor NIL

need the .242+.598

escape value and another c bits each. The value = .84, and the average size of a cdr is 6+.16,_c bits.

of t! is thus

Notice that c_ (and also c) is apt to be rather large: if a hash table is used, the simplest possible scheme requires an 18-bit key (to resolve collisions) and the 18bit true value of cdr. With c=36, the average siz_ _ of a cdr in SPARSER is 6+.16.36 = 11.76 bits. (In fact, c should be larger still, to account for the hash table's not being completely full. More on this in Section 4.)
Similar calculations can be made for SPABSEB's cars. The atom NIL accounts for .056 of them; list pointers with distances in the range -31 to +31 account for (according to Appendix A) .654 of list cars, and .206 of all SPARSER's cars. If we

114

COMPACTENCODINGOSFLISTSTRUCTURE

CHAP. 6

assume that all small integers used by SPARSER are actually in the range -512 to

+512--a reasonable and supportable approximati0n--then

.211 of the cars will

specify small integer._. SPARSER only has 2544 atoms (that are pointed to in the

list structures of Chapter fi), so the 3007 available codes will handle all of them,

for another .398 of the cars. This gives p - .056..206+.211+.398

= .871, leaving a

remainder of .12.9 that will use the escape mechanism. The average size of a car

under this encoding comes to 16.64.

is then 12+. 129.c bits. If c is 36, the average bit commitment

Although the present encoding was designed with SPARSER in mind, it seems

r oasohable to repeat the average bit calculations for the other four programs,

particularly Table 6- 1.

in view of the similarity

among them. These results are shown in

NOAH PIVOT SPARSER CONGEN WIRE

Table 6-1 Efficiency of a simple encoding

p (car)
.894 .820 .871 .914 .859

]2 (cdr)
.862 .845 .840 .941 .944

avg. size of car with c=36
15.82 18.48 16.64 15.10 17.08

avg. size of cdr with c=36
10.95 11.58 11.76 8.14 8.02

There are several interesting characteristics of this table. First, although the encoding was designed specifically for SPARSER, two programs have smaller average cars and three have smaller average cdrs. This is very encouraging (though not altogether surprising), for it suggests that the idea of having only one encoding in a Lisp system is reasonable. Second, cdrs have been compressed by the

SEC. 2.

A SIMPLE EXAMPEE

I 15

encoding with considerably more success than have cars. Of course, we did start

out with a car field twice the size of the cdr field, but there is still more going on here, as will be shown in a precise way in Section 6 of this chapter. Third, this

"compact" of which encodings

representation

has done nothing good for PIVOT's cars, the average size

(with c=36) is a tad greater than the unencoded size of 18 bits. Better

are needed.

3. Linearization

We have considered up to now only encodings of list structures that preserve

the original locations of their list cells. Linearization of these lists, explored in

Chapter 5 as a means of strengthening

their structural regularity, will clearly

improve encoding possibilities.

There are, of course, numerous imaginable

relocation schemes that might make encoding easier, but here we consider only

car-direction

and cdr-direction

linearization.

Linearization

only alters the

fro quoncy distribution

of list pointer distances; pointers to other data types are

unaffected.

Thus any payoff in increased encoding efficiency will come solely

from the heightened probability that a linearized list pointer points close by.

Cdr-direction

linearization suggests a very attractive encoding for cdr: two

bit._ would be used, and of the four available codes, one would stand for NIL, one

for the relative list pointer +1, and two would be available for escapes. (One of

the.qo e.scapes is free, in a sense; only three cOdes are needed, but they require two

bits.) Recall that after cdr-direction

linearization over 98 percent of every

program's list cdrs were successfully placed in the next cell, and that almost all

nonlist cdrs are NIL. In SPARSER, for example, .242 of all cdrs are NIL, and

lJnearization

succeeds on .987 of the list cdrs, which comprise .737 of all cdrs.

'1 1 6

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

The two-bit cdr encoding would thus handle, immediately after linearization, .969 of SPARSER's cdrs without using the escapes.
Table 6-P.shows the proportion,l!,of cdrs accomodated without using the escapes, and the average sizeof cdr if the escape costc is 36 bits,for the five programs under this two-bit encoding. These figures should be compared with the ones of Table 6-I.

Table ,6-2 Efficiency of a 2-bit cdr encoding

NOAH PIVOT SPARSER CONGEN WIRE

avg.sizeof cdr p with £=36
.980 2172 .997 4,63 .969 3. I P. .974 2.95 .989 Z.40

Appendix A shows that cdr-direction linearization does not have a strong effect on the clustering of list cars; the encoding efficiency achievable for car would therefore not be much changed.

The two-bit cdr encoding is approximately that used in several "half-cell"

list-processing

systems. Van der Poel's HISP [Van?] and Hansen's SWYM system

[IIan69] both rely on physical adjacency, rather than explicit pointers, to indicate

links in the cdr direction. Both require, obviously, linearizing garbage collection. Both use mark bits of various kinds to indicate a cdr of NIL, the next ceil, and

several kinds of escapes. (Hansen's system also prohibits the rplacd operation of

Lisp.) Greenblatt's proposal for a Lisp machine [Gre74] allows cdrs of several

SEC. 3

LINEARIZATION

I 17

kinds, including the conventional full machine address. A two-bit field indicates which. None of these three systems encodes the car field at all.

While linearizing cdrs alters the potential efficiency of a cdr encoding

dramatically without affecting cars very much, golnginstead in the car direction, as we observed in Chapter 5, improves both by a moderate amount. Consider once

again the example of SPARSER. Appendix A shows that after car-direction

linearization .785 of SPARSER's list cars point to the following cell, as do .793 of

list cdrs. Allowing cdrs to address the nearest 14 forward distances (leaving one

escape plus NIL in a four-bit field) gets .951 of them, almost as many as with the

two-bit

encoding and linearized cdrs. A quantitative

measure of the

"encodability"

of list pointers after linearizatlon will be given in Section 6.

4. Escape Mechanisms
There are a variety of mechanisms for handling the small fraction of pointers that will not fit into a compact encoding. The possibility of using a single global hash table was briefly considered in Section 2; here we will discuss that method, and others, in greater detail. There will, in general, be two important criteria by which these schemes must be judged, first, the cost in extra bits per escape pointer, and second, the cost in execution speed of using a particular method. We will usually be concerned with a uniform escape mechanism, i.e., all escapes are handled in the same way.

II8

COMPACTENCODINGOSFLISTSTRUCTURE

CHAP.O

4.1 Next-word escapes

One appealing possibility for an escape mechanism is to put the desired long pointer in the next consecutive word in memory. This is done for cdrs only in the SWYM [Han69], HISP [Van?], and MIT [Gre74] systems, and is, in fact, the only way to specify a nonlinear cdr in those systems. (Recall that car is unencoded in all three.) Greenblatt's proposal flags the escape condition by using one of its four cdr codes. Both SWYM and HISP identify the long pointer only in the cell containing it. That is, whether cdr of a cell is the following cell, or is contained in the following cell, can only be discovered by looking at the following cell itself. Here we consider only Greenblatt's method.

Since the precise location of the long cdris always known, only one codeword need be reserved to indicate this escape. This is a distinct advantage of the method. Notice, however, that if one is willing to use a whole bit of the short field as a flag, the remaining bits can be used to extend the contents of the

following word to form a very long pointer. Thisdoes, however, sacrifice one-

half of the possible short codes. Less costly ways of specifying can easily be conceived.

a shorter extension

One difficulty

with the next.word

technique involves the free-list.

Construction

of a new cell with one next-word escape requires two adjacent cells

on the free-list (assuming that the free-list allocates compact cells). This will

often be the case, particularly in a linearized system, but sometimes it will not.

Another problem arises when an existing short cdr is changed with rplacd. Yet

another potential difficulty store a full-size pointer.

is that an entire compact Cell might be too small to

Plainly, combination

then, it will be advisable to use a next-word escape only in with another method or methods. Some of these are discussed below.

SEC=4

ESCAPEMECHANISMS

tt9

4.2 Indirect relative pointers

An immediate generalization of the next-word idea is to put the long pointer in some nearby word, not necessarily the next one, and point to it with the short pointer. This will nicely take care of the free-list problem associated with the next-word technique, except in the presumably rare case that after allocation of a new cell needing an escape, the next word on the free-list is too far away to address with a short codeword. The rplacd problem becomes less severe, but can still cause trouble, and there remains the potential problem of a compact cell's not being big enough to store a long pointer.

The penalty here is that some fraction of the available codewords, not just one, must be assigned to indirect pointers. Surely the most obvious way to assign these codes is to have an indirect bit in the short field; but this consumes fully half of the available codes, which consumption will almost certainly not be the best way to use them. Better, probably, would be to allow only a small number of short indirect pointers, and to rely on a different escape mechanism (such as a hash link) if an indirect pointer cannot be used.

Note that if a conventional garbage collection sweep phase is done, these

indirect pointers need only be able to point in the direction of the free-list links.

That is, if (say) increasing address order is preserved on the free-list, the second of

any two consecutive

allocations will always (unless garbage collection

intervenes) have a larger address than the first. And every creation of one of these

short indirect pointers will involve two consecutive ce_l-allocations.

120

COMPACT ENCODINGSOFLISTSTRUCTURE

CHAP.6

4.3 Indirect tables

Now suppose that on each page (or other convenient block of contiguous list stora_;e), all of the words containing long pointers are grouped together into a table. The short indirect pointers of Section 4.2 need now only be indices into this table. This scheme costs a proportion of the codeword space sufficient to address the table, and a fixed commitment of space for the table itself.

The size of the table is an issue. One imagines that it should be roughly

proportional

to the page size, and be big enough to accomodate most of the long

pointers occurring on the page. If the table is too small, it will fill up before the

p_ge itself, creating a problem with several possible solutions: the page might

simply be declared "full", thus wasting the unoccupied cells; or an alternative

(and more costly) escape mechanism, such as a hash link, could be used when new

cells were put on a page whose table was full. The latter method would be

absolutely essential if a short pointer on such a page were to be changed with

rplacd into a long pointer; this drawback forbids the use of indirect tables alone.

Because the allocation of tables and the means of addressing them are fixed, it becomes possible to make the individual entries have different sizes. One might expect that in an 18-bit address space, a full 18-bit pointer would be needed only very rarely, even given that an escape is needed. Perhaps a 9-bit or 12-bit pointer wo_lld do much of the time. The indirect table could then contain various sizes of entries, arranged so that the size of the entry is a function only of the index. This kind of flexibility is present in none of the other escape methods discussed so far.

SEC. 4

ESCAPEMECHANISMS

121

4.4 Plash linking

Bobrow [Bob75] has suggested the application of a technique called hash Hnking to the compact encoding problem. It works as follows: if the (compact) cdr field, say, of cell x contains a particular escape codeword, the true value of cdr[x] is found in a hash table, where x is the key. Bobrow points out that one might have a separate hash table on each page; in that case the stored key would only need to be an index onto that page. He also proposes a single global table for all hash links_ there the key must be a full-size pointer,

If several codewords can be used, the value stored in the hash table can be

extended with additional bits, as Bobrow points out. For example, in a short field

of b bits, imagine that if the high order r bits all equal 1, the remaining b-r bits are to be used as an extension of the value retrieved from the hash table. One

might also consider using different escape values for different tables. A table for

each data type would seem a good idea if the long Pointers different types have different lengths.

associated

with

Another interesting possibility is to have one table that is guaranteed to contain no collisions. Should a collision occur when a new entry was being added, the new entry would be put in a second (or, in general, some other) table in which collisions were allowed. The first table would not need to store the hash key at a]l, while the second, of course, would.

Various familiar hash table costs must be faced. There is first the space

overhead of empty cells in the table, possibly maintained for lookup efficiency

[Knu73]. pointer.

This extra space must be counted in the average size of an escape Then there is the problem of rehashing entries into a new larger table

when the growth of list structure requires it. This could reasonably be done at

122

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

garbage collection time. Numerous other aspects of hash table implementation will not be discussed here; for more information, see [Knu73].

A hash link escape is the slowest of the alternatives discussed in this chapter.

·Where the other methods generally need only one extra memory fetch to get the

appropriate long pointer, using a hash link always requires computation hash function.

of the

Two other potential problems must be considered. One involves local tables, the other, global ones. In the case of a local table of fixed size, a page of list cells that has an unusually high number of escapes can simply not be stored. If the table is full (and if a global table is not available), no more cells can be put on the page. This potential waste of non-table bits on the page must be counted in the average cost of an escape.

Tho problem with global tables is that they must be large, and therefore may

not be in core when they are needed. This is another time penalty associated with

the dynamic use of a hash link: there _ _om_ _onz,_ro cb.ance that retrieving the desired value will cause a page fault $cherae_ in which the hash table is

somehow available

locked into main memory memory space.

bear a significant

cost in dynamically

4.5 Summary

Table 6-3 summarizes the costs and other characteristics mechanisms we have discussed.

of the escape

SEC. 4

ESCAPE MECHANISMS

123

Method Next-word
Indirect pointers
Indirect tables
Global hash table

Table 6-3 Costs of various encoding schemes

Number of codewords needed
1 or 2
Enough to address a range of relative offsets
Enough to Index the table
1 (or more for fancier schemes)

Execution time cost

Other costs

1 memory fetch '"

Fails if next cell is not free

1 memory fetch

Falls if nearby cell is notfree

1 memory fetch
hash table lookup; possible page fault

Fixed cost of tables, even If empty; possible failure If full
Key must be stored (unless no-colllslontable used); Space overhead In nonfull table

5. A Better Encoding for SPARSER

In this section we will look more closely at the encoding design problem for

SPARSER in order to achieve greater efficiency than the seat-of-the-pants

design

given in Section 2. We will need somenew notation. Let b be the number of bits in the car field of a particular encoding (generally clear from context), and let s be

the number of codewords, out of the possible 2 b, that are reserved for escapes. Let

f<typ,> (x) be the fraction of cars accounted for by the x th most common pointer of

type (type>.

In SPARSER, for example, the most common car list

pointer--forward

1 cell--accounts

for .085 of all cars; therefore fli,t(1) = .085.

For a particular encoding, let x<iyp._ give the number of car pointers of type (type)

124

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

that can be directly specified by the compact car field. Finally, let _b, _, fdype, , and Xdypo> be the corresponding objects for cdr.
Once b, _b, s, and s are determined, what values should be chosen for the w_rious X(type> and _X<typ,?> It is easy to see that the available gb-s car codewords sbollld be assigned to the most probable 2b-s car pointers, regardless of data type. (And similarly for cdr.) For example, in the unlikely event that 2b-s is just 7, Table 6-4 shows what to do. It gives, in order, the seven most common things found in SPARSER's cars, as measured in Chapter ft. Optimal choices for the Xdype> are xli.qt=3, Xatom=3, X_m_ll=1, and all other Xctype > --0.

Table 6-4 Car pointer hit parade in SPARSER

i th most i common pointer

type

1 list, +I

list

g NIL atom

3 list, +2

list

4 list, -1

list

5 1 small integer 6 T atom

7 START

atom

fraction of cars accounted for
8,5.10 .2 5.6.!0 -2 2.6, 10 -2 1.2, 10 -2 1.2.10 -fi 1.1.10 -2 1.1.10 -fi

There is another interesting and useful way to view this result. Notice that in Table 6-4, the values fhst(xlist), fatom(Xatom),and fsmaH(XsmaHa)re approximately equal. In other words, the marginal contributions of the rarest codewords of each type are about the same. If the functions fdype> are sufficiently well-behaved, this should always be the case for an optimal assignment of codewords to

SEC. 5

A BETTER ENCODING FOR SPARSER

125

pointers, t We have, then, two roughly equivalent strategies for this optimal allocation: one, sort all pointers by frequency of occurrence and pick the top _.b-s of them; and two, distribute the codewords to the competing data types in such a way that the rarest codeword of each type Is about as likely to occur as the others.

Armed with this principle we proceed to consider escapes. The escape

mechanism we will use is a combination of two from Section 4: on-page indirect

tables and a global hash table for emergencies. There will be separate on-page

tables for car and cdr escapes. The size of the page is an issue for further

exploration,

but here we will arbitrarily assume pages of sufficient size to hold

100 compact list cells, plus the two indirect tables. All table entries will be 18

bits wide, and we will not be concerned wide also.

if the cells themselves

are not 18 bits

With these ground rules there are two problems to solve for both car and cdr.

First, the width of the corresponding short field in each cell must be determined. It is not difficult to see that there is in fact an optimal size, given list structures of

fixed characteristics

and an encoding with a particular escape scheme

cost. We will find that optimal size for car and cdr in SPARSER.

of fixed

The second problem is determining the best size for the indirect tables. Here we will need some assumptions. We will assume that for each encoding design,

tThls result has an interesting

analog from the field of economics. Samuelson calls the corresponding

principle the "law of equal marginal utilities per dollar" [SamO7_ pp. 421-423].

If a consumer with a

fixed income (2b-s) can spend his dollars (codewords) on different goods (data types) from each of

which utility

he derives decreasing marginal utility (the decreasing functions f<type>), then his overall

(encoding efficiency)

is maximized when the marginal utilities per dollar from each good

(f<t3rpe>(X<type>))

are equal.

IZG

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

all cars are independently

and equally likely to need an escape, and likewise all

cdrs. We will also assume that the 100-cell pages are all full. (That is, we assume

that the original these assumptions

lists measured in Appendix A are packed on the page.) Under there is, for each size of the short car field (and cdr field), an

expected number of escapes per page. The smaller the field, the greater this

number. We will assume in this section that each page contains exactly the

expected number of escapes, and will let the sizes of the two indirect tables be the

expected numbers of car and cdr escapes on a full page.

As we observed in Section 4, there is an unavoidable requirement

for an

"emo.rgency" escape, should an indirect table be full when a new entry must be

placed in it because of a rplaca or rplacd. We will therefore have a global hash

table of the simplest links.

kind- one car code and one cdr code will be reserved

for hash

If we had a fixed number of escapes (table entries) s, we could optimally

allocate the Zb-s-1 codes (1 for the hash link) to the competing data types with

the equal-marginal-contribution

procedure outlined above. But notice that doing

this would yield an allocation with a particular expected number of escapes in 100 cells, which number should have been the value of s to begin with. Let g(n)

be the fraction of cars that can be accomodated by n codewords using this

procedure---in

other words, the best we can possibly do with n codes. Then 1-g(n)

is the probability that a car will need an escape. With b fixed and with full pages of 100 cells, we want s to be the solution of

s= I00.(I-g(3b-

s - I))

(6-I)

Since the function g is impossible to express analytically, with heuristic methods to solve (6-I).

we must be satisfied

Consider as an example b=9. We want to choose values of XB|om , X},s| , etc.so as

SEC. 5

A BETTER ENCODING FOR SPARSER

I Z7

to make the values of fdyp._ (xdyp.,) equal. This is achieved approximately with Xatom=2Z5, xli._=54, and x.mall=200. (In this discussion NIL is considered to be an atom. The other Xdyp,> will be ignored; pointers to these types will always use escapes.) These choices give values of f<typ,>(Xdype>)of about 3.10 -4. The 2P,5 most common atoms account for .31 of SPARSER's cars; list distances -12 to +42 (the best 54) get another .207; and the P,00 most popular small integers claim .161. These numbers sum to .678, so .322 of the cars will need an escape. On a full page of 100 cells, the expected number of car escapes is therefore 32.P,, and by happy coincidence, xatom+xli_t+X_m=II=+23P9-I, just what we want. The number of bits committed to cars on a pa_e is 100.9 + 32o.18= 1.476 bits. Dividing these among the 100 carsgives an average sizeof 14.76 bits.
Table 6-5 shows the resultsof applying this·procedurefor various values of b. The optimum is achieved at b=11, for which the avere_e size of car is 13.88 bits.

Table 6-5. Encoding car in SPARSER

b Xatom
8 106 9 P,25 10 60P, 1 1 1481 1P, P-545

Xlist
30 60 90 P,00 400

Expected Xsmal I no. escapes

s avg. bits

75 200 308 350 1135

43.9 32.2 _ P-2,9 15.8 11.3

44 15.92 32 14.76 _3 14.14 16 13.88 11 13.98

With cdr the situation is considerably less complicated. For small values of _b we need only worry about NIL and lists_ the other Xdypo_ will be unimportant. For

128

COMPACTENCODINGOSFLISTSTRUCTURE

CHAP.6

b=7, we get xi,_=I12, xNlt=l,and s=14, giving a sum of 27-I. The expected number of escapes on a page is 13.87,justabout right,and the average number of bits devoted to cdr is9.52. Itturns out that we do bestwith b=6. If_b=6 then s should be 18, xligsthould be 44, and the average sizeof cdr comes down to 9.24 bits.
If_b=5something very interestinghappens. Let/_(n)be to cdrs what g(n) isto cars. Recallthat we are looking fora value of§ thatsolves
s= I00.(I-_(2 s-s- I)) Itturns out that there isno solution!That isto say,regardlessof how many of the 3 1 available codes are used to index the escapetable,the expected number of cdr escapes on a full 100-cellpage will exceed the number of indices.We could use hash links here, but since the bitcostof a hash link isabout twice that of a table entry, the optimality of b=6 stands.
In summary, we have shown how SPARSER's liststructurescan be encoded into compact cellswith a car of 11 bitsand a cdr of 6 bits.On each lO0-cell page there should be two tables,one of 16 entriesand one of 18 entries,for car and cdr escapes, respectively.The average sizeof car in thisencoding is 13.88 bits;of cdr, 9.24 bits;and of a cell,23.12 bits.SPARSER's liststructurescould, of course, be represented in any addressspace of size218 or greater.(Infact,since the number of cells,atoms, numbers, etc.in SPARSER comes to lessthan 64K, an address space of 2 16 would do.) If insteadof 18-bitmemory addresseswe had k-bit addresses, th_ average sizeof a cellwould be 17 + .34k. This Should be compared with the unencoded size of 2k. Thus as k increases,the encoded representation of SPARSER's liststructure becomes more and more efficient,relativeto the simple unencoded alternative.

8EC. 6

ENTROPY OF POINTERS

129

6. Entropy of Pointers
Thus far in this chapter we have considered only a very limited class of encoding schemes: those with a fixed-sizesmall code used most of the time, .togetherwith some escape mechanism which gives accessto a long pointer when needed. Many other schemes can be imagined. Most prominent would surely be methods that use some sortof variable-lengthcodewords. We might expect the average bit requirements of pointerstogo steadilydown as we apply increasing amounts of cleverness to the encoding design.
It becomes worthwhile, therefore, to ask how far this process might conceivably go. Is there some encoding scheme that,for a particularset of list structures,is"optimal" in the senseof requiringno more bits,on the average, than any other? Can we know in advance some lower bound on the average sizeof car and cdr, over all encodings? Some simple concepts from information theory can be used to address these questions.Under somesimple and reasonableassumptions, the lower bound we seek is given by the information content or entropy of pointers.

Entropy isdefined as follows. Let S=(s1,sg,...,Sn)be a setof n symbols and

let P=(pl,p2,... ,Pn) be a set of probabilitiesuch that Pi is the probability of

occurrence (in a sense intentionallyleftvague) of si,for i=I,... ,n. Of course,
n
_Pi = I. If symbols are chosen from S according to probabilitiesP, then the
i"I
average information content of each choice--the entropy of the set--isgiven in

bits by
n
H = - _Pilog2p i
i=l
This formulation isdue to Shannon [Sha48].

(6-2)

130

COMPACTENCODINGOSFLISTSTRUCTURE

CHAP.6

Consider, for example, a stream of symbols drawn from the alphabet

S = {A, B, C, D} such that the symbols are equally likely to occur. Each has

probability

.2.5, and the entropy

per symbol of this system 4
H = - 1_=..1.25 logfi..25
= - 4.(.a5.-a)

is

=_.

That is, any scheme for encoding four equlprobable symbols must commit, on the

average, at least two bits to each symbol. This has a certain intuitive appeal.

Now suppose that the symbols A and B each have probability .495, and that C and D each have probability .005. The entropy per symbol becomes

H = -[2.(.495 = 1.08.

log 2 .495) + 2,(.00,5 log 2 .005)]

In other words, when a symbol stream is composed almost entirely of two

equiprobable

symbols, the per-symbol encoding efficiency is bounded from below

by just over one bit. To approach this efficiency, blocks of consecutive symbols

would have to be encoded together in single codewords [Sha48].

Suppose S is the set of all things that occur statically in cdrs of list cells used

by a particular some of which

Interlisp program. S contains a large number of 18-bit quantities, are pointers to (addresses of) atoms, some are pointers to (addresses

of) other list cells, and so on. Each of these "symbols" appears in the list structure

of the program with a certain frequency. Pointers to the atom NIL, for example,

occur with frequency between .2 and .3, according to the results of Chapter 2..

Individual pointers to list cells, on the other hand, occur with low frequency, since each is an absolute memory address.

To take advantage pointer be represented

of the regularity demonstrated instead by an offset relative

in Chapter 2, let each list to its location. This will

SEC. 6

ENTROPY OF POINTERS

131

greatly reduce the number of different symbols in S and greatly increase the frequency of occurrence of some of them, especially the relative offsets . 1 and - 1. In addition, let us regard pointers that point to separate instances of the same number (a possibility allowed by INTERLISP--see [Tei74] for details) as occurrences of the same "symbol".

Using symbol frequencies as estimates of symbol probabilities, we may now

use equation (6-2) to calculate, for a particular program, the entropy of cdr; the

same technique may clearly be applied also to car. The entire process can be repeated for linearized list structure, and we would expect this to reduce entropy

somewhat.

Table 6-6 shows the results of calculating the entropy of car and cdr

for each program before linearization, and Table 6-7 gives the same information

after linearization both tables.

in each direction. The sum of car and cdr entropy is shown in

Table 6-6 Entropy before linearization (in bits)

car

NOAH PIVOT SPARSER CONGEN WIRE

9.05 10.32 9.73 6.39 9.93

cdr
4.81 5.16 5.08 3.46 3.33

sum
13.86 15.48 14.80
9.86 13.26

Several observations may be made about Tables 6-6 and 6-7. First, car and cdr

entropy are both much less than the 18 bits used for each on the PDP-10, even

without

linearization.

(The entropy figures, however,

are completely

independent of the particular Lisp implementation and of the word or address size

132

COMPACTENCODINGOSFLISTSTRUCTURE

CHAP. 6

Table 6-7 Entropy after linearization (in bits)

NOAH PIVOT SPARSER CONGEN WIRE

car-direction car cdr sum
7.90 2.37 10.2.7 8.63 3.04 11.66 8.35 2.38 10.73 4.96 2..39 7.35 8.16 2.07 10.23

cdr-directton car cdr sum

9.15 1.13 10.2.7

9.88 1.98 1 1.86

9.56 1.28

10.84

6.13 1.17

7.30

9.19 .99 10.17

of the computer. Entropy is a function of symbol probabilities only, and does not

depend on how the symbols are represented.) Second, linearization in the cdr

direction affects car entropy only slightly and cdr entropy dramatically, while

linearization

in the car direction reduces both by one or two bits. This is due to

the distribution of data types in car and cdr: lists account for less than one third of

the former and almost three fourths of the latter. Third, the sum of car and cdr entropy is nearly the same after linearization in either direction. This observation

is difficult to explain. Fourth, car entropy in CONGEN is considerably less than in any other program. This reflects CONGEN's concentrated use of a small number of

atoms and numbers, which was mentioned in Chapter 2.

No encoding of car or cdr for these particular programs can require, on the average, fewer bits per pointer than the entropy figures given in Table 6-6 without taking advantage of possible regularity among groups of symbols. For example, if in some program the atoms A, B, and C always occurred together, in order, at the same level of a list, then a single symbol could represent an instance of all three atoms. Such a potential would be ignored by our single-symbol entropy calculation, and a list cell encoding which did not ignore it might commit

SEC.6

ENTROPYOF POINTERS

133

fewer bits, on the average, to car and cdr. There might also be some exploitable

regularity in the contents of car and cdr of the same cell; encoding car and cdr

tog, ether might then give an average bit requirement less than the sum of

separately-computed explored.

car and cdr entropy. Possibilities such as these have not been

In Tables 6-8 and 6-9 car and cdr entropy are broken down by data type. Not

surprisingly,

the contributions of literal atoms to car entropy and of lists to cdr

entropy were large.

Table 6-8 Contributions to car entropy by data type

LISTS (before linearization) LISTS (after car-dir, lin.) LISTS (after cdr-dir, lin.) ATOMS NIL SMALL INTS LARGE INTS OTHER TYPES

NOAH
2.61
1.46
2.71
5.44 .20 .52 .20 .08

PIVOT
2.39
.70
1.95
6.33 .10 .41 .98 .11

SPARSER CONGEN

2.85 2.19

1.47 .76

2.68 1.93

4.23 .23
2.12 .27 .03

2.36 .06
1.53 .24 .01

WIRE
2.60
.83
1.85
4.18 .15
2.73 .22 .05

That entropy is not minimized by either kind of linearization can be seen in a

simple example. Figure 6-1 shows how a "hybrid" cell-relocation

scheme can

achieve a lower sum of car and cdr entropy than can linearization in either

direction. It is easy to see, in fact, that no possible arrangement of cells can do

better than the hybrid scheme of the figure for that particular list structure.

13,4

COMPACTENCODINGSOF L_ST STRUCTURE

CHAP. 8

Contributions

Table 6-g to cdr entropy by data type

I,ISTS (before lJnearization) LISTS (after car-dir, lin.) I,ISTS (after cdr-dir, lin.) ATOMS NIL SMALL INTS LARGE INTS OTHER TYPES

NOAH
4.14
1.70
.46
.12 .50 .01 .04
-

PIVOT
3.67
1.55
.49
.67 .51 .14 .15 .01

SPARSER CONGEN
4.28 2.81
1.59 1.74
.48 .52
,16 .08 .50 .50 .06 .07 .08 .00
- .00

WIRE
Z.75
1.49
.40
.07 .50 .01
-

Whether there is an efficient entropy-minimizing cell-relocation strategy is an open question; an inefficient strategy that will do the job is exhaustive search.

7. Costs and Implementation

Questions

Any scheme for encoding and decoding information carries some time penalty.

In previous sections of this chapter we have looked chiefly at the space side of the

time-space

trade-off;

here we consider the time costs of compact encodings,

as

well as some other implementation

issues.

CDR DIRECTION

CAR DIRECTION "_......_

CAR: 5 atoms; lists +1, +2, +5 CDR: 4 NILs; 4 lists +1

Hca r = 5"(1/8"(-Iog 2 1/8))

+3"(1/8"(-Iog2

1/8))

=3

Hcd r = 1/2°(-Iog2 +1/2°(-Io92
=1

1/2) 1/2)

Hca r + Hcd r = 4

CAR: 5 atoms: 3 lists +1 CDR: 4 NILs; 2 lists +1; lists +3, .4
Hcar = 5"(1/8"(-Iog 2 1/8)) +3, 8"(-log 2 3/8)
= 2.41

Hcd r = 1/2"(-Iog 2 1/2)

+1/4'(-Iog 2 1/4)

+2"(1/8"(-Io92

1/8))

= 1.75

Hca r + Hcd r = 4.16

HYBRID

_'_
A _7
.,....e "I _
B
c.j.
D ---
E ..i __

CAR: 5 atoms; 3 lists .3 CDR: 4 t,:lLs; 4 lists +1 Hca r = 2. 4 1
"co--r'
Hcar + Hcdr = 3.41

Figure 6-1. Entropy calculations

for a particular list structure

after three kinds

of linearization:

cdr-direction,

car-direction,

and a hybrid scheme.

136

COMPACT ENCODINGS OF LIST STRUCTURE

CHAP. 6

7.1 Frequency of escape

Perhaps the most important question to ask is how often the (relatively

expensive) escape mechanism is used dynamically.

We know from the data of

Chapter 2. how many static instances of escape codes there will be for each of our

programs. The data of Chapter 3 can show how often the time penalty of the

escape mechanism will be incurred. We noted in Chapter 3 the rough agreement

between static and dynamic data: data type distributions were similar in the two

settings, es were the distributions of pointer distances; ranking atoms by static

frequency did not do violence to the cumulative distribution of dynamic atom

references (see Figures 3-4 and 3-5). This means that in general, the time penalty

of a proposed escape method will be borne approximately as often as the space

penalty occurs statically.

7.2 Impact on Lisp primitives

The standard approach to pointers in Lisp.--using full machine addresses all

the timemis, while wasteful of space, very fast: car and cdr are simple memory

read operations; cons is a simple write. Under a compact encoding scheme, the

primitive pointer-manipulating

operations of Lisp become, most of the time, only

slightly more complicated. In the rare case that an escape is used, the possibly

high costs discussed in Section 4 must be paid.

Consider car and cdr. These operations must fetch the appropriate small field 'and check its type: list, atom, NIL, escape, etc. This can be done trivially and
quickly in hardware or microcode. Most of the time an escape value will not be found, but if one is, various complicated things can happen. In the usual case, the true value of car or cdr can be obtained either by a single addition (for lists or small integers) or a table look-up (for atoms). There may be additional work if,

SEC. 7

COSTS AND IMPLEMENTATION QUESTIONS

!37

for exanlp]e, the short code for the relative list pointer +4 does not have the value

+4. But again, .a hardware or microcode implementation

of the mapping from

codes to values seems quite straightforward,

provided the mapping itself is

intelligently

designed.

The Lisp operations rplaca and rplacd replace an existing car or cdr,

respectively,

with a new value. If compact encodings are used, these operations

must first check to see whether the new value can fit into the small field. In the

case of lists, this will cost a subtraction and a comparison; in the case of atoms, a

table look-up to see if it is one of the common few hundred or so. Section 6 of

Chapter 3 discussed the patterns of data type replacement by rplac operations_

here we are interested chiefly in how often "small" pointers (compact ones) are

replaced by "large" pointers (escapes). That, after all, is the most difficult kind of

rplac under a compact encoding. The data reported in Appendix B unfortunately

do not include enough information to calculate how often this happens with

rplaca (atom and number distributions were not calculated)_ there is enough,

however, to evaluate the more common rplacd operation. If we approximate

"small" cdrs by NILs plus all list pointers with distance under 3Z, then Appendix B

says that for the runs of Chapter 3, the replacement of a small pointer by a large

pointer accounted for just Z.8 percent of rplacds in CONGEN, 5.5 percent in

SPARSELY, and 6.6 percent in NOAH. Most rplacds (63 to 90 percent) replaced small pointer by another: an easy operation under a compact encoding.

one

The final list-manipulating

primitive, cons, creates a new cell containing

two given pointers. Once the cell is allocated, a cons looks very much like a

rp]aca together with a rplacd, and the same computational work must be done.

Notice, however,

that the construction

of an escape is made easier by the

likplihood that nearby cells are unoccupied available for indirect escapes.

at the time of the cons, and thus are

138

COMPACT ENCODINGS OF LiST STRUCTURE

CHAP. 6

7.3 Atom frequencies

If a compact encoding scheme does not allow all atoms to be referenced

directly, some way must be found to discover which are the most common ones, in

order to realize the savings afforded by the encoding. The problem here is quite

different from the case of lists: whereas we can be reasonably sure beforehand

which relative list pointers will be common (namely, the small ones), we have no

way of knowing before a program runs exactly which atoms will be frequently

pointed to in list structure. The obvious way to find out is to count the references to each atom during garbage collection. The cost of doing this is a small time

penalty and space enough to store the counts, plus a sort of the counts when the

dust settles. The space, however, been tabulated.

can be returned after the common atoms have

Notice that if the static frequency distribution of atoms is not stable over

time, much effort can be spent in encoding and re-encoding

maintain coding efficiency.

But these distributions

almost

atom pointers to certainly change

slowly, so recalculating them at garbage collection time seems sufficient.

7
Conclusion
This chapter offers a detailed summary of the principal results of the thesis, and discusses some areas for further work.
139

140

CONCLUSION

CHAP. 7'

1. Summary of Results

The results of this thesis may be broadly stated as foilr_w:_: a characterization

of list structure use in five large Lisp programs, both as to tile sta!:ic appearance of

lists in memory and the ways in which they are _ccessed during program

execution; development of the two fastest known algorithms for ::i_._structiw ,. and

nondestructive

list linearization; an investigation of the effects of linearization in

a I,isp system; and an exploration of p.o:_s_b!e compac_ representations

of list

structure.

What follows is a chapter-by-chapter

recapitulation of these results in

greater detail.

Chapter 2 presented an en_tpirical look ,::_:th,, !i,,,.;!:_t.r'_,_cturedata used by five

large Interltsp programs drawn trom d'ilf_::ren_ _pptb::ation areas. The list

structures

of the f_ve showeci strc_n_.{... ,:_.milarit_/ and _"-_o.<:_a:rit;,_. iI_) a1 _ five

programs, roughly erie---third ot car:s a_,1 '_l._ce_:._.-?, :__i_...,.:.:_,_i :,I_:_:we, re. fot!rt::._, to point

to list cells. Almost all of the. r_maiali_£ car,'; point_:.:c: eit>.ez to, (n-,m-NIL) atoms

and small integers, or to atoms aion_-'.:,a_,_}os:t :_li ,::._,::a_!c_nil:;{ cdrs pointed to the

li/qt.-terminatin£,

atom Nil, ()f the poin_,r::.; to t{>:t_;_ c_!_i.i.po_n{>:.,._!:o _:_.,,athe :e_.i

containing especially

t.ho poi_.ter were ,.lsually p.,,),_ _,_.r _: :a memor:i_ _,, "t_.,,.. true of ;:drs: about half ot all i_:i-_ ,:{i_.:: _::_.,:_(-g_ !o o_::: ot _ihe two

immediately

adjacent cells, it was argu_,d i_. ,17:L,a>aer!: _hat ih._e,: t_c::_._li._:y-_vas a

thedlle to

ostengiDtr,

CieV(?_/IIeNS

_){: lgi_2

i[_'_,r-_"i_-_:

' : _ 7{1...{'c,)

i; /_iiq_C.)_Oi"

C?n@

_NrOt.lld

Pointers to atoms were sh_wn in Chapter Z t{> :>,,-_di>;t_ib_._.ed approximate|y

according

to Zipf's law [Zip4q], wh{,,:h

{!a, ::_:;tribu_i<::r! c_f ,,_,,::_rd

occurrences in natural language te.xt.

_o _,_:_!_b,.:_., ;_;,-_rodi_stri.bu.*__, q_lI i:t;,

SEC. 1

SUMMARY OF RESULTS

141

differently

from program to program. The only agreements found among them

were that pointers to small non-negative integers were common, and that a large

fraction of numbers were pointed to just once.

Chapter 3 provided a look at how list structure is accessed during program

execution.

Car and cdr were predominant among the five primitive Lisp list-

accessing functions, together accounting for between 82 and 95 percent of all

executions of these functions during short runs of three of the programs. Car and

cdr occurred about equally often. The distribution of dynamic references to

various data types (by car and cdr) was shown to vary a fair amount from

program to program, and not to reflect closely the static distribution

of the

various types of pointers. This fact, together with the observation that dynamic references to atoms did not follow Zipf's law, showed that none of the measured

programs made uniform references to its static list structure.

Further

measurements

showed that a cell, once referenced, is likely to be referenced again

soon. For example, in each of the programs measured, half or more of all dynamic

list cell references were to one of the ten most recently referenced cells. It was

also shown that at least 95 percent of all list-cell references were to one of the

five most recently referenced Tenex list pages.

Chapter 4 developed two algorithms for creating a linearized copy of a list

structure, the first destroying the original structure (list-moving),

and the second

restoring it (list-copying).

Neither algorithm uses more than a constant amount

of working storage, nor needs any mark bits. Both are significantly faster than

the best previous (linear-time)

algorithms for these problems: Cheney's list-

moving algorithm [Che70] and Fisher's list-copying algorithm [Fis75].

Chapter 5 showed that if all of a program's lists were linearized, the clustering of list pointers was increased to a degree limited only by the amount of

142:

CONCLUSION

CHAP. 7

structure-sharing

present. Cdrs were shown to point very rarely to shared cells,

since linearization made over 98 percent of each program's list cdrs point to the

next sequential cell in memory. Pointers to other data types are, of course,

unaffected by linearization.

One result of these measurements was that the list

structures pointed to by cars tended to be smaller in number of cells than those

pointed to by cdrs. An implementation of Minsky's linearization algorithm

[Min63] was described in Chapter 5. This algorithm was used for a series of

measurements

that required Lisp computation to proceed after a linearization.

It

was shown that linearized list structure tends to remain linear during subs.equent program execution, but that new cells created after a linearization have only

slightly better-than-average

locality. Although linearization compacts list

storage and increases the number of on-pa_e pointers, it had only a small effect on

paging activity in the measured programs: if linearization was done, at most a few

additional percent of subsequent list cell references were to the most recently

referenced list pages. If, however, the cost of a page fault is taken into account

and the average execution time of list cell references used as the criterion, then

linearization

was found to yield considerable savings.

In Chapter 6 some of the list-structure

regularities found in Chapter 2. were

exploited in the design of space-saving encoded representations of lists. In these

encodings car and cdr fields of a list cell are smaller than a full memory address,

and each contains either a commonly-occurring

pointer or one of a small set of

"escape" values. These escape codes indicate that the true value of the pointer is to

be found elsewhere. A variety of escape-handling mechanisms were considered in

Chapter 6. An encoding was designed for the data from one of the programs, and

"the result was a simple encoding that used an average of 23 bits per list cell,

counting overhead attributable to the escapes. This number can be decreased by

using more complex escape mechanisms and by linearizing lists from time to time.

SEC. 1

SUMMARY OF RESULTSI

143

The effect of list linearization on encodability was considered in Chapter 6, and

the efforts of others to develop "cdr-less" Lisp systems [Gre74, Han69, Van?] were

surveyed.

A lower bound on the efficiency of all possible encoding schemes is

given by the average information content or entropy of a list cell. Under some

simple assumptions,

this was calculated to be between 10 and 15 bits before

linearization,

and between 7 and 12 bits after, for all of the programs. Some

questions discussed.

concerning

the implementation

of compact encodings were briefly

2. Topics for Further Work

2.1 Measurements

There is, of course, no shortage of interesting and 3 explored only a few of the many possibilities,

things to measure. Chapters R which include the following:

* Measurements

of the lifetimes of list cells. How long a time passes

between the creation of a cell and the point at which it becomes inaccessible

(garbage)?

Is there any relationship between pointer distance and cell lifetime,

that is, if a pointer points a long distance away, is the cell pointed to (or perhaps

the cell containing the pointer) likely to have a longer-than-average

lifetime?

How much less likely are old cells to be referenced than new ones?

* An examination

of the sequences of car and cdr used to access list

structures.

A common belief about this is that computations often check car with

eq or some other predicate, and then (depending on the outcome) proceed to the

cell pointed of access?

to by cdr [Bob67].

Is this accurate?

Are there other common patterns

144

CONCLUSION

CHAP. 7

* Dynamic measurements of tile higher-level list-manipulating

functions of

Lisp. Chapter 3 dealt only with car, cdr, cons, rplaca, and rplacd, but clearly

many instances of these occur during the execution of less primitive functions.

Do different programs do similar things with these functions?

* Measurements

of other kinds of data. This thesis has been concerned

exclusively

with list structure, but other data types, especially

arrays, and hash arrays, deserve empirical study.

perhaps atoms,

* Further analysis of linearization. Chapter 5 offered some suggestions about

performance

improvements

that come from linearization, but additional work

clearly needs to be done. A more thorough study of possible gains in paging

performance

would be particularly interesting.

2.2 Algorithms

For systems in which sufficient free storage can be allocated beforehand, the

linearization

algorithm of Section 2, Chapter 4 seem,._ hard to beat. It is very fast

and makes no demands on the system for astackor for mark bits. If, on the other

hand, this free storage is not available° something like Minsky's algorithm

[Min63] must be used. This algorithm, however, u_;es a stack and requires a mark

bit in each list cell. Future work o_l linearization algc, rithms might include:

* The invention of linearization algorithms use of secondary storage, but that can do without

that operate like Minsky's a slack or mark bits.

in its

* Algorithms for linearizing encoded list structure. The principal difficulty here is that the forwarding addresses normally used may be too big to fit in a compact cell. Deutsch and Bobrow [Deu76] solve this problem by storing in an auxiliary table forwarding addresses fer only those cells that need them, namely,

SEC. 2

TOPICS FOR FURTHER WORK

145

cells pointed to more than once, of which there are usually few. Eliminating reducing this use of extra space is desireable.

or

* Correctness proofs of any of these algorithms.

2.;3 Encodings

Chapter 6 offered some simple ideas for compact representations

of list

structure.

The space of reasonable encodings is vast, however, and Chapter 6

explored only a small region of this space. Some possibilities for further work:

* Fancier encoding designs. Greater encoding efficiency can be achieved with codewords and escapes that have variable length, and a host of new problems arise when list cells can have a variety of sizes.

* A c|oser analysis of the costs of various escape mechanisms,

combinations

of different mechanisms. Chapter 6 showed that several

and of kinds of

escapes should be used together.

* The implementation

of some compact encoding(s).

* A study of the possibility of using several encodings simultaneously

on one

machine, either for different users or for different segments of One user's list

structure.

One encoding will probably not be ideal for all needs.

* The design of specialized hardware for handling encoded list structure, for efficiently dealing with multiple encodtngs.

and

APPENDIX A
List Pointer Distances

Tables A-1 through A-5 give, for each program, the frequency distributions of

static list pointer distances before and after linearization. Forward distances are

shown separately from backward distances, and cumulative percentages increase with distance in both directions. Thus the sum of the two numbers at the top and

bottom of each column labeled "cure" is 100 (percent). All numbers are rounded to

the precision shown; in some cases this has produced an apparent but

understandable

inconsistency between "pct" and "cure" columns. Distances greater

than 15 are grouped according to the next higher power of 2; the row labeled

"<64", for instance, means distances 32. through 63.

147

1,I_¢

APPENDIX A

List pointer

iable A-'I distribution

for NOAH

B,_! .r_, iinea,.tzatl<-:'.;

CAR

CDR

dlst, pet. c_m pet cure

_>256

6.4 54.8

4.6 55.8

<'2f.i.6"

1.3 48.4

.6 51,2

< 128

1.3 47 1

10

:50.6

<64 1.9 4r,.8

1,,5 49.6

<32 2.1 43.9 15 .2 41,8

2.7 ,18,1 , I ,t5 4

14 .3 41.6

,2 45.3

13

.3 4 ! ,.q

.2 ,15.1

12 .4 4 1.0 .3 44_8

1 1 .3 40.6

,3 ,1,1.6

10 .7 40.3

4 44.2

9 .9 o"q,:. '," 3 43,8

Aft_ -tar-dlrcctlr.v,

CAR

CDB

pet cure pet _ _m

A,_ic, , :t_ dl;,,,._lc_.:

CAR

CDR

pc, ,,urn

prt cure

.0 ";'9.5 .0 ",',95 .0 79,5 ,0 79,5 r) 79,5 .0 79,5 ,0 79.5 .0 79.5 .0 79.5 ,0 79,5 .0 7"9.5 .0 7 q. 5

.2 .3 .b, 1.4 2.2 ,s .3 .2 .4 ,4 ,4 .5

98£ 98? tt,q 4 9,.9 965 94.3 93.8 93.5 q :¢.,i_ q2.q ca2 ,q q ): 1

4,9 79 q 2.6 75.0 3. 7 "_'2,.-1 or,, 1 t38,7 7,8 63,6
.7 55.8 .5 55.1 1 3 54.6 , C_ 5.3.3 .- 52,4 1 O 51 ";' 1,5 50. !

.0 98.8 ,0 98.8 .0 98,8 ,0 98,8 .0 98.8 .0 98.8 .0 98,8 .0 98.8 .0 98.8 ,0 98,8 ,0 98,8 .('J 98.8

m <
m t0,.,l
2 1
o
1 2 3 <4 -% 5 ',.-:' 6 7 8 k9 10 11
12
13 14 15 <32 <64 ·'128 <256 >256

1.3 38,3 .8 37,0

.4 .9

2.7 ;_(', 2 2.3 33.5
2,0.6 3..1'_,2b,8

.8 2.3
2,,,93

6.3 21:1..3 1.2

23.0

23.0 34.7

.o .o .o

12.5

12,.5 21.9

4.3 16.8

.6

2.3 19, 1 4,<-q,

..q 20.1

1.5

2n _'> 0 1.0

.3 22.3

1.2

,8 2 ._ 1

.4

.2 _."4 ._

:_

.7 ,_,1.0

,{-,

.6 24.6

.2

.2 2,1.8

.3

.3
.2 .3 .3 1.8 1.2 .6 1.3 14,4

25.0
2.r-,. 3 25,6 25,9 27.7 28.9 29.5 :_(_.8 .45,2

.,_-_
.2 2 .2 1.9 1.5 1.0 .9 5,9

4.'4.3 42.9

.0 79.5 ,0 ;'9.5

42.() 4 1.2
3,t83..8f_

,0 79.5 .0 79.5
..00 7:"99..5f,

35.9

.0 79.5

3,1.7 79.5 79.5
.o .o .o

21.9

.0 ,0

22.6

.0 .0

_6.8

--

2 __.3

.0

--

2_ 2

-

:-¢0,,1

,3

.3

._O. 9

- :_

3 1.2

-3

31,7

.0 .3

;it1.9

.4

:_>; 2 .1 .4

,_ -_:. : _
,q2.6

0

32.8

33.0

34,9

.2

36.4

.3

37.4

.2

;_F,;.:!_

,1

44,2 19.2

.5
.5 .5 ,5 .7 1.0 1.2 1.3 20,5

.5 .7 1.0 2.4 7"6.6 14 ;"7.4
.o
.0 .0 .0 .0 _ t', c, ,_ .0 .() .0
,0
.S ..")
1.0

!) 1 i_ 3,5 47.3 9O5 2.0 43.8

.O 98.8 .0 98.8

,-29 _ £g _
8961,14i,

2,1 2, ,1
,3I . 2,_,

,!,1.8 ._f_ '7
34"*;"34C:

0 98.8 .0 98.8
.0,b 9q8t'._8 _'3

,-'_ R 6 6 ?_39

.0 98,8

7- .; 27'.3 2 ;', ;*, 98.S 98.3
.o 15........-.;.-.,......._5........:.S.......

,._ ¢, .0

C) ,0

r, n .0

....

.o

- ,_0

-

r, .0

- .0

-

i -- O

_0

-n

! .O -

1 ,"

--

. i - t .0

1 .i - -

; . i "" -

. : . 'i
] .;

. ,* ,,0

.i

.i !

,; 9. .4

, _ " .7"

,; .3 1.0

! _ ; ,5

_,,;: 18.8 2.0, I

.C)
"'_ _ ,q
-0
i. i

""
"
-1._.

LIST POINTER DISTANCES
Table A-2 List pointer distribution for PIVOT

149

dist.
?2,56 <256 (12,8
<64 <32
15 14 13 12 11 10
9

Before Ilnearization

CAR

CDR

pct cure

pet cure

4.4 1.8 2.5 4.0 4,6
,6 .6 6 .9 _8 I, 1 1.2

71.2 66.8 65.0 62.5 58,5 53,9 53.2 52.6 52.0 51.2 50,3 49.3

4.3 40,1 .5 35.8 .6 35,3 .9 34.7
1,1 33.8 .2 32.7 2 32.5 .2 32,4 .3 32.2 4 31.9 5 31,6 3 31. t

After car-direction

CAR

CDR

pet cure pct cure

.0 95.7 .0 95.7 .0 95.7 .0 95.7 .0 95.7 ,0 95.7 .0 95.7 .0 95.7 .0 95.7 .0 95.7 .0 95.7 .0 95.7

,1 99,1 - 99,0 .2 99.0 ,6 98.8 1,2 98.2 .1 97.0 .1 96_9 ,2 96.7 .2 96,5 .3 96,2 .4 96,0 .4 9-5,6

After cdr-dlrectlon

CAR

CDR

pct cure

pct cure

2.0 2.9 5.7 6.9 8.7
_8 .6 1.4 9 1.1 1,3 1.8

96_1 94.1 91,2 85.5 78.6 69.9 69.1 68,5 67,1 66.2 65.1 63.7

.0 99,1 .0 99.1 .0 99.1 .0 99,1 .0 99.1 .0 99.1 .0 99.1 .0 99.1 .0 99.1 ,0 99.1 .0 99.1 .0 99.1

a:: <
m 0l
2 1 0 1" 2
<
L) m
10 11 12 13 14 15 <32 <64 (,128 <256 :_:256

1.9 2.3
3.0 3.8 51..93 9,3 20,6
.0 6.6 3.7 1,0 I·1
·8 .6 6 .3 .3 _2 ,2 .1 ,1 2,
.9 1. 1 1.0
.7 9.2

46.8 44.8
42.6 39.6 3485,80 29.9 20.6
.0 6,6 10.3 11,3 1:34 13 1 13,7 I 4,3 14.@ 14,9 15,t 15.3 15.5 15.6 15.7 15.8 16.7 17.8 18.8 19,5 28.8

6 7
8 I3 1 74 2,1 23,2
,0 30.5
2.3 42 3.3 1,9 1,V 1,3 1.2
9 _ .6 .5 .4 ,3 .4 2.8 2.0 1,0 .6 33

30.q 29."

.0 95.7 .0 95.7

.8 94.2 1.0 93,4

2.7 60.7 1.7 58.0

.0 99.1 .0 99. I

29,1 28,,1 2307..1 :'" 25.3 2:.].2
,0 30.5 32.8 36.9 40.2 42.1 438 45.1 46,3 472 48,0 -186 49,1 49,5 49.8 50,2 53.0 ro.o_,O 56,0 56,6 59.9

,0 ,0 ..00 .0 95,7 .0 .0
.0 .1
-
.2 .3 .4 .3 2:6

95.7 95.7 9955.,77 95.7 95.7
.0 .0
,1 .1 ,1 ,2 .2 .3 ,3 3 4 4 ,4 ,4 .5 .7 1,0 1,4 1.7 4.3

1.8 1.8 91..00 4.3 75.5
,0 .... ..... .... -
-
,0 -. .0 0
.0 ....
.1 ,1
.5

92,4 90,6 9858,.29
79,8 75,5
o0

3.5 4,0 41,.22
2.9 41.7
,0

-- 1

1 .1 .1 .1
1 1 1 1 1 1 1 ,2 ,3 ,3
i .9 i

.0
_ .0 2 ,3 ,3 4 2,4

56,3 52,,9 4618.,98 44.6 41,7
O
1 .2 .2 .2 .2 .3 ,3 3 3 ,3 .3 .3 ,3 .5 .8 1.1 1.5 3.9

.0 .0 ,.00
.0 99.1
,0

99.1 99.1 9999..11
99.1 99.1
.0

.... -, 0 .1 .1 ,1 .1 .5

-
1 .1 .1 .1
.1 .1 .1 .1 .1 .2 ,3 .4 .5 .9

150 APPEI',_DtX A

List pointer

Table A-3 distribution

for SPARSER

dist.
_':_256 42._6 <|28
(64 <32
15 14 13 12
11 10
9

R_,fore ]lnearizatlon

CAR CDR

pct cum

pet cure

9.0 08.6

3.9 ,_Y,5

1.1 59.6

1.3 F.3,6

1.5 or-..a.. 5

l.,q

52.3

2.5 57.0 2,4 _04

3.7 54.5 2.6 48,0

,4 50.8

.3 45.4

.4 50.4

.3 45.1

.6 50.1 49.4
.6 48.8

,2 44.7
.3 4,t.5 .4 "1,t2

,6 48.3

.6 _1"_ 8

.8 47.F

_,1 ,1:_,2

After car-direction CAR CDB ! pct cure pct ,.':um

After cdr-directlon

CAR CDR

pet cum

pct cure

.0 7a,_ ,0 78,5 .0 78.5 .0 78.5 .0 78.5 .0 78,5 .0 75.5 .0 78.5
.0 78.5 .0 78.5 .0 785 .0 78.5

75.5 7.3 81.0

.1 98,5

1,g 73.7

.2 984

3,5 7, 1.9

.7 98.2

4.0 68.4

2,.3 97.5

5.7 64.4

·1 95.2

.6 58.7

.2 95.1

.4 58.1

,4 _74.9 i,4 57.6
I

.3 94,5 i .5 56.2 /

.6 94.3

.8 55.7

9, 9._.6

I, I 54.9

|.1

9,'_-.3

I ,6 53.8

.0 98.7 .O 98.7 .0 98.7 .0 98.7 .0 98.7 .0 98.7 .0 98.7 ,0 98.7
.0 98.7 .O 98.}' .0 98.7 .0 98.7

6 1.5 ,1,1,7

.9 ,ll . 8

.0 78, 5

.5 91,0

2,,4 49.2

.0 98,7

'¢_ r:.

5 2.0 43.2

.9 40.9

,0 ;:'8.

1.3 9C.4 3.4 46.8

,0 98.7

4 2._ 41.2.

1.1 40.()

.0 78,5

3.6

_7_ !

:_ _:; ,.I3 4

0 98,7

0
l

73' 3I.3.4 ,_168,.5i 2.8.5 :4__,R2,.39

..00 ;7' '8_._5,_i ._;ai

,'_[_7;!_ :'_::,

....
2._. _I f3;.?I ,2_

_0 _C:__A_,,:?'

A | 8.3 1 I 26.9

35,_ 26.9

2.4 33 7

36,2 39,.7

.0 78.5

_'a r_ _, I 78.5 7"9.3

_'_i: 4

;'_,,':, 36.8

,:-,c_..:,; 33,3 33.3

.0 98.7

98,7 98.7

0 I .0 .0 0

.0 .0 .0 .0

1 II 3.8

3.8 21.0

2 t .0

.0

.9 0

,; ('_ .0

r', O

'_

.0 -

.0 -

2 i 2,5 ,_q_ ,8

< 4 .6

55

_o 6

3

m 7 .4

,o, .1

9 .2

10 ..2,

II I

12 1

13 1

14 1

15 1

,'32 1 0 ' 64 1,0

" 128 (256

.9 ,,7

_>256

17.9

6.3 /.O 7.7 8.2 8.5 9.0 9. i 9,2 !:_ 1 q.5 9.6 9.'7 9.8 9,9 10.9 I I '_ 12,8 13,5 31,4

l+q 3.0 I0
.9 7 r-_ .5 .5 4 .3 .2_ .2 .2 .2, 1.5 1. I ,_n .,_,_ 7,1

22.8 _ .0

2,q.8

.0

.0 ,0

26.8 I

2"_.8

,3

3

.2,8.5

1 .4

"_" I

1 .5

_-_.6

,1 .6

?_r'. 1

.5 1.1

2 i !. 4

.1 I. ,_

30. "-" .I _.A.

30. 9

.1 !. 5

3i.1 .2 1.8

:31.4 I .2

!,9

31.6 .1 2,1

3_? 1 3 'I .)...

1,4 1 .7

;-_.5 ,-,,_,,_

3,1.8

_,5 o,,_'_'

..q..,._:,3.

.2 5,9

42.5 14,6 21 5

,0 .0
..... .0 ,0
-
-
.2 ..l... .i 1 _

1_ i ,1 _ i .;_

,? .4

4 1 .5

'

,1 .7

.7

," , 3

_ :_

; .1

!-_

: ,I ; 0

1 I .0

._ '

-

1,0

:_ _ l,O "-_ I ::t

2._ 3:1

5 _>_ 4 £

_,_{ ,2

4 ,I

1.5 : 14,5 190

0-
.0 ,0 -.0
,0 -, .0 .0 -
-
.... - .1
,1 _ ":, , i .3 ', .4 ,9 t,3

LIST POINTER DISTANCES
Table A-4 List pointer distribution for CONGEN

1,51

dist.

Before llnearlzatlon

CAR

CDR

pet cum

pct cure

_>2,66 <256 <128
<64 <32.
15 14 13 12 II 10
9

2.5 72.7 1.6 70.2 2.7 68.6 3,2 65,9 2.1 62.7
.2 60.6 .4 60.3 ,4 59,9 .8 59,5 ,9 58.8 1.3 57,8 1.4 56.5

1.6 23.0 ,2 21.4 .l 21.2 .I 21,1 .3 21.0 - 20.7 - 20.7 - 20.7 - 20.6 20.6 .I 20.6
1,0 2,0.5

After car-direction

CAR

CDR

pct cure pct cure

After cdr-dlrectlon

CAR

CDR

pct cure

pct cure

.0 93.6 .0 93.6 .0 93.6 .0 93,6 ,0 93.6 .0 93.6 .0 93.6 .0 93.6 .0 93.6 .0 93,6 ,0 93,6 .0 93.6

-" 1.4 1.7 .4 .1 .I .I .1 .2 7.3

98.3 98.3 98,3 96.9 95.2. 94.8 94.8 94,7 94.6 94.5 94.4 94.2.

5.1 .4 .7
I 1,7 9.7 ,2. ,4 .3 .6 .9 3.4 .6

93.8 88.7 88.3 87.6 75.9 66.2. 66.0 65,6 65.4 64.8 63.8 60.4

.0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3 .0 98.3

7 .9 41,7

.4

6 5
4 _I 38
2, 1
6
i

t:Z:;; <

2 3 4 5

L) 6

7
8 \9

10 II

12

13 14 15

<32 <64 < 128

<256 >_256

3.4. I,1
2.2 132..30
3.3 28.8
.0
7.5

40.9 37,5
364 35542. I
;32,.2 28.8
.0
7.5

_- 7,5 ,l 7,6

.4 8.1 -- 8,1

6 8,6

,i 8,7
3,0 I 1.7 .I 11.8

- I 1.8 .I 11.9

- 11.9

,I 12.0 - 12.0 - 12.0
,8 12.8 I .4 14.2 1.2 15.4

1.4 10,6

16.8 27.3

.2 ,6 .2 ..21 1.9 15.9
.0
49.9 8.8 1.7 1.0 1,0 ,6
1.5
.7 4.3
.6 ,5 .4 ,3 ,2 .2 1.0 ,6 .7 .5 2.3

19,4

.0 93.6

1I

19.0 18.8 18.2 1189,.05 17.8 15,9
.0
49.9 58.8 60,5 61.5 62,5 63.1
04,0
65.2 69.5 70. I 70.7 71.0 71.4 71.6 71.8 72.8 73,4 74.1 74.6 77.0

,0 ,0
.0 ,.00
.0 93.6
.0
-

93.6 93.6
93.6 9933,.66
93.6
93.6
.0

.0 -

.0 ,0 -

.....

°0 .....
,0 .0 -

.I

- .I

- .I - ,1 .I .2

.9 1.1 ,7 1.8 2.2 4.0

.8 4.8 1.6 6.4

.5 .6 1. I 1..44 10.6 71.2
.0
,0 .0
-
.0 -
.I
.1 -. .3 .3 .9

86.4 85.3 84.8 84.3 8836..29 81.7 71.2,
.0
.0 .0 -
-
-
.I .I .I .2 .2 .5 .8 1,7

1.5 58.0

1.5 2.2.0
1.7 51.4.8
1.6 24.3
.0
.0
.0 ,0
.0 .0

56.4 55.0 33.0 5319.,72
25.9 2,4.3
.0
.0
,0 .0
,0 .0

.I .I
,Z .I .3
.3 ,3 .3 3 .4 . I .4 ,4 .8 .8 1.6 2.2 3.8 .7 4.5 1.7 5,2

.0
.0 .0 .0 ..00 .0 98.3
.0
.0 -
0
-
.0 -
.0 .0
.0
.I .3 .2 1.0

98.3
98.3 98.3 98.3 9988..33 98.3 98.3
.0
.0 -
--
.I .I .I .I .I .I .I .1 .2 .5 .7 1.7

152 APPENDIX A
Table A-5 List pointer distribution for WIRE

Before ilneartzatlon CAR CDR

After car-direction CAR CDR

After calf-direction CAR CDR

dist. _>2,56 <256

pet cure 9.6 51.9
.9 42.3

pct cure 1.2 47.8
.4 46.6

pct cure .0 90.2, .0 90.2

pct cure - 99.3 .1 99.3

pct cure 7.9 90.3 2.7 82,.4

pct cure .0 99.3 ,0 99.3

< 12,8 <64

1.4 41.4 1.5 40.0

.9 46.2, 1.0 45,3

.0 90.2 .0 90.2

.2 99.2 .5 99.0

4.2 79.7 5.9 75.5

.0 99.3 .0 99.3

<32 15

2.1 38.5 .5 36.4

1.6 .14.3 .5 42.7

.0 90.2 .0 90.2

1.3 98.5 - 97.2

7:4 69.6 .1 62,2

.0 99.3 .0 99.3

.

1 4 .3 35.8

.2 42.8

.0 90,2

- 97, 1

.3 62.. 1

.0 99.3

13 .3 35.6 12 .4 35.3

.1 42,.0 .1 41.9

.0 90.2, ,0 90,2

97 I ,1 97.1

.8 61.8 .7 61.0

.0 99.3 .0 99.3

1 1 .3 34.9 10 .4 34.7

.2 41.8 .7 41.t!

.0 90.2

'_:_' ' 0

5.0 60.4

.0 90.2 4.3 SL 9

.4 55.4

.0 99,3 .0 99.3

9 .5 34.2 8 1,0 33,7

.1 41.0 .6 ,10.9

0 90.2 0 90,2

.1 92,6 .I 92.5

.5 54.9 .4 54,4

.0 99.3 .0 99.3

r-,

6 2.0 32,0

3.2

'_

5 3.7 ;30.0

.3

4 3.4 26.4

.i

_l 73 4.5.7 2323..70 .!3

2 5 6 18.5

,I

1 12 8 ,
00

12.8 .0

36,1 ,0

1 6 6 6.6 39.7

2 1 1 7.7

.O

c5

3 1 1 8.7

1,6

<

4

3 90

.9

-'_ 5 1 7 i0,7 b&

.9

0

6

1 10 7

I1

7

0 1O 7

.5

8
/ 9
10 I1 123 13 1,1 15 <32 <64 <128 <256 >25@

.0
.l
.3 .4 .5 .8 35.2

10

8

10 8

10 8

10 8

10 8

10.9

10. 9

10,9

11,2

11,6

12. i

1£.9

48.1

. _1
.2 .3 .4 .1 .1 .! ,3 1.8 6 .5 ,3 1.8

,10.2

0 90,2

7.4

37.0 36,7

0 90,2 0 90,2,

1,8 i.i

3406,.35 36,2

0 900..22 0 90,2

3,6.8 .h

3@,1 _0

90.2, 90.2, 77.1
.............................................................
0 .0 .0

39.7

0 .0

40.3 0 .0

41.9 0 .0 .0

,12.7"

0 .0

43,6 0 .0

4,1.7

0 ,0 .0

,15.3

0 ,0 0

"1 5.

_7

,16.0

(') , ()
0 ,0

46 2 ,16,fi

.0

-.

,t6.7 .0 -

,16.8

.0

-

,16.9

.0

_.

47.2

49.0 -

49.6 .;

50 i

.1

.2

50.4 .i ,3

52.2 9,5 9.8,

,0
,0 .0 .,9 .0 _0 .0 .0
-
,7

91,6

,6

84 2 82.4

.7 1.3

9821 43 28.11.1

77'./'

3.2

7"?.I 19. i -4. .0! ,0

....i ,0

;0

,0

.0

,0

(

52..9 52.3 51.6 540..03 22.Z 19.1
.0 .0 .0 ,0 ,0 .0 .0
-

.0 .0 .0 ,0 .0 99.3 .0 .0 .0 .0
.0 .0 .0

= _r)
.o

" . fi}
.0

ri ,,

'-

.0 -

99.3 99.3 99.3 999.,33 99.3 99.3
.0 .0 .0 ,0
-
--
-

_:; -

,0

-.

.0 ,0 -

.1 .1 -

.I .2

.I

.t ,3

- .1

"7 9.5

9,7

.7 ,7

APPENDIX B
Rplaca and Rplacd

This appendix reports data on rplaca and rplacd executions in CONGEN, NOAH,

and SPARSER. We are interested here in how often pointers of a given type are

rf_placed by pointers of another given type. Lists are broken down into three

distance ranges: adjacent lists (distance 1 in either direction from the cell

rplac'd); no.arby lists (distance 2 to 3_ in either direction); and distant lists (all

others). All rplaca and rplacd executions that occurred during the Chapter 3 runs

were recorded. The tables that follow give the percentage of rplacas and rplacds

account_:,d for by each combination of old type (the type of the pointer in car or

c_lr b_fore ol,eration).

the operation) and new type (the type of the pointer after the For example, Table B-1 says that in CONGEN, 2.1.7 percent of the

rpl,'_c,_s replaced a NIL by an atom. The corresponding row sum shows that the

replacement

of NILs by any type accounted for 34.5 percent of the rplacas; the

col um n sum shows that rplaca installed atoms 2,,5.7 percent of the time.

153

154 AP_'ENDIXB

Replacement

Table B- 1 of data types by rplaca in CONGEN (g15 executions)

()I.,D TYPE

adjacent list

ad.jacent list
noarby list di:;tant l l:;t NII,

0,0 0.2 0,0 o.0

atonl

0.0

:_mal]
i nte_,;er other
type,_ COLUMN SUM

O,n 0.0 0.2,

nearby list

distant list

NEW TYPE NIL atom

snla!l integer

other types

ROW SUM

l .5 3.5 0.0 2,4 0.0 0.0 7.4

1.3 0.1 0,0 0.3 0.0 0.0 2.0

0.2 31.9 6.0 0.5 0.0 0.0 38.7

_.c_

6.2 6.1 21,7

0.4 0.0 34.5

4.0

2.0 0.0

{).7 0.0 0.0

6.7

c) 0

0.0

0,0

__ 0 10.7

0,0

10.'7

O.0

0.0 0.0

_i_0 0.0 0.0

0.0

7,1 43,7 12.1 25.9" 11.l 0,0 100.0

Replacement

Table B-2 of data types by rplacd in CONGEN (i 6815 executions)

OLD TYPE

adjacent list

adjacent list
nearby list distant list NIL

0.2 -
32.3

other
types COLUMN SUM

0.0 32,5

nearby list

NEW TYPE distant
list NIL

other types

ROW SUM

0.4

- 24.4

- 25.0

1.6 0.4 - 3,0

5.3 0.0 _,5 0.0

7.3 5.6

12.1

1.7 14.3 0.7

61.0

0.0 0.0 0,3 0.8

1,2;

14.2

5. _ 46.8

I .5 100,0

RPLACAAND BPLACD
Table B-3 Replacement of data types by rplaca in NOAH (2535 executions)

155

OLD TYPE

adjacent list

adjacent list
nearby list
distant list
NIL

0.2 0.6 0.0 3.1

atom

0.0

small integer other types COLUMN SUM

0.0 0.0 3,9

nearby list

distant list

NEW TYPE NIL atom

small integer

other types

ROW SUM

0.4 0.7

- 0,0

0.0 0.0

1.3

0.4

1.9 0.0 0.0

0.0 0.0

2.9

0.6

5.0 0.0 0.2

0.0 0.0

5.8

0.4

0.4 3.6 0.4

0.0 0.0

7.8

0.0 0.4 0.9 1.7 4.5 0.0 7.5

0.0 0.0 0.0 4.5 70.3 0.0 74.7

0.0

0.0 0.0 0.0

0.0 0.0

0.0

1.8 8.4 4.5 6.7 74.7 0.0 100.0

Table B-4 Replacement of data types by rplacd in NOAH (4377 executions)

OLD TYPE
adjacent li.st nearby list distant ]ist NIl,

adjacent list 0.8 0.4
(3,8 ! 1.7

other type_;

I O.O miI

COLUMN ] 13.7

Sll M

1

nearby list

NEW TYPE distant
list NIL

other types

ROW SUM

4.8

0.9 10.5 0.0

17.0

9.2

0.7 9.1 0.0

19.4

0.g 20.8 6.g 0.0 28.0 13.5 4.8 3.3 0.2 33.7

0.0 0.0 O.O 1.9 1.9

27.7 27.3 ?_9.1 2.1 100.0

156 APPENDIX B
Table B-5 Replacement of data types by rplaca in SPARSER (688 executions)

OLD TYPE

adjacent list

adjacent list
nearby list distant list NIL

0,6 0.0 0,0 i7.8

atom

0,0

small
integer other
types COLUMN SUM

0.0 0,0 18.4

nearby list

distant list

NEW TYPE NIL atom

small integer

other types

ROW SUM

000

0_6 0.0

0,,0 0,0 0.0

1.2

2.2

4.4 0o0 0.0 0,0 0.0

6.6

0.1

3.9 0°0 0.0 0.0 0,0

4.1

2.8 13,.7 0.0 16o5 0.0 0.0 50.7

0,0

0.0 0,0 26.4

0.0 0.0

26.4

0.0

0,0 0,0

0°0 11.1 0.0

11.1

0.0

0.0 0.0

0.0 0.0 0,0

0.0

5.1 gg.6 0.0 42.9 11.1 0.0 100.0

Table B-@ Replacement of data types by rplacd in SPARSER (4619 executions)

OLD TYPE

adjacent list

adjacent list nearby list distant list NIL
other types COLUMN SUM

0.Z 3,6 Z.6 31.4 0,0 37.8

nearby iL_t

NE\V TYPE

distant list

NIL

other types

ROW SUM

0.Z 0,3 9.3 0.0 10.0

I o3 0,5 Z 1,0 0..0 Z6.3

0.5

5.0 9.0 0.0

17.0

6.6 4.7 3.8 0.0 46.6

0°0 0.0 - 0.0

-

8.7

10.5 43.1

0.0 100.0

APPENDIX C

Memory Reierences in Algorithm III and Fisher's Algorithm

As discussed in Section 3.5 of Chapter 4, we will compare the two algorithms

by counting thp number of times each must access memory to read or write a list cell. We assume that car and cdr are contained in a single word of memory. We

also assume a certain amount of straightforward

optimization with respect to this

measure, e.g., if car(x) and cdr(x) are both read (written) in a single iteration of

an algorithm,

we will ._;ay that one memory read (write) has taken place.

Operation counts will t,e functions of n, the number of cells in the structure to be

copipd; of a, the fraction of cars that point to lists; of d, the fraction of cdrs that

'point to lists; and of some additional oth_,r.

terms associated

with one algorithm

or the

1. Analysis of Aloorithm III
PassO_z_. Each cpll of the original structure is read once for every pointer to it. The first of these reads occurs when the trace first encounters that cell (Step A:3); the rest occur in ordeT to obtain the forwar4ing address stored there (Stops A5 and B3). The numb¢:,r of li,_t pointers, al_d therefore thr_ number of reads of" original cells, is an+du+l (_,he J comes from the pointer root). More reaet._ wJ.l] occur when cells are popped off the k-list in Step BE. If k 1 is the fraction of cells
157

158 APPENDIXC

that are put on the k-list in Step A5, then another kln reads will occur, for a total of au+dn+k l n+ 1 during Pass One. Write operations occur when each forwarding address is written in Step A4 (n writes), when eachnew cell is written in Step A5 (another n), and when cells on the k-list have their contents altered in Step B3 or B4 (kln writes). Thus 2n+kln write operations will occur during Pass One.
B-list Pass. Let b be the fraction of cells that are put on the b-list. Then for each b-list cell the following operations will occur: the original cell and its copy will be read and written once each, and the cells pointed to by the original car and cdr will be read to retrieve their forwarding addresses. The totals are 4bn reads and 2bn writes.

Pass Two. Cells formerly on the b-list will be visited once (Step C2.5), but

their copies will be neither read nor written. Thus there will be n reads of

original restored

cells, but n-bn of copy cells. B-list cells have had their original cars already, so n-bn writes will occur for this purpose. Copy cells will be

written in Step C4 if car is a list, but since copies of b-list cells never see Step C4,

this will contribute an-bn writes. Let k 2 be the fraction of cells that go on the klist in Step C4 (precisely those with "*" in their cdrs). Then k2.n reads and kzn writes will occur in Step D3. The totals for Pass Two are 3n-bn+k3n reads and

n+an-2bn+k2n

writes.

The grand totals for Algorithm III are fin+an+dn+3bn+kln+k2

n+l reads and

3n+an+k l n+kzn the sum is

writes.

Neglecting

the single read on behalf of the pointer root,

m emory accesses.

T = (5+2a+d+3b+gkl+Zkg)n

MEMf)Iq',? REFERENCES IN ALGORITHM ill AND FISHER'S ALGORITHM

159

2. Fisher's Algorithm
Fisher's ab;,orithm, slightly optimized and translated for easier comparison with Algorithm III, appears below. For a thorough discussion of this algorithm, see [Fis75].

Pass One

A 1. [Initialize.]

v,-j,-avail, x-root.

Ag. [Get free cell.] n,-avail, avail*avail+l.

A3. [Proce:;s cell x.] car(n)*car(x), address is stored in cdr rather below.

d,-cdr(_:), cdr(x)*n. (The forwarding than car.) Go to the appropriate box

atom cdr(n),-d

d AV list cdr(n)*cdr(d)

UV list cdr(n)*d, go to A2

x*d

A4. [Done?] If j>avail then go to Step B1.

AS. [Get nexl r:ar] x*car(.j), j*.j+l. (Note the sequential scan of the copy to find sublists.) Go to the appropriate box below:

atom

X
AV list

[IV list

go to A4

goCar(JA-t4o)1.-cd r(x)

goCar(JA-tZo)1*avail]

Pass Two

B1. [Initiali_'e.] n*,j,-v, ×*root.

. BE. [Exchanl;o (:drs.] d,-cdr(n), cdr(x),-d, cdr(n),-x, n*n+l.

B:3. [End of sogment?] If d is an old list (i,e., not(atom(d)) not(new(d ))), then x,-d and go to B2.

and

I_o APPENDIX C

B4. [Done.'?] If ,j>n then go to Step C1.
BS. [Check car(j).] If car(j)=n then d*car(cdr(j)), j,-j+l, and go to Step B3. Otherwise j*j+l and go to Step B4. (car(j)=n iff car(j) was the next list traced during Pass One.)

Pass Three

C 1. [Scan in reverse order.] n_n- 1, x_cdr(n), d,-cdr(x).

C2. [Write final cdrs.] d points to an atom, a new list cell (new(d)), or an old list cell. Go to the appropriate box below:

atom cdr(n)_d

d new list
cdr(n)*d cdr(x)*cdr(d)

old list ] cdr(n)_-n+l
]

C3. [Done?] If n>v then go to Step C 1. Otherwise, avail,-j and halt.

3. Analysis of Fisher's Algorithm
Pass One. Just as in Algorithm III, each cell of the original structure is read once for every pointer to it. Thus there are an+dn+l reads of cells in the original list. In Step A5 each copy cell is read once, so the total number of reads during Pass One is n+an+dn+ 1. Writes occur when forwarding addresses are written into original cells in Step A3 (n writes), when copy cells are first created in Step A3 (n writes), and when new list cars are written a second time in Step A5 (an writes). The total number of writes is 2n+an.

MF_MOrqY REF 'LP_£NCE,SIN ALGORITHM II1 AND FISHER'S ALGORITHM

.1 61

Pass Two. All n,_w cells are read in Steps Bg. and BS, for a total of 2n reads. One original ceil is l'ead each time car(j)=n in Step BS. This happens when car(j)

wa,_ a UV point or during Pass One; let c 1 be the fraction of cells for which this

occurs. Then 2n+cln reads occur in Pass Two. Writes happen only in Step B2, in

which P.n.

each cell in the original list and the copy has its cdr written,

for a total of

Pas,s Three. Each coll in original and copy is read in Step C1, for 3n reads. Ev_,ry new cdr is written once in Step Ca, for a total of n writes. Whenever d is a n_w list in Step C2, one additional read and write are done. This condition

corresponds to cdr(x) having been an AV pointer during Pass One; let c Z be the

fraction of cells for which this happens. wilt ,s in Pass Thrpo.

Then there are gn+c2n reads and n+c2n

Sun|ruing over all three passes, there are 5n+an+dn+cln+cgn+l

reads and

5n+an+c2n writes. Neglecting the single read associated with root, the total is

m em ory accesses.

T' = ( lO+2a+d+c 1+2c2)n

BIBLIOGRAPHY

[Bar68] [Ilat75]
[Bob67] [Bob68] [Bob72]
[1_ob74]
[11ob75] [Bobl_76] [Che70] [C']a75a]
[Cla75b] [ClaY6a]

Barnett, J.A., and Long, R.E. The SDC LISP 1.5 System for IBM 360

Computors.

System Development Corp., Santa Monica, Cal., Jan. 1968.

Bat_s, M. The Use of Syntax Trans. o12 Acoustics, Speech, pp. 112-117.

in a Speech Understanding

System. IEEE

and Signal Processing

23, 1 (Feb. 1975),

Bobrow, D.G., and Murphy, D.L. The Structure Two L_)vel Storage. CACM 1 O, 3 (March 1967),

of a LISP System 155-159.

Using

Bobrow, D.G., and Murphy, D.L. A Note on the Efficiency

of a LISP

Computation

in a Paged Machine. CACM 11, 8 (Aug. 1968), 558,560.

Bobrow, TENEX, (March

D.G., Burchfiel,

J.D., Murphy,

D.L., and Tomlinson,

a Paged Time Sharing System for the PDP-IO. CACM

1972), 135-143.

R.S. 15, 3

Bobrow, Artificial 153-174.

I).q., and Raphael, B. New Programming

Intelligence

Research. Computing Surt_eys

Languages

for

6, 3 (Sept. 1974),

I_obrow, 415.

I).G. A Note on Hash Linking.

CACM 18, 7 (July 1975), 413-

Bobrow, Interli._p

R., and Grignetti,

M. Initial Mea.qurements

on the

System. Bolt Beranek and Newman, Inc., July 1976.

1975

Cbeney, C..,I. A Nonrecursive (Nov. 1970), 677-678.

List Compacting Algorithm.

CACM 13, 11

Clark, D.W. Copying List Structures

Technical

Report,

Dept. of Computer

University,

Oct. 1975.

Without Science,

Auxiliary Carnegie

Storage. -Mellon

Clark, D.W. A Fast Algorithm for Copying Binary Trees.

Proce,_sing

Lettors 4, 3 (Dec. 1975), 67,-63.

Information

Clark, D.W. An Efficient

List-Moving

Algorithm

Workspace.

CAC1FI 19, 6 (June 1976), 352-354.

Using Constant

163

164 BIBLIOGRAPHY

[Cla76b]

Clark, D.W., and Green, C.C. An Empirical Study of List Structure Lisp. CACM, to appear.

in

[Coh67]" [Deu73a] [Deu73b] [Deu76] [Fen69]

Cohen, J. A Use of Fast and Slow Memories Languages. CACM 10, Z (Feb. 1967), 8Z-86.

in List-Processing

Deutsch, L.P. An Interactive

Program

University of Ca., Berkeley, May 1973.

Verifier.

Ph.D. Thesis,

Deutsch, L.P. A LISP Machine with Very Compact Programs. IJCAI, Stanford, Cal., 1973, pp. 697-703.

Third

Deutsch, L.P., and Bobrow, D.G. An Efficient, Incremental, Garbage Collector. CAC1PI, to appear.

Automatic

Fenichel, R.R., Virtual-Memory 612.

and Yochelson, J.C. A LISP Garbage-Collector Computer Systems. CACM 12, 11 (Nov. 1969),

for 611-

[Fis75] [Gre75] [Gre74] [IIan69]

Fisher, D.A. Copying Cyclic List Structures in Linear Time Using Bounded Workspace. CACM 18, 5 (May 1975), 251-252..

Green, C.C. Private communication, April 1975.

Greenblatt, Laboratory

R. The LISP Machine. M.I.T. Artificial Intelligence Working Paper 79, M.I.T., Cambridge, Mass., Nov., 1974.

lIansen, W.J. Compact List Representation:

Definition,

Garbage

Collection, and System Implementation.

CACM 12, 9 (Sept. 1969), 499-

507.

'[Knu68] [Knu73] [I,in73] [Lin74]

Knuth, D.C. The Art of Computer Programming,

Volume

Fundamental Algorithms. Addison-Wesley, Reading, Mass., 1968.

1.

Knuth, D.C. The Art of Computer Prof_ramming, Volume 3: Sorting and Searching. Addison-Wesley, Reading, Mass., 1973, Section 6.4.

Lindstrom, G. Scanning List Structures Without Stacks or Tag Bits. Information Processing Letters 2, Z (June 1973), 47-51.

Lindstrom, G. Copying List Structures CACM 17, 4 (April 1974), 198-ZOZ.

Using Bounded Workspace.

BIBLIOGRAPHY

165

[Mat70]

Mattson,
Techniques 117.

R.L., Gecsei, for Storage

J., Slutz, D.R., and Traiger, I.L. Evaluation

Hierarchies.

IBM Systems J. 9, 3 (1970), 78-

[McC60] [McC62]

McCarthy, Computation

J. Recursive Functions of Symbolic Expressions

and their

by Machine-Part

I. CACM 3, 4 (April 1960), 184- 195.

McCarthy, Cambridge,

J., et al. LISP 1.5 Programmer's Ma,s.s., 196fi.

Manual.

M.I.T. Press,

[Min63] [Moo74]

Minsky,

M.L. A LISP Garbage Collector

Algorithm

Using

Secondary

Storage.

Artificial

Intelligence

Project Memo 58

M.I.T. Project MAC, Cambridge, Mass., Dec. 1963.

Serial (rev.),

Moon, D.A. M/ICLI,SP Reference

Cambridge,

Ma,_,_., April 1974.

Manual.

M.I.T. Project

MAC,

, [Oua69] [I{eb73] [Rei73] [Sac75] [Sam67] [Sch67]
[Sha48] [Smi74]

Quam, I,.H. Stanford I, ISP 1.6 Manual. Pro.joct, Stanford, Cal., Sept. 1969.

Stanford

Artificial

Intelligence

Reboh, R., and Sacerdoti, E. A Preliminary

QLISP Manual.

Research Institute AI Center Technical Note 81, August 1973.

Stanford

Reingold, E.M. A Nonrecursive (May 1973), 30,5-307.

List Moving Algorithm.

CACM 16, 5

Sacerdoti,

E. The Nonlinear Nature

Georgia, I!.S.S.lt., 1975, pp. g06-314.

of Plans.

Fourth

IJCAI, Tbilisi,

Samll_'l:_(m,

P.A. Economics:

An Introductol

Edition. McGraw-Iiill,

NewYork,

1967.

y Analysis,

Seventh

Schorr, Proceduro 8 (Aug.

t!., and Waite. W. fr, r Garbag_ Collection
1967), 501-506.

An Efficient

Machine-Independent

in Various List Structures.

CACM 1 n

Shannon, Technical

C.E. A Mathematical

Theory of Communication.

Journal, vol. g7 (July 1948), 379-493.

BellSystem

Smith, D.H., Ma.sinter, L.M., and Sridharan,

N.S. Heuristic

DENDRAL:

Analysiz

of Molecular

Structure.

Computer

Representation

a.r_d

ll4anipu]ation

of Chemical

Information

(W.T. Wipke, S. tteller, R.

Yeldman, E. Hyde, eds.). John Wiley g: Sons, Inc., 1974.

166 BIBLIOGRAPHY

[Tei74] [Van?] [Wei67] [Zip49]

Teitelman, W. INTEBLISP Reference Research Center, Pale Alto, Cal., 1974.

Manual. Xerox Pale Alto

Van der Peel, W.L. A Manual of HISP for the PDP-9. Technical University, Delft, Netherlands.

Weissman, C. LISP 1.6 Primer. Belmont, Cal., 1967.

Dickenson Publishing

Company,

Zipf, G.K. Human Behavior and the Principle

Addison-Wesley

Press, Cambridge, Mass., 1949.

of Least Effort.

