Introduction to the T3D
A one day course
Stephen Booth, John Fisher, Neil MacDonald, Peter Maccallum, Joel Malard, Alistair Ewing, Elspeth Minty, Alan Simpson, Stuart Paton, Stephen Breuer
Edinburgh Parallel Computing Centre, The University of Edinburgh
ec pc

Table of Contents

1 Parallel Computing ......................................................................1 1.1 Introduction ............................................................................... 1 1.2 Why is Parallel Computing Important?................................. 1 1.3 Evolution of Parallel Computing............................................ 1 1.4 Parallel Computing at Edinburgh .......................................... 2 1.5 Concepts of Parallel Computing............................................. 3 1.6 Introduction to Parallel Architectures ................................... 4 1.7 The Future... ............................................................................... 7
2 Decomposing the Potentially Parallel ....................................9 2.1 Introduction ............................................................................... 9 2.2 Trivial decomposition .............................................................. 9 2.3 Functional decomposition ....................................................... 10 2.4 Task farms .................................................................................. 10 2.5 Regular domain decomposition.............................................. 11
3 Message-passing concepts ..........................................................15 3.1 Messages..................................................................................... 18 3.2 Moving Data Around: Point-to-Point Communication ..... 20 3.3 Cooperation: Collective Comms ............................................. 22 3.4 Applications............................................................................... 24 3.5 Summary .................................................................................... 24
4 Data Parallel Concepts ................................................................25 4.1 Data Parallel Languages .......................................................... 27 4.2 Moving data around: Data Shifts........................................... 29 4.3 Cooperation: Global operations............................................. 29 4.4 Applications............................................................................... 29 4.5 Summary ................................................................................... 29
5 Cray T3D Hardware and Software Overview .......................31 5.1 Introduction .............................................................................. 31 5.2 Hardware overview.................................................................. 31 5.3 Software overview ................................................................... 34
6 Compiling, Debugging and Profiling Codes on the Cray T3D 39
6.1 Overview ................................................................................... 39 6.2 Logging on to the T3D service ................................................ 39

Edinburgh Parallel Computing Centre

i

Writing Message Passing Parallel Programs with MPI
6.3 Compiling on the J90 front end .............................................. 41 6.4 Running code on the T3D........................................................ 44
ii Course notes

Parallel Computing

1 Parallel Computing

1.1 Introduction
This talk looks at the reasons which led to the development of parallel computers, and tries to provide an overview of the different architectures of parallel computing in use today. We end by speculating on the future of parallel computing.
We start on a positive note as we attempt to motivate the use of parallel computing over conventional serial machines for computationally intensive work.
1.2 Why is Parallel Computing Important?
The success of computational science to accurately describe and model the real world has helped to fuel the ever increasing demand for cheap computing power. Scientists are continually looking for ways to test the limits of theories, using high performance computing to allow them to simulate more realistic systems in greater detail. Parallel computing offers a way to tackle these problems in a cost effective manner.
One reason for this is economic. By making use of "off the shelf" components, parallel computers can offer higher performance at lower prices than machines which use specially developed processors. In addition, the inherent scalability of parallel computers allows for them to be upgraded as the need arises. Whereas serial architectures are upgraded by making the previous processors obsolete, parallel architectures can, in theory, be upgraded simply by adding more processors.
However, there exists another reason, fundamental physical law, which will ultimately limit the speed of a single processor, irrespective of the economics. Movement of information forms the basis of a computer, but the speed of this movement is eventually limited by the speed of light. If instead the distance travelled by this information was reduced, eventually the need to avoid the uncertainties introduced by quantum mechanics would limit the separations of the paths along which the information could travel.
These two reasons of economics and physics, coupled with the inherent scalability of parallel computers, points to a future of high performance computing which is based in some way on the ideas of parallelism. We now outline a brief history of parallel computers.
1.3 Evolution of Parallel Computing
The concept of parallelism is not new. Over 100 years ago the idea of performing calculations in parallel had occurred to Charles Babbage when he designed the Analytical Engine. In 1842, General L. F. Menabrea wrote a paper entitled Sketch of the Analytical Engine invented by Charles Babbage, describing the design and capabilities of the machine. In it he writes:

Edinburgh Parallel Computing Centre

1

Parallel Programming on the Cray T3D
"When a long series of identical computations is to be performed, such as those required for the formation of numerical tables, the machine can be brought into play so as to give several results at the same time, which will greatly abridge the whole amount of the processes."
Babbage was aware of the possible speed advantages that could be gained by performing independent calculations simultaneously. Unfortunately the technology available to him at the time was not capable of realising his ideas. In the early 50's John von Neumann propounded a basic design for electronic computers: a single processing unit connected to a single memory store. This basic architecture had the advantages of being conceptually simple (with only one event happening at a time), simple to build (with only one of every component) and made sense economically (with the expense and poor reliability of the components). Von Neumann too was aware of the potential of parallelism (he was perhaps the originator of the idea of the cellular automata), though again there was not the expertise in either hardware or software to realise this machine.
These technical difficulties were in part solved during the 60's, with the introduction of solid state components, which reduced the cost of large computing machines, and also the development of the software knowledge required to handle concurrency. However it wasn't until the 70's that parallel computing became a real prospect as opposed to a research curiosity. The process of Very Large Scale Integration (VLSI), allowing thousands of transistors on a single circuit offered the possibility of cost effective production of powerful chips. This coupled with the further development of the concurrent programming methods, provided the skills and the means to produce truly parallel machines, of which the ILLIAC IV built at the University of Illinois in the US was an early example.
Perhaps the greatest spur for the development of parallel computers was the success of large vector supercomputers in tackling scientific problems, such as weather modelling, aerodynamic analysis of aircraft design and particle physics. The growth of computational science through the use of vector computers fuelled the development of parallel machines to do more and better science at a cost accessible to the academic community. It wasn't until the 80's that parallel computers were being built for wide use, with early machines such as the ICL Distributed Array Processor (DAP) in the UK, and the CosmicCube (a hypercube type machine) in the US. These machines lead the way for the wide range of parallel machines available today.
Throughout this period, the lack of experience in producing algorithms with sufficiently large parallel sections also hampered the development of parallel computers as the initial attempts at parallelisation did not yield sufficient performance benefit. For example, if an algorithm has 20% sequential code and 80% parallel code, then the speed at which it could execute on a parallel computer is limited by the speed of the 20% of sequential code. This idea that the performance improvement to be gained by using parallelism is limited by the fraction of serial code has come to be known as Amdahl's Law, first proposed by Gene Amdahl in 1967. At the time, this appeared to be a serious obstacle to the development of parallel computing. However, as technology has become more advanced and algorithms more sophisticated, parallel computers have become an increasingly viable option. The important point to consider is not "can I parallelise part of my code" but "can I parallelise a computationally intensive part of my code". By scaling the problem size as the number of processors is increased the effects of these limitations can be reduced. Amdahl's Law serves as an important guide to how much parallelising code will improve performance and how best to use the resources available to gain maximum benefit.
1.4 Parallel Computing at Edinburgh
Edinburgh's involvement with parallel computing began in 1980, when physicists in Edinburgh used the ICL Distributed Array Processor (DAP) at Queen Mary College in
2 Course notes

Parallel Computing
London to run molecular dynamics and high energy physics simulations. Their pioneering results and wider University interest enabled Edinburgh to acquire two of these machines for its own use, and Edinburgh researchers soon achieved widespread recognition across a range of disciplines: high energy physics, molecular dynamics, phase transitions, neural network models, protein crystallography, stellar dynamics, image processing, meteorology and data processing.
With the imminent decommissioning of the DAPs, a replacement resource was needed and a transputer machine was chosen in 1986. This machine was the new multi-user transputer based Meiko Computing Surface. The Edinburgh Concurrent Supercomputer Project was then established in 1987 to create a national parallel computing facility, available to academics and industry throughout the UK.
In 1990 this project evolved into the Edinburgh Parallel Computing Centre (EPCC) to bring together the many strands of parallel research and applications activity in Edinburgh University, and to provide the basis for a broadening of these activities. Soon after, a number of i860 Meiko Computing Surfaces and a Thinking Machines Corporation CM-200 were installed.
The start of 1994 saw the arrival of a new generation Meiko machine, the CS2, and in April 1994, a 256-alpha Cray Research T3D was installed at EPCC (upgraded to 320 nodes in December 1994). This machine is currently one of the fastest computers in the world. Compared to the original DAPs installed at EPCC, the T3D is over 100 times as fast and has 1000 times as much memory.
The development of parallel computing at Edinburgh closely mimics the growth of parallel computing world wide. Every 3/4 years a new generation of parallel machines appears which provide an increase in speed of around an order of magnitude. In the case of EPCC, the machines have developed from the original DAPs to the transputer based Meiko Computing Surfaces to the i860 based Meiko Computing Surfaces and Connection Machine Corporation's CM-200 to the Cray Research T3D and the Meiko CS2, with each generation providing greater computing power, larger amounts of memory while providing a wider range of programming tools and more stable environment.
1.5 Concepts of Parallel Computing
With serial computing, on Von Neumann style machines, there is one processor which is executing a series of instructions in order to produce a result, i.e., there is a logical flow of control through the program. This is true even of programs, such as operating systems, which give the appearance of performing multiple tasks. At any one time there is only one operation being carried out by the processor.
Parallel computing is concerned with producing the same results using multiple processors. The problem which is to be solved is divided up between a number of processors. There are a variety of ways in which the problem can be divided up between the available processors. For example, the problem decomposition could be a functional decomposition or a data decomposition. A discussion on the different methods of decomposing problems is contained in the Decomposing the Potentially Parallel chapter. Dividing the problem up in a sensible and efficient manner is critical to achieving good performance on a parallel machine. The aim of an efficient code is to keep all the processors busy (load-balancing) while minimising the amount of communication (usually the bottleneck in parallel calculations), though often there is a trade off between communications and processing.

Edinburgh Parallel Computing Centre

3

Parallel Programming on the Cray T3D
1.6 Introduction to Parallel Architectures
This section looks at the two main types of computer architectures which are seen. A widely used way these architectures are classified is with Flynn's taxonomy, which labels hardware by instruction stream and data stream. For example, a conventional serial computer would be classified as Single Instruction Single Data (SISD) as the processor executes one instruction on one piece of data at each step. We list the main types of parallel architectures below.
Flynn's Taxonomy

Single Instruction

Multiple Instruction

SISD

SIMD

MISD

MIMD

SM DM VSM
1.6.1 Single Instruction Multiple Data (SIMD)
A typical example of a SIMD architecture is illustrated in Figure 1. Typically this consists of many simple processors, each with a local memory in which it keeps the data which it will work on. Each processor simultaneously performs the same instruction on its local data progressing through the instructions in lock-step, with the instructions issued by the controller processor. The processors can communicate with each other in order to perform shifts and other array operations.

Controller Host

4 Course notes

Array of processing elements
Figure 1: An example of a SIMD architecture

Parallel Computing
SMD architectures are good for problems where the same operation is performed on a number of different objects, for example image processing which acts on each pixel in turn. However, if the loads on the processors are not balanced performance is poor (because execution is synchronised at each step, with everything waiting for the slowest processor). Examples of this are the ICL Distributed Array Processor (DAP) and the Thinking Machine Corporation's CM-200 here at EPCC.
1.6.2 Multiple Instruction Multiple Data (MIMD)
MIMD architectures typically consist of several independent processors capable of executing individual instruction streams, possibly each processor executing a different program. Within this class of architectures a common method of subdividing them is on the relationship between processors and memory. This type of division leads to three main types of MIMD architectures.
Shared Memory
In the shared memory architecture, shown in Figure 2, there are a, usually small, number of processors, each of which has access to a global memory store via some interconnect or bus. The processors communicate with one another by one processor writing data into a location in memory and another processor reading the data. With this type of communications the time to access any piece of data is the same, as all communications goes through the bus.
The advantage of this type of architecture is that it is easy to program as there are no explicit communications between processors with communications handled via the global memory store. Access to this memory store can be controlled using techniques developed from multi-tasking computers, e.g., semaphores.
However, the shared memory architecture does not scale well. The main problem occurs when a number of processors attempt to access the global memory store at the same time, leading to a bottleneck. One method of avoiding this memory access conflict is dividing the memory into multiple memory modules, each connected to the processors via a high performance switching network. However, this approach tends to simply shift the problem to the communications network.
Examples of computers with shared memory architecture are the SGI PowerChallenge, Sequent Balance and Symmetry.

Global Memory Store

Bus

Processing Element

Processing Element

Processing Element

Processing Element

Figure 2: A example of a shared memory architecture

Edinburgh Parallel Computing Centre

5

Parallel Programming on the Cray T3D
Distributed Memory
The distributed memory architecture (see Figure 3) get around the drawbacks of the shared memory architecture by giving each processor its own memory. A processor can only access the memory which is attached directly to it. If a processor needs data which is contained in the memory of a remote processor, then it must send a message to the remote processor asking it to send it the data. Clearly access to local memory can occur much faster than access to data on a remote processor. In addition to this, the further the physical distance to the remote processor, the longer it can take to access remote data.
Interconnect

Processing Element

Memory

Figure 3: A example of a distributed memory architecture
This non-uniform access time can be affected by the way the processors are connected. While connecting each processor to every other one is a possibility for a small number of processors, it quickly becomes impractical as the number of connections rises. One solution to the problem of connecting the processors together is to connect a processor to a small subset of its neighbours. Each of the neighbours in the communication subset would be connected to a different subset of processors, allowing for messages to be sent from one processor to another via a number of intermediate processors. There are several ways this can be done, one option would be to use switching chips which allow the user to adapt the topology of the machine to their own particular needs. Another popular possibility is to connect the processors in a "hypercube" arrangement. This has the advantage of not radically increased number of connections as the number of processors is increased, while offering a number of different message routing paths. Also, there are many parallel algorithms and software for hypercubes. Distributed memory machines have been built using all the methods described.
Rather than connecting processors together directly current practice is to connect the processors to a network of routing chips. The same topology issues apply in this case but the processors no longer play any part in the message forwarding. As there can be a different number of processors and routing chips this allows greater freedom when constructing the network.
While this architecture has the drawback of requiring explicit communications, it is inherently far more scalable than the shared memory architecture which is limited by bottlenecks in accessing its global memory store.

6 Course notes

Parallel Computing

Another type of distributed memory parallel machine which is often overlooked are clusters of workstations. While typically more loosely coupled than the "single box" parallel architectures described above, they can provide an invaluable route for producing parallel code. Further to this, they offer the opportunity for users without the resources to buy massively parallel machines to gain some of the benefits of parallelism on machines available locally, and perhaps not used constantly.
Machines which have a distributed memory architecture include the Meiko Computing Surfaces.
Virtual Shared Memory
The above classifications are in a sense idealised architectures. Often actual machines are mixtures of the different types of architectures. An example of this is the so called Virtual Shared Memory architecture, which should be differentiated from the true shared memory machines mentioned earlier. Like the distributed memory machines, each processor has some local memory, but direct access can be made to remote memory by use of a global address space. This remote access is possible because of the incorporation of support circuitry which deals with the communication independently of the remote processor. This offers the possibility of very fast communications through use of sophisticated hardware (though of course not as fast as local memory access) but with increasing communications overhead as the distance the messages travel increases.
An example of a Virtual Shared Memory machine is the Cray Research T3D, like the one based here at EPCC.
1.6.3 Summary
As parallel software becomes more sophisticated, the distinction between the architecture of the machines and the methods of programming them is becoming more apparent. The different programming paradigms seen on the architectures discussed above are covered in the Message Passing Concepts and Data Parallel Concepts chapters.
1.7 The Future...
In the past, one criticism levelled at parallel computing was the necessity, and technical difficulty, of having to re-write code if the user wished to port it to a different platform. The emergence of standards for parallel computing, such as High Performance Fortran (HPF) for the data parallel programming model and the Message Passing Interface (MPI) for message passing programming model, have gone a long way to addressing this issue. This also encourages those users who were not keen to spend time and effort programming in low level languages, but who would want to take advantage of the price/performance benefits offered by parallel computing.
The ability to develop a program on, for example, a network of workstations and then carry out production runs on a High Performance Computer, such as the Cray T3D here in Edinburgh, presents enormous benefits for users by bringing parallel computing within the reach of an increasing number of people.
Awareness of these past problems with parallel computing have forced manufacturers to take account of the wider user community. Technical developments and good support of the machines themselves have also progressed a long way in presenting a "friendly" and stable programming environment to the user by, say, providing the development tools users would automatically expect from any serial machine.
Parallel computing, which was once the sole preserve of academia and industrial research, is now opening up to a much wider audience who are slowly awakening the advantages and potential of parallelism. As traditionally conservative areas, such as commerce and finance, embrace parallel computing as a means to obtain competitive

Edinburgh Parallel Computing Centre

7

Parallel Programming on the Cray T3D advantage by cost effective means, perhaps it will be market forces which decide the future of parallel computing.
8 Course notes

Decomposing the Potentially Parallel
2 Decomposing the Potentially Parallel
2.1 Introduction
Suppose we have a sequential program which we wish to run faster by executing it on parallel hardware, in particular, distributed memory multicomputers. We consider all sequential programs to be composed of inherently sequential parts and potentially parallel parts, so the runtime of a sequential program is the runtime of its inherently sequential parts plus the runtime of its potentially parallel parts. By somehow splitting up, or decomposing, the work involved in a potentially parallel section in such a way that a number of processors can work concurrently on the problem, we aim to reduce the runtime of the section. However, decomposition frequently introduces an overhead: for example, message passing between processors. We have to consider this overhead in the execution time of the parallel version of the subprogram, and it must be outweighed by the reduction in execution time as a result of the use of (many) processors in parallel. If we achieve this goal, we will have reduced the runtime of the subprogram and therefore the runtime of the whole program. Three important decomposition techniques and their derivatives are shown in Figure 4.
Decomposition

Trivial

Functional

Data

Balanced
Figure 4: Decomposition techniques
2.2 Trivial decomposition

Unbalanced

The simplest technique is trivial decomposition, which doesn't really involve decomposition at all. Suppose you have a sequential program which has to be run independ-

Edinburgh Parallel Computing Centre

9

Parallel Programming on the Cray T3D

ently on lots of different inputs. You can clearly introduce some parallelism by doing a number of runs of the sequential program in parallel. Since there are no dependencies between different runs, the number of processors which can be used is limited only by the number of runs to be performed. In this case the execution time of the set of runs will be the execution time of the most time consuming run in the set. Clearly, trivial parallelism can be exploited to provide almost linear speed-up if runs take a similar length of time. As each run is completely independent no communication is required between processes and this technique can be used effectively on workstation clusters with poor communication performance.
2.3 Functional decomposition
Functional decomposition, the first true decomposition technique, breaks up a program into a number of subprograms.

Smoothing

Feature Extraction

Object Recognition

Display

Image

Smoothed Image

Feature Descriptions
Figure 5: A pipeline

Object Descriptions

A simple form of functional decomposition is the pipeline (Figure 5), in which each input passes through each of the subprograms in a given order. Parallelism is introduced by having several inputs moving through the pipeline simultaneously. Con-
sider Figure 5. The pipeline is initially empty. The first data element flows into the first stage of the pipeline (smoothing). Once this element has been smoothed, it is passed on to the next stage of the pipeline (feature extraction). While this first element is being processed through the feature extraction stage, the second data element can flow into the smoothing stage. Parallelism is introduced as the second data element's progress through the pipeline is overlapped with that of the first element. This filling process continues until every element of the pipeline is processing a data element. When the data set is exhausted there is a analogous draining period in which the number of busy stages in the pipeline falls to zero.

Parallelism in a pipeline is limited by the number of stages in the pipeline. For greatest efficiency we want to keep all the stages busy. This requires the time taken for each stage of processing to be equal --- the pipeline is then balanced. One possible extension to functional decomposition is to attempt to even the work load between the processors by dividing the most computationally expensive task between a number of processors.

In general, functional decomposition is very problem dependent, and the amount of parallelism is limited by the program. This means that as the size of the input dataset grows, it may not be easy (or indeed possible) to exploit any more parallelism. As a result, the individual data items in a large dataset are unlikely to be processed any faster than those in a small dataset, and the larger dataset takes a proportionally longer time to be processed.

2.4 Task farms

As mentioned above, an individual task in a functional distribution can be distributed between a number of processors. This technique is usually referred to as task farming.
Small processing grains of data are chosen, and one processor, the work source, maintains a set of unprocessed grains. A number of worker processors repeatedly request a grain from the source, process it and then dispatch the results to a result sink, which

10 Course notes

Decomposing the Potentially Parallel
may or may not be the work source. An outline for a basic task farm is shown in Figure 6.
Work Source

Worker

Worker

Worker

Worker

Results Sink
Figure 6: A simple task farm.
This has the advantage that no prior assumptions need to be made about the data set: provided that the processing grains are sufficiently small, an even workload is ensured since a worker which happens to receive many grains requiring little work will simply request more grains. It is often possible to construct independent tasks from a problem in which there are data dependencies by including extra information (for example, values from neighbouring boundary points) with the processing grain. Task farms do, however, require a constant flow of grain requests and replies between the workers and the master. The costs associated with maintaining the dynamic load balance are particularly relevant in cases where many iterations are being performed over the data set. This is due to the necessity for retransmission every iteration
2.5 Regular domain decomposition
Many problems are based on taking a very large set of data, arranged in a regular grid structure, and applying transformations to the data elements. When the data can be split up into regular subgrids, and distributed over a set of processes, then the transformations can be applied in parallel, allowing the problem to be solved in a smaller time scale, or allowing much larger problems to be solved than could normally be attempted.
The regular domain decomposition method is to take a large grid of data elements, split it up into regular subgrids, and distribute these subgrids to separate processes where they can be operated on.
global data grid

decomposition

process grid

local data block
Figure 7: Regular domain decomposition.

Edinburgh Parallel Computing Centre

11

Parallel Programming on the Cray T3D
Not all of the required neighbouring elements will necessarily reside in the local data block of the same process. In order to access elements from other blocks, each process owns a "halo" of data elements shadowed from neighbouring processes, surrounding its local data block. This is known as the overlap area. The data block surrounded by an overlap area is known collectively as a data array, as shown in Figure 8.
overlap area in direction

overlap area in direction

data block

overlap area in direction

overlap area in direction
Figure 8: Overlap area
As the elements near the edge of each process's data block require a region of elements from a neighbouring process's data block arranged in an overlap halo region, so each process must be prepared to send copies of elements from the edges of its own data block, out to adjacent processes, to allow them to keep their overlap regions upto-date. This is known as boundary swapping.
Figure 9 shows two processes, A and B. Process A is performing an update on an internal element, and therefore it does not reference any data contained in an overlap area. Process B is performing an update on an element which requires data from its overlap area with process A. Process A must arrange to perform a boundary-swap with process B before process B is permitted to access this data.

data to be boundary-swapped into overlap area in process A
Process A

overlap from process A Process B

Updating internal element

Updating element using overlap from neighbouring data block

Figure 9: Neighbouring processes swap boundaries

Each process owns a data block of elements, and this block is surrounded by an overlap area containing "shadows" of elements from neighbouring blocks. But this doesn't say anything about the blocks which contain the "edge" elements of the global data grid. These blocks have no neighbours in the direction of the edge.

The two most common types of boundary conditions are periodic and static.

12 Course notes

Decomposing the Potentially Parallel
· A periodic boundary condition is one where the data grid can be considered to "wrap around" at an edge. Figure 10 shows a one-dimensional grid (or "vector") with a periodic boundary condition. This allows the grid to be considered as being a "ring".
global 1-dimensional data grid

1-dimensional grid with periodic boundary viewed as `wraparound'
Figure 10: Periodic boundary condition.
A two-dimensional grid becomes a torus if it has periodic boundaries on both of its dimensions.
· A static boundary condition is one where the boundary does not wrap around. The overlap area at an edge of the global data grid does not correspond to data elements anywhere.
When a application uses a static boundary condition, the static boundary data should be initialised in these overlap areas before any update iterations are performed.
Figure 11 shows a one-dimensional grid with a static boundary condition. The grid has been decomposed over four processes. The overlap areas between processes are filled in by boundary-swapping, while the "edge" overlap areas contain static boundary data.
overlap areas

boundary swap static boundary data

static boundary data

Figure 11: Static boundary condition.

Edinburgh Parallel Computing Centre

13

Parallel Programming on the Cray T3D
14 Course notes

Message-passing concepts

3 Message-passing concepts

The main concepts needed to build and program a serial computer are well understood. A physical device called a processor, is connected to a memory as illustrated in Figure 12. The data in the memory can be read or overwritten by that processor. The source code program written by the programmer is translated into machine language stored in the memory of the processor. The translated program, also know as an object code, is then interpreted by the processor to produce the required results.
This programming paradigm may in fact be implemented in various ways, perhaps in a time-sharing environment where other programs share the processor and memory. Most programming languages and environments shield the programmer from the intricacy of working directly with the processor. They supply instead, a higher level concept, called a process, that models the activation of a single program on a processor. Source code written in terms of processes can be much more easily ported to computers with a different architecture than code written explicitly in terms of a specific type of processor -- that is after all the point of a paradigm.

Memory

M

Processor

P

Figure 12: The sequential programming paradigm
In the context of parallel computing, two of the main programming paradigms will be examined in this course. They are the message passing and the data-parallel programming models. The data-parallel programming model is characterised among other things by the provision of a global memory that can be directly read from and written to by every process involved in a computation. Data-parallel programming is the subject of the next chapter. In the message-passing programming model, each process has a local memory and no other process can directly read from or write to that local memory. Distributed-memory programming models further assume that there is no globally addressable memory as in the data-parallel model. It is fairly common for parallel computers to support several programming models and so it has become the responsibility of the programmer to decide which of those models suits better their application. In the next two chapters, we will look at some of the similarities and differences of the two programming models.
For clarity sake, we will refer to the individual processes coordinating in a parallel computation as forming the parallel program, or simply the program. The term process will be used only to refer to a single computational activity on a single processor. Message passing programs are written in the same languages as conventional sequential programs however at any point the programmer is specifying the actions of a process performing part of the calculation rather than specifying the action of the entire program.

Edinburgh Parallel Computing Centre

15

Parallel Programming on the Cray T3D

3.0.1 Messages
Parallel programming by definition involves co-operation between processes to solve a common task. There are two sides to the question of programming processes that cooperate with one another. The programmer has first to define the processes that will be executed by the processors, they also have to specify how those processes are to synchronise and exchange data with one another. A central point of the message-passing model is that the processes communicate and synchronise by sending each other messages. As far as the processes are concerned, the message passing operations are just calls to a message passing interface that is responsible for dealing with the physical communication network linking the actual processors together. A parallel computer architecture supporting the message passing model is illustrated in Figure 13 below. Such architectures are referred to as distributed-memory architectures and the corresponding parallel computers are called multicomputers when the topology of the communication network is known before the computation starts. In contrast, the topology of a network of workstations may vary during the computation and is typically not known by the programmer.
Those with experience in using networks of workstations, client-server systems or even object-oriented programs will recognise the message-passing paradigm as nothing novel.

Memory

M

M

M

Processor

P

P

P

$Id: mp-paradigm.Cips,ov m1.1m199u4/n06i/c27a2t1i:2o1n:10sthNardeintgwEoxpr$k
Figure 13: The message-passing programming paradigm.
3.0.2 Message-passing programs
Let us now turn to the question of how a message-passing program might be written by a programmer. The most frequently encountered situation with multicomputers, is for the programmer to write a single source code program that is compiled and linked one a front-end computer. The resulting object code is copied in the local memory of every processor taking part in the computation. The parallel program is executed by having all processors interpret the same object code. This model of parallel computing is known as the Single-Program-Multiple-Data model, or SPMD for short. If all the variables with the same name in every process had the same value and if all these processes only had access to the same data there would be no parallelism at all, each process would act exactly the same as all others. In a message-passing context, the communication interface typically includes a function or a procedure that returns a value identifying the process that called that function or procedure. Further more the same value will always be returned to the same process. For the moment, let us call this returned value the process identifier. Typically the sequence of instructions executed by a process will be determined by the input data and the identifier of that process. As an illustration, consider Figure 14 below which shows the local memory of two
16 Course notes

Message-passing concepts

processes. The local variables called whoami in each process have been set somehow to the identifier of the corresponding process.

Process A

Process B

whoami : 0 x:0

whoami : 1 x:0

Figure 14: The local memories of two processes A and B.
When the two processes execute the following statement, process A will increment the value of its variable x but the value of the variable x in the local memory of process B will remain unchanged:
if ( whoami == 0 ) x = x + 111

or in Fortran: if ( whoami .EQ. 0 ) x = x + 111

The local memories of the two processes will then look as shown in Figure 15 below:

Process A

Process B

whoami : 0 x : 111

whoami : 1 x:0

Figure 15: The local memories of the same processes after they both completed the above conditional statement.
Note that the two processes are likely to take alternate branches of any subsequent conditional statement making use of the identifier x in its test-clause.
The distinctive feature of the SPMD model is that all processes are of the same type. In theory, this restriction does not affect the expressivity of the message-passing paradigm. All we have to do in order to run different types of processes is to write a conditional statement whose branches call the appropriate code for each process depending on their respective identifier. In practice, there are occasions when the SPMD model is not suitable. For one, under the SPMD model, the executable of every types of process has to be loaded on every processor. Depending on the application, this may place too large a burden on the memory capacity of individual processors. The alternative model of parallel computing that addresses this issue is the Multiple-Program-MultipleData model where different executables may be interpreted by distinct processors. The MPMD model assumes the ability to load object code on processors during a par-

Edinburgh Parallel Computing Centre

17

Parallel Programming on the Cray T3D
allel computation and is very natural on network of workstations. Many multicomputer vendors simplify their system software by only providing parallel environments which support SPMD parallel programs; the provision for the MPMD model on multicomputers is an active research topic.
Most current multicomputers do not support time-sharing, i.e. multiple processes per processor (some authorities understand the term ``SPMD" to include this further restriction). This is for efficiency reasons and for example the whole computation may come to a grinding halt when all processes need data from some swapped-out process. On the other hand, time-sharing may relieve any need to carefully balance the workload of each processor. Time-sharing on multicomputers is also an active area of research.
3.1 Messages
Messages are central to the message-passing programming model; they are exchanged between processes. When two processes exchange a message, data is copied from the local memory of one process into the local memory of the other process. The data sent in a message comes under two headings: contents and envelope. The contents of the message is purely user data and is interpreted neither by the communication interface nor by the communication system that lies behind that interface. The data on the envelope however is used by the communication system to copy the contents of the message between local memories; specifically the following information must be specified:
·Which processor is sending the message.
·Where is the data on the sending processor.
·What kind of data is being sent. How much data is there.
·Which processor(s) are receiving the message.
·Where should the data be left on the receiving processor.
·How much data is the receiving processor prepared to accept.
In general the sending and receiving processes will cooperate in providing this information. Some of this information provided by the sending process will be attached to the message as it travels through the system and forms the envelope data. The message passing system may make some of this information available to the receiving process. Other information may be provided by the receiving process.
Message querying functions. As well as delivering data the message passing system has to provide some information about progress of communications. A receiving process will be unable to use incoming data if it is unaware of its arrival. Similarly a sending process may wish to find out if its message has been delivered. Messages are therefore also a means to synchronise processes.
Every day life examples of communication systems/interfaces.The essence of message passing is communication and many of the important concepts can be understood by analogy with the methods that people use to communicate, phone, fax, letter, radio etc. Just as phones and radio provide different kinds of service different message passing systems can also take very different approaches. For the time being we are only interested in general concepts rather than the details of particular implementations.
3.1.1 The constituents of a communication:
What does it take to initiate and complete a communication? First, the processes that are to take part in a communication must be able to use the communication system.
18 Course notes

Message-passing concepts
The communication interface must also provide some mechanisms to specify which processes are taking part in a communication. Finally, because all communications in the message-passing model are done via explicit procedure calls, the programmer needs some way to relate the termination of these calls with the actual completion of the communication. These constituents of a communication are presented next.
Access. Registering processes to the message-passing interface is like telling the Post Office of one's new address or asking for a phone number to the telephone company. Connecting a processor to the communication network is like having a telephone installed or a a mailbox fitted to the front door. A person with two telephones can use one phone for business use and one phone for personal calls. Some message passing systems use distinct hardware for user and system messages. Other message passing systems support multiple communication networks each with different characteristics to cater for the various needs of application softwares. The message -passing interface on the other hand may or may not allow the programmer to take advantage of all the features of the communication hardware. Another distinctive aspect of communication interfaces is the time when processes are allowed to register. Within the SPMD model of computation, all the processes are typically known to the communication interface before the actual computation starts. Within the MPMD model of computation, processes may be started during the computation so the communication interface will typically include some facility for registering processes as needed.
Addressing. Messages have to be addressed in some way. Postal mail needs a town, street and house number. Fax messages require a phone number. A single letter may contain several completely independent items. For example my credit card bills invariably comes packaged with a mail order catalogue. However they were contained within a single envelope with a single address and travelled through the mail system as a single item so they count as a single message.
The postal system only looks at the information on the envelope. The message envelope may provide additional information on top of the address of the recipient, for example a return address or some indication of the contents of the message. It is usually possible to separate the morning mail into bills and letters without opening any of them. The envelope may also contain some contextual information such as a department name to avoid confusing recipients with the same name in a large organisation. The same is true of most message passing systems and similar information is typically written by the communication interface on the envelope of messages sent by processes
Reception. It is important that the receiving process be capable of dealing with the messages that have been sent to it. If a processor is sent a message that it is incapable of handling (e.g. larger than the buffer the processor is putting the message into) then various strange effects can occur. For example the message passing system may truncate or discard the message. Some communication systems will give processes some control on the management of buffer space for messages in transit. Some communication interfaces make it possible for all such buffer management to be entirely transparent to the programmer.
Alternatively, the correct execution of a parallel program may rely on the sending process being able to detect when the message failed to be delivered by the communication system. This information may be supplied to processes via the message querying procedures of the communication interface. This last point brings us to communication modes.
Modes and forms. The modes of a communication operation determines when that communication operation is completed. The form the procedure initiating a communication determines when a call to that procedure will return in the calling process.

Edinburgh Parallel Computing Centre

19

Parallel Programming on the Cray T3D
3.2 Moving Data Around: Point-to-Point Communication
The simplest form of message is a point-to-point communication in which a message is sent from a sending process to a receiving process. Only these two processes need to know anything about the message. The communication itself is composed of two operations: a send and a receive The send operation can be either synchronous or asynchronous depending on whether or not it completes before or after the corresponding receive operation has been initiated. Here are some illustrations of these two communication modes. Synchronous sends complete only when the corresponding message is being taken care of by some receiving process.
"Beep"
Figure 16: A synchronous communication does not complete until the message has been received.
20 Course notes

Message-passing concepts
Asynchronous sends completes as soon as the corresponding message has been delivered to the communication system.
?
Figure 17: An asynchronous communication completes as soon as the message is on its way. A fax message or registered mail is a synchronous operation. The sender can find out if the message has been delivered. A post card is an asynchronous message. The sender only knows that it has been put into the post-box but has no idea if it ever arrives unless the recipient sends a reply. The main two forms for communication procedures are blocking and non-blocking. They relate the completion times of a communication operation and of the procedure that initiates that communication operation. Blocking procedures only return when the corresponding communication has completed. An example might be someone placing your telephone call on-hold.

Edinburgh Parallel Computing Centre

21

Parallel Programming on the Cray T3D
Non-blocking procedures return straight away and allow the process to continue to perform other work. At some later time the process can test for the completion of the corresponding communication.
Figure 18: Non blocking communication allows useful work to be performed while waiting for the communication to complete
Normal fax machines provide blocking communication. The fax remains busy until the message has been sent. Some modern fax machines contain a memory. This allows you to load a document into the memory and if the remote number is engaged the machine can be left to keep trying to get through while you go and do something more important. This is a non-blocking operation. Receiving a message can also be a non-blocking operation. For example turning a fax machine on and leaving it on, so that a message can arrive. You then periodically test it by walking in to the room with the fax to see if a message has arrived.
3.3 Cooperation: Collective Comms
Up until now, we've only considered communications that involve a pair of processes. Many message-passing systems also provide operations which allow larger numbers of processes to communicate. Here we review three main types of collective communications that more or less have an equivalent in the data-parallel programming model. All collective communication operations can be built out of point to point communications but communication interfaces typically provide routines for them because efficient implementations based on point-to-point communications are typically not portable across different communication network topologies.
22 Course notes

Message-passing concepts
3.3.1 Barrier
A barrier operation synchronises processors. No data is exchanged but the barrier blocks until all of the participating processors have called the barrier routine.
Barrier
Barrier
Barrier
Figure 19: A barrier operation synchronises a number of processors.
3.3.2 Broadcast
A broadcast is a one-to-many communication. One processor send the same message to several destinations with a single operation.

Figure 20: A broadcast sends a message to a number of recipients.
3.3.3 Reduction Operations
A reduction operation takes data items from several processors and reduces them to a single data item that may or may not be made available to all of the participating processes. One example of a reduction operation is a strike vote where thousands of votes are reduced to a single decision. Another example of a reduction operation is

Edinburgh Parallel Computing Centre

23

Parallel Programming on the Cray T3D the summation of all the values stored in all the variables, say named x, in every process taking part in the operation.
STRIKE
Figure 21: Reduction operations reduce data from a number of processors to a single item.
3.4 Applications
The message-passing paradigm has become increasingly popular in recent times. One reason for this is the wide range of platforms which support a message-passing model. Programs written in a message-passing style can run on distributed or sharedmemory multi-processors, networks of workstations, or even uni-processor systems. The portability argument in favour of the message passing paradigm may weaken over the years as other programming models become more widely supported by vendors. On the other hand, two characteristics of the message passing paradigm seem to be at present, the ability to write communication kernels that fully exploit the topology of the communication network and the ability to change at run-time the way the data is distributed across the local memories of processes.
3.5 Summary
Message-passing has been described as the low-level programming of parallel computers. Indeed it is possible using the message paradigm to write source code that uses to its fullest the interconnection network of a multicomputer. On the other hand, message passing interfaces like MPI and PVM make it possible to write high quality software that is portable to a variety of platforms without sacrificing performance. Message-passing is popular, not because it is particularly easy, but because it is so general.
24 Course notes

Data Parallel Concepts

4 Data Parallel Concepts

The data-parallel programming model originates from vector programming where the programmer writes his applications in terms of highly optimised vector operations. Data parallel programming became widely known in association with SIMD computers architectures, such as the CM-200 and DAP. The gist of the data-parallel programming model is the provision by the programming language of parallel intrinsics and built-in procedure that operate on data that is known to all the processes or threads of computation.
The coordination of processes and the exchange of data between processes is determined by the operations applied to the data and by program annotations written by the programmer.
4.0.1 Data-parallel programs
SIMD implementation. In its simplest form the data-parallel programming model extends a sequential programming language with parallel constructs for handling large aggregates of data such as arrays. A program written with this paradigm resemble an ordinary sequential program. The flow of control of the program follows the strict sequential order except when a parallel intrinsics or built-in procedure is called. At that point, instructions are run on the available processors to compute the required result or transformation. The programmer need not be aware of the existence of multiple processors but only of the availability of some fast intrinsics and built-in operations for some specific types of data.
On a distributed-memory multi-computer, data movements before and after every parallel operation could easily blot out any advantage of parallelism. On such computers, it is usual for the compiler to determine which data is going to be processed in parallel and how this data is to be distributed among the local memories of processors. The programmer intersperses his source code program with specially formatted comments, called compiler directives, that guide the compiler in its task.
This form of data-parallel programming with a single thread of control is very natural for programming SIMD computers, such as the CM-200 or the DAP, that consist of a large number of simple processors which are tightly synchronised by a controller. A host computer interprets the sequential part of the data-parallel program until a parallel operation is called at which point the appropriate instructions are dispatched to the available processors. The first data-parallel compilers were written to allow use of SIMD machines, but since then the data-parallel model has been implemented on a wide variety of MIMD platforms.
MIMD implementation. MIMD computers, such as the Intel iPSC/860, Paragon, Meiko Computing Surfaces and the Cray T3D, typically have fewer, more powerful processors than SIMD computers. These machines are often programmed using Message-Passing, but many of the latest MIMD machines (e.g., T3D and CM-5) also support the data parallel programming model. The SIMD implementation of the dataparallel programming model may not be appropriate because the time needed to copy

Edinburgh Parallel Computing Centre

25

Parallel Programming on the Cray T3D
a single word of data across local memories is relatively large compared to the timed needed for a single arithmetic operation.
In MIMD implementations of the data-parallel paradigm, the processes are only synchronized at the beginning and end of parallel operations as opposed to marching through the same sequence of instructions all at the same pace. The processes in a MIMD implementation of the data-parallel model are said to be loosely synchronised, i.e., they perform the same operations on different data at approximately the same time.
Some scalar data like array indices and loop indices can be much more expensive to broadcast than to recompute in every process running on an MIMD computer. For this reason, it is common for implementation of the data-parallel model on such computers to incorporate the SPMD model and to support private data for each process; the SPMD model of programming is described on page 16. Parallel intrinsics and built-in procedures operate on data that has been identified by the compiler as being parallel data. The programmer can instruct the compiler via compiler directives that such and such data is to be parallel or private.
4.0.2 Parallel Data
Arrays are not only well suited as parallel data, they are in fact the prototype of parallel data since most data-parallel languages are extensions of Fortran. Parallel data is accessed in the source program as though it was shared by all processes, but it may none-the-less be physically distributed among the local memories of the processors taking part in the computation. Figures 22 and 23 illustrate this point by comparing the lay-out of an array x of 100 elements in a sequential program and in a data-parallel program.

26 Course notes

memory

real x x=1

single source program
serial.f

f77 compiler

_x

serial instruction set
single binary a.out program

PN Single Processor

Figure 22: Compiling and running serial code

real x(100) x=1

single source program
parallel.f

memories

compiler

ooo _x

serial and parallel instruction sets
single binary a.out program

ooo Parallel Architecture

Figure 23: Compiling and running parallel code.

Data Parallel Concepts

For serial code, a single source program is passed through a compiler to produce a binary executable (a.out) which consists of a number of serial instructions. When the executable is run, it is loaded on to a single processor. Variables are reserved space in that processor's memory and work is performed on them.
For parallel code, many things are similar. A single source program is passed through a compiler to produce an executable. However, in this case, the executable may contain both parallel and serial instructions. When the program is run, the parallel instructions may be performed on many different processors. Array variables, such as x(100), are typically distributed across these processors so that each process has only part of the array in its local memory. When operations are performed on these arrays, the compiler tries to ensure that each process updates the parts of the arrays in its local memory so that memory references are mostly local.
4.1 Data Parallel Languages
The main constituents of a parallel language are: a sequential language and extensions to that language, parallel data and compiler directives. These concepts are presented in this section with some level of detail.
Parallel Fortran comes in various dialects with much in common, but also some significant differences in syntax and in functionality. The most important dialects are High Performance Fortran, CM-Fortran and CRAFT. These are essentially F77 with F90 array extensions and intrinsics, features for data and work distribution and scientific libraries. All of the dialects support (or will soon support) the features described in the previous subsections although with slightly different syntax. However, other features, such as the distribution of loop iterations across processors are significantly different in approach ­ HPF and CMF have a new statement for this (FORALL) while, in CRAFT, similar functionality is provided via a compiler directive ( DO SHARED ). Another major compatibility concern is that there are few standard library routines and many routines supported under one dialect have no equivalent in another. In general, it is not always easy to port code between different dialects although it is often practical to target the intersection of two or more dialects. Even where code portability is not straightforward, algorithm portability is often practical.
For the above reasons, we concentrate here on basic concepts which are common to a range of languages and dialects. The examples are primarily in parallel Fortran but they could easily be translated to, e.g., C for the Connection Machine.
4.1.1 Array operations
One of the most important features of parallel languages is the facility to treat data arrays as `single entities.' Arrays may be physically distributed across processors although every process can access any part of the arrays. Typically, however, each process operates on the particular elements of the array which are held in its local memory.
As arrays are single entities, they can be assigned and operated on in the same way as any other type of data. For example, the following two pieces of code are essentially identical in functionality. The left-hand one is F77 while the other shows how similar operations could be performed using parallel Fortran.

REAL a(64), b(64) DO i = 1, 64
a(i) = 2.0 b(i) = b(i)*a(i) END DO

REAL a(64),b(64)
a = 2.0 b = b*a

Edinburgh Parallel Computing Centre

27

Parallel Programming on the Cray T3D

The expression a = 2.0 sets each element of the array a to be 2.0. Similarly, b = b*a performs an elemental multiplication of the two arrays as in the serial code. The data parallel compiler will try to distribute the work so that memory references are primarily local.
4.1.2 Parallel arrays
Data parallel programming would be quite limited if the whole array always had to be operated on simultaneously. There are a number of common features which allow the appropriate operation to be performed on only some of the array elements.
Array sections: most data parallel languages have extensions to simple array syntax which allow the programmer to specify parts of the array in a simple manner. This can be used to refer to the whole of the third column of an array to be zero or to all the odd-numbered elements. One way of doing this is by the use of array triplet notation which is of the form min : max : stride . For example, a(1:7:2) = 0 would zero a(1), a(3), a(5) and a(7).
WHERE: allows array elements to be selected by value, instead of position, as with array sections ­WHERE is the parallel analogue of IF. For example, when dividing the elements of two arrays, it is important to avoid those elements where the denominator is zero. The following code fragments show both the serial and parallel versions of this.

DO i = 1, Max IF (a(i) .GT. 0) THEN b(i) = b(i)/a(i) END IF
END

WHERE (a .GT. 0) b = b/a
END WHERE

Masks: many parallel languages provide quite general ways of selecting elements by way of a logical mask. This is an array of the same size as the data arrays and operations are only performed on the data elements which correspond to mask elements which are true.
4.1.3 Compiler directives
Compiler directives for a data-parallel language are mainly of two types; those relating to the layout of parallel data in the local memory of processes and those relating to the flow of control.
The simplest form of compiler directives relating to data instruct the compiler whether or not some data, typically an array, is to be accessible to all processes. A more elaborate from of compiler directives allow the programmer to specify the size and shape of array sections of parallel arrays that are to reside in the local memory of a single process.
Some programming languages like CRAFT extend the data-parallel model with compiler directives to spread the work of iterative loops over the available processes
MIMD implementations of the data-parallel model may supply some compiler directives for specifying master regions where the flow of control reverts from the SPMD execution to a sequential execution by a dedicated master process or vice-versa. There may also be compiler directives for specifying critical sections of codes that are guaranteed to be executed by at most one process at a time.

28 Course notes

Data Parallel Concepts

4.2 Moving data around: Data Shifts
In parallel computing, it is important to be able to move data around. Many calculations update an array element based on the values of its neighbours, e.g., cellular automata. As neighbouring array elements may be physically located on different processors, they must be moved to overlap with the element to be updated. In data parallel programming, this is often done by the use of routines known as data shifts. These move the whole array some constant distance in a particular direction and this approach results in regular data movement which is much less flexible than in message-passing. For example, a cyclic shift one place to the right would produce the following result in one dimension.

1234

4123

source Figure 24: A cyclic shift

target

4.3 Cooperation: Global operations

The three types of collective communications examined in the chapter Message-passing concepts on page 3 were the barrier, the broadcast and reduction operations. These have counter-parts in most data-parallel languages.
Barriers. Barriers are typically set by the compiler before and after each data-parallel operation. Some compilers may be written cleverly enough to omit inserting unnecessary barriers, some compilers will accept user written directives as to whether barrier should be inserted or not at specific locations in the source code.
Broadcasts. Whenever a scalar variable appears as an argument to an array expression, as in 2*A where A is some array, it is likely that this scalar will be broadcast to all processes. Some data-parallel compilers support directives for explicitly broadcasting the value of a variable stored at a master process.
Reductions. Many data parallel languages also include facilities to calculate the sum of all the elements of an array or to calculate the largest value in the array. There are also operations called scans, which scan through the array calculating, e.g., the partial sums. Global routines are very important in data parallel programming and are often implemented efficiently.
4.4 Applications

4.5

As data parallel languages essentially allow large data arrays to be operated on in parallel, they are primarily used for applications with large data sets that require similar operations on each element. The amount of work required for each array element should be roughly equal to ensure load-balance. To allow many processors to work on the arrays, many of the operations should be independent, i.e., should not depend on the results of previous operations. These requirements mean that data parallel programming is more restrictive than message-passing. However, it is a natural style for many grid-based systems such as lattice physics (e.g., QCD), atmospheric modelling, databases and cellular automata.
Summary

Data parallel programming is a style which is now available on many MIMD computers as well as the SIMD computers to which it is more commonly linked. There are

Edinburgh Parallel Computing Centre

29

Parallel Programming on the Cray T3D currently a range of competing dialects for parallel Fortran but these have a large overlap in facilities and functionality. While this style is more restrictive than message-passing, it has many important applications.
30 Course notes

Cray T3D Hardware and Software Overview

5 Cray T3D Hardware and Software Overview

5.1 Introduction
This is a brief overview of the design of Cray-T3D hardware, and some information about the software environment currently installed on the front end systems to the Cray T3D. These systems are darwin.ed.ac.uk which is the real front end to the T3D and kelvin.ed.ac.uk the IO/compile engine for the T3D run by the Edinburgh Parallel Computing centre.
5.2 Hardware overview
This section discusses the Cray T3D, Cray Research plans for the future, and the configuration chosen by EPCC. Nodes, PEs, the global address space, communication between nodes and I/O are described.
5.2.1 Cray MPP 3-phase plan
Cray Research have set themselves a long term goal of building a commercially viable massively parallel computer which has a sustained performance of 1 Teraflop/s, and have proposed a timescale for its achievement, with a family of related machines:
1. 1993 - T3D: 300 Gflop/s peak 2. 1996 - T3E: 1 Teraflop/s peak 3. 1997 - T3 nn : 1 Teraflop/s sustained
The aim throughout this plan is to keep the macroarchitecture constant, with a similar 3D arrangement of nodes and similar software environments, while changing the microarchitecture to take advantage of microprocessor evolution and developments in message passing hardware.
5.2.2 EPCC configuration
The CRAY T3D system at EPCC consists of four major components ­ the CRAY J90, the CRAY Y-MP host, the CRAY T3D system itself, and an IBM 3494 mass storage system.
5.2.3 Processing elements (PE)
In its initial configuration, the T3D array comprised 128 nodes each with 2 processing elements (PEs), giving a total of 256 PEs. The system was then upgraded to 384 PEs and has now been upgraded further to 512 PEs. Each PE consists of a DEC Alpha

Edinburgh Parallel Computing Centre

31

Parallel Programming on the Cray T3D
21064 processor running at 150 MHz, supporting 64-bit integer and 64-bit IEEE floating point operations and delivering 150 64-bit Mflop/s. The peak performance of the T3D array itself with 512 PEs is 76.8 Gflop/s. The DEC Alpha 21064 includes an 8 Kbyte direct-mapped data cache and an 8 Kbyte instruction cache. Each processor has 64 Mbyte of RAM, giving an aggregate memory for a 512 PE system of nearly 33 Gbytes.
The nodes are arranged in a three-dimensional torus, with each of the six links from each node simultaneously supporting hardware transfer rates of up to 300 Mbyte/s. Hardware support for a single address space across the array is provided.

FDDI Ring

FDDI
Y-MP/4E
100 MBytes/sec peak Bidirectional

FDDI Concentrator

Control Control FDDI

Data

2 channels, each up to 4.5 MBytes/sec sustained

HiPPI

Data

RS/6000 S/370 channel
TIBaMpe3s4i9lo4 w1.i2thT~B1ytTeBsyitleo capacity 2 drives, each at 3 MBytes/sec sustained

EPCC

J916
T3D

Figure 25: EPCC configuration

5.2.4 Global addressing
The global address space means that in principle each processor has direct access to the memory of every other processor, allowing the total memory to be considered as a single object. In practice local memory is slightly different, with a communication overhead associated with remote reads and some care being needed to ensure data consistency between PEs, but the global address space allows both optimised message passing and a data parallel programming style.

32 Course notes

Cray T3D Hardware and Software Overview

Computational node
PE 0

Block transfer

PE 1

Network Interface
Router

Data Control

Figure 26: A Computational Node
1. Address issued by processor
2. Support circuitry sees it's on a remote PE
3. ...and requests router to access.
4. router sends message to remote PE
5. remote PE replies
Routing and remote access
Remote reading and writing across the global address space is possible, with a small communication overhead. The added time penalty is two clock cycles per node traversed, plus one extra clock cycle to turn a corner, which is low enough to allow good performance across nodes.
It is still worth considering the overhead, and trying to avoid unneccessary remote operations. Always remember, remote is slower than local, the further apart the PEs the slower the communication, and remote reads are slower than remote writes. This is because a read consists of a request followed by a write from the remote PE, while a write allows the message to be sent direct to the remote PE.
In a message passing program, there will be additional overhead relating to the use of internal buffers to store data which will be required later by a remote PE. The size of this overhead depends on the particular message passing model being implemented.
5.2.5 T3D I/O
The array includes two I/O Gateways, special double nodes; both of these provide connection to the Y-MP host which currently provides all I/O services to the array. This connection has an aggregate bandwidth of 200 Mbyte/s bidirectional.
Each I/O gateway consists of two nodes, each of which contains a single PE with 4MW of memory (half the amount of a normal PE) and special communications hardware. One node handles I/O in each direction, and they are used for system calls as well as file access and direct communications with the Y-MP.
The Y-MP host has 2 Y-MP processors, 64 Mword RAM, a 32 Mword SSD, and 212 Gbyte of disk space, composed of DD-60 drives and DA-301 RAID disks. The DD-60s are extremely high performance disks for use by the operating system, have a formatted capacity of 1.96 Gbyte and a sustained transfer rate of 20 Mbyte/s. The DA-301s are RAID-3 devices, with a formatted storage capacity of 6 Gbyte and a sustained

Edinburgh Parallel Computing Centre

33

Parallel Programming on the Cray T3D

5.3

transfer rate of 32 Mbyte/s. The majority of disk space is used as work space for T3D applications thus providing fast disk for T3D IO.
The J90 host has 10 processors, 1 Gbyte of central memory and 300 Gbytes of disk space, composed of DD6 (SCSI) drives. The DD6s have a formatted capacity of 9.4 Gbytes with a peak transfer rate of 7.2 Mbytes/s. The disk space is used to house all the home directories of users ( these are cross-mounted to the Y-MP) and for the archive filesystem. AJ90 processor has a peak rate of 200 Mflops which is approximately two thirds of a Y-MPs. The J90 is binary compatible with the Y-MP system.
The IBM 3494 mass storage system is connected to the CRAY J90 front-end over two 4 Mbyte/s channels, achieving sustained data rates of approaching 3 Mbyte/s per channel (6 Mbyte/s aggregate) between the front-end and tape storage. Mass storage capacity is approximately 1 Tbyte. Upgrades anticipated in 1996 will increase this capacity to approximately 12 Tbyte, and data rate to around 9 Mbyte/s per channel.
Software overview

This section describes the current software environment for the T3D. The environment currently includes the UNICOS MAX operating system, a pattern of pools, partitions and account IDs for multi-user access, Cray specific code development tools, and several T3D specific compilers.
5.3.1 UNICOS MAX

UNICOS MAX is the Cray Research operating system for the T3D. It includes UNICOS, a UNIX-like operating system for the Y-MP host, a `UNIX agent' which is a daemon to do I/O etc for T3D programs, an operating system for the I/O Gateway, and a microkernel in each PE. One feature of the operating system that is likely to affect your programming style is the way file access is handled. The Y-MP host has only a finite number of I/O channels, an upper limit of 1024, so one situation to avoid is having each of 256 PEs attempting to read or write files simultaneously. As a result, it is more important than usual to program I/O routines defensively, either ensuring synchronisation and getting the PEs to access files sequentially or by checking for errors and retrying when reads or writes fail.
5.3.2 Pools and Partitions

The T3D MPP is not a time sharing machine unlike a workstation or the Y-MP host, so in order to allow more than one user to have access to the MPP at any one time it is neccessary to have a method of allocating sets of PEs to different users. The way this is done is by dividing the MPP into Pools, and allocating each job a Partition in which to run.
Pools the T3D is divided up into various pools as defined by the system administrator. A pool of processors may support several active partitions (running programs) at the same time. Access to a pool may be restricted to a particular group of users and/or batch queues. A pool must have PEs, and may allow batch jobs, interactive jobs or a mixture of the two.
Partitions a partition is the set of processors associated with a particular job and is allocated every time you run. A partition is restricted to a number of PEs that is a power of 2. The smallest possible partition has 2 PEs, so even single processor jobs will occupy 2 PEs (and be time charged for the same).

34 Course notes

Cray T3D Hardware and Software Overview

EPCC pool configurations
Users require both production and software development time and it is difficult always to achieve the correct compromise between these needs. Therefore, we plan to have a configuration schedule which is sufficiently flexible to accommodate changes in the users' needs. The set up is in continual review, so to find out what the current configuration is, look at the following web page:
http://www.epcc.ed.ac.uk/systems/t3d.html
The configuration at the current time is as follows :
DAY configuration (1000 to 1800 Mon to Fri)
· 384-PE (batch) and 128-PE (interactive) pools
NIGHT configuration (1800 to 1000 Mon to Fri)
· 384-PE pool (batch only)
· 128-PE pool (interactive 0900 - 1000, Tue - Fri and 1800 - 2000, Mon to Thu;
batch all other times
WEEKEND configuration (1800 Fri to 1000 Mon)
· 512-PE pool (batch only)
Batch in the above schedule means that these pools will be under the control of the batch scheduling system; otherwise the processors can be accessed by all users for interactive development.
These pool configurations have been chosen to try and produce the best possible use from the current machine setup. The night and weekend batch queues give priority to 256 and 512 PE jobs respectively, and only when all of these have completed the batch queue handler works through the remaining jobs in order of size (256, 128 and finally 64 PE jobs). NQS requests of 64 processors or less are run in the 64 PE pool or when all 128 PE jobs have finished.
During weekdays 128 processors are reserved for interactive use. The 384 PE pool is used to run batch jobs. During the day, the maximum size for a batch job is 128 PEs.
T3D time charging
It is important to keep track of the way your program's PE time will be charged. Batch jobs are charged for the whole elapsed time of the job, while interactive jobs are charged for the time your T3D program runs. This means that time spent running only on the Y-MP or J90 during the execution of a batch MPP job still incurs a charge for T3D time.
J90 or Y-MP only jobs currently have no T3D charge. However, the Y-MP is principally used for the support of T3D jobs, and its use for other work (eg. compilations) may cause serious problems for other users. The Y-MPs resources should, therefore, be preserved for real T3D front-end activity and not wasted on non-T3D work. All MPP and PVP compilations should be done on the J90 system. The compilation of jobs to be run on the T3D is carried out on the J90, with the appropriate flag or environment variable set to tell the compiler that it is an MPP compilation. You should not request T3D PEs for any job that includes compilation or any other significant use of the Y-MP/J90 . There is a CPU timelimit imposed on interactive sessions, so large compilations should be performed in batch. Batch jobs which do not require T3D resources should be sent to the vector queue, which runs on the J90.

Edinburgh Parallel Computing Centre

35

Parallel Programming on the Cray T3D

5.3.3 Account IDs (ACIDs)
Accounting is performed on the basis of ACIDs (ACcount ID's). Time is allocated, in PE hours, to ACIDs. There are three classes of ACID; personal, project or group. Some of the EPSRC users may run with time allocated to their own ACID, most are expected to use a group ACID.
The command mppacct will return the current allocation for each ACID accessible by the user (in processor hours).
Associated with each ACID will be a list of users who are allowed to access it (actually each entry in the user data base contains a list of ACIDs that the user may use).
So, there might be a EPCC-owned ACID called PROJECT, and users joe, fred and cedric are allowed to account against that ACID.
Users can find out which ACIDs they have access to by using the command,
kelvin$ newacct -a
Also, the command,
kelvin$ newacct -l
returns the name of the current ACID to be accounted against.
If the currently selected ACID has no allocation associated with it, the user will be denied access to the MPP. A job running at the time that the ACID value falls to 0 will not be terminated - but any subsequent jobs will fail.
A user may be associated with any number of ACIDs. If the user has access to multiple ACIDs, the account against which the T3D use is to be charged may be selected by using the command,
kelvin$ newacct <ACID>
5.3.4 Compiling and Running

prog.f

cft77

prog.o

a.out

mppldr

mppexec

HOST

running program

T3D

Figure 27: Compiling and running on the T3D

36 Course notes

Cray T3D Hardware and Software Overview
The T3D fortran77 compiler-suite runs on the J90 front end. The same compiler system is used to compile programs for the J90, so the target has to be defined as crayt3d, using compiler command line arguments or setting the TARGET environment variable. The T3D f90, C and C++ compiler-suites are also located on the J90. Vectorisation and Auto-tasking are not available on the T3D. If you compile a sequential program for the T3D a separate copy of the program will run on each node.
The FORTRAN77 compiler-suite is accessed via the cf77 command. This program may call other programs, such as cft77 (the compiler) and mppldr (the linker). We recommend that you always use cf77 rather than calling the low-level programs explicitly. This is because there are different versions of the low-level programs for the J90 and the T3D.
5.3.5 Development tools
Cray supply a number of tools to aid software development on the T3D. These are XWindows based programs, which can communicate with each other and work interactively using the tooltalk interface.
TotalView interactive and post-mortem debugger, which allows line by line debugging of T3D code.
Apprentice profiler, specifically designed for the T3D, with the ability to measure different types of communication overhead and cache use associated with the Cray/DEC Alpha hardware.
XBrowse source code browser, allowing extended call trees to be shown graphically and the scope of variables to be examined.
5.3.6 T3D Languages
Fortran 77, f90, C, C++ , Assembler and PGHPF are currently available for the T3D.
Cray Research Adaptive Fortran (CRAFT)
CRAFT is the language Cray have chosen to use for the first implementation of most of the message passing and data parallel functions. It is a version of Fortran 77 with extensions which include the PVM message-passing model by default (there is no need to explicitly set up the pvm3 directories as in workstation PVM). It also supports optimised shared memory functions such as shmem_get and shmem_put and allows a data parallel programming style using data distribution and work-sharing.

Edinburgh Parallel Computing Centre

37

Parallel Programming on the Cray T3D
38 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D

6 Compiling, Debugging and Profiling Codes on the Cray T3D
A Short Practical Introduction
6.1 Overview
The aim of this part of the course is to introduce you to the way the Edinburgh T3D is used - logging in from a remote site, compiling code on the J90 host, running parallel executables on the T3D, and using the debugging, profiling and code browsing tools. By the end of this session, you will have:
· used the cf77 Fortran 77 compiler. · used the totalview debugger in postmortem mode. · used the apprentice profiler. · used the xbrowse code browser. · run a version of the ising model on several processors, and attempted
some simple optimisations.
6.2 Logging on to the T3D service
6.2.1 Your own workstation
You will be running code on the T3D using the EPCC training room sun workstations. The procedure is similar to logging in from a remote site. Once you have been given an account name and password, you will be able to log in to the T3D. Before you do so, there are a few things you should do on your local workstation.
· Make sure you have at least one spare window on the workstation you are using in case of network problems.
· Make sure your workstation's X server will allow remote machines to run XWindows based programs using your screen (all the tools on the T3D are X based). You can do this by typing xhost +kelvin on your workstation.

Edinburgh Parallel Computing Centre

39

Parallel Programming on the Cray T3D

6.2.2 The T3D service
The T3D is a stand-alone national service, and is not part of any of Edinburgh University's cross-mounted file systems. As a result, you will always connect directly via telnet, and will have to transfer files by ftp, even when logging in from the EPCC cluster. Try logging in now using the account name and password that you have been given. What you will see whilst logging in will be similar to the following :
patons@celestine$ telnet kelvin.ed.ac.uk Trying 129.215.184.11 ... Connected to kelvin.ed.ac.uk. Escape character is `^]'.

Cray UNICOS (kelvin) (ttyp031)

This is a private computer facility.Access for any reason must be specifically authorized by the owner.Unless you are so authorized, your continued access and any other use may expose you to criminal and/or civil proceedings.

login: patons Password:

Active label set to : level0,none

Last successful login was : Wed Jan 24 12:24:46 from celestine.epcc.ed.ac.uk

Cray J916/10-1024 sn9192

UNICOS 8.0.4.2

----------------------------------------------------------------

kelvin.ed.ac.uk

129.215.184.11

Edinburgh Parallel Computing Centre

National Parallel Supercomputing Service on CRAY T3D/MCN384-8

The University of Edinburgh ----------------------------------------------------------------

Current T3D system configuration, operational schedule and planned downtime information is posted under:

http://www.epcc.ed.ac.uk/systems/t3d.html

Information on the J90 aspects of the service is posted under:

http://www.epcc.ed.ac.uk/systems/j90.html

THE J90 SHOULD BE USED IN PREFERENCE TO THE Y-MP FOR ALL INTERACTIVE EDITING, COMPILING AND JOB SUBMISSION TASKS. ----------------------------------------------------------------

kelvin$

You are now logged on to the J90 host.

40 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D
6.3 Compiling on the J90 front end
6.3.1 The host environment
All the commands which you type now will be executed on the J90 front end only. This should really be known as the J90 IO and compile engine as it is not strictly a front end to the T3D. The Y-MP is the strict front end and we will need to run our T3D executables from this system. Executables are submitted to the T3D using a Y-MP command, mppexec, which is described later. Useful commands on the J90 include news, which gives information about recent software upgrades, and sar, which gives usage information for the front end over the last 24 hours. Typing ps will give information about processes on the J90 only. You will be logged in with a home directory /c1/z1/course??. This home directory is local to the J90 system but is cross-mounted to the Y-MP front end. These course accounts belong to the group z1 which has a restriction of ~1.5 Gbytes of disk space. Typing the command quota will mislead you as this states that your own personal quota is unlimited but you belong to a group which does have a limit. To check how much is available in the group use the command quota -g z1. The course account shells (sh) are set up by the .profile in your top level directory. If you are not logging in from a course account, then you may find it useful to copy this example .profile to your own account.
PATH=$PATH:/usr/bin/X11:/mpp/bin:/usr/local/bin TARGET=cray-t3d TRACEBK=0
# # Do not run these commands in batch mode #
if test "X${ENVIRONMENT}X" != "XBATCHX" then set -o gmacs stty -extproc #
# Find out where I logged in from who am i | tr -d ´()´ | read A B C D E remhost DISPLAY=$remhost:0 export DISPLAY #
EDITOR=emacs FCEDIT=emacs
eval `resize` alias fixwindow='eval `resize`' fi
export DISPLAY EDITOR FCEDIT PATH TARGET TRACEBK

Edinburgh Parallel Computing Centre

41

Parallel Programming on the Cray T3D
By default, the compiler does not compile for the T3D nodes. This is achieved by setting the TARGET environment variable. Setting TRACEBK to 0 ensures that a core will be dumped by a parallel program which fails. This is useful for debugging. The next lines relate to terminal settings. Your DISPLAY is set using a simple script which extracts the address of the machine from which you have telneted in. The next two lines set default editors. Then a macro `fixwindow' is defined, which you can run to inform the system of changes in the window size. Finally, the values of the environment variables are exported.
The shell provides the usual emacs editing keys (^F moves forwards, ^B moves backwards) and history mechanism (^P to move back through history, ^N to move forwards).
6.3.2 Editing
You can use the vi, emacs or uemacs editors.
6.3.3 Transferring files to and from Kelvin
Files have to be explicitly transferred to and from the EPCC cluster using ftp. There is a tar file, ising_parallel.tar, in /home/etg/courses/cray/T3D-Intro/ Examples which contains the course materials you will need for the exercises in this course. The following example shows how to transfer the course materials that you will use in the following exercises from the EPCC cluster.
kelvin$ ftp ftp.epcc Connected to crystal.epcc.ed.ac.uk. 220- Hello user at *unknown* 220 NOTICE: all transactions are logged Remote system type is UNIX. Using binary mode to transfer files. Name (ftp.epcc:patons):patons 331 Password required for patons. Password: 230 User patons logged in. ftp> cd /home/etg/courses/cray/T3D-Intro/Examples 250 CWD command successful. ftp> get ising_parallel.tar 200 PORT command successful. 150 Opening BINARY mode data connection for ising_parallel.tar (65536 bytes). 226 Transfer complete. 65536 bytes received in 0.21 seconds (3e+02 Kbytes/s) ftp> bye 221 Goodbye.
Unpack ising_parallel.tar by typing tar xvf ising_parallel.tar
42 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D
kelvin$ tar xvf ising_parallel.tar x ising_parallel/edges.f, 743 bytes, 2 tape blocks x ising_parallel/energy.f, 1298 bytes, 3 tape blocks x ising_parallel/ising.f, 4016 bytes, 8 tape blocks x ising_parallel/startup.f, 1781 bytes, 4 tape blocks x ising_parallel/sweeps.f, 5165 bytes, 11 tape blocks x ising_parallel/spins.inc, 570 bytes, 2 tape blocks x ising_parallel/Makefile, 1192 bytes, 3 tape blocks
6.3.4 Compiling Code for the T3D
In your ising_parallel subdirectory you will find the source code for a Fortran program, along with a Makefile. Fortran code is compiled on the T3D using cf77, which is a generic wrapper function for both the T3D and J90 compilers. For T3D code it calls
· gpp, the generic preprocessor, for .F files, · cft77, the Fortran77 compiler, for .f files, · mppldr, the T3D linker, for .o files. Try running make to build the application.
kelvin$ make cf77 -Ccray-t3d -c -g -I /usr/include/mpp -Wf'-dp -Ta' ising.f cf77 -Ccray-t3d -c -g -I /usr/include/mpp -Wf'-dp -Ta' startup.f cf77 -Ccray-t3d -c -g -I /usr/include/mpp -Wf'-dp -Ta' sweeps.f cf77 -Ccray-t3d -c -g -I /usr/include/mpp -Wf'-dp -Ta' energy.f cf77 -Ccray-t3d -c -g -I /usr/include/mpp -Wf'-dp -Ta' edges.f cf77 -Ccray-t3d -Wl'-M MAP,e,a -X4' -o ising ising.o startup.o sweeps.o energy.o edges.o -lapp -lmpi

Exercise: The compiler system.
Use the man pages (eg. man 1m cf77) to find out what these flags are doing. You will find that some of the flags are specific to seperate stages of the compilation and are on the man page for the function called. Some commands have seperate manual pages for the J90 and T3D versions of the command. You can go directly to the T3D manual page by telling the man command to look in the 1m section of the manual.
There are also online versions of the paper documentation. There are two ways of browsing these manuals. On kelvin there is a plain text browser called docview
kelvin$ docview &
There is also a hypertext browser called cdoc. This program only runs on workstations. You can either start cdoc on the workstation in front of you or you can run the cdoc command on kelvin. This starts cdoc remotely on a machine called cdochost, so that you will have to run the following command on your workstation to allow Xwindow connections from this machine.
workstation$ xhost +cdochost.epcc.ed.ac.uk kelvin$ cdoc

Edinburgh Parallel Computing Centre

43

Parallel Programming on the Cray T3D
Try using these sources of information to find out what the following flags are for :
-Ccray-t3d -c -g -G -Wf' ' -dp -Ta -I /usr/include/mpp -Wl' ' -M MAP,e,a -X4 -o -lapp
6.4 Running code on the T3D
6.4.1 Finding out about the state of the system

Before running code, it is wise to find out a bit about the current state of the MPP itself. The command mppstat gives information about the status of the T3D configuration. This is a Y-MP command, but it will work on the J90. The command mppstat on kelvin is a script that starts a remote shell on darwin which executes this command. You may have noticed that you have a .rhosts file in your home directory. This file gives permission to the systems named to run remote shells. There is also another file /etc/hosts.equiv which is controlled by the system administrator. This limits the systems that can be included in this .rhosts file. All of the commands that you will run on the J90 that start with mpp are executed in this way ( with the exception of mppexec which is covered later ).

kelvin$ cat /mpp/bin/mppstat remsh darwin /mpp/bin/mppstat

kelvin$ mppstat Configuration Information:

Torus PE dimensions : 16 x 4 x 6 Redundant PEs : 8 (total) 4 (mapped in)

Maximum pools : Maximum partitions : Total PEs available : Disabled PE count : Barrier Initialized? Config Time :

7 Pools in use : 32 Partitions in use : 8 0 yes Pools Initialized? Thu Jan 25 11:33:15 1996

2 8
yes

Adding the -a flag gives more detailed and reliable information, including warnings about hardware problems. If you wish to execute this command you will have to explicitly start a remote shell to do this.

kelvin$ remsh darwin /mpp/bin/mppstat -a | more

44 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D

You can also find out details about what MPP programs are currently running by using the command mppusers. This gives the output that would be seen if the Y-MP command ps -faMe had been executed. The -M flag forces ps to only print information about T3D partitions.

kelvin$ mppusers

UID PID PRTN TYPE STATE

cmg 92288 15 HW ACTIVE

yck 92982 20 HW ACTIVE

esmyth 93446 19 HW ACTIVE

baex01 93635 13 HW ACTIVE

bx 93691 22 HW ACTIVE

lag 93745 10 HW ACTIVE

arj 93907

1 HW ACTIVE

ajw29 93973 18 HW ACTIVE

PEs 8
32 16 32 64 128 64 32

ETIME COMMAND 18:31 Koonquest8 11:42 comfort96.mpi32
8:20 hel_t3d_shmem_p 6:38 prog 5:48 angus_restart 4:50 t3dabc45.e 2:44 ap3m 1:18 xrandy

6.4.2 The Ising model
The code you have compiled is a simple implementation of the ising model. It has been chosen as an example code which is split over several files and has a few bugs in it, to give you a feel for how the tools work with real programs.
The Ising model is the fruit-fly of physics! It is the simplest model of ferromagnetism and the simplest of a general class of problems which can be applied in many areas of science. It is a grid-based problem, where each point on the grid or lattice contains spin which can be up (+1 ) or down (-1 )1 .
We formulate the model on a 2D , L×L , periodic lattice. Each configuration, S , consists of L2 spin variables . There is a local, stochastic update rule for each spin and so this is called a Monte Carlo (MC) simulation. Applying the update rule exactly once to each spin on the lattice is known as a sweep . We are interested in the average of certain quantities over MC time, e.g. , the energy of the system, defined by

E

=

­

--1--L2

(i,j)

i



j

The sum is over 2L2 the unique nearest neighbour pairs (i,j) . A sweep consists of the following steps:
1. Choose a spin 2. Propose flipping the spin and calculate the change in energy, E. 3. Accept/reject according to the following rule:

1 if E  0 Paccept = e ­k (L2E) if E > 0

1. or on/off, positive/negative, occupied/unoccupied!

Edinburgh Parallel Computing Centre

45

Parallel Programming on the Cray T3D

where k is called the coupling constant or inverse temperature and should normally be in the range 0.0  1.0. 4. Repeat for all spins. 5. Measure the interesting observables, e.g. , E. The simulation does a number of these sweeps, as follows: 1. Initialise the lattice - hot = random; cold = uniform. 2. Perform Neq sweeps to equilibrate the lattice.
3. Calculate the observables on the next Nsim sweeps.
4. Calculate the averages:

Neq + Nsim

 O

=

-----1-----Nsimi

=

Neq

+

O
1

(

Si)

The updating of each spin depends on the four neighbours of each. At the edges of the spin array (sigma(x,y)) there is a halo of spins from the opposite edge, which is updated twice during each sweep. This halo forms the basis of a potential parallel domain decomposition, but that is well beyond the scope of this exercise.
SetParams() sets up the lattice size, Neq , and so on. The coupling constant, k, is set to 0.4.
DoSweeps() runs through Neq , cycles of Sweep(), then runs through Nsim cycles, calculating the values of Nsim E and Nsim E2 . 100 sweeps are carried out in this example.
Sweep() carries out a single update of the spins sigma(x,y) array. It can do this because each spin flips depending on its four nearest neighbours, so the whole grid can be updated in two passes, which reduces the number of times the edges need updating. This is done in a subroutine called Sweepchess() which takes ``black'' or ``white'' as its parameter.
6.4.3 Running a Code on the Cray T3D
The compilation has produced a program ising. All code is run on the T3D using the command mppexec. In fact, the compiler puts #!mppexec as the first line of the executable code, so it is possible to simply type the name of the exectable instead - but it is worth remembering that in fact you are just calling a Y-MP function, which is in turn starting up the T3D execution.
You cannot run an interactive T3D job from kelvin unless you again open a remote shell. However, things can start to get a bit complicated then, so the best thing to do is to bring up a darwin window and run the T3D executable from there.
workstation$ telnet darwin.ed.ac.uk ...... darwin$ cd ising_parallel

46 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D

You can run the example with the command mppexec ising or with ./ising. In general it is not possible to run an executable which has been compiled for a set number of nodes on a different number, but if you leave out the -X flag, the code will be linked at runtime to run on the number of processors specified by the -npes flag. If there is space on the MPP, your code will be allocated a partition (the smallest possible size being 1 node, ie. 2 PEs) and will run with stdout from all the processors going to the screen.

darwin$ ./ising

Floating point exception

Beginning of Traceback (PE 0): Started from address 0x20000069c8 in routine `GETENERGY'. Called from line 82 (address 0x2000000d88) in routine `DOSWEEPS'. Called from line 34 (address 0x2000000394) in routine `ISING'. Called from line 307 (address 0x200000e1e0) in routine `$START$'.
End of Traceback.

Agent printing core file information: user exiting after receiving signal 8 Exit message came from virtual PE 0, logical PE 0x400 Register dump

... f00:0xfffbfffbfffbfffb
f02:0x0000000000000000 f03:0x0000000000000000
f05:0x0000000000000000 f06:0x0000000000000000
f08:0x0000000000000000 f09:0x0000000000000000
f11:0x4024000000000000 f12:0x3fc555555555555a
f14:0x4000000000000000 f15:0x3f8111111110f224
f17:0x0000000000000000 f18:0x0000004000014b18
f20:0x000000200000e1e0 f21:0x0000004000003200
f23:0x3fd101d128933330 f24:0x3f94dec899fffb46
f26:0x3fa3ca89f833b48f f27:0x3f69b61270e43860
f29:0x408ff00000000000 f30:0x0010000000000000
fpcr:0x8920000000000000

f01:0x0000000000000000 f04:0x0000000000000000 f07:0x0000000000000000 f10:0x0000000000000003 f13:0x3f90000000000000 f16:0x3ff0000000000000 f19:0x0000000000000133 f22:0x3fb213dcabe579ee f25:0x4000000000000000 f28:0x0000000000000000

Agent finished printing core file information. User core dump completed (./mppcore) Floating exception

The program is run on 4 PEs, and should dump a core in the file mppcore due to some sort of run-time error.

Edinburgh Parallel Computing Centre

47

Parallel Programming on the Cray T3D
6.4.4 Totalview: the Cray-T3D debugger.
The totalview debugger is provided on the system. You could use totalview either as a post-mortem or as an interactive debugger, but since interactive mode occupies the PEs fully for all the time you are using it this is a very wasteful use of T3D time (currently a scarce resource) and inconveniences the other users (of whom there are around 600). There has also been a serious bug found with this mode of operation which can bring the T3D down. Only post-mortem mode is now available.
Post-mortem debugging
Given an executable (in this case ising) and a core file generated by that executable (in this case mppcore), you can start the debugger by typing :
kelvin$ totalview ising mppcore
Try this now. Two windows will appear: a top-level window and a process window.
Totalview has used the core file to find out where the program crashed, and shows you this part of the source code.
The top left window, Call Sequence, shows the stack of routines which were called to reach this point. Clicking on a routine with either button shows its source in the main, source code window. $start$ is part of the system, which isn't instrumented, and is practically unintelligible.
When a particular routine is displayed, the top right window shows the values of its arguments and its local variables.
You can also "dive" on variables by clicking on them with the right button (in the source window too). This will pop up another window showing the value of the variable.
Exercise: debugging the code
· See if you can work out why the program crashed. · Have a look at some of the other variables at the time the program crashed. In
particular, see if you can look at the spins array sigma(x,y). Is there any way to see it arranged in two dimensions? · Fix the problem and recompile the code.
To exit totalview you select the "Quit" option from the "File" menu in the top-level window.
6.4.5 Apprentice: the Cray-T3D profiler
Once you have a working program, you should remove the -g flag from the Makefile, type make clean (to remove out of date files), recompile, and then run the program. The program is being run on 4 PEs of the T3D, and each PE outputs results to a file of the form results_pe.00 . The presence of the -g flag causes optimisation to be disabled, and in order to profile a code (or indeed for a production code) optimisation to some level is essential. You can now use the profiling tool apprentice to determine where the program is spending its time.
Your program will have produced a file app.rif which contains runtime information. This is only produced if the code runs to completion successfully. In addition, at compile time, each .f file will have had a corresponding .T file created. These are compiler information files, and allow the code to be read by the profiler to help you interpret the app.rif data.
48 Course notes

Compiling, Debugging and Profiling Codes on the Cray T3D
You can run the profiler by typing :
kelvin$ apprentice app.rif &
Each of the 4 PEs has a different value of k when the program is run, and so the PEs do not all take the same time to execute the program. You will see that the subroutine MPI_BARRIER is taking the most time. This is a synchronisation routine which causes all of the PEs to pause their execution of the program until they have all reached the same point - then they continue. This sort of routine is typical of parallel programming and ensures that the individual PEs know what stage of the calculation the other PEs are at. In the ising program this call occurs after the computation has been completed, but before the results are output. Therefore, the subroutine that completes the computation fastest waits longest, and the subroutine that completes the computation slowest waits virtually no time at all. By clicking on the name of the routine in the apprentice window (so that it is highlighted) and then selecting "Call Sites" from the "Displays" menu, you can find out where this routine is called in the program. By then clicking on the location that it is called in, you can obtain information about the load balancing at the bottom of the "Call Sites" window.
You will also see that the computational routine which takes up the most time in the program is the _EXP subroutine. Again, by clicking on name of the routine, and selecting "Call Sites" from the "Displays" menu, you can find out useful information about the execution of this routine by all of the PEs.
6.4.6 Xbrowse: The Cray source code browser
You can use a browser to locate routines in the code. You can invoke this using the "Source Code" option from the "Displays" menu in apprentice. Source code will then be loaded to correspond with the selections you make in the apprentice main window.
The xbrowse display consists of a set of three windows at the top, a source code window in the middle and an information window at the bottom. The three windows at the top show the item which is currently of interest.
Use xbrowse to view the source for the subprogram UPDATESPIN. Now use the "Find" menu and select the option "Calls" to list all calls made from UPDATESPIN. A list including EXP will appear. Click on EXP to find where it is used. You should find the expressions exp(-4.0*k) and exp(-8.0*k).
You can also invoke xbrowse on its own to view the entire program:
kelvin$ xbrowse *.f &
Find the menu options which allow you to display the call tree of the program, and see how the calls to EXP fit into the structure of the code. Use the "Find" menu option "Callers" to see where UPDATESPIN is called from.
Exercise: Possible code optimisation
· Is there any way of reducing the number of times EXP is called?
In fact, because the sweep is being carried out at a single temperature, the values of exp(-4.0*k) and exp(-8.0*k) need only be calculated once, perhaps in the initialisation routine. See if you can reduce the runtime of the program by reducing the number of times that the EXP is called.

Edinburgh Parallel Computing Centre

49

Parallel Programming on the Cray T3D
50 Course notes

