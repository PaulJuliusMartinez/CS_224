From: AAAI Technical Report SS-96-05. Compilation copyright © 1996, AAAI (www.aaai.org). All rights reserved.

Document Routing as Statistical Classification *

David Hull
Rank Xerox Research Centre 6, chemin de Maupertuis, 38240 Meylan, France
{david.hull}@xeroxf,r

Jan Pedersen and Hinrich Schfitze
Xerox Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA94304
{peders en, schuetze}Qparc, xerox, com

Abstract
In this paper, we comparelearning techniques based on statistical classification to traAitional methodsof relevance feedback for the documentrouting problem. Weconsider three classification techniques which have decision rules that are derived via explicit error minimization:linear discriminaatanalysis, logistic regression, and neural networks. Wedemonstrate that the classifiers perform10-15%better than relevance feedbackvia Rocchio expansion for the TREC-2and TREC-3routing tasks.
The Routing Problem
Of the two classical information retrieval tasks 1 document routing is most amenable to machine learning. A fixed, standing query, and a training collection of judged document2s is provided and the task is to assess
the relevance of a fresh set of test documents.This can clearly be approached as a problem of statistical text classification: documentsare to be assigned to one of two categories, relevant or non-relevant, and inference is possible from the labeled documents. In contrast, the classical ad-hoc search problem presumes only a query and an unlabelled collection is provided.
The standard approach to document routing models document content as a bag-of-words, represented as a sparse, very high-dimensional vector, with one component for each unique term in the vocabulary (Salton, Wong,&Yang 1975). Vector weights are proportional to term frequency and inversely proportional to collection frequency.3 The general technique is to score test documentswith respect to their closeness to the query (also represented a sparse, high-dimensional vector),
*Authorslisted in alphabetic order. l as defined and evaluated by the TRECconfer-
ences(Itarman 1994; 1995) 2Actually, only a few documentsare explicitly labeled,
including most of the relevant documentsand a few of the irrelevant documents.All other documentsare implicitly assumedto be irrelevant.
3Theexact expression varies across systems, but is typ-

where closeness is measuredby the cosine between vectors. A modified and expanded query is learned from
the training set via Rocchio-expansionRelevance Feedback (Buckley, Salton, &Allan 1994), which essentially constructs a linear combination of the query vector, the centroid of the relevant documents and, occasionally, the centroid of select irrelevant documents4. The net result is a scored list of test documents, which maybe ranked in decreasing score order for the purposes of presentation and evaluation. Evaluation typically proceeds by averaging precision 5 a6t a number of recall thresholds.
Rocchio-expansion Relevance Feedback employs a weak learning method. However, the application of stronger methods faces two problems: the very high dimensionality of the native feature space (typically hundreds of thousands of dimensions for moderate sized text collections), and the relatively small numberof positive examples in the training collection. The first difficulty maybe addressed through dimensionality reduction techniques, and the second by constraining the training domain. One key issue, then, is whether the improved classification power of strong learning methods can compensate for the loss of information due to
dimensionality reduction and local analysis. We have examined two dimensionality reduction
techniques, term selection and a variation of latent semantic indexing (Deerwester et ai. 1990), against numberof different error-minimizing classifiers: linear discriminant analysis (LDA), logistic regression

ified by

d,(j) = f,(wj)log (N/N(w,))

wheref~(wj) is the frequency of wordwj in documenti, N is the total numberof documents, and N(wj) the number of documentscontaining wj.
4those explicitly judgedirrelevant
5the proportion of relevant documentsin the documents
seen so far Sthe proportion of relevant documentseen so far out of
the total numberof relevant documents

49

(LR), linear neural networks (LNN), and non-linear neural networks (NNN)(Schfitze, Hull, & Pedersen 1995). These techniques offer significant theoretical advantages over Rocchio Expansion because they explicitly minimize misclassification error based on an underlying model with enough generality to take full advantage of the information contained in a large sample of relevant documents. In contrast, Roechio Expansion assumes independence between features and fits its parameters in a heuristic manner based on term frequency information from the corpus. This paper will demonstrate that these advantages translate directly into improvedretrieval performance for the routing problem. We use the Tipster collection and the TREC-2and TREC-3routing tasks to test classifiers and representations (Harman1994; 1995).
Methodology
Conventional classification techniques must be adapted for the routing problem. Traditional learning algorithms do not scale to the native feature set, either due to computational tractability issues or because of the risk of overfitting. Similarly, the training collection is simply too large, and relevant documents too rare to efficiently learn the optimal classification rule. Therefore, we have developed a three-step algorithm to approach the routing problem.
Step 1: Local Regions
In the first stage, standard RocchioExpansion7 is applied to form a modified query vector. All documents in the training collection are ranked according to their similarity to the expandedquery, and the top 2000 documents are selected to define the local region. These documentsthen form the training set.
There are a number of advantages to constraining the training domainin this manner. First, the size of the training set is controlled, makingit possible to apply computationally intensive variable selection techniques and learning algorithms in a reasonable length of time. Second, the density of relevant documentsis muchhigher in the local region than in the collection as a whole, which should improve classifier performance. Third, the non-relevant documentsselected for training are those which are most difficult to distinguish from the relevant documents. These non-relevant documents are clearly amongthe most valuable ones to use as training data for a learning algorithm.
7without considering irrelevant documents

Step 2: Document Representations
To reduce the dimensionality of the native feature space, whichfor the Tipster collection includes 2.5 million unique words and selected word pairs, we consider two separate approaches: optimal term selection and reparameterization of the documentspace.
The process of optimal term selection consists of identifying the wordsmost closely associated with relevance. Our approach is to apply a X2 test to the contingency table containing the number of relevant and non-relevant documents in which the term occurs (Nr+ and Nn+, respectively), and the number of relevant and non-relevant documents in which the term doesn't occur (Nr- and Nn-, respectively).
X2 -- 2N(Nr+N,_ - Nr_N,÷) (N~+ + N~-)(Nn+ + N,~_)(N~+ + Nn+)(N,_
The higher the score, the greater the association between that term and the relevant documents, s Weselect the 200terms with the highest scores for input into our learning algorithms.
As an alternative, we apply Latent Semantic Indexing (LSI) (Deerwester et al. 1990) to represent documents by a low-dimensional linear combination of orthogonal indexing variables. The LSI solution is computed by applying a singular value decomposition to the sparse document by term matrix constructed from the local region. Wethen select the 100 most important LSI factors to serve as an alternative document representation.
One expects optimal term selection to work best if there is a relatively small specific vocabulary associated with the query while one expects LSI to performs well of the query has a very large vocabulary which can be organized into a smaller numberof general concepts.
Step 3: Classification Algorithms
Wehave run a number of classification techniques against the reduced features computedin step 2. These are briefly described below.
Linear discriminant analysis (LDA)finds a linear combination a of the feature elements which maximizes the separation between the centroids of the relevant Sr and non-relevant xn documents: a = S-t(~,r - xn), where S is the pooled within-group covariance matrix. Logistic regression (LR) fits the log odds of the probability of relevance as a linear function of the features using maximumlikelihood estimation. The linear neural network (LNN)(no hidden units) uses backpropagation to iteratively fit a logistic model. To protect
sit is also theoretically possible to havea high score if term is highly associated with irrelevant documents,however since irrelevant documentsaxe so commotnhis is not observedin practice

50

againstoverfitting, and to distinguish it from the lo-
gistic regression classifier, the networkis not iterated to convergence,rather a validation set is used to determine the optimal number of training iterations. The non-linear neural net (NNN)adds a layer of hidden units to potentially capture non-linear structure.
To minimizemisclassification errors, a binary classifier is simply required to estimate whether the probably of relevance is greater than one half. However,the evaluation criterion for the routing task requires the test set to be ranked. This can be achieved by scoring documentsby probability of relevance. Note, this is not guaranteed to be optimal except under certain conditions, e.g. if the evaluation criterion is a linear utility function (Lewis 1995).
Summary of Routing Algorithm
Wepresent a simple flow chart that describes the training and testing process of our routing algorithm. The training process:
¯ Compute 2000 nearest documents to Rocchio expanded query (local region).
¯ ComputeLSI decomposition of local region and select 100largest factors.
¯ Compute X2 statistic to select 200 most valuable terms in local region.
¯ Applyclassification technique to documentsin local region using the appropriate representation.
The testing process:
¯ Select test documents whosesimilarity to the Rocchio expanded query exceeds a threshold similarity score, placing it in the local region.
¯ Obtain LSI document vectors for the selected test documents.
¯ Obtain selected term representation for the selected test documents.
¯ Score selected test documentsusing the probability output from the trained classification rule.
¯ Rank the test documents by descending probability score. All test documents in the local region are ranked ahead of those that fall below the threshold similarity score.

Experimental

Set-Up

The Tipster corpus used for our experiments consists of 3.3 gigabytes of text in over one million documents from several different sources: newswire, patents, sci-
entific abstracts, and the Federal Register (Harman 1993). In addition, there are 200 judged Tipster topics, detailed statements of information need, that can serve a routing queries.
Wepreprocessed the corpus using the TDBsystem (Cutting, Pedersen, & Halvorsen 1991), to parse and tokenize documents, and to stem and remove stop words. Indexed terms consisted of single words and two-word phrases that occurred over five times in the
corpus (where a phrase is defined as an adjacent word pair, not including stop words). This process produced over 2.5 million terms. Then, each documentwas partitioned into overlapping sections of average length 300 words with an average overlap of approximately 250 words. The highest scoring section of each document (with respect to the expanded query that defines the local region) was used in all further processing.
For our routing runs, we replicated the routing setup for the second and third TRECconferences. Disks 1 and 2 (about two gigabytes) are the training collection, Disk 3 (about one gigabyte) is the test set. Each combination of classifier and input representation is run
for two sets of topics: 51-100 (corresponding to the routing task in TREC2 (Harman 1994)) and 101-150 (corresponding to the routing task in TREC3 (Harman1995)). Our goals in these experiments were (1) to demonstrate that strong learning methods can perform better than Rocchio expansion, (2) to find the most effective classification technique for the routing problem, and (3) to make sure that our comparison between LSI and term-based methods is not based on the idiosyncrasies of a particular learning algorithm.

Experimental

Results

Table 1 presents routing results for 5 different classitiers and 4 different representations. The representations are:

a) Rocchio-expansion Relevance Feedback

b) LSI (100 factors from a query-specific local LSI)

c) 200 terms (200 highest ranking terms according to x~-test)

d) LSI + terms (100 LSI factors and 200 terms).
Theclassifiers are:
a) baseline (documents ranked by closeness to query vector for "LSI", "200 terms", and "LSI + 200

51

classifier baseline
logistic regression LDA
linear network non-linear network

input
expansion LSI 200 terms LSI + 200 terms LSI 200 terms LSI + 200 terms LSI 200 terms LSI + 200 terms LSI 200 terms LSI + 200 terms LSI 200 terms LSI + 200 terms

precision for Topics 51-100 precision for Topics 101-150 average average % change at 100 average % change at 100 change

0.3678 0.3240 0.3789 0.3359

+0.0% -11.9%
+3.0% -8.7%

0.4710 0.4210 0.4824 0.4426

0.3705 0.3268 0.3712 0.3358

+0.0% -11.8% +0.2% -9.4%

0.4194 0.3908 0.4440 0.3928

+0 -12 +2 -9

0.3980 0.3654 0.3494

+8.2% 0.5108 -0.7% 0.4788 -5.0% 0.4652

0.4057 0.3637 0.3457

+9.5% 0.4802 -1.8% 0.4434 -6.7% 0.4168

+9 -1
-6

0.4139 0.3966 0.3973
0.4098 0.4209 0.4273 0.4110 0.4210 0.4251

+12.5% +7.8% +8.0%
+11.4% +14.4% +16.2% +11.7% +14.5% +15.6%

0.5166 0.4916 0.5034
0.5094 0.5044 0.5180 0.5090 0.5026 0.5204

0.4230 0.3841 0.3910
0.4211 0.4121 0.4302 0.4208 0.4115 0.4318

+14.2% +3.7% +5.5%
+13.7% +11.2% +16.1% +13.6% +11.1% +16.5%

0.4870 0.4586 0.4616
0.4830 0.4742 0.4908 0.4834 0.4740 0.4882

+13 +6 +7
+13 +13 +16 +13 +13 +16

Table 1: Non-interpolated average precision, precision at 100 documents and improvement over expansion for routing runs on TRECdata.

terms" and closeness to expanded query vector "for expansion")
b) logistic regression
c) linear NN
d) non-linear NN
e) LDA(linear discriminant analysis)
The run "expansion" was tf-idf weighted, and terms in the baseline runs were idf-weighted. Inverse document frequency (idf) weights are derived from the entire training set, not from the local region. All other runs on terms were not weighted: the input was 1 if the term occurred in the document and 0 otherwise. This strategy was motivated by poor results for runs in which terms were weighted according to frequency of occurrence and a desire to let the learning algorithms select the proper weight for each term.
These experimental results are analyzed using ANOVAand the Friedman Test (Hull 1993) to measure their statistical significance. ANOVdAetermines that one method is significantly better than another if the average difference in performance is large compared to its variability, correcting for differences between queries. The Friedman test conducts a similar analysis, but it uses only the ranking of the methods within each query.

Conclusions
From Table 1 we can draw the following conclusions: Classification vs. Expansion. More advanced
learning algorithms increase performance by 10 to 15 percent over query expansion. LDAand neural networks perform significantly better than the baseline experiments, regardless of representation. Logistic regression only performs better when using an LSI representation (significant difference ~ .02).
LSI vs. Selected terms. LDAand logistic regression work significantly better with LSI features than with term features. Neural networks work equally well with either LSI or term-based features, and significantly better with a combination of LSI and termbased features (significant difference ~ .01).
Logistic Regression vs. Other Classifiers. For LSIfeatures, logistic regression is less effective than the other learning algorithms according to the FriedmanTest, although the magnitude of the difference is small. For word or combinedfeatures logistic regression performs a lot worse than either LDAor neural networks.
Linear vs. Non-linear neural networks. The results suggest that there is no advantage to adding non-linear componentsto the neural network.
LDAvs. Neural networks. For LSI features, LDAand neural networks perform about the same. Neural networks are superior to LDAfor the other representations. The best neural network performance (combined features) is slightly better than the best

52

LDAperformance (LSI features), but not enough be statistically significant.
The sharp observer will note that the magnitude of the significant difference changes, depending on the experiment. This occurs because the variability between learning algorithms is greater than the variability between representations. Therefore, comparisons between experimental runs using the same learning algorithm can detect the significance of a smaller average difference.
The most important conclusion is that advanced learning algorithms capture structure in the feature data that was not obtained from query expansion. It is also interesting that the linear neural network works better than logistic regression, since they are using exactly the same model. This indicates that the logistic modelis overfitting the training data, and the ability of the neural network to stop training before convergence is an important advantage. NN'scan also benefit from the additional information available by combining the word and LSI features unlike the other classification techniques. Evidenceof overfitting for logistic regression can be found by observing that performance decreases when going from LSI or term features to a combined representation. Using a more general feature space should only increase performance over the training set, yet it hurts performancein the final evaluation. Linear discriminant analysis also suffers from overfitting, which explains whyit works most successfully with the compact LSI representation. To the best of our knowledge, the results given here for LDAand neural networks are at least as goodas the best routing results published for TREC-2(Buckley, Salton, Allan 1994) and TREC-3(Robertson ct ai. 1994).
References
Buckley, C.; Salton, G.; and Allan, J. 1994. The effect of adding relevance information in a relevance feedback environment. In Proc. 17th Int'l Conference on R~Din IR (SIGIR), 292-300.
Cutting, D. R.; Pedersen, J. O.; and Halvorsen, P.K. 1991. An object-oriented architecture for text retrieval. In Conference Proceedings of RIAO'91, Intelligent Text and Image Handling, Barcelona, Spain, 285-298. Also available as Xerox PARCtechnical report SSL-90-83.
Deerwester, S.; Dumais, S.; Furnas, G.; Landauer, T.; and Harshman, R. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science 41(6):391-407.
Harman, D. 1993. Overview of the first trec conference. In Proceedings of SIGIR '93.

Harman, D., ed. 1994. The Second Text REtrieval Conference (TREC-P). NIST.
ttarman, D., ed. 1995. The Third Text REtrieval Conference (TREC-3). NIST.
Hull, D. 1993. Using statistical testing in the evaluation of retrieval performance. In Proc. of the 16th ACM/SIGIR Conference, 329-338.
Lewis, D. 1995. Evaluating and optimizing autonomous text classification systems. In Proc. 18th Annual Int'! ACMSIGIR Conference on RgJD in IR, 246-255.
Robertson, S. E.; Walker, S.; Jones, S.; HancockBeaulieu, M. M.; and Gatford, M. 1994. Okapi at tree-3. In Text Retrieval Conference 3 (preproceedings).
Salton, G.; Wong, A.; and Yang, C. 1975. A vector space model for automatic indexing. Communications of the ACM18(11):613-620.
Schiitze, H.; Hull, D.; and Pedersen, J. 1995. Acomparison of documentrepresentations and classifiers for the routing problem. In Proc. 18th lnt'l Conference on R~D in IR (SIGIR), 229-237.

53

