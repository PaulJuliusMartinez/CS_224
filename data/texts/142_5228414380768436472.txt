From: AAAI Technical Report SS-95-05. Compilation copyright © 1995, AAAI (www.aaai.org). All rights reserved.

Making Robots Conscious of their Mental States
John McCarthy Computer Science Department
Stanford University Stanford, CA 94305 jmc@cs, stanforde,du

Abstract
In AI, consciousnessof self consists in a programhaving certain kinds of facts about its ownmental processes and state of mind. Wediscuss whatconsciousness of its ownmental structures a robot will need in order to operate in the common sense world and accomplishthe tasks humanswill give it. It's quitea lot. Manyfeatures of humanconsciousness will be wanted, somewill not, and someabilities not possessed by humanswill be foundfeasible and useful. Wegive preliminary fragments of a logical language a robot can use to represent information about its own state of mind. Arobot will often haveto concludethat it cannotdecide a question on the basis of the information in memory and therefore mustseek informationexternally. GSdel's idea of relative consistency is used to formalize nonknowledge. Programswith the level of consciousness discussed in this article donot yet exist. Thinking about consciousness with a view to designing it provides a newapproach to someof the problemsof consciousness studied by philosophers. The advantage is that it focusses on the aspects of consciousnessimportant for intelligent behavior.
Introduction
In this article we discuss consciousness with the methodology of logical AI. (McCarthy 1989) contains a recent discussion of logical AI. TheRemarksection has a little
about how our ideas about consciousness might apply to other AI methodologies. However,it seems that systems that don't represent information by sentences will be limited in the amountof self-consciousness they can have.
(McCarthy 1959) proposed programs with common sense that represent what they know about particular situations and the world in general primarily by sentences in some language of mathematical logic. They decide what to do primarily by logical reasoning, i.e. when a logical AI program does an important action,
it is usually because it inferred as sentence saying it should. There may be other data structures and programs, but the maindecisions of what do by logical reasoning from sentences explicitly present in the robot's memory. Someof the sentences may get into memoryby processes that run independently of the robot's decisions,

e.g. facts obtained by vision. Developmentsin logical AI include situation calculus in various forms, logical learning, nonmonotonic reasoning in various forms, theories of concepts as objects (McCarthy 1979b) and theories contexts as objects (McCarthy 1993). (McCarthy 1959) mentionedself-observation but wasn't specific.
There have been manyprograms that decide what do
by logical reasoning with logical sentences. However,I don't knowof any that are conscious of their ownmental processes, i.e. bring sentences about the sentences generated by these processes into memory. Wehope to establish in this article that someconsciousness of their ownmental processes will be required for robots to reach a level intelligence needed to do manyof the tasks humans will want to give them. In our view, consciousness of self, i.e. introspection, is essential for humanlevel intelligence and not a mere epiphenomenon. However, we need to distinguish which aspects of humanconsciousness should be modelled, which humanqualities should not and where AI systems can go beyond human consciousnes.
For the purposes of this article a robot is a continuously acting computer program interacting with the outside world and not normally stopping. What physical senses and effectors or communication channels it has are irrelevant to this discussion except as examples.
In logical AI, robot consciousness maybe designed as follows. At any time a certain set of sentences are di-
rectly available for reasoning. Wesay these sentences are in the robot's consciousness. Somesentences come into consciousness by processes that operate all the time, i.e. by involuntary subconscious processes. Others come into consciousness as a result of mental actions, e.g. observations of its consciousness, that the robot decides to take. The latter are the results of introspection.
Here's an example of human introspection. Suppose I ask you whether the President of the United States
is standing, sitting or lying downat the moment,and suppose you answer that you don't know. Suppose I then ask you to think harder about it, and you answer that no amountof thinking will help. [See (Kraus, Perils and Horty 1991) for one formalization.] A certain amountof introspection is required to give this answer, and robots will need a corresponding ability if they are to decide correctly whether to think more about a question or to
seek the information they require externally.
Wediscuss what forms of consciousness and introspec-

tion are required and how some of them may be formalized. It seems that the designer of robots has many
choices to makeabout what features of humanconsciousness to include. Moreover, it is very likely that useful robots will include someintrospective abilities not fully possessed by humans.
Twoimportant features of consciousness and introspection are the ability to infer nonknowledgeand the ability to do nonmonotonicreasoning.
Human-like emotional structures are possible but unnecessary for useful intelligent behavior. Wewill also argue that it is best not to include any that wouldcause people to feel sorry for or to dislike robots.

What Consciousness

does a Robot Need?

In somerespects it is easy to provide computer programs with more powerful introspective abilities than humans

have. A computer program can inspect itself, and many programs do this in a rather trivial way. Namely, they compute check sums in order to verify that they have been read into computer memorywithout modification.
It is easy to makeavailable for inspection by the program the manuals for the programming language used, the manualfor the computeritself and a copy of the compiler. A computer program can use this information to simulate what it woulddo if provided with given inputs. It can answer a question like: "WouldI print "YES"in less than 1,000,000 steps for a certain input? A finite version of Turing's argument that the halting problem is unsolvable tells us that that a computer cannot in general answer questions about what it woulddo in n steps in less than n steps. If it could, we (or a computer program) could construct a program that would answer question about what it would do in n steps and then do the opposite.
Unfortunately, these easy forms of introspection are

not especially useful for intelligent behavior in many commonsense information situations.
We humans have rather weak memories of the events in our lives, especially of intellectual events. Theability to remembeirts entire intellectual history is possible for

a computer program and can be used by the program in modifyingits beliefs on the basis of newinferences or

observations. This mayprove very powerful. To do the tasks we will give them, a robot will need at
least the following formsof self-consciousness, i.e. ability to observe its own mental state. Whenwe say that something is observable, we meanthat a suitable action by the robot causes a sentence and possibly other data structures giving the result of the observation to appear

in the robot's consciousness.

¯ Observing its physical body, recognizing the positions of its effectors, noticing the relation of its body to the environment and noticing the values of important internal variables, e.g. the state of its power supply and of its communicationchannels.
¯ Observing that it does or doesn't knowthe value of
a certain term, e.g. observing whether it knowsthe telephone numberof a certain person. Observing that it does knowthe numberor that it can get it by some

procedure is likely to be straightforward. 1 Deciding that it doesn't knowand cannot infer the value of a telephone number is what should motivate the robot to look in the phone book or ask someone.
¯ Keepinga journal of physical and intellectual events so it can refer to its past beliefs, observationsand actions.
¯ Observing its goal structure and forming sentences about it. Notice that merely having a stack ofsubgoals doesn't achieve this unless the stack is observable and not merely obeyable.
¯ The robot mayintend to perform a certain action. It maylater infer that certain possibilities are irrelevant in view of its intentions. This requires the ability to observe intentions.
¯ Observinghowit arrived at its current beliefs. Mostof the important beliefs of the system will have been obtained by nonmonotonic reasoning, and therefore are usually uncertain. It will need to maintain a critical viewof these beliefs, i.e. believe recta-sentences about them that will aid in revising them whennew information warrants doing so. It will presumably be useful
to maintain a pedigree for each belief of the system so that it can be revised if its logical ancestors are revised. Reason maintenance systems maintain the
pedigrees but not in the form of sentences that can be used in reasoning. Neither do they have introspective subroutines that can observe the pedigrees and
generate sentences about them.
¯ Notonly pedigrees of beliefs but other auxiliary information should either be represented as sentences or be observable in such a wayas to give rise to sentences. Thus a system should be able to answer the questions: "Whydo I believe p?" or alternatively "Whydon't I believe p?".
¯ Regarding its entire mental state up to the present as an object, i.e. a context. (McCarthy 1993) discusses contexts as formal objects. The ability to transcend one's present context and think about it as an object is an important form of introspection, especially when we compare humanand machine intelligence as Roger Penrose (1994) and other philosophical AI critics do.
¯ Knowingwhat goals it can currently achieve and what its choices are for action. Weclaim that the ability
to understand one's ownchoices constitutes free will. The subject is discussed in detail in (McCarthy and Hayes 1969).
The above are only someof the needed forms of selfconsciousness. Research is needed to determine their properties and to find additional useful forms of selfconsciousness.
1
However,observing that it doesn't knowthe telephone numberand cmmotinfer what it is involves getting around G6del's theorem. Because, if there is any sentence that is not inferrable, a systempowerfulenoughfor arithmetic must be consistent. Therefore, it might seemthat G6del's famous theorem that the consistency of a system cannot be shown within the system wouldpreclude inferring non-knowledge except for systemstoo weakfor arithmetic. G~del's(1940) idea of relative consistencygets us out of the difficulty.

9O

Understanding and Awareness
Wedo not offer definitions of understanding and awareness. Instead we discuss which abilities related to these phenomenarobots will require.
Consider fish swimming. Fish do not understand swimmingin the following senses.
¯ A fish cannot, while not swimming,review its previous swimmingperformance so as to swim better next time.
¯ A fish cannot take instruction from a moreexperienced fish in howto swimbetter.
¯ A fish cannot contemplate designing aflsh better adapted to certain swimmingconditions than it is.
A human swimmermay understand more or less about swimming. 2
Wecontend that intelligent robots will need understanding of howthey do things in order to improve their behavior in ways that fish cannot. Aaron Sloman (1985) has also discussed understanding, makingthe point that understanding is not an all-or-nothing quality.
Consider a robot that swims. Besides having a program for swimmingwith which it can interact, a logicbased robot needs to use sentences about swimmingin order to give instructions to the program and to improve it.
The understanding a logical robot needs then requires it to use appropriate sentences about the matter being understood. The understanding involves both getting the sentences from observation and inference and using them appropriately to decide what to do.
Awareness is similar. It is a process of appropriate sentences about the world and its ownmental situation coming into the robot's consciousness, usually without intentional actions. Both understanding and awareness may be present to varying degrees in natural and artificiai systems. The swimmingrobot may understand some facts about swimmingand not others, and it may be aware of some aspects of its current swimmingstate and not others.
Formalized Self-Knowledge
Weassume a system in which a robot maintains its information about the world and itself primarily as a collection of sentences in a mathematical logical language. There will be other data structures where they is more compact or computationaily easier to process, but they will be used by programs whose results become stored as sentences. The robot decides what to do by logical reasoning, not only by deduction using rules of inference but also by nonmonotonic reasoning.
Wedo not attempt a full formalization of the rules that determine the effects of mental actions and other events in this paper. The main reason is that we are revising our theory of events to handle concurrent events
2Onecan understand aspects of a humanactivity better than the people whoare goodat doing it. Nadia Comenici's gymnastics coach wasa large, portly manhard to imagine cavorting on a gynmastics bar. Nevertheless, he understands women'sgymnastics well enough to have coached a world champion.

in a more modular way. There is something of this in the draft (McCarthy 1995a).
Robot consciousness involves including amongits sentences someabout the robot itself and about subsets of the collection of sentences itself, e.g. the sentences that were in consciousness just previous to the introspection, or at someprevious time, or the sentences about a particular subject, s
Wesay subsets in order to avoid self-reference as much as possible. References to the totality of the robot's beliefs can usually be replaced by references to the totality of its beliefs up to the present moment.
Mental Situation Calculus
The situation calculus, initiated in (McCarthyand Hayes 1969), is often used for describing howactions and other events affect the world. It is convenient to regard a robot's state of mind as a component of the situation and describe howmental events give rise to new situations. (Wecould use a formalism with a separate mental situation affected only by mental events, but this doesn't seem to be advantageous.) Wecontemplate a system in whichwhat holds is closed under deductive inference, but knowledgeis not.
The relevant notations are:
¯ holds(p, s) is the assertion that the proposition p holds in the situation s. Weshall mainly be interested in propositions p of a mental nature.
* Amongthe propositions that can hold are know p and believe p, where p again denotes a proposition. Thus we can have

holds(know p, s). * As wewill shortly see, sentences like

(1)

holds(knonwotknowp, s) (2)
are often useful. The sentence(2) asserts that the robot knowsit doesn't knowp.
¯ Besides knowledge of propositions we need a notation for knowledge of an individual concept, e.g. a telephone number. (McCarthy 1979b) treats this in some detail. That paper has separate names for objects
and concepts of objects and the argument of knowing is the latter. In that paper, the symbolmike denotes Mike himself, the function telephone takes a person into his telephone number. Thus telephone mike denotes Mike's telephone number. The symbol Mike is the concept of Mike, and the function Telephone takes a the concept of a person into the concept of his telephone number. Thus we distinguish between

S Toomuchwork concernedwith self-knowledge has considered self-referential sentencesand getting aroundtheir apparent paradoxes. This is mostly a distraction for AI, because humanself-consciousness and the self-consciousness we needto build into robots almostneverinvolvesself-referential sentencesor other self-referential linguistic constructions. A
simplereferenceto oneself is not a self-referential linguistic construction, becauseit isn't doneby a sentencethat refers
to itself.

Mike's telephone number, denoted by telephone mike and the concept of his telephone number denoted by Telephone Mike. This enables us to say

holds(knows Telephone Mike, s)

(3)

to assert knowledge of Mike's telephone number and

occurs(forget p, s) D holds(F not know p, (8)
In general, we shall wantto treat forgetting as a sideeffect of somemore complexevent. Suppose foo is the more complex event. We'll have

holds(know not knows Telephone Mike, s) (4)
to mean that the robot knowsit doesn't knowMike's telephone number. The notation is somewhat ponderous, but it avoids the unwanted inference that the robot knows Mary's telephone number from the facts that her telephone number is the same as Mike's and that the robot knows Mike's telephone number.4 Having the sentence (4) in consciousness might stimulate the robot to look in the phone book.
Mental Events, especially Actions
Mental events change the situation just as do physical events.
Here is a list of somemental events, mostly described informally.
¯ learn p. The robot learns the fact p. An obvious consequence is
holds(knpo, rwesult(leapr,ns)) (5)
provided the effects are definite enoughto justify the result formalism. More likely we'll want something like

occurs(foso),occurs(forgpe,ts) (9)
¯ The robot may decide to do action a. This has the property:
occurs(decide-to-do a, s) D holds(intend-to-do a, s ).
(10)
The distinction is that decide is an event, and we often don't need to reason about howlong it takes. intend-to-do is a fluent that persists until something changes it. Somecall these point fluents and continuous fluents respectively.
The robot may decide to assume p, e.g. for the sake of argument. The effect of this action is not exactly to believe p, but maybeit involves entering a context (see (McCarthy 1993)) in which p holds.
The robot mayinfer p from other sentences, either by deduction or by some nonmonotonicform of inference.
The robot may see some object. One result of seeing an object may be knowing that it saw the object. So we might have

occurs(learn p, s) D holds(F know s), (6)
where occurs(event, s) is a point fluent asserting that event occurs (instantaneously) in situation s. F p the proposition that the proposition p will be true at some time in the future. The temporal function F is used in conjunction with the function nezt and the axiom
holds(F p, s) D holds(p, nezt(p, s)). (7)
Here nezt(p, s) denotes the next situation following s in whichp holds. (7) asserts that if F p holds in s, then there is a next situation in which p holds. (This nezt is not the nezt of sometemporal logic formalism.) ¯ The robot learning p has an effect on the rest of its knowledge. We are not yet ready to propose one of the manybelief revision systems for this. Indeed we don't assumelogical closure. ¯ What about an event forget p? Forgetting p is definitely not an event with a definite result. What we can say is
4Someother formalismsgive up the law of substitution in logic in order to avoid this difficulty. Wefind the price of having separatetermsfor conceptsworth paying in order to retain all the resourcesof first order logic and even higher order logic whenneeded.

occurs(see o, s) D holds(F knowsdid see o, s). (11)
Formalizing other effects of seeing an object require a theory of seeing that is beyondthe scope of this article.
It should be obvious to the reader that we are far from having a comprehensivelist of the effects of mental events. However, I hope it is also apparent that the effects of a great variety of mental events on the mental part of a situation can be formalized. Moreover,it should be clear that useful robots will need to observe mental events and reason with facts about their effects.
Most work in logical AI has involve theories in which it can be shownthat a sequence of actions will achieve a goal. There are recent extensions to concurrent action, continuous action and strategies of action. All this work applies to mental actions as well.
Mostly outside this work is reasoning leading to the conclusion that a goal cannot be achieved. Similar reasoning is involved in showingthat actions are safe in the sense that a certain catastrophe cannot occur. Deriving both kinds of conclusion involves inductively inferring quantified propositions, e.g. "whatever I do the goal won't be achieved" or "whatever happens the catastrophe will be avoided." This is hard for today's automated reasoning techniques, but Reiter (199x) has madeimportant progress.

92

Inferring Non-knowledge
Let p be a proposition. The proposition that the robot does not know p will be written not know p, and we are interested in those mental situations s in which we have holds(not know p, s). If not is consistent with the robot's knowledge, then we certainly want holds(not knowp, s).
Howcan we assert that the proposition not p is consistent with the robot's knowledge? G~del's theorem tells us that we aren't going to do it by a formal proof using the robot's knowledge as axioms. 5 The most
perfunctory approach is for a program to try to prove holds(not p,s) from the robot's knowledge and fail. Logic programming with negation as failure does this for Horntheories.
However, we can often do better. If a person or a robot regards a certain collection of facts as all that are
relevant, it suffices to find a modelof these facts in which p is false. 6
Consider asserting ignorance of the value of a numerical parameter. The simplest thing is to say that there
are at least two values it could have, and therefore the robot doesn't knowwhat it is. However, we often want more, e.g. to assert that the robot knowsnothing of its value. Then we must assert that the parameter could have any value, i.e. for each possible value there are models of the relevant facts in which it has that value. Of course, complete ignorance of the values of two parameters requires that there be a model in which each pair of values is taken.
It is likely to be convenient in constructing these models to assumethat arithmetic is consistent, i.e. that there are models of arithmetic. Then the set of natural numbers, or equivalently Lisp S-expressions, can be used to construct the desired models. The larger the robot's collection of theories postulated to have models, the easier it will be to showignorance.
5Weassumethat our axiomsare strong enoughto do symbolic computationwhichrequires the samestrength as arithmetic. I think we won't get muchjoy from weakersystems.
6Aconviction of aboutwhatis relevant is responsible for a person's initial reaction to the well-knownpuzzle of the three activists and the bear. ThreeGreenpeaceactivists have just wona battle to protect the bears' prey, the bears being already protected. It washard work, and they decide to go see the bears whoserepresentatives they consider themselves to have been. They wanderabout with their cameras, each going his ownway.
Meanwhilea bear wakesup from a long sleep very hungry and heads South. After three miles, she comesacross one of the activists and eats him. Shethen goes three miles West, finds another activist and eats her. Three miles North he finds a third activist but is too full to eat. Howevera,nnoyed by the incessant blather, she kills the remainingactivist and drags him two miles East to her starting point for a nap, certain that she and her cubs can have a snack whenshe
wakes,
Whatcolor was the bear? Atfirst sight it seemsthat the color of the bear cannotbe determined from the information given. Whilewrongin this case, jumpingto such conclusions about whatis relevant is moreoften than not the correct thing to do.

Making a program that reasons about models of its knowledge looks difficult, although it mayturn out to be necessary in the long run. The notion of transcending a context maybe suitable for this.
For nowit seems more straightforward to use second order logic. The idea is to write the axiomsof the theory with predicate and function variables and to use existential statements to assert the existence of models. Here's a proposal.
Suppose the robot has some knowledge expressed as an axiomatic theory and it needs to infer that it cannot infer that President Clinton is sitting down. Weimmediately have a problem with G6ders incompleteness theorem, because if the theory is inconsistent, then every sentence is inferrable, and therefore a proof of non-inferrability of any sentence implies consistency. Weget around this by using another idea of G~del's--relative consistency/
In his (1940) G~del proved that if G~del-Bernays set theory is consistent, then it remains consistent whenthe axiom of choice and the continuum hypothesis are added to the axioms. He did this by supposing that set theory has a model, i.e. there is a domain and an E predicate satisfying GB.He then showedthat a subset of this domain, the constructible sets, provided a modelof set theory in which the axiom of choice and the continuum hypothesis are also true. Cohenproved that if set theory has any models it has models in which the axiom of choice and the continuum hypothesis are false. The G~del and Cohenproofs are long and difficult, and we don't want our robot to go through all that to showthat it doesn't knowthat President Clinton is sitting.
For example, suppose we have a first order theory with predicate symbols {P1,...,P,~,sits} and let
A(Pl,..., P,~, sits) be an axiomfor the theory. The second order sentence
(3P~,..., P~n sitst)A(P~, "'" , P~n, sitst) (12)
expressetsheconsistenocfythetheorya,ndthesentence
(3P~,. . . , P"sits')( A(P~,. . . , P', sits')^-~sits' ( Clinton, (13)
expresses the consistency of the theory with the added assertion that Clinton is not sitting in the situation 8.
Then
(12) D (13)
is then the required assertion of relative consistency. Sometimeswe will want to assert relative consistency
under fixed interpretations of someof the predicate symbols. This would be important when we have axioms involving these predicates but do not have formulas for them, e.g. of the form (Vz y)(P(z,y) - ...). Suppose, for example, that there are three predicate symbols (P1, ]>2, sits), and ]>1 has a fixed interpretation, and the other two are to be chosen so as to satisfy the axiom. Then the assertion of consistency with Clinton sitting takes the form
7 Our approachis a variant of that used by Kraus, Perlis
and Horty (1991).

93

sciousness that wouldnot be reached if certain addi-

(3P~P~)A(P1, P~, sits') A sits'(Clinton, (15)

tional ideas were also in consciousness, s 4. Humanemotions influence human thought by influ-

The straightforward wayof proving (15) is to find substitutions for the predicate variables P~ and sits' that make the matrix of (15) true. The most trivial ease this would be when the axiom A(P1, P~, sits) does not actually involve the predicate sits, and we already have an interpretation P1,..., P,,, sits in whichit is satisfied. Then we can define

encing what ideas come into consciousness. For example, anger brings into consciousness ideas about the target of anger and also about ways of attacking this target.
5. Humanemotions are strongly related to blood chemistry. Hormones and neurotransmitters belong to the same family of substances. The sight of something frightening puts certain substances in our blood streams, and these substances mayreduce the thresh-

sits' = ()~z ss)(-,(z Clinton Ass = s)V sit s(z, ss)
(16)

and (15) follows immediately. This just means that

6.

the newpredicate does not interact with what is already

known, then the values for which it is true can be as-

signed arbitrarily.

olds of synapses where the dendrites have receptors for these substances. 9
A design that uses environmental or internal stimuli to bring whole classes of ideas into consciousness is entirely appropriate for a lower animals. Weinherit this mechanismfrom our animal ancestors.

Observing its Motivations
Whatevermotivational structure we give to robots, they should be able to observe and reason about it. For many purposes a simple goal-subgoal structure is the right thing. However, there are some elaborations to consider.

According to these notions, paranoia, schizophrenia, . depression and other mental illnesses would involve
malfunctions of the chemical mechanisms that bring ideas into consciousness. A paranoid who believes the Mafia or the CIA is after him and acts accordingly can lose these ideas whenhe takes his medicine and regain them when he stops. Certainly his blood

1. There often will be auxiliary goals, e.g. curiosity. Whena robot is not otherwise occupied, we will want it to work at extending its knowledge.

chemistry cannot encode complicated paranoid theories, but they can bring ideas about threats from wherever or howeverthey are stored.

2. The obverse of an auxiliary goal is a constraint. Maybe These facts suggest the following design considerashall want somethinglike Asimov'sscience fiction laws tions.

of robotics, e.g. that a robot should not harmhumans. In a sufficiently general wayof looking at goals, achieving its other goals with the constraint of not harming humansis just an elaboration of the goal itself. However, since the same constraint will apply to the achievement of manygoals, it is likely to be convenient to formalize them as a separate structure. A constraint can be used to reduce the space of achievable states before the details of the goals are considered.
Robots Should Not be Equipped with Human-like Emotions
Some authors, e.g. Sloman and Croucher (1981), have argued that sufficiently intelligent robots wouldautomatically have emotions somewhatlike those of humans. Weargue that it is possible to give robots human-like emotions, but it would require a special effort. Moreover, it would be a bad idea if we want to use them as servants. In order to makethis argument, it is necessary to assume something, as little as possible, about human emotions. Here are somepoints.
1. Humanreasoning operates primarily on the collection of ideas of which the person is immediatelyconscious.
2. Other ideas are in the background and come into consciousness by various processes.
3. Because reasoning is so often nonmonotonie, conclusions can be reached on the basis of the ideas in con-

1. Wedon't want robots to bring ideas into consciousness in an uncontrolled way. Robots that are to react against people (say) considered harmful, should include such reactions in their goal structures and prioritize them together with other goals. Indeed we humans advise ourselves to react rationally to danger, insult and injury. "Panic" is our namefor reacting directly to perceptions of danger rather than rationally.
2. Putting such a mechanismin a robot is certainly feasible. It could be done by maintaining somenumerical variables, e.g. level of fear, in the system and making the mechanismthat brings sentences into conscious-
ness (short term memory)depend on these variables. However, human-like emotional structures are not an automatic byproduct of human-level intelligence.
3. It is also practically important to avoid makingrobots that are reasonable targets for either humansympathy or dislike. If robots are visibly sad, bored or angry, humans, starting with children, will react to them as
8Theseconclusionsare true in the simplest or moststandard or otherwise minimalmodelsof the ideas taken in consciousness. The point about nonmonotonicityis absolutely critical to understandingthese ideas about emotion.See, for example, (McCarthy 1980) and (McCarthy 1986)
9Admittedly referring to "reducing the thresholds of synapses"is speculative. Howeveri,t maybe possible to test these ideas experimentally. There should be a fixed set of these substances and therefore definite classes of ideas that they bring in.

persons. Then they would very likely come to occupy some status in humansociety. Humansociety is complicated enough already.
Remarks

7. The syntactic form is simple enough. If p is a proposition, then hope p is the proposition that the robot hopes for p to becometrue. In mental situation calculus we would write

1. Wedo not give a definition of consciousness or self consciousness in this article. Weonly give someproperties of the consciousness phenomenonthat we want
robots to have together with someideas of howto program robots accordingly.
2. The preceding sections are not to be taken as a theory of human consciousness. We do not claim that the humanbrain uses sentences as its primary way of representing information. Allen Newel] (1980) introduced the term logic level of analysis of a person or machine. The idea is that behavior can be understood as the person, animal or machine doing what it believes will achieve its goals. Ascribing beliefs and goals then accounts for muchof its behavior. Daniel Dennett 1978 first introduced this idea, and it is also discussed in (McCarthy 1979a).
Of course, logical AI involves using actual sentences in the memoryof the machine.
3. Daniel Dennett (1991) argues that humanconsciousness is not a single place in the brain with every conscious idea appearing there. I think he is right about the humanbrain, but I think a unitary consciousness will work quite well for robots. It wouldlikely also work for humans, but evolution happens to have pro-
duced a brain with distributed consciousness.
4. Francis Crick (1994) discusses howto find neurological correlates of consciousness in the humanand animal brain. I agree with all the philosophy in his paper and wish success to him and others using neuroscience. However,after reading his book, I think the artificial intelligence approach has a good chance of achieving important results sooner. They won't be quite the
same results, however.
5. What about the unconscious? Do we need it for robots? Very likely we will need some intermediate computational processes whose results are not appropriately included in the set of sentences we take as the consciousness of the robot. However, they should be observable whenthis is useful, i.e. sentences giving facts about these processes and their results should appear in consciousness as a result of mental actions aimed at observing them. There is no need for a full-
fiedged Freudian unconscious with purposes of its own.
6. Should a robot hope? In what sense might it hope? Howclose would this be to human hope? It seems that the answeris yes. If it hopes for various things, and enough of the hopes come true, then the robot can conclude that it is doing well, and its higher level strategy is ok. If its hopes are always disappointed, then it needs to changeits higher level strategy.
To use hopes in this wayrequires the self observation to remember what it hoped for.
Sometimesa robot must also infer that other robots or people hope or did hope for certain things.

holds(hope p, s)

(17)

to assert that in mental situation s, the robot hopes for p.
Humanhopes have certain qualities that I can't decide whether we will want. Hope automatically brings into consciousness thoughts related to what a situation realizing the hope would be like. Wecould design our programs to do the same, but this is more automatic in the human case than might be optimal. Wishful thinking is a well-known humanmalfunction.
8. Arobot should be able to wish that it had acted differently from the way it has done. A mental example is that the robot mayhave taken too long to solve a problem and might wish that it had thought of the solution immediately. This will cause it to think about howit might solve such problems in the future with less computation.
9. A human can wish that his motivations and goals were different from what he observes them to be. It would seem that a program with such a wish could just changeits goals.
10. Penrose (1994) emphasizes that a humanusing a logical system is prepared to accept the proposition that the system is consistent even though it can't be inferred within the system. The humanis prepared to iterate this self-confidence indefinitely. Our systems should do the same, perhaps using formalized transcendence. Programs with human capability in this respect will have to be able to regard logical systems as values of variables and infer general statements about
them. Wewill elaborate elsewhere (McCarthy 1995b) our disagreement with Penrose about whether the humanis necessarily superior to a computer program in these respects. For nowwe remark only that it would be interesting if he and others of similar opinion would say wherethey believe the efforts outlined in this article will get stuck.
11. Penrose also argues (1994)(p. 37 et seq.) that humans have understanding and awareness and machines cannot have them. He defines them in his own way, but
our usage is close enoughto his so that I think we are discussing how to make programs do what he thinks they cannot do. I don't agree with those defenders of AI who claim that some computer programs already possess understanding and awareness to the necessary extent.
12. Programs that represent information by sentences but generate new sentences by processes that don't correspond to logical reasoning present similar problems to logical AI for introspection. Approaches to AI that don't use sentences at all need someother wayof representing the results of introspection if they are to use it at all.

95

Acknowledgements
This work was partly supported by ARPA(ONR) grant N00014-94-1-0775and partly done while the author was Meyerhoff Visiting Professor at the WeizmannInstitute of Science, Rehovot,Israel.
Thanks to Yoav Shoham and Aaron Sloman for email commentsand to Sa~a Buva~ and TomCosteno for faceto-face comments.
References
This document is available via the URL:http: http://wwwformal.stanford.edu/jmc/home.html.
Crick, Francis (1994). The Astonishing Hypothesis: The Scientific Search for Soul, Scribners
Dennett, Daniel (1978). Brainstorms: Philosophic Essays on Mind and Psychology, Bradford Books.
Dennett, Daniel (1991). Consciousness Explained, (Boston : Little, Brownand Co.)
G~del, Kurt (1940). The Consistency of the Continuum Hypothesis. Annals of Mathematics Studies. Princeton University Press.
Kraus, Sarit, Donald Perlis and John Horty (1991). "Reasoning about Ignorance: A Note on the BushGorbachev Problem". Fundamenta Informatica, XV, 325-332.
McCarthy, John and Patrick J. Hayes (1969). "Some Philosophical Problems from the Standpoint of Artificial Intelligence", in D. Michie (ed), MachineIntelligence 4, American Elsevier, NewYork, NY, 1969. Reprinted in (McCarthy 1990).
McCarthy, John, 1979a. "Ascribing Mental Qualities to Machines" in Philosophical Perspectives in Artificial Intelligence, Ringle, Martin (ed.), Harvester Press, July 1979. Reprinted in (McCarthy 1990).
McCarthy, John (1979b). "First Order Theories of Individual Concepts and Propositions", in Michie, Donaid (ed.) Machine Intelligence 9, (University of Edinburgh Press, Edinburgh). Reprinted in (McCarthy 1990).
McCarthy, John (1980). "Circumscription - A Form of Non-MonotonicReasoning", Artificial Intelligence, Volume 13, Numbers 1,2, April. Reprinted in (McCarthy 1990).
McCarthy, John (1986). "Applications of Circumscription to Formalising CommoSnense Knowledge"Artificial Intelligence, April 1986. Reprinted in (McCarthy 1990).
McCarthy, John (1989). "Artificial Intelligence and Logic" in Thomason, Richmond (ed.) Philosophical Logic and Artificial Intelligence (Dordrecht ; Kluwer Academic)
McCarthy, John (1990). Formalizing CommonSense, Ablex, Norwood, NewJersey, 1990.
McCarthy, John (1993). "Notes on Formalizing Context" IJCAI-93.

Penrose, Roger (1994). Shadows of the Mind: A Search for the Missing Science of Consciousness. Oxford University Press.
McCarthy, John (1995a). "Situation Calculus with Concurrent Events and Narrative", in preparation. If you
want to take a look, a draft can be seen via the URL: http://www-formal.stanford.edu/jmc/home.html.
McCarthy, John (1995b). Forthcoming review of (Penrose 1994).
Sloman, Aaron (1985). 'What enables a machine understand?' in Proceedings 9th International Joint
Conference on AI, pp 995-1001, Los Angeles.
Sloman, Aaron and Monica Croucher (1981). "Why Robots will have Emotions". Proceedings 7th LICAI, 1981.

