In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 

              AFFECTIVE COMPUTING FOR GAME DESIGN 

Eva Hudlicka 

Psychometrix Associates, Inc. 

1805 Azalea Drive 

Blacksburg, VA, 24060 US 
E-mail: hudlicka@ieee.org 

 
 
 

                      KEYWORDS 

Affective Computing, Affect-Focused Game Design, 

                     Affective Gaming, Serious Games 

 

ABSTRACT 
 
Affective  gaming  has  received  much  attention  lately,  as  the 
gaming  community  recognizes  the  importance  of  emotion  in 
the development of engaging games.  Affect plays a key role in 
the  user  experience,  both  in  entertainment  and  in  ‘serious’ 
games.  Current focus in affective gaming is primarily on the 
sensing  and  recognition  of  the  players’  emotions,  and  on 
tailoring the game responses to these emotions.  A significant 
effort is also being devoted to generating ‘affective behaviors’ 
in  the  game  characters,  and  in  player  avatars,  to  enhance 
their  realism  and  believability.  Less  emphasis  is  placed  on 
modeling emotions, both their generation and their effects, in 
the  game  characters,  and  in  user  models  representing  the 
players.  This paper discusses how the emerging discipline of 
affective computing contributes to each of these three elements 
of affective game design, with emphasis on the importance of 
affective  modeling.  The  paper  provides  a  summary  of  a 
conference tutorial whose aim is to enable game designers to 
make informed choices about where to incorporate emotion in 
games,  and  provide  information  about  existing  data  and 
theories from the affective sciences, and relevant methods and 
techniques from affective computing, to support affect-focused 
game design. 

1.0 INTRODUCTION 
Affective  gaming  has  received  much  attention  lately,  as  the 
gaming  community  recognizes  the  importance  of  emotion  in 
the development of more engaging games (Becker et al. 2005; 
Gilleade et al. 2005; Sykes, 2004). Emotion plays a key role in 
the  user  experience,  both  in  entertainment,  and  in  ‘serious’ 
games developed for education,  training,  assessment, therapy 
or  rehabilitation.  Current  focus  in  affective  gaming 
is 
primarily  on  the  sensing  and  recognition  of  the  players’ 
emotions,  and  on  tailoring  the  game  responses  to  these 
emotions;  e.g.,  minimizing  frustration,  ensuring  appropriate 
challenge (Gilleade and Dix 2004; Sykes and Brown 2003).  A 
significant effort is also being devoted to generating ‘affective 
behaviors’  in  the  game  characters,  to  enhance  their  realism 
and  believability.  Less  emphasis  is  placed  on  modeling 

emotions, both their generation and their effects, in the game 
characters  themselves,  and  in  user  models  representing  the 
players.  
      This  paper  discusses  each  of  these  elements  of  affective 
gaming, and outlines how the emerging discipline of affective 
computing (Picard 1997) can contribute to integrating emotion 
in  game  design.  The  three  core  areas  of  affective  computing 
provide methods and techniques directly relevant to  affective 
game design: emotion sensing and recognition; computational 
models of emotion; and emotion expression by synthetic agents 
and robots. This paper emphasizes the importance of affective 
modeling  in  particular,  both  as  a  basis  for  more  realistic 
behavior  of  game  characters,  but  also  as  a  means  of 
developing more realistic and complete models of the players, 
to enable real-time affect-adaptive gameplay, and to enable the 
game system to induce a wide range of desired emotions in the 
players. 
     The paper summarizes a conference tutorial that introduces 
these  three  core  areas  of  affective  computing,  and  highlights 
their relevance  to  the development of engaging and  effective 
games.  The  aim  of  the  tutorial  is  to  provide  sufficient 
information 
and 
techniques, and data and theories from the affective sciences, 
to  enable  game  designers  to  make  informed  choices  about 
where and how to incorporate emotion in games.  

computing  methods 

affective 

about 

2.0 THE ROLES OF EMOTIONS IN GAMING 
   Emotions  are  critical  in  game  design.  One  only  has  to 
eavesdrop on a group of kids huddled over a Nintendo DS to 
hear  “AWWW!  I  GOT  KILLED”  or  “YES!!!!  I  GOT 
ANOTHER  LIFE”  to  get  a  sense  of  the  internal  affective 
drama  engendered  by  gaming.    Players  become  frustrated 
when  the  game  does  not  go  well,  pleased  with  themselves 
when they “beat  a  level”, or  may  turn  away  in disgust when 
they encounter a seemingly insoluble problem.  
   Players’ emotions can be  triggered by the gameplay  events 
(e.g., finding a treasure), by behavior of a game character, or 
as a result of interaction with the game (e.g., frustration when 
the game is  too difficult).   Emotions  can be  conveyed  to  the 
player  by  the  game  character  behavior,  and  by  the  look-and-
feel of the game environment; e.g., contrast the intense, high-
arousal  graphics  of  DOOM,  the  mysterious  and  foreboding 
environment of Myst, and the lighthearted Mario games. 
   The degree of explicit focus on players’ emotions in gaming 
varies. Players’ emotions may be a “side effect” of the game, 

for 

   Regarding the affect-adaptive gameplay, the designer needs 
to  be  clear  about  the  following:  What  role  do  the  player’s 
emotions play in the overall gameplay (e.g., side effect of the 
game  vs.  central  focus  in  therapeutic  games)?  Which  player 
emotions  or  moods  need  to  be  recognized  and  which 
modalities  and  signals  are  most  appropriate 
their 
recognition  (e.g.,  physiological  signals,  facial  expressions, 
player  behavior  within 
the  player’s 
personality  need  to  be  assessed?  Which  elements  of  the 
gameplay should be adapted (e.g., narrative and plot changes, 
game  character  behavior,  game  tasks)?  What  information 
about  the  player’s  affective  makeup  is  necessary  to  enable 
these adaptations? 
   The remainder of  this paper,  and the associated  conference 
tutorial,  discuss  how  the  emerging  discipline  of  affective 
computing,  and  existing  research  in  the  affective  sciences 
(psychology and neuroscience), help provide answers to these 
questions, and thereby support affect-focused game design. 

the  game)?  Does 

and 

to  accommodate 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
with  not  much  conscious  thought  given  to  emotion  during 
design:  as  long  as  the  game  is  more  ‘fun’  than  ‘frustrating’, 
the players remain engaged and their emotions can be ignored. 
The players’ emotions can also function as a means-to-an-end, 
to  control  the  players’  engagement  within  the  game.  This 
requires  more  systematic  attention  to  the  players’  affective 
reactions.  This  can  be  achieved  through  an  “open-loop” 
approach, one that does not require the game system to sense 
the player’s emotions; e.g., through carefully structured levels, 
plot  lines  and  sequences  of  increasingly  difficult  actions 
required  to achieve  the ultimate game goal, or through game 
character  behavior  such  as  taunting  or  encouragement.  In 
contrast to this, the player’s emotions can be incorporated into 
a game in a “closed-loop” manner, where they are sensed and 
recognized by the game system, and some aspect of the game 
is  modified  as  a  function  of  the  player’s  state:  the  game  is 
made  less  challenging  if  the  player  becomes  frustrated  and 
more  challenging  if  s/he  becomes  bored,  the  behavior  of  the 
game  characters  changes 
the  player’s 
affective state, or the game situation is changed to adapt to the 
player’s emotion (e.g., a shift to a less stressful ‘place’ within 
the  game).  Here  player’s  emotion  is  a  key  factor,  actively 
manipulated  to  ensure  engagement.  This  type  of  dynamic 
affective  adaptation  (affective  feedback  (Bersak  et  al.  2001)) 
is the focus of current affective gaming efforts (Becker et al. 
2005;  Gilleade  et  al.  2005).  Finally,  affective  games  can  be 
applied  in  therapeutic  contexts,  where  the  player’s  emotions 
are  the  central focus of the game; e.g., the achievement of a 
particular  emotional  state  (e.g.,  happiness,  pride)  or  the 
reduction  of  some  undesirable  state  (e.g.,  fear,  anger).  Here 
the recognition of the players’ emotions is essential to support 
the selection of appropriate gameplay, either affect-adaptive or 
affect-inducing. 
   Affect-inducing  elements  can  be  incorporated  into  multiple 
aspects of the game, including the look-and-feel and dynamics 
of  the  game  environment,  temporal  and  resource  constraints 
on player behavior (e.g., requirements to complete  a difficult 
task  within  a  short  timeframe  designed  to  induce  stress), 
choice of game tasks or situations provided to the player (e.g., 
easier  tasks  to  build  confidence,  difficult  task  to  challenge) 
and their integration within the overall plot or game narrative, 
as well as the appearance and behavior of the game characters 
or the players’ avatars.  
    A range of issues must be addressed by the game designer.  
In game character development, the game designer should be 
clear  about  the  following:  What  emotions,  moods  and 
personality  traits  should  they  express,  when,  and  how?  Are 
deep models of emotion necessary? Do the characters need to 
affectively  respond  to  all  situations  or  can  their  affective 
behavior  be  scripted  to  respond  to  selected  game  and  user 
events? How realistic do the affective expressions need to  be 
to make the game  characters believable and maintain player 
engagement?  Which  expressive  modalities  should  be  used 
(e.g.,  speech  tone  and  content,  behavior  selection,  gestures, 
facial expressions)? Should the game characters’ behavior be 
directed  to  the  player,  other  game  characters  or  the  game 
environment in general? 

and 

of 

computational 

affective  modeling, 

that  most  definitions 
affective 

3.0 WHAT DO WE KNOW ABOUT EMOTIONS? 
   Emotion research in the affective  sciences over  the past  20 
years  has  produced  data,  conceptual  and  computational 
models, and methods and techniques that are directly relevant 
to  affective  computing  and  affective  human-computer 
interaction, and to affective game development.  The emerging 
findings  inform  sensing  and  recognition  of  user  emotion  by 
machine, 
the 
generation  of  expressive  affective  behaviors  in  synthetic 
agents  and  robots.  This  section  summarizes  some  of  the  key 
findings relevant for affective game design. 
 
 Definitions 
 
When searching for a definition of emotions, it is interesting to 
note 
involve  descriptions  of 
characteristics 
fast, 
processing 
undifferentiated  processing),  or  roles  and  functions  of 
emotions. The latter are usefully divided into those involved in 
interpersonal,  social  behavior  (e.g.,  communication  of  intent, 
coordination  of  group  behavior,  attachment),  and  those 
involved  in  intrapsychic  regulation,  adaptive  behavior,  and 
motivation (e.g., homeostasis, goal management, coordination 
of  multiple  systems  necessary  for  action,  fast  selection  of 
appropriate adaptive behaviors). The fact that emotions are so 
often defined in terms of their roles, rather than their essential 
nature,  underscores  our  lack  of  understanding  of  these 
complex  phenomena.  Nevertheless,  emotion  researchers  do 
agree  on  a  high-level  definition  of  emotions,  as 
the  
“evaluative  judgments of  the  environment, the self and other 
social agents, in light of the agent’s goals and beliefs” and the 
associated  distinct  modes  of  neural  functioning  reflected 
across  multiple  modalities  (e.g.,  cognitive,  physiological, 
behavioral) 
and 
behavioral subsystems to achieve the agent’s goals.  
 
 

coordinating  multiple 

cognitive 

(e.g., 

is 

the  somatic 

the  basic  emotions; 

that 

is, 

likelihood,  goal 

/  subjective  modality: 

/  physiological  modality  - 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
 
 
Multiple Modalities 
 
A  key  characteristic  of  emotions  is  their  multi-modal  nature, 
which has direct implications for both sensing and recognition 
of player  emotion,  and behavioral  expression of emotions  by 
game characters. In biological agents, emotions are manifested 
across  four  interacting  modalities.  The  most  visible  is  the 
behavioral  /  expressive  modality;  e.g.,  facial  expressions, 
speech,  gestures,  posture,  and  behavioral  choices.  Closely 
related 
the 
neurophysiological substrate making behavior (and cognition) 
possible (e.g., changes in the neuroendocrine systems and their 
manifestations,  such  as  blood  pressure  and  heart  rate).  The 
cognitive  /  interpretive  modality  is  most  directly  associated 
with  the  evaluation-based  definition  of  emotions  above,  and 
emphasized  in  the  current  cognitive  appraisal  theories  of 
emotion  generation,  discussed  below.    Finally,  there  is  the 
experiential 
the  conscious,  and 
inherently  idiosyncratic,  experience  of  emotions  within  the 
individual.   
   Understanding  the  ‘signatures’  of  specific  emotions  across 
these  multiple  modalities  provides  guidance  for  sensing  and 
recognition  of  player  emotions,  and  for  the  generation  of 
affective  behavior  in  agents,  as  will  be  discussed  below  in 
section 4. 
 
Affective Factors 
 
The term ‘emotion’ can often be used rather loosely, to denote 
a  wide  variety  of  affective  factors,  each  with  different 
implications  for  sensing  and  recognition,  modeling  and 
expression.    Emotions  proper  represent  short  states  (lasting 
seconds 
reflecting  a  particular  affective 
assessment  of  the  state  of  self  or  the  world,  and  associated 
behavioral  tendencies  and  cognitive  biases.  Emotions  can  be 
further  differentiated  into  basic  and  complex,  based  on  their 
cognitive complexity, the universality of triggering stimuli and 
behavioral manifestations, and the degree to which an explicit 
representation  of  the  agent’s  ‘self’  is  required  (Ekman  and 
Davidson  1994;  Lewis  1993).  The  set  of  basic  emotions 
typically  includes  fear,  anger,  joy,  sadness,  disgust,  and 
surprise.  Complex  emotions  such  as  guilt,  pride,  and  shame 
have  a  much  larger  cognitive  component  and  associated 
idiosyncracies  in  both  their  triggering  elicitors  and  their 
behavioral  manifestations,  which  makes  both  their  detection 
and  their  expression  more  challenging.  Moods  reflect  less-
focused  and  longer  lasting  states  (hours  to  days  to  months).  
Finally,  affective  personality  traits  represent  more  or  less 
(e.g.,  extraversion  vs. 
permanent  affective 
introversion, 
affective 
emotionality).   
 
Emotion Generation and Emotion Effects 
 

While  multiple  modalities  play  a  role  in  emotion  generation 
(Izard  1993),  most  existing  theories  (and  computational 
models) emphasize the role of  cognition, both conscious and 
unconscious,  in  emotion  generation,  termed  the  ‘cognitive 
appraisal’ theories of emotion (Roseman and Smith 2001). A 
key component of most appraisal theories is a set of domain-
independent appraisal dimensions which capture aspects of the 
stimuli and the assessed situation the agent is facing, such as 
novelty,  urgency, 
relevance  and  goal 
congruence, responsible agent and the agent’s ability  to cope 
(Ellsworth  and  Scherer  2003;  Smith  and  Kirby  2000).  This 
approach  to  appraisal,  also  termed  componential  model  of 
emotions,  provides  an  elegant  conceptualization  of 
the 
generation  process  and  facilitates  modeling.  If  the  values  of 
the  dimensions  can  be  determined,  the  resulting  vector  of 
‘appraisal  dimensions’  can  readily  be  mapped  onto  the 
emotion  space  defined  by  these  dimensions,  which  in  turn 
provides a highly-differentiated set of possible emotions. 
   Less understood are the processes that mediate the effects of 
the 
triggered  emotions.  The  manifestations  of  specific 
emotions  in  behavior  are  certainly  well  documented,  at  least 
for 
the  associated  facial 
expressions,  gestures,  posture,  nature  of  movement,  speech 
content and tone characteristics. Some effects on cognition are 
also known; e.g., fear reduces attentional capacity  and biases 
attention  toward  threat  detection  (Isen  1993;  Mineka  et  al. 
2003)).  However,  the  mechanisms  mediating  these  observed 
effects  have  not  yet  been  identified.  The  interactions  among 
multiple  modalities  make  this  a  particularly  challenging 
problem.  
     Three  broad  categories  of  theories  postulate  specific 
mechanisms mediating emotion effects.  Spreading activation 
models,  such as   Bower’s “network theory of affect” (Bower 
1992;  Derryberry  1988),  were  developed  to  explain  the 
phenomenon  of  mood-congruent  recall.  These  conceptual 
models suggests that emotions can be represented as nodes in 
a network that contains both emotions and cognitive schemas. 
When  an  emotion  is  activated,  it  co-activates  (via  spreading 
activation)  schemas  with  similar  affective 
  The 
componential  theory  suggests  that  the  domain-independent 
appraisal  dimensions  that  mediate  emotion  generation  map 
directly  onto  specific  elements  of  affective  expressions,  such 
as  the  facial  musculature;  e.g.,  novelty  correlates  with 
eyebrow  raising,  pleasantness  with  raising  of  lip  corners  and 
eye lids (Scherer and  Ellgring 2007), and possibly  even onto 
emotion effects on cognition (Lerner and Tiedens, 2006). The 
parameter-based  models,  proposed 
independently  by  a 
number  of  researchers  (e.g.,  Hudlicka  1998;  Matthews  and 
Harley 1993; Ortony et al. 2005; Ritter and Avramides, 2000), 
suggest  that  affective  factors  act  as  parameters  inducing 
patterns  of  variations  in  cognitive  processes.  The  parameter-
based  models  appear  consistent  with  recent  neuroscience 
theories,  suggesting  that  emotion  effects  on  cognition  are 
implemented 
of 
neuromodulatory transmitters that act systemically on multiple 
brain structures (Fellous 2004).  
 

aggressiveness, 

to  minutes), 

tendencies 

in 

the 

brain 

via 

global 

effects 

tone. 

positive 

vs. 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
 
 
 
 
 
4.0 EMOTION SENSING, RECOGNITION AND 
EXPRESSION: EMOTION SIGNATURES ACROSS 
MULTIPLE MODALITIES AND TIME 
 
The multi-modal nature of emotions, and their evolution over 
time, both facilitate  and constrain recognition of  emotions  in 
players,  and  generation  of  expressive  affective  behavior  in 
game  characters.    Many  emotions  have  characteristic  multi-
feature,  multi-modal  ‘signatures’  that  serve  as  basis  for  both 
recognition  and  expression;  e.g.,  fear  is  characterized  by 
raising  of  the  eyebrows  (facial  expression),  fast  tempo  and 
higher  pitch  (speech),  threat  bias  attention  and  perception 
(cognition), a range of physiological responses mobilizing the 
energy required for fast reactions, and of course characteristic 
behavior  (flee  vs.  freeze).  Identifying  such  unique  emotion 
signatures  is  a  key  challenge  in  emotion  recognition  by 
machines.  Once  identified,  the  constituent  features  guide  the 
selection  of  appropriate  (non-intrusive)  sensors,  and  the 
algorithms  required  for  the  associated  signal  processing  to 
map  the  raw  data  onto  a  recognized  emotion.  For  example, 
frustration  can  be  identified  with  a  high  degree  of  accuracy 
(~80%) by combining facial expression analysis, posture, skin 
conductance and mouse pressure data (Kapoor et al. 2008). 
    The  multiple  modalities  thus  facilitate  recognition  by 
providing multiple “channels” of information, and options for 
the  selection  of  the  best  channel  for  a  particular  application. 
Affective  gaming  presents  a  unique  set  of  constraints  on 
recognition, by requiring non-intrusive sensors and precluding 
methods  that  require  fixed  player  positions.    For  example, 
sensors that detect arousal, a key component of emotions, such 
as finger-tip caps to detect galvanic skin response or heart-rate 
monitors,  are  not  optimal  for  gaming,  nor  are  facial 
recognition systems that require the player to remain in a fixed 
position.  Instead,  emotion  recognition  in  gaming  emphasizes 
sensors  that  can  be  readily  incorporated  into  existing  game 
controls; e.g., gamepad pressure to detect arousal (Sykes and 
Brown  2003).  Products  are  also  emerging  that  offer  helmet-
embedded  sensors  combining  multiple  channels  (EEG,  facial 
electromyogram,  blink  rate,  heart  rate,  head  motion  and  skin 
temperature) to recognize game-relevant player states, such as 
engagement  vs.  boredom  (e.g.,  http://www.emotiv.com/, 
http://emsense.com/).  The  advent  of  movement-oriented 
controls, such as those in the Wii, promises to provide a rich 
set  of  affective  sensors  based  on  movement  quality  and 
haptics.  
   The  identification  of  the  most  diagnostic  emotion  features 
also  guides  the  selection  of  best  expressive  ‘channels’  to 
convey a particular emotion to the player via game  character 
behavior.  In  expression  however,  multiple  modalities  also 
present  a  challenge,  by 
that  expression  be 
coordinated  and  synchronized  across  multiple  channels  to 
ensure  character  realism.  For  example,  expression  of  anger 

must  involve  consistent  signals  in  speech,  movement  and 
gesture  quality,  facial  expression,  body  posture  and  specific 
action  selection.    However,  for  a  given  game  character  or 
situation,  all  of  these  channels  may  not  be  required;  e.g., 
“cartoonish”  characters  may  be  able  to  express  many  basic 
emotions  (joy,  anger,  sadness)  with  minimal  changes  in 
expression,  movement  and  behavior.    However,  as  games 
mature and proliferate  into  more ‘serious’  applications,  these 
coordination requirements will become more stringent.  
   The  temporal  dimension  of  emotions  facilitates  recognition 
and  presents  challenges  for  expression.  Temporal  affective 
data  increase  recognition  accuracy.  In  some  channels  (e.g., 
facial expressions), recognition is much higher for video clips 
than  for  still  photographs.  In  many  modalities,  the  temporal 
dimension is an essential component (e.g., speech, movement, 
behavior monitoring, but also physiological data).  
    In  affective  expression,  the  temporal  dimension  presents  a 
challenge by requiring realistic evolution of the affective state, 
and  transitions  among  states.    This  requires  data  regarding 
how the affective dynamics are reflected in  changes  in facial 
expressions,  speech  and  movement,  as  the  emotion  intensity 
ramps  up  and  decays.    Particularly  challenging  are  the 
depictions of mixed affective states (e.g., sadness and joy, fear 
and anger) and transitions among states, which may need to be 
gradual for some situations but dramatic for others.  For some 
modalities,  these  dynamics  are  well-documented  (e.g.,  the 
facial  action  units  vocabulary  of  facial  expressions  (Ekman 
and  Friesen  1978)  that  define  the  onset  and  offset  patterns 
(Cohn  et  al.  2005)),  but  in  general,  these  dynamics  are 
determined empirically and require significant tuning. 
   The sensing and recognition of emotions, and the expression 
of  their  myriad  of  manifestations  in  game  characters,  thus 
require fundamental knowledge of emotions and their unique 
multi-modal  signatures,    selection  and  integration  of  sensors 
satisfying the desired constraints (e.g., degree of intrusiveness 
allowed,  cost  and  ease  of  use,  data  quality,  post-processing 
requirements  of  the  raw  data),  selection  or  development  of 
algorithms for data enhancement and filtering, and for pattern 
recognition and classification.   Given the idiosyncratic nature 
of  affective  expression,  the  use  of  player  baseline  data  is 
essential,  and 
the 
recognition algorithms is required to achieve the desired level 
of accuracy.  
   A  key  element  in  this  process  is  the  identification  of  the 
semantic  primitives 
for  each  sensed  channel,  and  a 
development of an associated vocabulary of primitives, whose 
distinct  configurations  can  then  characterize  the  different 
emotions  (Hudlicka,  2005).      Examples  of  such  semantic 
primitives  are  the  facial  action  units  comprising  the  Facial 
Action  Coding  System  developed  by  Ekman  and  Friesen 
(1978), the ‘basic posture units’ identified by Mota and Picard 
(2004) and used to  identify boredom  and engagement during 
training,  and  patterns  of  pitch  and  tonal  variations  in  speech 
used to identify basic emotions (Petrushin 2000).  
   The  conference 
emotion 
signatures,  and  associated  methods  and  approaches  for 
recognition and expression, in more detail.  

tutorial  discusses 

typically  user-specific 

training  of 

requiring 

specific 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
5.0 COMPUTATIONAL AFFECTIVE MODELING AND 
AFFECTIVE USER MODELS 
 
The  past  15  years  have  witnessed  a  rapid  growth  in 
computational  models of emotion and affective architectures. 
Researchers  in  cognitive  science,  artificial  intelligence  and 
human  computer  interaction  (HCI)  are  developing  models  of 
emotion  for  theoretical  research  regarding  the  nature  of 
emotion, as well as a range of applied purposes: to create more 
believable and effective synthetic characters and robots, and to 
enhance HCI (Becker et al. 2005; Breazeal 2005; Kapoor et al. 
2008).    Computational  models  of  emotion  are  relevant  for 
game  development  from  two  distinct  perspectives.  First, 
affective computational models enable the game characters to 
dynamically  generate  appropriate  affective  behavior  in  real 
time,  in response to evolving situations within the game, and 
to player behavior.  Such adaptive character behavior is more 
believable  than  ‘scripted’  behavior,  and  the  resulting  realism 
contributes  to  an  increased  sense  of  engagement.    These 
to  consistently  and 
models  also  enable 
realistically  portray  specific  emotions  when 
the  game 
objective is to induce a particular emotion in the player, as is 
the  case  in  psychotherapeutic  games.  Second,  computational 
affective  modeling  methods  can  also  be  used  to  create 
affective  models  of  the  players;  that  is,  user  models  that 
explicitly  include  information  about  the  player’s  affective 
makeup.  This  includes  information  such  as  what  emotional 
states  a  player  is  likely  to  experience,  information  about  the 
behavioral  indicators  associated  with  different  emotions  that 
can  aid  in  their  recognition  by  the  game  system,  and  what 
game situations are likely to induce a particular emotion.  Both 
of these uses of computational affective modeling are briefly 
described below, and elaborated in the conference tutorial. 

models’,  where  explicit  representations  of  some  of  the 
affective mechanisms are modeled, allowing a greater degree 
of generality.  
   In  an  effort  to  establish  more  systematic  guidelines  for 
affective  model  development,  and  to  facilitate  analysis  of 
existing models, Hudlicka has recently suggested dividing the 
modeling  processes 
those  responsible  for  emotion 
generation,  and  those  responsible  for  implementing  emotion 
effects  across  the  multiple  modalities  (Hudlicka  2008a; 
2008b).  Each of these broad categories of processes are then 
further  divided  into  their  underlying  primitive  computational 
tasks.  For  emotion  generation,  these  include  defining  the 
stimulus-to-emotion  mapping;  specifying  the  nature  of  the 
emotion dynamics, that is, the functions defining the emotion 
intensity calculation, as well as the ramp-up and decay of the 
emotion intensity over time; methods for combining multiple 
emotions,  necessary  for  combining  existing  emotions  with 
newly  derived  emotions,  and 
the  most 
appropriate  emotion  when  multiple  emotions  are  generated.  
For emotion effects, these tasks include defining the emotion-
to-behavior  and  emotion-to-cognitive  process  mappings; 
determining  the  magnitude  of  the  associated  effects  on  each 
affected process, as well as the dynamics of these effects; and 
the  integration  of  the  effects  of  multiple  emotions,  both  in 
cases where a residual effect of a prior emotion is still in force, 
and 
in  cases  where  multiple  emotions  are  generated 
simultaneously and their effects on cognition an behavior must 
be integrated.  

the  characters 

into 

for  selecting 

5.1 Computational Affective Modeling 
The  complexity  of  models  required  to  generate  affective 
behavior in game characters varies with the complexity of the 
game plot, the characters, the available behavior repertoire of 
the player within the game, and of course the game objectives 
(e.g.,  entertainment  vs.  education  vs.  therapy).    For  many 
games, very simple models are adequate, where a small set of 
gameplay or player behavior features is mapped onto a limited 
set of game  characters’ emotions, which are then depicted in 
terms  of  simple  manipulations  of  character  features  (e.g., 
player fails to find a treasure and the avatar shows a ‘sad face’, 
player loses to a game character and the character gloats).  
   Such simple models are termed ‘black-box’ models, because 
they  make  no  attempt  to  represent  the  underlying  affective 
mechanisms.  Data  available  from  the  affective  sciences 
provide  the  basis  for  defining  the  necessary  mappings 
(triggers-to-emotions,  emotions-to-effects).    However,  as  the 
complexity of the games increases, resulting in more involved 
plots  and  narratives,  and  associated 
the 
sophistication  of  the  game  characters  and  richness  of  player 
interactions, 
the  need  for  more  sophisticated  affective 
modeling  arises.    This  may  in  some  cases  require  ‘process-

increase 

in 

 Modeling Emotion Generation 
As  stated  above,  our  understanding  of  emotion  generation  is 
best within the cognitive modality and most existing models of 
emotion  generation  implement  cognitive  appraisal,  which  is 
best  suited  for  affective  modeling  in  gaming.  The  discussion 
below is therefore limited to these theories and models.  
   Many researchers have contributed to the current versions of 
cognitive  appraisal  theories  (Arnold  1960;  Frijda  1986; 
Lazarus  1984;  Mandler  1984;  Roseman  and  Smith  2001; 
Scherer  et  al.  2001;  Smith  and  Kirby,  2001).  Most  existing 
computational  models  of  appraisal  are  based  on  either  the 
OCC  model  (Ortony  et  al.  1988),  or  the  explicit  appraisal 
dimension  theories  developed  by  (Scherer  et  al.  2001;  Smith 
and  Kirby  2000),  and  outlined  in  section  3  above  (e.g., 
novelty, valence, goal relevance, goal congruence, responsible 
agent, coping potential).  
   A  number  of  computational  appraisal  models  have  been 
developed for both research and applied purposes (e.g., Andre 
et  al.  2000;  Bates  et  al.  1992;  Broekens  and  DeGroot  2006; 
Reilly  2006).    These  models  typically  focus  on  the  basic 
emotions (e.g., joy, fear, anger, sadness), and use a variety of 
methods for implementing a subset of the computational tasks 
outlined  above.    Most  frequently,  symbolic  methods  from 
artificial  intelligence  are  used  to  implement  the  stimulus-to-
emotion mapping, whether this is done via an intervening set 
of appraisal dimensions, or directly from the domain stimuli to 
the emotions. In general, the complexity of this process lies in 

its 

in  existing  psychological 

to 

in 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
analyzing  the  domain  stimuli  (e.g.,  features  of  a  game 
situation,  behavior  of  game  characters,  player  behavior),  to 
extract  the  appraisal  dimension  values.  This  may  require  the 
representation of a set of complex mental structures, including 
the  game  characters’  and  players’  goals,  plans,  beliefs  and 
values, 
their  current  assessment  of  the  evolving  game 
situation, and expectations of future developments, as well as 
complex  causal  representation  of  the  gameplay  dynamics. 
Rules, semantic nets and Bayesian belief nets are some of the 
most frequently used formalisms to implement this mapping.  
   Emotion  dynamics  are  generally  limited  to  calculating 
emotion intensity, which is usually a relatively simple function 
of  a  limited  set  of  the  appraisal  dimensions  (e.g.,  absolute 
value of the desirability of  an event or a situation multiplied 
by 
likelihood  (Reilly  2006)),  or  some  customized 
quantification  of  selected  feature(s)  of  the  stimuli  (e.g.,  a 
linear combination of weighted factors that contribute to each 
emotion  of  interest).  The  ramp-up  and  decay  of  emotion 
intensity generally follows a simple monotonically increasing 
(ramp-up)  and  decreasing  (decay)  function  over  time.    A 
variety  of  functions  have  been  used  in  appraisal  models, 
including linear, exponential, sigmoid and logarithmic (Reilly 
2006; Hudlicka 2008).  In general, the theories and conceptual 
models  developed  by  psychologists  do  not  provide  sufficient 
information  to  generate  computational  models  of  affective 
dynamics,  and  guesswork  and  model  tuning  are  required 
during this phase of affective modeling. 
   The  issue  of  integrating  multiple  emotions  is  the  most 
neglected,  both 
theories  and 
conceptual models, and  in computational  models.  Typically, 
very  simple  approaches  are  used  to  address  this  complex 
problem,  which  limits  the  realism  of  the  resulting  models  in 
any but the most simple  situations.  In general,  intensities  of 
synergistic  emotions  (e.g.,  all  positive  or  all  negative 
emotions)  are  combined  via  a  simple  sum,  average,  or  max 
functions, in some cases using customized, domain-dependent 
weightings (e.g., some  emotion  is emphasized in  a particular 
situation  over  another  emotion,  possibly  as  a  function  of  the 
character’s  personality).  Each  of 
these  approaches  has 
limitations, which are discussed in more detail in the tutorial. 
A  more  problematic  situation  occurs  when  opposing  or 
distinctly  different  emotions  are  derived  (e.g.,  a  particular 
situation brings both  joy and sadness).  Neither  the  available 
theories, nor existing empirical data, currently provide a basis 
for  a  principled  approach 
the 
computational  solutions  are  generally 
task-  or  domain- 
specific, and often ad hoc.  

data  regarding 
the  visible  manifestations  of  particular 
emotions,  and  the  established  techniques  for  3D  dynamic 
graphical  modeling  and  rendering  required  to  display  these 
expressions in virtual  characters.  We know,  in general, how 
the  basic  emotions  are  expressed 
terms  of  facial 
expressions,  quality  of  movement  and  gestures,  quality  of 
speech, and behavioral choices. (As with emotion generation, 
the degree of variability and complexity increases as we move 
from the fundamental emotions such as fear, joy, anger, to the 
more  cognitively-complex  emotions  such  as  pride,  shame, 
jealousy). While the  tutorial will address both the behavioral 
effects,  and the  effects on cognition, due  to space limitations 
the discussion below will focus on cognitive effects only.  
   The  internal  effects  that  emotions  exert  on  the  perceptual 
and  cognitive  processes  that  mediate  adaptive,  intelligent 
behavior  are  less  understood  than  those  involved  in  emotion 
generation.  This  is  true  both  for  the  fundamental  processes 
(attention,  working  memory,  long-term  memory  recall  and 
encoding),  and  for  higher-level  processes  such  as  situation 
assessment,  problem-solving,  goal  management,  decision-
making,  and  learning.    These  processes  are  generally  not 
modeled in existing game characters, and, indeed, may not be 
necessary.  However,  as  the  affective  complexity  of  games 
increases,  the  need  for  these  types  of  models  will  likely 
emerge,  particularly  so  in  therapeutic  games,  where  the 
assessment  and  triggering  of  specific  emotions  is  the  focus; 
e.g.,  games  designed 
support  cognitive-behavioral 
therapies,  and  the  associated  cognitive  restructuring,  will 
require  explicit  modeling  of  emotion  effects  on  cognition  to 
implement the treatment protocols.  
    While  data  are  available  regarding  some  of  the  emotion 
effects on cognition (see section 3), the  mechanisms of these 
processes have not been identified and this presents challenges 
for  the  modeler,  frequently  resulting  in  black-box  models 
rather  than  mechanism-based  process  models.  Nevertheless, 
several recent efforts focus on the process-models of emotion 
effects  on  cognition,  most  often  in  terms  of  parametric-
modification  of  cognitive  processes  (e.g.,    Hudlicka  2003; 
2007;  Ritter  et  al.  2007;  Sehaba  et  al.  2007).  For  example, 
Hudlicka’s  MAMID  model  uses  a  series  of  parameters  to 
control  processing  within  individual  modules  in  a  cognitive-
affective  architecture,  enabling  the  implementation  of  the 
observed emotion effects such as speed and capacity changes 
in  attention  and  working  memory,  as  well  as 
the 
implementation  of  specific  biases  in  processing  (e.g.,  threat 
and  self-focus  bias  in  anxiety).  Several  models  of  emotion 
effects  on  behavior  selection  use  a  decision-theoretic 
formalism,  where  emotions  bias  the  utilities  and  weights 
assigned to different behaviors (Busemeyer et al., 2007; Lisetti 
& Gmytrasiewicz, 2002). 
   Modeling  emotion  effect  magnitude  and  dynamics 
is 
problematic,  as  it  requires  going  beyond  the  qualitative 
relationships  typically  available  from  empirical  studies  (e.g., 
anxiety  biases  attention  towards  threatening  stimuli).  In  the 
majority  of  existing  models,  quantification  of  the  available 
qualitative  data  is  therefore  more  or  less  ad  hoc,  typically 
involving  some  type  of  linear  combinations  of  the  weighted 

Modeling Emotion Effects 
   For modeling purposes, it is useful to divide emotion effects 
into  two  categories:  the  visible,  often  dramatic,  behavioral 
expressions, and the less visible, but no less dramatic, effects 
on  attention,  perception  and  cognition.    Majority  of  existing 
emotion models of emotion effects focus on the former. While 
technically challenging, the behavioral effects are easier from 
a  modeling  perspective,  due  to  the  large  body  of  empirical 

to 

this  problem  and 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 
factors,  and  requiring  significant  fine-tuning  to  adjust  model 
performance. The same is true for modeling the integration of 
multiple  emotions.    Especially  challenging  for  both  of  these 
tasks  is  the  lack  of  data  regarding  the  internal  processes  and 
structures  (e.g.,  effects  on  goal  prioritization,  expectation 
generation,  planning). 
  The  difficulties  associated  with 
characterizing  these  highly  internal  and  transient  states  may 
indeed  provide 
for  process-level 
computational models of these phenomena.  

limiting 

factor 

a 

challenging and enjoyable experience. The methods developed 
in affective computing provide many of the tools necessary to 
take  affective  gaming  to  the  next  stage:  where  a  variety  of 
complex  emotions  can  be  induced  in  the  player,  for  both 
entertainment  and 
therapeutic  purposes.  
Affective  computing  thus  directly  supports  all  three  of  the 
phases comprising affective gaming, as suggested by Gilleade 
and  colleagues:  “Assist  Me,  Challenge  Me,  Emote  Me” 
(Gilleade  et  al.  2005),  and  the  methods  and  techniques 
developed in affective computing can serve as a foundation for 
affect-focused game design. 

training  and 

5.2 Affective User Modeling 
   Affective  user  models  are  representational  structures  that 
store  information  about  the  affective  makeup  of  the  player: 
which  stimuli  trigger  which  emotions,  what  behaviors  are 
associated  with  different  emotions,  etc.  Such  models  serve  a 
critical  role 
in  affect-adaptive  gaming,  supporting  both 
emotion  recognition,  and  the  generation  of  an  appropriate 
affect-adaptive strategy by the game system.   Since  affective 
behavior  can  be  highly 
idiosyncratic,  affective  models 
typically  involve  a  learning  component,  so  that  the  player’s 
behavior can be tracked over time and the important affective 
patterns  extracted  from  monitoring  of  the  player’s  state  and 
game  interactions.  For  example,  Player  A  may  express  his 
frustration  by  more  forceful  manipulation  of  the  game 
controls,  while  Player  B  may  express  frustration  through 
increasing delays between game inputs. 
   The  knowledge  and  inferencing  required  to  support  these 
functionalities  can  take  a  number  of  forms.    A  useful 
representation  is  an  augmented  state  transition  diagram  or  a 
hidden Markov model (Picard, 1997) that explicitly indicates 
the  known  states  of  the  user  (e.g.,  happy,  sad,  frustrated, 
bored,  excited),  the  situations  and  events  that  trigger  these 
transitions  (e.g.,  in  gaming  context,  loss  or  gain  of  points  or 
game  resources;  appearance  or  disappearance  of  a  particular 
game  character,  etc.),  and  the  player’s  behavior  (or  other 
monitored  characteristic)  that  indicate  each  emotion.    The 
tutorial  discusses  the  structures,  development,  and  use  of 
affective user models in more detail. 
 
6.0 SUMMARY AND CONCLUSIONS 
 
 This  paper  summarized  key 
ideas  from  an  associated 
GAMEON  08  tutorial  on  “Affective  Computing  and  Game 
Design”.  The  aim  of  the  tutorial  is  to  discuss  how  the 
emerging  discipline  of  affective  computing  contributes  to 
affect-focused  game  design.  The  paper  also  provided 
information about existing data and theories from the affective 
sciences  that  inform  decisions  about  approaches  to  emotion 
sensing  and  recognition,  generation  of  affective  behavior  in 
game  characters,  and  computational  affective  modeling  in 
affective gaming. 
   Gaming  researchers  emphasize  the  importance  of  affective 
game  adaptations 
to  ensure 
engagement  and  enhance  effectiveness  of  serious  games. 
Today, the  term ‘affective gaming’ generally means adapting 
to the player’s emotions, to minimize frustration and ensure a 

the  player’s  emotions, 

to 

7.0 REFERENCES 

Andre, E., Klesen, M., Gebhard, P., Allen, S., & Rist, T. (2000). 

Exploiting Models of Personality and Emotions to Control the 
Behavior of Animated Interactive Agents Proceedings of IWAI, 
Siena, Italy. 

Arnold, M. B. (1960). Emotion and personality. New York: 

Columbia Univeristy Press. 

Bates, J., Loyall, A. B., & Reilly, W. S. (1992). Integrating 

Reactivity, Goals, and Emotion in a Broad Agent. In Proceedings of 
the 14th Meeting of the Cognitive Science Society. 

Becker, C., Nakasone, A., Prendinger, H., Ishizuka, M., & 

Wachsmuth, I. (2005). Physiologically interactive gaming with the 
3D agent Max. International Workshop on Conversational 
Informatics at JSAI-05, Kitakyushu, Japan. 

Bersak, D., McDarby, G., Augenblick, N., McDarby, P., 
McDonnell, D., McDonal, B. (2001). Biofeedback using an 
Immersive Competitive Environment. Designing Ubiquitous 
Computing Games Workshop - Ubicomp.. 

Bower, G. H. (1992). How Might Emotions Affect Memory? In 

S. A. Christianson (Ed.), Handbook of Emotion and Memory. 
Hillsdale, NJ: Lawrence Erlbaum. 

Breazeal, C., Brooks, R. . (2005). Robot Emotion: A Functional 

Perspective. In J.-M. Fellous & M. A. Arbib (Eds.), Who Needs 
Emotions? NY: Oxford. 
   Broekens, J., & DeGroot, D. (2006). Formalizing Cognitive 
Appraisal: From Theory to Computation. ACE, Vienna, Austria. 

Busemeyer, J. R., Dimperio, E., & Jessup, R. K. (2007). 

Integrating emotional processes into decision-making models. In 
W.Gray (Ed.), Integrated Models of Cognitive Systems. NY: Oxford. 
Cohn, J. F., Ambadar, Z., & Ekman, P. (2005). Observer-Based 

Measurement of Facial Expression with the Facial Action Coding 
System. In J. A. Coan & J. B. Allen (Eds.), The handbook of emotion 
elicitation and assessment. NY: Oxford. 

Derryberry, D. (1988). Emotional influences on evaluative 

judgments: Roles of arousal, attention, and spreading activation. 
Motivation and Emotion, 12(1), 23-55. 

Ekman, P., & Davidson, R. J. (1994). The nature of emotion: 

Fundamental questions. NY: Oxford. 

Ellsworth, P. C., & Scherer, K. R. (2003). Appraisal Processes in 
Emotion. In R. J. Davidson, K. R. Scherer & H.H.Goldsmith (Eds.), 
Handbook of Affective Sciences. NY: Oxford. 

Fellous, J. M. (2004). From Human Emotions to Robot Emotions. 

AAAI Spring Symposium: Architectures for Modeling Emotion, 
Stanford University, CA. 

Frijda, N. H. (1986). The Emotions. Cambridge: Cambridge.  
Gilleade, K., Dix, A., & Allanson, J. (2005). Affective 

Videogames and Modes of Affective Gaming: Assist Me, Challenge 
Me, Emote Me. DIGRA, Vancouver, BC, Canada. 

In Proceedings of the 4th Intl. North American Conference on Intelligent Games and Simulation 
 (GAMEON-NA), McGill University, Montreal, Canada, 2008, pp. 5-12. 
 
 

Gilleade, K. M., & Dix, A. (2004). Using Frustration in the 

Design of Adaptive Videogames. ACW, Singapore. 

Hudlicka, E. (1998). Modeling Emotion in Symbolic Cognitive 
Architectures. AAAI Fall Symposium: Emotional and Intelligent I, 
Orlando, FL. 

Hudlicka, E. (2003). Modeling Effects of Behavior Moderators 

on Performance: Evaluation of the MAMID Methodology and 
Architecture. BRIMS-12, Phoenix, AZ. 

Hudlicka, E. (2005). Affect Sensing, Recognition and 

Expression: State-of-the-Art Overview First Intl. Conference on 
Augmented Cognition, Las Vegas, NV. 

Hudlicka, E. (2007). Reasons for Emotions. In W. Gray (Ed.), 
Advances in Cognitive Models and Cognitive Architectures. NY: 
Oxford. 
    Hudlicka, E. (2008a). Guidelines for Modeling Affect in Cognitive 
Architectures. Submitted for publication to Journal of Cognitive 
Systems Research (Also: Report # 0706, Psychometrix Associates, 
Inc. Blacksburg, VA). 
    Hudlicka, E. (2008b). What are we modeling when we model 
emotion? Proceedings of the AAAI Spring Symposium - Emotion, 
Personality, and Social Behavior, Stanford University, CA. 

Isen, A. M. (1993). Positive Affect and  Decision Making In J. M. 

Haviland & M. Lewis (Eds.), Handbook of Emotions. NY: Guilford. 

Kapoor, A., Burleson, W., & Picard, R. W. (2008). Automatic 

Prediction of Frustration. International Journal of Human-Computer 
Studies, 65(8), 724-736. 

Lazarus, R. S. (1984). On the primacy of cognition. American 

Psychologist 39(2), 124–129. 

Lerner, J. S., & Tiedens, L. Z. (2006). Portrait of the Angry 

Decision Maker: How Appraisal Tendencies Shape Anger's Influence 
on Cognition. Journal of Behavioral Decision Making, 19, 115-137. 

Lisetti, C., & Gmytrasiewicz, P. (2002). Can rational agents 

afford to be affectless? Applied Artificial Intelligence, 16(7-8), 577-
609. 

Mandler, G. (1984). Mind and Body: The Psychology of Emotion 

and Stress. New York: Norton. 

Matthews, G. A., & Harley, T. A. (1993). Effects of Extraversion 

and Self-Report Arousal on Semantic Priming: A Connectionist 
Approach. Journal of Personality and Social Psychology, 65(4), 735-
756. 

Mineka, S., Rafael, E., & Yovel, I. (2003). Cognitive Biases in 
Emotional Disorders: Information Processing and Social-Cognitive 
Perspectives. In R. J. Davidson, K. R. Scherer & H. H. Goldsmith 
(Eds.), Handbook of Affective Science. NY: Oxford. 

Ortony, A., Clore, G. L., & Collins, A. (1988). The Cognitive 

Structure of Emotions. NY: Cambridge. 

Ortony, A., Norman, D., & Revelle, W. (2005). Affect and Proto-

Affect in Effective Functioning  In J. M. Fellous & M. A. Arbib 
(Eds.), Who Needs Emotions? NY: Oxford. 

Petrushin, V. (2000). Emotion Recognition in Speech Signal. 6th 

ICSLP. 

MIT Press. 

Picard, R. (1997). Affective Computing. Cambridge, MA: The 

Reilly, W. S. N. (2006). Modeling What Happens Between 

Emotional Antecedents and Emotional Consequents. ACE, Vienna, 
Austria. 

Ritter, F. E., & Avramides, M. N. (2000). Steps Towards 

Including Behavior Moderators in Human  Performance Models in 
Synthetic Environments: The Pennsylvania State University. 

Ritter, F. E., Reifers, A. L., Klein, L. C., & Schoelles, M. J. 

(2007). Lessons from defining theories of stress for cognitive 
architectures. In W.Gray (Ed.), Advances in Cognitive Models and 
Cognitive Architectures. NY: OUP. 

Roseman, I. J., & Smith, C. A. (2001). Appraisal Theory: 

Overview, Assumptions, Varieties, Controversies. In K. R. Scherer, 
A. Schorr & T. Johnstone (Eds.), Appraisal Processes in Emotion: 
Theory, Methods, Research. NY: Oxford. 

Scherer, K., & Ellgring, H. (2007). Are Facial Expressions of 

Emotion Produced by Categorical Affect Programs or Dynamically 
Driven by Appraisal? Emotion, 7(1), 113-130. 

Scherer, K., Schorr, A., & Johnstone, T. (2001). Appraisal 

Processes in Emotion: Theory, Methods, Research. NY: Oxford. 

Sehaba, K., Sabouret, N., & Corruble, V. (2007). An emotional 
model for synthetic characters with personality Affective Computing 
and Intelligent Interaction (ACII), Lisbon.  

Smith, C. A., & Kirby, L. (2000). Consequences require 

antecedents: Toward a process model of emotion elicitation. In J. P. 
Forgas (Ed.), Feeling and Thinking: The role of affect in social 
cognition. NY: Cambridge. 

Smith, C. A., & Kirby, L. D. (2001). Toward Delivering on the 

Promise of Appraisal Theory. In K. R. Scherer, A. Schorr & T. 
Johnstone (Eds.), Appraisal Processes in Emotion. NY: OUP. 

Sykes, J. (2004). Affective Gaming.   Retrieved May 2008, from 

http://www.jonsykes.com/Ivory.htm 

Sykes, J., & Brown, S. (2003). Affective Gaming: Measuring 

emotion through the gamepad. CHI Extended Abstracts. 

EVA  HUDLICKA  is  a  Principal  Scientist  and  President  of 
Psychometrix  Associates,  Inc. 
in  Blacksburg,  VA.  Her 
primary  research  focus  is  the  development  of  computational 
models of emotion, aimed  at enhancing our understanding of 
the mechanisms underlying cognition-emotion interaction, and 
the  nature  of  affective  biases  in  decision-making.    This 
research  is  conducted  within  the  context  of  a  computational 
cognitive-affective  architecture,  the  MAMID  architecture, 
which  implements  a  generic  methodology  for  modelling  the 
interacting  effects  of  multiple  affective  factors  on  decision-
making.    She  is  currently  exploring  the  applications  of  this 
research in the development of ‘serious games’ in healthcare. 
Her  prior  research  includes  affect-adaptive  user  interfaces, 
visualization  and  user 
interface  design,  decision-support 
system  design,  and  knowledge  elicitation.  Dr.  Hudlicka  has 
authored numerous technical articles, and book chapters.  She 
was  recently  a  member  of  a  National  Research  Council 
committee  on  “Organizational  Models:  From  Individuals  to 
Societies”. Dr. Hudlicka received a BS in Biochemistry from 
Virginia  Tech,  an  MS  in  Computer  Science  from  The  Ohio 
State  University,  and  a  PhD  in  Computer  Science  from  the 
University  of  Massachusetts-Amherst.  Prior 
to  founding 
Psychometrix Associates in 1995, she was a Senior Scientist at 
Bolt, Beranek & Newman in Cambridge, MA. 

 
 

 
 

