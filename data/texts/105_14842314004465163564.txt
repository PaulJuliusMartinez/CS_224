Aplikace matematiky
Gene Howard Golub Least squares, singular values and matrix approximations
Aplikace matematiky, Vol. 13 (1968), No. 1, 44--51
Persistent URL: http://dml.cz/dmlcz/103138
Terms of use:
© Institute of Mathematics AS CR, 1968 Institute of Mathematics of the Academy of Sciences of the Czech Republic provides access to digitized documents strictly for personal use. Each copy of any part of this document must contain these Terms of use.
This paper has been digitized, optimized for electronic delivery and stamped with digital signature within the project DML-CZ: The Czech Digital Mathematics
Library http://project.dml.cz

SVAZEK 13 (1968)

APLIKACE MATEMATIKY

CÍSLO 1

LEAST SQUARES, SINGULAR VALUES AND MATRIX APPROXIMATIONS1)
GENE HOWARD GOLUB

0. Let A be a real, m x n matrix (for notational convenience we assume that m j _ n). It is well known (cf. [6]) that

(0.1) where

A - UIVT

and

I=
n) x n
The matrix U consists of the orthonormalized eigenvectors of AAr, and the matrix V consists of the orthonormalized eigenvectors of ATA. The diagonal elements of I are the non-negative square roots of the eigenvalues of ATA; they are called singular values or principal values of A. Throughout this note, we assume
0"i = °"2 = · · · = an = 0 ·
Thus if rank (A) = r, err+1 = err+2 = ... = an = 0. The decomposition (0.1) is called the singular value decomposition.
In this paper, we shall present a numerical algorithm for computing the singular value decomposition.
1. The singular value decomposition plays an important role in a number of least squares problems, and we will illustrate this with some examples. Throughout this discussion, we use the euclidean or Frobenius norm of a matrix, viz.
Ml = (EM2)1'2 ·
1 ) This work was in part supported by NSF and ONR.
44

A. Let tfln be the set of all n x n orthogonal matrices. For an arbitrary n x n real matrix A, determine Q e °l/n such that
|| A - Q|| ^ \\A - Kj| for any X e °lln.
It has been shown by FAN and HOFFMAN [2] that if A = UIVT, then Q = UVT .

B. An important generalization of problem A occurs in factor analysis. For arbitrary n x n real matrices A and B, determine 0 e °lln such that
||A - BQ|| ^ || A - BX\\ for any X e %n.
It has been shown by GREEN [5] and by SCHONEMANN [9] that if BTA = UIVT, then Q = UVT .

C. Let Jin\]n be the set of all m x n matrices of rank k. Assume A e Ji^n- Determine B e M^n (k <; r) such that
||A - B|| g |AL - X\\ for all Xe, f1(mk),n ·

It has been shown by ECKART and YOUNG [1] that if

(i.l)

UIV1

then B = UQ.V1

where (1.2)

ffi.O, . ..,0  0, ff2,. ..,0
o, o, . · ·. ff*

Note that (1.3)

B = 2 - fìt

(ff,2+1 + ... + (r2)1/2.

D. An n x m matrix X is said to be the pseudo-inverse of an m x n matrix A if X satisfies the following four properties:
i) AXA = A, ii) XAX = X, iii) (AX)T = AX, iv) (XA)T = XA.
We denote the pseudo-inverse by A + . We wish to determine A+ numerically. It can be shown [8] that A+ can always be determined and is unique.

45

It is easy to verify that (1.4)

A+ - VAU7

where

1
o-i
0,
=

0,
1
°

0, 0,

..,0 ..,0
1 or

nxm

In recent years there have been a number of algorithms proposed for computing the pseudo-inverse of a matrix. These algorithms usually depend upon a knowledge of the rank of the matrix or upon some suitably chosen parameter. For example in the latter case, if one uses (1.4) to compute the pseudo-inverse, then after one has computed the singular value decomposition numerically it is necessary to determine which of the singular values are zero by testing against some tolerance.
Alternatively, suppose we know that the given matrix A can be represented as
A = B + 8B
where SB is a matrix of perturbations and

\\8B\\ S *1 ·

Now, we wish to construct a matrix B such that
1-4 - B\\=ri and
rank (S) = minimum .

This can be accomplished with the aid of the solution to problem (C). Let

as in equation (1.2). Then using (1.3),
if

Bk = UQkVT B = B,,

and

(al + al+1 + ... +

a2y2>n.

46

Since rank (B) = p by construction, B+ = VQ+pUT .
Thus, we take B+ as our approximation to A + .

E. Let A be a given matrix, and let b be a known vector. Determine a vector x such that for
\\b -- Ax||2 = min and ||x||2 = min ,
where ||y|2 = CEy2)1/2 for any vector y. It is easy to verify that x = A + b. A norm is said to be unitarily invariant if \\AU\\ = \\VA\\ = ||A|| when U*U = I
and V*V = I. FAN and HOFFMAN [2] have shown that the solution to problem (A) is the same for all unitarily invariant norms and MIRSKY [7] has proved a similar result for the solution to problem (C).

2. In [4] it was shown by GOLUB and KAHAN that it is possible to construct a sequence of orthogonal matrices {P(fc)}£=i> {Q(k)}£=i via Householder transform-

ation so that

pOOpOi-l) pO)AQ<DQi2)

gOi-l) s pTAQ = j

and J is an m x n bi-diagonal matrix of the form

o-i, ßu 0, .. ., o
0, a2, ß2 .. ., 0
J = 0, 0, 0, .. ., /?n_i 0, 0, o, .. 0 } (m -- n) x n .

The singular values of J are the same as those of A. Thus if the singular value de-

composition of

J = XIYT

then A = PXIYTQT

so that U = PX, V= QY.

A number of algorithms were proposed in [4] for computing the singular value

decomposition of J. We now describe a new algorithm, based on the QR

algorithm of FRANCIS [3], for computing the singular value decomposition of J.

Let

"0 at (xx 0 pi

0

C =

ßl u a2
a2

· a,
a 0 2n x 2

47

It can be shown [4] that K is a symmetric, tri-diagonal matrix whose eigenvalues arc ± singular values of J. One of the most effective methods of computing the eigenvalues of a tri-diagonal matrix is the QR algorithm of Francis, which proceeds as follows:
Begin with the given matrix K = K0. Compute the factorization

where M0M0 = 1 and R0 is an upper triangular matrix, and then multiply the matrices in reverse order so that
Kx = R0M0 = MlK0M0 .
Now one treats Kx in the same fashion as the matrix K0, and a sequence of matrices is obtained by continuing ad infinitum. Thus

so that

K, = MiRi and Ki + 1 = .RtM, = Mi+1Ri+1 , Ki+1 = MjK-Mi = MjMj_1 ... MT0KM0MX ... M{.

The method has the advantage that K, remains tri-diagonal throughout the computation.
For suitably chosen shift parameters sif we can accelerate the convergence of the QR method by computing

(2.1)

(K; - s,7) = MiRi, RiMi + sj = Ki+1 .

Unfortunately, the shift parameter 5, may destroy the zeroes on the diagonal of K. Since the eigenvalues of K always occur in pairs, it would seem more appropriate
to compute the QR decomposition of

so that

(Ki-slI)(Ki

+ SiI) = K2i-s2iI

MiRi = K\ - s2I.

It has been shown by Francis that it is not necessary to compute (2.1) explicitly but it is possible to perform the shift implicitly. Let

{-V,}w = {Mi}kA, k = U2,...,2n.

(i.e., the elements of the first column of N are equal to the elements of the first column

of M) and

Then if

NjN( = I.

i) T l + 1 = NjKiNi, *0 -Ti+i 1s a tri-diagonal matrix,

48

iii) Ki is non-singular, iv) the sub-diagonal elements of Ti+i are positive, it follows that Ti+ i = KJ + 1 . The calculation proceeds quite simply. Dropping the iteration counter i), let
GO (P + i) (P + 2)

z,,=

cos 0p, o,
0, 1,

sin p 0

sin p, 0, -- cos Op

1

(P)
(P + l) (p + 2)

1 2n x 2  .

Then cos t9x is chosen so that

Then the matrix

{ZX(K2 - s 2 I ) } M - 0 for fc = 2,3

2n.^

"0
a;
o
Z^KZX = di

ai 0
o /;
/;; .
a2

d!
a2 .

0

a a,, 0 and T -- Z2n~2 · · · ZÍKZÍ
where Z2, ..., Z2rj_2 are constructed so that Tis tri-diagonal. The product of all the orthogonal transformations which gives the singular values yields the matrix of orthogonal eigenvectors of K. For ease of notation let us write

Jij-i = «j 9 I = 1, 2 , . . . , n , /2j = Pj, I = 1,2,..., n - 1 . Then explicitly, the calculation goes as follows: Dropping the iteration counter i,

To = y\ - s2, d0 = ri72 ·

49

For j = 0, 1 , . . . . 2/i -- 3,
·wtf + rfjr.
sin 0. = j . / r . } c o s 0. = y . / r . t
?J = ?j >
h+x = yj+l cos 0 ; + yj + 2 sin 0,-, h+2 = ?y+i sin 0y - f; + 2 cos 0,., tj + 3 = - y y + 3 c o s 0 J - , <*;+i -= yj + 3sm Oj. In the actual computation, no additional storage is required for
ih h h)
since they may overwrite {y.}. Furthermore, only one element of storage need be reserved for {dj}. When |?2,,_2| is sufficiently small, |y2n_i| is taken as a singular value and n is replaced by n -- 1.
Now let us define
[p/2 + 1] [p/2 + 2]

W =rrp

cos p sin Øn

sin p -- cos Øn

[p/2 + 2] [/2 + 1]

1 n Xn
where cos 0P is defined as above. It has been pointed out to the author by J. H. WIL KINSON that the above iteration is equivalent to forming
3 = PV2n_3 . . . W,WlJW0W2 . . . W2n_2
where 3 is again a bi-diagonal matrix. Thus,
x = Y[(w?wiiK..wtf,,3),
i
r^IlW-d)-
i
An ALGOL procedure embodying these techniques will soon be published by
Dr. PETER BUSINGER and the author.
50

References An xtniv lit of rfrn on ingular valu i givn in [4]. [1] C. Eckart and G. Young, "Th apprximatin f n matrix by anthr f lwr rank",
Pyhmtrika, 1 (1936), pp. 211-218. [2] K. Fan nd A. Hoff/nan, "Sm mtri inquliti in th p f mtri", Pr mr.
Mth. S, 6(1955), pp. 111-116. [3] J. Francis, "Th QR trnfrmtin.  unitry nlgu t th LR trnfrmtin", mput.
., 4 (1961, 1962), pp. 265-271. [4] G. Golub nd W. Kahan, "lulting th ingulr vlu nd pudinvr f  mtrix",
. SIM Numr. l. Sr. B, 2 (1965), pp. 205-224. [5] B. Green, "Th rthgnl pprximtin f n bliqu trutur in ftr nlyi'', Pyh-
mtrik, 17 (1952), pp. 429-440. [6] C. Lanczos, Linr Diffrntil Oprtr, Vn Ntrnd, Lndn, 1961, hp. 3. [7] L. Mirsky, "Smmtri gug funtin nd unitril invrint nrm", Qurt. . th.
Oxfrd (2), 11 (1960), pp. 5 0 - 5 9 . [8] R. Penrose, " gnrlizd invr fr mtri", r. mbridg Phil. S, 57 (1955),
pp. 406-413. [9] P. Schone/nann, " gnrlizd lutin f t rthgnl prrut prblm", Ph-
mtri, 31 (1966), pp. 1-10. Gene Howard Golub, mputr in Dpt., tnfrd, lif., 9405, U...
51

