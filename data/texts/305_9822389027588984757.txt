Machine Learning: Proceedings of the Fourteenth International Conference, 1997.
Using output codes to boost multiclass learning problems

 Robert E. Schapire AT&T Labs 600 Mountain Avenue, Room 2A-424 Murray Hill, NJ 07974 schapire@research.att.com

Abstract. This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Dietterich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the accuracy of a given base or "weak" learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multiclass problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods.
1 INTRODUCTION
Boosting is a general method for improving the accuracy of a learning algorithm. By definition, a boosting algorithm is one which can provably convert any base or "weak" learning algorithm with accuracy just slightly better than random guessing into one with arbitrarily high accuracy. Boosting algorithms work by repeatedly reweighting the examples in the training set and rerunning the weak learning algorithm on these reweighted examples. Boosting effectively forces the weak learning algorithm to concentrate on the hardest examples. Typically, the final combined hypothesis is a weighted vote of the weak hypotheses.
The first boosting algorithms were discovered by Schapire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund
¡and Schapire [7], Quinlan [15], Breiman [2] and others. AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971.

In its simplest form, ADABOOST requires that the accu-
¢racy of each weak hypothesis (or classification rule) pro-
duced by the weak learner must exceed 1 2. For binary
£ ¤ ¥classification problems (in which each example is labeled
by a value in 0 1 ), this requirement is about as minimal as can be hoped for since random guessing will achieve
¢accuracy 1 2. However, for multiclass problems in which ¦¨§ ¢ ¢ ¦2 labels are possible, accuracy 1 2 may be much harder
to achieve than the random-guessing accuracy rate of 1 .
For fairly powerful weak learners, such as decision-tree algorithms, this does not seem to be a problem. Experi-
¢mentally, C4.5 and CART seem to be capable of producing
hypotheses with accuracy 1 2, even on the difficult distri-
¢butions of examples produced by boosting [2, 5, 7, 15].
However, the accuracy 1 2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jackson and Craven [13] and Freund and Schapire [7] in their boosting experiments. Although overall error rate is often better when more powerful weak learners are used, these less expressive weak learners have the advantage that the final combined hypothesis is usually less complicated, and computation time may be more reasonable, especially for very large datasets.
Freund and Schapire [8] provide one solution to this problem by modifying the form of the weak hypotheses and refining the goal of the weak learner. In this approach, rather than predicting a single class for each example, the weak learner chooses a set of "plausible" labels for each example. For instance, in a character recognition task, the weak hypothesis may predict that a particular example is either a "6," "8" or "9," rather than choosing just a single label. Such a weak hypothesis is then evaluated using a "pseudoloss" measure which, for a given example, penalizes the weak hypothesis for (1) failing to include the correct label in the predicted plausible label set, and (2) for each incorrect label which is included in the plausible set. The final combined hypothesis, for a given example, chooses the single label which occurs most frequently in the plausible label sets chosen by the weak hypotheses (possibly giving more or less weight to some of the weak hypotheses).

 ¢¡

¢  ¡  

¡ 

 !

Given: 1 £¥¤ 1 ¦§£©©¨ ©¨ ¨£

£ ¤ ¦ where

,¤

For $" # 1©£ ©¨ ©¨ ©¨ £&% :

' '

Compute distribution (07) o6 ver 1 1£ ¨©©¨ ¨©£ 243 .

Compute coloring 5 ) :

1 0£ 13 .

' T¢  ¡ rain w  eak learner ¢¡ on exam  p les

'

1 £ 58) ¤ 1 9¦ ¦§£ ©¨ ©¨ ¨©£

£ 85 ) A¤ 6 ¦9¦ weighted according to (0) .

Get weak hypothesis @ ) :

1 0£ 13 .

FE

Compute coefficients B 1 £ ©¨ ¨©¨©£ DB C

.

Output the final hypothesis:

G  ¢¡ final ¦ #

RC
arg mH QI aP x

 ¢¡ T  W B )U U @ ) ¦ V# 5 ) ¦YX TX ¨

T) S 1

Figure 1: A generic algorithm combining boosting and ECOC.

The exact form of the pseudoloss is under the control of the boosting algorithm, and the weak learning algorithm must therefore be designed to handle changes in the form of the loss measure. This design gives the boosting algorithm the freedom to focus the weak learner not only on the hard to predict examples, but also on the labels which are hardest to distinguish from the correct label.
This approach works well experimentally [7], but suffers certain drawbacks. First, it requires the design of a weak learner which is responsive to the pseudoloss defined by the boosting algorithm and whose hypotheses generate predictions in the form of plausibility sets. Since most "offthe-shelf" learning algorithms are error-based, this may demand extra effort and creativity on the part of the programmer (and may be completely impossible if the source code for the weak learning algorithm is unavailable).
The second drawback of the pseudoloss approach is
¦that it can be fairly sldocw. Typically, the running time of ¦the weak learner is b` a times slower than that of an error-
based algorithm for a -class problem.
In this paper, we describe an alternative method for boosting multiclass learning algorithms. Our method combines boosting with Dietterich and Bakiri's [4] approach based on error-correcting output codes (ECOC), which is designed to handle multiclass problems using only a binary learning algorithm.
Briefly, their approach works as follows: As in boosting, a given "weak" learning algorithm (which need only be designed for two-class problems) is rerun repeatedly. However, unlike boosting, the examples are not reweighted. Instead, on each round, the labels assigned to each example are modified so as to create a new binary labeling of the data
¦ £ ¤ ¥which is induced by a simple mapping from the set of labels
to 0 1 . The sequence of bit assignments for each of the labels can then be viewed as a "code word." A given
test example is then classified by choosing the label whose associated code word is closest in Hamming distance to the sequence of predictions generated by the weak hypotheses. This coding-theoretic interpretation led Dietterich and

 ¢¡

 ¢¡  

¡ 

!

Given: Initialize

(

1 £¥¤
~1

 f1e

§¦ W£©©¨ ¨©¨£ £ ¦#

Whg UU #

£¤ ¤



¦

where
   ¢qsr

X pX i 2

1¦9¦

,¤

/* uniform over all incorrect labels */

For "$# 1©£ ¨©©¨ ©¨ &£ % :

' '

Train weak learner using Get weak hypothesis @~ ) :

pA seu6 doloP ss 2.

defined

by

t( ~ )

.

' Let



R

u~) #

1 2

R

(w~ ) f  e £ W ¦yx U U ¤  g

@ ~ ) ¢  ¡

 ¦YX X 

H9IvP

W  UU

@~ )  ¢¡

 ¦X X 

S1

' '

Let B ) # Update

1 2

ln



r
1

u~)

u ~)

.

( ~ )T

 fe W 1£¦

#

( ~ )  fe £ W ¦ x exp  B

$g )  UU ¤

@~ ) ¢  ¡  ¦YX X 

W UU

@ ~ ) ¢  ¡  ¦YX X 



)


where

)

is a normalization factor (chosen so that (t~ T) 

1 will

sum to 1).

Output the final hypothesis:

G  ¢¡ final ¦ #

RC
arg mH9QI aP x

W B )U U

)TS 1

@ ~ )  ¢¡ Y¦ X X¢¨

Figure 2: The pseudoloss-based boosting algorithm ADABOOST.M2.

Bakiri to the beautiful idea of choosing code words with strong error-correcting properties.
The algorithm presented in this paper is a hybrid of the boosting and ECOC approaches. As in boosting, on each round of rerunning the weak learner, the examples are reweighted in a manner focusing on the hardest examples. Then, as in ECOC, the labels are modified to create a binary classification problem. The result is an algorithm that combines the benefits of both approaches: As in ECOC, the weak learning algorithm need only be able to handle binary problems, and with respect to ordinary error rather than the more complicated and time-consuming pseudoloss. Like boosting, the algorithm comes with a strong theoretical guarantee, namely, that if the weak learner can consistently generate weak hypotheses that are slightly better than random guessing (with respect to the distribution and binary example labeling on which it was trained), then the error of the final combined hypothesis can be made arbitrarily small. This is the main theoretical result of this paper.
In the rest of the paper, we describe the new algorithm in detail and prove a strong theoretical bound on the error of the final hypothesis. We then describe the results of several experiments comparing the new algorithm to a number of other voting methods (including ECOC and pseudolossbased boosting).

2

¢  ¡

 ¢¡ 

¡   

Given: Initialize

(

1 ¥£ ¤
~1

f  1e

§¦ £©W ©¨ ©¨ ¨£
£ ¦ as

in

£ ¤ ¦
Figure

where 2.

For "$# 1£©©¨ ¨©¨©£&% :

!
,¤

.

'
 ' ¢¡ ¡ ¡  '

76

Compute colo ring 5 ) :

Let ) #

 9H IQP S1

 fe H9vI P ( ~ )

(~)

 fe

1 W 0£

13

.
 

£ ¦ UU 5 ) ¤

 ¦

f  e W    g

£ ¦ UU 5 ) ¤ ¦ #

g #
5

)

5 T  W

 TW )
¦X X

¦YX X .

Let (t) ¦ #

.

'

)
T ¢¡ rain w  eak learner ¢¡ on exam  p les

' '

1 £ 58) ¤ 1 ¦9¦§£¨©¨©¨©£

£ 85 ) A ¤ 6 9¦ ¦ weighted according to 0( ) .

Get Let

weak @~ )  ¢¡ ¦

hypothesis
W #1

@
:

)

:
 ¢¡

@ )

¦

#

1 0T £ W 13 . 85 ) 9¦ 3 .

' '

Let u~) and B ) Compute ( ~ p) 

bef  e asW in Figure 2. 1 £ ¦ as in Figure 2.

Output the final hypothesis:

G  ¢¡ final ¦ #

RC
arg mH IQaP x

¢  ¡  TW B ) U U @ ) ¦ V# 5 ) ¦YX TX ¨

T) S 1

Figure 3: The algorithm ADABOOST.OC combining boosting and output coding.

2 THE NEW ALGORITHM

Both boosting and ECOC work iteratively in rounds

by rerunning the weak learning algorithm many times. In

boosting, the weak learner is trained on each round on a

new distribution or weighting of the training examples. In

ECOC, the weak learner is trained on a new partition of

the class labels which induces a new binary labeling of the

data.

The key idea of the algorithm proposed in this paper

is to combine both approaches, i.e., on each round, both

to reweight and relabel the data. Generically, then, our

algorithm looks like the one in Figure 1.
£ ¥§¤ ¦ ¤In the figure, and later in the paper, we use the notation which we define to be 1 if proposition holds and 0

otherwise.
¨Thce algorithm is given training examples of the form ©§ ¤  ©§ a where is chosen from some space , and the ¦   associated class label is chosen from a set of finite cardinality . On each round , the algorithm computes a !distribution over the training examples, and a function " , which we refer to as a coloring and which partitions the label set into two parts. The data is then relabeled ac"cording to , and the weak learner trained on this relabeled !data weighted according to .
$#The resulting weak hypothesis is denoted . The goal

of the weak learning algorithm is to minimize its training

error with respect to the relabeled and reweighted data, that

is, to minimize
% '& &

R970 () 8@10 AC1 BEDEa 2 GFc §#£ §# a§©a34© Hc c6&5 &5

"

"

a3  c ¦ a Pc RI Q

(1)

name
soybean-small iris glass audiology
soybean-large vehicle vowel segmentation
splice satimage letter

# examples train test

47 150 214 226 -

307 846 528 2310

376 -
462 -

3190 4435 16000

2000 4000

# classes
4 3 7 24
19 4
11 7
3 6 26

# attributes disc. cont.
35 -4 -9
69 -
35 - 18 - 10 - 19
60 - 36 - 16

missing values
-
SS-
-
-

Table 1: The benchmark machine learning problems used in the experiments.

TFinally, the combined hypothesis final is computed.

This hypothesis can be viewed as a kind of weighted vote

©of the weak hypotheses. Givecn an example , we interpret

# 3© #the binary classification a of weak hc ypothesics as a

U #V 3© & " GUvote for all of the labels for which a

©a , i.e., all

#Vof the labels with the "color" selected by . This vote is

WR Uweighted by some real number . The label receiving the

Tmost weighted votes is then chosen as final's classification ©of . (Ties are broken arbitrarily, and, in our analysis, are

counted as errors.)

To complete the description of the algorithm, we need to
Xderive a reasonable choice for the distribution , the color" WYing and the coefficients . To do this, we will reduce to

the pseudoloss method used by Freund and Schapire [8] in

the development of ADABOOST.M2, one of the multiclass

versions of their boosting algorithm. This reduction will

also lead to an analysis of the resulting algorithm.

2.1 REVIEW OF ADABOOST.M2

We begin with a review of Freund and Schapire's [8] pseu-

doloss method and of the boosting algorithm ADABOOST.M2, shown in Figure 2. On each round, the boosting
 QQ@Q `¨ bac£ ¤ ¤ ¥algorithm computes a distribution ~ over 1  32   & 2 ¤such that ~ a c 0 for all . In other words, ~ can be

viewed as a distribution over pairs of examples and incorrect

labels. The idea is to enable the boosting algorithm to con-

centrate the weak learner not only on the hard examples, but

also on the incorrect labels which are hardest to distinguish

from the correct label.

Given this distribution, the weak learner computes a

#§ e d f f"soft" hypothesis1 ~ :

2 where 2 is the power

set # 3©~ a

ocf

. As explained in the introduction, we as a set of "plausible" labels for a given

interpret example

© . Intuitively, it is easier for the weak learner to identify

a set of labels which may plausibly be correct, rather than

selecting a single label.

hg1Freund and Schapire [8] allow so ft h!ypotheses to take a more

general form as functions mapping

into U 0£ 1X . The soft

hypotheses we consider are equivalent to restricting theirs to have

range 1 0£ 13 . Since this simplifying restriction is a special case of

theirs, there is no problem applying their results.

3

soybean-small

40

iris

glass

50 70 80

40

30

60 50

60

30 20 40

20 30 40

10

10

20 10

20

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

500. 0

5 10.

50. 100.

0 500.

soybean-large

vehicle

vowel

80 60 80 70

60

50 40

60

60 50

40

30

40

40 30

20

20 10

20

20 10

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

0 500.

5 10.

50. 100.

500. 0

splice

satimage

letter

70 40 60 80

30 50 60 40
20 30 40

10 20 20 10

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

0 500.

5 10.

50. 100.

500.

Figure 4: Comparison of several learning methods using FINDATTRTEST as the weak learner.

audiology

5 10.

50. 100.

segmentation

500.

5 10.

50. 100.

500.

AdaBoost.OC AdaBoost.M2 ECOC Bagging Arc-x4

The goal of the weak learner is to minimize the pseu-

doloss:

(R R

% Y&~

1 2

¢  ¡

0 ) f1

1~ a 2 P¤ U c¤£  £   ¥5

#§~ ©a ©§ c ¦§¦

£U ¥

#§~ a ©§ c ¦  Q

This loss measure penalizes the weak hypothesis for failing

 5 to include the correct label ©  5   §# 3©4with example (so that

¥

in the ~ a

c

plausible set associated ), and further penalizes

U &  each incorrect U # ©   2 ¤  &set (so that

la¥ be~l

a

c

).

which is included (Recall that ~

in
a

thec

plausible 0 so

correct labels contribute nothing to the sum.) Note that the

£ ¤ ¦ ¢pseudoloss is always in 0 1 and that pseudoloss 1 2 can

4# 3© & ©be obtained trivially by setting ~ a

c
©¨

for all

.

Freund and Schapire's [8] ADABOOST.M2 algorithm

©  Uworks by increasing, on each round, the weight placed on
examples and incorrect labels which contribute most to

the pseudoloss. The combined hypothesis then chooses the

single label which occurs in the largest number of plausible

label sets chosen by the weak hypotheses, where the votes
% Y& ¢of some weak hypotheses count for more than others. Let ~ 1 2  ~ . Freund and Schapire [8, Theorem 11]

Tshow that the training error of the combined hypothesis final of ADABOOST.M2 is bounded by

¦ ¦a

c 
1


1  4 ~2 

a

c R
 1 exp   2

 ~!2

c
a2

) )1 1
¢Thus, if the ~ 's are bounded away from 1 2, then training

error goes to zero exponentially fast. Note that, although the

Tweak hypotheses are evaluated with respect to pseudoloss,
the final hypothesis final is analyzed with respect to the usual error measure.
Freund and Schapire also give a method of bounding the generalization error of the combined hypothesis, but, more recently, Schapire et al. [18] have come up with a better analysis of voting methods such as ADABOOST.M2. Their analysis yields bounds on the generalization error in terms of the  ~ 's, the number of training examples, and a measure of the complexity of the weak hypothesis space (and independent of the number of rounds of boosting).

2.2 OUTPUT CODING AND PSEUDOLOSS

We are now ready to describe the new hybrid algorithm.

Returning to Figure 1, suppose that a weak hypothesis

#  d £ ¤ ¥: 0 1 has been computed with respect to some

"  d £ ¤ ¥coloring :

0 1 . As mentioned earlier, the binary

# ©classification of on an examc ple canc be viewed as a vote

U # © & " Ufor the labels for which a

a , or, said differently,

V#these labels are identified as "plausible" by . Therefore,

in reducing to the pseudoloss setting, it is natural to identify
#§ #4with the soft hypothesis ~ defined by:

§# © & " #§ 3© & U  4# 3© & " GU Q~ a c

¤" 1 a a §c c

£¥

c
:a

¥c
a

Given this choice of soft hypothesis, the update of the
cdistribution ~ is defined for us already by ADABOOST.M2. ! "We will see that and can in turn be defined sensibly 1in terms of ~ .

4

soybean-small 50 40 30 20 10

35 30 25 20 15 10
5

iris

glass 80
60
40
20

audiology 80 60 40 20

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

500. 0

5 10.

50. 100.

0 500.

5 10.

50. 100.

500.

soybean-large

vehicle

vowel

segmentation

80 60 80 70 50 60
60 40 60 50

40

30

40

40 30

20

20

10

20

20 10

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

0 500.

5 10.

50. 100.

500. 0

5 10.

50. 100.

500.

splice 40

satimage 70

letter

60 80

30 50 60

20

40 30

40

10 20 20 10

0

5 10.

50. 100.

0 500.

5 10.

50. 100.

0 500.

5 10.

50. 100.

500.

AdaBoost.OC AdaBoost.M2 ECOC Bagging Arc-x4

Figure 5: Comparison of several learning methods using FINDDECRULE as the weak learner.

4#It will be important below to relate the pseudoloss of ~ # #to the error of . The pseudoloss of ~ can be computed

owu£ 4#fshi©Unearg©§de12ti9c¡hf&&&f£ e&e5ra3f s2o" ¥¤5 l 1221fUlroaca4#~o101w21£ ©m&# a¢ic ©§n¦ t3a£iiig¡h©"fffciacn¦©""" t6caada¦ l2oi&c5aaa ¤cfuc6U£aUl"§ctt6ccc&ah5ec ¥tse&&&5ia¤¦ "o #§ci~nf"§##£oca:©©¦raU# r©§aaaac ¦e©§2©U ¦cccc b¡iit¦cc6£ns#ld&&5a3aiibn2a3c¤""©ecaU lot©cecraGaG sUUr&ecc i,cfta"thnoeda nU cc£ ot¦ ©hl aoe2 rc i(2n3t& hg)

!relabeled example. A convenient choice for turns out to be the following:

L¡ et0( )
Eq.

¥1&
(3),

#~%~a3¡'2'cs£(0 pa3)&2s3ac1e2 cu¡& d¦i12os¢  ¡ ¡Rl0R0to()f()h(s1¡e)1s ¢ ~R1£it¡¢ sr©¡ ¡faaa 2fi2 n1¤P ¢c ~U¡i1 Rn~cf¡ ¡agf2 a1¤~2eUa31 P¤r~2cUra¤ aoc2U1ab¡r¤Pc2 U,¤P¦ocUaa¡fcn2¡bb¡¤d# U©a©rca2 .ae2P¤ 2 c¤U P¤ UaTcU §clcQhlc ethna,t

%&
from

& §¥ %1 ¨¥2

a

1



c¦

(4)

¥Y % 1by definition of , and . Thus, with this definition

1 % §#of , the pseudoloss ~ of ~ can be expressed simply in

% #§ % & ¢terms of the error of . Setting

1 2   , Eq. (4)

% R& ©¥ Qbecomes

~

1 2





# %So, if has error slightly better than the random guessing ¢ ¢ § #error rate of 1 2, then the pseudoloss of ~ also will be

R¥slightly better than 1 2, provided that

0.

The resulting algorithm, called ADABOOST.OC, is shown in Figure 3. By our method of derivation, this algorithm is

4#in fact a special case of ADABOOST.M2 in which the weak
soft hypothesis ~ has a particular form. Therefore, we can

immediately apply the results of Freund and Schapire [8] to

obtain the following theorem, which is the main theoretical

result of this paper:

Q Q@Q " ¤ Q Q@Q ¤ "Theorem 1 # ¤ ¤#and let 1 % c& ¢returned by

Let the

1

be

any

be any sequence of colorings sequence of weak hypotheses

w eak learner. Let

1 2   be the

#error of with respect to the relabeled and reweighted

¥data on which it was trained (as in Eq. (1)). Let be as

Tin Figure 3. Then the training error of the final hypothesis final of algorithm ADABOOST.OC is bounded by

¦ ¦ Qc   ©¥ ©¥a  1
)  )1

1  4a 

c2

c R
a  1 exp  2

a

1

c2

5

soybean-small 5

10

48

36

24

12

00

5 10.

50. 100.

500.

soybean-large

15 40

10 30 20
5 10

00

5 10.

50. 100.

500.

splice 10

8 15
6 10
4 25

00

5 10.

50. 100.

500.

iris

5 10.

50. 100.

vehicle

5 10.

50. 100.

satimage

5 10.

50. 100.

40 30 20 10 500. 0
80 60 40 20
0 500.
8 6 4 2 0 500.

glass

5 10.

50. 100.

vowel

30 20 10
0 500.

5 10.

50. 100.

letter

3 2 1 500.0

5 10.

50. 100.

500.

audiology

5 10.

50. 100.

segmentation

500.

5 10.

50. 100.

500.

AdaBoost.OC AdaBoost.M1 ECOC Bagging Arc-x4

Figure 6: Comparison of several learning methods using C4.5 as the weak learner.

The proof of this theorem follows directly from Eq. (2)

and the argument given above on the relationship between

the error of the weak hypotheses and the pseudoloss of the

associated weak soft hypotheses.

We will
" ¢ ¥ ¢coloring

show which

below several gives a value

omfethod  s

of 1

choosing a 2 (possibly

¥Y &in expectation). Plugging

1 2 into the bound in

Theorem 1 gives a bound of

¦ ¦a

c  
1


1  2 

a



c
1 exp  

1R 
2

2

) )1 1

on the training error. Note that this bound approaches zero exponentially fast whenever  is bounded away from zero.

Thus, if the weak learner can perform just slightly better

than random guessing on the binary problems on which it
§is trained (so that all the  's are lower bounded by some
 0), then the training error of the final hypothesis can

quickly be made arbitrarily small.

Although, for simplicity, we have focused only on the

training error, the generalization error can also be bounded

using the methods of Schapire et al. [18]. This leads to a

bound on the generalization error of the combined hypoth-

esis of the form

¦ ©¥ ©¥a

c 
1

a 1  2

c

1¡

2 "£¢

¦
a1

2

c 1¡ 2¥¤ ¢

3) ¢ ¦ ¢1 §

3¨ ¨ ¢¦ ¨`

 ¦1

loga

©¨ c ¦¨ a  loga
2

¨ ¥c c
¦

 c  1¡ 2

log a 1

§ 
which¨ holds for all 0 with probability at least 1  .
Here, is the VC-dimension of the weak hypothesis space
¨used by the weak learner. The second term will be small when the sample size
is sufficiently large relative to the VC-dime nsion. And, as
©¥with the training error, for small values of , the first term
drops to zero exponentially fast whenever  is bounded away from zero. Details omitted for lack of space.

2.3 CHOOSING THE COLORING
"It remains then only to show how to choose a coloring . "From Theorem 1, it is clear that we want to choose to

maximize

& (  2 PU £ "   6&5 " U ¦ Q¥ R R ¢  ¡ 0 ) 1
Yf 1 "¥Note that the value of

¤~ a c

c a

c a

depends only on ~ and

c
a5 and not

on the weak hypothesis. This means that we can attempt

" ¥to find maximizing prior to calling the weak learner.

"Here, we propose a number of options for choosing . c " GUThe simplest option is to choose each value a uni-

5 £ ¤ ¥formly " GU U &5 "  ¢ U & Ulabelc
GUa

a¥ nd

independently at

a

.



c

Then for any is exactly 1

random from 0 1 for each
 , the probability that
2. Therefore, the expected

¥ ¢value of is also exactly 1 2.

"A slightly more refined method is to choose at ran-

"dom but ensuring a (near) even split of the labels. That is,
we choose uniformly at random among all colorings for

6

soybean-small

50

40

30

20

10

0 0.15 0.2

0.3

0.5

soybean-large

0.7

35 30 25 20 15 10
5 0
0.2

80 60 40 20
0 0.2 0.5

40 30 20 10
0 1

2

12 splice
5 10.

60 50 40 30 20 10
0 5 10. 20. 0.2
70 60 50 40 30 20 10
0 20. 50.

iris glass

0.3 0.5 vehicle

70 60 50 40 30 20 10 0.7 0

0.2

0.5 1 vowel

80 60 40 20
0 2

0.5 1

2

satimage

80 70

60 60 50

40 40 30

20 20

10

5

0 0.2 0.5 1 2

5 10. 0

letter

80

60

40

20

0

5 10. 20. 50. 100. 200.

10. 20. 50. 100. 200. 500.1000.

audiology

0.5 1

5 10.

segmentation

12

5 10. 20. 50.

AdaBoost.OC

AdaBoost.M2

Figure 7: Comparison of computation time versus error rate achieved by ADABOOST.OC and ADABOOST.M2 using FINDATTRTEST as the weak learner.

¦ ¢ 

which exactly
¢ U¢ &¦5 U " U &5 " GUbe shown then

2¡ tchat,¦

of if

the

labels are
 , §c tc hen

mapc ped
a

to

0.
a

c It can with

probability a 1 2 a 1 1 a  1 so the expected value of

¥ ¢will also be this value, which is slightly better than 1 2.

This is the method used in the experiments in Section 3.

¥YA last method is to attempt to use combinatorial opti-
mization methods to maximize . Eq. (5) can be rewritten

¥ Y& R

£ " Ua c9&5

" GU ¦ c §¦ ©a

GU©a ¤PU  c

£  ¢  ¥¤ ¡

GU U & ¡ f 0( )  32 U £ U &   ¦¤¦
where a

c

¤1 ~ a c 

. Written in this

¥form, it is straightforward to show that maximizing is a

special case of the "MAX-CUT" problem, which is known

to be NP-complete [14], but for which various, rather so-

phisticated approximation methods are also known [10, 11].

We did not attempt to use any of these methods, and it is

plausible that one of these might improve performance.

Y¥ ¢In addition, various greedy
also be used which guarantee

h  ill1-cl2i.mEbxinpgermimeethnotsdusscinang

these methods were attempted, but did not give significant

improvement over those reported in Section 3 (details not

reported).

3 EXPERIMENTS
We tested our method experimentally on a collection of eleven multiclass benchmark problems available from the

repository at University of California at Irvine. 2 Some of the characteristics of the benchmarks used are summarized in Table 1. If a test set was already provided, experiments were run 20 times and the results averaged (since many of the learning algorithms used are randomized). If no test set was provided, then 10-fold cross validation was used and rerun 10 times for a total of 100 runs of each algorithm.
For the weak learner, we used three algorithms of varying degrees of expressiveness. The first and simplest, called FINDATTRTEST, outputs a hypothesis that makes its prediction based on the result of a single test comparing one of the attributes to one of its possible values. For discrete attributes, equality is tested; for continuous attributes, a threshold value is compared. The best hypothesis of this form which minimizes error or pseudoloss can be found by a direct and efficient search method. These weak learners are similar in spirit to those studied by Holte [12].
The second weak learner, called FINDDECRULE, outputs a hypothesis which tests on a conjunction of attribute-value comparisons. Such a rule is built up using an entropic potential as in C4.5 and then pruned back using held-out data. This method is based loosely on the rule-formation part of Cohen's RIPPER algorithm [3] and Fu¨rnkranz and Widmer's IREP algorithm [9].
The algorithms FINDATTRTEST and FINDDECRULE are described in more detail by Freund and Schapire [7]. Note
2URL: http://www.ics.uci.edu/¨ mlearn/MLRepository.html

7

soybean-small 35
50 30
40 25

30 20
20 15 10
10 5

0 0.1

0.2 0.5 1 soybean-large

2

0 0.1 0.2

60 80
50

60 40

40 30 20
20 10

0

0.5 1

5 10.

0 50.100.

1

splice 40

70

60 30 50

40 20
30

10 0 12

5 10. 20.

20 10 50. 100. 200. 0

10.

iris

0.5 1 vehicle

2

10. satimage

100.

100.

1000.

glass 80
80 60
60
40 40
20 20

0 0.1
80 60 40 20
0

0.5 1

5 10.

vowel

1 10. 100. letter

0 50.100. 0.1
70 60 50 40 30 20 10 1000. 0 1

80

60

40

20

0 10.

100. 1000. 10000.

audiology

0.5 1

5 10. 50.100.

segmentation

10. 100. 1000. AdaBoost.OC
AdaBoost.M2

Figure 8: Comparison of computation time versus error rate achieved by ADABOOST.OC and ADABOOST.M2 using FINDDECRULE as the weak learner.

that both algorithms find a single rule for the entire problem, as opposed to learning one rule per class.
The last weak learner tested is Quinlan's C4.5 decisiontree algorithm [16], with all default options and pruning
turned on. Also, rather than modify C4.5 to handle weighted
examples, on each round of boosting, we reran C4.5 on
(unweighted) examples which were randomly resampled
according to . We first compared ADABOOST.OC to boosting (with-
out ECOC), i.e., to ADABOOST.M2 for FINDATTRTEST and FINDDECRULE, and, for C4.5, to the error-based multiclass version of ADABOOST called ADABOOST.M1. Since our purpose was to derive an algorithm as effective as boosting but one that only requires an error-based (rather than pseudoloss-based) weak learner, we then compared our algorithm to various other methods which combine errorbased weak hypotheses. These were:
  Dietterich and Bakiri's ECOC method [4]. However, rather than searching for an error-correcting code, we
"chose the "output code" at random by selecting each to be a random (nearly) even split (as described in Section 2.3). Such a random code is highly likely to have error-correcting properties, but it is certainly plausible that a more carefully designed code would perform better than the results reported here.
  Breiman's [1] "bagging" algorithm, which reruns the weak learner on randomly chosen bootstrap samples.

  Breiman's [2] "Arc-x4" algorithm, which, like ADABOOST, adaptively reweights the data, but using a different rule for computing the distribution over ex-
¢amples in a manner that does not require weak hy-
potheses with error less than 1 2. This algorithm was shown by Breiman to mimic the performance of ADABOOST when combined with CART, but its theoretical properties are unknown.
Each algorithm was run for 500 rounds. The results
©are shown in Figures 4, 5 and 6. The -axis shows the number of rounds, and the -axis shows the test error of
each algorithm (in percent). Note the log scale used in all figures.
For C4.5, there does not seem to be any advantage to using ADABOOST.OC over ADABOOST.M1, and in some cases, such as "audiology," the performance is actually worse for ADABOOST.OC. We also do not expect time savings in this case since we are comparing to ADABOOST.M1 which is already error-based.
For the other, less expressive, weak learners, we see that across all datasets, both ADABOOST.OC and ADABOOST.M2 quickly match the performance of the other three errorbased methods, and are usually clearly superior in performance.
Round for round, ADABOOST.OC is often unable to keep up with ADABOOST.M2. However, the savings in computation time can be very significant since error-based algorithms are usually faster than pseudoloss-based algorithms.

8

Figures 7 and 8 show a plot of computation time (in seconds) versus test error rate for these two algorithms. (The time figures used include both training time and evaluation on test data.) These plots show that, when computation time is limited, the performance of ADABOOST.OC can often surpass that of ADABOOST.M2.
4 CONCLUSION
In conclusion, we have described a new method for training combinations of classifiers that comes with the theoretical guarantees of a boosting algorithm, but, like an output coding algorithm, only requires that the weak learner be capable of handling binary classification problems.
Except for highly expressive weak learners (such as C4.5), the experiments show that this method is a compromise: The test error performance can often, but not always, keep up with pseudoloss-based boosting. On the other hand, the time savings and relative ease of programming the weak learning algorithm can be significant.
ACKNOWLEDGMENTS
Thanks to all those who contributed to the datasets used in this paper.
REFERENCES
[1] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123­140, 1996.
[2] Leo Breiman. Bias, variance, and arcing classifiers. Technical Report 460, Statistics Department, University of California at Berkeley, 1996.
[3] William Cohen. Fast effective rule induction. In Proceedings of the Twelfth International Conference on Machine Learning, pages 115­123, 1995.
[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263­286, January 1995.
[5] Harris Drucker and Corinna Cortes. Boosting decision trees. In Advances in Neural Information Processing Systems 8, pages 479­485, 1996.
[6] Yoav Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256­ 285, 1995.
[7] Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148­156, 1996.
[8] Yoav Freund and Robert E. Schapire. A decisiontheoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, To appear. An extended abstract appeared in EuroCOLT'95.
[9] Johannes Fu¨rnkranz and Gerhard Widmer. Incremental reduced error pruning. In Machine Learning: Pro-

ceedings of the Eleventh International Conference, pages 70­77, 1994. [10] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the Association for Computing Machinery, 42(6):1115­1145, November 1995. [11] Oded Goldreich, Shafi Goldwasser, and Dana Ron. Property testing and its connection to learning and approximation. In 37th Annual Symposium on Foundations of Computer Science, pages 339­348, 1996. [12] Robert C. Holte. Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11(1):63­91, 1993. [13] Jeffrey C. Jackson and Mark W. Craven. Learning sparse perceptrons. In Advances in Neural Information Processing Systems 8, pages 654­660, 1996. [14] R. M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations, pages 85­ 103. Plenum Press, 1972. [15] J. R. Quinlan. Bagging, boosting, and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 725­730, 1996. [16] J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993. [17] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197­227, 1990. [18] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. In Machine Learning: Proceedings of the Fourteenth International Conference, 1997.

9

