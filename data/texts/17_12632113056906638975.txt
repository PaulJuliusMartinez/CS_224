A deep look at contemporary operating systems finds data
flow principles of value to computer networks large and small.

Operating Systems Principles for
Data Flow Networks
Peter J. Denning Purdue University

L 1970 the term computer network conjured up
images of computing centers in different cities interconnected with telephone lines. By 1975 the prospect of inexpensive memory and processing showed that the next generation of operating systems at individual computing centers would be concerned much less with memory allocation and processor scheduling; thus the term computer network also began to conjure up images of computing centers internally organized as networks of specialpurpose processors. By 1980 the technology of large-scale integration will permit sophisticated computers to be configured as large arrays of very simple processing elements. The term computer network will then conjure up images of ganged microcomputers working concurrently on many portions of a single computation.
The important components of any network of computers are nodes and data links. The nodes are processing elements (service centers), each with its own private (dedicated) memory and processor units. The data links are communications channels that permit messages to be sent among the nodes. Data links usually have a queue at the receiver to store messages until the receiver is ready to process them. Because each node has its own resources, the only possibility of interaction among nodes is through the data links.
A network in which the nodes are activated by the arrival of messages (specifying tasks) is called a data flow network. A node is initially in an "idle" state. As soon as it receives a set of messages sufficient to specify a task, it becomes "active" and begins work on that task. It may accept yet further tasks while active. It will transmit messages

as it completes tasks, and it will return to the idle state when there are no more tasks.
The concept of a data flow network is at least as old as electronic computing. It is the basis of the "neuron net" model of finite state automata studied by John von Neumann and others in the 1940's and 1950's, and of the "Petri Net" model of concurrent systems, introduced by Carl Adam Petri in the early 1960's. It is the subject of the "program schema" model of computation studied by theoreticians in the late 1960's. It appeared in the form of ready-acknowledge signalling among hardware modules in the Illiac II computer at the University of Illinois in the late 1950's and in the Macromodular Computer Project at Washington University in St. Louis in the early. 1960's. It has been used in operating systems in' the mechanisms by which concurrent programs exchange messages.
Today, computer architects are searching for a solid conceptual basis for designing and controlling networks of computers. This quest resembles the search a decade ago for a conceptual basis of operating systems design. The purpose of this article is to cull from the field of operating systems a few concepts-those of data flow-which may both simplify networks of computers and help prove that they meet their specifications. This paper sketches out our experience with the software aspects of data flow, in a framework for structuring one's thinking about computer networks. (To emphasize the overall structure, the concepts are stated in simple terms-with references to the literature in which the nasty details appear. This is done at the risk of giving the impression that all the problems are understood fully and solved.)

86

0018-9200/78/0700-0086$00.75 (3 1978 IEEE

COM PUTER

The next section reviews the concepts underlying traditional operating systems and the reasons that some of these concepts do not apply in data flow networks. The concepts that do apply work in different ways depending on the scale of the network. Additional sections elaborate how data flow concepts can be used in networks large, medium, and small: dispersed networks connecting many computing centers, local networks of special-purpose processors at a given computing center, and modular computers for data flow programs.
Comparison with traditional operating systems
The traditional framework for an operating system is a single computing center having a limited amount of equipment with which to serve a large number of customers. Figure 1 illustrates that the jobs (programs) P1 P2,. . ., P, submitted by n users are presented concurrently to the computing facility for processing. Each job consists of a sequence of tasks, a task being an interval (burst) of processing on some device in the system. Typically, a job alternates between tasks that use a central processing unit and tasks that perform an input/output operation. For example, Task 1.1 might represent a 100-msec CPU burst, Task 1.2 a 50-msec disk swap, Task 1.3 a 150-msec CPU burst, and so on.
It is easy to conceive of jobs in which some tasks execute in parallel-for example, the job could continue to use the CPU while performing I/O. Unfortunately, present-day programming languages do not allow the user to express the possibility that one task can be concurrent with another; operating systems are typically able to uncover only about 5 percent CPU I/O overlap in jobs. A new kind of language-the dataflow language (discussed later)-does permit the programmer to specify parallel tasks.
The responsibilities of the operating system in the computing center fall in two broad classesallocating the hardware resources such as processors, I/O devices, and main memory, and implementing software resources such as files, processes, and message channels.,

Hardware resource allocation is required in a traditional computing center because the amount of equipment actually needed to process all the tasks of all the jobs far exceeds what is available. Moreover, the equipment itself is very expensive; no user could afford to pay for the entire machine during the period required to process his entire job. The operating system maintains a series of queues for various types of resources and attempts to schedule all the pending tasks so that all the devices are simultaneously busy most of the time. This tends to maximize the number of tasks per second that are completed and thereby to distribute the costs of running the equipment among as many different jobs as possible.
In a traditional operating system, none of the devices contains enough computing power to do any of the scheduling. The scheduling routines, which are activated by interrupt signals from the devices, are run preemptively on the CPU. The high level of preemptions of regular CPU tasks can constitute a considerable overhead.

is

Virtual needed

objects. Software to provide a more

resource implementation
useful and more reliable

programming environment for the user. Software

resources are often called virtual objects by operat-

ing systems specialists-a term signifying that

what appears to the user to be a simple object

may in fact be simulated by a complicated under-

lying mechanism. A file is an example of a virtual

object. It appears to the programmer simply as a

set of records which he may update or read on

command from his program. However, the operat-

ing system must format the records and store

them in various media, it must set up buffers and

provide records

subroutines to control the between main memory

tarnadnsmsiescsoinodnaroyf

memory, and it must protect files from unauthorized

access or accidental damage. Other examples of

virtual objects are

processes-programs in parallel execution; virtual memory-the set of addresses that can be
generated by a process that can exceed available main memory; semaphores-channels through which processes can send signals;

Figure 1. A computing center processing a set of concurrent job streams. July 1978

87

message queues-channels through which processes can exchange messages; and
catalogs-directories to the files.

scheduling routines on the CPU, this type of tends to decentralize its control and to the attributes of a data flow network.

system acquire

By implementing useful virtual objects, an operating system can save the programmer from a considerable amount of programming which he would otherwise have to do himself. By hiding the internal details of the objects and of the task of controlling the devices used to implement them, the operating system can protect its parts from misuse; it can then be more reliable for all users.
A few experimental programming languages

It is a mistake to suppose that operating systems

wtspooounelslidibmidliiintsaiatepesp.etahTreheihfatrrhedemwraaeirnewienrrgeesroseuusrfpfcoiencsiaeilnbltiolchiatatiriedosnw-afrroeer-

software resources-would larger processing capacity

increase because the would attract larger

programs-which objects to achieve

in turn make more modular design.

use

of

virtual

allow the user to declare his own virtual objects. Many operating system designers believe that

It is a mistake to suppose that operating

such languages-and operating systems that support them efficiently-will be essential for the very large data processing problems looming in the next decade.2-5

systems would disappear if there were sufficient hardware . . . hardware cannot
implement virtual objects.

Decentralization. Although it might seem that hardware resource allocation is a simpler responsibility than implementing software resources, there is evidence to the contrary. The SOLO operating system, which dispenses with all forms of dynamic storage allocation, has been completely expressed with 1300 lines of Concurrent Pascal.2 Traditional

Control flow vs. data flow. The realities of traditional computing centers-a small amount of hardware resources relative to the demand for computing, together with the requirement that most scheduling decisions be made by programs run on the CPU-have caused centralization of control to be an important principle in the design of operating

operating systems are typically a thousand times larger. (However, it is unfair to conclude that dynamic storage allocation is the major source of operating system complexity: typical operating systems implement many more kinds of software resources than SOLO.)
Through today's cheap processor and memory technology, adequate supplies of hardware resources can distribute the processing capability among the nodes of the computing center, reduce the amount of scheduling, and eliminate most CPU interruptions. As a result, operating systems can be significantly simplified.
Figure 2 illustrates this concept, suggesting that jobs can now move among many nodes within a computing center. Each node performs one task (using its own private set of hardware resources) and dispatches a request for a job's next task directly to another node capable of performing that task. Because there is less dependence on running

systems. For most of us, programs are sequences of instruc-
tions run in a computing system having one (or a very few) CPU(s). Therefore, control flow is also an important principle, not only of programming but also of operating systems. It is characterized by a fascination with instruction pointers. Much of the programming literature focuses attention on how "control structures" guide an instruction pointer in an orderly way through the program text. Progress in a program is measured by the number of instruction-pointer moves. An operation is activated by moving an instruction pointer to the entry point of a subroutine. Many explanations of concurrency treat process creation as the spawning of new instruction pointers, semaphore signalling as a method of preventing an instruction pointer from passing a certain position until a signal arrives, and mutual exclusion as a restriction on the number of instruction pointers allowed to be within a critical section at the same time.

By contrast, computer networks are based on the

principles of decentralization and data flow. Because

each node has its own set of hardware resources,

nodes can be in different physical locations-there

is no central resource allocator. Because each server

node is activated by the arrival of tasks, progress

is measured by task flows, not by the nurnber of

changes in the positions of instruction pointers. In

this tontext, a given level of concurrency is met by

providing more parallel paths through the network,

not by interrupt-driven central control mechanisms.

Given types of virtual objects are implemented

by the facilities of nodes dedicated to their

management.

Figure 2. Processing concurrent job streams through many nodes Even as there are some fundamental conceptual

of a modular computer center.

differences between networks of computers and

88 COMPUTER

traditional operating systems, there are important the internal configuration of nodes and data links,

similarities. Many operating systems include com- and no matter how many other jobs are present in

ponents that operate on the principle of data flow. the system.

The RC4000 multiprogramming system is an Long term storage is the memory system that

early operating system based on data flow.6 Its allows each user to store files of records indefinite-

nucleus allows users to regard the basic machine ly. It allows him to access these files as needed

as a network of processes connected by first-in- and to share them with other, selected users. The

first-out (FIFO) message links. If there is one send- user may need access to his files from various

er and one receiver for each link, the computation nodes of the network and an inexpensive method

performed by a network of processes is deter- of moving copies from one node to another.

minate.7 Because the operating system automati- Reliability means that the probability that (part

cally stamps each message with the identity of the of) a job is lost through a node or data link failure,

sender, it is possible to have secure data transmis- or through a deadlock, should be small. If a job is

sions in such a network.8-10 The concept of message damaged by a failure in the network, an error

link is implemented as the "queue" data structure message should be returned to the user.

in Concurrent Pascal2 and as the "pipe" in Bell

Labs' UNIX timesharing system."' The principle of organizing the operating system about a data flow network of processes has been extended and refined in the DEMOS operating system for the Cray-i computer. 12

Programming languages which allow the user to declare virtual objects, such
as files, will be essential in the next decade.

Since 1971, queueing network models have been

used with great success in computer performance evaluation. A queueing network is an example of a data flow network connecting independent service centers. Many of the analytic tools can be applied directly to networks of computers in order to predict delays, throughputs, and queue-length distributions, and to determine the capacity of the network for each class of jobs.'3 Many analytic tools have also been developed for the capacity and responsiveness of the communication part of geographically dispersed networks.'4
To summarize, traditional operating systems have been dominated by concepts of centralization and control flow, whereas networks of computers are dominated by decentralization and data flow. Those aspects of operating systems design dealing with centralized resource allocation are useful, at most, inside the nodes of a network of computers. However, the aspects dealing with networks of servers (or processes) are useful for the entire network of computers. These points will be expandled in the next sections for specific classes of networks.

Security means that it is (nearly) impossible for any private data to fall into unauthorized hands. This means not only that each node must secure any information under its care, but also that intelligible copies of messages in data links cannot easily be made and that spurious messages cannot easily be injected onto the data links.
Predictable response means that it should be possible to estimate the important parameters of jobs and nodes, and to use these parameters to calculate, quickly and accurately, performance measures for time periods of interest. Typical measures include the response time for a given job or for classes of jobs, and the task flows by job class on various data links. Sometimes, especially in real-time control systems, it must be possible to complete a job within a given deadline.
These properties are ambitious but within easy reach of today's technology. Some can be met by applying old results in the new context, others by recent innovations.
Because hardware resource. allocation is not an important issue of computer networks, the designer

can concentrate directly on satisfying the five

Five desirable properties

desirable properties in the three classes of networks-dispersed networks of computing centers,

local networks of special-purpose processors, and

Users expect computer systems to behave accord- machines that execute data flow programs. These

ing to minimum standards, irrespective of whether properties can be met by implementing special

these systems are based on central or distributed nodes to manage each kind of virtual object, by

computing, or on control or data flow. The five providing adequate capacity on the data links and

most important properties are job determinacy, at the nodes, and by constraining the manner in

long-term storage, reliability, security, and pre- which the data links are used.

dictable response.

Job determinacy is the requirement that the

system preserve the input/output function specified Dispersed networks

by each program. A job's output should depend

only on the input but not on the relative delays Figure 3 shows a common form of network in

either at the nodes or on the data links. In other which a collection of independent computing cen-

words,- a job should be reproducible in that it gives ters, usually at different locations, is connected by

the same result for the given input no matter what a communications net. Each computing center is

July 1978

89

medium with too many broadcasts), and maintaining secure data communications.
In the Arpanet the objective was to connect existing computing centers with minimal modifications in the (traditional) operating system at each center. A simple approach was adopted. A user at one center can log in, via the net, at another center and thereby gain access to the other center's facilities. In effect, the user submits his transactions to a remote site. It is very difficult to move data files among sites because file formats of one center are not observed at another. In short, it is far cheaper to log in at a distant site than it is to move needed data to the local site for processing.
The five desirable properties are conceptually straightforward to implement in a dispersed network:

Figure 3. A dispersed network of computers.
connected to the net by an interface computer, which translates between the standard formats and conventions of the net and the local formats and conventions of the center. The net itself may include telephone links, microwave links, and satellite links. This is a simple form of data flow network-the communications net supplies data links between any two computing centers, and a computing center is activated by the arrivals of messages specifying jobs.
To avoid long holding times on any communications link, the net is "packet switched." A message is broken into a sequence of equal-size packets for transmission on the net. A packet contains identifiers for the source and destination, a portion of the message, and a sequence number. The packets of a message may follow different paths, arriving out of order at the receiver; but the receiver can reassemble the message in proper order by using the sequence numbers.
The well-known Arpanet was the prototype of this class of networks.'4 The research was broken into three parts-the design of a reliable communications net with bounded delays, the design of the message formats and conventions,* and the design of the interface computers. The recent research for this type of network has concentrated on the communications net-finding the cheapest intercity routes for the links, maintaining reliability through redundant links, maintaining capacity as satellite
radio links are brought on line (it is easy to jam the
*For reasons now lost, these formats and conventions have come to be known as "protocols." However, the dictionary does not support this use of the word.

1. Job determinacy. In a dispersed processing system, a job's determinacy is implemented by standard methods in the operating system at a computing center.7'5 Sequence numbers assure that the packets constituting a message can be put back in proper order on reception. 2. Long term storage. Each computing center has its own file system. A user at a given node may log in at another node and thereby gain access to any of his files stored in the local file system. However, it is inconvenient and inexpensive to move a file to another site for processing. 3. Reliability. This is a function of the error recovery procedures at the computing center that processes the job'5 and also of the redundancy built into the communications net. Multinode deadlocks cannot occur since each job uses only one node. 4. Security. Messages must be encrypted for sending on the net.'7 It is interesting that the concept of signing a message with a secret encryption key 18-19 is similar to the concept of stamping an interprocess message with the identity of the sender.9'0 Individual computing centers must secure each job received from the net; this can be done using standard approaches to operating system security.120'2' It is important to note that communications security and internal computing center security are distinct problems-encryption cannot solve both. 5. Predictable response. I developed analytic
tools exist for calculating thro hputs and delays in the net'4 and in the center at processes a
job 13.22.23
There is increasing interest in using dispersed networks as distributed data management systems,
which would allow a user at one site to pose queries and updates for records throughout the network. This objective attaches much higher importance to the long-term storage facilities of the network as a whole than does the Arpanet. It cannot be met without imposing standard record and file formats throughout the network-hence the various nodes

90 COM PUTER

must have a similar design. Data security is of even greater importance in such a network, for the entire contents of files and records can be transmitted in the open medium; for this reason, it is not yet clear that distributed data management systems are a good idea. In any event, encryption mechanisms will be a standard part of the interface computers.' 18"9

Networks of special-purpose processors

Within a given.computing center, the various

devices make up a local network of special-purpose

processors. Figure 4 illustrates the network for a

timesharing system. The boxes represent network

nodes for various types of service. Each node Figure 4. A queueing network model of a timesharing system.

contains a queue, and is normally active when

tasks are present in its queue. The data links are paths by which jobs "move" from one device to another. Each job is a chain of tasks (requests) for devices.
In such a system, each job is stored in a central memory system. Corresponding to each job is a descriptor, which is a small block of words in main

3. Reliability. Standard methods of tion and recovery can prevent job

error
loss.16

detecDead-

lock can be a problem if a node can be stopped

(blocked) waiting for an action at another node

(see below). However, if no blocking is possible,

there can be no deadlocks.

memory telling where the job's program and data are stored, which task is next to execute, and what device the job is currently using. The data links actually carry simple packets that are enqueued at the receiving devices; each packet points to a job descriptor. To start a task for a job, a device refers to the descriptor for the details of the task; when the task is done, the device updates the descriptor

4. Security. The usual methods of access control can be applied to prevent any task of a job from
asynsteumn.a8'u9t'h21orized access to some object in the
5. Predictable response. As noted above, there are many easy-to-use analytic tools for answering questions about performance and system capacity.

to indicate completion, and it sends a message to the device required for the next task of the job.
Because of the central memory, no job is actually sent on the data links. The data transmission costs are negligible and the same data formats apply automatically throughout the system. These networks, therefore, have a different character from dispersed networks-but they may be components of dispersed networks.
This type of network is usually called a queueing network. A rich set of analytic tools for such networks exists. These tools allow the analyst to predict throughputs, utilizations, delays, and queue-length distributions at the various devices for various classes of jobs.1322 They allow finding bottlenecks under given assumptions about the frequencies with which jobs generate tasks and the speeds of the devices.1323
If the resources of a computing center can be organized into a queueing network, the five desirable properties are conceptually easy to meet:
1. Job determinacy. Each job is a linear chain of tasks. As long as the tasks are completed one at a time in the specified order, there is no danger that the results can depend on the relative speeds of the devices.

Traditional operating systems do not exhibit

all these convenient properties, since the require-

ment to reallocate dynamically some "passive"

resources, such as memory, among the nodes ruins

the assumption that the devices be independent.

For example, a device processing a given task

might stop to wait for buffer space to be released

by a second task at another device. A deadlock

can result if the second task can also wait for an

action of the stopped first task. Our analytic tools

do not handle networks in which one device can

block progress of another. Special deadlock pre-

vention procedures must be added to a system to

avoid deadlock limited passive

haamrodnwagreacrteisvoeurtceass.k7s'1"competing

for

Cheaper technology may remove some, but not

all, of the threat of interdevice blocking and dead-

lock. We will be able to dedicate to each device

all the passive hardware resource it needs. How-

ever, contention for virtual objects-especially

shared files and records-will still remain; it cannot

be avoided by any amount of memory. To prevent

this danger of deadlock, the internal data com-

imnuCnshioecmaaepteihroinetsreacrahcmnhooiclnaoglgystthwrieulclttuaarsleks.o7s"'aolflojwobmsormeusatmbbietipouuts

2. Long term storage. The central memory system shared by the jobs is a suitable repository for files and records. The design of such systems is well understood.

systems to be configured as queueing networks. For example, a network of processes, such as supported in the RC4000 and the DEMOS systems, could be built as a queueing network wherein each

July 1978

91

"device" would be a sophisticated service center. One can envisage service centers for communicating with users, compiling programs in various languages, managing files, and initiating new jobs-every important service of a traditional operating system could be realized as a node in such a network. Automatic reconfiguration of processing and memory resources, and of data links, would enable the system to adapt to the computational demands placed on it.
Queueing network. models are based on the assumption that each job forms a linear chain of tasks. A less restrictive model of a job, based on a partially ordered set of tasks, allows for concurrent execution of some tasks. (In Figure 5, for example, tasks 3, 4, and 5 could be executed in parallel. Task 6 cannot be started until 3, 4, and 5 are finished.) How this affects the network depends on the additional assumptions made about the model. One can view the diagram of Figure 5 as a "dataflow program," from whence it becomes .a dataflow network in which the tasks are operators that "fire" when all operands have arrived on input links (this concept is taken up in the next section). Alternatively, one can view the diagram as a precedence chart specifying the order in which procedures implementing the tasks are called. (This is a control-flow view.) In this case, the tasks read inputs from, and store results into, a central memory; determinacy can be guaranteed only if potentially concurrent tasks have no output location in common with another task's input or output locations.*715
Figure 5. A partially ordered set of tasks.
*This condition of noninterference can be proved to be equivalent to assuming that the sequence of values written in every memory cell is unique-i.e.. independent of the timings of the tasks." This means that the set of tasks is equivalent to a dataflow network whose links correspond to the original memory cells and implement FIFO queueing. In this sense nothing is gained conceptually by the control-flow view of a partially ordered set of noninterfering tasks.

We conclude that the queueing network is a useful scheme for organizing the function processors of a given computing center. It will become more commonplace in future systems, where cheap technology will permit each device to have its own, private set of hardware resources.
Dataflow programs
The two previous sections have considered, first, a large-scale network comprising different computing centers and, second, a medium-scale network comprising the processing devices of a given computing center. This section takes up the possibility of a small-scale network-a dataflow program.
A dataflow program implements a function from input streams of data packets to output streams of data packets. It is represented as a dataflow network in which FIFO data links connect operator nodes. As usual, a node activates as soon as one operand is present on each input data link; it consumes each input packet and produces packets containing results for its output links. Figure 6 shows a dataflow program that evaluates the expression R=A*B + C*D; the next three results that will appear on the output link are 5, 5, and 8. Note that this program is simply the precedence tree of the expression, with queues shown on the links. The queueing permits pipelining-the ability of the network to be at various stages of evaluating different expressions at. the same time. Pipelining enhances the network's ability to process many computations concurrently.
The ultimate purpose of a dataflow program is to give the programmer a natural way of expressing the concurrency inherent in an algorithm, thereby allowing high bandwidth parallel execution. This form of expression can be a much more effective way of representing concurrency than concepts such as "fork" and "join" in control-flow programs.
Figure 6 is an instance of the job model encountered in Figure 5-a partially ordered set of tasks. While this may be an adequate description of jobs, it is inadequate for most programs, which also require operations for selecting cases and for iteration.
Figure 7 shows a scheme for expressing case selection. A packet of the incoming data stream is routed leftward (to subnetwork N,) if the data contained in the packet satisfies condition C; otherwise, it is routed rightward (to subnetwork N2). The split operator also generates "control packets," each containing "T" or "F" to indicate whether the corresponding data packet was sent to N, or
to N2. The merge operator uses the incoming stream of control packets to route data from N, or N2 to the output-e.g., if the next control packet contains "T," the merge operator waits for a data packet from N, whereupon it will consume the control packet and pass the data packet to the output. The split/merge mechanism is needed to cause outputs to emerge in the same order as the

92 COMPUTER

Figure 6. A dataflow program.

corresponding inputs. Without this constraint, the dataflow program would be indeterminate: its results could depend on the relative processing speeds of its-components (e.g., N, and N2).
Figure 8 shows how the split/merge scheme can be used for iteration. Here the control link is initialized with a "T" packet. This iteration is determinate, but it suffers from the limitation that N2 can be used by only one task at a time; no internal pipelining is possible.
Dataflow languages are being developed at MIT,242" at IBM T. J. Watson Research Center,26 at the University of California at Irvine,27 and at IRIA Laboratory in France.26 A compiler of such a language can be envisaged as a translator into a primitive dataflow network which is executed on an interpreter.
Figure 9 is a conceptual view of an interpreter for a dataflow program. The operators and links are encoded and stored in a memory of active storage elements. Whenever an operator node becomes enabled because operands are present on all its input links, its host storage element sends a packet to the extractor network. A packet contains an operation code, the operands, and the addresses of links that are to receive the results. The extractor routes each packet to a function unit for the required kind of operation; the results are fed back through an insertion network and stored in the output data links of the operator. There are several published proposals for asyn-
chronous machines that implement the interpreter through wide bandwidth parallel execution of dataflow programs.24.25.28,29
Because we have little operating experience with dataflow programs and machines, the following remarks about the five desirable properties are more speculative than for other types of networks:

1. Job determinacy. This is assured automatically by connecting operators through FIFO data links and by properly merging data streams previously
split. 25.27.28
2. Long term storage. The cache memory technology can be used to back up the active memory (Figure 9) with a passive memory for long-term storage. The research problem is representing structured data for storage in a high-bandwidth medium accessible to many concurrent operations.25 3. Reliability. Freedom from deadlock can in principle be assured by configuring the network
Figure 7. Selection in a dataflow program.

July 1978

93

remain to be solved, such as pipelined iteration, recursion, processing structured data, security, and
predicting response. It is a fascinating subject.

Conclusion

Figure 8. Iteration in a dataflow program.
Figure 9. A dataf low interpreter.
so that it has properties similar to those of "live and safe" Petri Nets.30 Protection against failure of function units and memory in data flow machines can be achieved through redundant hardware; in one proposal, triple redundancy can be achieved at about 2.8 times the cost of a single machine, and double redundancy at 1.7 times the cost.3' 4. Security. Access controls can verify that dataflow programs refer only to authorized information. The source and destination addresses in packets increase security. 810 The explicitness of dataflow paths may allow better enforcement of information flow policies than is possible for sequential programs.32 5. Predictable response. Instruction processing rates and maximum delays can be calculated easily.33 However, the methods of queueing network analysis must be extended to answer performance questions for given programs on data flow machines. This is an open research area. Dataflow languages, machines, and programs are in a highly experimental stage. Many problems

A network of computers is characterized by a collection of independent processing nodes connected by data links that enqueue unreceived messages. Because the nodes are activated by the arrivals of messages, these systems are examples of dataflow programs.
The traditional context of an operating system is a single computing center in which a small number of resources must be multiplexed among a large number of customers. This context has produced op'erating systems built around a central resource controller. These systems are often dominated by the concept of control flow in their structure.
Despite their orientation around centralized resource control and control flow, many operating systems employ some concepts of dataflow, especially in their methods of exchanging messages among processes. The concepts and theory of these aspects of operating systems may be helpful in designing networks of computers in which the five desirable properties-job determinacy, long term storage, reliability, security, and predictable response-can be implemented in a straightforward way.
The application of these concepts depends on the scale of the network. Table 1 summarizes the five properties against scale. On the large scale, a dispersed processing network deals only with jobs that require processing at a single node. Traditional operating system principles can be used to realize the objectives at the individual nodes and modern communications technology, in the communication net. At present, the high cost of data communications impedes the use of these networks as distributed data management systems.
At the medium scale, a queueing network deals with jobs which are sequences of tasks for individual devices. Cheaper technology is about to make feasible the practice of dedicating to each device all the hardware resources it needs. By allowing simple messages on the data links to refer to complex tasks, a central memory keeps internal data communication costs negligible in these networks. Operating systems, which can be regarded as networks of processes rendering service to users, can be implemented as queueing networks.
At the small scale, a dataflow program is an expression of a dataflow network among the operators (tasks) of a job. The current technology is making feasible the implementation of dataflow machines. These machines can use the many data flow concepts developed over two decades. They can achieve much higher concurrency than is possible in traditional machines.

94 COMPUTER

Job Determinacy Long Term Storage Reliability
Security Performance Prediction

Table 1. Comparison of network classes.

Dispersed Network
Determinate computing centers Sequence-numbered packets

Local Network Determinate devices Jobs = task-chains

File systems at computing centers

Central memory shared by all devices File system

Redundant hardware and software at each computing center Redundant links Multinode deadlocks impossible
Access controls at the computing centers Data encryption in the net
Net delays analyzable Node delays analyzable.

Redundant hardware and software Deadlock avoidance by nonblocking devices
aprnodcebsyshsiiegrnaarlclhiincgal
Access controls in the central memory, assisted by hardware Signing packets with identifiers of senders
Queueing network analysis

Dataflow Program
Determinate operators FIFO data link queues Proper split/merge
operations
Central memory Structured data and virtual objects still an open problem
Redundant hardware an open problem Deadlock avoidance by analogy with live and sate Petri Nets
Access controls Packets signed with source/destination Possible information flow certification
Open problem

This article has suggested a framework for structuring one's thinking about dataflow networks-for letting considerations of software affect the design before the hardware is built. In this brief space it has been impossible to analyze all the ideas fully. The simplified discussions may give the impression that we know all the answerswe do not.
We may be more successful if we build real systems to be more like models
which embody the desired properties and which are tractable.
However, the simplified discussions are very definitely meant to suggest that we may well be more successful by building real systems to be more like models which embody the desired propertiesand which are tractable. This established principle of engineering has been slow to gain acceptance among computer engineers. But, at last, we begin to find programmers who are willing to use structured programming to obtain more understandable programs whose correctness is easier to establish. We begin to find operating systems designers who are willing to restrict process interactions with semaphores or to put processes in deadlockavoiding hierarchies. We begin to find performance evaluators who are willing to impose controls on schedulers in order to make the real system conform to the queueing network model, thereby making its performance questions more easily and accurately answerable.

If this trend continues, mirabile dictu, we might come to understand what our computers are doing! -
Acknowledgments
This work was supported in part by National Science Foundation Grant GJ-41289 at Purdue University.
I appreciate the help of Jack Dennis and John Shore in clarifying some of the concepts reported here, and of Douglas Comer and Herbert Schwetman in reading an early version of the manuscript. I am most deeply grateful to the guest editors, Steven and Svetlana Kartashev, without whose painstaking review and patient suggestions this paper would not be here.
References
1. P. J. Denning, J. C. Browne, and J. Peterson, "The Impact of Operating Systems Research on Software Technology," in Impact of Research on Software Technology (P. Wegner, Ed.), MIT Press, Cambridge, Massachusetts, 1978.
2. P. Brinch Hansen, The Architecture of Concurrent Programs, Prentice-Hall, Englewood Cliffs, New Jersey, 1977.
3. E. Cohen and D. Jefferson, "Protection in the Hydra Operating System," Proc. Fifth ACM Symposium on Operating Svstems Principles, November 1975, pp. 141-160.

July 1978

95

4. A. K. Jones, "The Narrowing Gap Between Language Systems and Operating Systems," Proc. IFIP Congress, North-Holland Publishing Co., New York, pp. 869-873.
5. T. A. Linden, "Operating Systems Structures To Support Security and Reliable Software," Computing Surveys, Vol. 8, No. 4, December 1976, pp. 409-445.
6. P. Brinch Hansen, "The Nucleus of a Multiprogramming System," CACM, Vol. 13, No. 4, April 1970, pp. 238-241, 250.
7. P. Brinch Hansen, Operating Systems Principles,
Prentice-Hall, Englewood Cliffs, New Jersey, 1973.
8. P. J. Denning, "Third Generation Computer Systems," Computing Surveys, Vol. 3, No. 4, December 1971, pp. 175-216.
9. G. S. Graham and P. J. Denning, "Protection: Principles and Practice," AFIPS Conf Proc., Vol. 40, 1972 SJCC, pp. 417-429.
10. B. W. Lampson, "Protection," Proc. Fifth Princeton Conf on Inform. Sciences and Systems, Princeton University, Princeton, New Jersey, March 1971, pp. 437-443. Reprinted in ACM SIGOPS Operating Systems Review 8, 1, January 1974, pp. 192-196.
11. D. M. Ritchie and K. Thompson, "The UNIX Timesharing System," CACM, Vol. 17, No. 7, July 1974, pp. 365-375.
12. F. Baskett, J. H. Howard, and J. T. Montague, "Task Communication in DEMOS," Proc. Sixth ACM Symposium on Operating Systems Principles, November 1977, pp. 23-31.
13. P. J. Denning and J. P. Buzen, "The Operational Analysis of Queueing Network Models," Computing Surveys, Vol. 10, No. 3, September 1978, to appear.
14. L. Kleinrock, Queueing Systems Vol. 2: Computer Applications, John Wiley and Sons, Somerset, New Jersey, 1976, 549 pp.
15. E. G. Coffman, Jr. and P. J. Denning, Operating Systems Theory, Prentice-Hall, Englewood Cliffs, New Jersey, 1973.
16. B. Randell, P. Lee, and P. Treleaven, "Reliability Issues in Computing Systems," Computing Surveys, Vol. 10, No. 2, June 1978.
17. G. J. Popek and C. S. Kline, "Design Issues for Secure Computer Networks," Technical Report, Computer Science Dept., UCLA, Los Angeles, California, January 1978.
18. W. Diffie and M. Hellman, "New Directions in Cryptography," IEEE Trans. on Inform. Theory, Vol. IT-22, No. 6, November 1976, pp. 644-654.
19. R. L. Rivest, A. Shamir, and L. Aldeman, "A Method for Obtaining Digital Signatures and Public Key Cryptosystems," CACM, Vol. 21, No. 2, February 1978, pp. 120-126.
20. P. J. Denning, "Fault Tolerant Operating Systems," Computing Surveys, Vol. 8, No. 4, December 1976,
pp. 359-389. 21. J. H. Saltzer and M. D. Schroeder, "The Protection
of Information in Computer Systems," Proc. IEEE, Vol. 63, No. 9, September 1975. 22. E. Gelenbe and R. R. Muntz, "Probability Models of Computer Systems I: Exact Results," Acta Informatica, Vol. 7, No. 1, 1976, pp. 35-60. 23. R. R. Muntz, "Analytic Modeling of Interactive Systems," Proc. IEEE, Vol. 63, No. 6, June 1975, pp. 946-953.

24. J. B. Dennis and R. P. Misuanas, "A Computer Architecure for Highly Parallel Signal Processing," Proc. ACM Annual Conf., 1974, pp. 402-409.
25. J. Rumbaugh, "A Data Flow Multiprocessor," IEEE Trans. Computers, Vol. C-26, No. 2, February 1977, pp. 138-146.
26. P. R. Kosinski, "A Dataflow Language for Operating Systems Programming," Proc. ACM SIGOPS/SIGPLAN Interface Meeting, SIGPLAN Notices, Vol. 9, No. 4, April 1975, pp. 50-59.
27. Arvind, K. P. Gostelow, and W. Plouffe, "Indeterminacy, Monitors, and Dataflow," Proc. Sixth ACM Symposium on Operating Systems Principles, November 1977, pp. 159-169.
28. A. L. Davis, "The Architecture and Systems Methodology of DDM1: A Recursively Structured Data Driven Machine," Proc. Fifth Annual ACM/IEEE Symposium on Computer Architecture, April 1978, pp. 210-215.
29. G. Kahn and D. B. McQueen, "Coroutines and Networks of Parallel Processes," Ibid, pp. 993-998.
30. A. Plas, "LAU System Architecture: A Parallel Data Driven Processor Based on Single Assign-
ment," Proc. Int'l Conf on Parallel Processing,
1976, pp. 293-302. 31. J. L. Peterson, "Petri Nets," Computing Surveys,
Vol. 9, No. 3, September 1977, pp. 223-252. 32. D. P. Misunas, "Error Detection and Recovery in
a Data Flow Processor," Proc. Int'l Conf on Parallel Processing, 1976, pp. 117-122. 33. D. E. Denning and P. J. Denning, "Certification of Programs for Secure Information Flow," CACM, Vol. 20, No. 7, July 1977, pp. 504-513. 34. D. P. Misunas, "Performance Analysis of a Data
Flow Processor," Proc. ACM/IEEE Int'l Conf on
Parallel Processing, 1976, pp. 100-105.

Peter J. Denning is perhaps best

known for his invention of the working

set concept for
A professor of

mceommopurtyermasncaigeencmeesnta.t

Purdue Unpiversity and a prolific writer,

he has published over 75 papers and

asrytsitcelemss.inHiospebroatoikngwiatnhdEp.Gr.ogCroafmfmmiann,g

Jr., Operating Systems Theory, was

published in 1973; his book with Jack

Dennis and Joseph Qualitz, Machines, Languages, and

Computation, appeared in 1978; and a third book (with

Jeffrey Buzen), on computer performance evaluation is

in preparation.

His research interests range widely: theory of computa-

tion, operating system theory and analysis performance

evaluation, protection and security, and software engi-

neering. He has held many editorial positions: editor-in-

chief of Computing Surveys, editor of the Elsevier/ North-

Holland Series on Operating and Programming Systems,

and associate editor of technical journals.

Denning received the SM and PhD degrees in electrical

engineering from MIT. He is a member of ACM, a

senior member of IEEE, and a member of the IEEE

Computer Society. He is vice-president of ACM until

July 1980.

96 COMPUTER

