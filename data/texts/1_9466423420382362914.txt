JOURNAL OF COMPUTER SCIENCE AND ENGINEERING, VOLUME 4, ISSUE 1, NOVEMBER 2010 
 
 

1 

A NOVEL TRUST MODEL FOR 

CHATTERBOTS 

Ram Gopal Raj 

Abstract— Trust models are quite commonplace today and have been adapted to a variety of security applications. 
Conversational bots or chatterbots also have considerable popularity. However, trust as a determinant variable in security has 
yet to be adapted for use in a chatterbot. In this paper, we look at the various trust models and concepts used in Public Key 
Infrastructure and Identity Management. Then using concepts from the existing trust models, we present a novel trust model 
that utilises trigonometric identities as trust and distrust curves as well as some examples of how trust can be used to determine 
whether a statement is true or false, should other factors, such as the logical integrity or the informational correctness of the 
statement, be unavailable or not clearly defined. Using this trust model would allow a chatterbot to learn with an additional filter, 
which if used correctly can result in chatterbot conversation believability. 

Index Terms— Chatterbot, Distrust, Trust, Trust Model  

——————————      —————————— 

1  INTRODUCTION

C

HATTERBOT  is  the  current  popular  culture  name 
for conversational systems or bots which are com-
puter  programs  built  to  enact  “intelligent”  con-
versations  with  one  or  more  human  users  via  speech 
or more commonly these days tele-text, [1]. It is gener-
ally  agreed  that  conversation  with  humans  requires  a 
tremendous amount of knowledge and that attempting 
to account for all such knowledge is difficult if not im-
possible. A good example of an attempt to represent a 
huge degree of human knowledge is CYC, [2]. The on-
tological complexity of accounting for so much know-
ledge  before  hand  is  a  prohibitively  difficult  pro-
gramming  task.  As  such,  more  and  more  focus  has 
been  placed  on  the  usage  of  machine  learning  tech-
niques in allowing chatterbots to give increasingly ac-
curate responses over time. Machine learning general-
ly involves three phases training, validation and appli-
cation  according  to  Finlay  and  Dix,  [3].  The  training 
phase involves introducing a system to a training set of 
“correct  behaviour”.  The  training  set  is  analysed, 
usually  via  statistical  methods  and  a  standard  repre-
sentation  of  the  new  knowledge  is  stored.  The  repre-
sentation of the new knowledge becomes a set of rules 
by  which  future  responses  are  composed.  These  rules 
are validated and if the rules are incomplete  or incor-
rect,  then  additional  “training”  can  be  given.  The  ap-
plication phase involves the actual usage of the learnt 
knowledge for conversation. Learning models focus on 
the  information  itself  without  regard  to  the  informa-
tion’s source, i.e. the person who gave the information. 

 
Different Chatterbots learn based on pattern match-
ing,  Bayesian  progression  or  by  information  dissemi-

———————————————— 

Ram Gopal Raj is with the Department of Artificial Intelligence, Universi-

ty of Malaya, 50603 Kuala Lumpur, Malaysia.  

 

 

nation, [4], but no weight is given to who is giving the 
information  and  how  trustworthy  that  individual  is. 
The ability to learn is indispensable to a chatterbot at-
tempting to remain relevant or even entertaining for a 
prolonged period of time. Even simply providing rea-
listic  human-like  responses  requires  learning.  Howev-
er,  though  many  learning  techniques  and  knowledge 
representation schemes exist, there is no way in which 
a  chatterbot  or  any  other  intelligent  agent  can  judge 
the  acceptability  of  a  comment,  statement  or  piece  of 
information based on the amount of trust it has in the 
individual  offering  that  piece  of  information.  There-
fore, we present in this paper a model of trust compu-
tation  for  a  chatterbot,  based  loosely  on  those  used  in 
Public  Key  Infrastructure  (PKI).  The  concept  of  trust 
has been used in PKI for some time. It is well defined 
and involves the computation of trust values as well as 
distrust  values  during  the  determination  of  an  indi-
vidual’s  trustworthiness  and  risk  assessments.  Some 
research  defines  trust  as  a  time  weighted  concept 
where  more  recent  experiences  whether  positive  or 
negative are given greater importance in the trust val-
ue calculation. 

2  CREATING A CONVERSATIONAL TRUST MODEL 

The  variables  involved  in  a  PKI  trust  system  and  a 
conversational  trust  system  appear  broadly  similar. 
Trust  values  in  different  PKI  trust  models  however 
tend  to  use  various  quantifiers  as  their  basis.  The  ad-
vantages  and  disadvantages  of  the  various  models  as 
well  as  the  path  construction  techniques  are  covered 
in, [5] and [6]. Some PKI trust models work on the as-
sumption  that  each  node  in  the  path  has  an  identical 
“level  of  risk”,  [7]  and  [8].  In  the  conversational  envi-
ronment, the initial level of risk of any new user can be 
identical, but it is unlikely that it will remain as such as 
social  trust  fluctuates  with  experience.  Our  model  for 

© 2010 JCSE 

http://sites.google.com/site/jcseuk/ 

 

2 

 

conversational  trust,  utilizes  comparative  trust  values 
between users as a gauge of the truthfulness of a given 
statement.  Here,  our  model  differs  greatly  from  PKI 
trust  models  which  use  one  user’s  or  node’s  trust  of 
another’s  in  determining  whether  to  trust  a  given 
node. In PKI it is possible for a chain of trust to be built 
up,  often  referred  to  as  a  certification  path,  and  the 
length of the “certification path” is an important varia-
ble  in  calculating  the  level  of  risk.  Reiter’s  and  Stub-
blebine’s model, [9], takes into account the proportion 
of  independent  paths  as  a  metric  in  measuring  risk. 
Our  model  on  the  other  hand  uses  cumulative  trust 
generated  by  supporting  statements,  i.e.  if  two  indi-
viduals  who  are  highly  trusted  have  said  the  same 
thing  and  a  distrusted  individual  has  said  something 
to the contrary, one is likely to believe the statement of 
the  two  trusted  individuals.  Setting  values  for  the 
states  of  “trust”,  “distrust”  and  “no  trust”  is  also  an 
important consideration. The models in, [10], [11] and 
[12], utilize a representation of between 0 and 1 for the 
values  of  “trust”,  “certificate”  and  “recommendation” 
within the certificate chain. The values between 0, “no 
trust”,  1,  “complete  trust”  and  -1,  “complete  distrust” 
are  used  by  Marsh,  [13].  We  see  Marsh’s  values  for 
trust  as  being  more  applicable  in  the  conversational 
context since a trust scale between only 0 and 1 has its 
shortcomings, because the scale ending in 0 creates an 
ambiguity  as  to  the  true  meaning  of  0  as  it  could  be 
interpreted as either “no trust” or “distrust”, [14].  To 
avoid  such  ambiguity  the  model  in,  [15],  denotes  0  as 
complete  distrust,  1  as  complete  trust  and  0.5  as  no 
trust.  The  logical  conditions  and  semantic  structures 
for trust and its effects, all of which are logical aspects 
being  used  in  a  formalization  of  trust  in  PKI  trust 
models are discussed in [16], [17] and [18]. Use of sub-
jective  logic  in  representing  the  concept  of  “uncertain 
belief” as shown in, [19], and the concept of “uncertain 
trust” as shown in, [20], are concepts that we noted but 
do  not  adapt  into  this  current  conversational  trust 
model.  We  acknowledge  that  “uncertain  trust”  poten-
tially exists in the conversational context, but we infer 
that if one’s trust  of a statement is uncertain, then the 
statement  simply  has  a  lesser  or  lower  level  of  trust. 
Therefore  we  use  a  scale  of  trust,  no  trust  (neither 
trusted  nor  distrusted)  and  distrust,  where  the  trust 
value  of  a  statement  could  fall  anywhere  in  the  scale, 
therefore  offering  varying  degrees  of  trust  or  distrust 
as  a  method  for  taking  into  account  the  “uncertain 
trust” of a statement.  

 
Whatever  similarities  that  do  exist  between  PKI 
trust  models  and  the  conversational  context  is  due  to 
the  comparable  dynamics  of  the  social  environment 
within  which  both  situations  exist.  In  this  study,  we 
explicitly  assume  that  the  system  or  chatterbot  will 
converse with one user at a time. However, the differ-
ent  users  that  do  interact  with  the  system  at  different 
times  are  taken  to  be  the  different  nodes  in  the  trust 
chain. Therefore a node in a PKI network is a user in a 
conversation.  From  a  conversation  perspective,  trust 

certificates  are  not  available,  so  we  used  the  track 
record of the user during his/her interactions with the 
system  moderated  over  time  as  a  trust  quotient,  TQ. 
Each  time  the  conversational  trust  value  is  calculated, 
the trust quotient is a moderating variable. Time decay 
is used in, [21], but we utilize the saved trust quotient 
which  is  updated  with  each  interaction  between  the 
system  and  a  user,  thereby  eliminating  the  need  for 
time based decay of trust. 

 
Huang  and  Fox,  [18],  show  trust  chains  formed  in 
serial  or  sequence  forms  and  parallel  forms.  Their 
work relies on the trust of a node of another node as a 
moderator, whereas we use each user’s track record as 
a moderator as well as the trust of one user to another. 

 
Based  loosely  on  the  formalization  of  trust  pre-
sented in, [22], we present a trust model for conversa-
tions.  We  utilize  the  concept  of  trust,  distrust  and  no 
trust, but with a difference in that the bounds are mod-
ified from 1, -1 and 0 to π, -π and 0 respectively. 

 
According  to  Shapiro,  [23],  gaining  initial  trust  or 
distrust requires greater effort than increasing existing 
trust or distrust. Therefore we infer that, once an initial 
state  of  trust  or  distrust  has  been  established,  with 
each consecutive action affirming the state of trust, the 
degree  of  trust  increases  by  ever  greater  margins. 
However, should the concept of absolute trust and ab-
solute distrust exist, then as the degree of trust or dis-
trust  approaches  the  absolute  margin,  the  required 
effort  to  achieve  an  increased  degree  of  either  state 
becomes greater. 

 

Fig. 1. A Graphical Illustration of the Growth of Trust for an Individual.  

 

 
Fig.  1  shows  the  concept  in  graphical  format,  the 
Positive  Trust  Quotient,  PTQ,  is  a  representation  of 
trust  while  the  Negative  Trust  Quotient,  NTQ,  is  a  re-
presentation of distrust. As can be seen, in region EC < 
-1,  initial  trust  or  distrust  requires  a  larger  number  of 

 

 

3 

conversational experiences, EC as compared to increas-
ing trust, region EC > -1. As trust or distrust increases, 
in  region  -1  <  EC  <  1,  the  number  of  affirming  EC  re-
quired  to  increase  the  degree  of  trust  decreases  until 
the absolute state, denoted by π, for trust, and – π, for 
distrust, is approached where the number of affirming 
EC required to increase trust becomes greater again. An 
individual telling the truth is a Positive Conversational 
Experience and has a positive value while an individ-
ual  telling  a  lie  is  a  Negative  Conversational  Expe-
rience and has a negative value. Therefore, EC can have 
negative  values  and  is  not  limited  to  only  natural 
numbers.  The  values  given  for  EC  and  TQ  will  be  ex-
plained later. 

 

Fig. 3. The Effect of Consecutive Postive Conversational Expe-
riences.  

 

For  the  purpose  of  simplicity,  we  are  not  focusing 
on the magnitude of each individual positive or nega-
tive  experience,  i.e.  “was  it  a  big  lie  or  a  small  lie”  is 
not  a  consideration.  To  reiterate,  it  is  not  simply  the 
total  number  of  positive  or  negative  experiences  but 
the  number  or  consecutive  positive  or  negative  expe-
riences,  PCC  and  NCC  respectively.  Based  on  the  con-
secutive count concept, we obtain Fig. 3 and Fig. 4. 

 

 

Fig. 2. Illustration of arctan and arccot curves.  

 
 
The representation of trust, shown in  Fig. 1, can  be 
modified into trigonometric functions since the curves 
for PTQ and NTQ correlate to the curves for arctan and 
arccot, [24], as seen in Fig. 2. The correlation is the rea-
son why the scale for EC and TQ are as such. The orien-
tation of the curves was shifted to allow the difference 
between PTQ and NTQ to be clear. As per previous re-
search, [13] and [18], trust or PTQ has a positive value 
and distrust or NTQ has a negative value. 

 

 

2.1 Consecutive Positive or Negative 
Conversational Experiences 
PKI trust models do not account for consistency of pos-
itive or negative trust states for a given node. Howev-
er, in conversation, we infer that the more consistently 
an  individual  gives  us  positive  experiences  the  more 
weight  we  place  on  each  subsequent  positive  expe-
rience and vice versa. For example, if a person “tells the 
truth”  once  and  then  “lies  three  times”,  we  are  most 
likely to think of the individual as untrustworthy since 
he or she has lied more of the time - “lies all the time” 
relative to the one time the individual told the truth.  
 

 
 

Fig. 4. The Effect of Consecutive Postive Conversational Expe-
riences.  

 

Fig. 5. Sine Wave Curve Segment.  

 

 

 
 

 

 

4 

As shown in Fig. 3 and Fig. 4, PCC and NCC have an 
increasing  effect  on  the  conversational  experience 
modifiers,  EC  add  and  EC  minus,  up  to  a  certain  degree. 
Once again the curves correlate to the curve of a trigo-
nometric function, in this case a sine wave, although it 
only correlates to a segment of the sine wave as in Fig. 
5, [24]. 
 

2.2  Trust Quotient 
Using the graph of the sine wave segment in Fig. 5, we 
raise  the  graph’s  y-axis  by  1  and  raise  the  graph’s  x-
axis by π/2 to produce the graph representing EC add in 
Fig. 3. We obtain: 

(
π

)
/ 2  

=

(
  sin  

(

PC
C

)

)

 

+    (1) 

 1

 

  sin  

 

+

C add

E
 
 
Therefore: 

E

C add

 

=


  sin 







PC
C

–  

π

2









 

+

 1 

where                    0  
(cid:1)

≤

PC
C

≤

 180
(cid:1)

(2) 

 
 
 
 
 
 
 

 

≤

≤

E

 2

C add

and                 0 
Next, using the graph of the sine wave segment in Fig. 
5,  we  lower  the  graph’s  y-axis  by  1  and  lower  the 
graph’s  x-axis  by  π/2 
the  graph 
representing EC minus in Fig. 4. We obtain: 
 
E
C
 
 
Therefore: 
 

to  produce 

(
  sin  

)
/ 2  

  sin  

NC
C

  –  1

(
π

 (3) 

  min

−

=

)

(

)

us

 

 

 

2

≥

+

=

≥

=

E

E

π

 0

(4) 













C minus

C minus

≥ −

− ≥

NC
C

NC
C

  –  1 

  180
(cid:1)

 arctan 


  sin 



where                    0  
(cid:1)

 
 
 
 
 
 
and                  2 
The  graph 
for  PTQ  in  Fig.  1  is  obtained  by  raising  the  arctan  x 
graph’s y-axis shown in Fig. 2 by π/2. So: 
 
PT
Q
 
 
EC is the sum of EC add and EC minus. So: 
 
PT
Q
 
 
Finally,  since  this  is  an  update  function,  we  need  to 
take into account the previous PTQ value so: 
 
 

 arctan 

E
C

C minus

(6) 

 
π

 
π

(5) 

/ 2

/ 2

C add

   

E

E

=

+

+

+

(

(

)

)

 

 

 

 

 

 

PT

Q

=





arctan tan









PT

Q previous

 

−





π

2

+

E

C add

 

+

E

C minus

 

 





+

π

2

where                    0  
(cid:1)

<

PT
Q

<

 180
(cid:1)

and                  0  
(cid:1)

<

 

PT

Q previous

 

<

 180
(cid:1)

(7) 

 
The graph for NTQ in Fig. 1 is obtained by lowering the 
arccot x graph’s y-axis shown in Fig. 2 by π. So: 
 
NT
Q

 arccot 

  –  

E
C

(8) 

π

=

)

(

 

 

 

 
 
EC is the sum of EC add and EC minus. So: 
 
NT
Q

 arccot 

C minus

C add

E

E

=

−

)

(

 

 

  –  

π

 

(9) 

 
 
Finally,  since  this  is  an  update  function,  we  need  to 
take into account the previous NTQ value so: 
 
 

(10) 

 

 

 

 

 

NT

Q

=

(

arccot cot

(

(

NT

Q previous

 

+

)
π

−

E

−

E

C minus

 

C add

 

)

)

–   

π

where                       0  
(cid:1)

>

NT
Q

> −

  180
(cid:1)

and                  0  
(cid:1)

>

NT

Q previous

 

> −

  180
(cid:1)

 

 

 

 

=

+

(11) 

PT
Q

NT
Q

 
 
Trust  Quotient,  TQ,  is  represented  by  the  sum  of  PTQ 
and NTQ. So: 
 
T
Q
 
Please note in this paper unless mentioned, π = 180˚; 
Our  model  for  trust  in  conversation  includes  the  fol-
lowing rules in pseudo code: 
 
 
IF (new Individual) 
THEN PTQ = 45˚, Update NTQ = -45˚;  
 
ELSE  
THEN retrieve PTQ value, retrieve NTQ value; 
  
 
IF (PCC > 180˚) 
THEN EC add = 2;  
 
IF (NCC < -180˚) 
THEN EC minus = -2;  
 
 

 

© 2010 JCSE 

http://sites.google.com/site/jcseuk/ 

 

 

5 

IF  (previous  EC  ==  positive  AND  current  EC  ==  nega-
tive) 
THEN PCC = 0 ˚, NCC++, Update PTQ, Update NTQ; 
 
ELSE IF (previous EC == negative AND current EC == 
positive)  
THEN PCC ++, NCC = 0 ˚, Update PTQ, Update NTQ;  
 
ELSE IF (previous EC == negative AND current EC == 
negative)  
THEN NCC ++, Update PTQ, Update NTQ;  
 
ELSE  IF  (previous  EC  ==  positive  AND  current  EC  == 
positive)  
THEN PCC ++, Update PTQ, Update NTQ;  
 
 
 
IF (TQ == 0) 
THEN current individual has “no trust”; 
 
ELSE IF (TQ < 0) 
THEN “distrust” individual; 
 
ELSE IF (TQ > 0) 
THEN “trust” individual; 
 
 
 

3  USING THE TRUST MODEL 

The  following  is  a  pseudo  code  representation  of  a 
possible method for determining a truth or a lie. Please 
note,  if  more  than  one  individual  has  given  the  same 
information, the value of TQ is the sum of the TQ ’s of 
all  the  individuals  who  gave  the  same  information. 
Example  2  and  Example  3  in  the  “Proof  of  Concept 
Section” is an example of a scenario where a contradic-
tion may arise not  with one but rather two sources  of 
existing information. This scenario is dealt with utiliz-
ing the cumulative trust of all the individuals. The cu-
mulative TQ for a given piece of information is the sum 
of the TQ of all  the  individuals who “agree” with  that 
piece of information minus the TQ of all the individuals 
who “disagree” with that piece of information. 

 
The method of determination of whether two pieces 
of  information  are  conflicting  is  provided  by  Raj  in 
[25].  This method requires that Raj’s information dis-
semination technique, [4], and knowledge/information 
representation  scheme  be  utilized.  Since  the  explana-
tion  of  the  information  conflict  determination  is  pro-
vided  in  [25],  for  simplicity’s  sake,  we  are  assuming 
that it is being used in the following pseudo code. 

 

 
IF (New Information == Existing Information) 
THEN 

SAVE  New  Information  Individual’s  identity 

with the others who said the same. 
 
Update  PTQ  and  NTQ  for  all  individuals  in-
volved  (all  individuals  are  deemed  to  have 
told the truth, EC = positive) 

 
IF (Information contradicts Existing Information) 
THEN 
 
New Information Cumulative TQ) 
THEN  
 
 
 
trusted) 

IF  (Existing  Information  Cumulative  TQ    >  

(The  new  information  is  not  to  be 

SAVE New Information for future ref-
erence  and  comparisons  but  Existing 
Information is taken as the Truth. 
 
Update  PTQ  and  NTQ  for  Existing  In-
formation  Individual  with  EC  =  posi-
tive.  (The  individual(s)  who  gave  the 
Existing information is(are) deemed to 
have told the truth.) 
 
Update  PTQ  and  NTQ  for  New  Infor-
mation Individual with EC  = negative. 
(The  individual(s)  who  gave  the  new 
information  is(are)  deemed  to  have 
told a lie.) 
 
 

 
 
< New Information Cumulative TQ) 
 

THEN  

ELSE  IF  (Existing  Information  Cumulative  TQ 

information 

(The  new  information  is  trusted  and 
the  existing 
is  disre-
garded) 
SAVE New Information for future ref-
erence  and  comparisons  and  New  In-
formation is taken as the Truth.  
 
Update  PTQ  and  NTQ  for  New  Infor-
mation  Individual  with  EC  =  positive. 
(The  individual(s)  who  gave  the  new 
information  is(are)  deemed  to  have 
told the truth.) 
 
Update  PTQ  and  NTQ  for  Existing  In-
formation  Individual  with  EC  =  nega-
tive.  (The  individual(s)  who  gave  the 
Existing information is(are) deemed to 
have told a lie.) 
 

 
 
== New Information Cumulative TQ) 
 

THEN  

ELSE  IF  (Existing  Information  Cumulative  TQ 

(Both  the  new  and  existing  informa-
tion have equal merit and are deemed 
to be true until future interactions) 

 

6 

 

SAVE New Information for future ref-
erence  and  comparisons  and  New  In-
formation is taken as the Truth.  
 
Do NOT Update PTQ and NTQ for any 
Individuals.  (Since  there  is  no  conclu-
sive  difference  between  the  TQs  of  ei-
ther  pieces  of  information,  no  deter-
mination of truth or lie could be made 
so no update is done.) 

 
 
IF (New Information == NEW) 
THEN 

SAVE New Information for future ref-
erence  and  comparisons  and  New  In-
formation  is  taken  as  being  unproven 
as a truth or a lie.  
 
Do NOT Update PTQ and NTQ for any 
Individuals.  (No  update  is  performed 
since  the  New  Information  has  yet  to 
be proven as true.) 

 

 

4  PROOF OF CONCEPT CASE STUDY 

(EXPERIMENTAL RESULTS) 

Our  case  study  deals  with  a  conversation  involving  a 
set  of  statements  or  pieces  of  information.  The  imple-
mentation  of  the  case  study  was  performed  using  10 
individuals  and  1000  randomly  generated  conversa-
tion  instances,  three  of  which  are  shown  below.  The 
reason for using the term “Statement” is that no logical 
or  grammatical  or  informational  processing  of  any 
kind is taken into account in this case study. Our case 
study  involves  a  scenario  where  Trust  Quotients  are 
used  as  the  determining  variable  of  whether  an  indi-
vidual has lied or is telling the truth. In other words it 
is a case of “the individual’s word versus the others”. 
There  are  10  individuals,  each  with  a  different  TQ,  in 
the  case  study.  The  TQ  for  the  individuals  range  from 
highly  trusted  to  highly  distrusted.  The  individuals 
and  their  respective  PTQ  and  NTQ  values  are  as  fol-
lows: 
 
(Highly Trusted Individual)Thomas's:    
PTQ = 178.67280711926713  
NTQ = -1.268438974469781 
 
(Trusted Individual)Richard's:          
PTQ = 169.29260094495942 
NTQ = -7.812230259502256 
 
(Distrusted Individual)Harold's:        
PTQ = 7.8122302595022575  
NTQ = -169.29260094495933 
 
(Highly Distrusted Individual)Uriah's:  

 

PTQ = 1.2684389744698024  
NTQ = -178.67280711926702 
 
(New Individual)Jane's:                 
PTQ = 45.0  
NTQ = -45.0 
 
(Distrusted Individual)Jack's:          
PTQ = 44.938989388624314 
NTQ = -45.06114082122718 
 
(Distrusted Individual)Jill's:          
PTQ = 44.60592498586619 
NTQ = -45.39957135509295 
 
(Trusted Individual)David's:             
PTQ = 46.26915489277626  
NTQ = -43.784677716840584 
 
(Trusted Individual)Abraham's:             
PTQ = 45.02182304059452 
NTQ = -44.9781935708407 
 
(Trusted Individual)Benjamin's:             
PTQ = 45.24088141944876  
NTQ = -44.761127095267916 
 

The  interactions  between  the  10  individuals  were 
performed  as  a  randomized  set  of  “agreements”  and 
“disagreements” over arbitrary “statements”. Since the 
determination of truth in this case study is entirely de-
pendent on TQ, the contents of the statements are irre-
levant.  When  calculating  TQ  Statement,  if  an  individual 
previously agreed with the statement, then his/her TQ 
is added to TQ Statement, if an individual previously disa-
greed with the statement, then his/her TQ is subtracted 
from  TQ  Statement.  Owing  to  the  need  for  brevity,  we 
present as examples three randomly selected instances 
from the set of 1000 instances used in the simulations. 
The following is the first instance.  

 

4.1  Example / Instance 1 
David  (Trusted  Individual)  agrees  with  a  statement 
that: 
 Thomas (Highly Trusted Individual) agrees with; 
 

Therefore, the TQ for the Statement is: 

TQ  Statement = 0 + Thomas's ((PTQ = 178.67280711926713) 
+ (NTQ = -1.268438974469781)) 
         = 177.40436814479736 
 
While  David's  TQ  =  46.26915489277626  + 
43.784677716840584) 
          = 2.484477175935673 
 
Since David's TQ is Positive, David is trusted, and Da-
vid  agrees  with  a  statement  that  has  a  Positive  TQ, 
therefore  David  and  everyone  who  agreed  to  the 
statement told the truth: 

(-

 

7 

 

    

     = (sin (1˚ – π/2)) + 1 

 
David told the Truth: 
 
It was the first truth so, PCC = 1 and NCC = 0. 
 
Using (2), EC add David = (sin (PCC David – π/2)) + 1 
 
 
 
 
 
Using (4), EC minus David = (sin (NCC David + π/2)) – 1 
 
 
 
 
 
Using (7),  
PTQ  David  =  (arctan  (tan  (PTQ  David  previous  -  π/2)  +  EC  add 
David + EC minus David)) + π/2 

       = (sin (0˚ + π/2)) – 1 

     = 1.523 x 10-4 

       = 0 

    

    

    

 

 

 

 
 =  (arctan  (tan  (46.26915489277626  -  π/2)  + 
1.523 x 10-4 + 0)) + π/2 
 
 = 46.273711687020146 

 
Using (10),  
NTQ David = (arccot (cot (NTQ David previous + π) - EC add David 
- EC minus David)) – π 

 
 = (arccot (cot (-43.784677716840584 + π) - 1.523 
x 10-4 - 0)) – π 
 

 = -43.78049986630108 
 
so  David's  PTQ  is  now  =  46.273711687020146  ,  from 
46.26915489277626 
and  David's  NTQ  is  now  =  -43.78049986630108,  from  -
43.784677716840584 
 
Thomas told the Truth: 
so  Thomas's  PTQ  = 
178.67280711926713 
and  Thomas's  NTQ  =  -1.2684346982716193,  from  -
1.268438974469781 
 

178.6728118007125, 

from 

Since  David  is  a  trustworthy  individual,  and  he  is 
agreeing  with  a  statement  that  a  Thomas,  highly 
trustworthy  individual  agreed  to,  the  statement  must 
be true and both Thomas and David told the truth. In 
addition,  as  Thomas  is  already  a  highly  trusted  indi-
vidual, with a TQ approaching absolute trust (Absolute 
trust  is  a  state  where  PTQ  =  180  and  NTQ  =  0)  the  TQ 
gain  that  he  obtained  from  the  update  was  small.  Let 
us look at the next simulation instance. 
 

4.2  Example / Instance 2 
Jack (Distrusted Individual) disagrees with a statement 
that: 

 

(-

Jack's  TQ  = 

44.938989388624314  + 

 Richard (Trusted Individual) previously agreed with; 
 Uriah (Highly Distrusted Individual) previously disa-
greed with; 
 
Therefore, the TQ for the Statement is: 
 TQ Statement = 0 + Richard's ((PTQ = 169.29260094495942) 
+ (NTQ = -7.812230259502256)) 
 -  Uriah's  ((PTQ  =  1.2684389744698024)  +  (NTQ  =  -
178.67280711926702)) 
         = 338.8847388302544 
 
While 
45.06114082122718) 
          = -0.12215143260286254 
 
Since  Jack's  TQ  is  Negative  and  the  Statement  has  a 
Positive  TQ,  Jack  cannot  be  trusted  and  anyone  who 
disagreed with the statement lied: 
 
Jack told a Lie: 
so  Jack's  PTQ  is  now  =  44.93463580000377,  from 
44.938989388624314 
and  Jack's  NTQ  is  now  =  -45.06551367862685,  from  -
45.06114082122718 
 
 Richard told the Truth: 
 so  Richard's  PTQ  =  169.29290216602058, 
169.29260094495942 
 and  Richard's  NTQ  =  -7.812069031957503,  from  -
7.812230259502256 
 
 Uriah told a Lie: 
 so  Uriah's  PTQ  = 
1.2684389744698024 
 and  Uriah's  NTQ  = 
178.67280711926702 
 

1.2684346982716335, 

-178.6728118007124, 

from 

from 

from 

-

Jack is an untrustworthy individual, and since Jack 
is  disagreeing  with  a  statement  that  a  trustworthy  in-
dividual,  Richard,  agreed  to  and  an  untrustworthy 
individual, Uriah, disagreed to, Jack cannot be trusted. 
Since Jack and Uriah, who are both untrustworthy dis-
agreed  with  the  statement,  but  Richard,  who  is  trust-
worthy, agreed with the statement, the statement must 
be  true.  Our  final  instance,  involves  Jane,  who  is  an 
individual  who  the  system  has  never  interacted  with, 
and therefore has no trust. 
 

4.3  Example / Instance 3 
Jane (New Individual) disagrees with a statement that: 
 Abraham  (Trusted  Individual)  previously  agreed 
with; 
 Jill (Distrusted Individual) previously disagreed with; 
 
Therefore, the TQ for the Statement is: 
 TQ Statement = 0 + Abraham's ((PTQ = 45.02182304059452) 
+ (NTQ = -44.9781935708407)) 
 -  Jill's  ((PTQ  =  44.477158816367925)  +  (NTQ  =  -
45.53256045683494)) 

8 

 

         = 1.0990311102208352 
 
While Jane's TQ = 45.0 + (-45.0) 
          = 0.0 
 
Since Jane's TQ is Zero, and has no trust, and Disagreed 
with  a  Statement  with  Positive  TQ,  Jane  cannot  be 
trusted.  The  statement  has  a  Positive  TQ,  so  the  state-
ment  is  true,  therefore  everyone  who  agreed  with  the 
statement told the truth: 
 
Jane told a Lie: 
so Jane's PTQ is now = 44.995637119883234, from 45.0 
and  Jane's  NTQ  is  now  =  -45.00436354465515,  from  -
45.0 
 
 Abraham told the Truth: 
 so  Abraham's  PTQ  =  45.02618990925714, 
45.02182304059452 
 and  Abraham's  NTQ  =  -44.973834011695374,  from  -
44.9781935708407 
 
 Jill told a Lie: 
 so 
44.477158816367925 
 and 
45.53256045683494 
 

-45.64381226021988, 

44.37033773829743, 

Jill's  NTQ  = 

from 

-

from 

Jill's 

PTQ 

= 

from 

Jane  has  become  a  slightly  distrusted  individual 
due  to  her  disagreement  with  a  statement  that  was 
agreed to by a trusted individual, Abraham, and disa-
greed  to  by  a  distrusted  individual,  Jill.  Since  Jane’s 
action  was  similar  to  that  of  a  distrusted  individual, 
Jane has now become distrusted herself. 

5  EVALUATION OF THE MODEL 

Our  evaluation  test  is  based  on  a  ‘restricted  Turing 
Test’ as described in [1]. Since we were evaluating the 
difference  that  the  trust  model  makes  to  a  learning 
chatterbot’s performance, we required a baseline learn-
ing chatterbot that we could implement the trust mod-
el  in.  Therefore,  the  model  was  evaluated  by  imple-
menting it in the chatterbot presented in [25]. 
 

A  total  of  113  participants  each  performed  eight  5 
minute  tele-text  conversations,  four  with  the  basic 
chatterbot (no trust model implemented) and four with 
the  upgraded  chatterbot  (with  the  trust  model  imple-
mented). The conversations were not held concurrent-
ly  but  rather  independently  and  at  random  intervals.  
The  participants  were  asked  to  judge  each  conversa-
tion with either chatterbot for believability. The partic-
ipants were not informed about the type of chatterbot 
they were conversing with, basic or upgraded. In order 
to  maximise  the  odds  that  conflicting  information  be 
given to the chatterbot, the participants were requested 
to only converse about the subject of “Is the Prolifera-
tion  of  Nuclear  Power  a  Threat  to  Global  Security” 
since  it  is  a  highly  contentious  issue.  The  participants 

rated the chatterbot’s believability based on the follow-
ing questions: 
 

Q1. How  convincing  were  the  chatterbot’s  res-
ponses?  1  (not  convincing  at  all)  to  5  (very 
convincing) 

 

 

 

 

 

 

 

 

 

Q2. To what degree did the chatterbot maintain fo-
cus on the subject of conversation? 1 (very low 
focus) to 5 (very high focus) 

Q3. What  degree  of  accuracy  did  you  perceive 
from  the  answers?  1  (not  accurate  at  all)  to  5 
(very accurate) 

Q4. How illogical were the arguments? 1 (not illog-

ical at all) to 5 (very illogical) 

Q5. How inconsistent were responses given? 1 (not 

inconsistent at all) to 5 (very inconsistent) 

Q6. How  logical  were  the  arguments?  1  (not  logi-

cal at all) to 5 (very logical) 

Q7. How  unconvincing  was  the  chatterbot?  1  (not 

unconvincing at all) to 5 (very unconvincing) 

Q8. Rate  the  degree  to  which,  the  chatterbot  was 
consistent with its answers. 1 (not consistent at 
all) to 5 (very consistent)  

Q9. How  inaccurate  were  the  chatterbot’s  an-
swers? 1 (not inaccurate at all) to 5 (very inac-
curate) 

Q10. 

How  easy  was  it  to  mislead  the  chat-

terbot? 1 (not easy at all) to 5 (very easy) 

 
The questions required that the respondents rated the 
chatterbots based on five criteria:  
 

1.  convincingness  of  responses  (question  1,  af-

firm,  and question 7, contradict),  

2.  consistency  (question  5,  contradict,  and  ques-

3. 

tion 8, affirm),  
focus  on  the  subject  of  conversation  (question 
2, affirm, and question 10, contradict),  

4.  accuracy  of  responses  (question  3,  affirm,  and 

question 9, contradict), and  

5.  whether  the  answers  were  logical  (question  4, 

contradict, and question 6, affirm).  

 
The  believability  rating,  BR,  of  the  chatterbots  was 
measured as the mean of the rating of the five factors. 

 
 
 

 

 

9 











(

(

(

(

(

Q1 rating 

−

 Q7 rating

Q8 rating 

−

 Q5 rating

)

)

+

+

Q2 rating 

−

 Q10 rating

Q3 rating 

−

 Q9 rating

Q6 rating 

−

 Q4 rating

)

)

)

+

+











5

BR

=

 

 
 
 
 
 
 
(12) 
 
 
 
 

The order in which the ratings are subtracted is im-
portant since half the questions affirm the criteria and 
the  other  half  contradict  the  criteria.  It  is  possible  to 
obtain  a  negative  value  for  the  Believability  Rating 
since some respondents may find the chatterbot to not 
be believable. 

 
The mean Believability Rating for the basic chatter-
bot was 2.3 while the mean Believability Rating for the 
upgraded chatterbot was 3.5. 

 

6  DISCUSSION AND CONCLUSION 

Our model generally assumes that when a conflict oc-
curs, the new information  will conflict the existing in-
formation  entirely.  We  acknowledge  that  this  is  a  ra-
ther  simplified  view,  which  rules  out  scenarios  where 
a  contradiction  may  arise  not  with  one  but  rather  2 
sources  of  existing  information.  For  example  suppose 
existing  information  includes  information  "P"  from 
source 1, and information "Q" from source 2, and sup-
pose  new  information  from  source  3  comes  in  "not 
both P and Q". Then there is no contradiction between 
source  3  and  source  1,  and  no  contradiction  between 
source 3 and source 2. But there source 3's information 
still  contradicts  the  information  from  1  and  2  taken 
‘together’.  This  scenario  requires  further  study  and 
tweaking of the model as well as the method used for 
determining the contradictions that exists between two 
pieces of information. A possible solution is to create a 
weighted  modifier  that  is  computed  by  determining 
the degree of the contradiction of the existing informa-
tion. Another potential general assumption our model 
makes is, if source 1 is trusted more than source 2 then 
the  trust  value  of  source  1  will  increase,  ‘even  if’  the 
difference between the trust values of the two sources 
is  minutely  small,  say  0.000000001.  In  such  a  case  it 
might make more sense to commit to neither the exist-
ing information nor the new information. Use of trust 
for  moderating  learning  is  still  a  new  concept,  and  as 
such  many  potential  weaknesses  as  well  as  uses  for 
this model could yet to have been found. These issues 
will be the subject of future work in the area. 
  

 The case study shows that our model could be used 
as  an  additional  streamline  for  learning  chatterbots  to 
not  just  filter  information  logically  or  grammatically 
but  also  in  terms  of  the  trust  a  chatterbot  has  in  indi-

viduals  from  whom  the  information  is  received. 
Though an accuracy of at least eight decimal points is 
necessary  to  allow  our  model  to  operate  and  update 
correctly,  implementation  should  not  be  an  issue  as 
most programming languages have the required accu-
racy. A major difference from previous trust models is 
that  each  individual  who  the  system  meets  must  be 
remembered, saved as a name or serial number, along 
with  that  individual’s  respective  PTQ  and  NTQ  values 
for  future  evaluation.  Another  important  variable  for 
the  system  to  remember  is  from  whom  each  piece  of 
information came from, since some lies or untruths are 
not known as such at the time at which they are learnt, 
but revealed to be so in consequent conversations with 
other individuals. The fact that TQ is continuously up-
dated  reflects  well  with  the  conversational  context 
since  trust  is  always  changing,  either  being  earned 
through trustworthy behaviour such as giving accurate 
information  consistently,  or  being  lost  through  un-
trustworthy behaviour such as lying. 

 
There  may  be  some  contention  as  to  the  validity  of 
our evaluation since our survey only had 10 questions 
or criteria for participants to judge the performance of 
the  chatterbots.  Even  the  duration  of  time  the  partici-
pants had with the chatterbots may be an issue. How-
ever, since our ratings are based on a ‘restricted Turing 
Test’ as in [1], the duration of 5 minutes per conversa-
tion is standard in such a test and generally judges are 
only  requested  to  state  how  convincing  the  subject 
was, in other words one yes/no question. We therefore 
wish  to  emphasize  that  our  evaluation  of  the  chatter-
bots’  respective  believability  ratings  was  up  to  the 
standards  of  a  ‘restricted  Turing  Test’.  Based  on  this, 
we conclude that the participants found the chatterbot 
with the trust model built in to be more believable. 

 
Therefore use of this trust model can help a learning 
chatterbot  to  achieve  a  higher  level  of  believability 
than  the  same  chatterbot  without  the  model  imple-
mented. 

ACKNOWLEDGMENT 

The  authors  wish  to  thank  Mr.  Gopal  Raj  K.  K.  Pillai, 
B.Sc.,  Dip.  Ed.,  MBA.  Director  Communications  Consult-
ing,  (Ret.),  Vaishnavi  Corporate  Communications,  Pte. 
Ltd.  New  Delhi,  India.  His  assistance  during  the  evalua-
tion  of  the  chatterbots  by  providing  participants  from 
different backgrounds was indespensable. This work was 
supported  in  part  by  a  grant  from  The  Institute  of  Re-
search  Management  and  Monitoring,  University  of  Ma-
laya. 

REFERENCES 

[1]  Mauldin,  M.  (1994),  "ChatterBots,  TinyMuds,  and  the  Turing 
Test: Entering the Loebner Prize Competition", Proceedings of the 
Eleventh National Conference on Artificial Intelligence, AAAI Press. 
[2]  Lenat,  D.  and  Guha,  R.  (1990).  Building  Large  Knowledge  Based 
Systems: Representation and Inference in the Cyc Project. Addisson 

 

10 

 

[25]  Raj,  R.  G.  (2010).  An  Adaptive  Learning  Tele-Text  Chatterbot.  PhD 

Thesis, University of Malaya. 

 
Ram Gopal Raj PhD. (2010), MSc.(2006), B.Eng (2004) is currently 
a  researcher  at  the  University  of  Malaya.  His  ongoing  research  in-
volves  the  development  of  a  self-learning,  self-adapting  chatterbot 
named RONE. He has published 3 Journal and 4 Conference papers 
on RONE and its development. 
 

Wesley Publishing. 

[3]  Finlay,  J.  and  Dix,  A.  (1996).  An  Introduction  to  Artificial  Intelli-

gence. UCL Press / Taylor and Francis. 

[4]  Raj, R.G. and Abdul-Kareem, S. (2009), Information Dissemina-
tion And  Storage  For  Tele-Text  Based  Conversational  Systems' 
Learning.  Malaysian  Journal  of  Computer  Science,    22(2):  pp  138-
159. 

[5]  Perlman, R. (1999). An overview of pki trust models. IEEE Net-

work, 13: pp 38-43. 

[6]  Linn,  J.  (2000).  Trust  models  and  management  in  public  key 

infrastructures. RSA Laboratories. 

[7]  Elley, Y., Anderson, A., Hanna, S., Mullan, S., Perlman, R. and 
Proctor,  S.  (2001).  Building  certification  paths:  Forward  vs.  re-
verse. In The 10th Annual Network and Distributed System Security 
Symposium. 

[8]  Zhao,  M. and  Smith,  S.  W.  (2006).  Modeling  and  evaluation  of 
certification  path  discovery  in  the  emerging  global  pki.  In  Eu-
roPKI2006. 

[9]  Reiter, M. K. and Stubblebine, S. G. (1999). Authentication me-
tric  analysis  and  design.  ACM  Transactions  Information  System 
Security, 2(2): pp 138-158. 

[10]  Maurer, U. M. (1996). Modelling a public-key infrastructure. In 
ESORICS  '96:  Proceedings  of  the  4th  European  Symposium  on  Re-
search  in  Computer  Security,  pp 325-350, London,  UK.  Springer-
Verlag. 

[11]  Mui,  L.  and  Halberstadt,  A.  (2002).  A  computational  model  of 
trust  and  reputation.  In  Proceedings  of  the  35th  Hawaii  Interna-
tional Conference on System Sciences. 

[12]  Kamvar,  S.  D.,  Schlosser,  M.  T.  and  Garcia-Molina,  H.  (2003).  The 
eigentrust  algorithm  for  reputation  management  in  p2p  networks.  In 
WWW '03: Proceedings of the 12th international conference on World 
Wide Web, pp 640-651, New York, NY, USA. ACM. 

[13]  Marsh,  S.  P.  (1994).  Formalising  Trust  as  a  Computational  Concept. 

Ph.D. Thesis, University of Stirling. 

[14]  Guha, R. and  Kumar, R.  (2004).  Propagation  of  trust and distrust.  In 
WWW '04: Proceedings of the 13th international conference on World 
Wide Web. 

[15]  Ding,  L.,  Kolari,  P.,  Finin,  S.  G.,  Joshi,  A.,  Peng,  Y.  and  Yesha,  Y. 
(2005).  Modeling  and  evaluating  trust  network  inference.  In  The  Se-
venth International Workshop on Trust in Agent Societies, at AAMAS 
2004. 

[16]  Burrows, M., Abadi, M. and Needham, R. (1990). A logic of authenti-

cation. ACM Transactions on Computer Systems, 8(1): pp 18-36. 

[17]  Demolombe, R. (2001). To trust information sources: a proposal for a 
modal logical framework. In C. Castelfranchi and Y.-H. Tan, editors, 
Trust and deception in virtual societies, pp 111-124. Kluwer Academ-
ic Publishers. 

[18]  Huang, J. and Fox, M. S. (2006). An ontology of trust - formal seman-
tics and transitivity. In Proceedings of The Eighth International Con-
ference on Electronic Commerce, pp 259-270. ACM, 2006. 

[19]  Josang,  A.  (2001).  A  logic  for  uncertain  probabilities.  International 
Journal  of  Uncertainty,  Fuzziness,  and  Knowledge-Based  Systems, 
9(3): pp 279-311. 

[20]  Josang,  A.,  Gray,  E.  and  Kinateder,  M.  (2006).  Simplification  and 
analysis of transitive trust networks. Web Intelligence and Agent Sys-
tems Journal, 4(2): pp 139-161. 

[21]  Bosse,  T.,  Jonker,  C.M.,  Meij,  L.  van  der,  Sharpanskykh,  A.,  and 
Treur, J. 2006. Specification and Verification of Dynamics in Cogni-
tive  Agent  Models.  In  Proceedings  of  the  Sixth  International  Confe-
rence  on  Intelligent  Agent  Technology,  IAT'06.  IEEE  Computer  So-
ciety Press, pp. 247-254. 

[22]  Huang, J. (2007). Knowledge Provenance: An Approach to Modeling 
and Maintaining The Evolution and Validity of Knowledge. PhD The-
sis, University of Toronto, http://hdl.handle.net/1807/11112. 

[23]  Shapiro, S. (1987). Social Control of Impersonal Trust. The American 

Journal of Sociology, 93(3), 623-658. 

[24]  Gullberg, J. (1997). Mathematics: From the Birth of Numbers. W.W. 

Norton & Company, Inc., 500 Fifth Avenue, New York. 

 

