From: AAAI Technical Report SS-94-03. Compilation copyright © 1994, AAAI (www.aaai.org). All rights reserved. 

The  First  Law  of  Robotics

(a  call 
Oren  Etzioni 

to  arms)

Daniel  Weld*

Department  of  Computer Science  and  Engineering

University  of  Washington

Seattle,  WA 98195

{etzioni, weld}@cs.washington.edu

Abstract

than  obeying  human orders. 

Even before the  advent of  Artificial 

Intelligence,  sci-
ence  fiction  writer  Isaac  Asimov recognized 
that  a
robot  must place  the  protection  of  humans from harm
at  a  higher  priority 
In-
spired  by  Asimov,  we pose  the  following  fundamental
questions:  (1)  How should  one formalize  the  rich,  but
informal,  notion  of  "harm"?  (2)  How can  an  agent
avoid  performing  harmful  actions,  and do so  in  a  com-
putationally 
agent  resolve  conflict  between its  goals  and the  need
to  avoid  harm?  (4)  When should  an  agent  prevent 
human from  harming  herself?  While  we address  some
of  these  questions  in  technical  detail, 
the  primary goal
of  this  paper is  to  focus  attention  on Asimov’s concern:
society  will  reject  autonomous agents  unless  we have
some credible  means of  making them safe!

tractable  manner?  (3)  How should 

The Three  Laws of  Robotics:

1. A robot  may not  injure 

a  human being,  or,

through  inaction,  allow  a  human being  to  come
to  harm.

2. A robot  must  obey  orders  given  it  by  human

beings  except  where such  orders  would conflict
with  the  First  Law.
3. A robot  must protect 

its  own existence  as  long
as  such  protection  does  not  conflict  with  the
First  or  Second Law.

Isaac Asimov [,~]:

Motivation

In  1940, Isaac  Asimov stated  the  First  Law of  Robotics,
a  robot  should  not
capturing  an  essential 
slavishly  obey human commands -- 
its 
foremost  goal
should  be  to  avoid  harming humans. Consider  the  fol-
lowing scenarios:

insight: 

*We thank  Steve  Hanks, Nick Kushmerick, Neat Lesh,
and Kevin Sullivan  for  helpful  discussions.  This research
was funded in  part  by Office of  Naval Research Grants 90-
J-1904 and 92-J-1946,  and by National Science Foundation
Grants IRI-8957302, IRI-9211045, and IRI-9357772.

¯ A construction  robot  is  instructed 

to  fill  a  pothole
in  the  road.  Although the  robot  repairs 
the  cavity,
it  leaves  the  steam roller,  chunks of  tar,  and an oil
slick 

in  the  middle of  a  busy highway.

¯ A software agent is  instructed  to  reduce disk  utiliza-
tion  below 90%. It  succeeds,  but  inspection  reveals
that  the  agent  deleted  irreplaceable  IATEX files  with-
out  backing  them up  to  tape.
While less  dramatic  than  Asimov’s stories, 

the  sce-
narios  illustrate  his  point:  not  all  ways of  satisfying  a
human order  are  equally  good; in  fact,  sometimes it  is
better  not  to  satisfy 
the  order  at  all.  As we begin  to
deploy  agents  in  environments  where they  can  do  some
real  damage,  the  time  has  come to  revisit  Asimov’s
Laws.  This  paper  explores  the  following  fundamental
questions:
¯  How  should
"harm"?

notion
and  , we  de-
of 
and  restore 
two domain-
fine  dent-disturb 
independent  primitives 
that  capture  aspects  of  Asi-
mov’s rich  but  informal  notion  of  harm within  the
classical  planning  framework.

one 
In  Sections 

formalize 

the

¯ How  can  an  agent 

harm-

avoid 

performing 

and  extend 

and  do  so  in  a  computationally

ful  actions, 
tractable  manner?  We leverage 
the
familiar  mechanisms of  planning  with  subgoal  inter-
actions  [35,  7,  24,  29]  to  detect  potential  harm in
polynomial  time.  In  addition,  we explain  how the
agent  can avoid  harm using  tactics  such  as  confronta-
tion  and evasion  (executing  subplans  to  defuse  the
threat  of  harm).

goals 

resolve 

¯ How should  an  agent 

between
its 
to  avoid  harm?  We
impose a  strict  hierarchy  where dent-disturb 
con-
straints  override  planners  goals,  but  restore  con-
straints  do not.

the  need 

conflict 

and 

¯ When  should  an  agent  prevent 

In  section 

harming  herself? 
framework could  be  extended  to  partially 
this  question.
The paper’s  main contribution 

to  arms:"
before  we release  autonomous agents  into  real-world

is  a  "call 

a  human  from
,  we  show  how our
address

environments,  we need  some credible  and  computation-
ally 
tractable  means of  making them  obey  Asimov’s
First  Law.

Survey  of  Possible  Solutions

decisions  regarding  which actions
To make intelligent 
are  harmful,  and under  what circumstances,  an  agent
requires  some explicit  model of  harm.  We could  pro-
vide  the  agent  with  an explicit  model that  induces  a
partial  order over world states  (i.e.,  a utility  function).
This  framework  is  widely  adopted  and  numerous  re-
searchers  are  attempting  to  render  it  computationally
tractable 
[13,  31,  34,  32,  37,  17],  but  many problems
remain  to  be  solved  [36].  In  many cases, 
the  intro-
duction  of  utility  models transforms  planning  into  an
optimization  problem --  instead  of  searching  for  some
plan  that  satisfies  the  goal,  the  agent is  seeking the  best
such  plan.  In  the  worst  case,  the  agent  may be  forced
to  examine all  plans  to  determine  which one is  best.
In  contrast,  we have explored  a  satisficing  approach --
our  agent  will  be  satisfied  with  any plan  that  meets
its  constraints  and achieves  its  goals.  The expressive
power of  our  constraint 
language  is  weaker than  that
functions,  but  our  constraints  are  easier  to
of  utility 
incorporate  into  standard  planning  algorithms.

By using  a  general, 

temporal  logic  such  as  that  of
[33]  or  [9,  Ch.  5]  we could  specify  constraints 
that
would ensure  the  agent  would not  cause  harm.  Before
executing  an  action,  we could  ask  an  agent  to  prove
that  the  action  is  not  harmful.  While elegant,  this  ap-
proach is  computationally  intractable  as  well.  Another
alternative  would be to  use  a planner  such as  ILP  [3,  2]
or  ZENO [27,  28]  which supports  temporally  quantified
goals  but,  at  present, 
these  planners  seem too  ineffi-
1
cient  for  our needs.

by  restricting 

Instead,  we aim to  make the  agent’s  reasoning  about
harm more tractable, 
the  content  and
theory  of  injury. 2  We adopt  the  standard
form of  its 
assumptions of  classical  planning:  the  agent  has  com-
plete  information  of  the  initial 
state  of  the  world,  the
agent  is  the  sole  cause  of  change,  and action  execu-
tion  is  atomic, indivisible,  and results  in  effects  which
are  deterministic  and completely predictable.  Section
considers  relaxing 
these  assumptions.  On a  more syn-
tactic 
level,  we make the  additional  assumption  that
the  agent’s  world  model is  composed of  ground  atomic

1We have also  examined previous  work on "plan  qual-
ity"  for  ideas,  but the bulk of  that  work has focused on the
problem of  leveraging a  single  action  to  accomplish multi-
ple  goals  thereby reducing the  number of  actions  in,  and
the  cost  of,  the  plan [20,  30, 39].  While this  class  of  op-
timizations  is  critical 
in  domains such as  database query
optimization, logistics  planning, and others,  it  does not ad-
dress our concerns here.

2Loosely speaking, our approach is  reminiscent of  clas-
sical  work on knowledge representation,  which renders  in-
ference tractable  by formulating restricted  representation
languages [21].

18

formuli.  This  sidesteps  the  ramification  problem [23],
since  domain axioms  are  banned.  Instead,  we demand
that  individual  action  descriptions  explicitly  enumer-
ate  changes to  every  predicate  that  is  affected. 3 Note,
however,  that  we are  not  assuming  the  STRIPS rep-
resentation; 
language
(based  on ADL [26])  which includes  universally  quanti-
fied  and disjunctive  preconditions  as  well as  conditional
effects [29].

Instead  we adopt  an  action 

Given the  above  assumptions,  the  next  two sections

explain  how they  should  be  treated  by  a  generative
planning  algorithm.  We are  not  claiming  that  the  ap-
proach  sketched  below  is  the  "right"  way to  design
agents  or  to  formalize  Asimov’s  First  Law. Rather,
our  formalization 
the  kinds  of
issues  to  which Asimov’s Law gives  rise  and
technical 
....  how they  might be  solved.  With this  in  mind,  the  pa-
per  concludes  with  a  critique  of  our  approach  and  a
(long)  list  of  open questions.

is  meant to  illustrate 

....... 

define  the:  primitives  dour-disturb  and restore,  and

Safety

are  so  hazardous 

Some conditions 
that  our  agent
should  never  cause  them.  For  example,  we might  de-
mand that  the  agent  never  delete  I~TEX files,  or  never
handle  a  gun.  Since  these  instructions 
hold  for  all
times,  we refer 
to  them as  dont-distuxb  constraints,
and say  that  an  agent  is  safe  when it  guarantees 
abide  by  them.  As  in  Asimov’s  Law,  dont-disturb
constraints  override  direct  human orders.  Thus, if  we
ask  a software  agent  to  reduce  disk  utilization 
can  only  do  so  by deleting  valuable  IbTEX files, 
agent should refuse  to  satisfy  this  request.

and it
the

to

We adopt  a  simple  syntax:  clout-disturb 

takes  a
logical  sentence as  argument. For

single,  function-free, 
example,  one  could  command the  agent  avoid  deleting
files 
that  are  not  backed up on tape  with  the  following
constraint:

dont-disturb(written.to.tape(f) 
Free  variables, 

V isa(f, file))
such  as  f  above,  are  interpreted  as
In  general,  a  sequence of  ac-
if  none of  the  actions

universally  quantified. 
tions  satisfies  dont-disturb(C) 
make C false.  Formally,  we say  that  a  plan  satisfies
constraint  when every  consistent,
an  dont-disturb 
totally-ordered, 
sequence of  plan  actions  satisfies 
constraint  as  defined  below.
Definition: 
Let  w0
be the  logical  theory describing  the  initial  state  of  the
world,  let  A1,...,A, 
sequence  of
actions  that  is  executable  in  w0, let  wj be the  theory
the  world  after  executing  Aj  in  wj-1,  and
describing 
let  C be a function.free, 
logical  sentence.  We say  that

be  a  totally-ordered 

of  dour-disturb: 

Satisfaction 

the

3Although unpalatable,  this  is  standard in  the  planning
literature.  For example, a STrtWs operator that  moves block
A from B to  C must delete  on(A,B) and also  add clear(B)
even though clear(z)  could be defined  as  Vy "-on(y, 

satisfies 

AI,...,A, 
if/or  all  j  E [1,  n],  for  all  sentences  C,  and for  all
substitutions  0,

the  constraint  dent-disturb(C)

does  not  require 

/fw0  ~  CO then  w  i  ~  CO 

(1)
Note that  unlike  the  behavioral  constraints  of  [11,
the
12]  and  others,  dent-disturb 
time  interval;
agent  to  make C true  over  a  particular 
rather, 
the  agent  must avoid  creating  any additional
violations  of  C.  For example, if  C specifies  that  all  of
then  dent-disturb(C)
Gore’s  files  be  read  protected, 
commands the  agent 
to  avoid  making  any  of  Gore’s
files  readable,  but  if  Gore’s .plan  file  is  already read-
able  in  the  initial 
the  agent  need not  protect
if  we want
that  file.  This subtle  distinction  is  critical 
to  make sure  that  the  behavioral  constraints  provided
to  an agent  are  mutually  consistent.  This  consistency
problem  is  undecidable  for  standard  behavioral  con-
straints  (by  reduction of  first-order  satisfiability) 
but
is  side-stepped  by our  formulation,  because any set  of
dent-disturb  constraints 

is  mutually  consistent.

state, 

Safe  Plans

through  every  constraint 

Synthesizing 
its  planner  must
To ensure  that  an  agent  acts  safely, 
con-
generate  plans  that  satisfy  every  dent-disturb 
straint.  This  can  be  accomplished  by  requiring 
that
the  planner  make a  simple  test  before  it  adds  new ac-
tions  into  the  plan.  Suppose that  the  planner  is  con-
sidering  adding  the  new action  Ap to  achieve  the  sub-
it  must it-
goal  G of  action  A~. Before it  can do this, 
and
erate 
every  effect  E of  Ap, determining  the  conditions 
(if
any)  under which E violates  C,  as  defined  in  figure  1.
For  example,  suppose  that  an  effect  asserts 
-,P  and
is  dont-disturb(P  V Q),  then  the  ef-
the  constraint 
fect  will  violate  the  constraint 
if--Q  is  true.  Hence,
violation(-~P, 
returns  true  then  the  effect  necessarily  denies the  con-
straint, 
if  false  is  returned,  then  there  is  no possible
conflict,  otherwise  violation  calculates  a  logical  ex-
4
pression  specifying  when a  conflict 

PVQ) = "~Q.  In  general, 

is  unavoidable.

dent-disturb(C) 

if  violation

Before  adding  Ap, the  planner  iterates 

through  ev-
ery  constraint  dont-disturb(C)  and every  effect  con-
sequent  E  of  Ap,  calculating 
violation 
ever  returns  something  other  than  False,
then  the  planner  must perform  one  of  the  following
four repairs:
1. Disavow: If  E is  true  in  the  initial 

violation(E, 

then  there

state, 

C). 

is  no  problem  and Ap may be  added  to  the  plan.

2. Confront:  If  Ap’s effect 

is  conditional  of  the  form
when 5  then  E  then  Ap may be  added  to  the  plan

4If E contains "lifted  variables" [24] (as opposed to  uni-
versally  quantified  variables  which pose no problem) then
violation  may return  an  overly  conservative  R.  Sound-
ness  and safety  are  maintained, but completeness could be
lost.  We believe  that  restoring  completeness would make
violation  take exponential time in  the  worst case.

19

vlolatlon(E,  C)
l. LetR :-{}
2. For each disjunction D E C do
3.
4.

For each literal  e E E do

If  e unifies  with f  E D then add

{’~  I  = ~ (D-  {/})}  to  R

5. Return R

Figure  1:  violation 
sented 

(repre-
in  DNF) under  which  an  effect  consequent 

computes  the  conditions 

....  ~will  violate  constraint  C.  Returning R = {} -_-  false
returning  {...  {}...}  means nec-

means no  violation, 
essary  violation.  We assume that  E is  a set  of  literals
(implicit  conjunction)  and C is  in  CNF: i.e., 
sets  representing  a  conjunction  of  disjunctions.

a set  of

as  long  as  the  planner  commits to  ensuring  that  ex-
ecution  will  not  result 
in  E.  This  is  achieved  by
adding  --S  as  a  new subgoal  to  be made true  at  the
5
time  when Ap is  executed.

3. Evade:  Alternatively,  by  definition  of  violation 

it
is  ok to  execute  Ap as  long  as  R = violation(E,  C)
will  not  be  true  after  execution.  The planner  can
achieve  this  via  goal  regression, 
i.e.  by computing
the  causation  preconditions  [25]  for  -~R and Ap, to
6
be made true  at  the  time  when Ap is  executed.

4. Refuse:  Otherwise,  the  planner  must  refuse  to  add
Ap and backtrack  to  find  another  way to  to  support
G for  A~.
For  example,  suppose  that  the  agent  is  operating
under  the  written, 
to.  tape  constraint  mentioned ear-
lier,  and is  given the  goal of  reducing disk  utilization.
Suppose  the  agent  considers  adding  arm  paper.rex
action  to  the  plan,  which has  an  effect  of  the  form
~isa(paper.tex,file). Since  violation returns
-~written.  to. tape  (paper.  tex),  the  rm action  threat-
ens  safety.  To disarm  the  threat, 
the  planner  must
perform  one of  the  options  above.  Unfortunately,  dis-
avowal (option  one) isn’t  viable  since  paper,  rex  exists
in  the  initial  state  (i.e., 
it  is  of  type file).  Option two
(confrontation)  is  also  impossible since  the  threatening
is  not  conditional.  Thus the  agent  must choose
effect 
between  either 
to  add the  action  or  evading
its  undesired  consequences by archiving  the  file.

refusing 

5Note that  -,S  is  strictly  weaker than Pednault’s preser-
vation  preconditions [25]  for  Ap and C; it  is  more akin  to
preservation preconditions to a single effect  of  the  action.

°While confrontation and evasion are  similar  in  the sense
that  they negate a disjunct (S and R, respectively),  they dif-
fer  in  two ways. First,  confrontation’s subgoal -,S is  derived
from the  antecedent of  a conditional effect  while evasion’s
-~R comes from a disjunctive  constraint,  dont-disturb(C),
via  violation.  Second, the  subgoals are  introduced at  dif-
ferent  times.  Confrontation demands that  -~S be made true
before  Ap is  executed,  while evasion requires  that  -~R be
true  after  execution of  Ap. This is  why evasion regresses R
through  Ap.

Analysis
Two factors  determine  the  performance  of  a  planning
system:  the  time  to  refine  a  plan  and  the  number of
plans  refined  on the  path  to  a  solution.  The time  per
refinement  is  affected  only  when new actions  are  added
to  plan:  each  call  to  violation 
takes  O(ec) time  where
in  the  action’s
e  is  the  number of  consequent literals 
in  the  CNF en-
effects  and c  is  the  number of  literals 
is
coding  of  the  constraint.  When a  threat 
the  cost  depends on the  planner’s  response:
detected, 
in  the  size  of  the  initial
disavowal takes  time  linear 
state, 
refusal  is  constant  time,  confrontation  is  linear
in  the  size  of  S,  and the  cost  of  evasion  is  simply the
time  to  regress  R through  Ap.

to  safety 

It 

is  more  difficult 

to  estimate 
the  effect  of
dent-disturb  constraints 
on  the  number of  plans  ex-
plored.  RefusM reduces  the  branching  factor  while  the
other  options  leave  it  unchanged (but  can  add new sub-
goals,  as  do confrontation  and evasion).  In  some cases,
the  reduced  branching  factor  may speed  planning;  how-
ever,  in  other  cases,  the  pruned search  space  may cause
the  planner  to  search  much deeper (or  even fail  to  halt)
to  find  a  safe  solution.  The essence  of  the  task,  how-
ever,  is  unchanged. Safe  planning  can  be  reformulated
as  a  standard  planning  problem.

Tidiness

constraints 

are  too  strong.

Sometimes dont-disturb 
Instead,  one  would be  content  if  the  constraint  were
satisfied  when the  agent  finished 
its  plan.  We de-
note  this  weaker restriction  with  restore;  essentially,
it  ensures  that  the  agent  will  clean  up  after 
itself
--  by  hanging  up  phones,  closing  drawers,  returning
utensils 
to  their  place,  etc.  An agent  that  is  guar-
anteed  to  respect  all  restore  constraints 
is  said  to
be  tidy.  For  instance, 
that  the  agent
will  re-compress  all  files 
that  have been uncompressed
in  the  process  of  achieving  its  goals,  we could  say
restore (eompress ed (f)  ).

to  guarantee 

As with  dent-disturb  constraints,  we don’t  require
that  the  agent  clean  up after  other  agents  --  the  state
of  the  world,  when the  agent  is  given  a  command, forms
a  reference  point.  However, what should  the  agent  do
when there  is  a  conflict  between restore  constraints
and top  level  goals?  For  example,  if  the  only  way to
satisfy 
a  user  command would leave  one file  uncom-
pressed,  should the  agent  refuse  the  user’s  command or
assume that  it  overrides  the  user’s  background desire
for  tidiness?  We propose  the  latter 
--  unlike  matters
of  safety,  the  agent’s  drive  for  tidiness  should be sec-
ondary to  direct  orders.  The following  definition  makes
these  intuitions  precise.
Definition:  Satisfaction 
of  restore:  Building  on
the  definition  of  dour-disturb,  we say  that  A1,...,  An
satisfies 
goal G if  for  all  substitutions  0

restore(C)  with  respect 

the  constraint 

/fw0  ~  C0 then  (w.  l=  CO or  G l=  -~CO) (2)

2O

Definition  2 differs  from Definition  I  in  two ways: (1)
restore  constraints  need only be satisfied 
in  w,,  after
the  complete  plan  is  executed,  and (2)  the  goal  takes
precedence  over  restore  constraints.  Our constraints
takes  priority
obey a  strict  hierarchy:  dent-disturb 
over  restore.  Note also  that  restore  constraints 
are
guaranteed  to  be  mutually  consistent  (assuming  a  con-
sistent 
state)  since  they  merely restore  aspects
of  w0.

initial 

If 

Tidy  Plans

:’:Synthesizing 
The most straightforward  way to  synthesize  a tidy  plan
is  to  elaborate  the  agent’s  goal  with a set  of  "cleanup"
goals  based  on its  restore  constraints  and the  initial
the  agent’s  control  comes from  a  subgoal
state. 
interleaving,  partial  order  planner  such as  ucPoP [29],
then  the  modification  necessary  to  ensure  tidiness 
is
straightforward.  The agent  divides 
the  planning  pro-
cess  into  two phases: first, 
it  plans  to  achieve the  top
level  goal,  then  it  plans  to  clean  up as  much as  pos-
sible.  In  the  first  phase,  the  planner  doesn’t  consider
tidiness  at  all.  Once a safe  plan  is  generated,  the  agent
performs  phase  two  by  iterating 
through  the  actions
and  using  the  violation 
function  (figure  1)  to  test
each  relevant  effect  against  each  constraint.  For  each
non-false  result, 
the  planner  generates  new goals  as
follows.  (1)  If  the  effect  is  ground and the  correspond-
ing  ground instance  of  the  restore  constraint,  C0,  is
not  true  in  the  initial  state, 
then  no new goals  are  nec-
essary.  (2)  If  the  effect  is  ground and C0 is  true  in  the
initial 
the  effect  is  universally  quantified,  then  a  conjunction
of  ground goals  (corresponding to  all  possible  unifica-
tions  as  in  case  2)  is  posted/  After  these  cleanup goals
have been  posted, 
the
previous solution  into  one that  is  tidy.  If  the  planner
ever  exhausts  the  ways of  satisfying 
a  cleanup  goal,
then  instead  of  quitting  altogether  it  simply  abandons
that  particular  cleanup  goal  and tries 

then  C0 is  posted  as  a  new goal.  (3) 

the  planner  attempts  to  refine 

the  next.

state, 

Note that 

in  some cases,  newly added  cleanup  ac-
tions  could  threaten 
tidiness.  For  example,  cleaning
the  countertop  might tend  to  dirty  the  previously  clean
floor.  To handle  these  cases,  the  planner  must continue
to  perform the  violation test  and cleanup-goal  gener-
ation  process  on  each  action  added during  phase  two.
Subsequent  refinements  will  plan  to  either  sweep the
floor  (white knight)  or  preserve  the  original  cleanliness
by  catching  the  crumbs as  they  fall 
from the  counter
(confrontation).

Analysis
Unfortunately, 
to
eliminate  mess as  specified  by  constraint  2.  For  ex-
ample,  suppose  that  a  top  level  goal  could  be safely

is  not  guaranteed 

this  algorithm 

rCase 3 is  similar to  the expansion of  a universally quan-
tiffed  goal into  the  universal base [29],  but case 3 removes
ground literals  that  aren’t  true in the initial  state.

achieved  with  A~ or  Ay and  in  phase  one,  the  planner
chose  to  use  A=. If  A= violates  a  restore  constraint,
A~ does  not,  and  no  other  actions  can  cleanup  the
mess, then  phase two will  fail 
could  fix  this  problem  by  making phase  two  failures
spawn backtracking  over  phase  one decisions,  but  this
will  engender exhaustive  search  over  all  possible  ways
of  satisfying  top level  goals.

to  achieve  tidiness.  One

Remarkably, this  problem does  not  arise  in  the  cases
we have  investigated.  For  instance,  a  software  agent
has  no difficulty 
grepping  through  old  mail  files  for
a  particular  message and subsequently  re-compressing
the  appropriate  files.  There are  two reasons  why tidi-
ness  is  often  easy to  achieve (e.g., 
and kitchens  [1]):

in  software domains

¯ Most  actions 

are  reversible. 

The  compress  ac-
tion  has  uncompress  as  an  inverse.  Similarly, 
a
short  sequence  of  actions  will  clean  up  any  mess
in  the  kitchen.  Many environments  have  been  sta-
bilized  [18]  (e.g.,  by implementing reversible  com-
mands or  adding  dishwashers) 
in  a  way that  makes
them easy  to  manipulate.

¯ We conjecture  that,  for  a  partial-order  planner,  most
serializable  [6]  with  re-

cleanup goals  are  trivially 
s
spect  to  each other,

When these  properties 

are  true  of  restore con-
in  a domain, our  tidiness  algorithm  does  sat-
straints 
isfy  constraint  2.  Trivial  serializability 
ensures  that
backtracking  over  phase  one decisions  (or  previously
achieved cleanup goals)  is  unnecessary.  Tractability 
is
another  issue.  Since  demanding that  plans  be tidy  is
tantamount  to  specifying  additional 
(cleanup)  goals,
requiring  tidiness  can clearly  slow a planner.  Further-
more if  a  cleanup  goal  is  unachievable, 
the  planner
might  not  halt.  However, as  long  as  the  mess-inducing
actions  in  the  world are  easily  reversible,  it  is  straight
forward to  clean  up for  each one.  Hence, trivial  serial-
izability  assures  that  the  overhead caused  by tidiness
in  the  number of  cleanup  goals  posted,
is  only  linear 
which is  linear 
in  the  length  of  the  plan  for  the  top
level goals.

SFormally, serializability 

[19] means that  there  exists
a  ordering  among the  subgoMs which allows  each  to  be
solved  in  turn without backtracking over  past  progress.
Trivial  serializability  means that  every subgoal ordering
allows monotonic progress  [6].  While goal  ordering is  of-
ten  important among the  top  level  goals,  we observe that
cleanup goals  are  usually  trivially 
serializable  once the
block  of  top  level  goals  have been solved.  For example,
the goal of  printing  a file  and the  constraint  of  restoring
files  to  their  compressed state  are  serializable.  And the  se-
rialization  ordering places the  printing  goal first  and the
cleanup goal  last.  As long as  the  pla~ner  considers  the
goals in  this  order,  it  is  guaranteed to  find  the  obvious
uncompress-print -compress plan.

21

Remaining  Challenges

Some changes  cannot  be  restored,  and  some resources
are  legitimately  consumed in  the  service  of  a goal.  To
make an  omelet,  you  have  to  break  some  eggs.  The
question 
is,  "How many?" Since  squandering  resources
clearly  constitutes  harm,  we could  tag  a  valuable  re-
and  demand
sources  with  a  min-consume  constraint 
that  the  agent  be thrifty- 
as
possible  when achieving  its  goals.  Unfortunately,  sat-
isfying  constraints  of  this  form may require  that  the
agent  examine every  plan  to  achieve  the  goal  in  order
to  find  the  thriftiest 
one.  We plan  to  seek  insights
into  this  problem in  the  extensive  body of  research  on
resource  management in  planning  [8,  10,  16,  39,  38].

that  it  use  as  little 

i.e., 

So far  the  discussion  has  focused  on  preventing  an
agent  from  actively  harming  a  human, but  as  Asimov
noted  -- 
inaction  can  be  just  as  dangerous.  We say
that  an  agent  is  vigilant  when it  prevents  a  human
from harming herself.  Primitive  forms  of  vigilance  are
already  present 
in  many computer systems,  as  the  "Do
you really  want to  delete  all  your  files?"  message at-
tests.

Alternatively, 

one could  extend dont-disturb 

and
restore primitives with an additional argument that
specifies the class of agents being restricted. By writ-
ing self as the first argument, one could achieve the
functionality described in sections and ; by writing
Sam as the argument, the agent will clean up after,
and attempt to prevent safety violations by Sam. Fi-
nally, by providing everyone as the first argument,
we demand that the agent attempt to clean up after
all other agents and attempt to prevent all safety vi-
olations. Other classes (besides self and everyone)
could be defined, leading to more refined behavior.

Our suggestion is problematic for several reasons.
(I) Since the agent has no representation 
of the goals
that other users are trying to accomplish, it might try
to enforce a generalized restore constraint with tidy-
ing actions that directly conflict with the user’s goal.
In addition, there is the question of when the agent
should consider the human "finished" -- without an
adequate method, the agent could tidy up while the
human is still actively working. (2) More generally,
the human interface issues are complex -- we conjec-
ture that users would find vigilance extremely annoy-
ing. (3) Given a complex world where the agent does
not have complete information, any any attempt to for-
malize the second half of Asimov’s First Law is fraught
with difficulties. 
The agent might reject direct requests
to perform useful work in favor of spending all of its
time sensing to see if some dangerous activity might be
happening that it might be able to prevent. A solution
to this problem appears difficult.

Conclusion
This  paper  explores 
the  fundamental  question  origi-
nally  posed  by  Asimov: how do  we stop  our  artifacts

from causing  us  harm in  the  process  of  obeying our  or-
ders?  This  question  becomes increasingly  pressing  as
we develop  more powerful,  complex,  and  autonomous
artifacts 
such  as  robots  and software  agents  [15,  14].
Since  the  positronic  brain  envisioned  by Asimov is  not
yet  within  our  grasp,  we adopted the  familiar  classical
planning  framework.  To facilitate 
progress,  we have
that  capture
focused  on  two well-defined  primitives 
aspects  of  the  problem:  dent-disturb 
and  restore.
Consequently,  we argued  that  the  well-understood,  and
computational  tractable i  mechanism of  threat  detec-
tion  can  be extended  to  avoid  harm.

is  mentioned  briefly 

Other researchers  have considered  related  questions.
A precursor  of  dent-disturb 
is  discussed  in  the  work
of  Wilensky and  more extensively  by  Luria  [22]  under
the  heading of  "goal  conflict."  Similarly,  a  precursor
of  restore 
in  Hammond et.  a/’s
analysis  of  "stabilization" 
[18]  under  the  heading  of
"clean  up  plans."  Our advances  include  precise  and
unified  semantics  for  the  notions,  a  mechanism for  in-
corporating  dent-disturb 
into  standard
planning  algorithms,  and  an analysis  of  the  computa-
tional  complexity of  enforcing  safety  and tidiness.

and  restore 

Even so,  our  work raises  more questions  than  it  an-

swers:  are constraints like dont-disturb and restore
the "right" way to represent harm to an agent? Can we
handle tradeofls short of using expensive decision theo-
retic techniques? What guarantees can one provide on
resource usage? Most importantly, how do we weaken
the assumptions (laid out in section ) of a static world
and complete information?

References

[1] P. Agre and I. Horswill. Cultural support for im-

provisation. In Proc. 10th Nat. Conf. on Artificial
Intelligence,  1992.

[3]

[2] J.  Allen.  Planning as  temporal reasoning.  In  Pro-
ceedings  of  the  Second International  Conference
on Principles  of  Knowledge Representation  and
Reasoning,  pages  3-14,  1991.
J.  Allen,  H. Kautz,  R.  Pelavin,  and J.  Tenenberg.
Reasoning  about  Plans.  Morgan Kaufmann,  San
Mateo,  CA, 1991.
Isaac  Asimov.  Runaround.  Astounding  Science
Fiction,  1942. Reprinted in  [5].
Isaac  Asimov.  /,  Robot.  Ballantine  Books,  New
York,  1983.

[4]

[5]

[6] A.  Barrett  and D.  Weld.  Characterizing 

subgoal
interactions  for  planning.  In  Proc.  13th Int.  Joint
Intelligence,  pages 1388-1393,
Conf.  on Artificial 
September  1993.

[7] D.  Chapman. Planning  for  conjunctive  goals.  Ar-

tificial 

Intelligence,  32(3):333-377, 1987.

[8] K.  Currie  and A.  Tate.  O-plan:  the  open planning
Intelligence,  52(1):49-86,

architecture.  Artificial 
November 1991.

22

[9] E.  Davis. 

Representations 

of  Commonsense

Knowledge.  Morgan Kaufmann Publishers, 
San  Mateo,  CA, 1990.

Inc.,

[10] T.  Dean,  J.  Firby,  and  D.  Miller.  Hierarchical

planning  involving  deadlines, 
travel 
resources.  Computational Intelligence, 
398,  1988.

times,  and
4(4):381-

[11] M. Drummond. Situated 

In  Pro-
ceedings  of  the  First  International  Conference on
and  Reasoning,  May
Knowledge  Representation 
1989.

control 

rules. 

[12] O.  Etzioni, 

S.  Hanks,  D.  Weld,  D.  Draper,
N.  Lesh,  and  M.  Williamson.  An Approach  to
Planning  with  Incomplete  Information. 
In  Proc.
3rd  Int.  Conf.  on  Principles  of  Knowledge Rep-
resentation  and Reasoning,  October  1992.  Avail-
able  via  anonymous  FTP  from  "~tp/pub/ai/at
cs. washington, edu.

[13] Oren  Etzioni.  Embedding decision-analytic 

trol  in  a  learning  architecture.  Artificial 
gence,  49(1-3):129-160,  1991.

con-
Intelli-

[14] Oren Etzioni.  Intelligence  without  robots  (a  reply
to  brooks).  AI  Magazine,  14(4),  December 1993.
[15] Oren  Etzioni,  Neal  Lesh,  and  Richard  Segal.
Building  softbots  for  UNIX (preliminary  report).
Technical  Report  93-09-01,  University  of  Wash-
ington,  1993.  Available  via  anonymous FTP from
"ftp/pub/ai/at cs.washington, edu.

[16] M. Fox and  S.  Smith.  ISIS  --  a  knowldges-based
system  for  factory  scheduling.  Expert  Systems,
1(1):25-49,  July  1984.

[17] Peter  Haddawy and  Steve  Hanks.  Utility  Mod-
els  for  Goal-Directed Decision-Theoretic  Planners.
Technical  Report  93-06-04,  Univ.  of  Washing-
ton,  Dept.  of  Computer Science  and  Engineering,
September  1993.  Available  via  anonymous  FTP
from  "ftp/pub/ai/at cs. washington, edu.

[18]K. Hammond, T. Converse, and J. Grass. The sta-

bilization of environments. 
1992. To appear.

Artificial Intelligence,

[19]R. Korf. Planning as search: A quantitative

approach.  Artificial 
September  1987.

Intelligence, 

33(1):65-88,

[20] Amy Lansky,  editor.  Working  Notes  of  the  AAAI

Spring  Symposium:  Foundations  of  Automatic
Planning:  The  Classical  Approach  and  Beyond,
Menlo Park,  CA, 1993.  AAAI Press.

[21] H.J.  Levesque  and  R.  Brachman.  A fundamental
In  R.  Brach-
in
Readings 
pages  42-70.  Morgan

tradeoff  in  knowledge representation. 
man and  H.J.  Levesque,  editors, 
Knowledge Representation, 
Kaufmann,  San  Mateo,  CA, 1985.

[22] Marc Luria.  Knowledge Intensive  Planning.  PhD

thesis,  UC Berkeley,  1988.  Available  as  technical
report  UCB/CSD 88/433.

[38] D. Wilkins.  Can AI planners  solve  practical  prob-
6(4):232-246,

lems?  Computational  Intelligence, 
November 1990.

[39] D. E.  Wilkins.  Practical  Planning.  Morgan Kauf-

mann,  San  Mateo,  CA, 1988.

[23] Ginsberg  M. and  D.  Smith.  Reasoning  about  ac-
In-

tion  I:  A possible  worlds approach.  Artificial 
telligence,  35(2):165-196,  June 1988.
and  D.  Rosenblitt. 
planning. 

System-
In  Proc.  9th  Nat.
pages  634-639,

[24] D.  McAllester 
atic  nonlinear 
Conf.  on  Artificial 
July  1991. 
/pub/users/dam/aaai91c.ps.

internet 

Intelligence, 

file  at  ftp.ai.mit.edu:

[25] E.  Pednault.  Synthesizing  plans  that  contain  ac-
tions  with  context-dependent  effects.  Computa-
tional  Intelligence,  4(4):356-372,  1988.

[26] E.  Pednault.  ADL: Exploring  the  middle  ground

between  STRIPS and  the  situation 
In  Proceedings  Knowledge Representation  Conf.,,
1989.

calculus.

[27] J.  S.  Penberthy  and  Daniel  S.  Weld.  A new ap-
proach  to  temporal  planning  (preliminary  report).
In  Proceedings  of  the  AAAI 1993  Symposium  on
Foundations  of  Automatic  Planning:  The  Classi-
cal  Approach and  Beyond,  pages  112-116,  March
1993.

[28]  J.S.  Penberthy.

Planning  with  Continuous
Change.  PhD thesis,  University  of  Washington,
1993.  Available  as  UW CSE Tech  Report  93-12-
01.

[29] J.S.  Penberthy  and  D.  Weld.  UCPOP: A sound,
complete,  partial  order  planner  for  ADL. In  Proc.
3rd  Int.  Conf.  on  Principles  of  Knowledge Rep-
resentation  and Reasoning,  pages  103-114,  Octo-
ber  1992.  Available 
via  anonymous  FTP  from
"ftp/pub/ai/at 

cs.  washSngeon,  edu.

[30] Martha Pollack.  The uses  of  plans.  Artificial 

telligence,  57(1),  1992.

In-

[31] S.  Russell  and  E.  Wefald.  On optimal  game-tree
In  Proceed-

search  using  rational-meta-reasoning. 
ings  IJCAI-89,  pages  334-340,  aug 1989.

[32] S.  Russell  and  E.  Wefald.  Do the  Right  Thing.

MIT Press,  Cambridge,  MA, 1991.

[33] Y.  Shoham.  Reasoning  about  Change:  Time  and
Causation  from the  Standpoint  @Artificial  Intel-
ligence.  MIT Press,  Cambridge,  MA, 1988.

[34] D. Smith.  Controlling  backward inference.  Artifi-

cial  Intelligence,  39:145-208, 1989.

[35] A.  Tare.  Generating  project  networks.  In  Proc.

5th  Int.  Joint  Conf.  on Artificial 
pages  888-893,  1977.

Intelligence,

[36] M. Wellman.  Challenges  for  decision-theoretic

In  Proceedings  of  the  AAA1 1993 Sym-
planning. 
posium  on  Foundations  of  Automatic  Planning:
The  Classical  Approach and  Beyond,  March 1993.

[37] M.  Wellman  and  J.  Doyle.  Modular  utility

reprsentation  for  decision  theoretic  planning.  In
Proc.  1st  Int.  Conf.  on  A.L  Planning  Systems,
pages  236-242,  June  1992.

23

