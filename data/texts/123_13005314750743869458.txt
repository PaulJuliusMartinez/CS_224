Journal of Machine Learning Research 6 (2005) 995­1018

Submitted 11/04; Revised 4/05; Published 6/05

Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection

Koji Tsuda Max Planck Institute for Biological Cybernetics Spemannstrasse 38 72076 Tu¨bingen, Germany, and Computational Biology Research Center National Institute of Advanced Science and Technology (AIST) 2-42 Aomi, Koto-ku, Tokyo 135-0064, Japan

KOJI.TSUDA@TUEBINGEN.MPG.DE

Gunnar Ra¨tsch Friedrich Miescher Laboratory of the Max Planck Society Spemannstrasse 35 72076 Tu¨bingen, Germany

GUNNAR.RAETSCH@TUEBINGEN.MPG.DE

Manfred K. Warmuth Computer Science Department University of California Santa Cruz, CA 95064, USA

MANFRED@CSE.UCSC.EDU

Editor: Yoram Singer

Abstract
We address the problem of learning a symmetric positive definite matrix. The central issue is to design parameter updates that preserve positive definiteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: on-line learning with a simple square loss, and finding a symmetric positive definite matrix subject to linear constraints. The updates generalize the exponentiated gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector (which in this context is a diagonal positive definite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive definiteness. Most importantly, we show how the derivation and the analyses of the original EG update and AdaBoost generalize to the non-diagonal case. We apply the resulting matrix exponentiated gradient (MEG) update and DefiniteBoost to the problem of learning a kernel matrix from distance measurements.
1. Introduction
Most learning algorithms have been developed to learn a vector of parameters from data. However, an increasing number of papers are now dealing with more structured parameters. More specifically, when learning a similarity or a distance function among objects, the parameters are defined as a symmetric positive definite matrix that serves as a kernel (e.g., Xing et al., 2003; Shai-Shwartz et al., 2004; Tsang and Kwok, 2003; Tsuda and Noble, 2004). Learning is typically formulated as a parameter updating procedure to optimize a loss function. The gradient descent update is one of
c 2005 Koji Tsuda, Gunnar Ra¨tsch and Manfred K. Warmuth.

TSUDA, RA¨ TSCH AND WARMUTH

the most commonly used algorithms, but it is not appropriate when the parameters form a positive definite matrix, because the updated parameter matrix does not necessarily stay positive definite. Xing et al. (2003) solved this problem by always correcting the updated matrix to be positive definite. However no bound has been proven for this update-and-correct approach. Also, Shai-Shwartz et al. (2004) proposed an on-line algorithm for learning a kernel matrix when only some of the class labels of the examples are provided. This algorithm is also based on the updateand-correction approach, but since the update step performs rank-one modification, the correction step can be efficiently implemented. They have shown a generalization bound inspired by similar previously known bounds for the perceptron.
In this paper, we introduce the matrix exponentiated gradient update which works as follows: First, the matrix logarithm of the current parameter matrix is computed. Then a step is taken in the direction of the steepest descent of the loss function. Finally, the parameter matrix is updated to the exponential of the modified log-matrix. Our update preserves symmetry and positive definiteness because the matrix exponential maps any symmetric matrix to a symmetric positive definite matrix.
Bregman divergences play a central role in the motivation and the analysis of on-line learning algorithms (Kivinen and Warmuth, 1997). A learning problem is essentially defined by a loss function and a divergence that measures the discrepancy between parameters. More precisely, the updates are motivated by minimizing the sum of the loss function and the Bregman divergence, where the loss function is multiplied by a positive learning rate. Different divergences lead to radically different updates (Kivinen and Warmuth, 1997, 2001). For example, the gradient descent update is derived from the squared Euclidean distance, and the exponentiated gradient update from the Kullback-Leibler divergence (relative entropy). In this work we use the von Neumann divergence (also called quantum relative entropy) for measuring the discrepancy between two positive definite matrices (Nielsen and Chuang, 2000). We derive a new matrix exponentiated gradient update from this divergence (which is a Bregman divergence for symmetric positive definite matrices). Finally we prove relative loss bounds using the von Neumann divergence as a measure of progress.
We apply our techniques to solve the following related key problem that has received a lot of attention recently (Xing et al., 2003; Shai-Shwartz et al., 2004; Tsang and Kwok, 2003; Tsuda and Noble, 2004). Find a symmetric positive definite matrix that satisfies a number of linear inequality constraints. The new DefiniteBoost algorithm greedily chooses a violated linear constraint and performs an approximated Bregman projection. In the diagonal case, we recover AdaBoost (Schapire and Singer, 1999). We also show how the convergence proof of AdaBoost generalizes to the non-diagonal case.

2. Preliminaries
In this section, we first present mathematical definitions and basic lemmas.

2.1 Matrix Basics
We denote matrices by capital bold letters and restrict ourselves to square matrices with real entries in this paper. For any such matrix A  Rd×d, exp A and log A denote the matrix exponential and logarithm, respectively. The matrix exponential is defined as the following power series,

exp(A) := I + A + 1 A2 + 1 A3 + · · · . 2! 3!

(2.1)

996

MATRIX EXPONENTIATED GRADIENT UPDATES

In the case of symmetric matrices, the matrix exponential operation can be computed using the
eigenvalue decomposition A = V V , where V is an orthonormal matrix with the eigenvectors of A as columns and  the diagonal matrix of eigenvalues. Thus, exp A = V (exp )V , where (exp )i,i = exp(i,i). The matrix logarithm log A is defined as the inverse function of exp A, which does not always exist for arbitrary A. However, when A is symmetric and strictly positive definite,
log A is computed as log A := V (log )V , where (log )i,i = log i,i. Throughout the paper log a and exp a denote the natural logarithm and exponential of scalar "a".
A square matrix is positive definite if all its eigenvalues are strictly positive. Positive semi-
definiteness only requires the non-negativity of the eigenvalues. For two matrices A and B, A
B iff B - A is positive semi-definite. Similarly, A  B iff B - A is (strictly) positive definite.
The trace of a matrix is the sum of its diagonal elements, i.e. tr(A) = i Ai,i and thus tr(AB) = i, j Ai, jB j,i = tr(BA). In matrix algebra, tr(AB) plays a similar role as the dot product for vectors. Furthermore, tr(A) = i i, where i are the eigenvalues of A and the determinant det(A) = i i.
If F(W ) : Rd×d  R is a real-valued function on matrices, then W F(W ) denotes the gradient with respect to matrix W :

 F

W F(W ) = 

W11
...

··· ...

F  W...1d  .

F Wd1

···

F Wdd

For example, it is easy to see that Atr(AB) = B . More examples of computing gradients are given in Appendix A.
For a square matrix X, sym(X) = (X + X )/2 denotes the symmetric part of X. If W is symmetric and X an arbitrary matrix, then

tr(W X) = tr

X +X W

+ tr

X -X W

= tr(W sym(X)).

22

(2.2)

Our analysis requires the use of the Golden-Thompson inequality (Golden, 1965):

tr(exp(A + B))  tr(exp(A) exp(B)),

(2.3)

which holds for arbitrary symmetric matrices A and B. We also need the following two basic inequalities for symmetric matrices. The first one gener-
alizes the following simple inequality, which is a realization of Jensen's inequality for the convex function exp(x): For any 0  a  1 and 1, 2  R,
exp(a1 + (1 - a)2)  a exp(1) + (1 - a) exp(2).
In the below generalization, the distribution (a, 1 - a) is replaced by (A, I - A), where A is any symmetric matrix for which 0 A I.
Lemma 2.1 For any symmetric matrix A  Rd×d such that 0 A I, and any 1, 2  R,
exp(A1 + (I - A)2) A exp(1) + (I - A) exp(2).

997

TSUDA, RA¨ TSCH AND WARMUTH
Proof Assume A is eigen-decomposed as A = V V , where  is the diagonal matrix of eigenvalues and V is an orthogonal matrix with the eigenvectors of A as columns. By assumption, 0  k  1. Let k be the k-th eigenvalue of the left hand side of the inequality that we are to prove. Clearly k = exp(k1 + (1 - k)2) and by Jensen's inequality, k  k exp(1) + (1 - k) exp(2). Let  be the diagonal matrix with entries k. Then   exp(1) + (I - ) exp(2), and by multiplying both sides by V from left and by V from right, we obtain the desired inequality.
Lemma 2.2 For any positive semi-definite symmetric matrix A  Rd×d and any two symmetric matrices B, C  Rd×d, B C implies tr(AB)  tr(AC).
Proof Let D = C - B, then D 0 by assumption. Suffices to show that tr(AD)  0. Let us eigen-decompose A as V V . Since V V = V V = I, D = V PV where P = V DV 0. Then tr(AD) = tr(V V V PV ) = tr(P) = ni=1 iPii. Since P is positive semi-definite, the diagonal elements Pii are nonnegative. Also by assumption the eigenvalues i of A are nonnegative. Thus we conclude that tr(AD)  0.
2.2 Von Neumann Divergence or Quantum Relative Entropy If F is a real-valued strictly convex differentiable function on the parameter domain (a subset of matrices in Rd×d) and f (W ) := W F(W ), then the Bregman divergence between two parameters W and W is defined as
F(W , W ) := F(W ) - F(W ) - tr((W - W )f (W ) ).
Since F is strictly convex, F(W , W ) is also strictly convex in its first argument. Furthermore, the gradient in the first argument has the following simple form:
W F(W , W ) = f (W ) - f (W ),
since Atr(AB) = B (cf. Section 2.1). For the divergences used in this paper, we restrict ourselves to the domain of symmetric positive
definite matrices. Our main choice of F is F(W ) = tr(W log W - W ), which is called von Neumann entropy or quantum entropy. The strict convexity of this function is well known (Nielsen and Chuang, 2000). Furthermore we show in Appendix A that W F(W ) = f (W ) = log W .
The Bregman divergence corresponding to this choice of F is the von Neumann divergence or quantum relative entropy (e.g., Nielsen and Chuang, 2000):
F(W , W ) = tr(W log W - W log W - W + W ).
In this paper, we are primarily interested in the case when the parameters are normalized in the sense that tr(W ) = tr(W ) = 1. Symmetric positive definite matrices of trace one are related to density matrices commonly used in Statistical Physics. For normalized parameters the divergence simplifies to
F(W , W ) = tr(W log W - W log W ).
998

MATRIX EXPONENTIATED GRADIENT UPDATES

If W = i ivivi is our notation for the eigenvalue decomposition, then the von Neumann entropy1 becomes F(W ) = i i log i. We can rewrite the normalized divergence2 as

F(W , W ) =  ~ i log ~ i -  ~ i log  j(v~i v j)2. i i, j

(2.4)

This divergence quantifies the difference in the eigenvalues as well as the eigenvectors. When both

eigen systems are the same (i.e., v~i = vi), then the divergence becomes the usual relative entropy

between

the

eigenvalues

F(W

,

W

)

=

i

~ i

log

~ i i

.

2.3 Rotation Invariance
One can visualize a symmetric positive definite matrix W = i ivivi = V V as an ellipse, where the eigenvectors vi are the axes of the ellipse and the square-roots of the eigenvalues (i.e.
i) are the lengths of the corresponding axes. Thus the von Neumann divergence quantifies the "discrepancy" between two ellipses and is invariant under a simultaneous rotation of both eigen systems. That is, for any orthonormal matrix U , the von Neumann divergence has the property that

F(W , W ) = F(U W U , U W U ). This follows from (2.4) and

(2.5)

F(V V , V V ) = F(U V (U V ) , U V (U V ) ).

However, the divergence is decidedly not invariant under the unitary rotation of both parameters, i.e. typically F(W , W ) = F(U W , U W ) for an orthonormal matrix U . This is because such rotations can change the sign of the eigenvalues. Also rotating symmetric matrices typically produces non-symmetric matrices.
There is a second important divergence between symmetric positive definite matrices that is invariant under the simultaneous rotation of both eigen systems (2.5). It is a Bregman divergence based on the strictly convex function F(W ) = - log det(W ) (e.g., Boyd and Vandenberghe (2004)) over the cone of positive definite matrices. Note that F(W ) = - i log i, where the i denote the eigenvalues of W . Also since f (W ) = W F(W ) = (W -1) = W -1, the Bregman divergence becomes:

F(W , W ) = log det(W ) + tr(W -1W ) - d det(W )

=

i

log

i ~i

+

tr(W

-1W

)

-

d,

where d is the dimension of the parameter matrices. We call this the LogDet divergence. Notice that in this case, F(W ) is essentially minus the log of the volume of the ellipse W , and the LogDet divergence is the relative entropy between two multidimensional Gaussians with fixed mean and covariance matrices W and W , respectively (see Singer and Warmuth, 1999). At the end of Section 3.1 we will also briefly discuss the updates derived from the LogDet divergence. Note that for this divergence F(W , W ) = F(U W , U W ) for any orthonormal matrix U and parameter matrices in the domain of F.

1. F(W ) can be extended to symmetric positive semi-definite matrices by using the convention 0 log 0 = 0. 2. The domain of the first argument can be extended to symmetric positive semi-definite matrices.

999

TSUDA, RA¨ TSCH AND WARMUTH

3. On-line Learning
In this section we present a natural extension of the exponentiated gradient (EG) update (Kivinen and Warmuth, 1997) to an update for symmetric positive definite matrices.

3.1 Motivation of the Updates
On-line learning proceeds in trials. In the most basic form, the on-line algorithm produces a parameter Wt at trial t and then incurs a loss Lt(Wt). In this paper, the parameters are square matrices in Rd×d .
In a refined form, the algorithm aims to predict a label and several actions occur in each trial:
The algorithm first receives an instance Xt in some instance domain X . It then produces a prediction
y^t for the instance Xt based on the algorithm's current parameter matrix Wt and receives a label yt.
(The prediction y^t and the label yt lie some labeling domain Y .) Finally the algorithm incurs a real
valued loss L(y^t, yt) and updates its parameter matrix to Wt+1.
For example in Section 3.3 we consider a case where the labeling domain Y is the real line.
The on-line algorithm we analyze for this case predicts with y^t = tr(WtXt) and is based on the loss Lt (Wt ) = L(y^t , yt ) = (y^t - yt )2.
In this section we only discuss updates at a high level and only consider the basic form of the on-line algorithm. We assume that Lt(W ) is convex in the parameter W (for all t) and that the gradient W Lt(W ) is a well defined matrix in Rd×d. In the update, we aim to solve the following problem (see Kivinen and Warmuth, 1997, 2001):

Wt+1 = argmin F(W , Wt) + Lt(W ),
W

(3.1)

where the convex function F defines the Bregman divergence and  is a non-negative learning rate. The update balances two conflicting goals: staying close to the old parameter Wt (as quantified by the divergence) and achieving small loss on the current labeled instance. The learning rate becomes a trade-off parameter.
We can eliminate the argmin by setting the gradient (with respect to W ) of its objective to zero:

Wt+1 = f -1 (f (Wt ) - W Lt (Wt+1)) .

(3.2)

If we assume that f and f -1 preserve symmetry, then constraining W in (3.1) to be symmetric changes the update to (cf. Appendix B for details):

Wt+1 = f -1 (f (Wt ) -  sym(W Lt (Wt+1))) .

(3.3)

The above implicit update is usually not solvable in closed form. A common way to avoid this problem (Kivinen and Warmuth, 1997) is to approximate W Lt(Wt+1) by W Lt(Wt), leading to the following explicit update for the constraint case:
Wt+1 = f -1 (f (Wt ) -  sym(W Lt (Wt ))) .
In the case of the von Neumann divergence, the functions f (W ) = log W and f -1(Q) = exp Q clearly preserve symmetry. When using this divergence we arrive at the following (explicit) update:

1000

MATRIX EXPONENTIATED GRADIENT UPDATES

 sym.pos.def.

pos. semi. def. 

Wt+1 = exp log Wt - sym( W Lt (Wt ) ) .

symmetric

(3.4)

symmetric positive definite
We call this update the unnormalized matrix exponentiated gradient update. Note that f (W ) = log W maps symmetric positive definite matrices to arbitrary symmetric matrices, and after adding a scaled symmetrized gradient, the function f -1(Q) = exp Q maps the symmetric exponent back to a symmetric positive definite matrix.
When the parameters are constrained to trace one, then we arrive at the Matrix Exponentiated Gradient (MEG) update, which generalizes the exponentiated gradient (EG) update of Kivinen and Warmuth (1997) to non-diagonal matrices:

Wt+1

=

1 Zt

exp (log Wt

-

 sym(W

Lt (Wt ))) ,

(3.5)

where Zt = tr (exp (log Wt -  sym(W Lt(Wt)))) is the normalizing constant (See Appendix B for
details.) Finally, observe that for the LogDet divergence f (W ) = W F = -W -1 and f -1(Q) =
-Q-1. Thus both f and f -1 negate and invert all eigenvalues. Both functions also preserve symmetry. However, f -1 does not map an arbitrary symmetric matrix back to a symmetric positive

definite matrix. Note that for this divergence update (3.3) becomes

 sym.pos.def.

pos.semi.def. -1

Wt+1 = - -( Wt )-1 -  sym(W Lt (Wt+1)) .

symmetric negative definite

symmetric positive definite

This update also preserves symmetric positive definiteness of the parameter matrix under the assumption that the gradient W Lt(Wt+1) is positive semi-definite: If Wt is symmetric positive definite, then f (Wt) is symmetric negative definite. Using this assumption, we have that the argument of f -1 is symmetric negative definite and therefore Wt+1 is again symmetric positive definite.
In this paper we prove a certain type of relative loss bound for the MEG update which generalize
the analogously known bounds for the EG algorithm to the non-diagonal case. To our knowledge,
no relative loss bounds have been proven for the above update that is derived from the LogDet
divergence. For this update, such bounds are not even known for the diagonal case. Also, if the gradients of the loss are only known to be symmetric then  must be small in order to guarantee that Wt+1 stays in the positive definite cone.

3.2 Numerically Stable MEG Update

The MEG update (3.5) is numerically unstable when the eigenvalues of Wt are around zero. However we can "unwrap" this update to the following:

Wt+1

=

1 Z~t

exp

t
ct I + log W1 -  sym(W Ls(Ws))
s=1

,

1001

TSUDA, RA¨ TSCH AND WARMUTH

where the constant Z~t normalizes the trace of Wt+1 to one. As long as the eigenvalues of W1 are not too small, the computation of log W1 is stable. Note that the update is independent of the choice of ct  R. We incrementally maintain an eigenvalue decomposition of the matrix in the exponent (O(n3) per iteration):
t
Vt t Vt = ct I + log W1 -  sym(W Ls(Ws)) s=1
where the constant ct is chosen so that the maximum eigenvalue of the above is zero. Now Wt+1 = Vt exp(t)Vt /tr(exp(t)). The pseudo-code is given in Algorithm 1.
Algorithm 1 Pseudo-code of the matrix exponentiated gradient (MEG) algorithm for quadratic Loss
Choose W1 and  Initialize G0 = log W1 for t = 1, 2, . . . do
Obtain instance matrix Xt Predict y^t = tr(Wt Xt ) Obtain label yt and determine the loss Lt = (yt - y^t )2 Update Gt = Gt-1 - 2(y^t - yt ) sym(Xt ) Compute spectral decomposition: Gt = Vt t Vt Update Wt+1 = Vt exp(t - ct I)Vt /tr(exp(t - ct I)), where ct = maxs(t )s,s end for

3.3 Relative Loss Bounds

For the sake of simplicity we now restrict ourselves to the case when the algorithm predicts with y^t = tr(WtXt) and the loss function is quadratic: Lt(Wt) = L(y^t, yt) := (y^t - yt)2.
We begin with the definitions needed for the relative loss bounds. Let S = (X1, y1), . . . , (XT , yT ) denote a sequence of examples, where the instance matrices Xt  Rd×d and the labels yt  R. The total loss of the on-line algorithm on the entire sequence S is LMEG(S) = tt=1(tr(WtXt) - yt)2. We prove a bound on the relative loss LMEG(S) - LU(S) that holds for any comparator parameter U . Such a comparator parameter is any symmetric positive semi-definite matrix U with trace one, and its total loss is defined as LU(S) = tT=1(tr(U Xt) - yt)2. The relative loss bound is derived in two steps: Lemma 3.1 upper bounds the relative loss for an individual trial in terms of the progress
towards the comparator parameter U (as measured by the divergence). In the second Lemma 3.2,
the bound for individual trials is summed to obtain a bound for a whole sequence. These two lem-
mas generalize similar lemmas previously proven for the exponentiated gradient update (Lemmas
5.8 and 5.9 of Kivinen and Warmuth, 1997).

Lemma 3.1 Let Wt be any symmetric positive definite matrix. Let Xt be any square matrix for which the eigenvalues of sym(Xt) have range at most r, i.e.

max(sym(Xt )) - min(sym(Xt ))  r.

Assume Wt+1 is produced from Wt by the MEG update with learning rate , and let U be any symmetric positive semi-definite matrix. Then for any b > 0 and a =  = 2b/(2 + r2b):

a (yt - tr(Wt Xt ))2 -b (yt - tr(U Xt ))2  F (U , Wt ) - F (U , Wt+1) .

(3.6)

MEG-loss

U -loss

progress towards U

1002

MATRIX EXPONENTIATED GRADIENT UPDATES

The above type of inequality is central to all relative loss bounds (Kivinen and Warmuth, 1997). If the loss of the algorithm is small, then the inequality becomes vacuous. However, if the algorithm incurs a large loss, then its parameter Wt must make progress towards any parameter vector U that has small loss on the current example (if such parameters exist).
The proof of this inequality is given in Appendix C. It has the same structure as the corresponding previous lemma proven for the exponentiated gradient algorithm, but now we apply the various matrix inequalities given at the end of Section 2.1 (in particular the Golden-Thompson inequality (2.3) and the approximation of the matrix exponential (Lemma 2.1)). These inequalities will also be essential for the analysis of DefiniteBoost in the next section.

Lemma 3.2 Let S be any sequence of examples with square real matrices as instances and real
labels, and let r be an upper bound on the range of eigenvalues of the symmetric part of each
instance matrix of S. Let the initial parameter W1 and comparison parameter U be arbitrary symmetric positive definite matrices of trace one. Then for any c such that  = 2c/(r2(2 + c)),

LMEG(S) 

1+ c 2

LU(S) +

1+1 2c

r2F (U , W1).

(3.7)

Proof For the maximum tightness of (3.6), a should be chosen as a =  = 2b/(2 + r2b). Let b = c/r2, and thus a = 2c/(r2(2 + c)). Then (3.6) is rewritten as

2c 2+

c

(yt

-

tr(Wt

Xt

))2

-

c(yt

-

tr(U

Xt

))2



r2(F

(U

,

Wt

)

-

F

(U

,

Wt

+1))

Adding the bounds for t = 1, · · · , T , we get

2c 2+

c

LME

G

(S)

-

cLU

(S)



r2(F

(U

,

W1)

-

F

(U

,

Wt+1))



r2F

(U

,

W1),

which is equivalent to (3.7).

Assuming LU(S)  Lmax and F (U , W1)  dmax, then the bound (3.7) is tightest when c = r 2dmax/Lmax. With this choice of c, we have

LMEG(S) - LU(S)  r

2Lmaxdmax

+

r2 2

F

(U

,

W1).

In

particular,

if

W1

=

1 d

I

,

then

F (U , W1)

=

log d

- i

i

log

1 i



log d.

Additionally,

when

Lmax

=

0,

then

the

total

loss

of

the

algorithm

is

bounded

by

r2

log 2

d

.

Note that the MEG algorithm generalizes the EG algorithm of Kivinen and Warmuth (1997). In

the case of linear regression, a square of a product of dual norms appears in the bounds for the EG

algorithm: ||u||21X2. Here u is a parameter vector and X is an upper bound on the infinity norm of the instance vectors xt. Note the correspondence with the above bound (which generalizes the

bounds for EG to the non-diagonal case): the one norm of the parameter vector is replaced by the

trace and the infinity norm by the maximum range of the eigenvalues.

4. Bregman Projection and DefiniteBoost
Using the von Neumann divergence, we will generalize the boosting algorithms for matrix parameters.

1003

TSUDA, RA¨ TSCH AND WARMUTH

4.1 Preliminaries

In this section, we address the following Bregman projection problem of finding a positive semidefinite symmetric matrix W  Rd×d of trace one satisfying a set of linear constraints:3

W  = argmin
W
s.t.

F (W , W1)
W = W , tr(W ) = 1 tr(W C j)  0, for j = 1, . . . , n,

(4.1)

where the symmetric positive definite matrix W1 of trace one is the initial parameter matrix and C1, . . . , Cn are arbitrary matrices. Note that we do not explicitly constrain W to be positive semidefinite because when the von Neumann divergence is used, then the solution W  will always be positive semi-definite. Prior knowledge about W is encoded in the constraints, and the matrix closest to W1 is chosen among the matrices satisfying all constraints. Tsuda and Noble (2004) employed this approach for learning a kernel matrix among graph nodes, and this method can be potentially applied to learn a kernel matrix in other settings (e.g., Xing et al., 2003; Tsang and Kwok, 2003). In the previous work by (Tsuda and Noble, 2004), an algorithm was developed that processes a batch of constraints. The problem was converted to a dual unconstraint problem (as done below) and an iterative gradient descent algorithm was given. However, no convergence proofs were provided previously. In this paper we give on-line algorithms with strong convergence proofs.4
The problem (4.1) is a projection of W1 to the intersection of convex regions defined by the constraints. It is well known that the Bregman projection into the intersection of convex regions can be solved by sequential projections to each region (Bregman, 1967; Censor and Lent, 1981). In the original papers only asymptotic convergence was shown. More recently a connection (Kivinen and Warmuth, 1999; Lafferty, 1999) was made to the AdaBoost algorithm which has an improved convergence analysis (Freund and Schapire, 1997; Schapire and Singer, 1999). We generalize the latter algorithm and its analysis to symmetric positive definite matrices and call the new algorithm DefiniteBoost. As in the original setting, only approximate projections (Figure 1) are required to show fast convergence.
Before presenting the algorithm, let us describe the dual problem of minimizing the von Neumann divergence subject to linear constraints (4.1). The dual variables are the Lagrange multipliers   Rn (  0) associated with this optimization problem:

n

 = argmax - log tr exp(log W1 -   j sym(C j))

0

j=1

.

(4.2)

See Appendix D for a detailed derivation of the dual problem that handles the case when the constraint matrix C j is allowed to be an arbitrary square matrix. Previous derivations required symmetric C j (Tsuda and Noble, 2004). When (4.1) is feasible, the optimal solution is described as

W



=

1 Z()

exp(log W1

-

n
j
j=1

sym(C j)),

3. Note that if  is large then the on-line update (3.1) becomes a Bregman projection subject to a single equality constraint tr(W Xt ) = yt .
4. The methodology employed in this paper is not limited to on-line learning. For example in Littlestone et al. (1992),
cf. Corollary 15, the EG algorithm was used for solving a system of linear equations and fast convergence was shown.

1004

MATRIX EXPONENTIATED GRADIENT UPDATES



  

 

¡

¡

¢£

¤

¥¦

  §¨

¡

¢£

©¨

 § ¥£



 

¨¤

  §

¡

¢£

©¨

 § ¥£



 





Figure 1: The intersection of two convex sets (here two straight lines) can be found by projecting back and forth between the two sets with exact Bregman projections (W1, W2, . . .). In this paper we use certain approximate projections (W1, W2, . . .). Now each projection may over or undershoot the alternating target set. Nevertheless, global convergence to the
optimal solution is still guaranteed via our proofs.

where Z() = tr exp(log W1 - nj=1 j sym(C j)) and  is the optimal dual solution.

4.2 Exact Bregman Projections

Problem (4.1) can be solved with the following algorithm: Start from some initial parameter W1

(for

instance

W1

=

1 d

I

).

At

the

t-th

step,

choose

an

unsatisfied

constraint

jt ,

i.e.

tr(Wt C jt ) > 0.5

Then solve the following Bregman projection with respect to the chosen constraint:

Wt+1 = argmin
W
s.t.

F (W , Wt) W = W , tr(W ) = 1,

(4.3)

tr(W C jt )  0.

By means of a Lagrange multiplier , the dual problem is described as (cf. Appendix D)

t = argmin tr (exp(log Wt -  sym(C jt ))) .
0

(4.4)

Using the solution of the dual problem, Wt is updated as

Wt+1

=

1 Zt (t)

exp(log Wt

- t

sym(C jt ))

(4.5)

where the normalization factor is Zt(t) = tr (exp(log Wt - t sym(C jt ))). If Wt is symmetric positive definite, then Wt+1 is as well. Note that we can use the same numerically stable reformu-
lation of the update as discussed in Section 3.2.

5. For instance, the most unsatisfied constraint, i.e. jt = argmax j=1,···,n tr(Wt C j), can be chosen.

1005

TSUDA, RA¨ TSCH AND WARMUTH

4.3 Approximate Bregman Projections

The solution of (4.4) cannot be obtained in closed form. However, one can use the following ap-

proximate choice of t:

t

=

tmax

1 - tmin

log

1 + rt /tmax 1 + rt /tmin

,

(4.6)

when the eigenvalues of sym(C jt ) lie in the interval [tmin, tmax] and rt = tr(WtC jt ). Since the most unsatisfied constraint is chosen, rt  0 and thus t  0. We call this approximate Bregman projection
algorithm DefiniteBoost. It may be seen as a natural extension of AdaBoost (cf. Section 4.5), where

probability distributions are replaced by symmetric positive definite matrices of trace one. The

pseudo-code of DefiniteBoost is given in Algorithm 2.

Algorithm 2 Pseudo-code of the DefiniteBoost algorithm; tmin and tmax are lower and upper bounds on the eigenvalues of sym(Ct).

Choose W1 Initialize G0 = log W1 for t = 1, 2, . . . do

Choose an unsatisfied constraint jt (i.e. tr(Wt C jt ) > 0) or stop when all constraints satisfied

Compute constraint violation rt = tr(Wt C jt )

Compute

approximate

step

size

t

=

tmax

1 - tmin

log

1 + rt /tmax 1 + rt /tmin

Update Gt = Gt-1 - t sym(C jt )

Compute spectral decomposition: Gt = Vt t Vt

Update Wt+1 = Vt exp(t - ct I)Vt /tr(exp(t - ct I)), where ct = maxs(t )s,s

end for

Although the projection is done only approximately,6 the convergence of the dual objective (4.2) can be shown using the following upper bound of the negative dual objective , i.e.
n
tr exp(log W1 -   j sym(C j)) . j=1

Theorem 4.1 The negative exponentiated dual objective is bounded from above by

T
tr exp log W1 - t sym(C jt ) t=1

T
  (rt), t=1

(4.7)

where and

t

=

tmax

1 - tmin

log

1 + rt /tmax 1 + rt /tmin

, rt = tr(Wt C jt ),

(rt) =

1

-

rt tmax

tmax tmax -tmin

1

-

rt tmin

-tmin
.tmax -tmin

6. The approximate Bregman projection (with t as in (4.6)) can also be motivated as an on-line algorithm based on an entropic loss and learning rate one (following Section 3 and Kivinen and Warmuth (1999)).

1006

MATRIX EXPONENTIATED GRADIENT UPDATES

The proof of this inequality for our setting is given in Appendix E. The bound (4.7) is monotonically decreasing, because (rt)  1. Also, since we always chose a violated constraint (if there is one), we have rt > 0 and therefore (rt) < 1 (or we stop). Thus the dual objective (4.2) continues to increase until all constraints are satisfied.

4.4 Convergence Speed

Next we determine the maximal number of iterations needed to find a matrix W which satisfies all

constraints up to the predetermined accuracy , i.e. tr(W C j)  , for 1  j  n. The algorithm

selects in each iteration an constraint jt that is violated by at least  (i.e. rt = tr(WtC jt )  ), or stops if no such constraint exists. Assuming the algorithm stops at (T + 1)-th step, we derive an

upper bound on T as a function of .

For

simplicity,

let

us

assume

W1

=

1 d

I

,

mj in

= -,

and

mj ax

=

(for

all

j).

Denote by

hprimal(W ) and hdual() the primal and dual objective functions in (4.1) and (4.2), respectively.

hprimal(W ) = F (W , W1)
n
hdual() = - log tr exp log W1 -  j sym(C j) j=1

(4.8) (4.9)

The primal objective is upper-bounded by log d, since F (W , W1) = i i log i + log d  log d. Since the algorithm stops at the (T + 1)-th iteration (with rt   for t = 1, . . . , T ), we get from
Theorem 4.1:

T
exp(-hdual(~ )) = tr exp log W1 - t sym(C jt ) t=1



2 - 2 2

T /2
,

where ~ is the cumulative coefficient vector for the constraints, i.e.  j = tT=1 t( jt = j), for

1  j  n.

Thus

the

objective

in

(4.2)

is

lower

bounded

by

1 2

T

2 2

,

since

hdual ()



- log

2 - 2 2

T /2



T 2 22

,

(4.10)

where the last inequality follows by convexity of - log

2 -2 2

with respect to . At the optimal

solution W  and , the values of the objective functions coincide, i.e. hdual() = hprimal(W ).

Finally, we obtain

T 2 22



hdual ()



hdual ( )

=

hprimal (W )



log d,

and

the

upper

bound

T



22 log 2

d

.

In

summary,

we

have

proven

the

following:

Corollary 4.2 Suppose we are solving problem (4.1) with DefiniteBoost, where C j ( j = 1, . . . , n)

are arbitrary solution W 

matrices with to (4.1) exists

min(C and the

j)  - and max algorithm selects

(C j)  in each



and

W1

=

1 d

I

.

Assume

an

optimal

iteration an -violated constraint, i.e.

1007

TSUDA, RA¨ TSCH AND WARMUTH

rt

= tr(Wt C jt ) 

,

or

stops

if

no

such

constraint

exists.

Then

after

at

most

T

=

22 log d 2

iterations,

DefiniteBoost stops and the resulting W satisfies all linear constraints up to accuracy , i.e.

tr(W C j)   for all j = 1, . . . , n.
This result implies that we can solve (4.1) with accuracy  in O(d3 log d/2) operations (exclud-
ing the cost of identifying violated constraints). Similar bounds on the number of iterations for solving a system of linear equations with the EG algorithm were first proven in (Littlestone et al., 1992, Corollary 15). Observe that if (4.1) is not feasible, then one may continue finding -violated constraints and the primal objective can become unbounded, i.e. t t may become unbounded.

4.5 Relation to Boosting

When all matrices are diagonal, then DefiniteBoost specializes to the AdaBoost algorithm (Schapire

and Singer, 1999). Let {xi, yi}id=1 be the training samples, where xi  Rm and yi  {-1, 1}. Let h1(x), . . . , hn(x)  [-1, 1] be the weak hypotheses. For the j-th hypothesis h j(x), let us define C j =

diag(y1h j(x1), . . . , ydh j(xd)). Since |yh j(x)|  1, we may choose tmax = 1 and tmin = -1 for any t.

Setting

W1

=

1 d

I

,

the

dual

objective

(4.7)

is

rewritten

as

 -log

1d exp
d i=1

n
-yi  jh j(xi)
j=1

,

which is equivalent to the exponential loss function used in AdaBoost. Since C j and W1 are di-

agonal, the matrix Wt stays diagonal after the update. If wt,i = (Wt)i,i, the updating formula (4.5)

becomes the AdaBoost update: wt+1,i = wt,i exp(-tyiht (xi))/Zt(t). The approximate solution of

t

(4.6)

is

described

as

t

=

1 2

log

1+rt 1-rt

,

where

rt

is

the

weighted

training

error

of

the

t-th

hypothesis,

i.e. rt = di=1 wt,iyiht (xi).

4.6 Solving Semi-definite Programs

Suppose we aim to solve the following semi-definite programming problem:

W  = argmin
W ,
s.t.


tr(W ) = 1, W 0, W = W tr(W C j)  , for j = 1, . . . , n.

(4.11)

If one would know the optimal  beforehand, then following problem would lead to an optimal solution of (4.11):

W  = argmin
W
s.t.

F

(W

,

1 d

I

)

tr(W ) = 1, W = W tr(W (C j - I))  0, for j = 1, . . . , n.

(4.12)

Running DefiniteBoost on the above problem with matrices C j = (C j - I) can approximate the solution of (4.12) rather efficiently and, hence, it is only left to determine the optimal value

1008

MATRIX EXPONENTIATED GRADIENT UPDATES

. If it is chosen too small, then no feasible solution to (4.12) exists and DefiniteBoost will not terminate after 22 log d/2 iterations with accuracy ,7 where min(Cj)  - and max(Cj)  .
If it is chosen too large, then a feasible solution exists and DefiniteBoost terminates in a bounded number of iterations. Hence one has a way of identifying when  <  and also  > . This allows the design of a binary search procedure to approximate  in a few steps. Based on this idea we
previously proposed a margin maximizing version of AdaBoost (Ra¨tsch and Warmuth, 2002). For
this algorithm we could show that after O(log d log(1/)/2) iterations the algorithm achieved an
optimal solution within accuracy . We claim that the outlined binary search procedure can also
be applied in combination with DefiniteBoost for solving the semi-definite problem (4.11) in time
O(d3 log d log(1/)/2) (excluding the cost of identifying violated constraints). Additionally we
assert that a slightly more advanced adaptation of  during the optimization (as was done by Ra¨tsch,
2001; Ra¨tsch and Warmuth, 2005, for the diagonal case) will yield the reduced time complexity of
O(d3 log d/2). Rigorous proofs of these conjectures go beyond the scope of this paper.

5. Experiments on Learning Kernels
In this section, our technique is applied to learning a kernel matrix from a set of distance measurements. This application is not on-line per se, but it shows nevertheless that the theoretical bounds can be reasonably tight on natural data.
When K is a d × d kernel matrix among d objects, then the Ki j characterizes the similarity between objects i and j. In the feature space, Ki j corresponds to the inner product between object i and j, and thus the Euclidean distance can be computed from the entries of the kernel matrix (Scho¨lkopf and Smola, 2002). In some cases, the kernel matrix is not given explicitly, but only a set of distance measurements is available. The data are represented either as (i) quantitative distance values (e.g., the distance between i and j is 0.75), or (ii) qualitative evaluations (e.g., the distance between i and j is small) (Xing et al., 2003; Tsuda and Noble, 2004). Our task is to obtain a positive definite kernel matrix which fits well to the given distance data.

5.1 On-line Kernel Learning

In the first experiment, we consider the on-line learning scenario in which only one distance example
is shown to the learner at each time step. The distance example at time t is described as {at, bt, yt},
which indicates that the squared Euclidean distance between objects at and bt is yt. Let us define a time-developing sequence of kernel matrices as {Wt}tT=1, and the corresponding points in the feature space as {xti}id=1 (i.e. (Wt)ab = xtaxtb). Then, the total loss incurred by this sequence is

 T

xtat - xtbt

2 - yt

2=

T
(tr(Wt Xt ) - yt )2,

t=1 t=1

where Xt is a symmetric matrix whose (at, at) and (bt, bt) elements are 0.5, (at, bt) and (bt, at)

elements are -0.5, and all the other elements are zero. We consider a controlled experiment in which

the distance examples are created from a known target kernel matrix. We used a 52 × 52 kernel

matrix among gyrB proteins of bacteria (d = 52). This data contains three bacteria species (see

Tsuda et al., 2003, for details). Each distance example is created by randomly choosing one element

of

the

target

kernel.

The

initial

parameter

was

set

as

W1

=

1 d

I.

When

the

comparison

matrix

U

is

set

7. This statement is slightly simplified. Please check Ra¨tsch and Warmuth (2002) for details.

1009

TSUDA, RA¨ TSCH AND WARMUTH

Total Loss Classification Error

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0 0 0.5 1 1.5 2 2.5 3

Iterations

x 105

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05 0 0.5 1 1.5 2 2.5 3

Iterations

x 105

Figure 2: Numerical results of on-line learning. (Left) total loss against the number of iterations. The dashed line shows the loss bound. (Right) classification error of the nearest neighbor classifier using the learned kernel. The dashed line shows the error by the target kernel.

to the target matrix, then because all the distance examples are derived from this matrix, LU (S) = 0 and Lmax = 0. Therefore we choose learning rate  = 2, which minimizes the relative loss bound of Lemma 3.2. The total loss of the kernel matrix sequence obtained by the matrix exponential update is shown in Figure 2 (left). In the plot, we have also shown the relative loss bound. The bound seems to give a reasonably tight performance guarantee--it is about twice the actual total loss. To evaluate the learned kernel matrix, the prediction accuracy of bacteria species by the nearest neighbor classifier is calculated (Figure 2, right), where the 52 proteins are randomly divided into 50% training and 50% testing data. The value shown in the plot is the test error averaged over 10 different divisions. It took a large number of iterations ( 2 × 105) for the error rate to converge to the level of the target kernel. In practice one can often increase the learning rate for faster convergence, but here we chose the small rate suggested by our analysis to check the tightness of the bound.
5.2 Kernel Learning by Bregman Projection
Next, let us consider a batch learning scenario where we have a set of qualitative distance evaluations (i.e. inequality constraints). Given n pairs of similar objects {a j, b j}nj=1, the inequality constraints are constructed as xaj -xbj  , j = 1, . . . , n, where  is a predetermined constant. If X j is defined as in the previous section and C j = X j - I, the inequalities are then rewritten as tr(W C j)  0, j = 1, . . . , n. The largest and smallest eigenvalues of any C j are 1 -  and -, respectively. As in the previous section, distance examples are randomly generated from the target kernel matrix between gyrB proteins. Setting  = 0.2/d, we collected all object pairs whose distance in the feature space is less than  to yield 980 inequalities (n = 980). Figure 3 (left) shows the convergence of the dual objective function as proven in Theorem 4.1. The convergence was much faster than the previous experiment, because in the batch setting, one can choose the most unsatisfied constraint and optimize the step size as well. Figure 3 (right) shows the classification error of the nearest
1010

MATRIX EXPONENTIATED GRADIENT UPDATES

Dual Obj Classification Error

55 0.8

50 0.7

45 0.6

40 0.5

35 0.4

30 0.3

25 0.2

20 0.1

15 0 50 100 150 200 250 300 Iterations

0 0 50 100 150 200 250 300
Iterations

Figure 3: Numerical results of Bregman projection. (Left) convergence of the dual objective function. (Right) classification error of the nearest neighbor classifier using the learned kernel.

neighbor classifier. As opposed to the previous experiment, the error rate is higher than that of the target kernel matrix, because a substantial amount of information is lost by the conversion to inequality constraints.
6. Summary and Discussion
We motivated and analyzed a new update for symmetric positive matrices using the von Neumann divergence. We showed that the standard bounds for on-line learning and boosting generalize to the case when the parameters are symmetric positive definite matrices of trace one instead of a probability vector. As in quantum physics, the eigenvalues act as probabilities. In addition to the applications suggested by the experiments, our algorithm can be straightforwardly applied to learning a covariance matrix. It would also be interesting to use a robust loss Lt(W ) for the purpose of ignoring outliers (Huber, 1981) and investigate possible applications of our learning algorithms to quantum statistical inference problems (Barndorff-Nielsen et al., 2003).
Our method is designed for learning a positive definite parameter matrix of fixed size. It is not straightforward to extend it to the case where the size of the parameter matrix grows on-line as more examples are seen. Our methods immediately generalize to the Hermitian matrices, i.e. square matrices in Cd×d for which A = A¯ = A. The spectral decomposition of these matrices becomes A = U U , where U is a unitary matrix (i.e. U U  = I) and  is a diagonal matrix of real eigenvalues. In the case when all entries of the matrix are real, then Hermitian is equivalent to symmetric. All algorithms of this paper (and their analyzes) immediately generalize to the case when symmetric is replaced by Hermitian and symmetric positive definite by positive Hermitian (i.e. Hermitian with positive eigenvalues). In particular, the Golden-Thompson inequality, Jensen's inequality for the matrix exponential (Lemma 2.1) and Lemma 2.2 all hold for Hermitian matrices. Note that density matrices (as used in Statistical Physics) are positive Hermitian matrices of trace one.
1011

TSUDA, RA¨ TSCH AND WARMUTH

Acknowledgments
We dedicate this paper to Nick Littlestone who first proved relative loss bounds for an algorithm in the EG family--his well known Winnow algorithm for learning disjunctions (Littlestone, 1988, 1989).
K.T. and G.R. gratefully acknowledge partial support from the PASCAL Network of Excellence (EU #506778). M.W. was supported by NSF grant CCR 9821087 and UC Discovery grant LSIT0210110. This work was partially done while G.R. was still at Fraunhofer FIRST in Berlin and Max Planck Institute for Biological Cybernetics in Tu¨bingen. Part of this work was done while all three authors were visiting the National ICT Australia in Canberra.

Appendix A. Derivatives of Matrix Functions

The matrix functions considered in this paper are mostly trace functions e.g. tr(exp(W )) and
tr(W log W ), which we will expand into power series. Thus we begin with computing the gradient of F(W ) = tr(W k). The partial derivative with respect to (i, j) element is described as

tr(W Wi j

k)

=

lim
0

tr((W

+

Ei j)k) - tr(W 

k),

where Ei j is the sparse matrix whose (i, j) element is one and all the others are zero. For example, when k = 3,

(W + Ei j)3 = (W 3 + Ei jW W + W Ei jW + W W Ei j) + O(2).

The trace is simply described as

tr((W + Ei j)3) = tr(W 3) + 3tr(Ei jW 2) + O(2) = tr(W 3) + 3[W 2] j,i + O(2).
Therefore, W tr(W 3) = 3(W 2) . For general k, we get W tr(W k) = k(W k-1) .

(A.1)

The matrix exponential is defined as

exp(W ) = I + W + 1 W 2 + 1 W 3 + · · · . 2! 3!

Applying (A.1) to all terms, we get W tr(exp(W )) = exp(W ) . Next, let us calculate the gradient of tr(W log W - W ). Using the expansion

log W =  (-1)i-1 (W - I)i,
i=1 i

we get

W

log W

-W

=



i=2

(-1)i i(i - 1)

(W

- I)i - I.

Applying the shifted version of (A.1), i.e. W tr((W - I)k) = k((W - I)k-1) , to all terms, the gradient is obtained as W tr(W log W - W ) = (log W ) . When W is symmetric, then one can drop the transposition. Thus in in this case W tr(exp W ) = exp W .

1012

MATRIX EXPONENTIATED GRADIENT UPDATES

Appendix B. Derivation of the MEG Update
In this appendix we derive parameter updates when the parameter must meet some linear constraints. One method is to incorporate such constraints into the strictly convex function F defining the Bregman divergence. The modified function F is then only defined when the constraints are met. The updates always have the simple form (3.2). However this method often leads to difficult forms of F and f = F. Here we choose the alternate method of keeping the linear constraints on the side. We begin by discussing how to enforce symmetry. Consider the following optimization problem, where Xt is an arbitrary matrix in Rd×d, Wt an arbitrary symmetric matrix in Rd×d and yt  R:

Wt+1 = argmin
W
s.t. W = W .

F(W , Wt) + Lt (W )

We assume that W Lt(W ) is always a well defined matrix in Rd×d. We introduce one Lagrange multiplier i, j for the each of the constraints Wi, j = W j,i. This
contributes the term i, j(Wi, j - W j,i) to the Lagrangian. In matrix form these constraints can be summarized as tr((W - W )) = tr(( - )W ). This gives us the Lagrangian

L(W , ) = F(W , Wt) + Lt(W ) + tr(( - )W ).
for   Rd×d. Setting the gradient with respect to W to zero yields:

Wt+1 = f -1 f (Wt ) - W Lt (Wt+1) - ( -  ) .

Since the objective is convex, it suffices to exhibit a choice of  such that the symmetry constraint is satisfied. Under the assumption that f and f -1 preserve symmetry,  = -W Lt(Wt+1)/2 achieves this and the update becomes (3.3):
Wt+1 = f -1 f (Wt ) -  sym(W Lt (Wt+1)) ) .
For the normalized case we still need to enforce the trace one constraint on Wt+1. This adds a term (tr(W ) - 1) to the Lagrangian and the update now has the form

Wt+1 = exp log Wt - W Lt (Wt+1) - ( -  ) - I .

Choosing  = -W Lt(Wt+1)/2 and
 = - log (tr(exp(log Wt -  sym(W Lt(Wt+1)))))
enforces the symmetry and trace constraints and after approximating the gradient we arrive at the explicit MEG update (3.5).

Appendix C. Proof of Lemma 3.1
Let t = -2(tr(XWt) - yt), then the right hand side of (3.6) can be reformulated as F (U , Wt) - F (U , Wt+1) = ttr(U Xt ) - log tr(exp(log Wt + t sym(Xt))).

1013

TSUDA, RA¨ TSCH AND WARMUTH

Therefore, (3.6) is equivalent to f  0, where

f = log tr(exp(log Wt + t sym(Xt))) - ttr(U Xt ) + a(yt - tr(WtXt ))2 - b(yt - tr(U Xt))2.

Let us bound the first term. Due to Golden-Thompson inequality (2.3), we have

tr (exp(log Wt + t sym(Xt)))  tr (Wt exp(t sym(Xt))) .

(C.1)

The right hand side can be rewritten as

exp(t sym(Xt)) = exp(r0t) exp(t(sym(Xt) - r0I)).

Let r0 be a lower bound of the eigenvalues of sym(Xt). By assumption, the range of the eigenvalues of sym(Xt) is at most r, i.e.
r0I sym(Xt) (r0 + r)I.

Thus 0 A I, for A = (sym(Xt) - r0I)/r. Applying Lemma 2.1 with this choice of A and 1 = rt, 2 = 0, we obtain

exp(t(sym(Xt) - r0I))

I

-

sym(Xt r

)

-

r0

I

(1

-

exp(rt

)).

Since Wt is symmetric positive definite and both sides of the above inequality are symmetric, we can apply Lemma 2.2 by pre-multiplying the inequality by Wt and taking a trace of both sides:

tr (Wt exp(t sym(Xt)))  exp(r0t)

1

-

tr(Wt

Xt r

)

-

r0

(1

-

exp(rt

))

.

Note that we used the assumption that tr(Wt) = 1. The above gives an upper bound on the right hand side of inequality (C.1) We now plug this upper bound into the first term of f and obtain f  g,
where

g=

r0t

+

log(1

-

tr(Wt

Xt r

)-r0

(1

-

exp(rt

)))

-

tr(U

Xt

)t

+a(yt - tr(Wt Xt ))2 - b(yt - tr(U Xt ))2.

(C.2)

Let

us

define

z

=

tr(U Xt)

and

maximize

the

upper

bound

(C.2)

with

respect

to

z.

Solving

g z

=

0,

we have z = yt - t/(2b) = yt + (tr(XtWt) - yt)/b. Substituting this into (C.2), we have the upper

bound g  h where

h=

2r0(yt - tr(XtWt )) + log

1

-

tr(Xt

Wt r

)-r0

(1

-

exp(2r(y

-

tr(Xt

Wt

))))

-2yt

(yt

-

tr(Xt

Wt

))

+

(a

+

2 b

(y

-

tr(Xt

Wt

))2.

We now upper bound the second term using the inequality log(1 - p(1 - exp q))  pq + q2/8, for 0  q  1 and q  R (Helmbold et al., 1997):

h  (yt - tr(XtWt ))2 ((2 + r2b)2 - 4b + 2ab). 2b
It remains to show q = (2 + r2b)2 - 4b + 2ab  0. We easily see that q is minimized for  = 2b/(2 + r2b) and that for this value of  we have q  0 if and only if a  2b/(2 + r2b).

1014

MATRIX EXPONENTIATED GRADIENT UPDATES

Appendix D. Derivation of the DefiniteBoost Dual Problem

For the sake of brevity we assume that the primal problem has one inequality constraint (note that (4.1) has multiple constraints):

W  = argmin
W
s.t.

tr(W (log W - log W1) + W1 - W
tr(W C)  0 tr(W ) = 1 W =W .

Following Appendix B we arrive at the Lagrangian

L(W , , , ) := tr(W (log W - log W1) + W1 - W + tr(W C) +
+ (tr(W ) - 1) + tr(( - )W ),

(D.1)

which is minimized w.r.t. W and maximized w.r.t.   0,   R and   Rd×d. Setting the gradient w.r.t. W to zero we obtain

W  = exp(log W1 - C - I - ( -  ) = exp(-) exp(log W1 - C - ( -  ).

We now enforce the symmetry constraint, giving us  = -(C - C )/2, and plug this choice into the above
W  = exp(-) exp(log W1 -  sym(C)).
Similarly,  = log tr (exp(log W1 -  sym(C))) enforces the trace constraint. Now
W  = exp(log W1 -  sym(C)/Z(),

where Z() = - log tr(exp(log W1 -  sym(C))). Plugging W  into in the Lagrangian, we obtain the dual optimization problem for one constraint:

 = argmax - log Zt().
0

One can easily verify that the solution of the problem with n constraints is of the form:

n

 = argmax - log tr(exp(log W1 -   j sym(C j))).

0

j=1

Appendix E. Proof of Theorem 4.1
Recall the definition of the normalization factor Zt() = tr (exp(log Wt -  sym(C jt ))) of DefiniteBoost. By the Golden-Thompson inequality,

Zt ()  tr(Wt exp(- sym(C jt ))).

(E.1)

1015

TSUDA, RA¨ TSCH AND WARMUTH

Similarly to the proof of Lemma 3.1, we now upper bound the right hand side of this inequality by applying lemmas 2.1 and 2.2. We choose A as (tminI + sym(C jt ))/(tmax + tmin). Then sym(C jt ) can be expressed as tmaxA - tmin(I - A) and 0 A I. Thus by Lemma 2.1,
exp(- sym(C jt )) exp(-tmax)A + exp(tmin)(I - A).

Since Wt is positive definite and both sides of the above inequality are symmetric, we can apply Lemma 2.2 by multiplying this inequality by Wt and taking a trace of both sides:

tr(Wt exp(- sym(C jt )))  exp(-tmax)tr(Wt A) + exp(tmin)tr (Wt (I - A)) .

By expanding A and using the shorthand rt = tr(WtC jt ), we obtain

Zt ()



exp(-tmax)

tmin + rt tmax + tmin

+

exp(min)

tmax - rt tmax + tmin

.

We now choose the  that minimizes the right hand side of the above inequality (which is the t given in equation (4.6)). With this choice, the inequality becomes

Zt

(t )



(1

-

rt tmax

)

tmax tmax +tmin

(1

+

rt tmin

)

tmin tmax +tmin

.

Applying the update rule (4.5) T times, we have

(E.2)

WT +1

=

exp(log W1 - tT=1 t t Zt (t )

sym(C jt )) .

Taking the trace of both sides and rearranging terms, we get

TT
tr exp(log W1 -  t sym(C jt )) =  Zt(t). t=1 t=1
By using the bound (E.2) for each Zt(t), the inequality of the theorem readily follows.

References
O. E. Barndorff-Nielsen, R. D. Gill, and P. E. Jupp. On quantum statistical inference. J. R. Statist. Soc. B, 65(4):775­816, 2003.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
L. M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Physics, 7:200­217, 1967.
Y. Censor and A. Lent. An iterative row-action method for interval convex programming. Journal of Optimization Theory and Applications, 34(3):321­353, July 1981.
Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119­139, 1997.

1016

MATRIX EXPONENTIATED GRADIENT UPDATES
S. Golden. Lower bounds for the Helmholtz function. Phys. Rev., 137:B1127­B1128, 1965.
D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. A comparison of new and old algorithms for amixture estimation problem. Machine Learning, 27(1):97­119, 1997.
P. J. Huber. Robust Statistics. John Wiley and Sons, New York, 1981.
J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1­63, 1997.
J. Kivinen and M. K. Warmuth. Boosting as entropy projection. In Proceedings of the 12th Annual Conference on Computational Learning Theory, pages 134­144. ACM Press, New York, NY, 1999.
J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Machine Learning, 45(3):301­329, 2001.
J. Lafferty. Additive models, boosting, and inference for generalized divergences. In Proceedings of the 12th Annual Conference on Computational Learning Theory, pages 125­133. ACM Press, New York, NY, 1999.
N. Littlestone. Learning when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285­318, 1988.
N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. PhD thesis, Technical Report UCSC-CRL-89-11, University of California, Santa Cruz, 1989.
N. Littlestone, P. M. Long, and M. K. Warmuth. On-line learning of linear functions. Technical Report UCSC-CRL-91-29, University of California, Santa Cruz, May 1992.
M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, 2000.
G. Ra¨tsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam, Potsdam, Germany, October 2001.
G. Ra¨tsch and M. K. Warmuth. Maximizing the margin with boosting. In Proceedings of the 15th Annual Conference on Computational Learning Theory, pages 319­333. Springer, Sydney, Australia, 2002.
G. Ra¨tsch and M. K. Warmuth. Efficient margin maximization with boosting. submitted to Journal of Machine Learning Research, 2005.
R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37:297­336, 1999.
B. Scho¨lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
S. Shai-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In C. E. Brodley, editor, Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004). ACM Press, New York, NY, 2004.
1017

TSUDA, RA¨ TSCH AND WARMUTH Y. Singer and M. K. Warmuth. Batch and on-line parameter estimation of Gaussian mixtures based
on the joint entropy. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11 (NIPS'98), pages 578­584. MIT Press, 1999. I. W. Tsang and J. T. Kwok. Distance metric learning with kernels. In Proceedings of the International Conference on Artificial Neural Networks (ICANN'03), pages 126­129. Springer Verlag, New York, NY, 2003. K. Tsuda, S. Akaho, and K. Asai. The em algorithm for kernel matrix completion with auxiliary data. Journal of Machine Learning Research, 4:67­81, May 2003. K. Tsuda and W. S. Noble. Learning kernels from biological networks by maximizing entropy. Bioinformatics, 20(Suppl. 1):i326­i333, 2004. E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 505­512. MIT Press, Cambridge, MA, 2003.
1018

